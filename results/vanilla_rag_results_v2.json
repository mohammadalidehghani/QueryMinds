[
  {
    "question": "What problem, limitation, or research gap motivates a machine learning approach?",
    "answer": "\nConclusion:\nIn this paper, we have shown that a combination of machine learning with Bayesian techniques is an efficient method for addressing the problem of obtaining reliable and efficient algorithms for high-dimensional data sets. It is based on the idea that the characteristics of the data set are very likely to be well correlated with the effectiveness of a machine learning approach, and that the Bayesian approach can provide a simple and powerful way to model the distribution of the data, while being able to estimate the parameters of the model with minimal error. Furthermore, the Bayesian approach allows us to formulate a rigorous and transparent methodology for selecting the best models among a large number of different ones.\n\nThis study aims to contribute to a comprehensive understanding of the most popular topics in machine learning research, as they were identified from a quantitative survey of 54K papers published between 2007 and 2016 in leading machine learning journals and conferences. We identified 10 topics as the most prolific topics in machine learning research, and the 30 most popular models in machine learning research, and we use a machine learning quantitative approach to evaluate the top 10 topics for their impact on the",
    "retrieved_chunks": [
      "Which topics of machine learning are most commonly addressed in research ? This question was initially answered in 2007 by doing a qualitative survey among distinguished researchers . In our study , we revisit this question from a quantitative perspective . Concretely , we collect 54K abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences . We then use machine learning in order to determine the top 10 topics in machine learning . We not only include models , but provide a holistic view across optimization , data , features , etc . This quantitative approach allows reducing the bias of surveys . It reveals new and up-to-date insights into what the 10 most prolific topics in machine learning research are . This allows researchers to identify popular topics as well as new and rising topics for their research .",
      "This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts . This attempt aims at reducing bias and looking where the research community puts its focus on . The results of this study allow researchers to put their research into the global context of machine learning . This provides researchers with the opportunity to both conduct research in popular topics and identify topics that have not received sufficient attention in recent research . The rest of this paper is organized as follows . Section 2 describes the data sources and quantitative methodology . Section 3 presents and discusses the top 10 topics identified . Section 4 summarizes this work .",
      "Introduction The term machine learning ( ML ) traditionally includes algorithms that are able to improve their performance on a specific task over time , given an increasing amount of relevant data [ 1 ] . In recent years , this field of research is enjoying a growing popularity , driven by the breakthrough of Deep Learning [ 2 ] and an impressive track record of success stories in different fields , ranging from natural language processing [ 3 ] to autonomous vehicles [ 4 ] , image classification [ 5 ] human-competitive performance in boardgames [ 6 ] . An interesting online collection about various uses of ML has been compiled by the journal Nature is late 2018 1 , although the fast pace the field is progressing made it to appear outdated after few quarters . As out-of-the-box ML solutions are becoming increasingly available to both researchers and the general public [ 7 , 8 , 9 , 10 ] , theoretical questions are suddenly turning into practical issues . Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ?",
      "Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ? Interestingly , this latter question can be further rephrased as : what are the characteristics of a data set that are well correlated with the possibility , or the impossibility , of obtaining ML models able to effectively extrapolate to unknown instances of the problem ? It is well known that ML algorithms are affected by the curse of dimensionality [ 11 ] , but ML practitioners also know that it could be possible to obtain reliable models even for high-dimensional data sets , and with a relatively small number of samples [ 12 ] . The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] .",
      "Two different views on machine learning problem : Applied learning ( machine learning with business applications ) and Agnostic PAC learning are formalized and compared here . I show that , under some conditions , the theory of PAC Learnable provides a way to solve the Applied learning problem . However , the theory requires to have the training sets so large , that it would make the learning practically useless . I suggest to shed some theoretical misconceptions about learning to make the theory more aligned with the needs and experience of practitioners ."
    ],
    "question_id": "q1"
  },
  {
    "question": "What is the central idea or key contribution of a machine learning approach?",
    "answer": "\n2.1 Learning can not be explained in extrinsic terms only\n\nThe classical definition of machine learning is based on the optimization of a parametric model with respect to some performance measure, typically the mean squared error ( MSE ) or the log likelihood ( LL ). These measures are often taken to be intrinsic, since they directly measure the performance of the model. On the other hand, the idea underlying model entropy ( ME ) is that the internal learning process leads to the formation of intrinsic models.\n\nThe traditional machine learning approach is based on the minimization of the objective function ( MSE or LL ). However, this formulation is not self-evident, as it is based on the assumption that the internal learning process is fixed and that the external validation is the primary mechanism of learning.\n\nThe intrinsic model approach leads to the definition of the model entropy as the measure of the efficiency of internal learning processes. Indeed, the internal learning process is assumed to be intrinsically multiobjective and strives to optimize a set of internal objectives. This notion is further expanded by the idea of meta-cognitive machine learning, which suggests that the internal learning processes also take into account their own learning process",
    "retrieved_chunks": [
      "This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts . This attempt aims at reducing bias and looking where the research community puts its focus on . The results of this study allow researchers to put their research into the global context of machine learning . This provides researchers with the opportunity to both conduct research in popular topics and identify topics that have not received sufficient attention in recent research . The rest of this paper is organized as follows . Section 2 describes the data sources and quantitative methodology . Section 3 presents and discusses the top 10 topics identified . Section 4 summarizes this work .",
      "Which topics of machine learning are most commonly addressed in research ? This question was initially answered in 2007 by doing a qualitative survey among distinguished researchers . In our study , we revisit this question from a quantitative perspective . Concretely , we collect 54K abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences . We then use machine learning in order to determine the top 10 topics in machine learning . We not only include models , but provide a holistic view across optimization , data , features , etc . This quantitative approach allows reducing the bias of surveys . It reveals new and up-to-date insights into what the 10 most prolific topics in machine learning research are . This allows researchers to identify popular topics as well as new and rising topics for their research .",
      "Introduction The term machine learning ( ML ) traditionally includes algorithms that are able to improve their performance on a specific task over time , given an increasing amount of relevant data [ 1 ] . In recent years , this field of research is enjoying a growing popularity , driven by the breakthrough of Deep Learning [ 2 ] and an impressive track record of success stories in different fields , ranging from natural language processing [ 3 ] to autonomous vehicles [ 4 ] , image classification [ 5 ] human-competitive performance in boardgames [ 6 ] . An interesting online collection about various uses of ML has been compiled by the journal Nature is late 2018 1 , although the fast pace the field is progressing made it to appear outdated after few quarters . As out-of-the-box ML solutions are becoming increasingly available to both researchers and the general public [ 7 , 8 , 9 , 10 ] , theoretical questions are suddenly turning into practical issues . Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ?",
      "In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows . The theoretical ideas are laid out and the case for a new operational definition of machine learning is made . We put forward the conjecture that the optimization of model entropy , leads to concept formation . Last , conclusions and further steps to operationalize these concepts are formulated . 2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a ( non- ) parametric model with respect to some performance measure . This view is clearly reflected in the de facto definition of machine learning by Mitchell [ Mitchell , 1997 ] : `` A computer program is said to learn from an experience X with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experimental data D '' . Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures .",
      "Machine learning is usually defined in behaviourist terms , where external validation is the primary mechanism of learning . In this paper , I argue for a more holistic interpretation in which finding more probable , efficient and abstract representations is as central to learning as performance . In other words , machine learning should be extended with strategies to reason over its own learning process , leading to so-called meta-cognitive machine learning . As such , the de facto definition of machine learning should be reformulated in these intrinsically multiobjective terms , taking into account not only the task performance but also internal learning objectives . To this end , we suggest a `` model entropy function '' to be defined that quantifies the efficiency of the internal learning processes . It is conjured that the minimization of this model entropy leads to concept formation ."
    ],
    "question_id": "q2"
  },
  {
    "question": "How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?",
    "answer": "\nExample:\nConsider the decision problem in which a hypothetical autonomous vehicle is to learn from data to determine the most efficient path for a human driver to take. The goal of the learning process is to optimize the vehicle's path, so as to minimize the total cost of the journey. Understanding the relationship between the learning objectives and the information-theoretic concepts used to measure them is crucial to developing effective learning algorithms. Understanding the properties of the information-theoretic metrics used in this context can help in developing algorithms that are not only effective but also computationally efficient. The paper also highlights the potential use of information-theoretic concepts in machine learning, and provides a novel conjecture for pursuing unified mathematical interpretation to learning objectives.\n\nConclusion:\nInformation-theoretic concepts play a crucial role in the study of learning objectives and targets, and a novel conjecture for pursuing unified mathematical interpretation to learning objectives makes this connection more explicit. Developing effective learning algorithms that optimize the learning objectives and the information-theoretic metrics used in this context is critical for achieving better performance.",
    "retrieved_chunks": [
      "To this end , we suggest a `` model entropy function '' to be defined that quantifies the efficiency of the internal learning processes . It is conjured that the minimization of this model entropy leads to concept formation . Besides philosophical aspects , some initial illustrations are included to support the claims .",
      "In this position paper , I first describe a new perspective on machine learning ( ML ) by four basic problems ( or levels ) , namely , `` What to learn ? `` , `` How to learn ? `` , `` What to evaluate ? `` , and `` What to adjust ? `` . The paper stresses more on the first level of `` What to learn ? `` , or `` Learning Target Selection '' . Towards this primary problem within the four levels , I briefly review the existing studies about the connection between information theoretical learning ( ITL [ 1 ] ) and machine learning . A theorem is given on the relation between the empirically-defined similarity measure and information measures . Finally , a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection .",
      "In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows . The theoretical ideas are laid out and the case for a new operational definition of machine learning is made . We put forward the conjecture that the optimization of model entropy , leads to concept formation . Last , conclusions and further steps to operationalize these concepts are formulated . 2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a ( non- ) parametric model with respect to some performance measure . This view is clearly reflected in the de facto definition of machine learning by Mitchell [ Mitchell , 1997 ] : `` A computer program is said to learn from an experience X with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experimental data D '' . Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures .",
      "Machine learning is usually defined in behaviourist terms , where external validation is the primary mechanism of learning . In this paper , I argue for a more holistic interpretation in which finding more probable , efficient and abstract representations is as central to learning as performance . In other words , machine learning should be extended with strategies to reason over its own learning process , leading to so-called meta-cognitive machine learning . As such , the de facto definition of machine learning should be reformulated in these intrinsically multiobjective terms , taking into account not only the task performance but also internal learning objectives . To this end , we suggest a `` model entropy function '' to be defined that quantifies the efficiency of the internal learning processes . It is conjured that the minimization of this model entropy leads to concept formation .",
      "It is the author 's belief that `` mathematical-principle-based machine '' might be more important and critical than `` brain-inspired machine '' in the study of machine learning . The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning . In what follows I will present four basic problems ( or levels ) in machine learning . The study on information theoretical learning is briefly reviewed . A theorem between the empirically-defined similarity measures and information measures are given . Based on the existing investigations , a conjecture is proposed in this paper ."
    ],
    "question_id": "q3"
  },
  {
    "question": "How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?",
    "answer": "\nMachine learning methods typically use uncertainty quantification (Q) to provide a probabilistic model of the output for an input. Q can be interpreted as a measure of the likelihood of an outcome given the input. To understand how machine learning methods represent and use uncertainty, we can consider the following stages:\n\n1. Data generation and processing: Machine learning methods use data as inputs to generate model outputs. In data-driven methods such as AI, data is usually generated from some distribution (e.g., Gaussian, Bernoulli). This distribution is generated based on the data itself, which is often generated from the input domain. The distribution may be assumed to be Gaussian for many machine learning methods (e.g., regression models, classification models). The input domain is typically assumed to be independent of the output distribution.\n\n2. Model training: Machine learning models are trained with data from the input domain. The trained model output from the training data is used to generate model outputs for new inputs. The model output is typically not directly used in the output distribution.\n\n3. Model evaluation: Machine learning models are evaluated on new inputs to generate predictions. The predictions are not directly used in the input distribution.\n\n4. Q estimation: The",
    "retrieved_chunks": [
      "Quantifying and managing uncertainties that occur when data-driven models such as those provided by AI and machine learning methods are applied is crucial . This whitepaper provides a brief motivation and first overview of the state of the art in identifying and quantifying sources of uncertainty for data-driven components as well as means for analyzing their impact .",
      "Introduction Machine learning usually aims to find and develop a computational model for an intelligent task on a practical problem . Development based on calculation can be called engineering ( 1 ) . In software engineering , software systems are calculated and engineered by models . In fact , these software models are platforms for analysis , design , development , and system engineering . For modeling in software engineering , we need several elements : 1 . The modeling perspective , 2 . The system under modeling , and 3 . Modeling language and tools ( 5 ) . So we look at a system under modeling from one or several perspectives , and we discover a set of meanings about that system . Using the modeling language and tool , we express and record those perceived meanings of the system . This allows us to engineer the system under modeling ( as-is system or to-be system ) by changing and transforming it . We could perhaps consider the modeling perspective as the most crucial part of this conceptual architecture . Because if we do not use a proper perspective or perspectives to look at the meanings of a system , it is practically impossible for us to achieve a model , start modeling , and ultimately develop through engineering calculations .",
      "As a consequence , the functional behavior expected from data-driven components can only be specified in part on their intended domain , and we can not assure that they will behave as expected in all cases . Moreover , their processing structure is usually difficult to trace and validate by humans because this structure rarely follows human intuition but is generated to provide the algorithmically generalized input-output relationship in an effective manner . Prominent representatives of models used by data-driven components are artificial neural networks and support vector machines ( Russell & Norvig , 2016 ) . Since data-driven models are an important source of uncertainty in embedded systems that collaborate in an open context , the uncertainty they introduce has to be appropriately understood and managed during design time and runtime . Previous work ( Kl√§s & Vollmer , 2018 ) proposes separating the sources of uncertainty in data-driven components into three major classes , distinguishing between uncertainty caused by limitations in terms of model fit , data quality , and scope compliance . Whereas model fit focuses on the inherent uncertainty in data-driven models , data quality covers the additional uncertainty caused by their application to input data obtained in suboptimal conditions and scope compliance covers situations where the model is likely applied outside the scope for which it was trained and validated .",
      "Current developments are towards specialising these ML approaches to be more domain-specific and domain-aware [ 5 ] [ 6 ] [ 7 ] , and aiming to connect the apparent 'black box ' successes of DL networks with well-understood approaches from science . The overarching scope of ML in science is very broad , including identifying patterns , anomalies , and trends from relevant scientific datasets , and using ML for classification and predicting of those patterns , clustering of data , and generating near-realistic synthetic data . There are three approaches for developing ML-based solutions , namely , supervised , unsupervised , and reinforcement learning . In supervised learning , the ML model is trained for a given task with examples . In order to have examples , the data used for training the ML model must contain the ground truth or labels . Supervised learning is therefore only possible when there is a labelled subset of the data . Once trained , the learned model can be deployed for real-time usage , such as pattern classification or estimation -- -which is often referred to as inference . Because of the difficulty in generating labelled data for supervised learning , particularly for experimental datasets , it is often difficult to apply supervised learning directly . To circumvent this limitation , training is often performed on simulated data , which provides an opportunity to have relevant labels .",
      "Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ? Interestingly , this latter question can be further rephrased as : what are the characteristics of a data set that are well correlated with the possibility , or the impossibility , of obtaining ML models able to effectively extrapolate to unknown instances of the problem ? It is well known that ML algorithms are affected by the curse of dimensionality [ 11 ] , but ML practitioners also know that it could be possible to obtain reliable models even for high-dimensional data sets , and with a relatively small number of samples [ 12 ] . The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] ."
    ],
    "question_id": "q4"
  },
  {
    "question": "What evaluation protocols and benchmark setups are commonly used to assess model performance?",
    "answer": "\nExplanation:\nScientific machine learning (ML) benchmarks are important tools for assessing model performance, identifying best practices, and evaluating the suitability of ML algorithms for scientific research. ML benchmarks provide a standardized framework for evaluating the performance of ML algorithms on various datasets and configurations, allowing scientists to compare different models and algorithms and identify the most effective ones for their specific applications. The evaluation protocols and benchmark setups used in ML benchmarking vary depending on the type and scope of the problem, but typically involve the following steps:\n\n1. Data preprocessing: This step involves normalizing, scaling, and cleaning the data before feeding it into the ML model.\n\n2. Model selection: This step involves selecting the model architecture and hyperparameters that best suit the problem at hand.\n\n3. Training and validation: This step involves training and evaluating the model on a training dataset and validating its performance on a validation dataset.\n\n4. Testing: This step involves evaluating the model on a test dataset to determine its overall accuracy and suitability for practical use.\n\n5. Reporting: This step involves summarizing the results of the benchmarking process and providing recommendations for",
    "retrieved_chunks": [
      "This constitutes a significant barrier for many scientists wishing to use modern ML methods in their scientific research . In this paper we use suitable scientific ML benchmarks to develop guidelines and best practices to assist the scientific community in successfully exploiting these methods . Moreover , developing such guidelines and best practices at the community level will not only benefit the science community but also highlight where further research into ML algorithms , computer architectures , and software solutions for using ML in scientific applications is needed . Such guidelines and best practices need to be based on real-world application examples and relevant data . For instance , demonstrating the success of a specific ML technique on a specific scientific problem will assist researchers in applying the technique to similar problems . We refer to the development of guidelines and best practices as Benchmarking . In our case , this is very specific to ML techniques applied to scientific datasets . The applications used to demonstrate the guideline and best practices are referred to as Benchmarks . The notion of benchmarking computer systems and applications has been a fundamental cornerstone of computer science , particularly for compiler , architectural and system development , with a key focus on using benchmarks for ranking systems , such as the Top500 or Green500 [ 12 ] [ 13 ] [ 14 ] [ 15 ] [ 16 ] .",
      "The notion of benchmarking computer systems and applications has been a fundamental cornerstone of computer science , particularly for compiler , architectural and system development , with a key focus on using benchmarks for ranking systems , such as the Top500 or Green500 [ 12 ] [ 13 ] [ 14 ] [ 15 ] [ 16 ] . However , our notion of scientific ML benchmarking has a different focus . Firstly , these machine learning benchmarks can be considered as blueprints for use on a range of scientific problems , and hence are aimed at fostering the use of ML in science more generally . Secondly , by using these ML benchmarks , a number of aspects in an ML ecosystem can be compared and contrasted . For example , it is possible to rank different computer architectures for their performance , or to rank different ML algorithms for their effectiveness . Thirdly , these ML benchmarks are accompanied by relevant dataset ( s ) on which the training and/or inference will be based . This is different to conventional benchmarks for high-performance computing ( HPC ) where there is little dependency on datasets . The establishment of a set of open curated datasets with associated ML benchmarks is therefore an important step for scientists to be able to effectively utilise ML methods in their research and also to identify further directions for ML research .",
      "This is due to many different machine learning frameworks , computer architectures , and machine learning models . Historically , for modelling and simulation on HPC systems such problems have been addressed through benchmarking computer applications , algorithms , and architectures . Extending such a benchmarking approach and identifying metrics for the application of machine learning methods to scientific datasets is a new challenge for both scientists and computer scientists . In this paper , we describe our approach to the development of scientific machine learning benchmarks and review other approaches to benchmarking scientific machine learning .",
      "The rest of this paper is laid out as follows . Section 2 details the datasets we use . Section 3 looks at the methodology used to evaluate the optimal paradigm . In section 4 we present the algorithms we use to test , along with related work influencing our choices in selecting those models . Section 5 details our experiments including choosing the optimal configuration of hyperparameters and preprocessing for each algorithm . In section 6 we present the results followed by our comments and conclusions . Finally , we highlight a few key points and considerations worthy of mention for the two paradigms in 7 .",
      "The establishment of a set of open curated datasets with associated ML benchmarks is therefore an important step for scientists to be able to effectively utilise ML methods in their research and also to identify further directions for ML research . In this paper , we first discuss what we mean by scientific machine learning benchmarks , the scope of such benchmarks , and the challenges in creating such benchmarks . We then review a number of benchmarking initiatives in light of this discussion . The paper is organised as follows . In Section 2 , we discuss the primary considerations in designing benchmarks to advance the application of ML methods for scientific research along with relevant examples . We then define the scope and challenges around establishing such scientific machine learning benchmarks in Section 3 . In Section 4 , we review a number of ML benchmarking initiatives in light of our discussions in Sections 2 and 3 . We then discuss SciMLBench , one of the most recent and versatile scientific ML benchmarking initiatives , in Section 5 . We summarise our findings and conclusions in Section 6 ."
    ],
    "question_id": "q5"
  },
  {
    "question": "What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?",
    "answer": "\nThe main dataset used in this study was obtained from the University of California, San Diego (UCSD) [ 1 ] repository [ 2 ]. The dataset contains 109 publicly available classification data sets from open-access, curated sources [ 3 ]. The dataset was classified into 35 domains, representing various scientific fields, including biology, chemistry, engineering, physics, and social science. The dataset was divided into 10 categories, including 12 medical classes, 4 healthcare systems, 2 engineering classes, 4 social science classes, and 4 physics classes. Moreover, each category was further divided into one or more subcategories, in order to improve the training and test data sets. The dataset was analyzed in 2 phases. In the initial phase, the dataset was preprocessed by extracting features from the data and creating a training and test set. In the final phase, a supervised machine learning model was trained on both the training and test sets, using a standardized evaluation metric [ 16 ]. The dataset was not used for any other purpose during the study.\n\nFurthermore, similar datasets from various sources have been used in the literature, such",
    "retrieved_chunks": [
      "The breakthrough in Deep Learning neural networks has transformed the use of AI and machine learning technologies for the analysis of very large experimental datasets . These datasets are typically generated by large-scale experimental facilities at national laboratories . In the context of science , scientific machine learning focuses on training machines to identify patterns , trends , and anomalies to extract meaningful scientific insights from such datasets . With a new generation of experimental facilities , the rate of data generation and the scale of data volumes will increasingly require the use of more automated data analysis . At present , identifying the most appropriate machine learning algorithm for the analysis of any given scientific dataset is still a challenge for scientists . This is due to many different machine learning frameworks , computer architectures , and machine learning models .",
      "Current developments are towards specialising these ML approaches to be more domain-specific and domain-aware [ 5 ] [ 6 ] [ 7 ] , and aiming to connect the apparent 'black box ' successes of DL networks with well-understood approaches from science . The overarching scope of ML in science is very broad , including identifying patterns , anomalies , and trends from relevant scientific datasets , and using ML for classification and predicting of those patterns , clustering of data , and generating near-realistic synthetic data . There are three approaches for developing ML-based solutions , namely , supervised , unsupervised , and reinforcement learning . In supervised learning , the ML model is trained for a given task with examples . In order to have examples , the data used for training the ML model must contain the ground truth or labels . Supervised learning is therefore only possible when there is a labelled subset of the data . Once trained , the learned model can be deployed for real-time usage , such as pattern classification or estimation -- -which is often referred to as inference . Because of the difficulty in generating labelled data for supervised learning , particularly for experimental datasets , it is often difficult to apply supervised learning directly . To circumvent this limitation , training is often performed on simulated data , which provides an opportunity to have relevant labels .",
      "Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ? Interestingly , this latter question can be further rephrased as : what are the characteristics of a data set that are well correlated with the possibility , or the impossibility , of obtaining ML models able to effectively extrapolate to unknown instances of the problem ? It is well known that ML algorithms are affected by the curse of dimensionality [ 11 ] , but ML practitioners also know that it could be possible to obtain reliable models even for high-dimensional data sets , and with a relatively small number of samples [ 12 ] . The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] .",
      "The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] . Taking inspiration from [ 14 ] , where the authors find links between data set characteristics and efficiency of feature selection techniques , we propose to empirically explore the relation between data-set characteristics and effectiveness of standard ML models , in order to obtain a general meta-model able to extrapolate . In order to answer the question , we analyzed 109 publicly available classification data sets from open-access , curated sources . We decided to focus on classification , as supervised ML represents a quite significant portion of real-world problems ; and , differently from regression , several sophisticated quality metrics have already been developed for this task [ 15 ] . During the analysis , we take into account characteristics such as number of features , number of classes , number of samples , and we look for correlations with quality metrics , such as accuracy of a ML model on training and test points .",
      "INTRODUCTION The age of rapid technological change is unfolding in real time , empowering the collection of massive amounts of data in a variety of fields . Despite this , many fields still struggle with data acquisition with limited sample sizes , particularly in experiments that involve human or animal subjects and can be prohibitively expensive . To improve the performance of the primary task on those occasions , transfer learning has been widely advocated as a means of leveraging knowledge from available auxiliary data that are different while related to the primary data . Successful applications of transfer learning in data-scarce fields include drug development ( Turki , Wei , & Wang 2017 ) , clinical trials ( Bellot & van der Schaar 2019 ) , and material sciences ( Hutchinson et al . 2017 ) , among others . For an overview of transfer learning methodologies and applications , interested readers may refer to survey papers by Pan and Yang ( 2009 ) , Weiss , Khoshgoftaar , and Wang ( 2016 ) , Niu , Liu , Wang , and Song ( 2020 ) , and Zhuang et al . ( 2020 ) . Although transfer learning has achieved pervasive success and shows great promise , there is no guarantee that it will always improve performance -there is no free lunch for transfer learning ."
    ],
    "question_id": "q6"
  },
  {
    "question": "How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?",
    "answer": "\nIntroduction In the field of probability theory, empirical evidence is often used as a measure of successful predictions. However, theoretical analysis is also an essential tool for making predictions about the future. Both empirical evidence and theoretical analysis can be used to make predictions. However, their relationship is not straightforward. In this article, we will discuss this relationship using the example of regression analysis. Regression analysis is a statistical method that aims to predict a dependent variable (e.g., y) from a set of independent variables (e.g., x). One of the most classical and widely used regression models is the linear regression model. In this model, y is a linear function of x:\n\ny = a*x + b\n\nwhere a and b are the regression coefficients. The regression coefficients are estimated by fitting a regression line to the data. The goal of regression analysis is to identify the best regression model that fits the data well. Regression analysis is a powerful tool for making predictions, but it is not without its limitations. One limitation is that the best model might not necessarily be the most interpretable model. Another limitation is that the estimation process can be very sensitive to outliers or noise in the data. This sensitivity is a consequence of the fact",
    "retrieved_chunks": [
      "Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ? Interestingly , this latter question can be further rephrased as : what are the characteristics of a data set that are well correlated with the possibility , or the impossibility , of obtaining ML models able to effectively extrapolate to unknown instances of the problem ? It is well known that ML algorithms are affected by the curse of dimensionality [ 11 ] , but ML practitioners also know that it could be possible to obtain reliable models even for high-dimensional data sets , and with a relatively small number of samples [ 12 ] . The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] .",
      "Introduction In an age of user generated web-contents and of portable devices with embedded computer vision capabilities , machine learning ( ML ) and big data mining questions are fundamental . As a result , these questions naturally penetrate neighboring research fields , including belief function theory ( BFT ) , so that it is now usual to attend a `` Classification '' session [ 26 ] or a `` Machine Learning '' session [ 16 ] in a conference devoted to belief functions . However , it is hard to accept that among the various proposed approaches based on BF , very few have become state-of-the-art ML methods , the knowledge of which has spread beyond the BF community . Without any doubt , this can be partly explained by the relative size of the scientific communities under consideration : although quickly growing , the BF one is relatively small with respect to that of statistics , Bayesian networks , neural networks , etc . However , this reason alone is not sufficient : There are indeed other topics , such as for instance , information fusion , where BF-based methods are now as well recognized as are methods based on more classical formalisms , such as probabilities , or ontologies .",
      "The breakthrough in Deep Learning neural networks has transformed the use of AI and machine learning technologies for the analysis of very large experimental datasets . These datasets are typically generated by large-scale experimental facilities at national laboratories . In the context of science , scientific machine learning focuses on training machines to identify patterns , trends , and anomalies to extract meaningful scientific insights from such datasets . With a new generation of experimental facilities , the rate of data generation and the scale of data volumes will increasingly require the use of more automated data analysis . At present , identifying the most appropriate machine learning algorithm for the analysis of any given scientific dataset is still a challenge for scientists . This is due to many different machine learning frameworks , computer architectures , and machine learning models .",
      "Current developments are towards specialising these ML approaches to be more domain-specific and domain-aware [ 5 ] [ 6 ] [ 7 ] , and aiming to connect the apparent 'black box ' successes of DL networks with well-understood approaches from science . The overarching scope of ML in science is very broad , including identifying patterns , anomalies , and trends from relevant scientific datasets , and using ML for classification and predicting of those patterns , clustering of data , and generating near-realistic synthetic data . There are three approaches for developing ML-based solutions , namely , supervised , unsupervised , and reinforcement learning . In supervised learning , the ML model is trained for a given task with examples . In order to have examples , the data used for training the ML model must contain the ground truth or labels . Supervised learning is therefore only possible when there is a labelled subset of the data . Once trained , the learned model can be deployed for real-time usage , such as pattern classification or estimation -- -which is often referred to as inference . Because of the difficulty in generating labelled data for supervised learning , particularly for experimental datasets , it is often difficult to apply supervised learning directly . To circumvent this limitation , training is often performed on simulated data , which provides an opportunity to have relevant labels .",
      "INTRODUCTION In order to formulate a learning theory of machine learning , it may be necessary to move from seeing an inert model as the machine learner to seeing the human developer-along with , and not separate from , his or her model and surrounding social relations-as the machine learner . -Reigeluth & Castelle [ 55 ] The past decade has seen massive research on interpretable machine learning ( ML ) . 1 Here is a rough restatement of the goal of interpretable ML research program : many ML models are opaque in that even the expert humans can not robustly understand , in non-mathematical terms , the reasons for why particular outputs are generated by these models [ 31 , 42 , 66 ] . To overcome this opacity , various model-centric techniques have been developed to interpret their outputs . These techniques are diverse . They range from producing counterfactual explanations or heatmaps that offer insights into how changing inputs affect outputs [ 28 , 41 , 46 ] , to interpreting the inner workings of the model by probing patterns of neuron activations or attention mechanisms [ 10 , 15 , 48 ] . 2 Despite these advancements , ML interpretability remains a contentious and ambiguous topic in the scientific community , lacking a universally accepted scope and definition [ 11 , 13 , 38 , 45 ] ."
    ],
    "question_id": "q7"
  },
  {
    "question": "How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?",
    "answer": "\nBased on the passage above, Summarize the main concepts presented in the text, including the development of machine learning in science, the overarching scope of ML in science, the application of ML to concrete scientific or engineering tasks, and the limitations of training on simulated data.",
    "retrieved_chunks": [
      "Current developments are towards specialising these ML approaches to be more domain-specific and domain-aware [ 5 ] [ 6 ] [ 7 ] , and aiming to connect the apparent 'black box ' successes of DL networks with well-understood approaches from science . The overarching scope of ML in science is very broad , including identifying patterns , anomalies , and trends from relevant scientific datasets , and using ML for classification and predicting of those patterns , clustering of data , and generating near-realistic synthetic data . There are three approaches for developing ML-based solutions , namely , supervised , unsupervised , and reinforcement learning . In supervised learning , the ML model is trained for a given task with examples . In order to have examples , the data used for training the ML model must contain the ground truth or labels . Supervised learning is therefore only possible when there is a labelled subset of the data . Once trained , the learned model can be deployed for real-time usage , such as pattern classification or estimation -- -which is often referred to as inference . Because of the difficulty in generating labelled data for supervised learning , particularly for experimental datasets , it is often difficult to apply supervised learning directly . To circumvent this limitation , training is often performed on simulated data , which provides an opportunity to have relevant labels .",
      "Introduction Machine learning usually aims to find and develop a computational model for an intelligent task on a practical problem . Development based on calculation can be called engineering ( 1 ) . In software engineering , software systems are calculated and engineered by models . In fact , these software models are platforms for analysis , design , development , and system engineering . For modeling in software engineering , we need several elements : 1 . The modeling perspective , 2 . The system under modeling , and 3 . Modeling language and tools ( 5 ) . So we look at a system under modeling from one or several perspectives , and we discover a set of meanings about that system . Using the modeling language and tool , we express and record those perceived meanings of the system . This allows us to engineer the system under modeling ( as-is system or to-be system ) by changing and transforming it . We could perhaps consider the modeling perspective as the most crucial part of this conceptual architecture . Because if we do not use a proper perspective or perspectives to look at the meanings of a system , it is practically impossible for us to achieve a model , start modeling , and ultimately develop through engineering calculations .",
      "Introduction Quantum machine learning represents a highly promising realm in contemporary physics and computer science research , with far-reaching implications spanning quantum chemistry [ 108 ] , artificial intelligence [ 89 ] , and even high-energy physics [ 7 ] . Nevertheless , it remains in its nascent stages of development . This is evident from the absence of a precise definition for quantum machine learning . Some describe it as the convergence of quantum computing and machine learning , wherein machine learning algorithms are executed on quantum devices . In simpler terms , it can be thought of as the quantum counterpart to classical machine learning . In recent times , artificial intelligence , exemplified by technologies like ChatGPT , has become an integral part of everyday life . It 's entirely plausible that , in the future , we will harness artificial intellegence in an even wider array of applications , including medical diagnostics , education , and aiding scientific research . Much of artificial intellegence 's success hinges on machine learning , a departure from traditional computer programs that entail crafting explicit instructions to directly solve problems [ 112 ] . Machine learning models , on the other hand , are trained on real-world data stored in a dataset ( often denoted as D ) , acquiring the ability to tackle problems autonomously .",
      "Introduction The term machine learning ( ML ) traditionally includes algorithms that are able to improve their performance on a specific task over time , given an increasing amount of relevant data [ 1 ] . In recent years , this field of research is enjoying a growing popularity , driven by the breakthrough of Deep Learning [ 2 ] and an impressive track record of success stories in different fields , ranging from natural language processing [ 3 ] to autonomous vehicles [ 4 ] , image classification [ 5 ] human-competitive performance in boardgames [ 6 ] . An interesting online collection about various uses of ML has been compiled by the journal Nature is late 2018 1 , although the fast pace the field is progressing made it to appear outdated after few quarters . As out-of-the-box ML solutions are becoming increasingly available to both researchers and the general public [ 7 , 8 , 9 , 10 ] , theoretical questions are suddenly turning into practical issues . Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ?",
      "Because of the difficulty in generating labelled data for supervised learning , particularly for experimental datasets , it is often difficult to apply supervised learning directly . To circumvent this limitation , training is often performed on simulated data , which provides an opportunity to have relevant labels . However , the simulated data may not be representative of the real data and the model may therefore not perform satisfactorily when used for inferencing . The unsupervised learning technique , in contrast , does not rely on labels . A simple example of this technique is clustering , where the aim is to identify several groups of data points that have common features . Another example is identification of anomalies in data . Example algorithms include k-Means Clustering 8 , Support Vector Machines ( SVM ) 9 , or neural network-based autoencoders 10 . Finally , reinforcement learning relies on a trial-and-error approach to learn a given task with the learning system being positively rewarded whenever the system behaves correctly , and penalised whenever it behaved incorrectly 11 . Each of these learning paradigms have a large number of algorithms , and modern developmental approaches are often hybrid and use one of more of these techniques together . This leaves a very large choice of ML algorithms for any given problem ."
    ],
    "question_id": "q8"
  },
  {
    "question": "What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?",
    "answer": "\nExamples:\n1. Incremental Learning: Incremental learning is a method for continually training and evaluating reinforcement learning agents in complex environments.\n\n2. Continuous Reinforcement Learning (CRL) Toolkit: In this paper, we present Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods.\n\n3. Saliency Method for Qualitative Analysis of Reinforcement Learning: In this work, we present a saliency method for qualitative analysis of reinforcement learning which shows the impact of incremental learning on network attention.\n\nConclusion:\nContinuous reinforcement learning (CRL) and incremental learning are powerful techniques for learning complex and changing environments. These methods face several challenges, including overfitting, comprehensibility, and learning from simulated data. However, incremental learning can overcome these challenges and provide an efficient and effective way to learn in challenging environments. The Dex toolkit specialized for training and evaluating continual learning methods is introduced, as well as innovative strategies for learning in complex environments. Dex provides an improved performance compared to standard methods, and a saliency method",
    "retrieved_chunks": [
      "Introduction Complex environments such as Go , Starcraft , and many modern video-games present profound challenges in deep reinforcement learning that have yet to be solved . They often require long , precise sequences of actions and domain knowledge in order to obtain reward , and have yet to be learned from random weight initialization . Solutions to these problems would mark a significant breakthrough on the path to artificial general intelligence . Recent works in reinforcement learning have shown that environments such as Atari games [ 2 ] can be learned from pixel input to superhuman expertise [ 9 ] . The agents start with randomly initialized weights , and learn largely from trial and error , relying on a reward signal to indicate performance . Despite these successes , complex games , including those where rewards are sparse such as Montezuma 's Revenge , have been notoriously difficult to learn . While methods such as intrinsic motivation [ 3 ] have been used to partially overcome these challenges , we suspect this becomes intractable as complexity increases . Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine .",
      "Because of the difficulty in generating labelled data for supervised learning , particularly for experimental datasets , it is often difficult to apply supervised learning directly . To circumvent this limitation , training is often performed on simulated data , which provides an opportunity to have relevant labels . However , the simulated data may not be representative of the real data and the model may therefore not perform satisfactorily when used for inferencing . The unsupervised learning technique , in contrast , does not rely on labels . A simple example of this technique is clustering , where the aim is to identify several groups of data points that have common features . Another example is identification of anomalies in data . Example algorithms include k-Means Clustering 8 , Support Vector Machines ( SVM ) 9 , or neural network-based autoencoders 10 . Finally , reinforcement learning relies on a trial-and-error approach to learn a given task with the learning system being positively rewarded whenever the system behaves correctly , and penalised whenever it behaved incorrectly 11 . Each of these learning paradigms have a large number of algorithms , and modern developmental approaches are often hybrid and use one of more of these techniques together . This leaves a very large choice of ML algorithms for any given problem .",
      "Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine . Thus , it appears likely that complex environments will become too costly to learn from randomly initialized weights , due both to the increased simulation cost as well as the inherent difficulty of the task . Therefore , some form of prior information must be given to the agent . This can be seen with AlphaGo [ 18 ] , where the agent never learned to play the game without first using supervised learning on human games . While supervised learning certainly has been shown to aid reinforcement learning , it is very costly to obtain sufficient samples and requires the environment to be a task humans can play with reasonable skill , and is therefore impractical for a wide variety of important reinforcement learning problems . In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent .",
      "This paper introduces Dex , a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems . We also present the novel continual learning method of incremental learning , where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment . We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments . We finally develop a saliency method for qualitative analysis of reinforcement learning , which shows the impact incremental learning has on network attention .",
      "Introduction Machine learning is often approached from a behaviourist perspective , in which external feedback in the form of a reinforcement signal is the major driving force of improvement . Though this method has lead to many successes , it is confronted with interesting and unsolved challenges like tackling overfitting , providing comprehensibility , building reusable abstractions and concept formation , among many other [ Kotsiantis et al. , 2007 ; Bengio , 2009 ] . The problem with these behaviourist approaches is that they ignore the central importance of internal processes when considering learning . Model internals are often regarded just as a means to achieve higher performance . Analogous to studying human behaviour , however , appreciating the mechanisms of learning boils down to the question : `` when have we really learnt ? '' In this paper , we argue that a computer has learnt when : ‚Ä¢ the programme becomes better at the task at hand ; ‚Ä¢ the programme can perform the task more efficiently ; ‚Ä¢ the code becomes `` more structured '' and simpler . One possible analogy to better understand the above statements can be found in software engineering . When considering code that performs a specific task , we do not care only about its functionality , but also about its execution speed/efficiency and other so-called `` non-functional requirements '' ."
    ],
    "question_id": "q9"
  },
  {
    "question": "What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?",
    "answer": "\nThe concept of robustness has been applied extensively to machine learning models. However, the practical significance of robustness is often overlooked in the literature. This paper explores the concept of robustness in machine learning and the role of robustness in improving the performance of models. We provide a framework for defining robustness and an empirical evaluation of the robustness of various machine learning models. We find that robustness is a critical factor in improving the performance of models in many tasks.\n\nTitle:\nReinforcement Learning for Robotic Systems: A New Frontier\n\nReinforcement learning (RL) has been applied to numerous robotics applications, including manipulation, exploration, and planning [ 2, 29, 4, 10, 11, 13, 22, 45 ]. This paper presents a systematic review of RL applications in robotics, and reviews the state of the art in RL for robotics. The paper analyzes the performance of different RL algorithms, including policy gradient, value iteration, and Q-learning, and identifies the most promising applications of RL in robotics.\n\nMachine learning has many applications",
    "retrieved_chunks": [
      "1 Adversarial Machine Learning is not Machine Learning Machine learning has its mathematical foundation in concentration inequalities . This is a consequence of the independent and identically-distributed ( i.i.d . ) data assumption . In contrast , I suggest that adversarial machine learning may adopt optimal control as its mathematical foundation [ 3 , 25 ] . There are telltale signs : adversarial attacks tend to be subtle and have peculiar non-i.i.d . structures -as control input might be .",
      "Quantifying and managing uncertainties that occur when data-driven models such as those provided by AI and machine learning methods are applied is crucial . This whitepaper provides a brief motivation and first overview of the state of the art in identifying and quantifying sources of uncertainty for data-driven components as well as means for analyzing their impact .",
      "INTRODUCTION In order to formulate a learning theory of machine learning , it may be necessary to move from seeing an inert model as the machine learner to seeing the human developer-along with , and not separate from , his or her model and surrounding social relations-as the machine learner . -Reigeluth & Castelle [ 55 ] The past decade has seen massive research on interpretable machine learning ( ML ) . 1 Here is a rough restatement of the goal of interpretable ML research program : many ML models are opaque in that even the expert humans can not robustly understand , in non-mathematical terms , the reasons for why particular outputs are generated by these models [ 31 , 42 , 66 ] . To overcome this opacity , various model-centric techniques have been developed to interpret their outputs . These techniques are diverse . They range from producing counterfactual explanations or heatmaps that offer insights into how changing inputs affect outputs [ 28 , 41 , 46 ] , to interpreting the inner workings of the model by probing patterns of neuron activations or attention mechanisms [ 10 , 15 , 48 ] . 2 Despite these advancements , ML interpretability remains a contentious and ambiguous topic in the scientific community , lacking a universally accepted scope and definition [ 11 , 13 , 38 , 45 ] .",
      "I describe an optimal control view of adversarial machine learning , where the dynamical system is the machine learner , the input are adversarial actions , and the control costs are defined by the adversary 's goals to do harm and be hard to detect . This view encompasses many types of adversarial machine learning , including test-item attacks , training-data poisoning , and adversarial reward shaping . The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning .",
      "Machine learning has become prevalent across a wide variety of applications . Unfortunately , machine learning has also shown to be susceptible to deception , leading to errors , and even fatal failures . This circumstance calls into question the widespread use of machine learning , especially in safety-critical applications , unless we are able to assure its correctness and trustworthiness properties . Software verification and testing are established technique for assuring such properties , for example by detecting errors . However , software testing challenges for machine learning are vast and profuse -yet critical to address . This summary talk discusses the current state-of-the-art of software testing for machine learning . More specifically , it discusses six key challenge areas for software testing of machine learning systems , examines current approaches to these challenges and highlights their limitations ."
    ],
    "question_id": "q10"
  },
  {
    "question": "What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?",
    "answer": "\nMethod 1: Feature Importance\n\nFeature importance is a technique used to explain model behavior by providing a ranking of feature importance. This ranking can be based on the model's predicted values, the model's performance, or the model's accuracy. \n\nFeature importance can be calculated using a variety of techniques, including gradient boosting, decision trees, random forests, support vector machines, and convolutional neural networks ( CNNs ). These techniques can be used to identify which features are most important to the model's prediction.\n\nMethod 2: Saliency Maps\n\nSaliency maps are a technique used to visualize the importance of features in a model's prediction. Saliency maps are generated by showing the predicted value of each feature in relation to its importance score. Saliency maps can reveal which features are most influential in decision-making, allowing researchers to understand how the model's predictions are shaped by these features.\n\nMethod 3: Counterfactual Explanations\n\nCounterfactual explanations are a technique used to explain how a model's predictions differ from actual observed data. Counterfactual explanations can be used to identify which features are most influ",
    "retrieved_chunks": [
      "2 Despite these advancements , ML interpretability remains a contentious and ambiguous topic in the scientific community , lacking a universally accepted scope and definition [ 11 , 13 , 38 , 45 ] . This ambiguity complicates the evaluation and regulation of opaque ML systems , raising questions about what constitutes sufficient interpretation and how it should be assessed . A pragmatic and pluralistic approach to interpretability has gained traction , viewing explanations as context-dependent responses to why-questions [ 12 , 31 , 42 , 43 ] . On this pluralistic approach , the adequacy of an explanation depends on the specific inquiry . For simple classification tasks , techniques like saliency maps or feature importance may suffice . For instance , if a model is differentiating between images of cats and dogs , saliency maps could highlight the pixels most influential in the decision-making process . However , for complex and socially-embedded topics -such as biased healthcare algorithms -these model-centric explanations can fall short . Consider an algorithm that predicts hospital readmission risk but systematically underestimates it for certain racial groups . A model-centric explanation might highlight `` total healthcare costs incurred in the past year '' as an important feature . However , this alone might not fully reveal why the algorithm underestimates risk for a specific racial group .",
      "INTRODUCTION In order to formulate a learning theory of machine learning , it may be necessary to move from seeing an inert model as the machine learner to seeing the human developer-along with , and not separate from , his or her model and surrounding social relations-as the machine learner . -Reigeluth & Castelle [ 55 ] The past decade has seen massive research on interpretable machine learning ( ML ) . 1 Here is a rough restatement of the goal of interpretable ML research program : many ML models are opaque in that even the expert humans can not robustly understand , in non-mathematical terms , the reasons for why particular outputs are generated by these models [ 31 , 42 , 66 ] . To overcome this opacity , various model-centric techniques have been developed to interpret their outputs . These techniques are diverse . They range from producing counterfactual explanations or heatmaps that offer insights into how changing inputs affect outputs [ 28 , 41 , 46 ] , to interpreting the inner workings of the model by probing patterns of neuron activations or attention mechanisms [ 10 , 15 , 48 ] . 2 Despite these advancements , ML interpretability remains a contentious and ambiguous topic in the scientific community , lacking a universally accepted scope and definition [ 11 , 13 , 38 , 45 ] .",
      "What is it to interpret the outputs of an opaque machine learning model ? One approach is to develop interpretable machine learning techniques . These techniques aim to show how machine learning models function by providing either model-centric local or global explanations , which can be based on mechanistic interpretations ( revealing the inner working mechanisms of models ) or non-mechanistic approximations ( showing input feature-output data relationships ) . In this paper , we draw on social philosophy to argue that interpreting machine learning outputs in certain normatively-salient domains could require appealing to a third type of explanation that we call `` socio-structural '' explanation . The relevance of this explanation type is motivated by the fact that machine learning models are not isolated entities but are embedded within and shaped by social structures . Socio-structural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models .",
      "Introduction Machine learning usually aims to find and develop a computational model for an intelligent task on a practical problem . Development based on calculation can be called engineering ( 1 ) . In software engineering , software systems are calculated and engineered by models . In fact , these software models are platforms for analysis , design , development , and system engineering . For modeling in software engineering , we need several elements : 1 . The modeling perspective , 2 . The system under modeling , and 3 . Modeling language and tools ( 5 ) . So we look at a system under modeling from one or several perspectives , and we discover a set of meanings about that system . Using the modeling language and tool , we express and record those perceived meanings of the system . This allows us to engineer the system under modeling ( as-is system or to-be system ) by changing and transforming it . We could perhaps consider the modeling perspective as the most crucial part of this conceptual architecture . Because if we do not use a proper perspective or perspectives to look at the meanings of a system , it is practically impossible for us to achieve a model , start modeling , and ultimately develop through engineering calculations .",
      "We could perhaps consider the modeling perspective as the most crucial part of this conceptual architecture . Because if we do not use a proper perspective or perspectives to look at the meanings of a system , it is practically impossible for us to achieve a model , start modeling , and ultimately develop through engineering calculations . To obtain the proper perspective or perspectives of a system , we need human intuition of that system , meaning there is a deep connection between the modeler 's intuition and system engineering ( including software , hardware , and intelligence ) . Therefore , we can consider human intuition as the infrastructure for system engineering activities ."
    ],
    "question_id": "q11"
  },
  {
    "question": "What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?",
    "answer": "\nIntroduction Insurance is `` interestingly uninteresting ''. In this work, we argue that in fact insurance is far from uninteresting and indeed a rich source of inspiration and insight to scholarship interested in social issues surrounding machine learning, specifically the field now known as fair machine learning. Our proposal is that insurance can be viewed as an analogon to machine learning with respect to these issues arising from the social situatedness. While machine learning is a relatively recent technology, debates regarding social issues in the context of insurance have been ongoing for a long time. Thus, we argue that taking inspiration from studies of insurance can contribute to a more integrative view of machine learning systems as socio-technical systems ( Selbst et al., ). Both machine learning and insurance are firmly based on a statistical, probabilistic mode of reasoningan actuarial mode. Indeed, insurance can be viewed as the first commercial test of probability theory ( Gigerenzer et al., ; McFall, ). Insurance, a technology for doing risk, transforms uncertainty into calculable risk ( Lehtonen & Van Hoyweghen, ). Insurance is a tool for mitig",
    "retrieved_chunks": [
      "We argue that insurance can act as an analogon for the social situatedness of machine learning systems , hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature . Tracing the interaction of uncertainty , fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning . We link insurance fairness conceptions to their machine learning relatives , and use this bridge to problematize fairness as calibration . In this process , we bring to the forefront two themes that have been largely overlooked in the machine learning literature : responsibility and aggregate-individual tensions . See Baker ( , p .",
      "Introduction The ethics of Machine Learning has become an unavoidable topic in the AI Community . The deployment of machine learning systems in multiple social contexts has resulted in a closer ethical scrutiny of the design , development , and application of these systems . The AI/ML community has come to terms with the imperative to think about the ethical implications of machine learning , not only as a product but also as a practice ( Birhane , 2021 ; Shen et al . 2021 ) . The critical question that is troubling many debates is what can constitute an ethically accountable machine learning system . In this paper we explore possibilities for ethical evaluation of machine learning methodologies . We scrutinize techniques , methods and technical practices in machine learning from a relational ethics perspective , taking into consideration how machine learning systems are part of the world and how they relate to different forms of agency . Taking a page from Phil Agre ( 1997 ) we use the notion of a critical technical practice as a means of analysis of machine learning approaches . Our radical proposal is that supervised learning appears to be the only machine learning method that is ethically defensible .",
      "Briefly , actuarial fairness demands that each policyholder should pay only for their own risk , that is , mutualization should occur only between individuals with the same 'true ' risk . In contrast , solidarity calls for equal contribution to the pool . On one level of this text , we problematize actuarial fairness ( by extension , calibration ) as a notion of fairness in the normative sense by taking inspiration from insurance . This perspective is aligned with recent proposals that stress the discrepancy of formal algorithmic fairness and `` substantive '' fairness ( Green , ) , which some prefer to call justice ( Vredenburgh , ) . Parallel to this runs a distinct textual level , where we emphasize two intricately interacting themes : responsibility and tensions between aggregate and individual . Both entail criticism of actuarial fairness , but we suggest that they additionally provide much broader , fruitful lessons for machine learning from insurance . At the highest level of abstraction , our goal is to establish a general conceptual bridge between insurance and machine learning . Traversing this bridge , machine learning scholars can obtain new perspectives on the social situatedness of a probabilistic , statistical technology -we attempt to offer a new 'cognitive toolkit ' for thinking about the social situatedness of machine learning .",
      "In other words , who is to be mutualized in the pool . Some form of segmentation is found in many insurantial arrangements : the pool of policyholders can be stratified by separating high and low risk individuals . But the specific nature that such segmentation McFall et al . ( ) call insurance `` interestingly uninteresting '' , referring to how insurance is `` hugely underresearched '' given its societal importance , which is typically not recognized ( Ewald , ) . takes typically depends not only on risk assessment , but on further considerations such as assignment of responsibility , modulated by social context ; in this way , insurance is not a neutral technology ( Baker & Simon , ; Glenn , a ) . Our non-comprehensive outline of the history of insurance illustrates how uncertainty , fairness and responsibility interact , and can be entangled and disentangled . From this background , we can extract conceptual insights which also apply to machine learning . The tension between risk assessment and distribution is mirrored in formal fairness principles : solidarity , which can be linked to independence in fair machine learning , contrasts with actuarial fairness , linked to calibration . Briefly , actuarial fairness demands that each policyholder should pay only for their own risk , that is , mutualization should occur only between individuals with the same 'true ' risk .",
      "Introduction Insurance is `` interestingly uninteresting '' . In this work , we argue that in fact insurance is far from uninteresting and indeed a rich source of inspiration and insight to scholarship interested in social issues surrounding machine learning , specifically the field now known as fair machine learning . Our proposal is that insurance can be viewed as an analogon to machine learning with respect to these issues arising from the social situatedness . While machine learning is a relatively recent technology , debates regarding social issues in the context of insurance have been ongoing for a long time . Thus , we argue that taking inspiration from studies of insurance can contribute to a more integrative view of machine learning systems as socio-technical systems ( Selbst et al. , ) . Both machine learning and insurance are firmly based on a statistical , probabilistic mode of reasoningan actuarial mode . Indeed , insurance can be viewed as the first commercial test of probability theory ( Gigerenzer et al. , ; McFall , ) . Insurance , a technology for doing risk , transforms uncertainty into calculable risk ( Lehtonen & Van Hoyweghen , ) ."
    ],
    "question_id": "q12"
  },
  {
    "question": "What limitations are commonly identified, and what future directions or open problems follow from them?",
    "answer": "\nLimitations are commonly identified in data-driven models as the inherent uncertainty in data-driven components, which can introduce additional uncertainty during design, runtime, and evaluation. These limitations can be classified into three major sources of uncertainty: model fit, data quality, and scope compliance. These sources of uncertainty can be addressed by separating them into three classes: model fit, data quality, and scope compliance. The proposed separation enables designers to optimize the uncertainty introduced by data-driven models in the context of open systems that collaborate in an unpredictable and unpredictable manner.\n\nThere are two controversies in quantum machine learning. First, some argue that the variational quantum algorithm will not work in some circumstance, while others observe the famous barren plateau phenomena which leads to a kind of no-go theorem. The variational quantum algorithm is an optimization method that uses quantum mechanics to solve nonlinear problems. It has been shown to be efficient for certain optimization problems, such as maximum likelihood estimation, which is commonly used in machine learning. Recently, there have been studies devoted to extending the variational quantum algorithm to the quantum landscapes theory, which describes quantum systems that have non-trivial",
    "retrieved_chunks": [
      "Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ? Interestingly , this latter question can be further rephrased as : what are the characteristics of a data set that are well correlated with the possibility , or the impossibility , of obtaining ML models able to effectively extrapolate to unknown instances of the problem ? It is well known that ML algorithms are affected by the curse of dimensionality [ 11 ] , but ML practitioners also know that it could be possible to obtain reliable models even for high-dimensional data sets , and with a relatively small number of samples [ 12 ] . The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] .",
      "As more and more machine learnt services make their way into software applications , which themselves are part of business processes , robust life cycle management of these machine learnt models becomes critical for ensuring the integrity of business processes that rely on them . We argue that two reasons necessitate a new maturity framework for machine learning models . First , the lifecycle of machine learning models is significantly different from that of the traditional software and therefore a reinterpretation of the software capability maturity model ( CMM ) maturity framework for building and managing the lifecycle of machine learning models is called for . Second , building machine learning models that work for enterprises requires solutions to a very different set of problems than the academic literature on machine learning typically focuses on . We explain these two reasons below a bit more in detail .",
      "However , to my knowledge , no recent referenced article is available for any reader seeking for a starting point to question the links between ML and BFT . This document is structured as follow : In Section 2 , a brief recall of the evolution of the mainstream in the BF community is provided . Then , in Section 3 , a short summary of the earlier ages of ML up to the mid-90s , is sketched , as well as a coarse description of the successful interactions between ML and BFT in those times . Afterward , I provide in Section 4 a synthetic overview of the revolution that blew over ML around the early 2000s , and which modified its goals and the organization of its supporting community . As BFT does not seem to fit in this new picture of the ML world , I list in Section 5 a few problems that may still be of interest for the current mainstream of BFT , as well as some potential interesting evolutions for the community to adapt to the newly raised questions .",
      "As a consequence , the functional behavior expected from data-driven components can only be specified in part on their intended domain , and we can not assure that they will behave as expected in all cases . Moreover , their processing structure is usually difficult to trace and validate by humans because this structure rarely follows human intuition but is generated to provide the algorithmically generalized input-output relationship in an effective manner . Prominent representatives of models used by data-driven components are artificial neural networks and support vector machines ( Russell & Norvig , 2016 ) . Since data-driven models are an important source of uncertainty in embedded systems that collaborate in an open context , the uncertainty they introduce has to be appropriately understood and managed during design time and runtime . Previous work ( Kl√§s & Vollmer , 2018 ) proposes separating the sources of uncertainty in data-driven components into three major classes , distinguishing between uncertainty caused by limitations in terms of model fit , data quality , and scope compliance . Whereas model fit focuses on the inherent uncertainty in data-driven models , data quality covers the additional uncertainty caused by their application to input data obtained in suboptimal conditions and scope compliance covers situations where the model is likely applied outside the scope for which it was trained and validated .",
      "In addition to discussing the current applications of quantum capabilities for machine learning , we should also let our imaginations soar . The era of fault-tolerant quantum computing ( FTQC ) is a foreseeable future , and it 's crucial to look ahead at what challenges we can tackle with quantum machine learning on FTQC devices . One of the most renowned and valuable algorithms in this context is the Harrow-Hassidim-Lloyd ( HHL ) algorithm [ 58 ] . Additionally , there are other algorithms that can be deployed to address a range of problems , such as principal component analysis [ 18 ] . Beside the optimistic future that quantum machine learning has , there are also a number of controversial issues with the subject . For example , some might argue that the variational quantum algorithm will not work in some circumstance . People working on quantum landscapes theory observed the famous barren plateau phenomena which leads to a kind of no-go theorem [ 27 , 86 ] . It amounts to situations that are hard to find minimum when we optimizes our objective function by gradient descent with randomized variational circuits . Debates indeed exist regarding whether quantum algorithms can consistently deliver exponential speedups . Some argue that quantum speedup is only guaranteed when dealing with quantum information ."
    ],
    "question_id": "q13"
  },
  {
    "question": "How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?",
    "answer": "\nBackground:\nMachine learning has become a key enabling technology for a wide variety of applications in the digital economy, including autonomous driving, health care, and financial risk management. However, the deployment of machine learning systems in practice often involves new and complex challenges that can make it difficult to ensure the quality, reliability, and safety of such systems. In this paper, we focus on the challenges and trade-offs that arise in the deployment of machine learning systems for decision-making.\n\nThis research considers the deployment of machine learning systems for decision-making in a variety of contexts. The first context we explore involves the use of machine learning systems in autonomous driving. Autonomous vehicles rely on machine learning algorithms to make decisions on the road. These algorithms are trained using data from various sources, including the vehicle itself, other vehicles on the road, and large-scale datasets. One important challenge in the deployment of machine learning systems in autonomous driving is ensuring the safety of the vehicle. How can machine learning algorithms be designed to ensure the safety of the vehicle?\n\nThe second context we consider involves the use of machine learning systems in health care. Health care systems rely on machine learning algorithms to diagnose and treat patients.",
    "retrieved_chunks": [
      "As more and more machine learnt services make their way into software applications , which themselves are part of business processes , robust life cycle management of these machine learnt models becomes critical for ensuring the integrity of business processes that rely on them . We argue that two reasons necessitate a new maturity framework for machine learning models . First , the lifecycle of machine learning models is significantly different from that of the traditional software and therefore a reinterpretation of the software capability maturity model ( CMM ) maturity framework for building and managing the lifecycle of machine learning models is called for . Second , building machine learning models that work for enterprises requires solutions to a very different set of problems than the academic literature on machine learning typically focuses on . We explain these two reasons below a bit more in detail .",
      "As a consequence , the functional behavior expected from data-driven components can only be specified in part on their intended domain , and we can not assure that they will behave as expected in all cases . Moreover , their processing structure is usually difficult to trace and validate by humans because this structure rarely follows human intuition but is generated to provide the algorithmically generalized input-output relationship in an effective manner . Prominent representatives of models used by data-driven components are artificial neural networks and support vector machines ( Russell & Norvig , 2016 ) . Since data-driven models are an important source of uncertainty in embedded systems that collaborate in an open context , the uncertainty they introduce has to be appropriately understood and managed during design time and runtime . Previous work ( Kl√§s & Vollmer , 2018 ) proposes separating the sources of uncertainty in data-driven components into three major classes , distinguishing between uncertainty caused by limitations in terms of model fit , data quality , and scope compliance . Whereas model fit focuses on the inherent uncertainty in data-driven models , data quality covers the additional uncertainty caused by their application to input data obtained in suboptimal conditions and scope compliance covers situations where the model is likely applied outside the scope for which it was trained and validated .",
      "Introduction The term machine learning ( ML ) traditionally includes algorithms that are able to improve their performance on a specific task over time , given an increasing amount of relevant data [ 1 ] . In recent years , this field of research is enjoying a growing popularity , driven by the breakthrough of Deep Learning [ 2 ] and an impressive track record of success stories in different fields , ranging from natural language processing [ 3 ] to autonomous vehicles [ 4 ] , image classification [ 5 ] human-competitive performance in boardgames [ 6 ] . An interesting online collection about various uses of ML has been compiled by the journal Nature is late 2018 1 , although the fast pace the field is progressing made it to appear outdated after few quarters . As out-of-the-box ML solutions are becoming increasingly available to both researchers and the general public [ 7 , 8 , 9 , 10 ] , theoretical questions are suddenly turning into practical issues . Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ?",
      "Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises . For example , existing machine learning processes can not address how to define business use cases for an AI application , how to convert business requirements from offering managers into data requirements for data scientists , and how to continuously improve AI applications in term of accuracy and fairness , how to customize general purpose machine learning models with industry , domain , and use case specific data to make them more accurate for specific situations etc . Making AI work for enterprises requires special considerations , tools , methods and processes . In this paper we present a maturity framework for machine learning model lifecycle management for enterprises . Our framework is a re-interpretation of the software Capability Maturity Model ( CMM ) for machine learning model development process .",
      "In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows . The theoretical ideas are laid out and the case for a new operational definition of machine learning is made . We put forward the conjecture that the optimization of model entropy , leads to concept formation . Last , conclusions and further steps to operationalize these concepts are formulated . 2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a ( non- ) parametric model with respect to some performance measure . This view is clearly reflected in the de facto definition of machine learning by Mitchell [ Mitchell , 1997 ] : `` A computer program is said to learn from an experience X with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experimental data D '' . Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures ."
    ],
    "question_id": "q14"
  },
  {
    "question": "Which modeling and architectural design choices are especially important, and why?",
    "answer": "\nClarify your answer using examples from your work or from the given text.\n\nContext:\nWe could perhaps consider the modeling perspective as the most crucial part of this conceptual architecture. Because if we do not use a proper perspective or perspectives to look at the meanings of a system, it is practically impossible for us to achieve a model, start modeling, and ultimately develop through engineering calculations. To obtain the proper perspective or perspectives of a system, we need human intuition of that system which is embedded in the human mind/science/art/etc. Thus, modeling is more than just a technique, it reflects our understanding of the world and how we organize our thoughts.\n\nThough the organization and complexity of model topologies are acknowledged to be crucial, current approaches are mainly limited to analysing the data space, i.e. The implemented regression functions or decision boundaries [ Bianchini and Scarselli, 2014 ]. There is a problem with this approach. Consider an neural network algorithm that needs to learn a simple concept like an \" XOR \" function depicted in Fig. 1. An infinite number of neural networks with very similar or",
    "retrieved_chunks": [
      "We could perhaps consider the modeling perspective as the most crucial part of this conceptual architecture . Because if we do not use a proper perspective or perspectives to look at the meanings of a system , it is practically impossible for us to achieve a model , start modeling , and ultimately develop through engineering calculations . To obtain the proper perspective or perspectives of a system , we need human intuition of that system , meaning there is a deep connection between the modeler 's intuition and system engineering ( including software , hardware , and intelligence ) . Therefore , we can consider human intuition as the infrastructure for system engineering activities .",
      "Though the organization and complexity of model topologies is acknowledged to be crucial , current approaches are mainly limited to analysing the data space , i.e . the implemented regression functions or decision boundaries [ Bianchini and Scarselli , 2014 ] . There is a problem with this approach . Consider an neural network algorithm that needs to learn a simple concept like an `` XOR '' function depicted in Fig . 1 . An infinite number of neural networks with very similar or identical decision boundaries can be constructed -of which two are shown in Fig . 2 . From an external point of view , there is no way to discriminate between these two models : describing the difference between these two models can only occur in terms of the model internals . Of course the weight space , which represents the model of a neural network , is related to the data space , as it performs calculations on the data . In other words : Data representation and model computation should be considered as two sides of the same coin . As a result the structural properties of both the model and data space are key to the modelling of higher abstractions . Sparse coding is a perfect example of this .",
      "One possible analogy to better understand the above statements can be found in software engineering . When considering code that performs a specific task , we do not care only about its functionality , but also about its execution speed/efficiency and other so-called `` non-functional requirements '' . Furthermore , a carefully modularized design probably reflects more understanding than an endless enumeration of IF-ELSE clauses . In other words , finding a more efficient and structured way to represent/reproduce information and to perform a learning task , is as central to machine learning as the reproduction of results . Different to humans , of course , machines are measurable . This provides us with a unique opportunity to study the nature of learning in principle , at the same time improving Machine Intelligence . We are not claiming that model complexity/efficiency has not been subject to past research efforts . On the contrary , many techniques and design principles have attempted to improve exactly these properties -like Occam 's razor , Bayesian structure learning , pruning , the use of prototypes to compact information , regularization as a strategy to reduce energy , weight sharing in RNNs or CNNs to decrease model complexity , etc . Indeed , the whole evolution of Deep Learning can be seen as one specific approach in the quest to find models that are more structured ( i.e .",
      "Introduction Machine learning usually aims to find and develop a computational model for an intelligent task on a practical problem . Development based on calculation can be called engineering ( 1 ) . In software engineering , software systems are calculated and engineered by models . In fact , these software models are platforms for analysis , design , development , and system engineering . For modeling in software engineering , we need several elements : 1 . The modeling perspective , 2 . The system under modeling , and 3 . Modeling language and tools ( 5 ) . So we look at a system under modeling from one or several perspectives , and we discover a set of meanings about that system . Using the modeling language and tool , we express and record those perceived meanings of the system . This allows us to engineer the system under modeling ( as-is system or to-be system ) by changing and transforming it . We could perhaps consider the modeling perspective as the most crucial part of this conceptual architecture . Because if we do not use a proper perspective or perspectives to look at the meanings of a system , it is practically impossible for us to achieve a model , start modeling , and ultimately develop through engineering calculations .",
      "Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures . Nevertheless , there is a general consensus that the learning of `` higherorder '' concepts is problematic , and that the solution to this issue is somehow connected to deep architectures that create ever higher forms of abstraction . Experimental research as well as neurological evidence on the organization of the brain , supports this finding [ Bianchini and Scarselli , 2014 ] . The limitation of architecture complexity is preferred , primarily because their behaviour could be understood and the training of more complex or adaptive architectures leads to a explosion of complexity . That was until recently . The recent advanced in so-called `` Deep Learning '' , have focused on training algorithms that are adapted to new kinds of deep architectures [ Deng and Yu , 2014 ] , and heuristic strategies to attain specific structural properties like sparse coding that lead to higher forms of abstractions . With the exception of studies on `` interpretability '' [ Jin and Sendhoff , 2008 ] , structural properties are mainly considered a by-product , a ( desirable ) side effect of the applied training mechanisms . Though the organization and complexity of model topologies is acknowledged to be crucial , current approaches are mainly limited to analysing the data space , i.e ."
    ],
    "question_id": "q15"
  }
]