---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- dense
- generated_from_trainer
- dataset_size:86
- loss:MultipleNegativesRankingLoss
base_model: sentence-transformers/all-MiniLM-L6-v2
widget:
- source_sentence: What methods are used to explain or interpret model behavior (e.g.,
    feature importance, saliency, counterfactual explanations)?
  sentences:
  - Because of the difficulty in generating labelled data for supervised learning
    , particularly for experimental datasets , it is often difficult to apply supervised
    learning directly . To circumvent this limitation , training is often performed
    on simulated data , which provides an opportunity to have relevant labels . However
    , the simulated data may not be representative of the real data and the model
    may therefore not perform satisfactorily when used for inferencing . The unsupervised
    learning technique , in contrast , does not rely on labels . A simple example
    of this technique is clustering , where the aim is to identify several groups
    of data points that have common features . Another example is identification of
    anomalies in data . Example algorithms include k-Means Clustering 8 , Support
    Vector Machines ( SVM ) 9 , or neural network-based autoencoders 10 . Finally
    , reinforcement learning relies on a trial-and-error approach to learn a given
    task with the learning system being positively rewarded whenever the system behaves
    correctly , and penalised whenever it behaved incorrectly 11 . Each of these learning
    paradigms have a large number of algorithms , and modern developmental approaches
    are often hybrid and use one of more of these techniques together . This leaves
    a very large choice of ML algorithms for any given problem .
  - During the analysis , we take into account characteristics such as number of features
    , number of classes , number of samples , and we look for correlations with quality
    metrics , such as accuracy of a ML model on training and test points . Extrapolation
    is assessed not just by alternatively dividing the data into training and test
    sets , but by analyzing whether data points fall inside or outside of the convex
    hull of the training data . After collecting the meta-data on the performance
    of a state-of-the-art classification algorithm on the data sets , the statistical
    analysis presents both predictable and surprising results , hinting at the fact
    that dimensionality might not be so cursed after all . Main contributions 1 .
    We ran a quantitative evaluations of ML models over 109 publicly available data
    sets . 2 . In Section 2 and Section 3 we provide a general overview on generalization
    in ML , and of the so-called curse of dimensionality .
  - In this paper , we draw on social philosophy [ 17 , 25 , 26 , 67 , 68 ] to advocate
    for a more comprehensive approach to ML interpretability research , expanding
    beyond model-centric explanations . We propose incorporating relevant socio-structural
    explanations to achieve a deeper understanding of ML outputs in domains with substantial
    societal impact . In the rest of the paper , we introduce the concept of socio-structural
    explanations and discuss their relevance to understanding ML outputs . We then
    examine how these explanations can enhance the interpretation of automated decision-making
    by ML systems in healthcare [ 49 ] . Our paper expands the discourse on transparency
    in machine learning , arguing that it extends beyond model interpretability .
    We propose that in high-stake decision domains , a sociostructural analysis could
    be necessary to understand system outputs , uncover societal biases , ensure accountability
    , and guide policy decisions .
- source_sentence: How are machine learning methods applied to concrete scientific
    or engineering tasks (e.g., physics, biology, control, optimization)?
  sentences:
  - 'INTRODUCTION In order to formulate a learning theory of machine learning , it
    may be necessary to move from seeing an inert model as the machine learner to
    seeing the human developer-along with , and not separate from , his or her model
    and surrounding social relations-as the machine learner . -Reigeluth & Castelle
    [ 55 ] The past decade has seen massive research on interpretable machine learning
    ( ML ) . 1 Here is a rough restatement of the goal of interpretable ML research
    program : many ML models are opaque in that even the expert humans can not robustly
    understand , in non-mathematical terms , the reasons for why particular outputs
    are generated by these models [ 31 , 42 , 66 ] . To overcome this opacity , various
    model-centric techniques have been developed to interpret their outputs . These
    techniques are diverse . They range from producing counterfactual explanations
    or heatmaps that offer insights into how changing inputs affect outputs [ 28 ,
    41 , 46 ] , to interpreting the inner workings of the model by probing patterns
    of neuron activations or attention mechanisms [ 10 , 15 , 48 ] . 2 Despite these
    advancements , ML interpretability remains a contentious and ambiguous topic in
    the scientific community , lacking a universally accepted scope and definition
    [ 11 , 13 , 38 , 45 ] .'
  - One of the pillars of any machine learning model is its concepts . Using software
    engineering , we can engineer these concepts and then develop and expand them
    . In this article , we present a SELM framework for Software Engineering of machine
    Learning Models . We then evaluate this framework through a case study . Using
    the SELM framework , we can improve a machine learning process efficiency and
    provide more accuracy in learning with less processing hardware resources and
    a smaller training dataset . This issue highlights the importance of an interdisciplinary
    approach to machine learning . Therefore , in this article , we have provided
    interdisciplinary teams ' proposals for machine learning .
  - The comparison between conventional data-driven MHMS and DL-based MHMS is given
    in Table I . A high-level illustration of the principles behind these three kinds
    of MHMS discussed above is shown in Figure 1 . Deep learning models have several
    variants such as Auto-Dncoders [ 23 ] , Deep Belief Network [ 24 ] , Deep Boltzmann
    Machines [ 25 ] , Convolutional Neural Networks [ 26 ] and Recurrent Neural Networks
    [ 27 ] . During recent years , various researchers have demonstrated success of
    these deep learning models in the application of machine health monitoring . This
    paper attempts to provide a wide overview on these latest DL-based MHMS works
    that impact the state-of-the art technologies . Compared to these frontiers of
    deep learning including Computer Vision and Natural Language Processing , machine
    health monitoring community is catching up and has witnessed an emerging research
    . Therefore , the purpose of this survey article is to present researchers and
    engineers in the area of machine health monitoring system , a global view of this
    hot and active topic , and help them to acquire basic knowledge , quickly apply
    deep learning models and develop novel DL-based MHMS . The remainder of this paper
    is organized as follows . The basic information on these above deep learning models
    are given in section II . Then , section III reviews applications of deep learning
    models on machine health monitoring .
- source_sentence: What evaluation protocols and benchmark setups are commonly used
    to assess model performance?
  sentences:
  - 'In this context , an important problem is that of finding matching pairs of records
    from heterogeneous databases , while maintaining privacy of the databases parties
    . To this purpose secure computation of distance metrics is important for secure
    record linkage [ 5 ] . The paper is organized as follows . Section 2 describes
    an introduction to the Record Linkage problem ; then the next Section 3 describes
    the method Electre Tri , used to solved the Record Linkage and in the last Section
    4 a preliminary experiment is conducted on simulated data . The paper closes with
    some final remarks and conclusions . 2 Linked Data : the Record Linkage Generally
    speaking , in integration of two data sets the objective is the detection of those
    records , in the different data sets , that belong to the same statistical unit
    . This action allows the reconstruction of a unique record of data that contains
    all the unit information collected from different data sources on that unit .
    Therefore , record linkage is the methodology of bringing together corresponding
    records from two or more files or finding duplicates within files [ 16 ] .'
  - To this end , we suggest a `` model entropy function '' to be defined that quantifies
    the efficiency of the internal learning processes . It is conjured that the minimization
    of this model entropy leads to concept formation . Besides philosophical aspects
    , some initial illustrations are included to support the claims .
  - In this work we perform a meta-analysis of 109 publicly-available classification
    data sets , modeling machine learning generalization as a function of a variety
    of data set characteristics , ranging from number of samples to intrinsic dimensionality
    , from class-wise feature skewness to F 1 evaluated on test samples falling outside
    the convex hull of the training set . Experimental results demonstrate the relevance
    of using the concept of the convex hull of the training data in assessing machine
    learning generalization , by emphasizing the difference between interpolated and
    extrapolated predictions . Besides several predictable correlations , we observe
    unexpectedly weak associations between the generalization ability of machine learning
    models and all metrics related to dimensionality , thus challenging the common
    assumption that the curse of dimensionality might impair generalization in machine
    learning .
- source_sentence: What fairness or bias risks arise in machine learning systems,
    and what mitigation approaches are used?
  sentences:
  - 'Socio-structural explanations aim to illustrate how social structures contribute
    to and partially explain the outputs of machine learning models . We demonstrate
    the importance of socio-structural explanations by examining a racially biased
    healthcare allocation algorithm . Our proposal highlights the need for transparency
    beyond model interpretability : understanding the outputs of machine learning
    systems could require a broader analysis that extends beyond the understanding
    of the machine learning model itself .'
  - Which topics of machine learning are most commonly addressed in research ? This
    question was initially answered in 2007 by doing a qualitative survey among distinguished
    researchers . In our study , we revisit this question from a quantitative perspective
    . Concretely , we collect 54K abstracts of papers published between 2007 and 2016
    in leading machine learning journals and conferences . We then use machine learning
    in order to determine the top 10 topics in machine learning . We not only include
    models , but provide a holistic view across optimization , data , features , etc
    . This quantitative approach allows reducing the bias of surveys . It reveals
    new and up-to-date insights into what the 10 most prolific topics in machine learning
    research are . This allows researchers to identify popular topics as well as new
    and rising topics for their research .
  - In this work we perform a meta-analysis of 109 publicly-available classification
    data sets , modeling machine learning generalization as a function of a variety
    of data set characteristics , ranging from number of samples to intrinsic dimensionality
    , from class-wise feature skewness to F 1 evaluated on test samples falling outside
    the convex hull of the training set . Experimental results demonstrate the relevance
    of using the concept of the convex hull of the training data in assessing machine
    learning generalization , by emphasizing the difference between interpolated and
    extrapolated predictions . Besides several predictable correlations , we observe
    unexpectedly weak associations between the generalization ability of machine learning
    models and all metrics related to dimensionality , thus challenging the common
    assumption that the curse of dimensionality might impair generalization in machine
    learning .
- source_sentence: What evaluation protocols and benchmark setups are commonly used
    to assess model performance?
  sentences:
  - These critical subtasks are what expert humans utilize to quickly learn in new
    environments that share subtasks with previously learned environments , and are
    a reason for humans superior data efficiency in learning complex tasks . In the
    case of deliberately similar environments , we can construct the subtasks such
    that they are similar in function and representation that an agent trained on
    the first environment can accelerate learning on the second environment due to
    its preconstructed subtask representations , thus partially avoiding the more
    complex environment 's increased simulation cost and inherent learning difficulty
    .
  - Introduction Complex environments such as Go , Starcraft , and many modern video-games
    present profound challenges in deep reinforcement learning that have yet to be
    solved . They often require long , precise sequences of actions and domain knowledge
    in order to obtain reward , and have yet to be learned from random weight initialization
    . Solutions to these problems would mark a significant breakthrough on the path
    to artificial general intelligence . Recent works in reinforcement learning have
    shown that environments such as Atari games [ 2 ] can be learned from pixel input
    to superhuman expertise [ 9 ] . The agents start with randomly initialized weights
    , and learn largely from trial and error , relying on a reward signal to indicate
    performance . Despite these successes , complex games , including those where
    rewards are sparse such as Montezuma 's Revenge , have been notoriously difficult
    to learn . While methods such as intrinsic motivation [ 3 ] have been used to
    partially overcome these challenges , we suspect this becomes intractable as complexity
    increases . Additionally , as environments become more complex , they will become
    more expensive to simulate . This poses a significant problem , since many Atari
    games already require upwards of 100 million steps using state-of-the-art algorithms
    , representing days of training on a single machine .
  - As machine learning becomes more and more available to the general public , theoretical
    questions are turning into pressing practical issues . Possibly , one of the most
    relevant concerns is the assessment of our confidence in trusting machine learning
    predictions . In many real-world cases , it is of utmost importance to estimate
    the capabilities of a machine learning algorithm to generalize , i.e. , to provide
    accurate predictions on unseen data , depending on the characteristics of the
    target problem . In this work we perform a meta-analysis of 109 publicly-available
    classification data sets , modeling machine learning generalization as a function
    of a variety of data set characteristics , ranging from number of samples to intrinsic
    dimensionality , from class-wise feature skewness to F 1 evaluated on test samples
    falling outside the convex hull of the training set .
pipeline_tag: sentence-similarity
library_name: sentence-transformers
---

# SentenceTransformer based on sentence-transformers/all-MiniLM-L6-v2

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) <!-- at revision c9745ed1d9f207416be6d2e6f8de32d1f16199bf -->
- **Maximum Sequence Length:** 256 tokens
- **Output Dimensionality:** 384 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/huggingface/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ðŸ¤— Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'What evaluation protocols and benchmark setups are commonly used to assess model performance?',
    'As machine learning becomes more and more available to the general public , theoretical questions are turning into pressing practical issues . Possibly , one of the most relevant concerns is the assessment of our confidence in trusting machine learning predictions . In many real-world cases , it is of utmost importance to estimate the capabilities of a machine learning algorithm to generalize , i.e. , to provide accurate predictions on unseen data , depending on the characteristics of the target problem . In this work we perform a meta-analysis of 109 publicly-available classification data sets , modeling machine learning generalization as a function of a variety of data set characteristics , ranging from number of samples to intrinsic dimensionality , from class-wise feature skewness to F 1 evaluated on test samples falling outside the convex hull of the training set .',
    "Introduction Complex environments such as Go , Starcraft , and many modern video-games present profound challenges in deep reinforcement learning that have yet to be solved . They often require long , precise sequences of actions and domain knowledge in order to obtain reward , and have yet to be learned from random weight initialization . Solutions to these problems would mark a significant breakthrough on the path to artificial general intelligence . Recent works in reinforcement learning have shown that environments such as Atari games [ 2 ] can be learned from pixel input to superhuman expertise [ 9 ] . The agents start with randomly initialized weights , and learn largely from trial and error , relying on a reward signal to indicate performance . Despite these successes , complex games , including those where rewards are sparse such as Montezuma 's Revenge , have been notoriously difficult to learn . While methods such as intrinsic motivation [ 3 ] have been used to partially overcome these challenges , we suspect this becomes intractable as complexity increases . Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine .",
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[1.0000, 0.2208, 0.1065],
#         [0.2208, 1.0000, 0.3090],
#         [0.1065, 0.3090, 1.0000]])
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 86 training samples
* Columns: <code>sentence_0</code> and <code>sentence_1</code>
* Approximate statistics based on the first 86 samples:
  |         | sentence_0                                                                         | sentence_1                                                                           |
  |:--------|:-----------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|
  | type    | string                                                                             | string                                                                               |
  | details | <ul><li>min: 15 tokens</li><li>mean: 23.98 tokens</li><li>max: 35 tokens</li></ul> | <ul><li>min: 63 tokens</li><li>mean: 209.27 tokens</li><li>max: 256 tokens</li></ul> |
* Samples:
  | sentence_0                                                                                                                                          | sentence_1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
  |:----------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | <code>What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?</code>        | <code>INTRODUCTION In order to formulate a learning theory of machine learning , it may be necessary to move from seeing an inert model as the machine learner to seeing the human developer-along with , and not separate from , his or her model and surrounding social relations-as the machine learner . -Reigeluth & Castelle [ 55 ] The past decade has seen massive research on interpretable machine learning ( ML ) . 1 Here is a rough restatement of the goal of interpretable ML research program : many ML models are opaque in that even the expert humans can not robustly understand , in non-mathematical terms , the reasons for why particular outputs are generated by these models [ 31 , 42 , 66 ] . To overcome this opacity , various model-centric techniques have been developed to interpret their outputs . These techniques are diverse . They range from producing counterfactual explanations or heatmaps that offer insights into how changing inputs affect outputs [ 28 , 41 , 46 ] , to interpreting the in...</code> |
  | <code>What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?</code>        | <code>Current developments are towards specialising these ML approaches to be more domain-specific and domain-aware [ 5 ] [ 6 ] [ 7 ] , and aiming to connect the apparent 'black box ' successes of DL networks with well-understood approaches from science . The overarching scope of ML in science is very broad , including identifying patterns , anomalies , and trends from relevant scientific datasets , and using ML for classification and predicting of those patterns , clustering of data , and generating near-realistic synthetic data . There are three approaches for developing ML-based solutions , namely , supervised , unsupervised , and reinforcement learning . In supervised learning , the ML model is trained for a given task with examples . In order to have examples , the data used for training the ML model must contain the ground truth or labels . Supervised learning is therefore only possible when there is a labelled subset of the data . Once trained , the learned model can be deployed for r...</code> |
  | <code>How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?</code> | <code>In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows . The theoretical ideas are laid out and the case for a new operational definition of machine learning is made . We put forward the conjecture that the optimization of model entropy , leads to concept formation . Last , conclusions and further steps to operationalize these concepts are formulated . 2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a ( non- ) parametric model with respect to some performance measure . This view is clearly reflected in the de facto definition of machine learning by Mitchell [ Mitchell , 1997 ] : `` A computer program is said to learn from an experience X with respect to some class of tas...</code> |
* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:
  ```json
  {
      "scale": 20.0,
      "similarity_fct": "cos_sim",
      "gather_across_devices": false
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `num_train_epochs`: 1
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 1
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `parallelism_config`: None
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch_fused
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `project`: huggingface
- `trackio_space_id`: trackio
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `hub_revision`: None
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: no
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `liger_kernel_config`: None
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: True
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin
- `router_mapping`: {}
- `learning_rate_mapping`: {}

</details>

### Framework Versions
- Python: 3.13.1
- Sentence Transformers: 5.2.0
- Transformers: 4.57.3
- PyTorch: 2.9.1+cpu
- Accelerate: 1.12.0
- Datasets: 4.5.0
- Tokenizers: 0.22.1

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

#### MultipleNegativesRankingLoss
```bibtex
@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply},
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->