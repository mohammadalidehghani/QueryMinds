question_id,chunk_id,question_text,chunk_text,person_1,person_2,person_3,final_label
q1,1501.04309v1_abstract_0000,"What problem, limitation, or research gap motivates a machine learning approach?","In this position paper , I first describe a new perspective on machine learning ( ML ) by four basic problems ( or levels ) , namely , `` What to learn ? `` , `` How to learn ? `` , `` What to evaluate ? `` , and `` What to adjust ? `` . The paper stresses more on the first level of `` What to learn ? `` , or `` Learning Target Selection '' . Towards this primary problem within the four levels , I briefly review the existing studies about the connection between information theoretical learning ( ITL [ 1 ] ) and machine learning . A theorem is given on the relation between the empirically-defined similarity measure and information measures . Finally , a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection .",0,1,0,0
q1,1501.04309v1_introduction_0002,"What problem, limitation, or research gap motivates a machine learning approach?","It is the author 's belief that `` mathematical-principle-based machine '' might be more important and critical than `` brain-inspired machine '' in the study of machine learning . The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning . In what follows I will present four basic problems ( or levels ) in machine learning . The study on information theoretical learning is briefly reviewed . A theorem between the empirically-defined similarity measures and information measures are given . Based on the existing investigations , a conjecture is proposed in this paper .",0,0,0,0
q1,1504.03874v1_abstract_0000,"What problem, limitation, or research gap motivates a machine learning approach?","Machine learning is a quickly evolving field which now looks really different from what it was 15 years ago , when classification and clustering were major issues . This document proposes several trends to explore the new questions of modern machine learning , with the strong afterthought that the belief function framework has a major role to play .",1,0,0,0
q1,1504.03874v1_introduction_0002,"What problem, limitation, or research gap motivates a machine learning approach?","However , this reason alone is not sufficient : There are indeed other topics , such as for instance , information fusion , where BF-based methods are now as well recognized as are methods based on more classical formalisms , such as probabilities , or ontologies . In this report , I assume an additional reason : that some researchers focused on BFT ( especially the youngest ) , who have progressively turned their interests towards ML problems , may not capture the newest trends of this field . In fact , I used to be an example of such researchers , and I acknowledge that my first perceptions of ML were clearly outdated . This is why , I propose a short review of the respective evolution of BFT and of ML , as well as an attempt to put them in perspective . Of course , many senior researchers may find this exercise futile , as they have their own broad view on the question . However , to my knowledge , no recent referenced article is available for any reader seeking for a starting point to question the links between ML and BFT . This document is structured as follow : In Section 2 , a brief recall of the evolution of the mainstream in the BF community is provided .",1,1,0,1
q1,1504.03874v1_introduction_0003,"What problem, limitation, or research gap motivates a machine learning approach?","However , to my knowledge , no recent referenced article is available for any reader seeking for a starting point to question the links between ML and BFT . This document is structured as follow : In Section 2 , a brief recall of the evolution of the mainstream in the BF community is provided . Then , in Section 3 , a short summary of the earlier ages of ML up to the mid-90s , is sketched , as well as a coarse description of the successful interactions between ML and BFT in those times . Afterward , I provide in Section 4 a synthetic overview of the revolution that blew over ML around the early 2000s , and which modified its goals and the organization of its supporting community . As BFT does not seem to fit in this new picture of the ML world , I list in Section 5 a few problems that may still be of interest for the current mainstream of BFT , as well as some potential interesting evolutions for the community to adapt to the newly raised questions .",1,0,0,0
q1,1505.06614v1_abstract_0000,"What problem, limitation, or research gap motivates a machine learning approach?","In this short paper , the Electre Tri-Machine Learning Method , generally used to solve ordinal classification problems , is proposed for solving the Record Linkage problem . Preliminary experimental results show that , using the Electre Tri method , high accuracy can be achieved and more than 99 % of the matches and nonmatches were correctly identified by the procedure .",1,1,0,1
q1,1505.06614v1_introduction_0001,"What problem, limitation, or research gap motivates a machine learning approach?","Introduction Machine Learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to `` learn data '' . More precisely , `` learn '' is here intended as the possibility to automatically recognize complex patterns and make `` intelligent '' decisions , based on information data . Hence , machine learning is closely related to fields such as statistics , probability theory , data mining , pattern recognition , artificial intelligence , adaptive control and theoretical computer science . Machine learning algorithms can be classified in the following types : • supervised learning algorithms : a function/classifier is generated , that maps outputs on the training inputs , based on labeled examples inputoutput ; • unsupervised learning algorithms : patterns in the input are recognized , the examples have no labels ; • semi-supervised learning algorithms : supervised and unsupervised learning information is combined ; • reinforcement learning : actions from observation of the world are generated . Every action has some impact in the environment and the environment provides feedbacks that are translated into a score that guide the learning process . The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] .",0,1,0,0
q1,1505.06614v1_introduction_0002,"What problem, limitation, or research gap motivates a machine learning approach?","The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] . In this short paper , another machine learning technique is proposed to solve the record linkage problem : the multi-criteria classification method Electre Tri . It is the first time that multi-criteria machine learning technique is used to solve the record linkage problem . This application answers to one of `` many challenges in applying supervised machine learning to record linkage matching '' [ 10 ] , showing that the use of multi-criteria classification method Electre Tri to solve the record linkage problem provides good results in term of classification model performances . The importance of this application is in light of the increasing development of the use of administrative sources data . In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows .",1,1,1,1
q1,1505.06614v1_introduction_0003,"What problem, limitation, or research gap motivates a machine learning approach?","In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows . Section 2 describes an introduction to the Record Linkage problem ; then the next Section 3 describes the method Electre Tri , used to solved the Record Linkage and in the last Section 4 a preliminary experiment is conducted on simulated data . The paper closes with some final remarks and conclusions . 2 Linked Data : the Record Linkage Generally speaking , in integration of two data sets the objective is the detection of those records , in the different data sets , that belong to the same statistical unit . This action allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] .",1,1,0,1
q1,1505.06614v1_introduction_0004,"What problem, limitation, or research gap motivates a machine learning approach?","This action allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] . In the first situation , the definition of record linkage in [ 9 ] is more precise `` Record linkage is a solution to the problem of recognizing those records in two files which represent identical persons , objects , or events ( said to be matched ) '' The term record linkage originated in the public health area when files of individual patients were brought together using name , date-of-birth and other information [ 16 ] . One of the main motivations for the utilize of the record linkage method is the construction of the big data bases for answer to the new informative needs [ 8 ] . In order to better understand the problem , small practical example is now presented . Suppose the user wants to link two datasets of persons A and B , whose the variables Name , Address and Age are known .",1,1,0,1
q1,1505.06614v1_introduction_0005,"What problem, limitation, or research gap motivates a machine learning approach?","In order to better understand the problem , small practical example is now presented . Suppose the user wants to link two datasets of persons A and B , whose the variables Name , Address and Age are known . Suppose that Table A contains the following values : Table A : Data in the first dataset Unit Name Address Age a1 John A Smith 16 Main Street 16 a2 Javier Martinez 49 E Applecross Road 33 a3 Gillian Jones 645 Reading Aev 22 Furthermore , suppose that Table B contains the following values : Table B : Data in the second dataset Unit Name Address Age b1 J H Smith 16 Main St 17 b2 Haveir Marteenez 49 Aplecross Raod 36 b3 Jilliam Brown 123 Norcross Blvd 43 The matching table A × B contains two units referring probably to the same persons , that the method should individuate as matches : 'John A Smith ' with ' J H Smith ' and 'Javier Martinez ' with 'Haveir Marteenez ' . Modern record linkage begins with the pioneering work of Newcombe et al . [ 14 ] , who introduced odds ratio of frequencies and the decision rules for delineating matches and nonmatches . In recent years , advances have yielded computer system that incorporate sophisticated ideas from computer sciences , statistics and operational research [ 16 ] .",0,1,0,0
q1,1505.06614v1_introduction_0007,"What problem, limitation, or research gap motivates a machine learning approach?","Between these two sets , the intermediate set of the possible matches exists . The decision rule reported below helps to classify the pairs : • if R > U pper , then the pair ( a , b ) is a designated match , • if Lower ≤ R ≤ U pper , then the pair ( a , b ) is a designated potential match , • if R < Lower , then the pair ( a , b ) is a designated nonmatch . The estimation of the thresholds Upper and Lower is not easy in an objective way ; the choice is competence of the analyst . In the decision rule , three different sets were created : the designated matches , designated potential matches , designated nonmatches . They constitute the partition of the set of all the records in the space Γ in three subsets C 3 ( matches ) , C 2 ( potential matches ) and C 1 ( nonmatches ) , whose intersections are empty sets . The idea is to solve the record linkage problem as a multi-criteria based classification problem , whose a priori defined classes are the subsets of the partition . Without going into too much details , in the next section a brief introduction to the method Electre Tri is presented .",1,1,0,1
q1,1612.04858v1_abstract_0000,"What problem, limitation, or research gap motivates a machine learning approach?","The engineering of machine learning systems is still a nascent field ; relying on a seemingly daunting collection of quickly evolving tools and best practices . It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques . We outline four example machine learning problems that can be solved using open source machine learning libraries , and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications .",1,1,0,1
q1,1612.04858v1_introduction_0001,"What problem, limitation, or research gap motivates a machine learning approach?","Introduction Recently , there has been interest in applying Bayesian black-box optimization strategies to better conduct optimization over hyperparameter configurations of machine learning models and systems [ 19 ] [ 21 ] [ 11 ] . Most of these techniques require that the objective be a scalar value depending on the hyperparamter configuration x. x opt = arg max x∈X f ( x ) A more detailed introduction to Bayesian optimization and related techniques is provided in [ 8 ] . The focus of this guidebook is on demonstrating several example problems where Bayesian optimization provides a noted benefit . Our hope is to clearly show how Bayesian optimization can assist in better designing and optimizing real-world machine learning systems . All of the examples in this guidebook have corresponding code available on SigOpt 's example github repo .",1,0,0,0
q1,1612.07640v1_introduction_0006,"What problem, limitation, or research gap motivates a machine learning approach?","It is shown that the representation defines the upper-bound performances of machine learning algorithms [ 19 ] . However , it is difficult to know and determine what kind of good features should be designed . To alleviate this issue , feature extraction/selection methods , which can be regarded as a kind of information fusion , are performed between hand-crafted feature design and classification/regression models [ 20 ] , [ 21 ] , [ 22 ] . However , manually designing features for a complex domain requires a great deal of human labor and can not be updated on-line . At the same time , feature extraction/selection is another tricky problem , which involves prior selection of hyperparameters such as latent dimension . At last , the above three modules including feature design , feature extraction/selection and model training can not be jointly optimized which may hinder the final performance of the whole system . Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values .",1,1,1,1
q1,1612.07640v1_introduction_0007,"What problem, limitation, or research gap motivates a machine learning approach?","Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values . Therefore , the application of one layer can learn a new representation of the input data and then , the stacking structure of multiple layers can enable MHMS to learn complex concepts out of simpler concepts that can be constructed from raw input . In addition , DL-based MHMS achieve an end-to-end system , which can automatically learn internal representations from raw input and predict targets . Compared to conventional data driven MHMS , DL-based MHMS do not require extensive human labor and knowledge for handcrafted feature design . All model parameters including feature module and pattern classification/regression module can be trained jointly . Therefore , DL-based models can be applied to addressing machine health monitoring in a very general way . For example , it is possible that the model trained for fault diagnosis problem can be used for prognosis by only replacing the top softmax layer with a linear regression layer . The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 .",1,1,0,1
q1,1703.10121v1_introduction_0001,"What problem, limitation, or research gap motivates a machine learning approach?","Introduction In 2007 , a paper named `` Top 10 algorithms in data mining '' identified and presented the top 10 most influential data mining algorithms within the research community [ 1 ] . The selection criteria were created by consolidating direct nominations from award winning researchers , the research community opinions and the number of citations in Google Scholar . The top 10 algorithms in that prior work are : C4.5 , k-means , support vector machine , Apriori , EM , PageRank , Ad-aBoost , kNN , naive Bayes and CART . In the decade that passed since then , machine learning has expanded , responding to incremental development of computational capabilities and substantial increase of problems in the commercial applications . This study reflects on the top 10 most popular fields of active research in machine learning , as they emerged from the quantitative analysis of leading journals and conferences . This work sees some topics in the broader sense including not only models but also concepts like data sets , features , optimization techniques and evaluation metrics . This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts .",1,0,0,0
q1,1706.05749v1_abstract_0000,"What problem, limitation, or research gap motivates a machine learning approach?","This paper introduces Dex , a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems . We also present the novel continual learning method of incremental learning , where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment . We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments . We finally develop a saliency method for qualitative analysis of reinforcement learning , which shows the impact incremental learning has on network attention .",1,0,0,0
q1,1706.05749v1_introduction_0001,"What problem, limitation, or research gap motivates a machine learning approach?","Introduction Complex environments such as Go , Starcraft , and many modern video-games present profound challenges in deep reinforcement learning that have yet to be solved . They often require long , precise sequences of actions and domain knowledge in order to obtain reward , and have yet to be learned from random weight initialization . Solutions to these problems would mark a significant breakthrough on the path to artificial general intelligence . Recent works in reinforcement learning have shown that environments such as Atari games [ 2 ] can be learned from pixel input to superhuman expertise [ 9 ] . The agents start with randomly initialized weights , and learn largely from trial and error , relying on a reward signal to indicate performance . Despite these successes , complex games , including those where rewards are sparse such as Montezuma 's Revenge , have been notoriously difficult to learn . While methods such as intrinsic motivation [ 3 ] have been used to partially overcome these challenges , we suspect this becomes intractable as complexity increases . Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine .",1,1,1,1
q1,1706.05749v1_introduction_0002,"What problem, limitation, or research gap motivates a machine learning approach?","Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine . Thus , it appears likely that complex environments will become too costly to learn from randomly initialized weights , due both to the increased simulation cost as well as the inherent difficulty of the task . Therefore , some form of prior information must be given to the agent . This can be seen with AlphaGo [ 18 ] , where the agent never learned to play the game without first using supervised learning on human games . While supervised learning certainly has been shown to aid reinforcement learning , it is very costly to obtain sufficient samples and requires the environment to be a task humans can play with reasonable skill , and is therefore impractical for a wide variety of important reinforcement learning problems . In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent .",1,1,1,1
q2,1501.04309v1_introduction_0001,What is the central idea or key contribution of a machine learning approach?,"Introduction Machine learning is the study and construction of systems that can learn from data . The systems are called learning machines . When Big Data emerges increasingly , more learning machines are developed and applied in different domains . However , the ultimate goal of machine learning study is insight , not machine itself . By the term insight I mean learning mechanisms in descriptions of mathematical principles . In a loose sense , learning mechanisms can be regarded as the natural entity . As the `` Tao ( 道 ) '' reflects the most fundamental of the universe by Lao Tzu ( 老子 ) , Einstein suggests that we should pursue the simplest mathematical interpretations to the nature . Although learning mechanisms are related to the subjects of psychology , cognitive and brain science , this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms . Up to now , we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles . It is the author 's belief that `` mathematical-principle-based machine '' might be more important and critical than `` brain-inspired machine '' in the study of machine learning . The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning .",1,0,0,0
q2,1501.04309v1_introduction_0002,What is the central idea or key contribution of a machine learning approach?,"It is the author 's belief that `` mathematical-principle-based machine '' might be more important and critical than `` brain-inspired machine '' in the study of machine learning . The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning . In what follows I will present four basic problems ( or levels ) in machine learning . The study on information theoretical learning is briefly reviewed . A theorem between the empirically-defined similarity measures and information measures are given . Based on the existing investigations , a conjecture is proposed in this paper .",1,0,0,0
q2,1612.07640v1_introduction_0008,What is the central idea or key contribution of a machine learning approach?,"The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 . Deep learning models have several variants such as Auto-Dncoders [ 23 ] , Deep Belief Network [ 24 ] , Deep Boltzmann Machines [ 25 ] , Convolutional Neural Networks [ 26 ] and Recurrent Neural Networks [ 27 ] . During recent years , various researchers have demonstrated success of these deep learning models in the application of machine health monitoring . This paper attempts to provide a wide overview on these latest DL-based MHMS works that impact the state-of-the art technologies . Compared to these frontiers of deep learning including Computer Vision and Natural Language Processing , machine health monitoring community is catching up and has witnessed an emerging research . Therefore , the purpose of this survey article is to present researchers and engineers in the area of machine health monitoring system , a global view of this hot and active topic , and help them to acquire basic knowledge , quickly apply deep learning models and develop novel DL-based MHMS . The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring .",1,0,0,0
q2,1706.05749v1_abstract_0000,What is the central idea or key contribution of a machine learning approach?,"This paper introduces Dex , a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems . We also present the novel continual learning method of incremental learning , where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment . We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments . We finally develop a saliency method for qualitative analysis of reinforcement learning , which shows the impact incremental learning has on network attention .",1,0,1,1
q2,1706.05749v1_introduction_0002,What is the central idea or key contribution of a machine learning approach?,"Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine . Thus , it appears likely that complex environments will become too costly to learn from randomly initialized weights , due both to the increased simulation cost as well as the inherent difficulty of the task . Therefore , some form of prior information must be given to the agent . This can be seen with AlphaGo [ 18 ] , where the agent never learned to play the game without first using supervised learning on human games . While supervised learning certainly has been shown to aid reinforcement learning , it is very costly to obtain sufficient samples and requires the environment to be a task humans can play with reasonable skill , and is therefore impractical for a wide variety of important reinforcement learning problems . In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent .",1,1,1,1
q2,1706.05749v1_introduction_0003,What is the central idea or key contribution of a machine learning approach?,"In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent . This task can be split into a series of subtasks that are solved simultaneously . Similar to how natural language processing and object detection are subtasks of neural image caption generation [ 23 ] , reinforcement learning environments also have subtasks relevant to a given environment . These subtasks often include player detection , player control , obstacle detection , enemy detection , and player-object interaction , to name a few . These subtasks are common to many environments , but they are often sufficiently different in function and representation that reinforcement learning algorithms fail to generalize them across environments , such as in Atari . These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments , and are a reason for humans superior data efficiency in learning complex tasks .",1,1,1,1
q2,1811.04871v1_abstract_0000,What is the central idea or key contribution of a machine learning approach?,"Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises . For example , existing machine learning processes can not address how to define business use cases for an AI application , how to convert business requirements from offering managers into data requirements for data scientists , and how to continuously improve AI applications in term of accuracy and fairness , how to customize general purpose machine learning models with industry , domain , and use case specific data to make them more accurate for specific situations etc . Making AI work for enterprises requires special considerations , tools , methods and processes . In this paper we present a maturity framework for machine learning model lifecycle management for enterprises . Our framework is a re-interpretation of the software Capability Maturity Model ( CMM ) for machine learning model development process .",1,1,1,1
q2,1811.04871v1_abstract_0001,What is the central idea or key contribution of a machine learning approach?,In this paper we present a maturity framework for machine learning model lifecycle management for enterprises . Our framework is a re-interpretation of the software Capability Maturity Model ( CMM ) for machine learning model development process . We present a set of best practices from authors ' personal experience of building large scale real-world machine learning models to help organizations achieve higher levels of maturity independent of their starting point .,1,1,1,1
q2,1907.07543v1_introduction_0005,What is the central idea or key contribution of a machine learning approach?,"Howard & Ruder ( 2018 ) note , and Devlin et . al . ( 2018 ) hypothesize that their respective approaches can be used with low quantities of data to give good results . However , in sources such as ( Howard & Ruder , 2018 ) , results on low-shot learning are presented relative to training deep models from scratch , but as mentioned in ( Goodfellow , Bengio , & Courville , 2016 ) , deep learning generally only achieves reasonable performance at about 5000 examples per class and is therefore not necessarily the best paradigm at these scales . This is shown quantitatively in ( Chen , Mckeever , & Delany , 2018 ) where , at scales of 2000+ labels per class , an SVM outperforms several deep learning approaches on text classification tasks . As such , we propose that to evaluate the low-shot learning benefits of deep transfer learning models , we should in fact look at performance against the strongest classical machine learning methods . However , we have yet to find a comprehensive quantitative study performing this analysis and show that low-shot transfer learning in NLP is actually the optimal approach when dealing with small quantities of data . In this paper we attempt to answer this question in the context of classification tasks .",1,1,1,1
q2,1907.07543v1_introduction_0006,What is the central idea or key contribution of a machine learning approach?,"However , we have yet to find a comprehensive quantitative study performing this analysis and show that low-shot transfer learning in NLP is actually the optimal approach when dealing with small quantities of data . In this paper we attempt to answer this question in the context of classification tasks . What is the best paradigm to use in the case where we have 100 -1000 labelled training examples per class -classical machine learning or deep transfer learning ? We seek to compare the best-in-class approaches from both deep transfer learning and classical machine learning by training a variety of models and evaluating by analysing intra-domain and inter-domain performance ( details in section 2 ) . The choice of 100 -1000 is motivated by the amount of data feasible for companies and researchers to tag in-house , as well as the scale of data occurring organically through other means . For example , in marketing these figures typically represent the base sizes of surveys that can be used as training data . The rest of this paper is laid out as follows . Section 2 details the datasets we use . Section 3 looks at the methodology used to evaluate the optimal paradigm . In section 4 we present the algorithms we use to test , along with related work influencing our choices in selecting those models .",1,1,1,1
q2,1907.07543v1_introduction_0007,What is the central idea or key contribution of a machine learning approach?,"The rest of this paper is laid out as follows . Section 2 details the datasets we use . Section 3 looks at the methodology used to evaluate the optimal paradigm . In section 4 we present the algorithms we use to test , along with related work influencing our choices in selecting those models . Section 5 details our experiments including choosing the optimal configuration of hyperparameters and preprocessing for each algorithm . In section 6 we present the results followed by our comments and conclusions . Finally , we highlight a few key points and considerations worthy of mention for the two paradigms in 7 .",0,1,1,1
q2,2006.15680v1_introduction_0004,What is the central idea or key contribution of a machine learning approach?,"The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] . Taking inspiration from [ 14 ] , where the authors find links between data set characteristics and efficiency of feature selection techniques , we propose to empirically explore the relation between data-set characteristics and effectiveness of standard ML models , in order to obtain a general meta-model able to extrapolate . In order to answer the question , we analyzed 109 publicly available classification data sets from open-access , curated sources . We decided to focus on classification , as supervised ML represents a quite significant portion of real-world problems ; and , differently from regression , several sophisticated quality metrics have already been developed for this task [ 15 ] . During the analysis , we take into account characteristics such as number of features , number of classes , number of samples , and we look for correlations with quality metrics , such as accuracy of a ML model on training and test points .",1,1,1,1
q2,2103.11249v1_abstract_0000,What is the central idea or key contribution of a machine learning approach?,"One of the pillars of any machine learning model is its concepts . Using software engineering , we can engineer these concepts and then develop and expand them . In this article , we present a SELM framework for Software Engineering of machine Learning Models . We then evaluate this framework through a case study . Using the SELM framework , we can improve a machine learning process efficiency and provide more accuracy in learning with less processing hardware resources and a smaller training dataset . This issue highlights the importance of an interdisciplinary approach to machine learning . Therefore , in this article , we have provided interdisciplinary teams ' proposals for machine learning .",1,1,1,1
q2,2110.12773v1_abstract_0001,What is the central idea or key contribution of a machine learning approach?,"This is due to many different machine learning frameworks , computer architectures , and machine learning models . Historically , for modelling and simulation on HPC systems such problems have been addressed through benchmarking computer applications , algorithms , and architectures . Extending such a benchmarking approach and identifying metrics for the application of machine learning methods to scientific datasets is a new challenge for both scientists and computer scientists . In this paper , we describe our approach to the development of scientific machine learning benchmarks and review other approaches to benchmarking scientific machine learning .",1,1,1,1
q2,2205.00210v1_introduction_0003,What is the central idea or key contribution of a machine learning approach?,"This means that for constant test inputs and preconditions , an ML-trained software component can produce different outputs in consecutive runs . Researchers have tried using testing techniques from traditional software development ( Hutchison et al . 2018 ) , to deal with some of these challenges . However , it has been observed that traditional testing approaches in general fail to adequately address fundamental challenges of testing ML ( Helle and Schamai 2016 ) , and that these traditional approaches require adaptation to the new context of ML . The better we understand current research challenges of testing ML , the more successful we can be in developing novel techniques that effectively address these challenges and advance this scientific field . In this paper , we : i ) identify and discuss the most challenging areas in software testing for ML , ii ) synthesize the most promising approaches to these challenges , iii ) spotlight their limitations , and iv ) make recommendations of further research efforts on software testing of ML . We note that the aim of the paper is not to exhaustively list all published work , but distill the most representative work .",1,0,0,0
q2,2205.14136v1_introduction_0003,What is the central idea or key contribution of a machine learning approach?,"As another example , if there is an anomalous drop in purchase of a product in an online store , it is possible that the product is out of stock , which needs attention . The state-of-the-art technique Fig . 1 . A New Framework for Anomaly Detection for anomaly detection is machine learning [ 4 ] , [ 7 ] , [ 9 ] , [ 11 ] , [ 12 ] , [ 14 ] . Machine learning techniques learn distributions on continuous variables . Anomaly events can be captured as deviations from established patterns ( distributions ) . However , there are certain temporal behaviors and relations that can not be easily learned by machine learning techniques , but can be easily characterized by formal languages such as PSL . In this paper , we propose a new framework called TEmporal Filtering ( TEF ) for anomaly detection ( Fig . 1 ) . The idea is to merge machine learning with PSL monitors . The machine learning module takes as input a number of continuous variables x 1 , x 2 , . . . . . . , x m , and outputs some discrete events y 1 , y 2 , . . . . . . , y n , which become the input of the PSL monitor .",1,1,1,1
q2,2305.00520v1_abstract_0000,What is the central idea or key contribution of a machine learning approach?,"Transfer learning is an essential tool for improving the performance of primary tasks by leveraging information from auxiliary data resources . In this work , we propose Adaptive Robust Transfer Learning ( ART ) , a flexible pipeline of performing transfer learning with generic machine learning algorithms . We establish the non-asymptotic learning theory of ART , providing a provable theoretical guarantee for achieving adaptive transfer while preventing negative transfer . Additionally , we introduce an ART-integrated-aggregating machine that produces a single final model when multiple candidate algorithms are considered . We demonstrate the promising performance of ART through extensive empirical studies on regression , classification , and sparse learning . We further present a real-data analysis for a mortality study .",1,1,1,1
q2,2401.11351v2_introduction_0011,What is the central idea or key contribution of a machine learning approach?,"We have structured our review as follows . In section 2 , we focus on the historical and ongoing developments in Quantum Machine Learning ( QML ) during the Noisy Intermediate-Scale Quantum ( NISQ ) era . Within this era , one of the central frameworks is the Variational Quantum Algorithm ( VQA ) [ 17 , 44 , 108 ] , which we explore in detail in section 2.1 . VQA comprises four key elements : the choice of an objective function ( discussed in section 2.1.1 ) , the employment of Parameterized Quantum Circuits ( PQC ) with adjustable parameters optimized by classical algorithms ( covered in section 2.1.2 ) , measurement strategies ( explored in section 2.1.3 ) , and the classical optimizer responsible for minimizing the objective function ( explained in section 2.1.4 ) . Subsequently , we provide insights into the construction of the Quantum Neural Tangent Kernel ( QNTK ) [ 87 ] in section 2.2 , offering a theoretical foundation for quantum neural networks and an understanding of stochastic gradient descent dynamics from first principles . Finally , we address the issue of barren plateaus in section 2.3 , approaching this topic through the lens of quantum landscape theory [ 27 ] . Additionally , we present an alternative formulation of laziness [ 86 ] , providing another perspective on the same problem .",1,1,1,1
q2,2401.11351v2_introduction_0012,What is the central idea or key contribution of a machine learning approach?,"Finally , we address the issue of barren plateaus in section 2.3 , approaching this topic through the lens of quantum landscape theory [ 27 ] . Additionally , we present an alternative formulation of laziness [ 86 ] , providing another perspective on the same problem . In section 3 , our focus shifts to quantum algorithms that have the potential for exponential speedup in the Fault-Tolerant Quantum Computing ( FTQC ) era . We begin by introducing Quantum Phase Estimation ( QPE ) [ 29 ] in section 3.1 , where we also delve into the Quantum Principal Component Analysis ( QPCA ) program [ 91 ] . Subsequently , we present a counterpoint regarding the attainability of exponential speedup for programs such as the recommendation system in section 3.2 . Moving on to section 3.3 , we introduce the pivotal Harrow-Hassidim-Lloyd ( HHL ) algorithm [ 58 ] , which opens up possibilities for various new quantum machine learning algorithms . In section 3.3.1 , we illustrate a visionary perspective on the future of machine learning algorithms with the aid of Carleman linearization [ 90 ] and the HHL algorithm . Finally , in section 3.4 , we introduce Quantum Random Access Memory ( QRAM ) , focusing on theoretical designs [ 56 ] , practical implementation efforts [ 54 ] , and its necessity in certain algorithms [ 18 ] .",0,1,1,1
q2,2401.11351v2_introduction_0013,What is the central idea or key contribution of a machine learning approach?,"Finally , in section 3.4 , we introduce Quantum Random Access Memory ( QRAM ) , focusing on theoretical designs [ 56 ] , practical implementation efforts [ 54 ] , and its necessity in certain algorithms [ 18 ] . In section 4 , we delve into various topics that amalgamate quantum principles with statistical learning theory . Our primary focus is on shadow tomography [ 2 ] in section 4.1 , where we explore the motivation behind shadow tomography and delve into the construction of the theorem . A pivotal subject inspired by shadow tomography , the classical shadow formalism [ 41 ] , is thoroughly discussed in section 4.2 . Moving on to section 4.2.1 , we examine the application of classical shadow as an efficient quantum-to-classical information converter [ 66 ] . In section 4.3 , we then shift our attention to the applications of Quantum Machine Learning ( QML ) in the study of quantum data and quantum simulators [ 18 ] . It 's essential to note that our review does n't encompass various topics , including quantum machine learning algorithms whose advantages stem from sampling-related statements , the studies of quantum error corrections , quantum memory , and designs of quantum data centers .",0,1,1,1
q3,1505.06614v1_introduction_0003,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows . Section 2 describes an introduction to the Record Linkage problem ; then the next Section 3 describes the method Electre Tri , used to solved the Record Linkage and in the last Section 4 a preliminary experiment is conducted on simulated data . The paper closes with some final remarks and conclusions . 2 Linked Data : the Record Linkage Generally speaking , in integration of two data sets the objective is the detection of those records , in the different data sets , that belong to the same statistical unit . This action allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] .",0,0,0,0
q3,1505.06614v1_introduction_0007,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","Between these two sets , the intermediate set of the possible matches exists . The decision rule reported below helps to classify the pairs : • if R > U pper , then the pair ( a , b ) is a designated match , • if Lower ≤ R ≤ U pper , then the pair ( a , b ) is a designated potential match , • if R < Lower , then the pair ( a , b ) is a designated nonmatch . The estimation of the thresholds Upper and Lower is not easy in an objective way ; the choice is competence of the analyst . In the decision rule , three different sets were created : the designated matches , designated potential matches , designated nonmatches . They constitute the partition of the set of all the records in the space Γ in three subsets C 3 ( matches ) , C 2 ( potential matches ) and C 1 ( nonmatches ) , whose intersections are empty sets . The idea is to solve the record linkage problem as a multi-criteria based classification problem , whose a priori defined classes are the subsets of the partition . Without going into too much details , in the next section a brief introduction to the method Electre Tri is presented .",0,0,0,0
q3,1612.04858v1_introduction_0001,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","Introduction Recently , there has been interest in applying Bayesian black-box optimization strategies to better conduct optimization over hyperparameter configurations of machine learning models and systems [ 19 ] [ 21 ] [ 11 ] . Most of these techniques require that the objective be a scalar value depending on the hyperparamter configuration x. x opt = arg max x∈X f ( x ) A more detailed introduction to Bayesian optimization and related techniques is provided in [ 8 ] . The focus of this guidebook is on demonstrating several example problems where Bayesian optimization provides a noted benefit . Our hope is to clearly show how Bayesian optimization can assist in better designing and optimizing real-world machine learning systems . All of the examples in this guidebook have corresponding code available on SigOpt 's example github repo .",0,0,0,0
q3,1711.01431v1_abstract_0000,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","Machine learning is usually defined in behaviourist terms , where external validation is the primary mechanism of learning . In this paper , I argue for a more holistic interpretation in which finding more probable , efficient and abstract representations is as central to learning as performance . In other words , machine learning should be extended with strategies to reason over its own learning process , leading to so-called meta-cognitive machine learning . As such , the de facto definition of machine learning should be reformulated in these intrinsically multiobjective terms , taking into account not only the task performance but also internal learning objectives . To this end , we suggest a `` model entropy function '' to be defined that quantifies the efficiency of the internal learning processes . It is conjured that the minimization of this model entropy leads to concept formation .",0,1,0,0
q3,1711.01431v1_abstract_0001,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","To this end , we suggest a `` model entropy function '' to be defined that quantifies the efficiency of the internal learning processes . It is conjured that the minimization of this model entropy leads to concept formation . Besides philosophical aspects , some initial illustrations are included to support the claims .",1,1,0,1
q3,1711.01431v1_introduction_0004,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","Indeed , the whole evolution of Deep Learning can be seen as one specific approach in the quest to find models that are more structured ( i.e . have a lower entropy ) , by organizing and training them in a layer-wise fashion [ Bengio , 2009 ] . The focus has been mainly on training algorithms and designing model architectures that are adapted to these kinds of `` deep '' structures [ Deng and Yu , 2014 ] . Similar to efforts in multiobjective machine learning , these techniques are considered as a means to improve ( externally measured ) performance rather than a goal in itself [ Jin and Sendhoff , 2008 ] . We , however , do believe that minimizing the model 's structural complexity and optimizing its efficiency of representation , is not only a means to improve ( externally validated ) performance , but a central pillar to machine intelligence that leads to concept formulation and should be made explicit . In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows .",1,1,0,1
q3,1711.01431v1_introduction_0005,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows . The theoretical ideas are laid out and the case for a new operational definition of machine learning is made . We put forward the conjecture that the optimization of model entropy , leads to concept formation . Last , conclusions and further steps to operationalize these concepts are formulated . 2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a ( non- ) parametric model with respect to some performance measure . This view is clearly reflected in the de facto definition of machine learning by Mitchell [ Mitchell , 1997 ] : `` A computer program is said to learn from an experience X with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experimental data D '' . Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures .",0,1,0,0
q3,1811.04871v1_introduction_0002,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","I . INTRODUCTION Software and Services development has gone through various phases of maturity in the past few decades . The community has evolved lifecycle management theories and practices to disseminate best practices to developers , companies and consultants alike . For example , in software field , Software Development Life Cycle ( SDLC ) Management , capability maturity models ( CMM ) Application Life Cycle Management ( ALM ) , Product Life Cycle Management ( PLM ) models prescribe systematic theories and practical guidance for developing products in general , and software products in particular . Information Technology Infrastructure Library ( ITIL ) organization presents a set of detailed practices for IT Services management ( ITSM ) by aligning IT services with business objectives . All these practices provide useful guidance for developers in systematically building software and services assets . However , these methods fall short in managing a new breed of software services being developed rapidly in the industry . These are software services built with machine learnt models . We are well into the era of Artificial Intelligence ( AI ) , spurred by algorithmic , and computational advances , the availability of the latest algorithms in various software libraries , Cloud technologies , and the desire of companies to unleash insights from the vast amounts of untapped unstructured data lying in their enterprises .",0,0,0,0
q3,2306.14624v2_introduction_0005,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","Traversing this bridge , machine learning scholars can obtain new perspectives on the social situatedness of a probabilistic , statistical technology -we attempt to offer a new 'cognitive toolkit ' for thinking about the social situatedness of machine learning . Our point of view is that fairness can not be reduced to a formal , mathematical issue , but that it requires taking broader social context into account , reasoning for instance about responsibility . And for this , we suggest , insurance is an insightful analogon . Therefore , our objective is to furnish the reader with a guide that charts the landscape of insurance with respect to social issues and to establish links to machine learning . On a formal level , we use the following analogy . In a machine learning task , we are given some features X and associated outcomes Y , which we attempt to approximate by predictions Ŷ . The structural relation to insurance is established by conceiving of X as the features of policyholders ( e.g . age , gender ) with outcomes Y ( e.g . having an accident or not ) , and the task is to set a corresponding premium Ŷ .",0,0,0,0
q3,2401.11351v2_introduction_0006,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","However , information protected by QEC can persist if it remains undamaged within certain limits . It 's worth noting that all error correction codes have their constraints , implying that information will inevitably be lost if it 's severely damaged . Nevertheless , error correction provides a protective buffer zone against such losses . Our objective is to implement QEC codes across all quantum devices , ushering in an era of fault-tolerant quantum computing ( FTQC ) in the future . This trajectory parallels the history of classical computing , where , before the invention of classical error correction , scaling up and running useful algorithms on classical computers was also a formidable challenge [ 29 ] . Today , we can reliably operate classical computers everyday . Given the promising advancements in quantum error correction in recent times , our optimism about the future of quantum computing remains steadfast . However , when it comes to machine learning , classical machine learning does n't inherently reject noise [ 88 ] . The widely recognized learning algorithm , known as stochastic gradient descent , explicitly incorporates noise , and surprisingly , this noise addition actually enhances its performance . To grasp this concept , consider that noise can effectively steer us away from saddle points , offering an automatic mechanism for avoiding them .",0,0,0,0
q3,2401.11351v2_introduction_0008,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","In addition to discussing the current applications of quantum capabilities for machine learning , we should also let our imaginations soar . The era of fault-tolerant quantum computing ( FTQC ) is a foreseeable future , and it 's crucial to look ahead at what challenges we can tackle with quantum machine learning on FTQC devices . One of the most renowned and valuable algorithms in this context is the Harrow-Hassidim-Lloyd ( HHL ) algorithm [ 58 ] . Additionally , there are other algorithms that can be deployed to address a range of problems , such as principal component analysis [ 18 ] . Beside the optimistic future that quantum machine learning has , there are also a number of controversial issues with the subject . For example , some might argue that the variational quantum algorithm will not work in some circumstance . People working on quantum landscapes theory observed the famous barren plateau phenomena which leads to a kind of no-go theorem [ 27 , 86 ] . It amounts to situations that are hard to find minimum when we optimizes our objective function by gradient descent with randomized variational circuits . Debates indeed exist regarding whether quantum algorithms can consistently deliver exponential speedups . Some argue that quantum speedup is only guaranteed when dealing with quantum information .",0,0,0,0
q3,2401.11351v2_introduction_0009,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","It amounts to situations that are hard to find minimum when we optimizes our objective function by gradient descent with randomized variational circuits . Debates indeed exist regarding whether quantum algorithms can consistently deliver exponential speedups . Some argue that quantum speedup is only guaranteed when dealing with quantum information . When it comes to classical information , it 's conceivable to design classical machine learning models capable of achieving comparable average prediction accuracy [ 121 , 66 ] . In such cases , the computational complexity difference between classical and quantum approaches may , at worst , be a modest polynomial factor . This debate underscores the importance of carefully assessing the specific problem and context when considering the potential advantages of quantum algorithms . However , the landscape shifts when the objective is to attain a low worst-case prediction error . In this scenario , it becomes feasible to achieve an exponential divergence in computational complexities between classical and quantum approaches . This underscores the potential superiority of quantum algorithms when stringent requirements for worst-case accuracy are in play . This phenomenon can be viewed as a manifestation of classical effects casting their `` shadow '' in the quantum realm . It should come as no surprise to individuals well-versed in quantum theory that quantum predictions tend to align with classical theory when dealing with systems possessing a high degree of freedom .",0,0,1,0
q3,2401.11351v2_introduction_0011,"How are information-theoretic concepts (e.g., entropy, mutual information, KL divergence) connected to learning objectives or targets?","We have structured our review as follows . In section 2 , we focus on the historical and ongoing developments in Quantum Machine Learning ( QML ) during the Noisy Intermediate-Scale Quantum ( NISQ ) era . Within this era , one of the central frameworks is the Variational Quantum Algorithm ( VQA ) [ 17 , 44 , 108 ] , which we explore in detail in section 2.1 . VQA comprises four key elements : the choice of an objective function ( discussed in section 2.1.1 ) , the employment of Parameterized Quantum Circuits ( PQC ) with adjustable parameters optimized by classical algorithms ( covered in section 2.1.2 ) , measurement strategies ( explored in section 2.1.3 ) , and the classical optimizer responsible for minimizing the objective function ( explained in section 2.1.4 ) . Subsequently , we provide insights into the construction of the Quantum Neural Tangent Kernel ( QNTK ) [ 87 ] in section 2.2 , offering a theoretical foundation for quantum neural networks and an understanding of stochastic gradient descent dynamics from first principles . Finally , we address the issue of barren plateaus in section 2.3 , approaching this topic through the lens of quantum landscape theory [ 27 ] . Additionally , we present an alternative formulation of laziness [ 86 ] , providing another perspective on the same problem .",0,0,0,0
q4,1504.03874v1_introduction_0001,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Introduction In an age of user generated web-contents and of portable devices with embedded computer vision capabilities , machine learning ( ML ) and big data mining questions are fundamental . As a result , these questions naturally penetrate neighboring research fields , including belief function theory ( BFT ) , so that it is now usual to attend a `` Classification '' session [ 26 ] or a `` Machine Learning '' session [ 16 ] in a conference devoted to belief functions . However , it is hard to accept that among the various proposed approaches based on BF , very few have become state-of-the-art ML methods , the knowledge of which has spread beyond the BF community . Without any doubt , this can be partly explained by the relative size of the scientific communities under consideration : although quickly growing , the BF one is relatively small with respect to that of statistics , Bayesian networks , neural networks , etc . However , this reason alone is not sufficient : There are indeed other topics , such as for instance , information fusion , where BF-based methods are now as well recognized as are methods based on more classical formalisms , such as probabilities , or ontologies .",0,1,0,0
q4,1505.06614v1_introduction_0007,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Between these two sets , the intermediate set of the possible matches exists . The decision rule reported below helps to classify the pairs : • if R > U pper , then the pair ( a , b ) is a designated match , • if Lower ≤ R ≤ U pper , then the pair ( a , b ) is a designated potential match , • if R < Lower , then the pair ( a , b ) is a designated nonmatch . The estimation of the thresholds Upper and Lower is not easy in an objective way ; the choice is competence of the analyst . In the decision rule , three different sets were created : the designated matches , designated potential matches , designated nonmatches . They constitute the partition of the set of all the records in the space Γ in three subsets C 3 ( matches ) , C 2 ( potential matches ) and C 1 ( nonmatches ) , whose intersections are empty sets . The idea is to solve the record linkage problem as a multi-criteria based classification problem , whose a priori defined classes are the subsets of the partition . Without going into too much details , in the next section a brief introduction to the method Electre Tri is presented .",0,0,0,0
q4,1612.04858v1_abstract_0000,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","The engineering of machine learning systems is still a nascent field ; relying on a seemingly daunting collection of quickly evolving tools and best practices . It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques . We outline four example machine learning problems that can be solved using open source machine learning libraries , and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications .",0,1,0,0
q4,1612.04858v1_introduction_0001,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Introduction Recently , there has been interest in applying Bayesian black-box optimization strategies to better conduct optimization over hyperparameter configurations of machine learning models and systems [ 19 ] [ 21 ] [ 11 ] . Most of these techniques require that the objective be a scalar value depending on the hyperparamter configuration x. x opt = arg max x∈X f ( x ) A more detailed introduction to Bayesian optimization and related techniques is provided in [ 8 ] . The focus of this guidebook is on demonstrating several example problems where Bayesian optimization provides a noted benefit . Our hope is to clearly show how Bayesian optimization can assist in better designing and optimizing real-world machine learning systems . All of the examples in this guidebook have corresponding code available on SigOpt 's example github repo .",0,1,0,0
q4,1612.07640v1_introduction_0006,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","It is shown that the representation defines the upper-bound performances of machine learning algorithms [ 19 ] . However , it is difficult to know and determine what kind of good features should be designed . To alleviate this issue , feature extraction/selection methods , which can be regarded as a kind of information fusion , are performed between hand-crafted feature design and classification/regression models [ 20 ] , [ 21 ] , [ 22 ] . However , manually designing features for a complex domain requires a great deal of human labor and can not be updated on-line . At the same time , feature extraction/selection is another tricky problem , which involves prior selection of hyperparameters such as latent dimension . At last , the above three modules including feature design , feature extraction/selection and model training can not be jointly optimized which may hinder the final performance of the whole system . Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values .",1,0,1,1
q4,1703.10121v1_introduction_0001,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Introduction In 2007 , a paper named `` Top 10 algorithms in data mining '' identified and presented the top 10 most influential data mining algorithms within the research community [ 1 ] . The selection criteria were created by consolidating direct nominations from award winning researchers , the research community opinions and the number of citations in Google Scholar . The top 10 algorithms in that prior work are : C4.5 , k-means , support vector machine , Apriori , EM , PageRank , Ad-aBoost , kNN , naive Bayes and CART . In the decade that passed since then , machine learning has expanded , responding to incremental development of computational capabilities and substantial increase of problems in the commercial applications . This study reflects on the top 10 most popular fields of active research in machine learning , as they emerged from the quantitative analysis of leading journals and conferences . This work sees some topics in the broader sense including not only models but also concepts like data sets , features , optimization techniques and evaluation metrics . This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts .",0,0,0,0
q4,1706.05749v1_introduction_0002,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine . Thus , it appears likely that complex environments will become too costly to learn from randomly initialized weights , due both to the increased simulation cost as well as the inherent difficulty of the task . Therefore , some form of prior information must be given to the agent . This can be seen with AlphaGo [ 18 ] , where the agent never learned to play the game without first using supervised learning on human games . While supervised learning certainly has been shown to aid reinforcement learning , it is very costly to obtain sufficient samples and requires the environment to be a task humans can play with reasonable skill , and is therefore impractical for a wide variety of important reinforcement learning problems . In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent .",0,0,0,0
q4,1711.01431v1_introduction_0003,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","One possible analogy to better understand the above statements can be found in software engineering . When considering code that performs a specific task , we do not care only about its functionality , but also about its execution speed/efficiency and other so-called `` non-functional requirements '' . Furthermore , a carefully modularized design probably reflects more understanding than an endless enumeration of IF-ELSE clauses . In other words , finding a more efficient and structured way to represent/reproduce information and to perform a learning task , is as central to machine learning as the reproduction of results . Different to humans , of course , machines are measurable . This provides us with a unique opportunity to study the nature of learning in principle , at the same time improving Machine Intelligence . We are not claiming that model complexity/efficiency has not been subject to past research efforts . On the contrary , many techniques and design principles have attempted to improve exactly these properties -like Occam 's razor , Bayesian structure learning , pruning , the use of prototypes to compact information , regularization as a strategy to reduce energy , weight sharing in RNNs or CNNs to decrease model complexity , etc . Indeed , the whole evolution of Deep Learning can be seen as one specific approach in the quest to find models that are more structured ( i.e .",1,1,1,1
q4,1711.01431v1_introduction_0008,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","In other words : Data representation and model computation should be considered as two sides of the same coin . As a result the structural properties of both the model and data space are key to the modelling of higher abstractions . Sparse coding is a perfect example of this . Without sparse coding , although the information is intrinsically `` present '' in the data , neural networks become intractable to train due to the extremely volatile and complex decision surface . From this perspective we follow the observations that have been made by Bengio in [ Bengio et al. , 2013 ] on representation learning . One of the interesting phenomena is `` information entanglement '' [ Glorot et al. , 2011 ] . In this case , the model space is of a lower dimensionality or complexity than the data space . The projection of the data onto a high-dimensional space using sparse coding , then , has the advantage that the representations are more likely to be linearly separable , or at least less nonlinear . On the other hand , when the model complexity is increased considerably ( e.g . by adding layers ) , the neural network becomes untrainable using traditional techniques , because the dimensionality of the search space explodes . Deep learning techniques tackle this issue by -among other techniques -pre-initializing the model-space of particular layer in a maximum-likelihood/minimal-energy state .",1,0,0,0
q4,1811.11669v1_abstract_0000,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?",Quantifying and managing uncertainties that occur when data-driven models such as those provided by AI and machine learning methods are applied is crucial . This whitepaper provides a brief motivation and first overview of the state of the art in identifying and quantifying sources of uncertainty for data-driven components as well as means for analyzing their impact .,0,0,1,0
q4,1811.11669v1_introduction_0001,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","As a consequence , the functional behavior expected from data-driven components can only be specified in part on their intended domain , and we can not assure that they will behave as expected in all cases . Moreover , their processing structure is usually difficult to trace and validate by humans because this structure rarely follows human intuition but is generated to provide the algorithmically generalized input-output relationship in an effective manner . Prominent representatives of models used by data-driven components are artificial neural networks and support vector machines ( Russell & Norvig , 2016 ) . Since data-driven models are an important source of uncertainty in embedded systems that collaborate in an open context , the uncertainty they introduce has to be appropriately understood and managed during design time and runtime . Previous work ( Kläs & Vollmer , 2018 ) proposes separating the sources of uncertainty in data-driven components into three major classes , distinguishing between uncertainty caused by limitations in terms of model fit , data quality , and scope compliance . Whereas model fit focuses on the inherent uncertainty in data-driven models , data quality covers the additional uncertainty caused by their application to input data obtained in suboptimal conditions and scope compliance covers situations where the model is likely applied outside the scope for which it was trained and validated .",0,0,1,0
q4,1812.10422v1_introduction_0001,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Introduction 1.Digital Agenda of the Federal Statistical Office of Germany On 10 October 2017 , the development of a Digital Agenda of the Federal Statistical Office of Germany ( Destatis ) has started ( Statistisches Bundesamt 2018 ) . One of many topics that were intensively discussed was Machine Learning . In a meeting at 13-15 November 2017 , the office and department heads of Destatis evaluated and prioritised 59 measures of the Digital Agenda according to their benefits and costs . A `` Proof of Concept Machine Learning '' was given high priority and classified as one of four lighthouse projects of the Digital Agenda . The content specification was `` Proof of Concept Machine Learning -Set up Proof of Concept for Machine Learning , e.g . in business statistics , to perform automatic categorization and improve analysis potential '' . The deadline for completion of the project was set for mid-2018 . foot_0",0,0,0,0
q4,1903.00092v2_introduction_0001,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Introduction Online decision-making problems fundamentally address the issue of dealing with the uncertainty inherently present in the future . In broad terms , these problems can be addressed in two ways . First , a predictive approach like a machine learning algorithm can be used to guess at future events and to act accordingly . This method , while clearly powerful , has the drawback that it is very difficult to make any guarantees about the performance of the algorithm . Another paradigm for solving these problems is to use competitive analysis to guarantee a bound on the performance of a given algorithm . In this approach , we consider some cost function and note the cost that would be incurred by an omniscient algorithm that could access future data . Then , we compare this optimal omniscient cost to the worst-case cost of an online algorithm which can not use future data . By bounding the ratio of these two costs , a guarantee can be made as to how well the online algorithm will perform in terms of the performance of an omniscient algorithm . A classic problem in competitive analysis is the Ski Rental Problem . In this problem , a skier must decide whether to rent skis at a rate of $ 1 per day , or to buy skis at a price of $ B .",0,0,1,0
q4,1903.00092v2_introduction_0002,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","A classic problem in competitive analysis is the Ski Rental Problem . In this problem , a skier must decide whether to rent skis at a rate of $ 1 per day , or to buy skis at a price of $ B . The uncertainty lies in the number of ski days left in the season -if there are very few days , it is optimal to rent the skis . On the other hand , if there are many ski days it may be more cost-effective to buy the skis outright . This simple paradigm extends to various practical problems , such as a firm deciding between purchasing servers or using an on-demand cloud computing solution to provide the computational power needed to satisfy an unknown future demand .",0,0,0,0
q4,2007.01503v1_introduction_0002,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Once the size of each layer and the choice of each activation function is made , one usually uses , so called , back propagation algorithm , adjusting the values of each weight vector according to some type of gradient descent rule . In other words , one is trying to solve an optimization problem , minimizing the `` difference norm '' C : = f -f d , where f = P r P r-1 ( . . . P 1 ) ( 3 ) and r ∈ N is the number of layers of the neural network . So apriori , we are making a choice of the function f of weights w 1 , . . . , w r . Note that the dimension of each vector w i is the size of the layer i . To simplify the problem we may always find the maximum , m , of the size layers , and assume that each w i ∈ R m . We are implicitly assuming that for each i = 1 , ... , r , P i ( 0 ) = 0. foot_0 2 Existence of function f and the toll of cost function C First thing to consider , given a labeled data set { ( x j , y j ) : j = 1 , . . .",0,0,0,0
q4,2008.08080v2_introduction_0001,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Introduction Survival analysis is the field of statistics concerned with the estimation of time-to-event distributions while accounting for censoring and truncation . mlr3proba introduces survival modelling to the mlr3 ( Lang et al. , 2019a ) ecosystem of machine learning packages . By utilising a probabilistic supervised learning ( Gressmann et al. , 2018 ) framework mlr3proba allows for multiple survival analysis predictions : predicting the time to an event , the probability of an event over time , and the relative risk of an event . mlr3proba includes an extensive collection of classical and machine learning models and many specialised survival measures . The R programming language ( R Core Team , 2020 ) provides extensive support for both survival analysis and machine learning via its core functionality and through open-source add-on packages available from CRAN and Bioconductor . mlr3proba leverages these packages by connecting a multitude of machine learning models and measures for survival analysis . mlr3proba currently supports simulation of survival data , classical survival models , prediction of survival distributions by machine learning , and support for high-dimensional data . Interfacing other packages in the mlr3 family provides functionality for optimisation , tuning , benchmarking , and more .",0,1,1,1
q4,2205.00210v1_introduction_0002,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Introduction Applications of machine learning ( ML ) technology have become vital in many innovative domains . At the same time , the vulnerability of ML has become evident , sometimes leading to catastrophic failures foot_0 . This entails that comprehensive testing of ML needs to be performed , to ensure the correctness and trustworthiness of ML-enabled systems . Software testing of ML systems is susceptible to a number of challenges compared to testing of traditional software systems . In this paper , by traditional systems we mean software systems not integrating ML , and by ML systems we mean software systems containing ML-trained components ( e.g self-driving cars , autonomous ships , or space exploration robots ) . As an example , one such challenge of testing ML systems stems from non-determinism intrinsic to ML . Traditional systems are typically pre-programmed and execute a set of rules , while ML systems reason in a probabilistic manner and exhibit non-deterministic behavior . This means that for constant test inputs and preconditions , an ML-trained software component can produce different outputs in consecutive runs . Researchers have tried using testing techniques from traditional software development ( Hutchison et al . 2018 ) , to deal with some of these challenges .",0,1,0,0
q4,2209.02057v3_abstract_0000,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","Machine Learning permeates many industries , which brings new sources of benefits for companies . However , within the life insurance industry , Machine Learning is not widely used in practice as over the past years statistical models have shown their efficiency for risk assessment . Insurers may thus face difficulties assessing the value of the artificial intelligence . Focusing on the modification of the life insurance industry over time highlights the stake of using Machine Learning for insurers and benefits that it can bring by unleashing data value . This paper reviews traditional actuarial methodologies for survival modeling and extends them with Machine Learning techniques . It points out differences with regular machine learning models and emphasizes the importance of specific implementations to face censored data with the Machine Learning models family . In complement to this article , a Python library has been developed .",0,0,0,0
q4,2209.02057v3_abstract_0001,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","It points out differences with regular machine learning models and emphasizes the importance of specific implementations to face censored data with the Machine Learning models family . In complement to this article , a Python library has been developed . Different open-source Machine Learning algorithms have been adjusted to adapt the specificities of life insurance data , namely censoring and truncation . Such models can be easily applied from this SCOR library to accurately model life insurance risks . This library is briefly presented in section 5 Why consider Machine Learning for risk modeling ? The life insurance industry became more complex as the modern life insurance industry now offers all kinds of products covering death events or other hazards related to the health condition of the insured . The main products are covering Mortality , Critical Illness , Disability , Medical Expenses , and Longevity .",0,0,0,0
q4,2209.02057v3_abstract_0002,"How do machine learning methods represent and use uncertainty (e.g., probabilistic modeling, Bayesian methods, predictive uncertainty)?","The main products are covering Mortality , Critical Illness , Disability , Medical Expenses , and Longevity . Besides , thanks to technological advances and data storage capacity improvements , information considered for risk assessment has increased a lot and is still increasing . For instance , nowadays , life insurance applicants are asked to share , besides their age , part of their medical history , financial situation , and their profession . In addition to this new information , thanks to the seniority of the industry , insurers may also have access to centuries of data accumulation to deepen their knowledge .",0,0,0,0
q5,1501.04309v1_abstract_0000,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"In this position paper , I first describe a new perspective on machine learning ( ML ) by four basic problems ( or levels ) , namely , `` What to learn ? `` , `` How to learn ? `` , `` What to evaluate ? `` , and `` What to adjust ? `` . The paper stresses more on the first level of `` What to learn ? `` , or `` Learning Target Selection '' . Towards this primary problem within the four levels , I briefly review the existing studies about the connection between information theoretical learning ( ITL [ 1 ] ) and machine learning . A theorem is given on the relation between the empirically-defined similarity measure and information measures . Finally , a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection .",0,0,0,0
q5,1505.06614v1_introduction_0002,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] . In this short paper , another machine learning technique is proposed to solve the record linkage problem : the multi-criteria classification method Electre Tri . It is the first time that multi-criteria machine learning technique is used to solve the record linkage problem . This application answers to one of `` many challenges in applying supervised machine learning to record linkage matching '' [ 10 ] , showing that the use of multi-criteria classification method Electre Tri to solve the record linkage problem provides good results in term of classification model performances . The importance of this application is in light of the increasing development of the use of administrative sources data . In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows .",0,0,0,0
q5,1505.06614v1_introduction_0003,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows . Section 2 describes an introduction to the Record Linkage problem ; then the next Section 3 describes the method Electre Tri , used to solved the Record Linkage and in the last Section 4 a preliminary experiment is conducted on simulated data . The paper closes with some final remarks and conclusions . 2 Linked Data : the Record Linkage Generally speaking , in integration of two data sets the objective is the detection of those records , in the different data sets , that belong to the same statistical unit . This action allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] .",0,0,0,0
q5,1505.06614v1_introduction_0006,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"Modern record linkage begins with the pioneering work of Newcombe et al . [ 14 ] , who introduced odds ratio of frequencies and the decision rules for delineating matches and nonmatches . In recent years , advances have yielded computer system that incorporate sophisticated ideas from computer sciences , statistics and operational research [ 16 ] . Then , Fellegi and Sunter [ 9 ] introduced a mathematical foundation for record linkage . Their theory demonstrated the optimality of the decision rules used by Newcombe and introduced a variety of ways of estimating crucial matching probabilities ( parameters ) directly from the files being matches . Formally , given two files A and B to be matched , each pair ( a , b ) ∈ Γ = A×B has to be classified into true match or true nonmatch . The odds ratios of probabilities is : R = P r ( γ ∈ Γ | M ) P r ( γ ∈ Γ | U ) where γ is an arbitrary agreement pattern in the comparison space Γ , M is the set of of true matches and U is the set of true nonmatches . Between these two sets , the intermediate set of the possible matches exists .",0,0,0,0
q5,1612.07640v1_introduction_0007,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values . Therefore , the application of one layer can learn a new representation of the input data and then , the stacking structure of multiple layers can enable MHMS to learn complex concepts out of simpler concepts that can be constructed from raw input . In addition , DL-based MHMS achieve an end-to-end system , which can automatically learn internal representations from raw input and predict targets . Compared to conventional data driven MHMS , DL-based MHMS do not require extensive human labor and knowledge for handcrafted feature design . All model parameters including feature module and pattern classification/regression module can be trained jointly . Therefore , DL-based models can be applied to addressing machine health monitoring in a very general way . For example , it is possible that the model trained for fault diagnosis problem can be used for prognosis by only replacing the top softmax layer with a linear regression layer . The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 .",0,0,0,0
q5,1612.07640v1_introduction_0008,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 . Deep learning models have several variants such as Auto-Dncoders [ 23 ] , Deep Belief Network [ 24 ] , Deep Boltzmann Machines [ 25 ] , Convolutional Neural Networks [ 26 ] and Recurrent Neural Networks [ 27 ] . During recent years , various researchers have demonstrated success of these deep learning models in the application of machine health monitoring . This paper attempts to provide a wide overview on these latest DL-based MHMS works that impact the state-of-the art technologies . Compared to these frontiers of deep learning including Computer Vision and Natural Language Processing , machine health monitoring community is catching up and has witnessed an emerging research . Therefore , the purpose of this survey article is to present researchers and engineers in the area of machine health monitoring system , a global view of this hot and active topic , and help them to acquire basic knowledge , quickly apply deep learning models and develop novel DL-based MHMS . The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring .",0,0,0,0
q5,1703.10121v1_introduction_0001,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"Introduction In 2007 , a paper named `` Top 10 algorithms in data mining '' identified and presented the top 10 most influential data mining algorithms within the research community [ 1 ] . The selection criteria were created by consolidating direct nominations from award winning researchers , the research community opinions and the number of citations in Google Scholar . The top 10 algorithms in that prior work are : C4.5 , k-means , support vector machine , Apriori , EM , PageRank , Ad-aBoost , kNN , naive Bayes and CART . In the decade that passed since then , machine learning has expanded , responding to incremental development of computational capabilities and substantial increase of problems in the commercial applications . This study reflects on the top 10 most popular fields of active research in machine learning , as they emerged from the quantitative analysis of leading journals and conferences . This work sees some topics in the broader sense including not only models but also concepts like data sets , features , optimization techniques and evaluation metrics . This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts .",0,0,0,0
q5,1706.05749v1_abstract_0000,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"This paper introduces Dex , a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems . We also present the novel continual learning method of incremental learning , where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment . We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments . We finally develop a saliency method for qualitative analysis of reinforcement learning , which shows the impact incremental learning has on network attention .",1,0,0,0
q5,1812.10422v1_introduction_0001,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"Introduction 1.Digital Agenda of the Federal Statistical Office of Germany On 10 October 2017 , the development of a Digital Agenda of the Federal Statistical Office of Germany ( Destatis ) has started ( Statistisches Bundesamt 2018 ) . One of many topics that were intensively discussed was Machine Learning . In a meeting at 13-15 November 2017 , the office and department heads of Destatis evaluated and prioritised 59 measures of the Digital Agenda according to their benefits and costs . A `` Proof of Concept Machine Learning '' was given high priority and classified as one of four lighthouse projects of the Digital Agenda . The content specification was `` Proof of Concept Machine Learning -Set up Proof of Concept for Machine Learning , e.g . in business statistics , to perform automatic categorization and improve analysis potential '' . The deadline for completion of the project was set for mid-2018 . foot_0",0,0,0,0
q5,1907.03010v1_introduction_0001,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"INTRODUCTION In the field of machine learning Time Series are very special data needed their specific processing and methods [ 1 ] [ 2 ] . On top of that , Financial data adds a big challenge due to their proportion of randomness and their non-stationary nature [ 4 ] [ 3 ] . There is a lot of research relative to the Financial Market forecast with Machine Learning [ 5 ] . However , many studies only cover one type of data scaling or labelling while the decisions made on this step can have a huge impact on the results . Not only in term of pure model performances metrics but in term of capabilities to really implement a profitable trading strategy based on the model . This study covers the following points : • Pre-processing and Stationarity • Pre-processing and preservation of useful prices relationships • Labelling for classifiers and regressors",0,0,1,0
q5,1907.07543v1_introduction_0005,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"Howard & Ruder ( 2018 ) note , and Devlin et . al . ( 2018 ) hypothesize that their respective approaches can be used with low quantities of data to give good results . However , in sources such as ( Howard & Ruder , 2018 ) , results on low-shot learning are presented relative to training deep models from scratch , but as mentioned in ( Goodfellow , Bengio , & Courville , 2016 ) , deep learning generally only achieves reasonable performance at about 5000 examples per class and is therefore not necessarily the best paradigm at these scales . This is shown quantitatively in ( Chen , Mckeever , & Delany , 2018 ) where , at scales of 2000+ labels per class , an SVM outperforms several deep learning approaches on text classification tasks . As such , we propose that to evaluate the low-shot learning benefits of deep transfer learning models , we should in fact look at performance against the strongest classical machine learning methods . However , we have yet to find a comprehensive quantitative study performing this analysis and show that low-shot transfer learning in NLP is actually the optimal approach when dealing with small quantities of data . In this paper we attempt to answer this question in the context of classification tasks .",1,0,0,0
q5,1907.07543v1_introduction_0006,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"However , we have yet to find a comprehensive quantitative study performing this analysis and show that low-shot transfer learning in NLP is actually the optimal approach when dealing with small quantities of data . In this paper we attempt to answer this question in the context of classification tasks . What is the best paradigm to use in the case where we have 100 -1000 labelled training examples per class -classical machine learning or deep transfer learning ? We seek to compare the best-in-class approaches from both deep transfer learning and classical machine learning by training a variety of models and evaluating by analysing intra-domain and inter-domain performance ( details in section 2 ) . The choice of 100 -1000 is motivated by the amount of data feasible for companies and researchers to tag in-house , as well as the scale of data occurring organically through other means . For example , in marketing these figures typically represent the base sizes of surveys that can be used as training data . The rest of this paper is laid out as follows . Section 2 details the datasets we use . Section 3 looks at the methodology used to evaluate the optimal paradigm . In section 4 we present the algorithms we use to test , along with related work influencing our choices in selecting those models .",1,0,0,0
q5,1907.07543v1_introduction_0007,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"The rest of this paper is laid out as follows . Section 2 details the datasets we use . Section 3 looks at the methodology used to evaluate the optimal paradigm . In section 4 we present the algorithms we use to test , along with related work influencing our choices in selecting those models . Section 5 details our experiments including choosing the optimal configuration of hyperparameters and preprocessing for each algorithm . In section 6 we present the results followed by our comments and conclusions . Finally , we highlight a few key points and considerations worthy of mention for the two paradigms in 7 .",0,0,1,0
q5,2006.15680v1_abstract_0000,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"As machine learning becomes more and more available to the general public , theoretical questions are turning into pressing practical issues . Possibly , one of the most relevant concerns is the assessment of our confidence in trusting machine learning predictions . In many real-world cases , it is of utmost importance to estimate the capabilities of a machine learning algorithm to generalize , i.e. , to provide accurate predictions on unseen data , depending on the characteristics of the target problem . In this work we perform a meta-analysis of 109 publicly-available classification data sets , modeling machine learning generalization as a function of a variety of data set characteristics , ranging from number of samples to intrinsic dimensionality , from class-wise feature skewness to F 1 evaluated on test samples falling outside the convex hull of the training set .",1,1,0,1
q5,2006.15680v1_abstract_0001,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"In this work we perform a meta-analysis of 109 publicly-available classification data sets , modeling machine learning generalization as a function of a variety of data set characteristics , ranging from number of samples to intrinsic dimensionality , from class-wise feature skewness to F 1 evaluated on test samples falling outside the convex hull of the training set . Experimental results demonstrate the relevance of using the concept of the convex hull of the training data in assessing machine learning generalization , by emphasizing the difference between interpolated and extrapolated predictions . Besides several predictable correlations , we observe unexpectedly weak associations between the generalization ability of machine learning models and all metrics related to dimensionality , thus challenging the common assumption that the curse of dimensionality might impair generalization in machine learning .",1,1,0,1
q5,2006.15680v1_introduction_0003,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ? Interestingly , this latter question can be further rephrased as : what are the characteristics of a data set that are well correlated with the possibility , or the impossibility , of obtaining ML models able to effectively extrapolate to unknown instances of the problem ? It is well known that ML algorithms are affected by the curse of dimensionality [ 11 ] , but ML practitioners also know that it could be possible to obtain reliable models even for high-dimensional data sets , and with a relatively small number of samples [ 12 ] . The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] .",1,1,0,1
q5,2006.15680v1_introduction_0004,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] . Taking inspiration from [ 14 ] , where the authors find links between data set characteristics and efficiency of feature selection techniques , we propose to empirically explore the relation between data-set characteristics and effectiveness of standard ML models , in order to obtain a general meta-model able to extrapolate . In order to answer the question , we analyzed 109 publicly available classification data sets from open-access , curated sources . We decided to focus on classification , as supervised ML represents a quite significant portion of real-world problems ; and , differently from regression , several sophisticated quality metrics have already been developed for this task [ 15 ] . During the analysis , we take into account characteristics such as number of features , number of classes , number of samples , and we look for correlations with quality metrics , such as accuracy of a ML model on training and test points .",1,1,1,1
q5,2006.15680v1_introduction_0005,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"During the analysis , we take into account characteristics such as number of features , number of classes , number of samples , and we look for correlations with quality metrics , such as accuracy of a ML model on training and test points . Extrapolation is assessed not just by alternatively dividing the data into training and test sets , but by analyzing whether data points fall inside or outside of the convex hull of the training data . After collecting the meta-data on the performance of a state-of-the-art classification algorithm on the data sets , the statistical analysis presents both predictable and surprising results , hinting at the fact that dimensionality might not be so cursed after all . Main contributions 1 . We ran a quantitative evaluations of ML models over 109 publicly available data sets . 2 . In Section 2 and Section 3 we provide a general overview on generalization in ML , and of the so-called curse of dimensionality .",1,0,1,1
q5,2007.01503v1_introduction_0004,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"One should also consider the norm • d of the approximation function f. It is well known that various classes of `` nice '' functions are dense in L p spaces . In particular , the class of functions f defined in ( 3 ) are dense with respect to convergence in measure and L p norm ( see [ 1 ] ) . This important fact implies that given any functions f and any ε > 0 , there is a function f s.t . f -f L p < ε . ( 7 ) The approximation function f is a function of weights vectors w i . The pursuit of such function f is a two part problem . The first part , defining the structure of the neural network , is done by a human . The methodology behind the choice of NN structures is at the stage of experimental science . The second part , weights optimization , is done by a computer , usually capable of trillions of operations per second . Needless to say that the effectiveness of latter part depends heavily on the former . In practice one usually does not use an approximation with respect to L p norm . Computationally one may only evaluate the function f at finitely many points and approximate it by f at such points , often with respect to the • l p norm .",0,0,0,0
q5,2007.01503v1_introduction_0005,What evaluation protocols and benchmark setups are commonly used to assess model performance?,"In practice one usually does not use an approximation with respect to L p norm . Computationally one may only evaluate the function f at finitely many points and approximate it by f at such points , often with respect to the • l p norm . Since for all 0 < p < q < ∞ , f ( y j ) -f ( y j ) l q f ( y j ) -f ( y j ) l p , ( 8 ) one can choose any p > 0 to obtain the approximation for all q p. foot_1 Here is an interesting question . Given f ∈ L p ( R k ) , does it follow that the sequence { f ( y j ) } ∞ j=1 ∈ l p for any choice y j ∈ R k ? One can easily show it is not the case . What about a sequence of randomly chosen points y j ∈ R k ? In this case the answer is affirmative . What about the converse statement ? Given a sequence { f ( y j ) } ∞ j=1 ∈ l p , does it follow that f ∈ L p ( R k ) ?",0,0,0,0
q6,1505.06614v1_introduction_0003,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows . Section 2 describes an introduction to the Record Linkage problem ; then the next Section 3 describes the method Electre Tri , used to solved the Record Linkage and in the last Section 4 a preliminary experiment is conducted on simulated data . The paper closes with some final remarks and conclusions . 2 Linked Data : the Record Linkage Generally speaking , in integration of two data sets the objective is the detection of those records , in the different data sets , that belong to the same statistical unit . This action allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] .",1,0,0,0
q6,1505.06614v1_introduction_0004,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","This action allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] . In the first situation , the definition of record linkage in [ 9 ] is more precise `` Record linkage is a solution to the problem of recognizing those records in two files which represent identical persons , objects , or events ( said to be matched ) '' The term record linkage originated in the public health area when files of individual patients were brought together using name , date-of-birth and other information [ 16 ] . One of the main motivations for the utilize of the record linkage method is the construction of the big data bases for answer to the new informative needs [ 8 ] . In order to better understand the problem , small practical example is now presented . Suppose the user wants to link two datasets of persons A and B , whose the variables Name , Address and Age are known .",1,0,0,0
q6,1505.06614v1_introduction_0005,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","In order to better understand the problem , small practical example is now presented . Suppose the user wants to link two datasets of persons A and B , whose the variables Name , Address and Age are known . Suppose that Table A contains the following values : Table A : Data in the first dataset Unit Name Address Age a1 John A Smith 16 Main Street 16 a2 Javier Martinez 49 E Applecross Road 33 a3 Gillian Jones 645 Reading Aev 22 Furthermore , suppose that Table B contains the following values : Table B : Data in the second dataset Unit Name Address Age b1 J H Smith 16 Main St 17 b2 Haveir Marteenez 49 Aplecross Raod 36 b3 Jilliam Brown 123 Norcross Blvd 43 The matching table A × B contains two units referring probably to the same persons , that the method should individuate as matches : 'John A Smith ' with ' J H Smith ' and 'Javier Martinez ' with 'Haveir Marteenez ' . Modern record linkage begins with the pioneering work of Newcombe et al . [ 14 ] , who introduced odds ratio of frequencies and the decision rules for delineating matches and nonmatches . In recent years , advances have yielded computer system that incorporate sophisticated ideas from computer sciences , statistics and operational research [ 16 ] .",1,0,1,1
q6,1703.10121v1_introduction_0001,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","Introduction In 2007 , a paper named `` Top 10 algorithms in data mining '' identified and presented the top 10 most influential data mining algorithms within the research community [ 1 ] . The selection criteria were created by consolidating direct nominations from award winning researchers , the research community opinions and the number of citations in Google Scholar . The top 10 algorithms in that prior work are : C4.5 , k-means , support vector machine , Apriori , EM , PageRank , Ad-aBoost , kNN , naive Bayes and CART . In the decade that passed since then , machine learning has expanded , responding to incremental development of computational capabilities and substantial increase of problems in the commercial applications . This study reflects on the top 10 most popular fields of active research in machine learning , as they emerged from the quantitative analysis of leading journals and conferences . This work sees some topics in the broader sense including not only models but also concepts like data sets , features , optimization techniques and evaluation metrics . This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts .",1,0,0,0
q6,1703.10121v1_introduction_0002,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?",This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts . This attempt aims at reducing bias and looking where the research community puts its focus on . The results of this study allow researchers to put their research into the global context of machine learning . This provides researchers with the opportunity to both conduct research in popular topics and identify topics that have not received sufficient attention in recent research . The rest of this paper is organized as follows . Section 2 describes the data sources and quantitative methodology . Section 3 presents and discusses the top 10 topics identified . Section 4 summarizes this work .,1,0,0,0
q6,1711.01431v1_abstract_0000,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","Machine learning is usually defined in behaviourist terms , where external validation is the primary mechanism of learning . In this paper , I argue for a more holistic interpretation in which finding more probable , efficient and abstract representations is as central to learning as performance . In other words , machine learning should be extended with strategies to reason over its own learning process , leading to so-called meta-cognitive machine learning . As such , the de facto definition of machine learning should be reformulated in these intrinsically multiobjective terms , taking into account not only the task performance but also internal learning objectives . To this end , we suggest a `` model entropy function '' to be defined that quantifies the efficiency of the internal learning processes . It is conjured that the minimization of this model entropy leads to concept formation .",0,0,0,0
q6,1807.10681v1_introduction_0001,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","Introduction Machine learning includes a practical side as well as a theoretical side . Practitioners solve real life problems , theoreticians study theory of learning . Practitioners need help answering the questions the life poses . Theoreticians give the answers . Unfortunately , they are , apparently , answering different questions , about some-what different subjects . For example , practitioners deal with limited data and deadlines , while the theory talks about what happens when training data increase indefinitely . There appears to be some disconnect here . More the over , the practitioners often can not formulate their questions exactly , because the language of the existing theory was developed by theoreticians to study different issues and different situations . Here I formulate the learning problem as it is encountered in practical applications , pose the related real life questions and show the answers to these questions which follow from PAC learnable theory . To do it , I express both questions of practitioners and the results of the PAC learnable theory in terms of problem solving .",0,0,0,0
q6,1907.07543v1_abstract_0000,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","Despite the recent success of deep transfer learning approaches in NLP , there is a lack of quantitative studies demonstrating the gains these models offer in low-shot text classification tasks over existing paradigms . Deep transfer learning approaches such as BERT and ULMFiT demonstrate that they can beat state-of-the-art results on larger datasets , however when one has only 100-1000 labelled examples per class , the choice of approach is less clear , with classical machine learning and deep transfer learning representing valid options . This paper compares the current best transfer learning approach with top classical machine learning approaches on a trinary sentiment classification task to assess the best paradigm . We find that BERT , representing the best of deep transfer learning , is the best performing approach , outperforming top classical machine learning algorithms by 9.7 % on average when trained with 100 examples per class , narrowing to 1.8 % at 1000 labels per class .",1,0,1,1
q6,1907.07543v1_introduction_0003,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","However , the fine-tuning task is usually much quicker to train as only a few parameters are added to the model , typically a single dense layer to the end of a multilayer LSTM or Transformer ( Vaswani et al. , 2017 ) . The model continues training either all , or part , of the network , but this is typically on much less data and for much less time , as only the task specific information is being learned and the general `` understanding '' of the language is transferred . These approaches have , on multiple occasions , broken the state-of-the-art records ( SO-TAs ) across the board on a range of NLP tasks and datasets ( Devlin , Chang , Lee , & Toutanova , 2018 ) ( Howard & Ruder , 2018 ) . However , all of these datasets are designed for deep learning : they are typically large enough that they warrant the use of deep learning ( 5000+ examples per class ) , without the necessity of transfer learning . It is our view that what transfer learning does , in these cases , is push the boundaries of performance . The prevalence of deep learning algorithms in surpassing SOTA records suggests quite clearly that , for the datasets assessed , deep learning surpasses the limits of classical machine learning algorithms in NLP tasks .",1,0,1,1
q6,1907.07543v1_introduction_0004,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","It is our view that what transfer learning does , in these cases , is push the boundaries of performance . The prevalence of deep learning algorithms in surpassing SOTA records suggests quite clearly that , for the datasets assessed , deep learning surpasses the limits of classical machine learning algorithms in NLP tasks . Low-shot transfer learning is another usecase for transfer learning in NLP , one of particular interest to companies working with real-world data . Low-shot transfer learning ( also referred to as `` few-shot '' ) is the use of transfer learning in training models where we have little training data available . This is important as many potential real-world applications of machine learning NLP do not have access to sufficiently large datasets to train deep learning algorithms , and obtaining such a dataset can often be too expensive or time consuming . Howard & Ruder ( 2018 ) note , and Devlin et . al . ( 2018 ) hypothesize that their respective approaches can be used with low quantities of data to give good results .",1,0,1,1
q6,1907.07543v1_introduction_0006,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","However , we have yet to find a comprehensive quantitative study performing this analysis and show that low-shot transfer learning in NLP is actually the optimal approach when dealing with small quantities of data . In this paper we attempt to answer this question in the context of classification tasks . What is the best paradigm to use in the case where we have 100 -1000 labelled training examples per class -classical machine learning or deep transfer learning ? We seek to compare the best-in-class approaches from both deep transfer learning and classical machine learning by training a variety of models and evaluating by analysing intra-domain and inter-domain performance ( details in section 2 ) . The choice of 100 -1000 is motivated by the amount of data feasible for companies and researchers to tag in-house , as well as the scale of data occurring organically through other means . For example , in marketing these figures typically represent the base sizes of surveys that can be used as training data . The rest of this paper is laid out as follows . Section 2 details the datasets we use . Section 3 looks at the methodology used to evaluate the optimal paradigm . In section 4 we present the algorithms we use to test , along with related work influencing our choices in selecting those models .",1,0,1,1
q6,1907.07543v1_introduction_0007,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","The rest of this paper is laid out as follows . Section 2 details the datasets we use . Section 3 looks at the methodology used to evaluate the optimal paradigm . In section 4 we present the algorithms we use to test , along with related work influencing our choices in selecting those models . Section 5 details our experiments including choosing the optimal configuration of hyperparameters and preprocessing for each algorithm . In section 6 we present the results followed by our comments and conclusions . Finally , we highlight a few key points and considerations worthy of mention for the two paradigms in 7 .",0,0,0,0
q6,2006.15680v1_abstract_0000,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","As machine learning becomes more and more available to the general public , theoretical questions are turning into pressing practical issues . Possibly , one of the most relevant concerns is the assessment of our confidence in trusting machine learning predictions . In many real-world cases , it is of utmost importance to estimate the capabilities of a machine learning algorithm to generalize , i.e. , to provide accurate predictions on unseen data , depending on the characteristics of the target problem . In this work we perform a meta-analysis of 109 publicly-available classification data sets , modeling machine learning generalization as a function of a variety of data set characteristics , ranging from number of samples to intrinsic dimensionality , from class-wise feature skewness to F 1 evaluated on test samples falling outside the convex hull of the training set .",1,0,1,1
q6,2006.15680v1_abstract_0001,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","In this work we perform a meta-analysis of 109 publicly-available classification data sets , modeling machine learning generalization as a function of a variety of data set characteristics , ranging from number of samples to intrinsic dimensionality , from class-wise feature skewness to F 1 evaluated on test samples falling outside the convex hull of the training set . Experimental results demonstrate the relevance of using the concept of the convex hull of the training data in assessing machine learning generalization , by emphasizing the difference between interpolated and extrapolated predictions . Besides several predictable correlations , we observe unexpectedly weak associations between the generalization ability of machine learning models and all metrics related to dimensionality , thus challenging the common assumption that the curse of dimensionality might impair generalization in machine learning .",1,0,1,1
q6,2006.15680v1_introduction_0002,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","Introduction The term machine learning ( ML ) traditionally includes algorithms that are able to improve their performance on a specific task over time , given an increasing amount of relevant data [ 1 ] . In recent years , this field of research is enjoying a growing popularity , driven by the breakthrough of Deep Learning [ 2 ] and an impressive track record of success stories in different fields , ranging from natural language processing [ 3 ] to autonomous vehicles [ 4 ] , image classification [ 5 ] human-competitive performance in boardgames [ 6 ] . An interesting online collection about various uses of ML has been compiled by the journal Nature is late 2018 1 , although the fast pace the field is progressing made it to appear outdated after few quarters . As out-of-the-box ML solutions are becoming increasingly available to both researchers and the general public [ 7 , 8 , 9 , 10 ] , theoretical questions are suddenly turning into practical issues . Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ?",1,0,1,1
q6,2006.15680v1_introduction_0003,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ? Interestingly , this latter question can be further rephrased as : what are the characteristics of a data set that are well correlated with the possibility , or the impossibility , of obtaining ML models able to effectively extrapolate to unknown instances of the problem ? It is well known that ML algorithms are affected by the curse of dimensionality [ 11 ] , but ML practitioners also know that it could be possible to obtain reliable models even for high-dimensional data sets , and with a relatively small number of samples [ 12 ] . The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] .",1,0,0,0
q6,2006.15680v1_introduction_0004,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] . Taking inspiration from [ 14 ] , where the authors find links between data set characteristics and efficiency of feature selection techniques , we propose to empirically explore the relation between data-set characteristics and effectiveness of standard ML models , in order to obtain a general meta-model able to extrapolate . In order to answer the question , we analyzed 109 publicly available classification data sets from open-access , curated sources . We decided to focus on classification , as supervised ML represents a quite significant portion of real-world problems ; and , differently from regression , several sophisticated quality metrics have already been developed for this task [ 15 ] . During the analysis , we take into account characteristics such as number of features , number of classes , number of samples , and we look for correlations with quality metrics , such as accuracy of a ML model on training and test points .",1,0,1,1
q6,2006.15680v1_introduction_0005,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","During the analysis , we take into account characteristics such as number of features , number of classes , number of samples , and we look for correlations with quality metrics , such as accuracy of a ML model on training and test points . Extrapolation is assessed not just by alternatively dividing the data into training and test sets , but by analyzing whether data points fall inside or outside of the convex hull of the training data . After collecting the meta-data on the performance of a state-of-the-art classification algorithm on the data sets , the statistical analysis presents both predictable and surprising results , hinting at the fact that dimensionality might not be so cursed after all . Main contributions 1 . We ran a quantitative evaluations of ML models over 109 publicly available data sets . 2 . In Section 2 and Section 3 we provide a general overview on generalization in ML , and of the so-called curse of dimensionality .",1,0,0,0
q6,2007.01503v1_introduction_0002,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?","Once the size of each layer and the choice of each activation function is made , one usually uses , so called , back propagation algorithm , adjusting the values of each weight vector according to some type of gradient descent rule . In other words , one is trying to solve an optimization problem , minimizing the `` difference norm '' C : = f -f d , where f = P r P r-1 ( . . . P 1 ) ( 3 ) and r ∈ N is the number of layers of the neural network . So apriori , we are making a choice of the function f of weights w 1 , . . . , w r . Note that the dimension of each vector w i is the size of the layer i . To simplify the problem we may always find the maximum , m , of the size layers , and assume that each w i ∈ R m . We are implicitly assuming that for each i = 1 , ... , r , P i ( 0 ) = 0. foot_0 2 Existence of function f and the toll of cost function C First thing to consider , given a labeled data set { ( x j , y j ) : j = 1 , . . .",0,0,0,0
q6,2007.01503v1_introduction_0003,"What types of datasets or data sources are used for experiments, and what are typical reasons for choosing them?",". . , N } , if there is a representation function f such that f ( x j ) = y j for all j = 1 , . . . , N . This important step is often overlooked in practice . In theory , if there is x j = x k such that y j = y k ( 4 ) then no such function exist . In other words f is a function of more variables than provided in the data set and the idea approximation by f is meaningless . One may always assume some measurement error ε > 0 ( noise ) of the data set and consider instead a weaker condition x j = x k =⇒ y j -y k l 2 < ε ( 5 ) necessary for existence of a function f. In such case one may look at the average values of duplicate points f ave = x j = x k y k x j = x k 1 ( 6 ) and choose to approximate f ave instead of f. There is no guarantee that a good approximation f of function f ave is a good approximation of f itself . One should also consider the norm • d of the approximation function f. It is well known that various classes of `` nice '' functions are dense in L p spaces .",0,0,0,0
q7,1501.04309v1_abstract_0000,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","In this position paper , I first describe a new perspective on machine learning ( ML ) by four basic problems ( or levels ) , namely , `` What to learn ? `` , `` How to learn ? `` , `` What to evaluate ? `` , and `` What to adjust ? `` . The paper stresses more on the first level of `` What to learn ? `` , or `` Learning Target Selection '' . Towards this primary problem within the four levels , I briefly review the existing studies about the connection between information theoretical learning ( ITL [ 1 ] ) and machine learning . A theorem is given on the relation between the empirically-defined similarity measure and information measures . Finally , a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection .",1,0,0,0
q7,1501.04309v1_introduction_0002,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","It is the author 's belief that `` mathematical-principle-based machine '' might be more important and critical than `` brain-inspired machine '' in the study of machine learning . The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning . In what follows I will present four basic problems ( or levels ) in machine learning . The study on information theoretical learning is briefly reviewed . A theorem between the empirically-defined similarity measures and information measures are given . Based on the existing investigations , a conjecture is proposed in this paper .",0,0,0,0
q7,1504.03874v1_introduction_0001,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","Introduction In an age of user generated web-contents and of portable devices with embedded computer vision capabilities , machine learning ( ML ) and big data mining questions are fundamental . As a result , these questions naturally penetrate neighboring research fields , including belief function theory ( BFT ) , so that it is now usual to attend a `` Classification '' session [ 26 ] or a `` Machine Learning '' session [ 16 ] in a conference devoted to belief functions . However , it is hard to accept that among the various proposed approaches based on BF , very few have become state-of-the-art ML methods , the knowledge of which has spread beyond the BF community . Without any doubt , this can be partly explained by the relative size of the scientific communities under consideration : although quickly growing , the BF one is relatively small with respect to that of statistics , Bayesian networks , neural networks , etc . However , this reason alone is not sufficient : There are indeed other topics , such as for instance , information fusion , where BF-based methods are now as well recognized as are methods based on more classical formalisms , such as probabilities , or ontologies .",0,0,0,0
q7,1505.06614v1_abstract_0000,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","In this short paper , the Electre Tri-Machine Learning Method , generally used to solve ordinal classification problems , is proposed for solving the Record Linkage problem . Preliminary experimental results show that , using the Electre Tri method , high accuracy can be achieved and more than 99 % of the matches and nonmatches were correctly identified by the procedure .",0,0,0,0
q7,1505.06614v1_introduction_0001,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","Introduction Machine Learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to `` learn data '' . More precisely , `` learn '' is here intended as the possibility to automatically recognize complex patterns and make `` intelligent '' decisions , based on information data . Hence , machine learning is closely related to fields such as statistics , probability theory , data mining , pattern recognition , artificial intelligence , adaptive control and theoretical computer science . Machine learning algorithms can be classified in the following types : • supervised learning algorithms : a function/classifier is generated , that maps outputs on the training inputs , based on labeled examples inputoutput ; • unsupervised learning algorithms : patterns in the input are recognized , the examples have no labels ; • semi-supervised learning algorithms : supervised and unsupervised learning information is combined ; • reinforcement learning : actions from observation of the world are generated . Every action has some impact in the environment and the environment provides feedbacks that are translated into a score that guide the learning process . The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] .",0,0,0,0
q7,1505.06614v1_introduction_0002,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] . In this short paper , another machine learning technique is proposed to solve the record linkage problem : the multi-criteria classification method Electre Tri . It is the first time that multi-criteria machine learning technique is used to solve the record linkage problem . This application answers to one of `` many challenges in applying supervised machine learning to record linkage matching '' [ 10 ] , showing that the use of multi-criteria classification method Electre Tri to solve the record linkage problem provides good results in term of classification model performances . The importance of this application is in light of the increasing development of the use of administrative sources data . In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows .",0,0,0,0
q7,1505.06614v1_introduction_0003,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows . Section 2 describes an introduction to the Record Linkage problem ; then the next Section 3 describes the method Electre Tri , used to solved the Record Linkage and in the last Section 4 a preliminary experiment is conducted on simulated data . The paper closes with some final remarks and conclusions . 2 Linked Data : the Record Linkage Generally speaking , in integration of two data sets the objective is the detection of those records , in the different data sets , that belong to the same statistical unit . This action allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] .",0,0,0,0
q7,1505.06614v1_introduction_0006,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","Modern record linkage begins with the pioneering work of Newcombe et al . [ 14 ] , who introduced odds ratio of frequencies and the decision rules for delineating matches and nonmatches . In recent years , advances have yielded computer system that incorporate sophisticated ideas from computer sciences , statistics and operational research [ 16 ] . Then , Fellegi and Sunter [ 9 ] introduced a mathematical foundation for record linkage . Their theory demonstrated the optimality of the decision rules used by Newcombe and introduced a variety of ways of estimating crucial matching probabilities ( parameters ) directly from the files being matches . Formally , given two files A and B to be matched , each pair ( a , b ) ∈ Γ = A×B has to be classified into true match or true nonmatch . The odds ratios of probabilities is : R = P r ( γ ∈ Γ | M ) P r ( γ ∈ Γ | U ) where γ is an arbitrary agreement pattern in the comparison space Γ , M is the set of of true matches and U is the set of true nonmatches . Between these two sets , the intermediate set of the possible matches exists .",0,0,0,0
q7,1612.07640v1_introduction_0005,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","The study in [ 3 ] predicts that 30 billion devices will be connected by 2020 . Therefore , the huge amount of data is able to offset the complexity increase behind deep learning and improve its generalization capability . * Advanced Deep Learning Research : the first breakthrough of deep learning is the pre-training method in an unsupervised way [ 15 ] , where Hinton proposed to pre-train one layer at a time via restricted Boltzmann machine ( RBM ) and then fine-tune using backpropagation . This has been proven to be effective to train multilayer neural networks . Considering the capability of deep learning to address largescale data and learn high-level representation , deep learning can be a powerful and effective solution for machine health monitoring systems ( MHMS ) . Conventional data-driven MHMS usually consists of the following key parts : handcrafted feature design , feature extraction/selection and model training . The right set of features are designed , and then provided to some shallow machine learning algorithms including arXiv:1612.07640v1 [ cs.LG ] 16 Dec 2016 Support Vector Machines ( SVM ) , Naive Bayes ( NB ) , logistic regression [ 16 ] , [ 17 ] , [ 18 ] . It is shown that the representation defines the upper-bound performances of machine learning algorithms [ 19 ] .",0,0,0,0
q7,1612.07640v1_introduction_0006,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","It is shown that the representation defines the upper-bound performances of machine learning algorithms [ 19 ] . However , it is difficult to know and determine what kind of good features should be designed . To alleviate this issue , feature extraction/selection methods , which can be regarded as a kind of information fusion , are performed between hand-crafted feature design and classification/regression models [ 20 ] , [ 21 ] , [ 22 ] . However , manually designing features for a complex domain requires a great deal of human labor and can not be updated on-line . At the same time , feature extraction/selection is another tricky problem , which involves prior selection of hyperparameters such as latent dimension . At last , the above three modules including feature design , feature extraction/selection and model training can not be jointly optimized which may hinder the final performance of the whole system . Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values .",0,0,0,0
q7,1703.10121v1_introduction_0002,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?",This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts . This attempt aims at reducing bias and looking where the research community puts its focus on . The results of this study allow researchers to put their research into the global context of machine learning . This provides researchers with the opportunity to both conduct research in popular topics and identify topics that have not received sufficient attention in recent research . The rest of this paper is organized as follows . Section 2 describes the data sources and quantitative methodology . Section 3 presents and discusses the top 10 topics identified . Section 4 summarizes this work .,0,0,0,0
q7,1706.05749v1_abstract_0000,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","This paper introduces Dex , a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems . We also present the novel continual learning method of incremental learning , where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment . We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments . We finally develop a saliency method for qualitative analysis of reinforcement learning , which shows the impact incremental learning has on network attention .",0,0,0,0
q7,1711.01431v1_introduction_0003,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","One possible analogy to better understand the above statements can be found in software engineering . When considering code that performs a specific task , we do not care only about its functionality , but also about its execution speed/efficiency and other so-called `` non-functional requirements '' . Furthermore , a carefully modularized design probably reflects more understanding than an endless enumeration of IF-ELSE clauses . In other words , finding a more efficient and structured way to represent/reproduce information and to perform a learning task , is as central to machine learning as the reproduction of results . Different to humans , of course , machines are measurable . This provides us with a unique opportunity to study the nature of learning in principle , at the same time improving Machine Intelligence . We are not claiming that model complexity/efficiency has not been subject to past research efforts . On the contrary , many techniques and design principles have attempted to improve exactly these properties -like Occam 's razor , Bayesian structure learning , pruning , the use of prototypes to compact information , regularization as a strategy to reduce energy , weight sharing in RNNs or CNNs to decrease model complexity , etc . Indeed , the whole evolution of Deep Learning can be seen as one specific approach in the quest to find models that are more structured ( i.e .",0,0,0,0
q7,1711.01431v1_introduction_0004,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","Indeed , the whole evolution of Deep Learning can be seen as one specific approach in the quest to find models that are more structured ( i.e . have a lower entropy ) , by organizing and training them in a layer-wise fashion [ Bengio , 2009 ] . The focus has been mainly on training algorithms and designing model architectures that are adapted to these kinds of `` deep '' structures [ Deng and Yu , 2014 ] . Similar to efforts in multiobjective machine learning , these techniques are considered as a means to improve ( externally measured ) performance rather than a goal in itself [ Jin and Sendhoff , 2008 ] . We , however , do believe that minimizing the model 's structural complexity and optimizing its efficiency of representation , is not only a means to improve ( externally validated ) performance , but a central pillar to machine intelligence that leads to concept formulation and should be made explicit . In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows .",0,0,0,0
q7,1711.01431v1_introduction_0005,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows . The theoretical ideas are laid out and the case for a new operational definition of machine learning is made . We put forward the conjecture that the optimization of model entropy , leads to concept formation . Last , conclusions and further steps to operationalize these concepts are formulated . 2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a ( non- ) parametric model with respect to some performance measure . This view is clearly reflected in the de facto definition of machine learning by Mitchell [ Mitchell , 1997 ] : `` A computer program is said to learn from an experience X with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experimental data D '' . Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures .",0,0,0,0
q7,1711.01431v1_introduction_0006,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures . Nevertheless , there is a general consensus that the learning of `` higherorder '' concepts is problematic , and that the solution to this issue is somehow connected to deep architectures that create ever higher forms of abstraction . Experimental research as well as neurological evidence on the organization of the brain , supports this finding [ Bianchini and Scarselli , 2014 ] . The limitation of architecture complexity is preferred , primarily because their behaviour could be understood and the training of more complex or adaptive architectures leads to a explosion of complexity . That was until recently . The recent advanced in so-called `` Deep Learning '' , have focused on training algorithms that are adapted to new kinds of deep architectures [ Deng and Yu , 2014 ] , and heuristic strategies to attain specific structural properties like sparse coding that lead to higher forms of abstractions . With the exception of studies on `` interpretability '' [ Jin and Sendhoff , 2008 ] , structural properties are mainly considered a by-product , a ( desirable ) side effect of the applied training mechanisms . Though the organization and complexity of model topologies is acknowledged to be crucial , current approaches are mainly limited to analysing the data space , i.e .",0,0,0,0
q7,1711.01431v1_introduction_0007,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","Though the organization and complexity of model topologies is acknowledged to be crucial , current approaches are mainly limited to analysing the data space , i.e . the implemented regression functions or decision boundaries [ Bianchini and Scarselli , 2014 ] . There is a problem with this approach . Consider an neural network algorithm that needs to learn a simple concept like an `` XOR '' function depicted in Fig . 1 . An infinite number of neural networks with very similar or identical decision boundaries can be constructed -of which two are shown in Fig . 2 . From an external point of view , there is no way to discriminate between these two models : describing the difference between these two models can only occur in terms of the model internals . Of course the weight space , which represents the model of a neural network , is related to the data space , as it performs calculations on the data . In other words : Data representation and model computation should be considered as two sides of the same coin . As a result the structural properties of both the model and data space are key to the modelling of higher abstractions . Sparse coding is a perfect example of this .",0,0,0,0
q7,1807.10681v1_abstract_0000,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","Two different views on machine learning problem : Applied learning ( machine learning with business applications ) and Agnostic PAC learning are formalized and compared here . I show that , under some conditions , the theory of PAC Learnable provides a way to solve the Applied learning problem . However , the theory requires to have the training sets so large , that it would make the learning practically useless . I suggest to shed some theoretical misconceptions about learning to make the theory more aligned with the needs and experience of practitioners .",1,0,0,0
q7,1807.10681v1_introduction_0001,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","Introduction Machine learning includes a practical side as well as a theoretical side . Practitioners solve real life problems , theoreticians study theory of learning . Practitioners need help answering the questions the life poses . Theoreticians give the answers . Unfortunately , they are , apparently , answering different questions , about some-what different subjects . For example , practitioners deal with limited data and deadlines , while the theory talks about what happens when training data increase indefinitely . There appears to be some disconnect here . More the over , the practitioners often can not formulate their questions exactly , because the language of the existing theory was developed by theoreticians to study different issues and different situations . Here I formulate the learning problem as it is encountered in practical applications , pose the related real life questions and show the answers to these questions which follow from PAC learnable theory . To do it , I express both questions of practitioners and the results of the PAC learnable theory in terms of problem solving .",1,0,0,0
q7,1811.04422v1_abstract_0000,"How does theoretical analysis (e.g., proofs or bounds) relate to empirical evidence (e.g., experiments or ablations)?","I describe an optimal control view of adversarial machine learning , where the dynamical system is the machine learner , the input are adversarial actions , and the control costs are defined by the adversary 's goals to do harm and be hard to detect . This view encompasses many types of adversarial machine learning , including test-item attacks , training-data poisoning , and adversarial reward shaping . The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning .",1,0,0,0
q8,1501.04309v1_introduction_0001,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Introduction Machine learning is the study and construction of systems that can learn from data . The systems are called learning machines . When Big Data emerges increasingly , more learning machines are developed and applied in different domains . However , the ultimate goal of machine learning study is insight , not machine itself . By the term insight I mean learning mechanisms in descriptions of mathematical principles . In a loose sense , learning mechanisms can be regarded as the natural entity . As the `` Tao ( 道 ) '' reflects the most fundamental of the universe by Lao Tzu ( 老子 ) , Einstein suggests that we should pursue the simplest mathematical interpretations to the nature . Although learning mechanisms are related to the subjects of psychology , cognitive and brain science , this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms . Up to now , we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles . It is the author 's belief that `` mathematical-principle-based machine '' might be more important and critical than `` brain-inspired machine '' in the study of machine learning . The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning .",0,0,1,0
q8,1504.03874v1_introduction_0001,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Introduction In an age of user generated web-contents and of portable devices with embedded computer vision capabilities , machine learning ( ML ) and big data mining questions are fundamental . As a result , these questions naturally penetrate neighboring research fields , including belief function theory ( BFT ) , so that it is now usual to attend a `` Classification '' session [ 26 ] or a `` Machine Learning '' session [ 16 ] in a conference devoted to belief functions . However , it is hard to accept that among the various proposed approaches based on BF , very few have become state-of-the-art ML methods , the knowledge of which has spread beyond the BF community . Without any doubt , this can be partly explained by the relative size of the scientific communities under consideration : although quickly growing , the BF one is relatively small with respect to that of statistics , Bayesian networks , neural networks , etc . However , this reason alone is not sufficient : There are indeed other topics , such as for instance , information fusion , where BF-based methods are now as well recognized as are methods based on more classical formalisms , such as probabilities , or ontologies .",0,1,0,0
q8,1505.06614v1_introduction_0001,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Introduction Machine Learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to `` learn data '' . More precisely , `` learn '' is here intended as the possibility to automatically recognize complex patterns and make `` intelligent '' decisions , based on information data . Hence , machine learning is closely related to fields such as statistics , probability theory , data mining , pattern recognition , artificial intelligence , adaptive control and theoretical computer science . Machine learning algorithms can be classified in the following types : • supervised learning algorithms : a function/classifier is generated , that maps outputs on the training inputs , based on labeled examples inputoutput ; • unsupervised learning algorithms : patterns in the input are recognized , the examples have no labels ; • semi-supervised learning algorithms : supervised and unsupervised learning information is combined ; • reinforcement learning : actions from observation of the world are generated . Every action has some impact in the environment and the environment provides feedbacks that are translated into a score that guide the learning process . The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] .",0,1,1,1
q8,1505.06614v1_introduction_0002,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] . In this short paper , another machine learning technique is proposed to solve the record linkage problem : the multi-criteria classification method Electre Tri . It is the first time that multi-criteria machine learning technique is used to solve the record linkage problem . This application answers to one of `` many challenges in applying supervised machine learning to record linkage matching '' [ 10 ] , showing that the use of multi-criteria classification method Electre Tri to solve the record linkage problem provides good results in term of classification model performances . The importance of this application is in light of the increasing development of the use of administrative sources data . In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows .",0,0,1,0
q8,1505.06614v1_introduction_0005,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","In order to better understand the problem , small practical example is now presented . Suppose the user wants to link two datasets of persons A and B , whose the variables Name , Address and Age are known . Suppose that Table A contains the following values : Table A : Data in the first dataset Unit Name Address Age a1 John A Smith 16 Main Street 16 a2 Javier Martinez 49 E Applecross Road 33 a3 Gillian Jones 645 Reading Aev 22 Furthermore , suppose that Table B contains the following values : Table B : Data in the second dataset Unit Name Address Age b1 J H Smith 16 Main St 17 b2 Haveir Marteenez 49 Aplecross Raod 36 b3 Jilliam Brown 123 Norcross Blvd 43 The matching table A × B contains two units referring probably to the same persons , that the method should individuate as matches : 'John A Smith ' with ' J H Smith ' and 'Javier Martinez ' with 'Haveir Marteenez ' . Modern record linkage begins with the pioneering work of Newcombe et al . [ 14 ] , who introduced odds ratio of frequencies and the decision rules for delineating matches and nonmatches . In recent years , advances have yielded computer system that incorporate sophisticated ideas from computer sciences , statistics and operational research [ 16 ] .",0,0,0,0
q8,1505.06614v1_introduction_0006,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Modern record linkage begins with the pioneering work of Newcombe et al . [ 14 ] , who introduced odds ratio of frequencies and the decision rules for delineating matches and nonmatches . In recent years , advances have yielded computer system that incorporate sophisticated ideas from computer sciences , statistics and operational research [ 16 ] . Then , Fellegi and Sunter [ 9 ] introduced a mathematical foundation for record linkage . Their theory demonstrated the optimality of the decision rules used by Newcombe and introduced a variety of ways of estimating crucial matching probabilities ( parameters ) directly from the files being matches . Formally , given two files A and B to be matched , each pair ( a , b ) ∈ Γ = A×B has to be classified into true match or true nonmatch . The odds ratios of probabilities is : R = P r ( γ ∈ Γ | M ) P r ( γ ∈ Γ | U ) where γ is an arbitrary agreement pattern in the comparison space Γ , M is the set of of true matches and U is the set of true nonmatches . Between these two sets , the intermediate set of the possible matches exists .",0,0,0,0
q8,1612.04858v1_abstract_0000,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","The engineering of machine learning systems is still a nascent field ; relying on a seemingly daunting collection of quickly evolving tools and best practices . It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques . We outline four example machine learning problems that can be solved using open source machine learning libraries , and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications .",0,1,1,1
q8,1612.04858v1_introduction_0001,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Introduction Recently , there has been interest in applying Bayesian black-box optimization strategies to better conduct optimization over hyperparameter configurations of machine learning models and systems [ 19 ] [ 21 ] [ 11 ] . Most of these techniques require that the objective be a scalar value depending on the hyperparamter configuration x. x opt = arg max x∈X f ( x ) A more detailed introduction to Bayesian optimization and related techniques is provided in [ 8 ] . The focus of this guidebook is on demonstrating several example problems where Bayesian optimization provides a noted benefit . Our hope is to clearly show how Bayesian optimization can assist in better designing and optimizing real-world machine learning systems . All of the examples in this guidebook have corresponding code available on SigOpt 's example github repo .",0,1,0,0
q8,1612.07640v1_abstract_0001,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Meanwhile , deep learning provides useful tools for processing and analyzing these big machinery data . The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring . After the brief introduction of deep learning techniques , the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects : Autoencoder ( AE ) and its variants , Restricted Boltzmann Machines and its variants including Deep Belief Network ( DBN ) and Deep Boltzmann Machines ( DBM ) , Convolutional Neural Networks ( CNN ) and Recurrent Neural Networks ( RNN ) . Finally , some new trends of DL-based machine health monitoring methods are discussed .",0,0,0,0
q8,1612.07640v1_introduction_0002,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","I . INTRODUCTION I NDUSTRIAL Internet of Things ( IoT ) and data-driven techniques have been revolutionizing manufacturing by enabling computer networks to gather the huge amount of data from connected machines and turn the big machinery data into actionable information [ 1 ] , [ 2 ] , [ 3 ] . As a key component in modern manufacturing system , machine health monitoring has fully embraced the big data revolution . Compared to top-down modeling provided by the traditional physics-based models [ 4 ] , [ 5 ] , [ 6 ] , data-driven machine health monitoring systems offer a new paradigm of bottom-up solution for detection of faults after the occurrence of certain failures ( diagnosis ) and predictions of the future working conditions and the remaining useful life ( prognosis ) [ 1 ] , [ 7 ] . As we know , the complexity and noisy working condition hinder the construction of physical models . And most of these physicsbased models are unable to be updated with on-line measured data , which limits their effectiveness and flexibility . On the other hand , with significant development of sensors , sensor networks and computing systems , data-driven machine health monitoring models have become more and more attractive . To extract useful knowledge and make appropriate decisions from big data , machine learning techniques have been regarded as a powerful solution .",0,1,1,1
q8,1612.07640v1_introduction_0004,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","In fact , deep learning is not a new idea , which even dates back to the 1940s [ 12 ] , [ 13 ] . The popularity of deep learning today can be contributed to the following points : * Increasing Computing Power : the advent of graphics processor unit ( GPU ) , the lowered cost of hardware , the better software infrastructure and the faster network connectivity all reduce the required running time of deep learning algorithms significantly . For example , as reported in [ 14 ] , the time required to learn a four-layer DBN with 100 million free parameters can be reduced from several weeks to around a single day . * Increasing Data Size : there is no doubt that the era of Big Data is coming . Our activities are almost all digitized , recorded by computers and sensors , connected to Internet , and stored in cloud . As pointed out in [ 1 ] that in industry-related applications such as industrial informatics and electronics , almost 1000 exabytes are generated per year and a 20-fold increase can be expected in the next ten years . The study in [ 3 ] predicts that 30 billion devices will be connected by 2020 . Therefore , the huge amount of data is able to offset the complexity increase behind deep learning and improve its generalization capability .",0,0,0,0
q8,1612.07640v1_introduction_0007,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values . Therefore , the application of one layer can learn a new representation of the input data and then , the stacking structure of multiple layers can enable MHMS to learn complex concepts out of simpler concepts that can be constructed from raw input . In addition , DL-based MHMS achieve an end-to-end system , which can automatically learn internal representations from raw input and predict targets . Compared to conventional data driven MHMS , DL-based MHMS do not require extensive human labor and knowledge for handcrafted feature design . All model parameters including feature module and pattern classification/regression module can be trained jointly . Therefore , DL-based models can be applied to addressing machine health monitoring in a very general way . For example , it is possible that the model trained for fault diagnosis problem can be used for prognosis by only replacing the top softmax layer with a linear regression layer . The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 .",0,0,1,0
q8,1612.07640v1_introduction_0008,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 . Deep learning models have several variants such as Auto-Dncoders [ 23 ] , Deep Belief Network [ 24 ] , Deep Boltzmann Machines [ 25 ] , Convolutional Neural Networks [ 26 ] and Recurrent Neural Networks [ 27 ] . During recent years , various researchers have demonstrated success of these deep learning models in the application of machine health monitoring . This paper attempts to provide a wide overview on these latest DL-based MHMS works that impact the state-of-the art technologies . Compared to these frontiers of deep learning including Computer Vision and Natural Language Processing , machine health monitoring community is catching up and has witnessed an emerging research . Therefore , the purpose of this survey article is to present researchers and engineers in the area of machine health monitoring system , a global view of this hot and active topic , and help them to acquire basic knowledge , quickly apply deep learning models and develop novel DL-based MHMS . The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring .",0,0,1,0
q8,1612.07640v1_introduction_0009,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring . Finally , section IV gives a brief summary of the recent achievements of DL-based MHMS and discusses some potential trends of deep learning in machine health monitoring . II . DEEP LEARNING Originated from artificial neural network , deep learning is a branch of machine learning which is featured by multiple nonlinear processing layers . Deep learning aims to learn hierarchy representations of data . Up to date , there are various deep learning architectures and this research topic is fast-growing , in which new models are being developed even per week . And the community is quite open and there are a number of deep learning tutorials and books of good-quality [ 28 ] , [ 29 ] . Therefore , only a brief introduction to some major deep learning techniques that have been applied in machine health monitoring is given . In the following , four deep architectures including Auto-encoders , RBM , CNN and RNN and their corresponding variants are reviewed , respectively .",0,0,1,0
q8,1703.10121v1_abstract_0000,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Which topics of machine learning are most commonly addressed in research ? This question was initially answered in 2007 by doing a qualitative survey among distinguished researchers . In our study , we revisit this question from a quantitative perspective . Concretely , we collect 54K abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences . We then use machine learning in order to determine the top 10 topics in machine learning . We not only include models , but provide a holistic view across optimization , data , features , etc . This quantitative approach allows reducing the bias of surveys . It reveals new and up-to-date insights into what the 10 most prolific topics in machine learning research are . This allows researchers to identify popular topics as well as new and rising topics for their research .",0,1,0,0
q8,1703.10121v1_introduction_0001,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Introduction In 2007 , a paper named `` Top 10 algorithms in data mining '' identified and presented the top 10 most influential data mining algorithms within the research community [ 1 ] . The selection criteria were created by consolidating direct nominations from award winning researchers , the research community opinions and the number of citations in Google Scholar . The top 10 algorithms in that prior work are : C4.5 , k-means , support vector machine , Apriori , EM , PageRank , Ad-aBoost , kNN , naive Bayes and CART . In the decade that passed since then , machine learning has expanded , responding to incremental development of computational capabilities and substantial increase of problems in the commercial applications . This study reflects on the top 10 most popular fields of active research in machine learning , as they emerged from the quantitative analysis of leading journals and conferences . This work sees some topics in the broader sense including not only models but also concepts like data sets , features , optimization techniques and evaluation metrics . This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts .",0,1,0,0
q8,1706.05749v1_introduction_0002,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine . Thus , it appears likely that complex environments will become too costly to learn from randomly initialized weights , due both to the increased simulation cost as well as the inherent difficulty of the task . Therefore , some form of prior information must be given to the agent . This can be seen with AlphaGo [ 18 ] , where the agent never learned to play the game without first using supervised learning on human games . While supervised learning certainly has been shown to aid reinforcement learning , it is very costly to obtain sufficient samples and requires the environment to be a task humans can play with reasonable skill , and is therefore impractical for a wide variety of important reinforcement learning problems . In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent .",1,0,1,1
q8,1706.05749v1_introduction_0003,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent . This task can be split into a series of subtasks that are solved simultaneously . Similar to how natural language processing and object detection are subtasks of neural image caption generation [ 23 ] , reinforcement learning environments also have subtasks relevant to a given environment . These subtasks often include player detection , player control , obstacle detection , enemy detection , and player-object interaction , to name a few . These subtasks are common to many environments , but they are often sufficiently different in function and representation that reinforcement learning algorithms fail to generalize them across environments , such as in Atari . These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments , and are a reason for humans superior data efficiency in learning complex tasks .",1,1,1,1
q8,1706.05749v1_introduction_0004,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments , and are a reason for humans superior data efficiency in learning complex tasks . In the case of deliberately similar environments , we can construct the subtasks such that they are similar in function and representation that an agent trained on the first environment can accelerate learning on the second environment due to its preconstructed subtask representations , thus partially avoiding the more complex environment 's increased simulation cost and inherent learning difficulty .",1,0,1,1
q8,1711.01431v1_introduction_0002,"How are machine learning methods applied to concrete scientific or engineering tasks (e.g., physics, biology, control, optimization)?","Introduction Machine learning is often approached from a behaviourist perspective , in which external feedback in the form of a reinforcement signal is the major driving force of improvement . Though this method has lead to many successes , it is confronted with interesting and unsolved challenges like tackling overfitting , providing comprehensibility , building reusable abstractions and concept formation , among many other [ Kotsiantis et al. , 2007 ; Bengio , 2009 ] . The problem with these behaviourist approaches is that they ignore the central importance of internal processes when considering learning . Model internals are often regarded just as a means to achieve higher performance . Analogous to studying human behaviour , however , appreciating the mechanisms of learning boils down to the question : `` when have we really learnt ? '' In this paper , we argue that a computer has learnt when : • the programme becomes better at the task at hand ; • the programme can perform the task more efficiently ; • the code becomes `` more structured '' and simpler . One possible analogy to better understand the above statements can be found in software engineering . When considering code that performs a specific task , we do not care only about its functionality , but also about its execution speed/efficiency and other so-called `` non-functional requirements '' .",1,1,1,1
q9,1501.04309v1_introduction_0001,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","Introduction Machine learning is the study and construction of systems that can learn from data . The systems are called learning machines . When Big Data emerges increasingly , more learning machines are developed and applied in different domains . However , the ultimate goal of machine learning study is insight , not machine itself . By the term insight I mean learning mechanisms in descriptions of mathematical principles . In a loose sense , learning mechanisms can be regarded as the natural entity . As the `` Tao ( 道 ) '' reflects the most fundamental of the universe by Lao Tzu ( 老子 ) , Einstein suggests that we should pursue the simplest mathematical interpretations to the nature . Although learning mechanisms are related to the subjects of psychology , cognitive and brain science , this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms . Up to now , we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles . It is the author 's belief that `` mathematical-principle-based machine '' might be more important and critical than `` brain-inspired machine '' in the study of machine learning . The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning .",0,1,1,1
q9,1504.03874v1_introduction_0002,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","However , this reason alone is not sufficient : There are indeed other topics , such as for instance , information fusion , where BF-based methods are now as well recognized as are methods based on more classical formalisms , such as probabilities , or ontologies . In this report , I assume an additional reason : that some researchers focused on BFT ( especially the youngest ) , who have progressively turned their interests towards ML problems , may not capture the newest trends of this field . In fact , I used to be an example of such researchers , and I acknowledge that my first perceptions of ML were clearly outdated . This is why , I propose a short review of the respective evolution of BFT and of ML , as well as an attempt to put them in perspective . Of course , many senior researchers may find this exercise futile , as they have their own broad view on the question . However , to my knowledge , no recent referenced article is available for any reader seeking for a starting point to question the links between ML and BFT . This document is structured as follow : In Section 2 , a brief recall of the evolution of the mainstream in the BF community is provided .",0,0,0,0
q9,1504.03874v1_introduction_0003,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","However , to my knowledge , no recent referenced article is available for any reader seeking for a starting point to question the links between ML and BFT . This document is structured as follow : In Section 2 , a brief recall of the evolution of the mainstream in the BF community is provided . Then , in Section 3 , a short summary of the earlier ages of ML up to the mid-90s , is sketched , as well as a coarse description of the successful interactions between ML and BFT in those times . Afterward , I provide in Section 4 a synthetic overview of the revolution that blew over ML around the early 2000s , and which modified its goals and the organization of its supporting community . As BFT does not seem to fit in this new picture of the ML world , I list in Section 5 a few problems that may still be of interest for the current mainstream of BFT , as well as some potential interesting evolutions for the community to adapt to the newly raised questions .",0,0,0,0
q9,1505.06614v1_introduction_0001,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","Introduction Machine Learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to `` learn data '' . More precisely , `` learn '' is here intended as the possibility to automatically recognize complex patterns and make `` intelligent '' decisions , based on information data . Hence , machine learning is closely related to fields such as statistics , probability theory , data mining , pattern recognition , artificial intelligence , adaptive control and theoretical computer science . Machine learning algorithms can be classified in the following types : • supervised learning algorithms : a function/classifier is generated , that maps outputs on the training inputs , based on labeled examples inputoutput ; • unsupervised learning algorithms : patterns in the input are recognized , the examples have no labels ; • semi-supervised learning algorithms : supervised and unsupervised learning information is combined ; • reinforcement learning : actions from observation of the world are generated . Every action has some impact in the environment and the environment provides feedbacks that are translated into a score that guide the learning process . The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] .",0,1,1,1
q9,1505.06614v1_introduction_0002,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] . In this short paper , another machine learning technique is proposed to solve the record linkage problem : the multi-criteria classification method Electre Tri . It is the first time that multi-criteria machine learning technique is used to solve the record linkage problem . This application answers to one of `` many challenges in applying supervised machine learning to record linkage matching '' [ 10 ] , showing that the use of multi-criteria classification method Electre Tri to solve the record linkage problem provides good results in term of classification model performances . The importance of this application is in light of the increasing development of the use of administrative sources data . In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows .",0,0,0,0
q9,1612.04858v1_introduction_0001,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","Introduction Recently , there has been interest in applying Bayesian black-box optimization strategies to better conduct optimization over hyperparameter configurations of machine learning models and systems [ 19 ] [ 21 ] [ 11 ] . Most of these techniques require that the objective be a scalar value depending on the hyperparamter configuration x. x opt = arg max x∈X f ( x ) A more detailed introduction to Bayesian optimization and related techniques is provided in [ 8 ] . The focus of this guidebook is on demonstrating several example problems where Bayesian optimization provides a noted benefit . Our hope is to clearly show how Bayesian optimization can assist in better designing and optimizing real-world machine learning systems . All of the examples in this guidebook have corresponding code available on SigOpt 's example github repo .",0,0,0,0
q9,1706.05749v1_abstract_0000,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","This paper introduces Dex , a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems . We also present the novel continual learning method of incremental learning , where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment . We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments . We finally develop a saliency method for qualitative analysis of reinforcement learning , which shows the impact incremental learning has on network attention .",1,1,0,1
q9,1706.05749v1_introduction_0001,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","Introduction Complex environments such as Go , Starcraft , and many modern video-games present profound challenges in deep reinforcement learning that have yet to be solved . They often require long , precise sequences of actions and domain knowledge in order to obtain reward , and have yet to be learned from random weight initialization . Solutions to these problems would mark a significant breakthrough on the path to artificial general intelligence . Recent works in reinforcement learning have shown that environments such as Atari games [ 2 ] can be learned from pixel input to superhuman expertise [ 9 ] . The agents start with randomly initialized weights , and learn largely from trial and error , relying on a reward signal to indicate performance . Despite these successes , complex games , including those where rewards are sparse such as Montezuma 's Revenge , have been notoriously difficult to learn . While methods such as intrinsic motivation [ 3 ] have been used to partially overcome these challenges , we suspect this becomes intractable as complexity increases . Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine .",1,1,1,1
q9,1706.05749v1_introduction_0002,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine . Thus , it appears likely that complex environments will become too costly to learn from randomly initialized weights , due both to the increased simulation cost as well as the inherent difficulty of the task . Therefore , some form of prior information must be given to the agent . This can be seen with AlphaGo [ 18 ] , where the agent never learned to play the game without first using supervised learning on human games . While supervised learning certainly has been shown to aid reinforcement learning , it is very costly to obtain sufficient samples and requires the environment to be a task humans can play with reasonable skill , and is therefore impractical for a wide variety of important reinforcement learning problems . In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent .",1,1,1,1
q9,1706.05749v1_introduction_0003,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent . This task can be split into a series of subtasks that are solved simultaneously . Similar to how natural language processing and object detection are subtasks of neural image caption generation [ 23 ] , reinforcement learning environments also have subtasks relevant to a given environment . These subtasks often include player detection , player control , obstacle detection , enemy detection , and player-object interaction , to name a few . These subtasks are common to many environments , but they are often sufficiently different in function and representation that reinforcement learning algorithms fail to generalize them across environments , such as in Atari . These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments , and are a reason for humans superior data efficiency in learning complex tasks .",1,1,0,1
q9,1706.05749v1_introduction_0004,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments , and are a reason for humans superior data efficiency in learning complex tasks . In the case of deliberately similar environments , we can construct the subtasks such that they are similar in function and representation that an agent trained on the first environment can accelerate learning on the second environment due to its preconstructed subtask representations , thus partially avoiding the more complex environment 's increased simulation cost and inherent learning difficulty .",1,0,0,0
q9,1711.01431v1_introduction_0005,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows . The theoretical ideas are laid out and the case for a new operational definition of machine learning is made . We put forward the conjecture that the optimization of model entropy , leads to concept formation . Last , conclusions and further steps to operationalize these concepts are formulated . 2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a ( non- ) parametric model with respect to some performance measure . This view is clearly reflected in the de facto definition of machine learning by Mitchell [ Mitchell , 1997 ] : `` A computer program is said to learn from an experience X with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experimental data D '' . Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures .",0,0,0,0
q9,1711.01431v1_introduction_0008,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","In other words : Data representation and model computation should be considered as two sides of the same coin . As a result the structural properties of both the model and data space are key to the modelling of higher abstractions . Sparse coding is a perfect example of this . Without sparse coding , although the information is intrinsically `` present '' in the data , neural networks become intractable to train due to the extremely volatile and complex decision surface . From this perspective we follow the observations that have been made by Bengio in [ Bengio et al. , 2013 ] on representation learning . One of the interesting phenomena is `` information entanglement '' [ Glorot et al. , 2011 ] . In this case , the model space is of a lower dimensionality or complexity than the data space . The projection of the data onto a high-dimensional space using sparse coding , then , has the advantage that the representations are more likely to be linearly separable , or at least less nonlinear . On the other hand , when the model complexity is increased considerably ( e.g . by adding layers ) , the neural network becomes untrainable using traditional techniques , because the dimensionality of the search space explodes . Deep learning techniques tackle this issue by -among other techniques -pre-initializing the model-space of particular layer in a maximum-likelihood/minimal-energy state .",0,0,0,0
q9,1811.04422v1_abstract_0000,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","I describe an optimal control view of adversarial machine learning , where the dynamical system is the machine learner , the input are adversarial actions , and the control costs are defined by the adversary 's goals to do harm and be hard to detect . This view encompasses many types of adversarial machine learning , including test-item attacks , training-data poisoning , and adversarial reward shaping . The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning .",0,1,0,0
q9,1811.04871v1_abstract_0001,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?",In this paper we present a maturity framework for machine learning model lifecycle management for enterprises . Our framework is a re-interpretation of the software Capability Maturity Model ( CMM ) for machine learning model development process . We present a set of best practices from authors ' personal experience of building large scale real-world machine learning models to help organizations achieve higher levels of maturity independent of their starting point .,0,0,0,0
q9,1903.00092v2_introduction_0001,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","Introduction Online decision-making problems fundamentally address the issue of dealing with the uncertainty inherently present in the future . In broad terms , these problems can be addressed in two ways . First , a predictive approach like a machine learning algorithm can be used to guess at future events and to act accordingly . This method , while clearly powerful , has the drawback that it is very difficult to make any guarantees about the performance of the algorithm . Another paradigm for solving these problems is to use competitive analysis to guarantee a bound on the performance of a given algorithm . In this approach , we consider some cost function and note the cost that would be incurred by an omniscient algorithm that could access future data . Then , we compare this optimal omniscient cost to the worst-case cost of an online algorithm which can not use future data . By bounding the ratio of these two costs , a guarantee can be made as to how well the online algorithm will perform in terms of the performance of an omniscient algorithm . A classic problem in competitive analysis is the Ski Rental Problem . In this problem , a skier must decide whether to rent skis at a rate of $ 1 per day , or to buy skis at a price of $ B .",1,0,0,0
q9,1907.03010v1_introduction_0001,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","INTRODUCTION In the field of machine learning Time Series are very special data needed their specific processing and methods [ 1 ] [ 2 ] . On top of that , Financial data adds a big challenge due to their proportion of randomness and their non-stationary nature [ 4 ] [ 3 ] . There is a lot of research relative to the Financial Market forecast with Machine Learning [ 5 ] . However , many studies only cover one type of data scaling or labelling while the decisions made on this step can have a huge impact on the results . Not only in term of pure model performances metrics but in term of capabilities to really implement a profitable trading strategy based on the model . This study covers the following points : • Pre-processing and Stationarity • Pre-processing and preservation of useful prices relationships • Labelling for classifiers and regressors",0,0,0,0
q9,1907.07543v1_introduction_0002,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","Introduction Transfer learning in the Natural Language Processing ( NLP ) field has advanced significantly in the last two years , introducing fine-tuning approaches akin to those seen in computer vision some years earlier ( Donahue et al. , 2013 ) . This growth originated from feature-based transfer learning , which in the form of word embeddings has been in use for some years , particularly driven by ( Mikolov , Chen , Corrado , & Dean , 2013 ) . As part of this new wave , we have seen advancements in feature-based transfer learning in the form of ELMo ( Peters et al. , 2018 ) . In addition a characteristic trend in this wave of transfer learning models is a class of algorithms that primarily focus on a finetuning approach , where a base language model ( Bengio , Ducharme , Vincent , & Jauvin , 2003 ) is trained and then fine-tuned on a target task . This base language model is typically very large ( 100M + parameters ) and takes a relatively long time to train . However , the fine-tuning task is usually much quicker to train as only a few parameters are added to the model , typically a single dense layer to the end of a multilayer LSTM or Transformer ( Vaswani et al. , 2017 ) .",1,0,0,0
q9,1907.07543v1_introduction_0003,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","However , the fine-tuning task is usually much quicker to train as only a few parameters are added to the model , typically a single dense layer to the end of a multilayer LSTM or Transformer ( Vaswani et al. , 2017 ) . The model continues training either all , or part , of the network , but this is typically on much less data and for much less time , as only the task specific information is being learned and the general `` understanding '' of the language is transferred . These approaches have , on multiple occasions , broken the state-of-the-art records ( SO-TAs ) across the board on a range of NLP tasks and datasets ( Devlin , Chang , Lee , & Toutanova , 2018 ) ( Howard & Ruder , 2018 ) . However , all of these datasets are designed for deep learning : they are typically large enough that they warrant the use of deep learning ( 5000+ examples per class ) , without the necessity of transfer learning . It is our view that what transfer learning does , in these cases , is push the boundaries of performance . The prevalence of deep learning algorithms in surpassing SOTA records suggests quite clearly that , for the datasets assessed , deep learning surpasses the limits of classical machine learning algorithms in NLP tasks .",1,0,0,0
q9,1907.07543v1_introduction_0004,"What strategies support learning in complex or changing environments, especially in reinforcement learning (e.g., exploration, adaptation, non-stationarity)?","It is our view that what transfer learning does , in these cases , is push the boundaries of performance . The prevalence of deep learning algorithms in surpassing SOTA records suggests quite clearly that , for the datasets assessed , deep learning surpasses the limits of classical machine learning algorithms in NLP tasks . Low-shot transfer learning is another usecase for transfer learning in NLP , one of particular interest to companies working with real-world data . Low-shot transfer learning ( also referred to as `` few-shot '' ) is the use of transfer learning in training models where we have little training data available . This is important as many potential real-world applications of machine learning NLP do not have access to sufficiently large datasets to train deep learning algorithms , and obtaining such a dataset can often be too expensive or time consuming . Howard & Ruder ( 2018 ) note , and Devlin et . al . ( 2018 ) hypothesize that their respective approaches can be used with low quantities of data to give good results .",1,0,0,0
q10,1505.06614v1_introduction_0002,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] . In this short paper , another machine learning technique is proposed to solve the record linkage problem : the multi-criteria classification method Electre Tri . It is the first time that multi-criteria machine learning technique is used to solve the record linkage problem . This application answers to one of `` many challenges in applying supervised machine learning to record linkage matching '' [ 10 ] , showing that the use of multi-criteria classification method Electre Tri to solve the record linkage problem provides good results in term of classification model performances . The importance of this application is in light of the increasing development of the use of administrative sources data . In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows .",0,0,0,0
q10,1612.07640v1_introduction_0004,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","In fact , deep learning is not a new idea , which even dates back to the 1940s [ 12 ] , [ 13 ] . The popularity of deep learning today can be contributed to the following points : * Increasing Computing Power : the advent of graphics processor unit ( GPU ) , the lowered cost of hardware , the better software infrastructure and the faster network connectivity all reduce the required running time of deep learning algorithms significantly . For example , as reported in [ 14 ] , the time required to learn a four-layer DBN with 100 million free parameters can be reduced from several weeks to around a single day . * Increasing Data Size : there is no doubt that the era of Big Data is coming . Our activities are almost all digitized , recorded by computers and sensors , connected to Internet , and stored in cloud . As pointed out in [ 1 ] that in industry-related applications such as industrial informatics and electronics , almost 1000 exabytes are generated per year and a 20-fold increase can be expected in the next ten years . The study in [ 3 ] predicts that 30 billion devices will be connected by 2020 . Therefore , the huge amount of data is able to offset the complexity increase behind deep learning and improve its generalization capability .",0,0,0,0
q10,1612.07640v1_introduction_0005,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","The study in [ 3 ] predicts that 30 billion devices will be connected by 2020 . Therefore , the huge amount of data is able to offset the complexity increase behind deep learning and improve its generalization capability . * Advanced Deep Learning Research : the first breakthrough of deep learning is the pre-training method in an unsupervised way [ 15 ] , where Hinton proposed to pre-train one layer at a time via restricted Boltzmann machine ( RBM ) and then fine-tune using backpropagation . This has been proven to be effective to train multilayer neural networks . Considering the capability of deep learning to address largescale data and learn high-level representation , deep learning can be a powerful and effective solution for machine health monitoring systems ( MHMS ) . Conventional data-driven MHMS usually consists of the following key parts : handcrafted feature design , feature extraction/selection and model training . The right set of features are designed , and then provided to some shallow machine learning algorithms including arXiv:1612.07640v1 [ cs.LG ] 16 Dec 2016 Support Vector Machines ( SVM ) , Naive Bayes ( NB ) , logistic regression [ 16 ] , [ 17 ] , [ 18 ] . It is shown that the representation defines the upper-bound performances of machine learning algorithms [ 19 ] .",0,0,0,0
q10,1612.07640v1_introduction_0006,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","It is shown that the representation defines the upper-bound performances of machine learning algorithms [ 19 ] . However , it is difficult to know and determine what kind of good features should be designed . To alleviate this issue , feature extraction/selection methods , which can be regarded as a kind of information fusion , are performed between hand-crafted feature design and classification/regression models [ 20 ] , [ 21 ] , [ 22 ] . However , manually designing features for a complex domain requires a great deal of human labor and can not be updated on-line . At the same time , feature extraction/selection is another tricky problem , which involves prior selection of hyperparameters such as latent dimension . At last , the above three modules including feature design , feature extraction/selection and model training can not be jointly optimized which may hinder the final performance of the whole system . Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values .",0,0,0,0
q10,1612.07640v1_introduction_0009,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring . Finally , section IV gives a brief summary of the recent achievements of DL-based MHMS and discusses some potential trends of deep learning in machine health monitoring . II . DEEP LEARNING Originated from artificial neural network , deep learning is a branch of machine learning which is featured by multiple nonlinear processing layers . Deep learning aims to learn hierarchy representations of data . Up to date , there are various deep learning architectures and this research topic is fast-growing , in which new models are being developed even per week . And the community is quite open and there are a number of deep learning tutorials and books of good-quality [ 28 ] , [ 29 ] . Therefore , only a brief introduction to some major deep learning techniques that have been applied in machine health monitoring is given . In the following , four deep architectures including Auto-encoders , RBM , CNN and RNN and their corresponding variants are reviewed , respectively .",0,0,0,0
q10,1711.01431v1_introduction_0006,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures . Nevertheless , there is a general consensus that the learning of `` higherorder '' concepts is problematic , and that the solution to this issue is somehow connected to deep architectures that create ever higher forms of abstraction . Experimental research as well as neurological evidence on the organization of the brain , supports this finding [ Bianchini and Scarselli , 2014 ] . The limitation of architecture complexity is preferred , primarily because their behaviour could be understood and the training of more complex or adaptive architectures leads to a explosion of complexity . That was until recently . The recent advanced in so-called `` Deep Learning '' , have focused on training algorithms that are adapted to new kinds of deep architectures [ Deng and Yu , 2014 ] , and heuristic strategies to attain specific structural properties like sparse coding that lead to higher forms of abstractions . With the exception of studies on `` interpretability '' [ Jin and Sendhoff , 2008 ] , structural properties are mainly considered a by-product , a ( desirable ) side effect of the applied training mechanisms . Though the organization and complexity of model topologies is acknowledged to be crucial , current approaches are mainly limited to analysing the data space , i.e .",0,0,0,0
q10,1711.01431v1_introduction_0008,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","In other words : Data representation and model computation should be considered as two sides of the same coin . As a result the structural properties of both the model and data space are key to the modelling of higher abstractions . Sparse coding is a perfect example of this . Without sparse coding , although the information is intrinsically `` present '' in the data , neural networks become intractable to train due to the extremely volatile and complex decision surface . From this perspective we follow the observations that have been made by Bengio in [ Bengio et al. , 2013 ] on representation learning . One of the interesting phenomena is `` information entanglement '' [ Glorot et al. , 2011 ] . In this case , the model space is of a lower dimensionality or complexity than the data space . The projection of the data onto a high-dimensional space using sparse coding , then , has the advantage that the representations are more likely to be linearly separable , or at least less nonlinear . On the other hand , when the model complexity is increased considerably ( e.g . by adding layers ) , the neural network becomes untrainable using traditional techniques , because the dimensionality of the search space explodes . Deep learning techniques tackle this issue by -among other techniques -pre-initializing the model-space of particular layer in a maximum-likelihood/minimal-energy state .",0,0,0,0
q10,1811.04422v1_abstract_0000,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","I describe an optimal control view of adversarial machine learning , where the dynamical system is the machine learner , the input are adversarial actions , and the control costs are defined by the adversary 's goals to do harm and be hard to detect . This view encompasses many types of adversarial machine learning , including test-item attacks , training-data poisoning , and adversarial reward shaping . The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning .",1,1,1,1
q10,1811.04422v1_introduction_0001,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","1 Adversarial Machine Learning is not Machine Learning Machine learning has its mathematical foundation in concentration inequalities . This is a consequence of the independent and identically-distributed ( i.i.d . ) data assumption . In contrast , I suggest that adversarial machine learning may adopt optimal control as its mathematical foundation [ 3 , 25 ] . There are telltale signs : adversarial attacks tend to be subtle and have peculiar non-i.i.d . structures -as control input might be .",0,1,1,1
q10,1811.04871v1_introduction_0003,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","We are well into the era of Artificial Intelligence ( AI ) , spurred by algorithmic , and computational advances , the availability of the latest algorithms in various software libraries , Cloud technologies , and the desire of companies to unleash insights from the vast amounts of untapped unstructured data lying in their enterprises . Companies are actively exploring and deploying trial versions of AI-enabled applications such as chat bots , personal digital assistants , doctors ' assistants , radiology assistants , legal assistants , health and wellness coaches in their enterprises . Powering these applications are the AI building block services such as conversation enabling service , speech-to-text and text to speech , image recognition service , language translation and natural language understanding services that detect entities , relations , keywords , concepts , sentiments and emotions in text . Several of these services are machine learnt , if not all . As more and more machine learnt services make their way into software applications , which themselves are part of business processes , robust life cycle management of these machine learnt models becomes critical for ensuring the integrity of business processes that rely on them . We argue that two reasons necessitate a new maturity framework for machine learning models .",0,0,1,0
q10,1811.04871v1_introduction_0004,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","As more and more machine learnt services make their way into software applications , which themselves are part of business processes , robust life cycle management of these machine learnt models becomes critical for ensuring the integrity of business processes that rely on them . We argue that two reasons necessitate a new maturity framework for machine learning models . First , the lifecycle of machine learning models is significantly different from that of the traditional software and therefore a reinterpretation of the software capability maturity model ( CMM ) maturity framework for building and managing the lifecycle of machine learning models is called for . Second , building machine learning models that work for enterprises requires solutions to a very different set of problems than the academic literature on machine learning typically focuses on . We explain these two reasons below a bit more in detail .",0,0,1,0
q10,1811.11669v1_introduction_0001,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","As a consequence , the functional behavior expected from data-driven components can only be specified in part on their intended domain , and we can not assure that they will behave as expected in all cases . Moreover , their processing structure is usually difficult to trace and validate by humans because this structure rarely follows human intuition but is generated to provide the algorithmically generalized input-output relationship in an effective manner . Prominent representatives of models used by data-driven components are artificial neural networks and support vector machines ( Russell & Norvig , 2016 ) . Since data-driven models are an important source of uncertainty in embedded systems that collaborate in an open context , the uncertainty they introduce has to be appropriately understood and managed during design time and runtime . Previous work ( Kläs & Vollmer , 2018 ) proposes separating the sources of uncertainty in data-driven components into three major classes , distinguishing between uncertainty caused by limitations in terms of model fit , data quality , and scope compliance . Whereas model fit focuses on the inherent uncertainty in data-driven models , data quality covers the additional uncertainty caused by their application to input data obtained in suboptimal conditions and scope compliance covers situations where the model is likely applied outside the scope for which it was trained and validated .",1,0,0,0
q10,1903.00092v2_abstract_0000,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","We consider a variant of the classic Ski Rental online algorithm with applications to machine learning . In our variant , we allow the skier access to a black-box machine learning algorithm that provides an estimate of the probability that there will be less than a threshold number of ski-days . We derive a class of optimal randomized algorithms to determine the strategy that minimizes the worst-case expected competitive ratio for the skier given a prediction from the machine learning algorithm , and analyze the performance and robustness of these algorithms .",0,0,1,0
q10,1907.07543v1_abstract_0001,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","We find that BERT , representing the best of deep transfer learning , is the best performing approach , outperforming top classical machine learning algorithms by 9.7 % on average when trained with 100 examples per class , narrowing to 1.8 % at 1000 labels per class . We also show the robustness of deep transfer learning in moving across domains , where the maximum loss in accuracy is only 0.7 % in similar domain tasks and 3.2 % cross domain , compared to classical machine learning which loses up to 20.6 % .",0,0,1,0
q10,1907.07543v1_introduction_0004,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","It is our view that what transfer learning does , in these cases , is push the boundaries of performance . The prevalence of deep learning algorithms in surpassing SOTA records suggests quite clearly that , for the datasets assessed , deep learning surpasses the limits of classical machine learning algorithms in NLP tasks . Low-shot transfer learning is another usecase for transfer learning in NLP , one of particular interest to companies working with real-world data . Low-shot transfer learning ( also referred to as `` few-shot '' ) is the use of transfer learning in training models where we have little training data available . This is important as many potential real-world applications of machine learning NLP do not have access to sufficiently large datasets to train deep learning algorithms , and obtaining such a dataset can often be too expensive or time consuming . Howard & Ruder ( 2018 ) note , and Devlin et . al . ( 2018 ) hypothesize that their respective approaches can be used with low quantities of data to give good results .",0,0,0,0
q10,1907.07543v1_introduction_0005,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","Howard & Ruder ( 2018 ) note , and Devlin et . al . ( 2018 ) hypothesize that their respective approaches can be used with low quantities of data to give good results . However , in sources such as ( Howard & Ruder , 2018 ) , results on low-shot learning are presented relative to training deep models from scratch , but as mentioned in ( Goodfellow , Bengio , & Courville , 2016 ) , deep learning generally only achieves reasonable performance at about 5000 examples per class and is therefore not necessarily the best paradigm at these scales . This is shown quantitatively in ( Chen , Mckeever , & Delany , 2018 ) where , at scales of 2000+ labels per class , an SVM outperforms several deep learning approaches on text classification tasks . As such , we propose that to evaluate the low-shot learning benefits of deep transfer learning models , we should in fact look at performance against the strongest classical machine learning methods . However , we have yet to find a comprehensive quantitative study performing this analysis and show that low-shot transfer learning in NLP is actually the optimal approach when dealing with small quantities of data . In this paper we attempt to answer this question in the context of classification tasks .",0,0,0,0
q10,2006.15680v1_abstract_0000,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","As machine learning becomes more and more available to the general public , theoretical questions are turning into pressing practical issues . Possibly , one of the most relevant concerns is the assessment of our confidence in trusting machine learning predictions . In many real-world cases , it is of utmost importance to estimate the capabilities of a machine learning algorithm to generalize , i.e. , to provide accurate predictions on unseen data , depending on the characteristics of the target problem . In this work we perform a meta-analysis of 109 publicly-available classification data sets , modeling machine learning generalization as a function of a variety of data set characteristics , ranging from number of samples to intrinsic dimensionality , from class-wise feature skewness to F 1 evaluated on test samples falling outside the convex hull of the training set .",1,0,0,0
q10,2006.15680v1_abstract_0001,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","In this work we perform a meta-analysis of 109 publicly-available classification data sets , modeling machine learning generalization as a function of a variety of data set characteristics , ranging from number of samples to intrinsic dimensionality , from class-wise feature skewness to F 1 evaluated on test samples falling outside the convex hull of the training set . Experimental results demonstrate the relevance of using the concept of the convex hull of the training data in assessing machine learning generalization , by emphasizing the difference between interpolated and extrapolated predictions . Besides several predictable correlations , we observe unexpectedly weak associations between the generalization ability of machine learning models and all metrics related to dimensionality , thus challenging the common assumption that the curse of dimensionality might impair generalization in machine learning .",0,0,0,0
q10,2006.15680v1_introduction_0005,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?","During the analysis , we take into account characteristics such as number of features , number of classes , number of samples , and we look for correlations with quality metrics , such as accuracy of a ML model on training and test points . Extrapolation is assessed not just by alternatively dividing the data into training and test sets , but by analyzing whether data points fall inside or outside of the convex hull of the training data . After collecting the meta-data on the performance of a state-of-the-art classification algorithm on the data sets , the statistical analysis presents both predictable and surprising results , hinting at the fact that dimensionality might not be so cursed after all . Main contributions 1 . We ran a quantitative evaluations of ML models over 109 publicly available data sets . 2 . In Section 2 and Section 3 we provide a general overview on generalization in ML , and of the so-called curse of dimensionality .",0,0,0,0
q10,2007.01503v1_introduction_0003,"What does robustness mean in machine learning, including robustness to distribution shift, noise, and adversarial perturbations?",". . , N } , if there is a representation function f such that f ( x j ) = y j for all j = 1 , . . . , N . This important step is often overlooked in practice . In theory , if there is x j = x k such that y j = y k ( 4 ) then no such function exist . In other words f is a function of more variables than provided in the data set and the idea approximation by f is meaningless . One may always assume some measurement error ε > 0 ( noise ) of the data set and consider instead a weaker condition x j = x k =⇒ y j -y k l 2 < ε ( 5 ) necessary for existence of a function f. In such case one may look at the average values of duplicate points f ave = x j = x k y k x j = x k 1 ( 6 ) and choose to approximate f ave instead of f. There is no guarantee that a good approximation f of function f ave is a good approximation of f itself . One should also consider the norm • d of the approximation function f. It is well known that various classes of `` nice '' functions are dense in L p spaces .",0,0,1,0
q11,1706.05749v1_abstract_0000,"What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?","This paper introduces Dex , a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems . We also present the novel continual learning method of incremental learning , where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment . We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments . We finally develop a saliency method for qualitative analysis of reinforcement learning , which shows the impact incremental learning has on network attention .",1,1,1,1
q11,1711.01431v1_introduction_0006,"What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?","Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures . Nevertheless , there is a general consensus that the learning of `` higherorder '' concepts is problematic , and that the solution to this issue is somehow connected to deep architectures that create ever higher forms of abstraction . Experimental research as well as neurological evidence on the organization of the brain , supports this finding [ Bianchini and Scarselli , 2014 ] . The limitation of architecture complexity is preferred , primarily because their behaviour could be understood and the training of more complex or adaptive architectures leads to a explosion of complexity . That was until recently . The recent advanced in so-called `` Deep Learning '' , have focused on training algorithms that are adapted to new kinds of deep architectures [ Deng and Yu , 2014 ] , and heuristic strategies to attain specific structural properties like sparse coding that lead to higher forms of abstractions . With the exception of studies on `` interpretability '' [ Jin and Sendhoff , 2008 ] , structural properties are mainly considered a by-product , a ( desirable ) side effect of the applied training mechanisms . Though the organization and complexity of model topologies is acknowledged to be crucial , current approaches are mainly limited to analysing the data space , i.e .",0,0,1,0
q11,2110.12773v1_introduction_0002,"What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?","Introduction In the past decade , a sub-field of artificial intelligence ( AI ) , namely Deep Learning ( DL ) neural networks ( or deep neural networks , DNNs ) , has made significant breakthroughs in many scientifically and commercially 2 important applications 1 . Such neural networks are themselves a subset of a wide range of machine learning ( ML ) methods ( Figure 1 . ) ML methods have been widely used for many years in several domains of science , but DNNs have been transformational and are gaining a lot of traction in many scientific communities 3 . Most of the national laboratories that host large-scale experimental facilities are now relying on DNN-based data analytic methods to extract scientific insights from their increasingly large datasets . A recent spectacular success is DeepMind 's use of Deep Learning in their Alpha Fold-1 and Alpha Fold-2 4 solutions to the protein folding 'Grand Challenge ' . This promises to transform much of biological science and open up exciting new research avenues . Other domains of science are exploring physical representations of the system with the data-driven learning ability of neural networks . Current developments are towards specialising these ML approaches to be more domain-specific and domain-aware [ 5 ] [ 6 ] [ 7 ] , and aiming to connect the apparent 'black box ' successes of DL networks with well-understood approaches from science .",0,0,0,0
q11,2110.12773v1_introduction_0003,"What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?","Current developments are towards specialising these ML approaches to be more domain-specific and domain-aware [ 5 ] [ 6 ] [ 7 ] , and aiming to connect the apparent 'black box ' successes of DL networks with well-understood approaches from science . The overarching scope of ML in science is very broad , including identifying patterns , anomalies , and trends from relevant scientific datasets , and using ML for classification and predicting of those patterns , clustering of data , and generating near-realistic synthetic data . There are three approaches for developing ML-based solutions , namely , supervised , unsupervised , and reinforcement learning . In supervised learning , the ML model is trained for a given task with examples . In order to have examples , the data used for training the ML model must contain the ground truth or labels . Supervised learning is therefore only possible when there is a labelled subset of the data . Once trained , the learned model can be deployed for real-time usage , such as pattern classification or estimation -- -which is often referred to as inference . Because of the difficulty in generating labelled data for supervised learning , particularly for experimental datasets , it is often difficult to apply supervised learning directly . To circumvent this limitation , training is often performed on simulated data , which provides an opportunity to have relevant labels .",0,0,0,0
q11,2409.03632v1_abstract_0000,"What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?","What is it to interpret the outputs of an opaque machine learning model ? One approach is to develop interpretable machine learning techniques . These techniques aim to show how machine learning models function by providing either model-centric local or global explanations , which can be based on mechanistic interpretations ( revealing the inner working mechanisms of models ) or non-mechanistic approximations ( showing input feature-output data relationships ) . In this paper , we draw on social philosophy to argue that interpreting machine learning outputs in certain normatively-salient domains could require appealing to a third type of explanation that we call `` socio-structural '' explanation . The relevance of this explanation type is motivated by the fact that machine learning models are not isolated entities but are embedded within and shaped by social structures . Socio-structural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models .",1,0,1,1
q11,2409.03632v1_abstract_0001,"What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?",Socio-structural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models . We demonstrate the importance of socio-structural explanations by examining a racially biased healthcare allocation algorithm . Our proposal highlights the need for transparency beyond model interpretability : understanding the outputs of machine learning systems could require a broader analysis that extends beyond the understanding of the machine learning model itself .,1,0,1,1
q11,2409.03632v1_introduction_0002,"What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?","INTRODUCTION In order to formulate a learning theory of machine learning , it may be necessary to move from seeing an inert model as the machine learner to seeing the human developer-along with , and not separate from , his or her model and surrounding social relations-as the machine learner . -Reigeluth & Castelle [ 55 ] The past decade has seen massive research on interpretable machine learning ( ML ) . 1 Here is a rough restatement of the goal of interpretable ML research program : many ML models are opaque in that even the expert humans can not robustly understand , in non-mathematical terms , the reasons for why particular outputs are generated by these models [ 31 , 42 , 66 ] . To overcome this opacity , various model-centric techniques have been developed to interpret their outputs . These techniques are diverse . They range from producing counterfactual explanations or heatmaps that offer insights into how changing inputs affect outputs [ 28 , 41 , 46 ] , to interpreting the inner workings of the model by probing patterns of neuron activations or attention mechanisms [ 10 , 15 , 48 ] . 2 Despite these advancements , ML interpretability remains a contentious and ambiguous topic in the scientific community , lacking a universally accepted scope and definition [ 11 , 13 , 38 , 45 ] .",1,1,1,1
q11,2409.03632v1_introduction_0003,"What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?","2 Despite these advancements , ML interpretability remains a contentious and ambiguous topic in the scientific community , lacking a universally accepted scope and definition [ 11 , 13 , 38 , 45 ] . This ambiguity complicates the evaluation and regulation of opaque ML systems , raising questions about what constitutes sufficient interpretation and how it should be assessed . A pragmatic and pluralistic approach to interpretability has gained traction , viewing explanations as context-dependent responses to why-questions [ 12 , 31 , 42 , 43 ] . On this pluralistic approach , the adequacy of an explanation depends on the specific inquiry . For simple classification tasks , techniques like saliency maps or feature importance may suffice . For instance , if a model is differentiating between images of cats and dogs , saliency maps could highlight the pixels most influential in the decision-making process . However , for complex and socially-embedded topics -such as biased healthcare algorithms -these model-centric explanations can fall short . Consider an algorithm that predicts hospital readmission risk but systematically underestimates it for certain racial groups . A model-centric explanation might highlight `` total healthcare costs incurred in the past year '' as an important feature . However , this alone might not fully reveal why the algorithm underestimates risk for a specific racial group .",0,1,1,1
q11,2409.03632v1_introduction_0004,"What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?","Consider an algorithm that predicts hospital readmission risk but systematically underestimates it for certain racial groups . A model-centric explanation might highlight `` total healthcare costs incurred in the past year '' as an important feature . However , this alone might not fully reveal why the algorithm underestimates risk for a specific racial group . The algorithmic choice could come from the fact that this racial group , due to systemic inequities , have historically been unable to afford adequate healthcare and thus incurred lower costs . As a result , the low value for the `` total healthcare costs incurred in the past year '' feature does not necessarily indicate better health . Instead , it may suggest unmet healthcare needs , leading to higher readmission rates that the algorithm does not effectively account for . In such cases , interpretations that consider both model-specific details like feature importance and relevant social and structural factors like healthcare affordability disparities among racial groups are crucial for understanding ML predictions or decisions . In this paper , we draw on social philosophy [ 17 , 25 , 26 , 67 , 68 ] to advocate for a more comprehensive approach to ML interpretability research , expanding beyond model-centric explanations . We propose incorporating relevant socio-structural explanations to achieve a deeper understanding of ML outputs in domains with substantial societal impact .",1,1,1,1
q11,2409.03632v1_introduction_0005,"What methods are used to explain or interpret model behavior (e.g., feature importance, saliency, counterfactual explanations)?","In this paper , we draw on social philosophy [ 17 , 25 , 26 , 67 , 68 ] to advocate for a more comprehensive approach to ML interpretability research , expanding beyond model-centric explanations . We propose incorporating relevant socio-structural explanations to achieve a deeper understanding of ML outputs in domains with substantial societal impact . In the rest of the paper , we introduce the concept of socio-structural explanations and discuss their relevance to understanding ML outputs . We then examine how these explanations can enhance the interpretation of automated decision-making by ML systems in healthcare [ 49 ] . Our paper expands the discourse on transparency in machine learning , arguing that it extends beyond model interpretability . We propose that in high-stake decision domains , a sociostructural analysis could be necessary to understand system outputs , uncover societal biases , ensure accountability , and guide policy decisions .",1,0,1,1
q12,1703.10121v1_abstract_0000,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?","Which topics of machine learning are most commonly addressed in research ? This question was initially answered in 2007 by doing a qualitative survey among distinguished researchers . In our study , we revisit this question from a quantitative perspective . Concretely , we collect 54K abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences . We then use machine learning in order to determine the top 10 topics in machine learning . We not only include models , but provide a holistic view across optimization , data , features , etc . This quantitative approach allows reducing the bias of surveys . It reveals new and up-to-date insights into what the 10 most prolific topics in machine learning research are . This allows researchers to identify popular topics as well as new and rising topics for their research .",0,1,1,1
q12,1703.10121v1_introduction_0002,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?",This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts . This attempt aims at reducing bias and looking where the research community puts its focus on . The results of this study allow researchers to put their research into the global context of machine learning . This provides researchers with the opportunity to both conduct research in popular topics and identify topics that have not received sufficient attention in recent research . The rest of this paper is organized as follows . Section 2 describes the data sources and quantitative methodology . Section 3 presents and discusses the top 10 topics identified . Section 4 summarizes this work .,0,1,1,1
q12,1811.04871v1_abstract_0000,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?","Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises . For example , existing machine learning processes can not address how to define business use cases for an AI application , how to convert business requirements from offering managers into data requirements for data scientists , and how to continuously improve AI applications in term of accuracy and fairness , how to customize general purpose machine learning models with industry , domain , and use case specific data to make them more accurate for specific situations etc . Making AI work for enterprises requires special considerations , tools , methods and processes . In this paper we present a maturity framework for machine learning model lifecycle management for enterprises . Our framework is a re-interpretation of the software Capability Maturity Model ( CMM ) for machine learning model development process .",0,1,1,1
q12,2306.14624v2_abstract_0000,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?","We argue that insurance can act as an analogon for the social situatedness of machine learning systems , hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature . Tracing the interaction of uncertainty , fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning . We link insurance fairness conceptions to their machine learning relatives , and use this bridge to problematize fairness as calibration . In this process , we bring to the forefront two themes that have been largely overlooked in the machine learning literature : responsibility and aggregate-individual tensions . See Baker ( , p .",1,1,1,1
q12,2306.14624v2_introduction_0002,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?","Indeed , insurance can be viewed as the first commercial test of probability theory ( Gigerenzer et al. , ; McFall , ) . Insurance , a technology for doing risk , transforms uncertainty into calculable risk ( Lehtonen & Van Hoyweghen , ) . The key idea is to share the risk of a loss in a collective , organized through an abstract mutuality ; due to the 'law ' of large numbers , uncertainty thus becomes manageable and the effect of chance can be offset ( Ewald , ) . In this way , insurance creates a `` community of fate '' in the face of uncertainty ( Heimer , ) . To enter into this community ( the insurance pool ) , the insurer demands a certain fee , called premium , from the policyholder . In insurance , questions of fairness inevitably arise , and have been the subject of much debate . The central point of debate is the tension between risk assessment and distribution ( Abraham , ) . In other words , who is to be mutualized in the pool . Some form of segmentation is found in many insurantial arrangements : the pool of policyholders can be stratified by separating high and low risk individuals . But the specific nature that such segmentation McFall et al .",1,1,1,1
q12,2306.14624v2_introduction_0003,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?","In other words , who is to be mutualized in the pool . Some form of segmentation is found in many insurantial arrangements : the pool of policyholders can be stratified by separating high and low risk individuals . But the specific nature that such segmentation McFall et al . ( ) call insurance `` interestingly uninteresting '' , referring to how insurance is `` hugely underresearched '' given its societal importance , which is typically not recognized ( Ewald , ) . takes typically depends not only on risk assessment , but on further considerations such as assignment of responsibility , modulated by social context ; in this way , insurance is not a neutral technology ( Baker & Simon , ; Glenn , a ) . Our non-comprehensive outline of the history of insurance illustrates how uncertainty , fairness and responsibility interact , and can be entangled and disentangled . From this background , we can extract conceptual insights which also apply to machine learning . The tension between risk assessment and distribution is mirrored in formal fairness principles : solidarity , which can be linked to independence in fair machine learning , contrasts with actuarial fairness , linked to calibration . Briefly , actuarial fairness demands that each policyholder should pay only for their own risk , that is , mutualization should occur only between individuals with the same 'true ' risk .",1,1,1,1
q12,2306.14624v2_introduction_0004,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?","Briefly , actuarial fairness demands that each policyholder should pay only for their own risk , that is , mutualization should occur only between individuals with the same 'true ' risk . In contrast , solidarity calls for equal contribution to the pool . On one level of this text , we problematize actuarial fairness ( by extension , calibration ) as a notion of fairness in the normative sense by taking inspiration from insurance . This perspective is aligned with recent proposals that stress the discrepancy of formal algorithmic fairness and `` substantive '' fairness ( Green , ) , which some prefer to call justice ( Vredenburgh , ) . Parallel to this runs a distinct textual level , where we emphasize two intricately interacting themes : responsibility and tensions between aggregate and individual . Both entail criticism of actuarial fairness , but we suggest that they additionally provide much broader , fruitful lessons for machine learning from insurance . At the highest level of abstraction , our goal is to establish a general conceptual bridge between insurance and machine learning . Traversing this bridge , machine learning scholars can obtain new perspectives on the social situatedness of a probabilistic , statistical technology -we attempt to offer a new 'cognitive toolkit ' for thinking about the social situatedness of machine learning .",1,1,1,1
q12,2306.14624v2_introduction_0005,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?","Traversing this bridge , machine learning scholars can obtain new perspectives on the social situatedness of a probabilistic , statistical technology -we attempt to offer a new 'cognitive toolkit ' for thinking about the social situatedness of machine learning . Our point of view is that fairness can not be reduced to a formal , mathematical issue , but that it requires taking broader social context into account , reasoning for instance about responsibility . And for this , we suggest , insurance is an insightful analogon . Therefore , our objective is to furnish the reader with a guide that charts the landscape of insurance with respect to social issues and to establish links to machine learning . On a formal level , we use the following analogy . In a machine learning task , we are given some features X and associated outcomes Y , which we attempt to approximate by predictions Ŷ . The structural relation to insurance is established by conceiving of X as the features of policyholders ( e.g . age , gender ) with outcomes Y ( e.g . having an accident or not ) , and the task is to set a corresponding premium Ŷ .",1,1,1,1
q12,2401.11351v2_abstract_0000,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?","Quantum machine learning , which involves running machine learning algorithms on quantum devices , has garnered significant attention in both academic and business circles . In this paper , we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning . This includes techniques used in Noisy Intermediate-Scale Quantum ( NISQ ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware . Our review covers fundamental concepts , algorithms , and the statistical learning theory pertinent to quantum machine learning .",0,0,0,0
q12,2409.03632v1_abstract_0001,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?",Socio-structural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models . We demonstrate the importance of socio-structural explanations by examining a racially biased healthcare allocation algorithm . Our proposal highlights the need for transparency beyond model interpretability : understanding the outputs of machine learning systems could require a broader analysis that extends beyond the understanding of the machine learning model itself .,1,0,0,0
q12,2409.03632v1_introduction_0003,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?","2 Despite these advancements , ML interpretability remains a contentious and ambiguous topic in the scientific community , lacking a universally accepted scope and definition [ 11 , 13 , 38 , 45 ] . This ambiguity complicates the evaluation and regulation of opaque ML systems , raising questions about what constitutes sufficient interpretation and how it should be assessed . A pragmatic and pluralistic approach to interpretability has gained traction , viewing explanations as context-dependent responses to why-questions [ 12 , 31 , 42 , 43 ] . On this pluralistic approach , the adequacy of an explanation depends on the specific inquiry . For simple classification tasks , techniques like saliency maps or feature importance may suffice . For instance , if a model is differentiating between images of cats and dogs , saliency maps could highlight the pixels most influential in the decision-making process . However , for complex and socially-embedded topics -such as biased healthcare algorithms -these model-centric explanations can fall short . Consider an algorithm that predicts hospital readmission risk but systematically underestimates it for certain racial groups . A model-centric explanation might highlight `` total healthcare costs incurred in the past year '' as an important feature . However , this alone might not fully reveal why the algorithm underestimates risk for a specific racial group .",0,0,0,0
q12,2409.03632v1_introduction_0005,"What fairness or bias risks arise in machine learning systems, and what mitigation approaches are used?","In this paper , we draw on social philosophy [ 17 , 25 , 26 , 67 , 68 ] to advocate for a more comprehensive approach to ML interpretability research , expanding beyond model-centric explanations . We propose incorporating relevant socio-structural explanations to achieve a deeper understanding of ML outputs in domains with substantial societal impact . In the rest of the paper , we introduce the concept of socio-structural explanations and discuss their relevance to understanding ML outputs . We then examine how these explanations can enhance the interpretation of automated decision-making by ML systems in healthcare [ 49 ] . Our paper expands the discourse on transparency in machine learning , arguing that it extends beyond model interpretability . We propose that in high-stake decision domains , a sociostructural analysis could be necessary to understand system outputs , uncover societal biases , ensure accountability , and guide policy decisions .",1,0,0,0
q13,1612.07640v1_introduction_0002,"What limitations are commonly identified, and what future directions or open problems follow from them?","I . INTRODUCTION I NDUSTRIAL Internet of Things ( IoT ) and data-driven techniques have been revolutionizing manufacturing by enabling computer networks to gather the huge amount of data from connected machines and turn the big machinery data into actionable information [ 1 ] , [ 2 ] , [ 3 ] . As a key component in modern manufacturing system , machine health monitoring has fully embraced the big data revolution . Compared to top-down modeling provided by the traditional physics-based models [ 4 ] , [ 5 ] , [ 6 ] , data-driven machine health monitoring systems offer a new paradigm of bottom-up solution for detection of faults after the occurrence of certain failures ( diagnosis ) and predictions of the future working conditions and the remaining useful life ( prognosis ) [ 1 ] , [ 7 ] . As we know , the complexity and noisy working condition hinder the construction of physical models . And most of these physicsbased models are unable to be updated with on-line measured data , which limits their effectiveness and flexibility . On the other hand , with significant development of sensors , sensor networks and computing systems , data-driven machine health monitoring models have become more and more attractive . To extract useful knowledge and make appropriate decisions from big data , machine learning techniques have been regarded as a powerful solution .",1,0,0,0
q13,1711.01431v1_introduction_0006,"What limitations are commonly identified, and what future directions or open problems follow from them?","Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures . Nevertheless , there is a general consensus that the learning of `` higherorder '' concepts is problematic , and that the solution to this issue is somehow connected to deep architectures that create ever higher forms of abstraction . Experimental research as well as neurological evidence on the organization of the brain , supports this finding [ Bianchini and Scarselli , 2014 ] . The limitation of architecture complexity is preferred , primarily because their behaviour could be understood and the training of more complex or adaptive architectures leads to a explosion of complexity . That was until recently . The recent advanced in so-called `` Deep Learning '' , have focused on training algorithms that are adapted to new kinds of deep architectures [ Deng and Yu , 2014 ] , and heuristic strategies to attain specific structural properties like sparse coding that lead to higher forms of abstractions . With the exception of studies on `` interpretability '' [ Jin and Sendhoff , 2008 ] , structural properties are mainly considered a by-product , a ( desirable ) side effect of the applied training mechanisms . Though the organization and complexity of model topologies is acknowledged to be crucial , current approaches are mainly limited to analysing the data space , i.e .",1,1,1,1
q13,1811.11669v1_introduction_0001,"What limitations are commonly identified, and what future directions or open problems follow from them?","As a consequence , the functional behavior expected from data-driven components can only be specified in part on their intended domain , and we can not assure that they will behave as expected in all cases . Moreover , their processing structure is usually difficult to trace and validate by humans because this structure rarely follows human intuition but is generated to provide the algorithmically generalized input-output relationship in an effective manner . Prominent representatives of models used by data-driven components are artificial neural networks and support vector machines ( Russell & Norvig , 2016 ) . Since data-driven models are an important source of uncertainty in embedded systems that collaborate in an open context , the uncertainty they introduce has to be appropriately understood and managed during design time and runtime . Previous work ( Kläs & Vollmer , 2018 ) proposes separating the sources of uncertainty in data-driven components into three major classes , distinguishing between uncertainty caused by limitations in terms of model fit , data quality , and scope compliance . Whereas model fit focuses on the inherent uncertainty in data-driven models , data quality covers the additional uncertainty caused by their application to input data obtained in suboptimal conditions and scope compliance covers situations where the model is likely applied outside the scope for which it was trained and validated .",1,0,1,1
q13,1903.00092v2_introduction_0001,"What limitations are commonly identified, and what future directions or open problems follow from them?","Introduction Online decision-making problems fundamentally address the issue of dealing with the uncertainty inherently present in the future . In broad terms , these problems can be addressed in two ways . First , a predictive approach like a machine learning algorithm can be used to guess at future events and to act accordingly . This method , while clearly powerful , has the drawback that it is very difficult to make any guarantees about the performance of the algorithm . Another paradigm for solving these problems is to use competitive analysis to guarantee a bound on the performance of a given algorithm . In this approach , we consider some cost function and note the cost that would be incurred by an omniscient algorithm that could access future data . Then , we compare this optimal omniscient cost to the worst-case cost of an online algorithm which can not use future data . By bounding the ratio of these two costs , a guarantee can be made as to how well the online algorithm will perform in terms of the performance of an omniscient algorithm . A classic problem in competitive analysis is the Ski Rental Problem . In this problem , a skier must decide whether to rent skis at a rate of $ 1 per day , or to buy skis at a price of $ B .",1,0,0,0
q13,2007.01503v1_abstract_0000,"What limitations are commonly identified, and what future directions or open problems follow from them?","We take a closer look at some theoretical challenges of Machine Learning as a function approximation , gradient descent as the default optimization algorithm , limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective .",1,0,1,1
q13,2110.12773v1_introduction_0003,"What limitations are commonly identified, and what future directions or open problems follow from them?","Current developments are towards specialising these ML approaches to be more domain-specific and domain-aware [ 5 ] [ 6 ] [ 7 ] , and aiming to connect the apparent 'black box ' successes of DL networks with well-understood approaches from science . The overarching scope of ML in science is very broad , including identifying patterns , anomalies , and trends from relevant scientific datasets , and using ML for classification and predicting of those patterns , clustering of data , and generating near-realistic synthetic data . There are three approaches for developing ML-based solutions , namely , supervised , unsupervised , and reinforcement learning . In supervised learning , the ML model is trained for a given task with examples . In order to have examples , the data used for training the ML model must contain the ground truth or labels . Supervised learning is therefore only possible when there is a labelled subset of the data . Once trained , the learned model can be deployed for real-time usage , such as pattern classification or estimation -- -which is often referred to as inference . Because of the difficulty in generating labelled data for supervised learning , particularly for experimental datasets , it is often difficult to apply supervised learning directly . To circumvent this limitation , training is often performed on simulated data , which provides an opportunity to have relevant labels .",1,1,1,1
q13,2110.12773v1_introduction_0004,"What limitations are commonly identified, and what future directions or open problems follow from them?","Because of the difficulty in generating labelled data for supervised learning , particularly for experimental datasets , it is often difficult to apply supervised learning directly . To circumvent this limitation , training is often performed on simulated data , which provides an opportunity to have relevant labels . However , the simulated data may not be representative of the real data and the model may therefore not perform satisfactorily when used for inferencing . The unsupervised learning technique , in contrast , does not rely on labels . A simple example of this technique is clustering , where the aim is to identify several groups of data points that have common features . Another example is identification of anomalies in data . Example algorithms include k-Means Clustering 8 , Support Vector Machines ( SVM ) 9 , or neural network-based autoencoders 10 . Finally , reinforcement learning relies on a trial-and-error approach to learn a given task with the learning system being positively rewarded whenever the system behaves correctly , and penalised whenever it behaved incorrectly 11 . Each of these learning paradigms have a large number of algorithms , and modern developmental approaches are often hybrid and use one of more of these techniques together . This leaves a very large choice of ML algorithms for any given problem .",1,1,1,1
q13,2205.00210v1_abstract_0000,"What limitations are commonly identified, and what future directions or open problems follow from them?","Machine learning has become prevalent across a wide variety of applications . Unfortunately , machine learning has also shown to be susceptible to deception , leading to errors , and even fatal failures . This circumstance calls into question the widespread use of machine learning , especially in safety-critical applications , unless we are able to assure its correctness and trustworthiness properties . Software verification and testing are established technique for assuring such properties , for example by detecting errors . However , software testing challenges for machine learning are vast and profuse -yet critical to address . This summary talk discusses the current state-of-the-art of software testing for machine learning . More specifically , it discusses six key challenge areas for software testing of machine learning systems , examines current approaches to these challenges and highlights their limitations .",1,0,1,1
q13,2205.00210v1_abstract_0001,"What limitations are commonly identified, and what future directions or open problems follow from them?","More specifically , it discusses six key challenge areas for software testing of machine learning systems , examines current approaches to these challenges and highlights their limitations . The paper provides a research agenda with elaborated directions for making progress toward advancing the state-of-the-art on testing of machine learning . Index termstesting challenges , machine learning , machine learning testing , testing ML , testing AI",1,0,1,1
q13,2205.00210v1_introduction_0003,"What limitations are commonly identified, and what future directions or open problems follow from them?","This means that for constant test inputs and preconditions , an ML-trained software component can produce different outputs in consecutive runs . Researchers have tried using testing techniques from traditional software development ( Hutchison et al . 2018 ) , to deal with some of these challenges . However , it has been observed that traditional testing approaches in general fail to adequately address fundamental challenges of testing ML ( Helle and Schamai 2016 ) , and that these traditional approaches require adaptation to the new context of ML . The better we understand current research challenges of testing ML , the more successful we can be in developing novel techniques that effectively address these challenges and advance this scientific field . In this paper , we : i ) identify and discuss the most challenging areas in software testing for ML , ii ) synthesize the most promising approaches to these challenges , iii ) spotlight their limitations , and iv ) make recommendations of further research efforts on software testing of ML . We note that the aim of the paper is not to exhaustively list all published work , but distill the most representative work .",1,0,1,1
q13,2205.14136v1_introduction_0002,"What limitations are commonly identified, and what future directions or open problems follow from them?","I . INTRODUCTION Property Specification Language ( PSL ) is a form of temporal logic that is designed to capture temporal relations between discrete variables over discrete time . Due to this nature , PSL has been mainly used in hardware design and verification since it was standardized by IEEE in 2004 [ 1 ] , [ 2 ] , [ 8 ] . There have been attempts to extend PSL to deal with continuous variables over continuous time [ 6 ] . Due to its inherent limitation of expressibility , there have not been many successful applications . In recent years , anomaly detection has been widely used in practice [ 3 ] , [ 13 ] . There are many applications where realtime streaming events are monitored and analyzed in order to detect abnormal behaviors . For example , if the amount of free memory of a computer is below a certain threshold , it can be considered as an anomaly . As another example , if there is an anomalous drop in purchase of a product in an online store , it is possible that the product is out of stock , which needs attention . The state-of-the-art technique Fig . 1 .",0,1,1,1
q13,2205.14136v1_introduction_0004,"What limitations are commonly identified, and what future directions or open problems follow from them?","The machine learning module takes as input a number of continuous variables x 1 , x 2 , . . . . . . , x m , and outputs some discrete events y 1 , y 2 , . . . . . . , y n , which become the input of the PSL monitor . The PSL monitor encodes a user-defined temporal relation , which filters the output from the machine learning module . In this new framework , machine learning techniques extend the capability of PSL by discretizing continuous time and events ; the PSL monitor refines the results produced by the machine learning module . This combination of machine learning and formal methods yields a whole that is greater than the sum of its parts . The rest of this paper is organized as follows . In Section II , we give a brief introduction to anomaly detection . Section III discusses the overall architecture of TEF . In Section IV , we describe how TEF is implemented . Section V illustrates how TEF can be used to capture temporal relations . Section VI summarizes the conclusions of the paper and future work .",0,1,1,1
q13,2401.11351v2_introduction_0005,"What limitations are commonly identified, and what future directions or open problems follow from them?","And by harnessing the capabilities of classical machine learning to gain deeper insights into quantum physics , it might lead to advancements in quantum computing too . In this review , we primarily focus on quantum machine learning in its narrower sense , which pertains to execute quantum algorithms designed for machine learning purposes . Presently , we find ourselves in what 's referred to as ( perhaps the end of ) the noisy intermediate scale quantum or NISQ era of quantum computing . Quantum computers are susceptible to background noise , which imposes limitations on our ability to construct quantum computers with sufficient depth for executing tasks demanding fast and precise computations . The quantum computers available today can only handle on the order of around 100 qubits , and they all exhibit noise , making it challenging to derive tangible benefits for our daily lives . The solution to this predicament is known as quantum error correction ( QEC ) code [ 113 ] . Think of QEC as a safeguard for quantum information . Typically , quantum information is lost once it 's measured , which becomes especially likely in noisy environments . However , information protected by QEC can persist if it remains undamaged within certain limits . It 's worth noting that all error correction codes have their constraints , implying that information will inevitably be lost if it 's severely damaged .",1,0,1,1
q14,1504.03874v1_introduction_0001,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","Introduction In an age of user generated web-contents and of portable devices with embedded computer vision capabilities , machine learning ( ML ) and big data mining questions are fundamental . As a result , these questions naturally penetrate neighboring research fields , including belief function theory ( BFT ) , so that it is now usual to attend a `` Classification '' session [ 26 ] or a `` Machine Learning '' session [ 16 ] in a conference devoted to belief functions . However , it is hard to accept that among the various proposed approaches based on BF , very few have become state-of-the-art ML methods , the knowledge of which has spread beyond the BF community . Without any doubt , this can be partly explained by the relative size of the scientific communities under consideration : although quickly growing , the BF one is relatively small with respect to that of statistics , Bayesian networks , neural networks , etc . However , this reason alone is not sufficient : There are indeed other topics , such as for instance , information fusion , where BF-based methods are now as well recognized as are methods based on more classical formalisms , such as probabilities , or ontologies .",0,0,0,0
q14,1505.06614v1_introduction_0001,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","Introduction Machine Learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to `` learn data '' . More precisely , `` learn '' is here intended as the possibility to automatically recognize complex patterns and make `` intelligent '' decisions , based on information data . Hence , machine learning is closely related to fields such as statistics , probability theory , data mining , pattern recognition , artificial intelligence , adaptive control and theoretical computer science . Machine learning algorithms can be classified in the following types : • supervised learning algorithms : a function/classifier is generated , that maps outputs on the training inputs , based on labeled examples inputoutput ; • unsupervised learning algorithms : patterns in the input are recognized , the examples have no labels ; • semi-supervised learning algorithms : supervised and unsupervised learning information is combined ; • reinforcement learning : actions from observation of the world are generated . Every action has some impact in the environment and the environment provides feedbacks that are translated into a score that guide the learning process . The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] .",0,0,0,0
q14,1505.06614v1_introduction_0002,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] . In this short paper , another machine learning technique is proposed to solve the record linkage problem : the multi-criteria classification method Electre Tri . It is the first time that multi-criteria machine learning technique is used to solve the record linkage problem . This application answers to one of `` many challenges in applying supervised machine learning to record linkage matching '' [ 10 ] , showing that the use of multi-criteria classification method Electre Tri to solve the record linkage problem provides good results in term of classification model performances . The importance of this application is in light of the increasing development of the use of administrative sources data . In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows .",0,0,1,0
q14,1505.06614v1_introduction_0003,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows . Section 2 describes an introduction to the Record Linkage problem ; then the next Section 3 describes the method Electre Tri , used to solved the Record Linkage and in the last Section 4 a preliminary experiment is conducted on simulated data . The paper closes with some final remarks and conclusions . 2 Linked Data : the Record Linkage Generally speaking , in integration of two data sets the objective is the detection of those records , in the different data sets , that belong to the same statistical unit . This action allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] .",0,0,1,0
q14,1505.06614v1_introduction_0004,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","This action allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] . In the first situation , the definition of record linkage in [ 9 ] is more precise `` Record linkage is a solution to the problem of recognizing those records in two files which represent identical persons , objects , or events ( said to be matched ) '' The term record linkage originated in the public health area when files of individual patients were brought together using name , date-of-birth and other information [ 16 ] . One of the main motivations for the utilize of the record linkage method is the construction of the big data bases for answer to the new informative needs [ 8 ] . In order to better understand the problem , small practical example is now presented . Suppose the user wants to link two datasets of persons A and B , whose the variables Name , Address and Age are known .",0,0,0,0
q14,1505.06614v1_introduction_0005,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","In order to better understand the problem , small practical example is now presented . Suppose the user wants to link two datasets of persons A and B , whose the variables Name , Address and Age are known . Suppose that Table A contains the following values : Table A : Data in the first dataset Unit Name Address Age a1 John A Smith 16 Main Street 16 a2 Javier Martinez 49 E Applecross Road 33 a3 Gillian Jones 645 Reading Aev 22 Furthermore , suppose that Table B contains the following values : Table B : Data in the second dataset Unit Name Address Age b1 J H Smith 16 Main St 17 b2 Haveir Marteenez 49 Aplecross Raod 36 b3 Jilliam Brown 123 Norcross Blvd 43 The matching table A × B contains two units referring probably to the same persons , that the method should individuate as matches : 'John A Smith ' with ' J H Smith ' and 'Javier Martinez ' with 'Haveir Marteenez ' . Modern record linkage begins with the pioneering work of Newcombe et al . [ 14 ] , who introduced odds ratio of frequencies and the decision rules for delineating matches and nonmatches . In recent years , advances have yielded computer system that incorporate sophisticated ideas from computer sciences , statistics and operational research [ 16 ] .",0,0,0,0
q14,1505.06614v1_introduction_0006,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","Modern record linkage begins with the pioneering work of Newcombe et al . [ 14 ] , who introduced odds ratio of frequencies and the decision rules for delineating matches and nonmatches . In recent years , advances have yielded computer system that incorporate sophisticated ideas from computer sciences , statistics and operational research [ 16 ] . Then , Fellegi and Sunter [ 9 ] introduced a mathematical foundation for record linkage . Their theory demonstrated the optimality of the decision rules used by Newcombe and introduced a variety of ways of estimating crucial matching probabilities ( parameters ) directly from the files being matches . Formally , given two files A and B to be matched , each pair ( a , b ) ∈ Γ = A×B has to be classified into true match or true nonmatch . The odds ratios of probabilities is : R = P r ( γ ∈ Γ | M ) P r ( γ ∈ Γ | U ) where γ is an arbitrary agreement pattern in the comparison space Γ , M is the set of of true matches and U is the set of true nonmatches . Between these two sets , the intermediate set of the possible matches exists .",0,0,0,0
q14,1612.04858v1_abstract_0000,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","The engineering of machine learning systems is still a nascent field ; relying on a seemingly daunting collection of quickly evolving tools and best practices . It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques . We outline four example machine learning problems that can be solved using open source machine learning libraries , and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications .",1,0,1,1
q14,1612.04858v1_introduction_0001,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","Introduction Recently , there has been interest in applying Bayesian black-box optimization strategies to better conduct optimization over hyperparameter configurations of machine learning models and systems [ 19 ] [ 21 ] [ 11 ] . Most of these techniques require that the objective be a scalar value depending on the hyperparamter configuration x. x opt = arg max x∈X f ( x ) A more detailed introduction to Bayesian optimization and related techniques is provided in [ 8 ] . The focus of this guidebook is on demonstrating several example problems where Bayesian optimization provides a noted benefit . Our hope is to clearly show how Bayesian optimization can assist in better designing and optimizing real-world machine learning systems . All of the examples in this guidebook have corresponding code available on SigOpt 's example github repo .",1,0,0,0
q14,1612.07640v1_abstract_0000,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","Since 2006 , deep learning ( DL ) has become a rapidly growing research direction , redefining state-of-the-art performances in a wide range of areas such as object recognition , image segmentation , speech recognition and machine translation . In modern manufacturing systems , data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet . Meanwhile , deep learning provides useful tools for processing and analyzing these big machinery data . The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring .",1,1,1,1
q14,1612.07640v1_abstract_0001,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","Meanwhile , deep learning provides useful tools for processing and analyzing these big machinery data . The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring . After the brief introduction of deep learning techniques , the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects : Autoencoder ( AE ) and its variants , Restricted Boltzmann Machines and its variants including Deep Belief Network ( DBN ) and Deep Boltzmann Machines ( DBM ) , Convolutional Neural Networks ( CNN ) and Recurrent Neural Networks ( RNN ) . Finally , some new trends of DL-based machine health monitoring methods are discussed .",1,0,0,0
q14,1612.07640v1_introduction_0002,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","I . INTRODUCTION I NDUSTRIAL Internet of Things ( IoT ) and data-driven techniques have been revolutionizing manufacturing by enabling computer networks to gather the huge amount of data from connected machines and turn the big machinery data into actionable information [ 1 ] , [ 2 ] , [ 3 ] . As a key component in modern manufacturing system , machine health monitoring has fully embraced the big data revolution . Compared to top-down modeling provided by the traditional physics-based models [ 4 ] , [ 5 ] , [ 6 ] , data-driven machine health monitoring systems offer a new paradigm of bottom-up solution for detection of faults after the occurrence of certain failures ( diagnosis ) and predictions of the future working conditions and the remaining useful life ( prognosis ) [ 1 ] , [ 7 ] . As we know , the complexity and noisy working condition hinder the construction of physical models . And most of these physicsbased models are unable to be updated with on-line measured data , which limits their effectiveness and flexibility . On the other hand , with significant development of sensors , sensor networks and computing systems , data-driven machine health monitoring models have become more and more attractive . To extract useful knowledge and make appropriate decisions from big data , machine learning techniques have been regarded as a powerful solution .",0,0,0,0
q14,1612.07640v1_introduction_0003,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","On the other hand , with significant development of sensors , sensor networks and computing systems , data-driven machine health monitoring models have become more and more attractive . To extract useful knowledge and make appropriate decisions from big data , machine learning techniques have been regarded as a powerful solution . As the hottest subfield of machine R. Yan is the corresponding author . E-mail : ruqiang @ seu.edu.cn This manuscript has been submitted to IEEE Transactions on Neural Networks and Learning Systems learning , deep learning is able to act as a bridge connecting big machinery data and intelligent machine health monitoring . As a branch of machine learning , deep learning attempts to model high level representations behind data and classify ( predict ) patterns via stacking multiple layers of information processing modules in hierarchical architectures . Recently , deep learning has been successfully adopted in various areas such as computer vision , automatic speech recognition , natural language processing , audio recognition and bioinformatics [ 8 ] , [ 9 ] , [ 10 ] , [ 11 ] . In fact , deep learning is not a new idea , which even dates back to the 1940s [ 12 ] , [ 13 ] .",1,0,0,0
q14,1612.07640v1_introduction_0004,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","In fact , deep learning is not a new idea , which even dates back to the 1940s [ 12 ] , [ 13 ] . The popularity of deep learning today can be contributed to the following points : * Increasing Computing Power : the advent of graphics processor unit ( GPU ) , the lowered cost of hardware , the better software infrastructure and the faster network connectivity all reduce the required running time of deep learning algorithms significantly . For example , as reported in [ 14 ] , the time required to learn a four-layer DBN with 100 million free parameters can be reduced from several weeks to around a single day . * Increasing Data Size : there is no doubt that the era of Big Data is coming . Our activities are almost all digitized , recorded by computers and sensors , connected to Internet , and stored in cloud . As pointed out in [ 1 ] that in industry-related applications such as industrial informatics and electronics , almost 1000 exabytes are generated per year and a 20-fold increase can be expected in the next ten years . The study in [ 3 ] predicts that 30 billion devices will be connected by 2020 . Therefore , the huge amount of data is able to offset the complexity increase behind deep learning and improve its generalization capability .",0,1,1,1
q14,1612.07640v1_introduction_0007,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values . Therefore , the application of one layer can learn a new representation of the input data and then , the stacking structure of multiple layers can enable MHMS to learn complex concepts out of simpler concepts that can be constructed from raw input . In addition , DL-based MHMS achieve an end-to-end system , which can automatically learn internal representations from raw input and predict targets . Compared to conventional data driven MHMS , DL-based MHMS do not require extensive human labor and knowledge for handcrafted feature design . All model parameters including feature module and pattern classification/regression module can be trained jointly . Therefore , DL-based models can be applied to addressing machine health monitoring in a very general way . For example , it is possible that the model trained for fault diagnosis problem can be used for prognosis by only replacing the top softmax layer with a linear regression layer . The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 .",0,0,0,0
q14,1612.07640v1_introduction_0008,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 . Deep learning models have several variants such as Auto-Dncoders [ 23 ] , Deep Belief Network [ 24 ] , Deep Boltzmann Machines [ 25 ] , Convolutional Neural Networks [ 26 ] and Recurrent Neural Networks [ 27 ] . During recent years , various researchers have demonstrated success of these deep learning models in the application of machine health monitoring . This paper attempts to provide a wide overview on these latest DL-based MHMS works that impact the state-of-the art technologies . Compared to these frontiers of deep learning including Computer Vision and Natural Language Processing , machine health monitoring community is catching up and has witnessed an emerging research . Therefore , the purpose of this survey article is to present researchers and engineers in the area of machine health monitoring system , a global view of this hot and active topic , and help them to acquire basic knowledge , quickly apply deep learning models and develop novel DL-based MHMS . The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring .",0,0,0,0
q14,1612.07640v1_introduction_0009,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring . Finally , section IV gives a brief summary of the recent achievements of DL-based MHMS and discusses some potential trends of deep learning in machine health monitoring . II . DEEP LEARNING Originated from artificial neural network , deep learning is a branch of machine learning which is featured by multiple nonlinear processing layers . Deep learning aims to learn hierarchy representations of data . Up to date , there are various deep learning architectures and this research topic is fast-growing , in which new models are being developed even per week . And the community is quite open and there are a number of deep learning tutorials and books of good-quality [ 28 ] , [ 29 ] . Therefore , only a brief introduction to some major deep learning techniques that have been applied in machine health monitoring is given . In the following , four deep architectures including Auto-encoders , RBM , CNN and RNN and their corresponding variants are reviewed , respectively .",0,0,0,0
q14,1703.10121v1_introduction_0001,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","Introduction In 2007 , a paper named `` Top 10 algorithms in data mining '' identified and presented the top 10 most influential data mining algorithms within the research community [ 1 ] . The selection criteria were created by consolidating direct nominations from award winning researchers , the research community opinions and the number of citations in Google Scholar . The top 10 algorithms in that prior work are : C4.5 , k-means , support vector machine , Apriori , EM , PageRank , Ad-aBoost , kNN , naive Bayes and CART . In the decade that passed since then , machine learning has expanded , responding to incremental development of computational capabilities and substantial increase of problems in the commercial applications . This study reflects on the top 10 most popular fields of active research in machine learning , as they emerged from the quantitative analysis of leading journals and conferences . This work sees some topics in the broader sense including not only models but also concepts like data sets , features , optimization techniques and evaluation metrics . This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts .",0,0,0,0
q14,1706.05749v1_introduction_0002,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine . Thus , it appears likely that complex environments will become too costly to learn from randomly initialized weights , due both to the increased simulation cost as well as the inherent difficulty of the task . Therefore , some form of prior information must be given to the agent . This can be seen with AlphaGo [ 18 ] , where the agent never learned to play the game without first using supervised learning on human games . While supervised learning certainly has been shown to aid reinforcement learning , it is very costly to obtain sufficient samples and requires the environment to be a task humans can play with reasonable skill , and is therefore impractical for a wide variety of important reinforcement learning problems . In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent .",0,1,1,1
q14,1706.05749v1_introduction_0004,"How do real-world constraints (e.g., deployment, cost, latency, privacy, safety) shape machine learning systems used for decision-making?","These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments , and are a reason for humans superior data efficiency in learning complex tasks . In the case of deliberately similar environments , we can construct the subtasks such that they are similar in function and representation that an agent trained on the first environment can accelerate learning on the second environment due to its preconstructed subtask representations , thus partially avoiding the more complex environment 's increased simulation cost and inherent learning difficulty .",0,1,1,1
q15,1504.03874v1_introduction_0001,"Which modeling and architectural design choices are especially important, and why?","Introduction In an age of user generated web-contents and of portable devices with embedded computer vision capabilities , machine learning ( ML ) and big data mining questions are fundamental . As a result , these questions naturally penetrate neighboring research fields , including belief function theory ( BFT ) , so that it is now usual to attend a `` Classification '' session [ 26 ] or a `` Machine Learning '' session [ 16 ] in a conference devoted to belief functions . However , it is hard to accept that among the various proposed approaches based on BF , very few have become state-of-the-art ML methods , the knowledge of which has spread beyond the BF community . Without any doubt , this can be partly explained by the relative size of the scientific communities under consideration : although quickly growing , the BF one is relatively small with respect to that of statistics , Bayesian networks , neural networks , etc . However , this reason alone is not sufficient : There are indeed other topics , such as for instance , information fusion , where BF-based methods are now as well recognized as are methods based on more classical formalisms , such as probabilities , or ontologies .",0,0,0,0
q15,1505.06614v1_introduction_0001,"Which modeling and architectural design choices are especially important, and why?","Introduction Machine Learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to `` learn data '' . More precisely , `` learn '' is here intended as the possibility to automatically recognize complex patterns and make `` intelligent '' decisions , based on information data . Hence , machine learning is closely related to fields such as statistics , probability theory , data mining , pattern recognition , artificial intelligence , adaptive control and theoretical computer science . Machine learning algorithms can be classified in the following types : • supervised learning algorithms : a function/classifier is generated , that maps outputs on the training inputs , based on labeled examples inputoutput ; • unsupervised learning algorithms : patterns in the input are recognized , the examples have no labels ; • semi-supervised learning algorithms : supervised and unsupervised learning information is combined ; • reinforcement learning : actions from observation of the world are generated . Every action has some impact in the environment and the environment provides feedbacks that are translated into a score that guide the learning process . The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] .",0,0,0,0
q15,1505.06614v1_introduction_0002,"Which modeling and architectural design choices are especially important, and why?","The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] . In this short paper , another machine learning technique is proposed to solve the record linkage problem : the multi-criteria classification method Electre Tri . It is the first time that multi-criteria machine learning technique is used to solve the record linkage problem . This application answers to one of `` many challenges in applying supervised machine learning to record linkage matching '' [ 10 ] , showing that the use of multi-criteria classification method Electre Tri to solve the record linkage problem provides good results in term of classification model performances . The importance of this application is in light of the increasing development of the use of administrative sources data . In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows .",0,0,0,0
q15,1612.04858v1_introduction_0001,"Which modeling and architectural design choices are especially important, and why?","Introduction Recently , there has been interest in applying Bayesian black-box optimization strategies to better conduct optimization over hyperparameter configurations of machine learning models and systems [ 19 ] [ 21 ] [ 11 ] . Most of these techniques require that the objective be a scalar value depending on the hyperparamter configuration x. x opt = arg max x∈X f ( x ) A more detailed introduction to Bayesian optimization and related techniques is provided in [ 8 ] . The focus of this guidebook is on demonstrating several example problems where Bayesian optimization provides a noted benefit . Our hope is to clearly show how Bayesian optimization can assist in better designing and optimizing real-world machine learning systems . All of the examples in this guidebook have corresponding code available on SigOpt 's example github repo .",1,0,0,0
q15,1612.07640v1_abstract_0001,"Which modeling and architectural design choices are especially important, and why?","Meanwhile , deep learning provides useful tools for processing and analyzing these big machinery data . The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring . After the brief introduction of deep learning techniques , the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects : Autoencoder ( AE ) and its variants , Restricted Boltzmann Machines and its variants including Deep Belief Network ( DBN ) and Deep Boltzmann Machines ( DBM ) , Convolutional Neural Networks ( CNN ) and Recurrent Neural Networks ( RNN ) . Finally , some new trends of DL-based machine health monitoring methods are discussed .",1,1,0,1
q15,1612.07640v1_introduction_0003,"Which modeling and architectural design choices are especially important, and why?","On the other hand , with significant development of sensors , sensor networks and computing systems , data-driven machine health monitoring models have become more and more attractive . To extract useful knowledge and make appropriate decisions from big data , machine learning techniques have been regarded as a powerful solution . As the hottest subfield of machine R. Yan is the corresponding author . E-mail : ruqiang @ seu.edu.cn This manuscript has been submitted to IEEE Transactions on Neural Networks and Learning Systems learning , deep learning is able to act as a bridge connecting big machinery data and intelligent machine health monitoring . As a branch of machine learning , deep learning attempts to model high level representations behind data and classify ( predict ) patterns via stacking multiple layers of information processing modules in hierarchical architectures . Recently , deep learning has been successfully adopted in various areas such as computer vision , automatic speech recognition , natural language processing , audio recognition and bioinformatics [ 8 ] , [ 9 ] , [ 10 ] , [ 11 ] . In fact , deep learning is not a new idea , which even dates back to the 1940s [ 12 ] , [ 13 ] .",1,0,0,0
q15,1612.07640v1_introduction_0004,"Which modeling and architectural design choices are especially important, and why?","In fact , deep learning is not a new idea , which even dates back to the 1940s [ 12 ] , [ 13 ] . The popularity of deep learning today can be contributed to the following points : * Increasing Computing Power : the advent of graphics processor unit ( GPU ) , the lowered cost of hardware , the better software infrastructure and the faster network connectivity all reduce the required running time of deep learning algorithms significantly . For example , as reported in [ 14 ] , the time required to learn a four-layer DBN with 100 million free parameters can be reduced from several weeks to around a single day . * Increasing Data Size : there is no doubt that the era of Big Data is coming . Our activities are almost all digitized , recorded by computers and sensors , connected to Internet , and stored in cloud . As pointed out in [ 1 ] that in industry-related applications such as industrial informatics and electronics , almost 1000 exabytes are generated per year and a 20-fold increase can be expected in the next ten years . The study in [ 3 ] predicts that 30 billion devices will be connected by 2020 . Therefore , the huge amount of data is able to offset the complexity increase behind deep learning and improve its generalization capability .",0,1,0,0
q15,1612.07640v1_introduction_0005,"Which modeling and architectural design choices are especially important, and why?","The study in [ 3 ] predicts that 30 billion devices will be connected by 2020 . Therefore , the huge amount of data is able to offset the complexity increase behind deep learning and improve its generalization capability . * Advanced Deep Learning Research : the first breakthrough of deep learning is the pre-training method in an unsupervised way [ 15 ] , where Hinton proposed to pre-train one layer at a time via restricted Boltzmann machine ( RBM ) and then fine-tune using backpropagation . This has been proven to be effective to train multilayer neural networks . Considering the capability of deep learning to address largescale data and learn high-level representation , deep learning can be a powerful and effective solution for machine health monitoring systems ( MHMS ) . Conventional data-driven MHMS usually consists of the following key parts : handcrafted feature design , feature extraction/selection and model training . The right set of features are designed , and then provided to some shallow machine learning algorithms including arXiv:1612.07640v1 [ cs.LG ] 16 Dec 2016 Support Vector Machines ( SVM ) , Naive Bayes ( NB ) , logistic regression [ 16 ] , [ 17 ] , [ 18 ] . It is shown that the representation defines the upper-bound performances of machine learning algorithms [ 19 ] .",0,1,1,1
q15,1612.07640v1_introduction_0006,"Which modeling and architectural design choices are especially important, and why?","It is shown that the representation defines the upper-bound performances of machine learning algorithms [ 19 ] . However , it is difficult to know and determine what kind of good features should be designed . To alleviate this issue , feature extraction/selection methods , which can be regarded as a kind of information fusion , are performed between hand-crafted feature design and classification/regression models [ 20 ] , [ 21 ] , [ 22 ] . However , manually designing features for a complex domain requires a great deal of human labor and can not be updated on-line . At the same time , feature extraction/selection is another tricky problem , which involves prior selection of hyperparameters such as latent dimension . At last , the above three modules including feature design , feature extraction/selection and model training can not be jointly optimized which may hinder the final performance of the whole system . Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values .",0,1,0,0
q15,1612.07640v1_introduction_0007,"Which modeling and architectural design choices are especially important, and why?","Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values . Therefore , the application of one layer can learn a new representation of the input data and then , the stacking structure of multiple layers can enable MHMS to learn complex concepts out of simpler concepts that can be constructed from raw input . In addition , DL-based MHMS achieve an end-to-end system , which can automatically learn internal representations from raw input and predict targets . Compared to conventional data driven MHMS , DL-based MHMS do not require extensive human labor and knowledge for handcrafted feature design . All model parameters including feature module and pattern classification/regression module can be trained jointly . Therefore , DL-based models can be applied to addressing machine health monitoring in a very general way . For example , it is possible that the model trained for fault diagnosis problem can be used for prognosis by only replacing the top softmax layer with a linear regression layer . The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 .",0,1,0,0
q15,1612.07640v1_introduction_0008,"Which modeling and architectural design choices are especially important, and why?","The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 . Deep learning models have several variants such as Auto-Dncoders [ 23 ] , Deep Belief Network [ 24 ] , Deep Boltzmann Machines [ 25 ] , Convolutional Neural Networks [ 26 ] and Recurrent Neural Networks [ 27 ] . During recent years , various researchers have demonstrated success of these deep learning models in the application of machine health monitoring . This paper attempts to provide a wide overview on these latest DL-based MHMS works that impact the state-of-the art technologies . Compared to these frontiers of deep learning including Computer Vision and Natural Language Processing , machine health monitoring community is catching up and has witnessed an emerging research . Therefore , the purpose of this survey article is to present researchers and engineers in the area of machine health monitoring system , a global view of this hot and active topic , and help them to acquire basic knowledge , quickly apply deep learning models and develop novel DL-based MHMS . The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring .",1,0,0,0
q15,1612.07640v1_introduction_0009,"Which modeling and architectural design choices are especially important, and why?","The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring . Finally , section IV gives a brief summary of the recent achievements of DL-based MHMS and discusses some potential trends of deep learning in machine health monitoring . II . DEEP LEARNING Originated from artificial neural network , deep learning is a branch of machine learning which is featured by multiple nonlinear processing layers . Deep learning aims to learn hierarchy representations of data . Up to date , there are various deep learning architectures and this research topic is fast-growing , in which new models are being developed even per week . And the community is quite open and there are a number of deep learning tutorials and books of good-quality [ 28 ] , [ 29 ] . Therefore , only a brief introduction to some major deep learning techniques that have been applied in machine health monitoring is given . In the following , four deep architectures including Auto-encoders , RBM , CNN and RNN and their corresponding variants are reviewed , respectively .",0,1,0,0
q15,1706.05749v1_introduction_0003,"Which modeling and architectural design choices are especially important, and why?","In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent . This task can be split into a series of subtasks that are solved simultaneously . Similar to how natural language processing and object detection are subtasks of neural image caption generation [ 23 ] , reinforcement learning environments also have subtasks relevant to a given environment . These subtasks often include player detection , player control , obstacle detection , enemy detection , and player-object interaction , to name a few . These subtasks are common to many environments , but they are often sufficiently different in function and representation that reinforcement learning algorithms fail to generalize them across environments , such as in Atari . These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments , and are a reason for humans superior data efficiency in learning complex tasks .",0,0,0,0
q15,1706.05749v1_introduction_0004,"Which modeling and architectural design choices are especially important, and why?","These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments , and are a reason for humans superior data efficiency in learning complex tasks . In the case of deliberately similar environments , we can construct the subtasks such that they are similar in function and representation that an agent trained on the first environment can accelerate learning on the second environment due to its preconstructed subtask representations , thus partially avoiding the more complex environment 's increased simulation cost and inherent learning difficulty .",0,0,0,0
q15,1711.01431v1_abstract_0000,"Which modeling and architectural design choices are especially important, and why?","Machine learning is usually defined in behaviourist terms , where external validation is the primary mechanism of learning . In this paper , I argue for a more holistic interpretation in which finding more probable , efficient and abstract representations is as central to learning as performance . In other words , machine learning should be extended with strategies to reason over its own learning process , leading to so-called meta-cognitive machine learning . As such , the de facto definition of machine learning should be reformulated in these intrinsically multiobjective terms , taking into account not only the task performance but also internal learning objectives . To this end , we suggest a `` model entropy function '' to be defined that quantifies the efficiency of the internal learning processes . It is conjured that the minimization of this model entropy leads to concept formation .",0,0,0,0
q15,1711.01431v1_introduction_0003,"Which modeling and architectural design choices are especially important, and why?","One possible analogy to better understand the above statements can be found in software engineering . When considering code that performs a specific task , we do not care only about its functionality , but also about its execution speed/efficiency and other so-called `` non-functional requirements '' . Furthermore , a carefully modularized design probably reflects more understanding than an endless enumeration of IF-ELSE clauses . In other words , finding a more efficient and structured way to represent/reproduce information and to perform a learning task , is as central to machine learning as the reproduction of results . Different to humans , of course , machines are measurable . This provides us with a unique opportunity to study the nature of learning in principle , at the same time improving Machine Intelligence . We are not claiming that model complexity/efficiency has not been subject to past research efforts . On the contrary , many techniques and design principles have attempted to improve exactly these properties -like Occam 's razor , Bayesian structure learning , pruning , the use of prototypes to compact information , regularization as a strategy to reduce energy , weight sharing in RNNs or CNNs to decrease model complexity , etc . Indeed , the whole evolution of Deep Learning can be seen as one specific approach in the quest to find models that are more structured ( i.e .",1,0,0,0
q15,1711.01431v1_introduction_0004,"Which modeling and architectural design choices are especially important, and why?","Indeed , the whole evolution of Deep Learning can be seen as one specific approach in the quest to find models that are more structured ( i.e . have a lower entropy ) , by organizing and training them in a layer-wise fashion [ Bengio , 2009 ] . The focus has been mainly on training algorithms and designing model architectures that are adapted to these kinds of `` deep '' structures [ Deng and Yu , 2014 ] . Similar to efforts in multiobjective machine learning , these techniques are considered as a means to improve ( externally measured ) performance rather than a goal in itself [ Jin and Sendhoff , 2008 ] . We , however , do believe that minimizing the model 's structural complexity and optimizing its efficiency of representation , is not only a means to improve ( externally validated ) performance , but a central pillar to machine intelligence that leads to concept formulation and should be made explicit . In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows .",1,1,0,1
q15,1711.01431v1_introduction_0005,"Which modeling and architectural design choices are especially important, and why?","In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows . The theoretical ideas are laid out and the case for a new operational definition of machine learning is made . We put forward the conjecture that the optimization of model entropy , leads to concept formation . Last , conclusions and further steps to operationalize these concepts are formulated . 2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a ( non- ) parametric model with respect to some performance measure . This view is clearly reflected in the de facto definition of machine learning by Mitchell [ Mitchell , 1997 ] : `` A computer program is said to learn from an experience X with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experimental data D '' . Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures .",0,0,0,0
q15,1711.01431v1_introduction_0006,"Which modeling and architectural design choices are especially important, and why?","Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures . Nevertheless , there is a general consensus that the learning of `` higherorder '' concepts is problematic , and that the solution to this issue is somehow connected to deep architectures that create ever higher forms of abstraction . Experimental research as well as neurological evidence on the organization of the brain , supports this finding [ Bianchini and Scarselli , 2014 ] . The limitation of architecture complexity is preferred , primarily because their behaviour could be understood and the training of more complex or adaptive architectures leads to a explosion of complexity . That was until recently . The recent advanced in so-called `` Deep Learning '' , have focused on training algorithms that are adapted to new kinds of deep architectures [ Deng and Yu , 2014 ] , and heuristic strategies to attain specific structural properties like sparse coding that lead to higher forms of abstractions . With the exception of studies on `` interpretability '' [ Jin and Sendhoff , 2008 ] , structural properties are mainly considered a by-product , a ( desirable ) side effect of the applied training mechanisms . Though the organization and complexity of model topologies is acknowledged to be crucial , current approaches are mainly limited to analysing the data space , i.e .",1,1,1,1
q15,1711.01431v1_introduction_0007,"Which modeling and architectural design choices are especially important, and why?","Though the organization and complexity of model topologies is acknowledged to be crucial , current approaches are mainly limited to analysing the data space , i.e . the implemented regression functions or decision boundaries [ Bianchini and Scarselli , 2014 ] . There is a problem with this approach . Consider an neural network algorithm that needs to learn a simple concept like an `` XOR '' function depicted in Fig . 1 . An infinite number of neural networks with very similar or identical decision boundaries can be constructed -of which two are shown in Fig . 2 . From an external point of view , there is no way to discriminate between these two models : describing the difference between these two models can only occur in terms of the model internals . Of course the weight space , which represents the model of a neural network , is related to the data space , as it performs calculations on the data . In other words : Data representation and model computation should be considered as two sides of the same coin . As a result the structural properties of both the model and data space are key to the modelling of higher abstractions . Sparse coding is a perfect example of this .",0,0,0,0
