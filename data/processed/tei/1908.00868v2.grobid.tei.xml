<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine Learning as Ecology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-08-26">August 26, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Owen</forename><surname>Howell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<addrLine>590 Commonwealth Ave.</addrLine>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cui</forename><surname>Wenping</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<addrLine>590 Commonwealth Ave.</addrLine>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Boston College</orgName>
								<address>
									<addrLine>140 Commonwealth Avenue Chestnut Hill</addrLine>
									<postCode>02467</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Marsland</surname><genName>III</genName></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<addrLine>590 Commonwealth Ave.</addrLine>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pankaj</forename><surname>Mehta</surname></persName>
							<email>pankajm@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<addrLine>590 Commonwealth Ave.</addrLine>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Machine Learning as Ecology</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-26">August 26, 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">EFE957B6CFB12F083BFBE94B98C086C9</idno>
					<idno type="arXiv">arXiv:1908.00868v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning methods have had spectacular success on numerous problems. Here we show that a prominent class of learning algorithms -including Support Vector Machines (SVMs) -have a natural interpretation in terms of ecological dynamics. We use these ideas to design new online SVM algorithms that exploit ecological invasions, and benchmark performance using the MNIST dataset. Our work provides a new ecological lens through which we can view statistical learning and opens the possibility of designing ecosystems for machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Machine learning (ML) is one of the most exciting and useful areas of modern computer science <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. One common machine learning task is classification: given labeled data from one or more categories, predict the category of a new, unlabeled data point. Another common task is to perform outlier detection (i.e. find data points that appear to be irregular). Both of these difficult problems can be solved efficiently using kernel-based methods such as Support Vector Machines (SVMs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>The basic idea behind SVMs is to use a non-linear map to embed the input data in a high-dimensional feature space where it can be classified using a simple linear classifier (see Figure <ref type="figure">1</ref>). To ensure good generalization and avoid overfitting, SVMs focus on the "hardest to classify" points that lie closest to the linear decision surface in the high-dimensional feature space. These points are called "support vectors" and play a prominent role in SVM algorithms.</p><p>The real power and utility of SVMs comes from the fact that these ideas can be implemented quickly and efficiently using kernel methods and quadratic optimization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. The idea of a kernel function is to replace the explicit mapping to a high-dimensional feature space with an implicit kernel function that specifies the similarity (dot product) between data points in the highdimensional feature space. Once the kernel function is specified, the support vectors and decision surface can be easily computed as an instance of a Quadratic Programming (QP) problem. There exist efficient exact and approximate optimization algorithms for QP that scale weakly polynomially in input size.</p><p>The original motivation for SVMs and other kernel methods were deep results in statistical learning theory concerning generalization errors <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Here, we show that these statistical problems can also be understood using ideas from niche theory in community ecology (see Table <ref type="table">I</ref>) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Our construction exploits the recently discovered duality between ecological dynamics and constrained optimization problems, specifically quadratic programming <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. In particular, we show FIG. <ref type="figure">1</ref>: Overview of Support Vector Machines (SVMs). Data points are mapped into a high-dimensional feature space via φ(X) where they can be separated using a linear decision surface. The SVM tries to maximize the distance (margin) from the decision boundary to the nearest data point. Points that lie on the maximum-margin planes (circled) are called support vectors and used to classify new, unlabeled data.</p><p>that data points can be viewed as "species" that compete for resources, with each feature identified with a distinct resource, and the kernel function specifying the niche overlap between species/datapoints <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>This mapping allows us to reinterpret SVMs as complex ecosystems that self-organize into ecologically stable steady states defined by their support vectors. This new ecological perspective naturally leads to a new online algorithms based on ecological invasion for SVMs as well as for outlier detection kernel methods such as Support Vector Data Description (SVDD) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. We also show that our ecological SVDD method is equivalent to the online algorithm derived in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVMS AS QP</head><p>Consider a classification problem where each pdimensional data point x i (i = 1, 2, 3 . . . N ) comes with a binary label t i = ±1. A SVM fits a linear classifier to the data of the form</p><formula xml:id="formula_0">y(x) = w T φ(x) + b<label>(1)</label></formula><p>where φ : R p → R q , q p denotes a mapping to a highdimensional feature space. The scalar offset b and the q-dimensional weight vector w are tunable model parameters.</p><p>A new data point x k is assigned to class t k = +1 if y(x k ) &gt; 0 and to class t k = -1 if y(x k ) &lt; 0. In the main text, we restrict our discussion to linearly separable datasets, i.e., datasets for which exists a hyperplane in the feature space φ(x) that partitions the dataset into two regions with every point in class +1 in one region and every point in class -1 in the other (see <ref type="bibr">Fig 1)</ref>. However, our construction can be easily generalized to non-separable datasets (see Supporting Information).</p><p>SVMs are trained by maximizing the margin, defined as the Euclidean distance from the line y(x) = 0 (the decision boundary) to the nearest data point. It is easy to show that distance from the point x i to the line y(x) = 0 is given by the expression t i y(xi) |w| . Maximizing the margin corresponds to choosing the parameters w and b so that</p><formula xml:id="formula_1">w, b = arg max w,b 1 |w| min i [t i (w T φ(x i ) + b)]<label>(2)</label></formula><p>The above maximization problem can be recast by noting that Equation (2) has a gauge degree of freedom: the decision surface is invariant under the scaling transformation w → Dw and b → Db <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. We can fix this gauge by choosing the margin to be exactly 1. In this gauge, Equation ( <ref type="formula" target="#formula_1">2</ref>) is equivalent to the following convex quadratic programming problem</p><formula xml:id="formula_2">arg min w,b 1 2 |w| 2 subject to t i (w T φ(x i ) + b) ≥ 1 for all i,<label>(3)</label></formula><p>where i labels the N data points in the training dataset.</p><p>As with all constrained optimization problems, we can also solve the equivalent dual optimization problem by introducing generalized Lagrange multipliers a i (often called KKT multipliers in the optimization literature) corresponding to each of the inequality constraints in (3) <ref type="bibr" target="#b15">[16]</ref>. Since there is one constraint per data point i, we can uniquely associate each a i with a data point in the training set. For data points that saturate the inequality in (3), a i is positive, and acts as an ordinary Lagrange multiplier to enforce the constraint. For the rest of the data points, no Lagrange multiplier is required, and a i = 0. These observations give rise to the Karush-Kuhn-Tucker conditions, which are necessary and sufficient to determine the optimum <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>:</p><formula xml:id="formula_3">0 = ∇ w,b L(w, b, a i ) 1 ≤ t i (w T φ(x i ) + b) 0 ≤ a i 0 = a i [t i (w T φ(x i ) + b) -1] (4)</formula><p>where the last three expressions hold for all i, with the SVM Lagrangian</p><formula xml:id="formula_4">L(w, b, a i ) = 1 2 |w| 2 - N i=1 a i [t i (w T φ(x i ) + b) -1]. (5)</formula><p>Solving the first condition for w and b yields the equations w = N i=1 a i t i φ(x i ) and N i=1 a i t i = 0. Inserting these results into Equation <ref type="bibr" target="#b4">(5)</ref> gives equations for optimal a i :</p><formula xml:id="formula_5">argmax ai L(a i ) = N i=1 a i - 1 2 N i,j=1</formula><p>a i a j t i t j K(x i , x j ) subject to 0 ≤ a i for all i and N i=1</p><formula xml:id="formula_6">a i t i = 1<label>(6)</label></formula><p>L(a i ) is called the dual SVM Lagrangian. In writing this equation, we have introduced the kernel function K(x i , x j ) ≡ φ T (x i )φ(x j ) which is just the dot product of the data points in the high-dimensional feature space φ.</p><p>In this dual formulation, the support vectors correspond precisely to those data points x k for which the corresponding KKT multiplier is greater than zero a k &gt; 0. The SVM can be used to classify a new point x using t = sign(y(x)) with</p><formula xml:id="formula_7">y(x) = i∈S t i a i K(x, x i ) + b b = 1 |S| i∈S   t i - j∈S a j t j K(x i , x j )  </formula><p>and S the set of support vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THE ECOLOGY OF SVMS</head><p>Consider the maximization of the dual Lagrangian L(a i ) given in Equation ( <ref type="formula" target="#formula_6">6</ref>), subject to the constraints N i=1 a i t i = 0 and a i ≥ 0. Recently, it was shown there exists a duality between constrained optimization and ecological dynamics <ref type="bibr" target="#b7">[8]</ref>. Using this duality, it is straightforward to show that the solution to this problem is encoded in the steady state of a generalized Lotka-Volterra equation of the form</p><formula xml:id="formula_8">da i dt = a i   1 + λt i - N j=1 t i t j K(x i , x j )a j   dλ dt = - N i=1 a i t i ,<label>(7)</label></formula><p>This system of differential equations has a natural ecological interpretation as the dynamics of N species with</p><p>SVM Ecology Data point Species KKT Multiplier Species Abundance Feature Space Trait Space Kernel Niche Overlap Support Vectors Species that survive in ecosystem TABLE I: Conceptual mapping between SVMs and ecology abundances a i (i = 1 . . . N ) whose interactions are represented by the matrix α ij with elements</p><formula xml:id="formula_9">α ij = t i t j K(x i , x j ). (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>Since each a i corresponds to a data point, we can think of this as an ecological network where data points i and j from the same class ( t i = t j ) compete with each other (i.e. α ij &gt; 0) whereas species of from different classes (t i = -t j ) are mutualistic (i.e. α ij &lt; 0). The level of competition or mutualism depends on the overlap kernel K(x i , x j ) with similar data points having stronger interactions.</p><p>Ecologically, a combination of competitive and mutualistic interactions such as these naturally occur in plantpollinator networks <ref type="bibr" target="#b16">[17]</ref>. In such networks, different species of plants compete with each other for pollinators, pollinators compete with each other for plants, and plantpollinators interactions are beneficial for both kinds of species. The λ term corresponds to an abiotic environmental factor that is produced or consumed by different species. In this plant-pollinator analogy, λ could represent an environmental CO 2 concentration. Specifically, plants consume CO 2 and benefit from high CO 2 concentration while pollinators produce CO 2 and are harmed by high CO 2 concentration.</p><p>Note that this interpretation differs from the consumer-resource interpretation given to a generic constrained optimization problem in <ref type="bibr" target="#b7">[8]</ref>. The Lagrange multiplier λ plays the role of a "resource," but is not required to be positive, since it is enforcing an equality constraint rather than an inequality. The variables a i of the optimization are now treated as the species rather than as resources.</p><p>These observations suggest a new ecological interpretation of SVMs (see Table <ref type="table">I</ref>). Data points act like species that either compete or promote each others' survival. The abundance of each species is the value of KKT multiplier that enforces the corresponding constraint in (3). Since only the support vectors have non-zero KKT multipliers, the only data points that survive in the ecosystem are support vectors. As noted above, data points from the same category compete with each whereas data points from different categories are mutualistic. As is widely appreciated in the ecological literature, the ecological dynamics depends only on the overlap of resource utilization function encoded in the similarity kernel K(x i , x j ) between points <ref type="bibr" target="#b10">[11]</ref>. The data points most likely to survive in the ecosystem are data points from one category that are similar to data points from the opposite category since they have large mutualistic interactions. For this reason, the data points that survive in the ecosystem are precisely those lie near the boundary between the two categories, that is, the support vectors.</p><p>This mapping can also be easily generalized to the case where the data is not linearly separable, and to Support Vector Data Description (SVDD) algorithms <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> for outlier detection (see SI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ECOSVM: AN ONLINE ALGORITHM</head><p>One interesting class of processes that has been extensively studied in the ecological literature is ecological invasion <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. In the context of SVMs, invasion by new species corresponds to addition of a new data point (x 0 , t 0 ) to our existing dataset. If we denote the existing support vectors by the set S, the condition for a successful invasion is the intuitive statement that the initial growth rate must be positive when the new data point is introduced into the ecosystem:</p><formula xml:id="formula_11">0 &lt; 1 a 0 da 0 dt = 1 + λt 0 - j∈S t 0 t j K(x 0 , x j )a j .<label>(9)</label></formula><p>When this equation is satisfied the new data point can successfully invade the ecosystem and fixate (i.e. become a support vector). If the condition is not satisfied, the point goes "extinct" and the set of support vectors does not change. If a data point can invade successfully, the species abundances "a i " are modified and can be found by solving for the steady state of (7) using either forward integration or quadratic programming <ref type="bibr" target="#b7">[8]</ref>. This suggests a simple new approximate algorithm for online SVM learning we term the EcoSVM. In online learning, rather than seeing all the data at once, training data is presented in a sequential pattern. In the EcoSVM algorithm when a new training data point is presented, the invasion condition ( <ref type="formula" target="#formula_11">9</ref>) is used to determine whether it can successfully invade the ecosystem. If it cannot, the training data point is discarded. If it can, we recompute the steady-states using Equation <ref type="bibr" target="#b6">(7)</ref>. This algorithm can be easily generalized to the case of non-separable data (see Supporting Information and attached code implementing the algorithm).</p><p>Because we use the ecologically inspired invasion condition, there is no need to recompute the support vectors at each learning step, resulting in a faster and more memory-efficient online algorithm than those that were previously suggested <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. The EcoSVM algorithm also reduces the amount of training data that needs to be stored in memory. Specifically, instead of needing to store all data points, we keep only the support vectors. Since the number of support vectors is in general a small subset of all the training data, this greatly reduces the memory requirements. This increased efficiency comes at the expense of introducing small errors that come from the contingent nature of ecological invasions. Occasionally, a successful invasion by a new species (new data point) will allow a species that could not previously invade the ecosystem (designated not a support vector) to become viable (a support vector). This kind of historical contingency introduces errors in our online algorithm since we discard all data points that do not fix in the ecosystem. In practice, we find that these errors are generally quite small for real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NUMERICAL TESTS</head><p>We tested EcoSVM using several numerical examples. We started with two toy datasets with N = 200 data points x = (c 1 , c 2 ) drawn uniformly from a twodimensional hypercube x ∈ [0, 1] 2 . We enforced linear separability using a decision boundary given by c 1 = 1/2 so that data points with c 1 &lt; 1/2 were assigned to one class t = -1 and those with c 1 &gt; 1/2 were assigned to a different class t = 1 (see Figure <ref type="figure" target="#fig_0">2</ref>). We also created a dataset where the decision boundary was given by c 1 = 1 2 + 1 10 sin(2πc 2 ), which is not linearly separable in the given feature space. After initialization using the first 10 data points, we trained an EcoSVM using the online scheme described above. As can be seen visually in Figure <ref type="figure" target="#fig_0">2</ref>, the decision boundaries found by the EcoSVM were very similar to those found using an ordinary batch SVM algorithm. Furthermore, the final accuracy and number of support vectors of the EcoSVM algorithms and ordinary batch SVM algorithm were almost identical (See SI for more detail).</p><p>Next, we tested the performance of EcoSVM algorithm on MNIST <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, a standard benchmark dataset in machine learning. The MNIST dataset consists of 6, 000 training images and 1, 000 test images of each of the handwritten digits '0'-'9'. To test the EcoSVM, we considered the binary classification task of distinguishing fours and nines. For this classification problem, we used a standard Gaussian (RBF) kernel given by</p><formula xml:id="formula_12">K σ (x, y) = exp -1 2σ 2 (x -y) 2</formula><p>, where the kernel width σ was determined via cross validation on the batch SVM. The performance of our EcoSVM algorithm was comparable to a traditional SVM trained on the full dataset ( 98.1% accuracy compared to 98.5% accuracy for traditional SVMs, as shown in Figure <ref type="figure">3</ref>). The EcoSVM algorithm also ends up finding a similar number of support vectors as a traditional SVM: ∼ 750. We note that since the MNIST dataset is not completely linearly separable in the RBF feature space, in these numerical simulations we used the generalization of the EcoSVM algorithm for non-linearly separable datasets discussed in the SI. FIG. <ref type="figure">3</ref>: An ecologically inspired online SVM algorithm EcoSVM applied to digit classification of ones and fours from the MNIST dataset <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Accuracy of EcoSVM for 25 different realizations. The average accuracy over all realizations is shown using the solid blue line and the accuracy of an SVM trained on the entire dataset is shown using the blue dotted line. The inset panel shows the number of active support vectors in each realization (black lines), the mean number of support vectors across all realizations (blue solid line) and the number of support vectors for SVM trained on full dataset (dotted blue line).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>In this work, we have shown how we can think about kernel methods using ideas from ecology. This ecological mapping allowed us to formulate a new ecologically inspired online SVM algorithm, the EcoSVM. We have shown that the performance of EcoSVM is comparable to traditional SVMs that work with all data simultaneously. Our algorithm differs from previous online SVMs which must recompute the support vectors at each learning step <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> allowing for faster implementation and smaller memory requirements. While in the main text we focus on linearly separable data, as shown in the SI these same ideas can be generalized to non-separable data and for outlier detection (SVDD) using unlabeled data. Our results suggest that ecological dynamics may provide a rich new setting for thinking about biologically-inspired machine learning. They also suggest the tantalizing possibility that it maybe possible to engineer synthetic ecosystems that implement sophisticated statistical learning algorithms <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>OH acknowledges support from BU UROP student funding. The work was supported by NIH NIGMS grant 1R35GM119461, Simons Investigator in the Mathematical Modeling of Living Systems (MMLS) to PM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting Information</head><p>Code implementing the EcoSVM algorithm is found at <ref type="url" target="https://github.com/owenhowell20/EcoSVM">https://github.com/owenhowell20/EcoSVM</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADDING THE SLACK</head><p>In the main text we have focused on datasets that are linearly separable. For the majority of practical applications this is not the case. For overlapping class distributions, the primal SVM problem is modified so that points are allowed to be on the wrong side of the margin. Specifically, slack variables ζ i ≥ 0 are introduced with t i y(x i ) ≥ 1 -ζ i . This should be compared with the linearly separable case where the constraint is instead t i y(x i ) ≥ 1. The new minimization is weighted to penalize points that lie on the wrong side of the margin arg min w,b,ζi</p><formula xml:id="formula_13">1 2 |w| 2 + C N i=1 ζ i subject to t i (w T φ(x i ) + b) ≥ 1 for all i,<label>(10)</label></formula><p>where the slack parameter C determines the extent to which points on the wrong side of the margin are tolerated. In practice, C is a hyper-parameter that is tuned to minimize generalization error. The KKT conditions for this new minimization problem are:</p><formula xml:id="formula_14">0 = ∇ w,b,ζi L(w, b, ζ i , a i , µ i ) 1 -ζ i ≤ t i y(x i ) 0 ≤ a i 0 = a i [t i y(x i ) -1 + ζ i ] 0 ≤ ζ i 0 ≤ µ i 0 = µ i ζ i (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>where the µ i are additional KKT multipliers enforcing the constraints ζ i ≥ 0, and the primal Lagrangian is</p><formula xml:id="formula_16">L(w, b, ζ i , a i , µ i ) = 1 2 |w| 2 + N i=1 [Cζ i -a i (t i y(x i ) -1 + ζ i ) -µ i ζ i ]</formula><p>Minimizing the Lagrangian ∂L ∂wi = 0 , ∂L ∂b = 0 and ∂L ∂ζi = 0 gives equations</p><formula xml:id="formula_17">w = N i=1 t i a i φ(x i ) , N i=1 a i t i = 0 , a i = C -µ i (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>Each µ i ≥ 0 so the last equation is equivalent to a i ≤ C. Inserting these results into the primal Lagrangian transforms the problem into maximization of the dual SVM Lagrangian</p><formula xml:id="formula_19">argmax ai L(a i ) = N i=1 a i - 1 2 N i,j=1 a i a j t i t j K(x i , x j ) subject to 0 ≤ a i ≤ C for all i and N i=1 a i t i = 1 (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>We can enforce the second constraint by introducing a Lagrange multiplier λ, resulting in the following set of equations for the optimal a i :</p><formula xml:id="formula_21">argmax ai,λ L(a i , λ) subject to 0 ≤ a i ≤ C for all i (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>with Lagrangian</p><formula xml:id="formula_23">L(a i , λ) = N i=1 a i - 1 2 N i,j=1 a i a j t i t j K(x i , x j ) + λ N i=1 t i a i<label>(15</label></formula><p>) Using the duality described in <ref type="bibr" target="#b7">[8]</ref>, we can map the quadratic programming problem ( <ref type="formula" target="#formula_21">14</ref>) to ecological dynamics</p><formula xml:id="formula_24">da i dt = a i (C -a i )(1 + λt i - N j=1 t i t j K(x i , x j )a j ) dλ dt = - N i=1 a i t i .<label>(16)</label></formula><p>where the prefactor a i (C -a i ) enforces the constraints on a i . Equation ( <ref type="formula" target="#formula_24">16</ref>) has a similar interpretation to the Lotka-Volterra equations for the linearly separable case, with the additional (C -a i ) factor can be interpreted as each species having a maximum carrying capacity C <ref type="bibr" target="#b29">[30]</ref>. Now consider the addition of new point P 0 = (x 0 , t 0 ). This point changes the set of support vectors if the initial growth rate is positive.</p><formula xml:id="formula_25">0 &lt; 1 a 0 da 0 dt = 1 + λt 0 - N j=1 t 0 t j K(x 0 , x j )a j<label>(17)</label></formula><p>Let x k be any "active" support vector, that is, a point whose KKT multiplier a k satisfies C &gt; a k &gt; 0. Then, solving the steady state equation ( <ref type="formula" target="#formula_24">16</ref>) for auxiliary variable λ gives</p><formula xml:id="formula_26">λ = -t k + N i=1 t i K(x i , x k )a i (<label>18</label></formula><formula xml:id="formula_27">)</formula><p>Inserting this into Equation <ref type="bibr" target="#b16">(17)</ref> gives the invasion condition</p><formula xml:id="formula_28">0 &lt; 1 a 0 da 0 dt = 1 -t k t 0 + N i=1 t i t 0 (K(x i , x k ) -K(x i , x 0 ))a i<label>(19)</label></formula><p>This invasion condition can be used to construct an online learning algorithm. Specifically, when a new data point is presented, the condition ( <ref type="formula" target="#formula_28">19</ref>) can be used to determine whether the new point changes the set of support vectors without having to recompute the minimum of ( <ref type="formula" target="#formula_19">13</ref>). The nonzero KKT multipliers a i &gt; 0 and corresponding support vectors x i are kept in memory.</p><p>A new point x is classified using t = sign(y(x)) with</p><formula xml:id="formula_29">y(x) = i t i a i K(x, x i ) + b b = 1 |M | i∈M   t i - j∈S a j t j K(x i , x j )  </formula><p>where S is the full set of support vectors and M is the subset of active support vectors. Note that this formula requires at least one active support vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PERFORMANCE ON TOY MODELS</head><p>We test our proposed online learning algorithms on two toy datasets. We consider one dataset that is linearly separable in the feature space φ(x) = x. Specifically, we choose all data points to be drawn from the [0, 1] p p-dimensional hypercube. We then define the decision surface:</p><formula xml:id="formula_30">B 1 : (x 1 = 1 2 , x 2 , ...x p )</formula><p>We consider a second dataset that is not linearly separable. Specifically, we define the second dataset to have decision boundary given by:</p><formula xml:id="formula_31">B 2 : (x 1 = 1 2 + 1 10 sin(2πx 2 ) sin(2πx 3 )... sin(2πx p ), x 2 , ...x p )</formula><p>To test our proposed algorithm, we draw N points from the p dimensional hypercube. The minimum of the SVM Lagrangian is found for N s points with N s N . We require that N s is greater then p or else there are flat directions and our algorithm can become unstable. At each step, a new point is presented and the invasion condition ( <ref type="formula" target="#formula_25">17</ref>) is used to determine whether the set of support vectors is changed. If ( <ref type="formula" target="#formula_25">17</ref>) is satisfied, the steady state is recomputed using quadratic programming. This is continued for all N points. We find an excellent agreement between the predictions of our online algorithm and an batch SVM trained using all N points for both the linearly separable and non-linearly separable datasets.</p><p>We study how the test accuracy and number of support vectors depend on the training epoch T . For this purpose, let us define the accuracy</p><formula xml:id="formula_32">A(T ) = 1 - 1 |N test | x∈Ntest 1 2 |t T (x) -t Exact (x)|</formula><p>where N test is the set of testing data and t Exact (x) is the true label corresponding to point x. t T (x) denotes the prediction of the online SVM trained with T data points.</p><p>In addition, let us define</p><formula xml:id="formula_33">N (T ) =         </formula><p>Number of a i (T ) &gt; 0 for linearly seperable case Number of C &gt; a i (T ) &gt; 0 for non-linearly seperable case where a i (T ) are the support vector coefficients of the online SVM trained with T data points. In both the linear and non-linear case N (T ) counts the number of active support vectors. Figure <ref type="figure" target="#fig_1">4</ref> and 5 show that in both the linear and nonlinear case for large T the online algorithm converges to an accuracy A(T ) that is just below the accuracy of a batch SVM. Furthermore the number of active support vectors that the the online method finds after training is slightly below the number of support vectors that the batch SVM has. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ECOLOGY TO SVDD</head><p>We show how the method presented in the main text can be used to derive an approximate online SVDD learning algorithm first constructed by Jiang et al. in 2017 <ref type="bibr" target="#b14">[15]</ref>.</p><p>The Support Vector Data Description (SVDD) problem is concerned with unsupervised location of outliers in single-class classification problems (see <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> for a good overview). The SVDD problem consists finding a FIG. 6: Schematic showing instances of a supervised classification problem (left) and an outlier detection problem (right). In the supervised classification problem the goal is to partition the space into two distinct volumes, one for each label. The thick black line denotes the decision boundary. An incorrectly classified four (inside the red hexagon) is shown on the wrong side of the decision boundary. In the outlier detection problem, a sphere is created in the feature space to enclose the minimum volume while still containing all data points (in this case fours). Points on the boundary of this sphere are called outliers, they are starred in the schematic. The thick black line denotes the sphere boundary.</p><p>sphere of minimum radius in kernel space that contains all data points. Figure <ref type="figure">6</ref> shows schematically a classification problem and an outlier detection problem. Points that lie on the surface of the sphere are called outliers and are analogous to active support vectors in the SVM problem.</p><p>The problem is formulated as follows. Given a set of unlabeled data points</p><formula xml:id="formula_34">D = (x i ) N i=1 : minimize R 2 subject to |φ(x i ) -µ| 2 ≤ R 2 for all i (<label>20</label></formula><formula xml:id="formula_35">)</formula><p>where R is the sphere radius in feature space φ(x) and µ is the center of the sphere in the feature space. This problem is simplified by the introduction of KKT multipliers a i for each inequality constraint. The KKT conditions for the minimization problem are:</p><formula xml:id="formula_36">0 = ∇ R,µ L(R, µ, a i ) 0 ≤ a i |φ(x i ) -µ| 2 -R 2 ≤ 0 a i [|φ(x i ) -µ| 2 -R 2 ] = 0 (21) with L(R, µ, a i ) = R 2 + N i=1 a i (|φ(x i ) -µ| 2 -R 2 )</formula><p>Minimizing L(R, µ, a i ) with respect to µ and R gives equations:</p><formula xml:id="formula_37">N i=1 a i = 1 and µ = N i=1 a i φ(x i )<label>(22)</label></formula><p>Substituting Equation <ref type="bibr" target="#b21">(22)</ref> into Equation ( <ref type="formula">21</ref>) gives maximization problem for the optimal a i :</p><formula xml:id="formula_38">argmax ai L(a i ) = N i=1 a i K(x i , x i ) - N i,j=1 a i a j K(x i , x j ) subject to 0 ≤ a i for all i and N i=1 a i = 1<label>(23)</label></formula><p>L(a i ) is called the dual SVDD Lagrangian and K(x i , x j ) = φ(x i ) T φ(x j ). For simplicity we set K(x i , x i ) = 1 for the diagonal elements in the rest of the derivation, although the results we present are easily generalized for arbitrary kernel. The Python code included in the supplemental material works for any choice of K(x, y).</p><p>After the SVDD is trained, the sphere radius in feature space R can be determined via</p><formula xml:id="formula_39">R 2 = max i |φ(x i ) -µ| 2 = max i (φ(x i ) -µ) T (φ(x i ) -µ) = max i (φ(x i ) T φ(x i ) -2µ T φ(x i ) + µ T µ)<label>(24)</label></formula><p>The explict dependence on φ(x) in ( <ref type="formula" target="#formula_39">24</ref>) can be removed using</p><formula xml:id="formula_40">µ = N i=1 a i φ(x i ) : φ(x i ) T φ(x i ) -2µ T φ(x i ) + µ T µ = 1 -2 N j=1 K(x i , x j )a j + N j,k=1 K(x j , x k )a j a k</formula><p>where we have used φ(x i ) T φ(x j ) = K(x i , x j ) and K(x i , x i ) = 1. Thus, (24) can be written in terms of kernel function and support vectors as</p><formula xml:id="formula_41">R 2 = max i 1 -2 N j=1 K(x i , x j )a j + N j,k=1 K(x j , x k )a j a k<label>(25)</label></formula><p>We apply our method to the SVDD Lagrangian 23. As K(x i , x i ) = 1 and N i=1 a i = 1 the first term in the sum can be ignored. A Lagrange multiplier λ is introduced to enforce the latter constraint. The quantity to be maximized is then</p><formula xml:id="formula_42">argmax ai,λ L(a i , λ) = - N i,j=1 a i a j K(x i , x j ) + λ( N i=1 a i -1) subject to 0 ≤ a i for all i<label>(26)</label></formula><p>Using the quadratic programming-ecology duality, we can embed the solution to <ref type="bibr" target="#b22">(23)</ref> as the steady state of the dynamical equations</p><formula xml:id="formula_43">da i dt = a i (λ - N j=1 K(x i , x j )a j ) dλ dt = 1 - N i=1 a n<label>(27)</label></formula><p>Now, suppose we are at the steady state of Equation ( <ref type="formula" target="#formula_43">27</ref>) and consider the addition of a new point x 0 . The invasion condition is that the initial growth rate is positive</p><formula xml:id="formula_44">0 &lt; 1 a 0 da 0 dt = λ - N j=1 K(x 0 , x j )a j<label>(28)</label></formula><p>Let x k be any point which has non-zero support vector, a k &gt; 0. Then, solving the steady state equation ( <ref type="formula" target="#formula_43">27</ref>) for auxiliary variable λ gives</p><formula xml:id="formula_45">λ = N i=1 K(x i , x k )a i<label>(29)</label></formula><p>Inserting this into Equation <ref type="bibr" target="#b27">(28)</ref> gives the invasion condition</p><formula xml:id="formula_46">0 &lt; 1 a 0 da 0 dt = N i=1 a i [K(x k , x i ) -K(x 0 , x i )]<label>(30)</label></formula><p>which is identical to Equation (2.9) derived in <ref type="bibr" target="#b14">[15]</ref> (Note that they use notation z for our variable x 0 ). Equation <ref type="bibr" target="#b29">(30)</ref> can be used to formulate an online learning algorithm in the same manner as the EcoSVM algorithm presented in the main text. The paper by Jiang et al. derives Equation <ref type="bibr" target="#b29">(30)</ref> and calls the algorithm Fast Incremental Support Vector Data Description (FISVDD). They also numerically shows that this algorithm performs well on real-world datasets. The fact that this algorithm can be constructed simply and elegantly using a duality between quadratic programming and ecology suggests that there is a deeper connection between machine learning and ecological dynamics than previously realized and, more significantly, that ecologically inspired machine learning models are not just of theoretical interest but can be used for real world data analysis.</p><p>We illustrate the ability of FISVDD/EcoSVDD numerically. We draw data points from a p-dimensional multinomial Gaussian distribution with identity covariance matrix and mean uniformly sampled from the pdimensional hypercube x ∈ [0, 1] p . Figure <ref type="bibr" target="#b6">(7)</ref> shows the two-dimensional case.</p><p>We define R(T ) to be the SVDD radius <ref type="bibr" target="#b24">(25)</ref> of an FISVDD/EcoSVDD trained on T points. The left panel of Figure <ref type="figure" target="#fig_3">7</ref> shows R(T ) as a function of T . R(T ) converges to the batch SVDD radius (shown with a dashed blue line).</p><p>Similarly, we define a similarity metric between the FISVDD/EcoSVDD kernel sphere center trained on T points µ(T ) <ref type="bibr" target="#b21">(22)</ref> and the batch SVDD sphere center μ as  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 2 :</head><label>2</label><figDesc>FIG. 2: Comparison of classical SVM and EcoSVM algorithms for a data set with N = 200 total training points points for (Left) a linearly separable dataset and (Right) a dataset where the data is not linearly separable (see SI for more detail on algorithm in non-linear case). The true decision boundary is given by dashed black line. Cyan regions show data that the full SVM and online SVM both identify as ti = 1. Blue regions show data that full SVM and online SVM both identify as ti = -1. Purple regions show area in which SVM and online SVM disagree. ti = 1 data points are shown as green plus symbols, ti = -1 data points are shown as red circles. Active support vectors are shown with larger symbols.</figDesc><graphic coords="4,317.93,52.08,120.08,90.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 4 :</head><label>4</label><figDesc>FIG. 4: Test accuracy and number of support vectors as a function of time for the linearly separable toy model, with true decision boundary B1 defined in the SI text. Left panel shows accuracy of online SVM algorithm A(T ) as a function of the number of points T that the online SVM has seen. Black lines show individual realization, blue line shows mean accuracy. Dotted blue line shows full SVM accuracy. Right panel shows the number of support vectors N (T ) as a function of the number of points T . Black lines show individual realization, blue line shows mean number of support vectors. Dotted blue line shows number of support vectors in SVM trained on entire dataset at once. The dimension of the data space is p = 100 and the online training is initialized with Ns = 30 data points.</figDesc><graphic coords="7,55.99,315.36,122.54,91.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG. 5 :</head><label>5</label><figDesc>FIG.5: Same as Figure4, but for the non-linearly separable toy model, with true decision boundary B2 defined in the SI text. The dimension is p = 30 and the initial number of points is Ns = 30.</figDesc><graphic coords="7,319.01,52.07,122.54,91.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIG. 7 :</head><label>7</label><figDesc>FIG. 7: Comparison of batch SVDD and online SVDD algorithms for a data set with N = 100 total points (shown in black) drawn from a Gaussian distribution. Batch SVDD active support vectors are shown with red "+" symbols. EcoSVDD active support vectors are shown with green stars. The kernel function is Gaussian K(x, y) = exp( -12 (x-y) T (xy)). The EcoSVDD algorithm was started with 10 points.</figDesc><graphic coords="9,54.00,52.07,245.08,183.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>S</head><figDesc>(T ) = µ(T ) T μ µ(T ) T µ(T ) μT μ(31)S(T ) ≤ 1 with equality if and only if µ(T ) = μ. The right panel of Figure8shows S(T ) as a function of T . S(T ) converges to 1 illustrating the fact that the FISVDD/EcoSVDD and batch SVDD produce the same kernel sphere center and radius.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG. 8 :</head><label>8</label><figDesc>FIG. 8: Online SVDD radius and kernel sphere center similarity score as function of T . Left panel shows online SVDD radius R(T ) as a function of the number of points T that the online SVM has seen. Black lines show individual realization, blue line shows mean radius. Dotted blue line shows batch SVDD radius. Right panel shows the normalized dot product between µ(T ) and μ. Black lines show individual realizations, blue line shows average over realizations. The dimension of the data space is p = 15 and the online training is initialized with Ns = 30 data points.</figDesc><graphic coords="9,319.01,52.07,122.54,91.91" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern recognition and machine learning</title>
		<imprint>
			<publisher>springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Reports</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">273</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer science &amp; business media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Macarthur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Population Biology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Chesson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Population Biology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marsland</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.99.052111</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">52111</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Macarthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Naturalist</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">3949</biblScope>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mehta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03957</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Macarthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Naturalist</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">377</biblScope>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Futuyma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">567</biblScope>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<title level="m">Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS&apos;99</title>
		<meeting>the 12th International Conference on Neural Information Processing Systems, NIPS&apos;99<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="582" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:MACH.0000008084.60811.49</idno>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kakde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhuri</surname></persName>
		</author>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3991" to="3998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandernberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Valdovinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moisset De Espans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramos-Jiliberto</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1600-0706.2012.20830.x</idno>
	</analytic>
	<monogr>
		<title level="j">Oikos</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Resource competition and community structure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tilman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chesson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Case</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.87.24.9610</idno>
		<ptr target="https://www.pnas.org/content/87/24/9610.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">9610</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cauwenbergus</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1011.1576</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1909</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NNSP&apos;03</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003. 2003</date>
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stockholmsmssan</forename><surname>Pmlr</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, Machine Learning Research<address><addrLine>Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Sohrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raitoharju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iosifidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03989</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zomorrodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Segre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">428</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Valdovinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Berlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M D</forename><surname>Espanes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramos-Jiliberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Physics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">424</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
