<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-Learning: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-10-08">8 Oct 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
							<email>j.vanschoren@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<addrLine>5600MB Eindhoven</addrLine>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-Learning: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-10-08">8 Oct 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">7326A1187ECE5185009B245861E08FED</idno>
					<idno type="arXiv">arXiv:1810.03548v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When we learn new skills, we rarely -if ever -start from scratch. We start from skills learned earlier in related tasks, reuse approaches that worked well before, and focus on what is likely worth trying based on experience <ref type="bibr" target="#b86">(Lake et al., 2017)</ref>. With every skill learned, learning new skills becomes easier, requiring fewer examples and less trial-and-error. In short, we learn how to learn across tasks. Likewise, when building machine learning models for a specific task, we often build on experience with related tasks, or use our (often implicit) understanding of the behavior of machine learning techniques to help make the right choices.</p><p>The challenge in meta-learning is to learn from prior experience in a systematic, datadriven way. First, we need to collect meta-data that describe prior learning tasks and previously learned models. They comprise the exact algorithm configurations used to train the models, including hyperparameter settings, pipeline compositions and/or network architectures, the resulting model evaluations, such as accuracy and training time, the learned model parameters, such as the trained weights of a neural net, as well as measurable properties of the task itself, also known as meta-features. Second, we need to learn from this prior meta-data, to extract and transfer knowledge that guides the search for optimal models for new tasks. This chapter presents a concise overview of different meta-learning approaches to do this effectively.</p><p>The term meta-learning covers any type of learning based on prior experience with other tasks. The more similar those previous tasks are, the more types of meta-data we can leverage, and defining task similarity will be a key overarching challenge. Perhaps needless to say, there is no free lunch <ref type="bibr" target="#b191">(Wolpert and Macready, 1996;</ref><ref type="bibr" target="#b59">Giraud-Carrier and Provost, 2005)</ref>. When a new task represents completely unrelated phenomena, or random noise, leveraging prior experience will not be effective. Luckily, in real-world tasks, there are plenty of opportunities to learn from prior experience.</p><p>In the remainder of this chapter, we categorize meta-learning techniques based on the type of meta-data they leverage, from the most general to the most task-specific. First, in Section 2, we discuss how to learn purely from model evaluations. These techniques can be used to recommend generally useful configurations and configuration search spaces, as well as transfer knowledge from empirically similar tasks. In Section 3, we discuss how we can characterize tasks to more explicitly express task similarity and build meta-models that learn the relationships between data characteristics and learning performance. Finally, Section 4 covers how we can transfer trained model parameters between tasks that are inherently similar, e.g. sharing the same input features, which enables transfer learning (Pan and <ref type="bibr" target="#b114">Yang, 2010)</ref> and few-shot learning <ref type="bibr" target="#b130">(Ravi and Larochelle, 2017)</ref>.</p><p>Note that while multi-task learning <ref type="bibr" target="#b24">(Caruana, 1997)</ref> (learning multiple related tasks simultaneously) and ensemble learning <ref type="bibr" target="#b35">(Dietterich, 2000)</ref> (building multiple models on the same task), can often be meaningfully combined with meta-learning systems, they do not in themselves involve learning from prior experience on other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learning from Model Evaluations</head><p>Consider that we have access to prior tasks t j ∈ T , the set of all known tasks, as well as a set of learning algorithms, fully defined by their configurations θ i ∈ Θ; here Θ represents a discrete, continuous, or mixed configuration space which can cover hyperparameter settings, pipeline components and/or network architecture components. P is the set of all prior scalar evaluations P i,j = P (θ i , t j ) of configuration θ i on task t j , according to a predefined evaluation measure, e.g. accuracy, and model evaluation technique, e.g. cross-validation. P new is the set of known evaluations P i,new on a new task t new . We now want to train a meta-learner L that predicts recommended configurations Θ * new for a new task t new . The meta-learner is trained on meta-data P ∪ P new . P is usually gathered beforehand, or extracted from meta-data repositories <ref type="bibr" target="#b178">(Vanschoren et al., 2014</ref><ref type="bibr" target="#b180">(Vanschoren et al., , 2012))</ref>. P new is learned by the meta-learning technique itself in an iterative fashion, sometimes warm-started with an initial P ′ new generated by another method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task-Independent Recommendations</head><p>First, imagine not having access to any evaluations on t new , hence P new = ∅. We can then still learn a function f : Θ × T → {θ * k }, k = 1..K, yielding a set of recommended configurations independent of t new . These θ * k can then be evaluated on t new to select the best one, or to warm-start further optimization approaches, such as those discussed in Section 2.3.</p><p>Such approaches often produce a ranking, i.e. an ordered set θ * k . This is typically done by discretizing Θ into a set of candidate configurations θ i , also called a portfolio, evaluated on a large number of tasks t j . We can then build a ranking per task, for instance using success rates, AUC, or significant wins <ref type="bibr">(Brazdil et al., 2003a;</ref><ref type="bibr" target="#b34">Demšar, 2006;</ref><ref type="bibr" target="#b89">Leite et al., 2012)</ref>. However, it is often desirable that equally good but faster algorithms are ranked higher, and multiple methods have been proposed to trade off accuracy and training time <ref type="bibr">(Brazdil et al., 2003a;</ref><ref type="bibr" target="#b174">van Rijn et al., 2015)</ref>. Next, we can aggregate these single-task rankings into a global ranking, for instance by computing the average rank <ref type="bibr" target="#b94">(Lin, 2010;</ref><ref type="bibr" target="#b0">Abdulrahman et al., 2018</ref>) across all tasks. When there is insufficient data to build a global ranking, one can recommend subsets of configurations based on the best known configurations for each prior task <ref type="bibr" target="#b171">(Todorovski and Dzeroski, 1999;</ref><ref type="bibr" target="#b73">Kalousis, 2002)</ref>, or return quasi-linear rankings <ref type="bibr" target="#b30">(Cook et al., 1996)</ref>.</p><p>To find the best θ * for a task t new , never before seen, a simple anytime method is to select the top-K configurations <ref type="bibr">(Brazdil et al., 2003a)</ref>, going down the list and evaluating each configuration on t new in turn. This evaluation can be halted after a predefined value for K, a time budget, or when a sufficiently accurate model is found. In time-constrained settings, it has been shown that multi-objective rankings (including training time) converge to near-optimal models much faster <ref type="bibr" target="#b0">(Abdulrahman et al., 2018;</ref><ref type="bibr" target="#b174">van Rijn et al., 2015)</ref>, and provide a strong baseline for algorithm comparisons <ref type="bibr" target="#b0">(Abdulrahman et al., 2018;</ref><ref type="bibr" target="#b89">Leite et al., 2012)</ref>.</p><p>A very different approach to the one above is to first fit a differentiable function f j (θ i ) = P i,j on all prior evaluations of a specific task t j , and then use gradient descent to find an optimized configuration θ * j per prior task <ref type="bibr">(Wistuba et al., 2015a)</ref>. Assuming that some of the tasks t j will be similar to t new , those θ * j will be useful for warm-starting Bayesian optimization approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Configuration Space Design</head><p>Prior evaluations can also be used to learn a better configuration space Θ * . While again independent from t new , this can radically speed up the search for optimal models, since only the more relevant regions of the configuration space are explored. This is critical when computational resources are limited, and proves to be an important factor in practical comparisons of AutoML systems <ref type="bibr" target="#b33">(De Sa et al., 2017)</ref>.</p><p>First, in the functional ANOVA <ref type="bibr">(Hutter et al., 2014a)</ref> approach, hyperparameters are deemed important if they explain most of the variance in algorithm performance on a given task. van Rijn and Hutter (2018) evaluated this technique using 250,000 OpenML experiments with 3 algorithms across 100 datasets.</p><p>An alternative approach is to first learn an optimal hyperparameter default setting, and then define hyperparameter importance as the performance gain that can be achieved by tuning the hyperparameter instead of leaving it at that default value. Indeed, even though a hyperparameter may cause a lot of variance, it may also have one specific setting that always results in good performance. <ref type="bibr" target="#b123">Probst et al. (2018)</ref> do this using about 500,000 OpenML experiments on 6 algorithms and 38 datasets. Default values are learned jointly for all hyperparameters of an algorithm by first training surrogate models for that algorithm for a large number of tasks. Next, many configurations are sampled, and the configuration that minimizes the average risk across all tasks is the recommended default configuration. Finally, the importance (or tunability) of each hyperparameter is estimated by observing how much improvement can still be gained by tuning it. <ref type="bibr" target="#b186">Weerts et al. (2018)</ref> learn defaults independently from other hyperparameters, and defined as the configurations that occur most frequently in the top-K configurations for every task. In the case that the optimal default value depends on meta-features (e.g. the number of training instances or features), simple functions are learned that include these metafeatures. Next, a statistical test defines whether a hyperparameter can be safely left at this default, based on the performance loss observed when not tuning a hyperparameter (or a set of hyperparameters), while all other parameters are tuned. This was evaluated using 118,000 OpenML experiments with 2 algorithms (SVMs and Random Forests) across 59 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Configuration Transfer</head><p>If we want to provide recommendations for a specific task t new , we need additional information on how similar t new is to prior tasks t j . One way to do this is to evaluate a number of recommended (or potentially random) configurations on t new , yielding new evidence P new . If we then observe that the evaluations P i,new are similar to P i,j , then t j and t new can be considered intrinsically similar, based on empirical evidence. We can include this knowledge to train a meta-learner that predicts a recommended set of configurations Θ * new for t new . Moreover, every selected θ * new can be evaluated and included in P new , repeating the cycle and collecting more empirical evidence to learn which tasks are similar to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Relative Landmarks</head><p>A first measure for task similarity considers the relative (pairwise) performance differences, also called relative landmarks, RL a,b,j = P a,j -P b,j between two configurations θ a and θ b on a particular task t j <ref type="bibr" target="#b55">(Fürnkranz and Petrak, 2001)</ref>. Active testing <ref type="bibr" target="#b89">(Leite et al., 2012)</ref> leverages these as follows: it warm-starts with the globally best configuration (see Section 2.1), calls it θ best , and proceeds in a tournament-style fashion. In each round, it selects the 'competitor' θ c that most convincingly outperforms θ best on similar tasks. It deems tasks to be similar if the relative landmarks of all evaluated configurations are similar, i.e., if the configurations perform similarly on both t j and t new then the tasks are deemed similar. Next, it evaluates the competitor θ c , yielding P c,new , updates the task similarities, and repeats. A limitation of this method is that it can only consider configurations θ i that were evaluated on many prior tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Surrogate Models</head><p>A more flexible way to transfer information is to build surrogate models s j (θ i ) = P i,j for all prior tasks t j , trained using all available P. One can then define task similarity in terms of the error between s j (θ i ) and P i,new : if the surrogate model for t j can generate accurate predictions for t new , then those tasks are intrinsically similar. This is usually done in combination with Bayesian optimization Rasmussen (2004) to determine the next θ i . <ref type="bibr" target="#b190">Wistuba et al. (2018)</ref> train surrogate models based on Gaussian Processes (GPs) for every prior task, plus one for t new , and combine them into a weighted, normalized sum, with the (new) mean µ defined as the weighted sum of the individual µ j 's (obtained from prior tasks t j ). The weights of the µ j 's are computed using the Nadaraya-Watson kernelweighted average, where each task is represented as a vector of relative landmarks, and the Epanechnikov quadratic kernel <ref type="bibr" target="#b107">(Nadaraya, 1964)</ref> is used to measure the similarity between the relative landmark vectors of t j and t new . The more similar t j is to t new , the larger the weight s j , increasing the influence of the surrogate model for t j . <ref type="bibr">Feurer et al. (2018a)</ref> propose to combine the predictive distributions of the individual Gaussian processes, which makes the combined model a Gaussian process again. The weights are computed following the agnostic Bayesian ensemble of <ref type="bibr" target="#b85">Lacoste et al. (2014)</ref>, which weights predictors according to an estimate of their generalization performance.</p><p>Meta-data can also be transferred in the acquisition function rather than the surrogate model <ref type="bibr" target="#b190">(Wistuba et al., 2018)</ref>. The surrogate model is only trained on P i,new , but the next θ i to evaluate is provided by an acquisition function which is the weighted average of the expected improvement <ref type="bibr" target="#b72">(Jones et al., 1998)</ref> on P i,new and the predicted improvements on all prior P i,j . The weights of the prior tasks can again be defined via the accuracy of the surrogate model or via relative landmarks. The weight of the expected improvement component is gradually increased with every iteration as more evidence P i,new is collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Warm-Started Multi-task Learning</head><p>Another approach to relate prior tasks t j is to learn a joint task representation using P. Perrone et al. ( <ref type="formula">2017</ref>) train task-specific Bayesian linear regression <ref type="bibr" target="#b19">(Bishop, 2006)</ref> surrogate models s j (θ i ) and combine them in a feedforward Neural Network N N (θ i ) which learns a joint task representation that can accurately predict P i,new . The surrogate models are pretrained on OpenML meta-data to provide a warm-start for optimizing N N (θ i ) in a multitask learning setting. Earlier work on multi-task learning <ref type="bibr" target="#b165">(Swersky et al., 2013)</ref> assumed that we already have a set of 'similar' source tasks t j . It transfers information between these t j and t new by building a joint GP model for Bayesian optimization that learns and exploits the exact relationship between the tasks. Learning a joint GP tends to be less scalable than building one GP per task, though. <ref type="bibr" target="#b160">Springenberg et al. (2016)</ref> also assume that the tasks are related and similar, but learns the relationship between tasks during the optimization process using Bayesian Neural Networks. As such, their method is somewhat of a hybrid of the previous two approaches. <ref type="bibr" target="#b60">Golovin et al. (2017)</ref> assume a sequence order (e.g., time) across tasks. It builds a stack of GP regressors, one per task, training each GP on the residuals relative to the regressor below it. Hence, each task uses the tasks before it as its priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Other Techniques</head><p>Multi-armed bandits <ref type="bibr" target="#b139">(Robbins, 1985)</ref> provide yet another approach to find the source tasks t j most related to t new <ref type="bibr">(Ramachandran et al., 2018a)</ref>. In this analogy, each t j is one arm, and the (stochastic) reward for selecting (pulling) a particular prior task (arm) is defined in terms of the error in the predictions of a GP-based Bayesian optimizer that models the prior evaluations of t j as noisy measurements and combines them with the existing evaluations on t new . The cubic scaling of the GP makes this approach less scalable, though.</p><p>Another way to define task similarity is to take the existing evaluations P i,j , use Thompson Sampling <ref type="bibr" target="#b167">(Thompson, 1933)</ref> to obtain the optima distribution ρ j max , and then measure the KL-divergence <ref type="bibr" target="#b84">(Kullback and Leibler, 1951</ref>) between ρ j max and ρ new max <ref type="bibr">(Ramachandran et al., 2018b)</ref>. These distributions are then merged into a mixture distribution based on the similarities and used to build an acquisition function that predicts the next most promising configuration to evaluate. It is so far only evaluated to tune 2 SVM hyperparameters using 5 tasks.</p><p>Finally, a complementary way to leverage P is to recommend which configurations should not be used. After training surrogate models per task, we can look up which t j are most similar to t new , and then use s j (θ i ) to discover regions of Θ where performance is predicted to be poor. Excluding these regions can speed up the search for better-performing ones. <ref type="bibr">Wistuba et al. (2015b)</ref> do this using a task similarity measure based on the Kendall tau rank correlation coefficient <ref type="bibr" target="#b76">(Kendall, 1938)</ref> between the ranks obtained by ranking configurations θ i using P i,j and P i,new , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learning Curves</head><p>We can also extract meta-data about the training process itself, such as how fast model performance improves as more training data is added. If we divide the training in steps s t , usually adding a fixed number of training examples every step, we can measure the performance P (θ i , t j , s t ) = P i,j,t of configuration θ i on task t j after step s t , yielding a learning curve across the time steps s t . Learning curves are used extensively to speed up hyperparameter optimization on a given task <ref type="bibr" target="#b79">(Kohavi and John, 1995;</ref><ref type="bibr" target="#b124">Provost et al., 1999;</ref><ref type="bibr" target="#b166">Swersky et al., 2014;</ref><ref type="bibr" target="#b27">Chandrashekaran and Lane, 2017)</ref>. In meta-learning, however, learning curve information is transferred across tasks.</p><p>While evaluating a configuration on new task t new , we can halt the training after a certain number of iterations r &lt; t, and use the partially observed learning curve to predict how well the configuration will perform on the full dataset based on prior experience with other tasks, and decide whether to continue the training or not. This can significantly speed up the search for good configurations.</p><p>One approach is to assume that similar tasks yield similar learning curves. First, define a distance between tasks based on how similar the partial learning curves are: dist(t a , t b ) = f (P i,a,t , P i,b,t ) with t = 1, ..., r. Next, find the k most similar tasks t 1..k and use their complete learning curves to predict how well the configuration will perform on the new complete dataset. Task similarity can be measured by comparing the shapes of the partial curves across all configurations tried, and the prediction is made by adapting the 'nearest' complete curve(s) to the new partial curve <ref type="bibr">(Leite and</ref><ref type="bibr">Brazdil, 2005, 2007)</ref>. This approach was also successful in combination with active testing <ref type="bibr" target="#b90">(Leite and Brazdil, 2010)</ref>, and can be sped up further by using multi-objective evaluation measures that include training time <ref type="bibr" target="#b174">(van Rijn et al., 2015)</ref>.</p><p>Interestingly, while several methods aim to predict learning curves during neural architecture search <ref type="bibr" target="#b42">(Elsken et al., 2018)</ref>, as of yet none of this work leverages learning curves previously observed on other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning from Task Properties</head><p>Another rich source of meta-data are characterizations (meta-features) of the task at hand. Each task t j ∈ T is described with a vector m(t j ) = (m j,1 , ..., m j,K ) of K meta-features m j,k ∈ M , the set of all known meta-features. This can be used to define a task similarity measure based on, for instance, the Euclidean distance between m(t i ) and m(t j ), so that we can transfer information from the most similar tasks to the new task t new . Moreover, together with prior evaluations P, we can train a meta-learner L to predict the performance P i,new of configurations θ i on a new task t new .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Meta-Features</head><p>Table <ref type="table" target="#tab_3">1</ref> provides a concise overview of the most commonly used meta-features, together with a short rationale for why they are indicative of model performance. Where possible, we also show the formulas to compute them. More complete surveys can be found in the literature <ref type="bibr" target="#b138">(Rivolli et al., 2018;</ref><ref type="bibr" target="#b179">Vanschoren, 2010;</ref><ref type="bibr" target="#b101">Mantovani, 2018;</ref><ref type="bibr" target="#b134">Reif et al., 2014;</ref><ref type="bibr" target="#b25">Castiello et al., 2005)</ref>.</p><p>Name Formula Rationale Variants Nr instances n Speed, Scalability (Michie et al., 1994) p/n, log(n), log(n/p) Nr features p Curse of dimensionality (Michie et al., 1994) log(p), % categorical Nr classes c Complexity, imbalance (Michie et al., 1994) ratio min/maj class Nr missing values m Imputation effects (Kalousis, 2002) % missing Nr outliers o Data noisiness (Rousseeuw and Hubert, 2011) o/n Skewness E(X-µ X ) 3 σ 3 X Feature normality (Michie et al., 1994) min,max,µ,σ,q1, q3 Kurtosis E(X-µ X ) 4 σ 4 X Feature normality (Michie et al., 1994) min,max,µ,σ,q1, q3 Correlation ρX 1 X 2 Feature interdependence (Michie et al., 1994) min,max,µ,σ,ρXY Covariance covX 1 X 2 Feature interdependence (Michie et al., 1994) min,max,µ,σ,covXY Concentration τX 1 X 2 Feature interdependence (Kalousis and Hilario, 2001) min,max,µ,σ,τXY Sparsity sparsity(X) Degree of discreteness (Salama et al., 2013) min,max,µ,σ Gravity gravity(X) Inter-class dispersion (Ali and Smith-Miles, 2006a) ANOVA p-value p val X 1 X 2 Feature redundancy (Kalousis, 2002) p val XY <ref type="bibr" target="#b158">(Soares et al., 2004)</ref> Coeff. of variation</p><formula xml:id="formula_0">σ Y µ Y</formula><p>Variation in target <ref type="bibr" target="#b158">(Soares et al., 2004)</ref> PCA</p><formula xml:id="formula_1">ρ λ 1 λ 1 1+λ 1</formula><p>Variance in first PC <ref type="bibr" target="#b102">(Michie et al., 1994)</ref> λ 1 i λ i <ref type="bibr" target="#b102">(Michie et al., 1994)</ref> PCA skewness</p><p>Skewness of first PC <ref type="bibr">(Feurer et</ref> al., 2014) PCA kurtosis PCA 95% dim 95%var p Intrinsic dimensionality (Bardenet et al., 2013) Class probability P (C) Class distribution (Michie et al., 1994) min,max,µ,σ Class entropy H(C) Class imbalance (Michie et al., 1994) Norm. entropy H(X) log 2 n</p><p>Feature informativeness <ref type="bibr" target="#b25">(Castiello et al., 2005)</ref> min,max,µ,σ Mutual inform.</p><p>M I(C, X) Feature importance <ref type="bibr" target="#b102">(Michie et al., 1994)</ref> min,max,µ,σ Uncertainty coeff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M I(C,X) H(C)</head><p>Feature importance <ref type="bibr" target="#b2">(Agresti, 2002)</ref> min,max,µ,σ Equiv. nr. feats</p><formula xml:id="formula_2">H(C) M I(C,X)</formula><p>Intrinsic dimensionality <ref type="bibr" target="#b102">(Michie et al., 1994)</ref> Noise-signal ratio</p><formula xml:id="formula_3">H(X)-M I(C,X) M I(C,X)</formula><p>Noisiness of data <ref type="bibr" target="#b102">(Michie et al., 1994)</ref> Fisher's discrimin.</p><formula xml:id="formula_4">(µ c1 -µ c2 ) 2 σ 2 c1 -σ 2 c2</formula><p>Separability classes c1, c2</p><p>(Ho and Basu, 2002) See Ho:2002 Volume of overlap Class distribution overlap (Ho and Basu, 2002) See Ho and Basu (2002) Concept variation Task complexity (Vilalta and Drissi, 2002) See Vilalta (1999) Data consistency Data quality (Köpf and Iglezakis, 2002) See Köpf and Iglezakis (2002) Nr nodes, leaves |η|, |ψ| Concept complexity (Peng et al., 2002) Tree depth Branch length Concept complexity (Peng et al., 2002) min,max,µ,σ Nodes per feature |ηX | Feature importance (Peng et al., 2002) min,max,µ,σ Leaves per class |ψc| |ψ|</p><p>Class complexity <ref type="bibr" target="#b51">(Filchenkov and Pendryak, 2015)</ref> min,max,µ,σ Leaves agreement</p><formula xml:id="formula_5">n ψ i n</formula><p>Class separability <ref type="bibr">(Bensusan et al., 2000)</ref> min,max,µ,σ Information gain Feature importance <ref type="bibr">(Bensusan et</ref>  </p><formula xml:id="formula_6">= j π ij , entropy H(X) = -i π i+ log 2 (π i+ ).</formula><p>To build a meta-feature vector m(t j ), one needs to select and further process these meta-features. Studies on OpenML meta-data have shown that the optimal set of metafeatures depends on the application <ref type="bibr" target="#b16">(Bilalli et al., 2017)</ref>. Many meta-features are computed on single features, or combinations of features, and need to be aggregated by summary statistics (min,max,µ,σ,quartiles,...) or histograms <ref type="bibr" target="#b75">(Kalousis and Hilario, 2001)</ref>. One needs to systematically extract and aggregate them <ref type="bibr" target="#b119">(Pinto et al., 2016)</ref>. When computing task similarity, it is also important to normalize all meta-features <ref type="bibr" target="#b8">(Bardenet et al., 2013)</ref>, perform feature selection <ref type="bibr" target="#b172">(Todorovski et al., 2000)</ref>, or employ dimensionality reduction techniques (e.g. PCA) <ref type="bibr" target="#b16">(Bilalli et al., 2017)</ref>. When learning meta-models, one can also use relational meta-learners <ref type="bibr" target="#b171">(Todorovski and Dzeroski, 1999)</ref> or case-based reasoning methods <ref type="bibr" target="#b95">(Lindner and Studer, 1999;</ref><ref type="bibr" target="#b66">Hilario and Kalousis, 2001;</ref><ref type="bibr" target="#b74">Kalousis and Hilario, 2003)</ref>.</p><p>Beyond these general-purpose meta-features, many more specific ones were formulated. For streaming data one can use streaming landmarks <ref type="bibr">(van Rijn et al., 2018</ref><ref type="bibr">(van Rijn et al., , 2014))</ref>, for time series data one can compute autocorrelation coefficients or the slope of regression models <ref type="bibr" target="#b6">(Arinze, 1994;</ref><ref type="bibr">Prudêncio and Ludermir, 2004;</ref><ref type="bibr" target="#b38">dos Santos et al., 2004)</ref>, and for unsupervised problems one can cluster the data in different ways and extract properties of these clusters <ref type="bibr" target="#b159">(Soares et al., 2009)</ref>. In many applications, domain-specific information can be leveraged as well <ref type="bibr" target="#b155">(Smith-Miles, 2009;</ref><ref type="bibr" target="#b112">Olier et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Meta-Features</head><p>Instead of manually defining meta-features, we can also learn a joint representation for groups of tasks. One approach is to build meta-models that generate a landmark-like metafeature representation M ′ given other task meta-features M and trained on performance meta-data P, or f : M → M ′ . <ref type="bibr">Sun and Pfahringer (2013)</ref> do this by evaluating a predefined set of configurations θ i on all prior tasks t j , and generating a binary metafeature m j,a,b ∈ M ′ for every pairwise combination of configurations θ a and θ b , indicating whether θ a outperformed θ b or not, thus m ′ (t j ) = (m j,a,b , m j,a,c , m j,b,c , ...). To compute m new,a,b , meta-rules are learned for every pairwise combination (a,b), each predicting whether θ a will outperform θ b on task t j , given its other meta-features m(t j ).</p><p>We can also learn a joint representation based entirely on the available P meta-data, i.e. f : P × Θ → M ′ . We previously discussed how to do this with feed-forward neural nets <ref type="bibr" target="#b117">(Perrone et al., 2017)</ref> in Section 2.3. If the tasks share the same input space, e.g., they are images of the same resolution, one can also use Siamese networks to learn a meta-feature representation <ref type="bibr" target="#b78">(Kim et al., 2017</ref>). These are trained by feeding the data of two different tasks to two twin networks, and using the differences between the predicted and observed performance P i,new as the error signal. Since the model parameters between both networks are tied in a Siamese network, two very similar tasks are mapped to the same regions in the latent meta-feature space. They can be used for warm starting Bayesian hyperparameter optimization <ref type="bibr" target="#b78">(Kim et al., 2017)</ref> and neural architecture search <ref type="bibr" target="#b1">(Afif, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Warm-Starting Optimization from Similar Tasks</head><p>Meta-features are a very natural way to estimate task similarity and initialize optimization procedures based on promising configurations on similar tasks. This is akin to how human experts start a manual search for good models, given experience on related tasks.</p><p>Starting a genetic search algorithm in regions of the search space with promising solutions can significantly speed up convergence to a good solution. <ref type="bibr" target="#b61">Gomes et al. (Gomes et al., 2012)</ref> recommend initial configurations by finding the k most similar prior tasks t j based on the L1 distance between vectors m(t j ) and m(t new ), where each m(t j ) includes 17 simple and statistical meta-features. For each of the k most similar tasks, the best configuration is evaluated on t new , and used to initialize a genetic search algorithm (Particle Swarm Optimization), as well as Tabu Search. <ref type="bibr" target="#b133">Reif et al. (2012)</ref> follow a very similar approach, using 15 simple, statistical, and landmarking meta-features. They use a forward selection technique to find the most useful meta-features, and warm-start a standard genetic algorithm (GAlib) with a modified Gaussian mutation operation. Variants of active testing (see Sect. 2.3) that use meta-features were also tried <ref type="bibr" target="#b103">(Miranda and Prudêncio, 2013;</ref><ref type="bibr" target="#b89">Leite et al., 2012)</ref>, but did not perform better than the approaches based on relative landmarks.</p><p>Also model-based optimization approaches can benefit greatly from an initial set of promising configurations. SCoT <ref type="bibr" target="#b8">(Bardenet et al., 2013)</ref> trains a single surrogate ranking model f : M × Θ → R, predicting the rank of θ i on task t j . M contains 4 meta-features (3 simple ones and one based on PCA). The surrogate model is trained on all the rankings, including those on t new . Ranking is used because the scale of evaluation values can differ greatly between tasks. A GP regression converts the ranks to probabilities to do Bayesian optimization, and each new P i,new is used to retrain the surrogate model after every step. <ref type="bibr" target="#b147">Schilling et al. (2015)</ref> use a modified multilayer perceptron as a surrogate model, of the form s j (θ i , m(t j ), b(t j )) = P i,j where m(t j ) are the meta-features and b(t j ) is a vector of j binary indications which are 1 if the meta-instance is from t j and 0 otherwise. The multi-layer perceptron uses a modified activation function based on factorization machines <ref type="bibr" target="#b136">(Rendle, 2010)</ref> in the first layer, aimed at learning a latent representation for each task to model task similarities. Since this model cannot represent uncertainties, an ensemble of 100 multilayer perceptrons is trained to get predictive means and simulate variances.</p><p>Training a single surrogate model on all prior meta-data is often less scalable. Yogatama and Mann (2014) also build a single Bayesian surrogate model, but only include tasks similar to t new , where task similarity is defined as the Euclidean distance between meta-feature vectors consisting of 3 simple meta-features. The P i,j values are standardized to overcome the problem of different scales for each t j . The surrogate model learns a Gaussian process with a specific kernel combination on all instances.</p><p>Feurer et al. ( <ref type="formula">2014</ref>) offer a simpler, more scalable method that warm-starts Bayesian optimization by sorting all prior tasks t j similar to Gomes et al. ( <ref type="formula">2012</ref>), but including 46 simple, statistical, and landmarking meta-features, as well as H(C). The t best configurations on the d most similar tasks are used to warm-start the surrogate model. They search over many more hyperparameters than earlier work, including preprocessing steps. This warm-starting approach was also used very effectively, and combined with ensembling, in autosklearn <ref type="bibr" target="#b49">(Feurer et al., 2015)</ref>.</p><p>Finally, one can also use collaborative filtering to recommend promising configurations <ref type="bibr" target="#b161">(Stern et al., 2010)</ref>. By analogy, the tasks t j (users) provide ratings (P i,j ) for the configurations θ i (items), and matrix factorization techniques are used to predict unknown P i,j values and recommend the best configurations for any task. An important issue here is the cold start problem, since the matrix factorization requires at least some evaluations on t new . <ref type="bibr" target="#b192">Yang et al. (2018)</ref> use a D-optimal experiment design to sample an initial set of evaluations P i,new . They predict both the predictive performance and runtime, to recommend a set of warm-start configurations that are both accurate and fast. <ref type="bibr" target="#b105">Misir and Sebag (2013)</ref> and <ref type="bibr" target="#b106">Mısır and Sebag (2017)</ref> leverage meta-features to solve the cold start problem. <ref type="bibr" target="#b56">Fusi et al. (2017)</ref> also use meta-features, following the same procedure as <ref type="bibr" target="#b49">Feurer et al. (2015)</ref>, and use a probabilistic matrix factorization approach that allows them to perform Bayesian optimization to further optimize their pipeline configurations θ i . This approach also yields useful latent embeddings of both the tasks and configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Meta-Models</head><p>We can also learn the complex relationship between a task's meta-features and the utility of specific configurations by building a meta-model L that recommends the most useful configurations Θ * new given the meta-features M of the new task t new . There exists a rich body of earlier work <ref type="bibr" target="#b21">(Brazdil et al., 2009;</ref><ref type="bibr" target="#b91">Lemke et al., 2015;</ref><ref type="bibr" target="#b58">Giraud-Carrier, 2008;</ref><ref type="bibr" target="#b97">Luo, 2016)</ref> on building meta-models for algorithm selection <ref type="bibr">(Bensusan and Giraud-Carrier, 2000;</ref><ref type="bibr" target="#b118">Pfahringer et al., 2000;</ref><ref type="bibr" target="#b73">Kalousis, 2002;</ref><ref type="bibr" target="#b18">Bischl et al., 2016)</ref> and hyperparameter recommendation <ref type="bibr" target="#b83">(Kuba et al., 2002;</ref><ref type="bibr" target="#b158">Soares et al., 2004;</ref><ref type="bibr">Ali and Smith-Miles, 2006b;</ref><ref type="bibr" target="#b111">Nisioti et al., 2018)</ref>. Experiments showed that boosted and bagged trees often yielded the best predictions, although much depends on the exact meta-features used <ref type="bibr" target="#b75">(Kalousis and Hilario, 2001;</ref><ref type="bibr" target="#b80">Köpf and Iglezakis, 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Ranking</head><p>Meta-models can also generate a ranking of the top-K most promising configurations. One approach is to build a k-nearest neighbor (kNN) meta-model to predict which tasks are similar, and then rank the best configurations on these similar tasks <ref type="bibr">(Brazdil et al., 2003b;</ref><ref type="bibr" target="#b38">dos Santos et al., 2004)</ref>. This is similar to the work discussed in Section 3.3, but without ties to a follow-up optimization approach. Meta-models specifically meant for ranking, such as predictive clustering trees <ref type="bibr" target="#b173">(Todorovski et al., 2002)</ref> and label ranking trees <ref type="bibr" target="#b29">(Cheng et al., 2009)</ref> were also shown to work well. Approximate Ranking Trees Forests (ART Forests) <ref type="bibr">(Sun and Pfahringer, 2013)</ref>, ensembles of fast ranking trees, prove to be especially effective, since they have 'built-in' meta-feature selection, work well even if few prior tasks are available, and the ensembling makes the method more robust. autoBagging <ref type="bibr" target="#b120">(Pinto et al., 2017)</ref> ranks Bagging workflows including four different Bagging hyperparameters, using an XGBoost-based ranker, trained on 140 OpenML datasets and 146 meta-features. <ref type="bibr" target="#b96">Lorena et al. (2018)</ref> recommend SVM configurations for regression problems using a kNN meta-model and a new set of meta-features based on data complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Performance Prediction</head><p>Meta-models can also directly predict the performance, e.g. accuracy or training time, of a configuration on a given task, given its meta-features. This allows us to estimate whether a configuration will be interesting enough to evaluate in any optimization procedure. Early work used linear regression or rule-base regressors to predict the performance of a discrete set of configurations and then rank them accordingly <ref type="bibr" target="#b13">(Bensusan and Kalousis, 2001;</ref><ref type="bibr" target="#b81">Köpf et al., 2000)</ref>. <ref type="bibr" target="#b64">Guerra et al. (Guerra et al., 2008)</ref> train an SVM meta-regressor per classification algorithm to predict its accuracy, under default settings, on a new task t new given its meta-features. <ref type="bibr" target="#b134">Reif et al. (Reif et al., 2014)</ref> train a similar meta-regressor on more meta-data to predict its optimized performance. <ref type="bibr" target="#b32">Davis et al. (Davis and Giraud-Carrier, 2018</ref>) use a MultiLayer Perceptron based meta-learner instead, predicting the performance of a specific algorithm configuration.</p><p>Instead of predicting predictive performance, a meta-regressor can also be trained to predict algorithm training/prediction time, for instance, using an SVM regressor trained on meta-features <ref type="bibr" target="#b132">(Reif et al., 2011)</ref>, itself tuned via genetic algorithms <ref type="bibr" target="#b122">(Priya et al., 2012)</ref>. <ref type="bibr" target="#b192">Yang et al. (2018)</ref> predict configuration runtime using polynomial regression, based only on the number of instances and features. <ref type="bibr">Hutter et al. (2014b)</ref> provide a general treatise on predicting algorithm runtime in various domains.</p><p>Most of these meta-models generate promising configurations, but don't actually tune these configurations to t new themselves. Instead, the predictions can be used to warm-start or guide any other optimization technique, which allows for all kinds of combinations of meta-models and optimization techniques. Indeed, some of the work discussed in Section 3.3 can be seen as using a distance-based meta-model to warm-start Bayesian optimization <ref type="bibr" target="#b48">(Feurer et al., 2014;</ref><ref type="bibr" target="#b56">Fusi et al., 2017)</ref> or evolutionary algorithms <ref type="bibr" target="#b61">(Gomes et al., 2012;</ref><ref type="bibr" target="#b133">Reif et al., 2012)</ref>. In principle, other meta-models could be used here as well.</p><p>Instead of learning the relationship between a task's meta-features and configuration performance, one can also build surrogate models predicting the performance of configurations on specific tasks <ref type="bibr" target="#b41">(Eggensperger et al., 2018)</ref>. One can then learn how to combine these per-task predictions to warm-start or guide optimization techniques on a new task t new <ref type="bibr">(Feurer et al., 2018a;</ref><ref type="bibr" target="#b117">Perrone et al., 2017;</ref><ref type="bibr" target="#b160">Springenberg et al., 2016;</ref><ref type="bibr" target="#b190">Wistuba et al., 2018)</ref>, as discussed in Section 2.3. While meta-features could also be used to combine per-task predictions based on task similarity, it is ultimately more effective to gather new observations P i,new , since these allow to refine the task similarity estimates with every new observation <ref type="bibr">(Feurer et al., 2018b;</ref><ref type="bibr" target="#b190">Wistuba et al., 2018;</ref><ref type="bibr" target="#b89">Leite et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Pipeline Synthesis</head><p>When creating entire machine learning pipelines <ref type="bibr" target="#b152">(Serban et al., 2013)</ref>, the number of configuration options grows dramatically, making it even more important to leverage prior experience. One can control the search space by imposing a fixed structure on the pipeline, fully described by a set of hyperparameters. One can then use the most promising pipelines on similar tasks to warm-start a Bayesian optimization <ref type="bibr" target="#b49">(Feurer et al., 2015;</ref><ref type="bibr" target="#b56">Fusi et al., 2017)</ref>.</p><p>Other approaches give recommendations for certain pipeline steps <ref type="bibr" target="#b121">(Post et al., 2016;</ref><ref type="bibr" target="#b162">Strang et al., 2018)</ref>, and can be leveraged in larger pipeline construction approaches, such as planning <ref type="bibr" target="#b108">(Nguyen et al., 2014;</ref><ref type="bibr" target="#b77">Kietz et al., 2012;</ref><ref type="bibr" target="#b57">Gil et al., 2018;</ref><ref type="bibr" target="#b187">Wever et al., 2018)</ref> or evolutionary techniques <ref type="bibr" target="#b113">(Olson et al., 2016;</ref><ref type="bibr">Sun et al., 2013)</ref>. <ref type="bibr" target="#b108">Nguyen et al. (2014)</ref> construct new pipelines using a beam search focussed on components recommended by a metalearner, and is itself trained on examples of successful prior pipelines. <ref type="bibr" target="#b17">Bilalli et al. (2018)</ref> predict which pre-processing techniques are recommended for a given classification algorithm. They build a meta-model per target classification algorithm that, given the t new meta-features, predicts which preprocessing technique should be included in the pipeline.</p><p>Similarly, <ref type="bibr" target="#b151">Schoenfeld et al. (2018)</ref> build meta-models predicting when a preprocessing algorithm will improve a particular classifier's accuracy or runtime.</p><p>AlphaD3M <ref type="bibr" target="#b39">(Drori et al., 2018)</ref> uses a self-play reinforcement learning approach in which the current state is represented by the current pipeline, and actions include the addition, deletion, or replacement of pipeline components. A Monte Carlo Tree Search (MCTS) generates pipelines, which are evaluated to train a recurrent neural network (LSTM) that can predict pipeline performance, in turn producing the action probabilities for the MCTS in the next round. The state description also includes meta-features of the current task, allowing the neural network to learn across tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">To Tune or Not to Tune?</head><p>To reduce the number of configuration parameters to be optimized, and to save valuable optimization time in time-constrained settings, meta-models have also been proposed to predict whether or not it is worth tuning a given algorithm given the meta-features of the task at hand <ref type="bibr" target="#b137">(Ridd and Giraud-Carrier, 2014)</ref> and how much improvement we can expect from tuning a specific algorithm versus the additional time investment <ref type="bibr" target="#b144">(Sanders and Giraud-Carrier, 2017)</ref>. More focused studies on specific learning algorithms yielded meta-models predicting when it is necessary to tune SVMs <ref type="bibr">(Mantovani et al., 2015a)</ref>, what are good default hyperparameters for SVMs given the task (including interpretable meta-models) <ref type="bibr">(Mantovani et al., 2015b)</ref>, and how to tune decision trees <ref type="bibr" target="#b99">(Mantovani et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning from Prior Models</head><p>The final type of meta-data we can learn from are prior machine learning models themselves, i.e., their structure and learned model parameters. In short, we want to train a meta-learner L that learns how to train a (base-) learner l new for a new task t new , given similar tasks t j ∈ T and the corresponding optimized models l j ∈ L, where L is the space of all possible models. The learner l j is typically defined by its model parameters W = {w k }, k = 1..K and/or its configuration θ i ∈ Θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transfer Learning</head><p>In transfer learning <ref type="bibr" target="#b170">(Thrun and Pratt, 1998)</ref>, we take models trained on one or more source tasks t j , and use them as starting points for creating a model on a similar target task t new . This can be done by forcing the target model to be structurally or otherwise similar to the source model(s). This is a generally applicable idea, and transfer learning approaches have been proposed for kernel methods <ref type="bibr" target="#b44">(Evgeniou et al., 2005;</ref><ref type="bibr" target="#b43">Evgeniou and Pontil, 2004)</ref>, parametric Bayesian models <ref type="bibr" target="#b140">(Rosenstein et al., 2005;</ref><ref type="bibr" target="#b126">Raina et al., 2006;</ref><ref type="bibr" target="#b7">Bakker and Heskes, 2003)</ref>, Bayesian networks <ref type="bibr" target="#b110">(Niculescu-Mizil and Caruana, 2005)</ref>, clustering <ref type="bibr" target="#b168">(Thrun, 1998)</ref> and reinforcement learning <ref type="bibr" target="#b65">(Hengst, 2002;</ref><ref type="bibr" target="#b36">Dietterich et al., 2002)</ref>. Neural networks, however, are exceptionally suitable for transfer learning because both the structure and the model parameters of the source models can be used as a good initialization for the target model, yielding a pre-trained model which can then be further fine-tuned using the available training data on t new <ref type="bibr" target="#b169">(Thrun and Mitchell, 1995;</ref><ref type="bibr" target="#b10">Baxter, 1996;</ref><ref type="bibr" target="#b12">Bengio, 2012;</ref><ref type="bibr" target="#b23">Caruana, 1995)</ref>. In some cases, the source network may need to be modified before transferring it <ref type="bibr" target="#b154">(Sharkey and Sharkey, 1993)</ref>. We will focus on neural networks in the remainder of this section.</p><p>Especially large image datasets, such as ImageNet <ref type="bibr" target="#b82">(Krizhevsky et al., 2012)</ref>, have been shown to yield pre-trained models that transfer exceptionally well to other tasks <ref type="bibr" target="#b37">(Donahue et al., 2014;</ref><ref type="bibr" target="#b153">Sharif Razavian et al., 2014)</ref>. However, it has also been shown that this approach doesn't work well when the target task is not so similar <ref type="bibr" target="#b194">(Yosinski et al., 2014)</ref>. Rather than hoping that a pre-trained model 'accidentally' transfers well to a new problem, we can purposefully imbue meta-learners with an inductive bias (learned from many similar tasks) that allows them to learn new tasks much faster, as we will discuss below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Meta-Learning in Neural Networks</head><p>An early meta-learning approach is to create recurrent neural networks (RNNs) able to modify their own weights <ref type="bibr" target="#b148">(Schmidhuber, 1992</ref><ref type="bibr" target="#b149">(Schmidhuber, , 1993))</ref>. During training, they use their own weights as additional input data and observe their own errors to learn how to modify these weights in response to the new task at hand. The updating of the weights is defined in a parametric form that is differentiable end-to-end and can jointly optimize both the network and training algorithm using gradient descent, yet is also very difficult to train. Later work used reinforcement learning across tasks to adapt the search strategy <ref type="bibr" target="#b150">(Schmidhuber et al., 1997)</ref> or the learning rate for gradient descent <ref type="bibr" target="#b31">(Daniel et al., 2016)</ref> to the task at hand.</p><p>Inspired by the feeling that backpropagation is an unlikely learning mechanism for our own brains, <ref type="bibr" target="#b11">Bengio et al. (1995)</ref> replace backpropagation with simple biologically-inspired parametric rules (or evolved rules <ref type="bibr" target="#b26">(Chalmers, 1991)</ref>) to update the synaptic weights. The parameters are optimized, e.g. using gradient descent or evolution, across a set of input tasks. <ref type="bibr" target="#b142">Runarsson and Jonsson (2000)</ref> replaced these parametric rules with a single layer neural network. <ref type="bibr">Santoro et al. (2016b)</ref> instead use a memory-augmented neural network to learn how to store and retrieve 'memories' of prior classification tasks. <ref type="bibr" target="#b68">Hochreiter et al. (2001)</ref> use LSTMs <ref type="bibr" target="#b69">(Hochreiter and Schmidhuber, 1997)</ref> as a meta-learner to train multilayer perceptrons. <ref type="bibr" target="#b5">Andrychowicz et al. (2016)</ref> also replace the optimizer, e.g. stochastic gradient descent, with an LSTM trained on multiple prior tasks. The loss of the meta-learner (optimizer) is defined as the sum of the losses of the base-learners (optimizees), and optimized using gradient descent. At every step, the meta-learner chooses the weight update estimated to reduce the optimizee's loss the most, based on the learned model weights {w k } of the previous step as well as the current performance gradient. Later work generalizes this approach by training an optimizer on synthetic functions, using gradient descent <ref type="bibr" target="#b28">(Chen et al., 2016)</ref>. This allows meta-learners to optimize optimizees even if these do not have access to gradients.</p><p>In parallel, <ref type="bibr" target="#b93">Li and Malik (2016)</ref> proposed a framework for learning optimization algorithms from a reinforcement learning perspective. It represents any particular optimization algorithm as a policy, and then learns this policy via guided policy search. Follow-up work <ref type="bibr" target="#b93">(Li and Malik, 2017)</ref> shows how to leverage this approach to learn optimization algorithms for (shallow) neural networks.</p><p>The field of neural architecture search includes many other methods that build a model of neural network performance for a specific task, for instance using Bayesian optimization or reinforcement learning. See <ref type="bibr" target="#b42">Elsken et al. (2018)</ref> for an in-depth discussion. However, most of these methods do not (yet) generalize across tasks and are therefore not discussed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Few-Shot Learning</head><p>A particularly challenging meta-learning problem is to train an accurate deep learning model using only a few training examples, given prior experience with very similar tasks for which we have large training sets available. This is called few-shot learning. Humans have an innate ability to do this, and we wish to build machine learning agents that can do the same (Lake et al., 2017). A particular example of this is 'K-shot N-way' classification, in which we are given many examples (e.g., images) of certain classes (e.g., objects), and want to learn a classifier l new able to classify N new classes using only K examples of each.</p><p>Using prior experience, we can, for instance, learn a common feature representation of all the tasks, start training l new with a better model parameter initialization W init and acquire an inductive bias that helps guide the optimization of the model parameters, so that l new can be trained much faster than otherwise possible.</p><p>Earlier work on one-shot learning is largely based on hand-engineered features <ref type="bibr" target="#b46">(Fei-Fei et al., 2006;</ref><ref type="bibr" target="#b45">Fei-Fei, 2006;</ref><ref type="bibr" target="#b52">Fink, 2005;</ref><ref type="bibr" target="#b9">Bart and Ullman, 2005)</ref>. With meta-learning, however, we hope to learn a common feature representation for all tasks in an end-to-end fashion. <ref type="bibr" target="#b184">Vinyals et al. (2016)</ref> state that, to learn from very little data, one should look to nonparameteric models (such as k-nearest neighbors), which use a memory component rather than learning many model parameters. Their meta-learner is a Matching Network that apply the idea of a memory component in a neural net. It learns a common representation for the labelled examples, and matches each new test instance to the memorized examples using cosine similarity. The network is trained on minibatches with only a few examples of a specific task each. <ref type="bibr" target="#b156">Snell et al. (2017)</ref> propose Prototypical Networks, which map examples to a p-dimensional vector space such that examples of a given output class are close together. It then calculates a prototype (mean vector) for every class. New test instances are mapped to the same vector space and a distance metric is used to create a softmax over all possible classes. <ref type="bibr" target="#b135">Ren et al. (2018)</ref> extend this approach to semi-supervised learning.</p><p>Ravi and Larochelle (2017) use an LSTM-based meta-learner to learn an update rule for training a neural network learner. With every new example, the learner returns the current gradient and loss to the LSTM meta-learner, which then updates the model parameters {w k } of the learner. The meta-learner is trained across all prior tasks.</p><p>Model-Agnostic Meta-Learning (MAML) <ref type="bibr">(Finn et al., 2017)</ref>, on the other hand, does not try to learn an update rule, but instead learns a model parameter initialization W init that generalizes better to similar tasks. Starting from a random {w k }, it iteratively selects a batch of prior tasks, and for each it trains the learner on K examples to compute the gradient and loss (on a test set). It then backpropagates the meta-gradient to update the weights {w k } in the direction in which they would have been easier to update. In other words, after each iteration, the weights {w k } become a better W init to start finetuning any of the tasks. <ref type="bibr">Finn and Levine (2017)</ref> show that MAML is able to approximate any learning algorithm when using a sufficiently deep ReLU network and certain losses. They also conclude that the MAML initializations are more resilient to overfitting on small samples, and generalize more widely than meta-learning approaches based on LSTMs. <ref type="bibr" target="#b62">Grant et al. (2018)</ref> present a novel derivation of and extension to MAML, illustrating that this algorithm can be understood as inference for the parameters of a prior distribution in a hierarchical Bayesian model. REPTILE <ref type="bibr" target="#b109">(Nichol et al., 2018)</ref> is an approximation of MAML that executes stochastic gradient descent for K iterations on a given task, and then gradually moves the initialization weights in the direction of the weights obtained after the K iterations. The intuition is that every task likely has more than one set of optimal weights {w * i }, and the goal is to find a W init that is close to at least one of those {w * i } for every task. Finally, we can also derive a meta-learner from a black-box neural network. <ref type="bibr">Santoro et al. (2016a)</ref> propose Memory-Augmented Neural Networks (MANNs), which train a Neural Turing Machine (NTM) <ref type="bibr" target="#b63">(Graves et al., 2014)</ref>, a neural network with augmented memory capabilities, as a meta-learner. This meta-learner can then memorize information about previous tasks and leverage that to learn a learner l new . SNAIL <ref type="bibr" target="#b104">(Mishra et al., 2018</ref>) is a generic meta-learner architecture consisting of interleaved temporal convolution and causal attention layers. The convolutional networks learn a common feature vector for the training instances (images) to aggregate information from past experiences. The causal attention layers learn which pieces of information to pick out from the gathered experience to generalize to new tasks.</p><p>Overall, the intersection of deep learning and meta-learning proves to be particular fertile ground for groundbreaking new ideas, and we expect this field to become more important over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Beyond Supervised Learning</head><p>Meta-learning is certainly not limited to (semi-)supervised tasks, and has been successfully applied to solve tasks as varied as reinforcement learning, active learning, density estimation and item recommendation. The base-learner may be unsupervised while the meta-learner is supervised, but other combinations are certainly possible as well. <ref type="bibr" target="#b40">Duan et al. (2016)</ref> propose an end-to-end reinforcement learning (RL) approach consisting of a task-specific fast RL algorithm which is guided by a general-purpose slow meta-RL algorithm. The tasks are interrelated Markov Decision Processes (MDPs). The meta-RL algorithm is modeled as an RNN, which receives the observations, actions, rewards and termination flags. The activations of the RNN store the state of the fast RL learner, and the RNN's weights are learned by observing the performance of fast learners across tasks.</p><p>In parallel, <ref type="bibr" target="#b185">Wang et al. (2016)</ref> also proposed to use a deep RL algorithm to train an RNN, receiving the actions and rewards of the previous interval in order to learn a baselevel RL algorithm for specific tasks. Rather than using relatively unstructured tasks such as random MDPs, they focus on structured task distributions (e.g., dependent bandits) in which the meta-RL algorithm can exploit the inherent task structure. <ref type="bibr" target="#b115">Pang et al. (2018)</ref> offer a meta-learning approach to active learning (AL). The baselearner can be any binary classifier, and the meta-learner is a deep RL network consisting of a deep neural network that learns a representation of the AL problem across tasks, and a policy network that learns the optimal policy, parameterized as weights in the network. The meta-learner receives the current state (the unlabeled point set and base classifier state) and reward (the performance of the base classifier), and emits a query probability, i.e. which points in the unlabeled set to query next. <ref type="bibr" target="#b131">Reed et al. (2017)</ref> propose a few-shot approach for density estimation (DE). The goal is to learn a probability distribution over a small number of images of a certain concept (e.g., a handwritten letter) that can be used to generate images of that concept, or compute the probability that an image shows that concept. The approach uses autoregressive image models which factorize the joint distribution into per-pixel factors, usually conditioned on (many) examples of the target concept. Instead, a MAML-based few-shot learner is used, trained on examples of many other (similar) concepts.</p><p>Finally, <ref type="bibr" target="#b181">Vartak et al. (2017)</ref> address the cold-start problem in matrix factorization. They propose a deep neural network architecture that learns a (base) neural network whose biases are adjusted based on task information. While the structure and weights of the neural net recommenders remain fixed, the meta-learner learns how to adjust the biases based on each user's item history.</p><p>All these recent new developments illustrate that it is often fruitful to look at problems through a meta-learning lens and find new, data-driven approaches to replace handengineered base-learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Meta-learning opportunities present themselves in many different ways, and can be embraced using a wide spectrum of learning techniques. Every time we try to learn a certain task, whether successful or not, we gain useful experience that we can leverage to learn new tasks. We should never have to start entirely from scratch. Instead, we should systematically collect our 'learning exhaust' and learn from it to build AutoML systems that continuously improve over time, helping us tackle new learning problems ever more efficiently. The more new tasks we encounter, and the more similar those new tasks are, the more we can tap into prior experience, to the point that most of the required learning has already been done beforehand. The ability of computer systems to store virtually infinite amounts of prior learning experiences (in the form of meta-data) opens up a wide range of opportunities to use that experience in completely new ways, and we are only starting to learn how to learn from prior experience effectively. Yet, this is a worthy goal: learning how to learn any task empowers us far beyond knowing how to learn specific tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Overview of commonly used meta-features. Groups from top to bottom: simple, statistical, information-theoretic, complexity, model-based, and landmarkers. Continuous features X and target Y have mean µ X , stdev σ X , variance σ 2 X . Categorical features X and class C have categorical values π i , conditional probabilities π i|j , joint probabilities π i,j , marginal probabilities π i+</figDesc><table><row><cell>al., 2000)</cell><cell>min,max,µ,σ, gini</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The author would like to thank <rs type="person">Pavel Brazdil</rs>, <rs type="person">Matthias Feurer</rs>, <rs type="person">Frank Hutter</rs>, <rs type="person">Raghu Rajan</rs>, and <rs type="person">Jan van Rijn</rs> for many invaluable discussions and feedback on the manuscript.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speeding up Algorithm Selection using Average Ranking and Active Testing by Introducing Runtime</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abdulrahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="79" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Warm-starting deep learning model construction using meta-learning</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Nur</forename><surname>Afif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>TU Eindhoven</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Categorical Data Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agresti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Wiley Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On learning algorithm selection for classification</title>
		<author>
			<persName><forename type="first">Shawkat</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><forename type="middle">A</forename><surname>Smith-Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="138" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Metalearning approach to automatic kernel selection for support vector machines</title>
		<author>
			<persName><forename type="first">Shawkat</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><forename type="middle">A</forename><surname>Smith-Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomput</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="173" to="186" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selecting appropriate forecasting models using rule induction</title>
		<author>
			<persName><surname>Arinze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Omega</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="647" to="658" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Task Clustering and Gating for Bayesian Multitask Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="83" to="999" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collaborative hyperparameter tuning</title>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mátyás</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Kégl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2013</title>
		<meeting>ICML 2013</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-generalization: Learning novel classes from a single example by feature replacement</title>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="672" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Internal Representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the search for new learning rules for anns</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jocelyn</forename><surname>Cloutier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="26" to="30" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Unsupervised and Transfer Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating the predictive accuracy of a classifier</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bensusan</surname></persName>
		</author>
		<author>
			<persName><surname>Kalousis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2167</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discovering task neighbourhoods through landmark learning performances</title>
		<author>
			<persName><forename type="first">Hilan</forename><surname>Bensusan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Giraud-Carrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PKDD</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="325" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A higher-order approach to meta-learning</title>
		<author>
			<persName><forename type="first">Hilan</forename><surname>Bensusan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Giraud-Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ILP</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the predictive power of metafeatures in OpenML</title>
		<author>
			<persName><forename type="first">Besim</forename><surname>Bilalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Abelló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomàs</forename><surname>Aluja-Banet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Mathematics and Computer Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="697" to="712" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tomàs Aluja-Banet, and Robert Wrembel. Intelligent assistance for data pre-processing</title>
		<author>
			<persName><forename type="first">Besim</forename><surname>Bilalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Abelló</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Standards &amp; Interf</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="101" to="109" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ASLib: A benchmark library for algorithm selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kerschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Malitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fréchette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="41" to="58" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ranking learning algorithms: Using IBL and meta-learning on accuracy and time results</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pinto Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="277" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Metalearning: Applications to Data Mining</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Giraud-Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Vilalta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ranking learning algorithms: Using IBL and meta-learning on accuracy and time results</title>
		<author>
			<persName><forename type="first">Pavel</forename><forename type="middle">B</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquim</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Coasta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="277" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning many related tasks at the same time with backpropagation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multitask Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-data: Characterization of input features for meta-learning</title>
		<author>
			<persName><forename type="first">Ciro</forename><surname>Castiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanna</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">Maria</forename><surname>Fanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Modeling Decisions for Artificial Intelligence (MDAI)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The evolution of learning: An experiment in genetic connectionism</title>
		<author>
			<persName><forename type="first">Chalmers</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionist Models</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speeding up hyper-parameter optimization by extrapolation of learning curves using previous builds</title>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Chandrashekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">R</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="477" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning to learn without gradient descent by gradient descent</title>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03824</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decision tree and instance-based learning for label ranking</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Hühn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A general framework for distance-based consensus in ordinal ranking models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Seiford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="392" to="397" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning step size controllers for robust neural network training</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1519" to="1525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Annotative experts for hyperparameter selection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giraud-Carrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AutoML Workshop at ICML 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RECIPE: A grammarbased framework for automatically evolving classification pipelines</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz Otavio</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gisele</forename><surname>Pappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Genetic Programming</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="246" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Statistical Comparisons of Classifiers over Multiple Data Sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multiple classifier systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Action Refinement in Reinforcement Learning by Probability Smoothing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Busquets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lopez De Mantaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sierra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Selection of time series forecasting models based on performance information</title>
		<author>
			<persName><forename type="first">Santos</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ludermir</surname></persName>
		</author>
		<author>
			<persName><surname>Prudêncio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Hybrid Intelligent Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="366" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">AlphaD3M: Machine learning pipeline synthesis</title>
		<author>
			<persName><forename type="first">Iddo</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yamuna</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Rampin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raoni</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paula</forename><surname>Lourenco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">Piazentin</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliana</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AutoML Workshop at ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02779</idno>
	</analytic>
	<monogr>
		<title level="m">Fast reinforcement learning via slow reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<title level="m">Efficient Benchmarking of Algorithm Configuration Procedures via Model-Based Surrogates</title>
		<imprint>
			<publisher>Machine Learning</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="15" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Multiple Tasks with Kernel Methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="615" to="637" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Knowledge transfer in learning to recognize visual objects classes</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intern. Conf. on Development and Learning, page Art</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">One-shot learning of object categories. Pattern analysis and machine intelligence</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Scalable meta-learning for Bayesian optimization</title>
		<author>
			<persName><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><surname>Letham</surname></persName>
		</author>
		<author>
			<persName><surname>Bakshy</surname></persName>
		</author>
		<idno>arXiv, 1802.02219</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Using meta-learning to initialize Bayesian optimization of hypxerparameters</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Meta-learning and Algorithm Selection</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Efficient and robust automated machine learning</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2944" to="2952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scalable meta-learning for bayesian optimization using ranking-weighted gaussian process ensembles</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Bakshy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AutoML Workshop at ICML 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dataset metafeature description for recommending feature selection</title>
		<author>
			<persName><forename type="first">Andray</forename><surname>Filchenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arseniy</forename><surname>Pendryak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMW FRUCT</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Object classification from a single example utilizing class relevance metrics</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing syst</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>arXiv 1710.11622</idno>
		<title level="m">Meta-learning and universality</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An evaluation of landmarking variants</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><surname>Petrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD 2001 Workshop on Integrating Aspects of Data Mining, Decision Support and Meta-Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Probabilistic matrix factorization for automated machine learning</title>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishit</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huseyn</forename><surname>Melih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elibol</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05355</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">4ML: A phased performance-based pipeline planner for automated machine learning</title>
		<author>
			<persName><forename type="first">Yolanda</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke-Thia</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Ratnakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Garijo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Brekelmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Kejriwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AutoML Workshop at ICML 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Metalearning-a tutorial</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Giraud-Carrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorial at the International Conference on Machine Learning and Applications</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Toward a justification of meta-learning: Is the no free lunch theorem a show-stopper</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Giraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Foster</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML-2005 Workshop on Meta-learning</title>
		<meeting>the ICML-2005 Workshop on Meta-learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Google vizier: A service for black-box optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Solnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1487" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">André LD Rossi, and André Carvalho. Combining meta-learning and search techniques to select parameters for support vector machines</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Taciana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">Bc</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Prudêncio</surname></persName>
		</author>
		<author>
			<persName><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Recasting gradient-based meta-learning as hierarchical bayes</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08930</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Predicting the performance of learning algorithms using support vector machines as meta-regressors</title>
		<author>
			<persName><forename type="first">Silvio</forename><forename type="middle">B</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">Bc</forename><surname>Prudêncio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><forename type="middle">B</forename><surname>Ludermir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="523" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Discovering Hierarchy in Reinforcement Learning with HEXQ</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hengst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fusion of meta-knowledge and meta-data for case-based model selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hilario</surname></persName>
		</author>
		<author>
			<persName><surname>Kalousis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2168</biblScope>
			<biblScope unit="page" from="180" to="191" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Complexity measures of supervised classification problems</title>
		<author>
			<persName><forename type="first">Kam</forename><surname>Tin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Machine Intellig</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes on Computer Science, 2130</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">An Efficient Approach for Assessing Hyperparameter Importance</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Algorithm runtime prediction: Methods &amp; evaluation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page" from="79" to="111" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Efficient global optimization of expensive black-box functions</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Donald R Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Schonlau</surname></persName>
		</author>
		<author>
			<persName><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="492" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Algorithm Selection via Meta-Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalousis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Geneva, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Representational issues in meta-learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hilario</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2003</title>
		<meeting>ICML 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="313" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Model selection via meta-learning: a comparative study</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Kalousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Hilario</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl Journ. on Artificial Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="554" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName><forename type="first">Maurice G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Designing KDDworkflows via HTN-planning for intelligent discovery assistance</title>
		<author>
			<persName><forename type="first">Jörg-Uwe</forename><surname>Kietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floarea</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Fischer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>In 5th Planning to Learn Workshop at ECAI 2012</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Learning to warm-start Bayesian hyperparameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06219</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Automatic parameter selection by minimizing estimated error</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Machine Learning</title>
		<meeting>the International Conference Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="304" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Combination of task description strategies and case base properties for meta-learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><surname>Iglezakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD Workshop on Integration and Collaboration Aspects of Data Mining</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="65" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Meta-analysis: From data characterization for metalearning to meta-regression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PKDD Workshop on Data Mining, Decision Support, Meta-Learning and ILP</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Exploiting sampling and meta-learning for parameter setting support vector machines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Woznica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IBERAMIA 2002</title>
		<meeting>IBERAMIA 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Agnostic Bayesian learning of ensembles</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tomer D Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Beh. and Brain Sc</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Predicting relative performance of classifiers from samples</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><surname>Brazdil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">An iterative process for building learning curves and predicting relative performance of classifiers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><surname>Brazdil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4874</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Selecting Classification Algorithms with Active Testing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Artif. Intel.</title>
		<imprint>
			<biblScope unit="volume">10934</biblScope>
			<biblScope unit="page" from="117" to="131" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Active testing strategy to predict the best classification algorithm via sampling and metalearning</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Brazdil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Metalearning: a survey of trends and technologies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lemke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gabrys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="130" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Utilizing regression-based landmarkers within a meta-learning framework for algorithm selection</title>
		<author>
			<persName><forename type="first">Daren</forename><surname>Ler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Koprinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<idno>569</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
		<respStmt>
			<orgName>University of Sydney</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Ke Li and Jitendra Malik</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01885</idno>
		<idno>arXiv:1703.00441</idno>
	</analytic>
	<monogr>
		<title level="m">Learning to optimize neural nets</title>
		<imprint>
			<date type="published" when="2016">2016. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning to optimize</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Rank aggregation methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WIREs Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">AST: Support for algorithm selection with a CBR approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Studer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Recent Advances in Meta-Learning and Future Work</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="38" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Data complexity meta-features for regression problems</title>
		<author>
			<persName><forename type="first">Ana</forename><surname>Carolina Lorena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aron</forename><forename type="middle">I</forename><surname>Maciel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Péricles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><forename type="middle">G</forename><surname>De Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">B C</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><surname>Prudêncio</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-017-5681-1</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="246" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A review of automatic selection methods for machine learning algorithms and hyper-parameter values</title>
		<author>
			<persName><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network Modeling Analysis in Health Informatics and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">To tune or not to tune: recommending when to adjust SVM hyper-parameters via meta-learning</title>
		<author>
			<persName><surname>Rafael G Mantovani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André Cplf</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Hyper-parameter tuning of a decision tree induction algorithm</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Rafael G Mantovani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Horváth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André Cplf De</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian Conference on Intelligent Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Joaquin Vanschoren, and André Carlos Carvalho. Meta-learning recommendation of default hyper-parameter values for SVMs in classifications tasks</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gomes Mantovani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD Workshop on Meta-Learning and Algorithm Selection</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Use of meta-learning for hyperparameter tuning of classification problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mantovani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Brazil</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Sao Carlos</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">Donald</forename><surname>Michie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Campbell</surname></persName>
		</author>
		<title level="m">Machine Learning, Neural and Statistical Classification</title>
		<imprint>
			<publisher>Ellis Horwood</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Active testing for SVM parameter selection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B C</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B C</forename><surname>Prudêncio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Algorithm Selection as a Collaborative Filtering Problem</title>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Misir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michèle</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research report</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>INRIA</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Alors: An algorithm recommender system</title>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Mısır</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michèle</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="page" from="291" to="314" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Elizbar</surname></persName>
		</author>
		<author>
			<persName><surname>Nadaraya</surname></persName>
		</author>
		<title level="m">On estimating regression. Theory of Probability &amp; Its Applications</title>
		<imprint>
			<date type="published" when="1964">1964</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="141" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Using meta-mining to support data mining workflow planning and optimization</title>
		<author>
			<persName><forename type="first">Phong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Hilario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Kalousis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="605" to="644" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms. arXiv</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1803">1803.02999v2, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Learning the Structure of Related Tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS Workshop on Inductive Transfer</title>
		<meeting>NIPS Workshop on Inductive Transfer</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Predicting hyperparameters from metafeatures in binary classification problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nisioti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chatzidimitriou</surname></persName>
		</author>
		<author>
			<persName><surname>Symeonidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AutoML Workshop at ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Meta-QSAR: learning how to learn QSARs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Olier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sadawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Bickerton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grosan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soldatova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="285" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Evaluation of a tree-based pipeline optimization tool for automating data science</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Randal S Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Bartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">H</forename><surname>Urbanowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GECCO</title>
		<meeting>GECCO</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="485" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Meta-learning transferable active learning policies by deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AutoML Workshop at ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Improved dataset characterisation for metalearning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><surname>Brazdil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Com. Sc.</title>
		<imprint>
			<biblScope unit="volume">2534</biblScope>
			<biblScope unit="page" from="141" to="152" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Multiple adaptive Bayesian linear regression for scalable Bayesian optimization with warm start</title>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Valerio Perrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><surname>Archambeau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02902</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Meta-learning by landmarking various learning algorithms</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hilan</forename><surname>Bensusan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><forename type="middle">G</forename><surname>Giraud-Carrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="743" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Towards automatic generation of metafeatures</title>
		<author>
			<persName><forename type="first">Fábio</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Mendes-Moreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PAKDD</title>
		<meeting>PAKDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="215" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<author>
			<persName><forename type="first">Fábio</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vítor</forename><surname>Cerqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Mendes-Moreira</surname></persName>
		</author>
		<idno>arXiv, 1706.09367</idno>
		<title level="m">Learning to rank bagging workflows with metalearning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Does Feature Selection Improve Classification? A Large Scale Experiment in OpenML</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martijn</surname></persName>
		</author>
		<author>
			<persName><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">N</forename><surname>Peter Van Der Putten</surname></persName>
		</author>
		<author>
			<persName><surname>Van Rijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Data Analysis XV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="158" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Using genetic algorithms to improve prediction of execution times of ML tasks</title>
		<author>
			<persName><forename type="first">Rattan</forename><surname>Priya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">De</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Comp. Science</title>
		<imprint>
			<biblScope unit="volume">7208</biblScope>
			<biblScope unit="page" from="196" to="207" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Tunability: Importance of hyperparameters of machine learning algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Boulesteix</surname></persName>
		</author>
		<idno>ArXiv 1802.09596</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Efficient progressive sampling</title>
		<author>
			<persName><forename type="first">Foster</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the fifth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Meta-learning approaches to selecting time series models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prudêncio</surname></persName>
		</author>
		<author>
			<persName><surname>Ludermir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="121" to="137" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Transfer Learning by Constructing Informative Priors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Selecting optimal source for transfer learning in Bayesian optimisation</title>
		<author>
			<persName><forename type="first">Anil</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santu</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PRICAI</title>
		<meeting>PRICAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="42" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Informationtheoretic transfer learning framework for Bayesian optimisation</title>
		<author>
			<persName><forename type="first">Anil</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santu</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECMLPKDD</title>
		<meeting>ECMLPKDD</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Gaussian processes in machine learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmussen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced lectures on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Aäron van den Oord</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Paine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10304</idno>
	</analytic>
	<monogr>
		<title level="m">SM Eslami, Danilo Rezende, Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn distributions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Prediction of classifier training time including parameter optimization</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of GfKI</title>
		<meeting>of GfKI</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Meta-learning for evolutionary parameter optimization of classifiers</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="357" to="380" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Automatic classifier selection for non-experts</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="96" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>arXiv 1803.00676</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM 2015</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Using metalearning to predict when parameter optimization is likely to improve classification accuracy</title>
		<author>
			<persName><forename type="first">Parker</forename><surname>Ridd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Giraud-Carrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI Workshop on Metalearning and Algorithm Selection</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="18" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rivolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P F</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C P L F</forename><surname>De Carvalho</surname></persName>
		</author>
		<title level="m">Towards reproducible empirical research in meta-learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1808" to="10406" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Some aspects of the sequential design of experiments</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Herbert Robbins Selected Papers</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">To Transfer or Not To Transfer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NIPS Workshop on transfer learning</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Robust statistics for outlier detection</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Hubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="79" />
			<date type="published" when="2011">2011</date>
			<publisher>Wiley Interdisciplinary Reviews</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Evolution and design of distributed learning rules</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runarsson</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Thor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonsson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Employment of neural network and rough set in meta-learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aboul</forename><surname>Salama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Ella Hassanien</surname></persName>
		</author>
		<author>
			<persName><surname>Revett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memetic Comp</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="177" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Informing the use of hyperparameter optimization through metalearning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giraud-Carrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDM</title>
		<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1051" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">One-shot learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06065</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Hyperparameter optimization with factorized multilayer perceptrons</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML PKDD</title>
		<meeting>ECML PKDD</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to dynamic recurrent networks</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">A neural network that embeds its own meta-levels</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICNN</title>
		<meeting>ICNN</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="407" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Shifting inductive bias with successstory algorithm, adaptive levin search, and incremental self-improvement</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="130" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Feature selection for high-dimensional data: A fast correlation-based filter solution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schoenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giraud-Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poggeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AutoML Workshop at ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A survey of intelligent assistants for data analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">U</forename><surname>Kietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR 2014</title>
		<meeting>CVPR 2014</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Adaptive Generalization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Sharkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J C</forename><surname>Sharkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="313" to="328" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Cross-disciplinary perspectives on meta-learning for algorithm selection</title>
		<author>
			<persName><forename type="first">Kate</forename><forename type="middle">A</forename><surname>Smith-Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Sampling based relative landmarks: Systematically testdriving algorithms before choosing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><surname>Petrak</surname></persName>
		</author>
		<author>
			<persName><surname>Brazdil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3201</biblScope>
			<biblScope unit="page" from="250" to="261" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">A meta-learning method to select the kernel width in support vector regression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="195" to="209" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">An analysis of meta-learning techniques for ranking clustering algorithms applied to artificial data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F De</forename><surname>Ludermir</surname></persName>
		</author>
		<author>
			<persName><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">5768</biblScope>
			<biblScope unit="page" from="131" to="140" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust Bayesian neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Thore Graepel, Luca Pulina, and Armando Tacchella. Collaborative expert portfolio management</title>
		<author>
			<persName><forename type="first">Horst</forename><surname>David H Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Samulowitz</surname></persName>
		</author>
		<author>
			<persName><surname>Herbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="179" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Don&apos;t Rule Out Simple Models Prematurely</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Strang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Van Der Putten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Intelligent Data Analysis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Towards a Framework for Designing Full Model Selection and Optimization Systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Multiple Classifier Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Pairwise meta-rules for better meta-learning-based algorithm ranking</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="161" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Multi-task Bayesian optimization</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2004" to="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prescott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3896</idno>
		<title level="m">Freeze-thaw bayesian optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName><surname>William R Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Lifelong Learning Algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to Learn</title>
		<imprint>
			<publisher>MA</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Learning One More Thing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1217" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Learning to Learn: Introduction and Overview</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to Learn</title>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Experiments in meta-level learning with ILP</title>
		<author>
			<persName><forename type="first">L</forename><surname>Todorovski</surname></persName>
		</author>
		<author>
			<persName><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1704</biblScope>
			<biblScope unit="page" from="98" to="106" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Report on the experiments with feature selection in meta-level learning</title>
		<author>
			<persName><surname>Todorovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PKDD 2000 Workshop on Data mining, Decision support, Metalearning and ILP</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Ranking with predictive clustering trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Todorovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2430</biblScope>
			<biblScope unit="page" from="444" to="455" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Fast Algorithm Selection Using Learning Curves</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abdulrahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IDA</title>
		<meeting>IDA</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">The Online Performance Estimation Framework. Heterogeneous Ensemble Learning for Data Streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="149" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Hyperparameter importance across datasets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2367" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Algorithm selection on data streams</title>
		<author>
			<persName><forename type="first">Jan N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discovery Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="325" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">OpenML: networked science in machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">Understanding Machine Learning Performance with Experiment Databases</title>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Leuven Univeristy</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Experiment databases</title>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Blockeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="127" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">A meta-learning perspective on cold-start recommendations for items</title>
		<author>
			<persName><forename type="first">Manasi</forename><surname>Vartak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conrado</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeshua</forename><surname>Bratman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6904" to="6914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Understanding accuracy performance through concept characterization and algorithm analysis</title>
		<author>
			<persName><surname>Vilalta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Recent Advances in Meta-Learning and Future Work</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">A characterization of difficult problems in classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vilalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Drissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICMLA</title>
		<meeting>ICMLA</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<author>
			<persName><forename type="first">Jane X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeb</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruva</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Botvinick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05763</idno>
		<title level="m">Learning to reinforcement learn</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<title level="m" type="main">Importance of tuning hyperparameters of machine learning algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Weerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>TU Eindhoven</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Ml-plan for unlimited-length machine learning pipelines</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Wever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AutoML Workshop at ICML 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Learning hyperparameter optimization initializations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Hyperparameter search space pruning, a new component for sequential model-based hyperparameter optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="104" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Scalable Gaussian processbased transfer surrogates for hyperparameter optimization</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="78" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">No free lunch theorems for search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<idno>SFI-TR-95-02-010</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>The Santa Fe Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Akimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Udell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03233</idno>
		<title level="m">Oboe: Collaborative filtering for automl initialization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Efficient transfer learning method for automatic hyperparameter tuning</title>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI and Statistics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
