<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Teaching Uncertainty Quantification in Machine Learning through Use Cases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matias</forename><surname>Valdenegro-Toro</surname></persName>
						</author>
						<title level="a" type="main">Teaching Uncertainty Quantification in Machine Learning through Use Cases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3C0DD419046E9B41EEE0C4A8B66984F7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Uncertainty in machine learning is not generally taught as general knowledge in Machine Learning course curricula. In this paper we propose a short curriculum for a course about uncertainty in machine learning, and complement the course with a selection of use cases, aimed to trigger discussion and let students play with the concepts of uncertainty in a programming setting. Our use cases cover the concept of output uncertainty, Bayesian neural networks and weight distributions, sources of uncertainty, and out of distribution detection. We expect that this curriculum and set of use cases motivates the community to adopt these important concepts into courses for safety in AI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural networks and machine learning models are ubiquitous in real-world applications, but in general model and data uncertainty are not well explored, and this propagates on how machine learning is taught at different levels. Uncertainty is an important concept that should be taught to all students interested in machine learning.</p><p>Overall Uncertainty Quantification of machine learning models <ref type="bibr" target="#b4">(Gawlikowski et al., 2021)</ref> is not part of the standard curricula at the undergraduate or graduate level, mostly being present in advanced summer schools (like MLSS, EEML, DeepLearn, SMILES, etc), with some exceptions at graduate courses aimed mostly at theory of Bayesian NNs (BNNs).</p><p>In this paper we aim to develop a concept for teaching uncertainty quantification in machine learning, first with a short curriculum, and then through different use cases, starting from why we need models with uncertainty and ending at out of distribution detection. We hope that this material can be used for easier planning of future courses.</p><p>Teaching with clear use cases can be beneficial for student's learning <ref type="bibr" target="#b8">(Lynn Jr, 1999)</ref>, specially when they are combined with practical experience.</p><p>Uncertainty in ML is a subject that is heavy on probability and statistics, and this is a topic that might not be easy for some students. We believe that having clear use cases for this purpose can help students learn and to clarify concepts. These use cases can be implemented in code using standard machine learning frameworks like Keras, TensorFlow, and PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Curricula for UQ in ML</head><p>We first introduce a short curricula template for a uncertainty in machine learning course. This could be a graduate-level course, requiring students to know basic neural networks, machine learning theory, and probability and statistics, as well as having appropriate coding skills in a programming language in order to understand and implement the use cases in a framework of their choice.</p><p>The overall curriculum is presented in Table <ref type="table">1</ref>. Any teacher should of course adapt this course to their institution or student body, and we encourage the teacher to also include seminar-style discussions including state of the art research in BNNs and uncertainty in ML, as this is still a very research heavy field.</p><p>The ultimate goal of this course is to enable students to perform research in this field, and to apply this knowledge into neighboring task field like Computer Vision, Reinforcement Learning, or Robotics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Use Cases</head><p>In this section we present a selection of use cases to teach concepts of uncertainty in machine learning settings. These represent what we think are the most difficult concepts for students to grasp, which motivate the application of use cases as teaching methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Output Uncertainty</head><p>The best use case to teach the concept of uncertainty at the output of a machine learning model is a simple regression setting, as the output mean can be associated to the output</p><p>Unit Content Introduction to UQ Point-wise outputs versus distribution outputs in ML models. Two-headed models for regression. Sources of uncertainty. Representations of output uncertainty. Applications and possible legal requirements. Relationship to Explainable AI. Connections to safety and trustworthiness in AI. Statistical Methods Categorical, Gaussian, and Dirichlet Distributions. Predictive intervals, Quantile Regression. Bayesian NNs Distribution over weights. Predictive posterior distribution. Inference using Bayes Rule. Methods for UQ Deep Ensembles <ref type="bibr" target="#b7">(Lakshminarayanan et al., 2016)</ref>, Monte Carlo methods like Dropout <ref type="bibr" target="#b3">(Gal &amp; Ghahramani, 2016)</ref> and DropConnect <ref type="bibr" target="#b9">(Mobiny et al., 2019)</ref>. For advanced courses, Gaussian Processes <ref type="bibr" target="#b10">(Rasmussen &amp; Williams, 2005)</ref> and Markov Chain Monte Carlo methods <ref type="bibr" target="#b0">(Betancourt, 2017)</ref> can also be included. Metrics and Evaluation Losses with uncertainty, Entropy, Calibration, Reliability plots, and related calibration metrics <ref type="bibr" target="#b5">(Guo et al., 2017)</ref>. Advanced topics can be proper scoring rules <ref type="bibr" target="#b1">(DeGroot &amp; Fienberg, 1983</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out of Distribution</head><p>In distribution and Out of distribution data.</p><p>Evaluation protocol with standard datasets Detection (CIFAR10 vs SVHN, MNIST vs Fashion MNIST). Evaluation using histograms and ROC curves. Challenges and Scalability of BNNs, Generalization of out of distribution detection, Computational Future Research performance, Datasets with uncertainty, and Real-world applications (Valdenegro-Toro, 2021). Table 1. Curriculum for a graduate course in Uncertainty Quantification in Machine Learning</p><p>of a classical model (without uncertainty), and the standard deviation of the output can be directly associated with the uncertainty in the output. In a classification setting with probabilities associated to each class, it is more difficult to directly see the effect of uncertainty in the model.</p><p>Learning Objective. Students will learn about the difference between a classical machine learning model and one with output uncertainty.</p><p>Use Case. Students will implement a standard neural network using a framework of their or the teacher's choice. Students will generate data by sampling the following function:</p><formula xml:id="formula_0">f (x) = sin(x) + (1) ∼ N (0, σ(x)) (2) σ(x) = 0.15(1 + e -x ) -1<label>(3)</label></formula><p>For the range x ∈ [-π, π]. Two neural network models can be used. One is a standard neural network and the other is a ensemble of 5 neural networks <ref type="bibr" target="#b7">(Lakshminarayanan et al., 2016)</ref>, which is a simple method to estimate uncertainty. An example of this setting can be seen in Figure <ref type="figure" target="#fig_0">1</ref>, where output uncertainty is represented as confidence intervals. Students can the visually compare their results, and relate on how the standard neural network does not model the training data points variations, while the neural network with uncertainty does. This is specially noticeable as the standard deviation of the noise is variable, which is not captured with a standard neural network.</p><p>A variation of this exercise is to use a deep ensemble, where each ensemble member has two output heads, one for the task (µ(x)) and another for uncertainty (σ 2 (x)), which can be trained with a negative log-likelihood loss that does not require labels for uncertainty (Eq 5). This exercise helps students see that a model needs to be "added something" to estimate uncertainty properly, an output head in this case. The uncertainty head σ 2 (x) represents the variance of the output .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bayesian Neural Networks</head><p>Bayesian Neural Networks are difficult to understand conceptually since the formulation is heavy in probability, and weights are replaced by probability distributions. In this use case we simplify the concept for easy understanding.</p><p>Learning Objective. Students will learn the conceptual differences between a standard and an Bayesian neural network and how it relates to produce uncertainty at the model output.</p><p>Use Case. Students will implement the forward pass of a simple neural network using numpy or a similar linear algebra framework. For a standard neural network, scalar or point-wise weights are used, and for a BNN, weights will be drawn from a given Gaussian distribution (the actual weight values for this use case do not matter). Sampling can be used to produce predictions from a BNN, by sampling a set of weights and producing a forward pass with a given input.</p><p>Students will compare the outputs given random weights for each of their networks, and compare how the BNN is a stochastic model, meaning that predictions vary with a given input, as different weights are sampled and propagate</p><formula xml:id="formula_1">-3 -2 -1 0 1 2 3 -2 -1 0 1 2 Prediction Ground Truth (a) Classic Neural Network -3 -2 -1 0 1 2 3 -2 -1 0 1 2</formula><p>Prediction Uncertainty (b) Neural Network with Output Uncertainty through the network to produce different outputs, but these predictions are not completely random, and are samples of the predictive posterior distribution.</p><p>In comparison, the standard neural network has fixed predictions with a given input and weights, which cannot model uncertainty. An additional experiment is to vary the depth or width of the network, as a way to increase the number of weights, and see how predictions change in terms of stochasticity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bayesian NN Intractability</head><p>Connecting to the previous use case, it is well known that inference in BNNs is intractable, due to the high computational complexity required to estimate weight distributions, particularly for highly parameterized neural networks. In this use case we wish the student to form an intuition on why this is the case.</p><p>Learning Objective. Students will learn an intuition on why BNNs are intractable with a thought experiment and validate it with a code implementation.</p><p>Use Case. As a thought experiment, students should think about the predictive posterior distribution (Eq 4), which integrates a term over the weights of the network to produce a distribution output.</p><formula xml:id="formula_2">P (y | x) = w P (y | x, w)P (w)dw<label>(4)</label></formula><p>For the experimental setting, students should implement a simple BNN using numpy, with randomly initialized weight distributions (Gaussian distributions can be used for simplicity), and then produce predictions with random data (similarly to the previous use case). But then students are asked to vary their network architectures, increasing depth from a few layer to over 50 layers, or the width from a small number to a large number (over 1024 neurons), and then estimate and plot the computation time as network depth and number of samples is varied.</p><p>Students the analyze their results and comment on the appli-cability of BNNs for real-world applications, considering their computational costs. Additional experiments for discussion are the possibilities of computing the integral in Eq 4 with analytical or numerical methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Aleatoric vs Epistemic Uncertainty</head><p>Different sources of uncertainty <ref type="bibr" target="#b2">(Der Kiureghian &amp; Ditlevsen, 2009)</ref> and their separation <ref type="bibr" target="#b6">(Kendall &amp; Gal, 2017)</ref> are not always easy to see and learn intuitively. This use case tries to show the difference with a practical example in a regression setting.</p><p>Learning Objective. Students will learn the difference between aleatoric and epistemic uncertainty through a simple regression problem, and how different parts of the model contribute to these sources of uncertainty.</p><p>Use Case. We will use the same setting as the output uncertainty use case (Sec 3.1), but only a model with uncertainty.</p><p>Since an ensemble is used to estimate uncertainty in this case, we will use the negative log-likelihood loss formulation to estimate aleatoric uncertainty:</p><formula xml:id="formula_3">L(y n , x n ) = log σ 2 i (x n ) 2 + (µ i (x n ) -y n ) 2 2σ 2 i (x n )<label>(5)</label></formula><p>Students should train an ensemble of 5 networks, and then plot the predictions separately. First students plot the predictions of the mean output of each ensemble member (which produces epistemic uncertainty), and then separately plot the standard deviation outputs of each ensemble member (which estimate aleatoric uncertainty). This concept is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Students then compare both kinds of predictions and try to explain the differences, and how they relate to the epistemic and aleatoric sources of uncertainty. A plot of the training data might also help students visualize aleatoric uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Out of Distribution Detection</head><p>Out of distribution detection entails detecting input samples outside of the training set distribution, through output un- certainty or other confidence measures. In this setting we present two use cases.</p><formula xml:id="formula_4">-4 -2 0 2 4 6 -2 -1 0 1 2 3σ 2σ σ (a) Aleatoric Uncertainty -6 -4 -2 0 2 4 6 -2 -1 0 1 2 (b) Epistemic Uncertainty</formula><p>Learning Objective. Students will learn how to perform and evaluate out of distribution using standard image classification datasets and in a regression toy example, and to get the intuitions on how uncertainty enables the out of distribution detection task.</p><p>Classification Use Case. Using an appropriate neural network framework, students will implement and train a BNN (or an approximation) on the SVHN dataset (ID, indistribution), and evaluate in the train and test splits. Then students are asked to make predictions using their model on the CIFAR10 test set (OOD, out-of-distribution) and to look at the class probabilities that their model produces. Entropy can be used to obtain a single measure for each sample, and then compare the ID vs OOD entropy values using a histogram. The use case can be completed by obtaining a threshold between ID and OOD entropy distributions using an ROC curve, in order to perform out of distribution detection in the wild.</p><p>Regression Use Case. For a toy example in regression, we use the same setting as Sec 3.1, keeping the training set x ∈ [-π, π], and introducing an OOD set of x ∈ [-2π, -π] ∪ <ref type="bibr">[π, 2π]</ref>. Students then plot the predictions from their model, noting the values on the two datasets (ID and OOD). A sample result can be seen in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Students should compare the output standard deviation produced by their model in the ID and OOD datasets. They should observe that uncertainty as predicted by the output standard deviation should be higher in the OOD data than in the ID data, which indicates that the model is extrapolating. Students can add additional evidence of extrapolation by plotting the function f (x) = sin(x) which is the true function that generated the training data, and confirm that the model predictions in the OOD data are very incorrect when compared to the true function, while predictions in the ID data are correct inside the training range ([-π, π]). Error metrics like mean absolute error can be used to confirm this difference.</p><p>The teacher can also show that uncertainty in the OOD set should be proportional to the distance (in input space) from the sample to the edge of the OOD set, and that this proportionality is expected for proper uncertainty quantification.</p><p>Misconceptions. Students might be confused that some OOD examples have low uncertainty and are easily confused with ID examples. This can be explained with models are not perfect and make mistakes, and this also translates into mistakes in OOD.</p><p>Another issue is the definition of out of distribution data can be very abstract, as it is an open set that corresponds to anything not in the training data distribution. Multiple OOD datasets can be used to show this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and Future Work</head><p>In this paper we have presented a small course curriculum and a selection of use cases to teach students about uncertainty quantification in machine learning models. We hope that this work can motivate the community about the importance of teaching uncertainty quantification and BNNs to students learning about machine learning, and how it relates to the concept of safety in artificial intelligence.</p><p>Future course contents and use cases can be centered in specific applications of machine learning and artificial intelligence, such as Computer Vision, Robotics, or Autonomous Systems. There is a good demand to connect theoretical fields (BNNs in particular) into practical applications as a way to lead future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparison between classic and neural networks with output uncertainty for regression of f (x) = sin(x) + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison between Epistemic and Aleatoric Uncertainty in the Toy Regression example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Out of Distribution Detection in the Toy Regression Example. Values x &gt; π and x &lt; -π are out of distribution in this example, which triggers large epistemic uncertainty and can be used to detect this condition.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>German Research Center for Artificial Intelligence, Bremen, Germany. Correspondence to: Matias Valdenegro-Toro &lt;matias.valdenegro@dfki.de&gt;.Proceedings of the</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>nd Teaching in Machine Learning Workshop, PMLR, 2021. Copyright 2021 by the author(s).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02434</idno>
		<title level="m">A conceptual introduction to hamiltonian monte carlo</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The comparison and evaluation of forecasters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Degroot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series D (The Statistician)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="12" to="22" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Aleatory or epistemic? does it matter? Structural safety</title>
		<author>
			<persName><forename type="first">A</forename><surname>Der Kiureghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ditlevsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gawlikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R N</forename><surname>Tassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kruspe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roscher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03342</idno>
		<title level="m">A survey of uncertainty in deep neural networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04599</idno>
		<title level="m">On calibration of modern neural networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04977</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01474</idno>
		<title level="m">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Teaching and learning with cases: A guidebook</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Lynn</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>CQ Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Mobiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04569</idno>
		<title level="m">Dropconnect is effective in modeling uncertainty of bayesian deep networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning (adaptive computation and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">I find your lack of uncertainty in computer vision disturbing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valdenegro-Toro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
