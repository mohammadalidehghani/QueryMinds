<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Theoretical Robopsychology: Samu Has Learned Turing Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-10-25">October 25, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
							<email>batfai.norbert@inf.unideb.hu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">University of Debrecen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Theoretical Robopsychology: Samu Has Learned Turing Machines</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-10-25">October 25, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">60CF4CF50D25E6F672E6F8D29170F05B</idno>
					<idno type="arXiv">arXiv:1606.02767v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From the point of view of a programmer, the robopsychology is a synonym for the activity is done by developers to implement their machine learning applications. This robopsychological approach raises some fundamental theoretical questions of machine learning. Our discussion of these questions is constrained to Turing machines. Alan Turing had given an algorithm (aka the Turing Machine) to describe algorithms. If it has been applied to describe itself then this brings us to Turing's notion of the universal machine. In the present paper, we investigate algorithms to write algorithms. From a pedagogy point of view, this way of writing programs can be considered as a combination of learning by listening and learning by doing due to it is based on applying agent technology and machine learning. As the main result we introduce the problem of learning and then we show that it cannot easily be handled in reality therefore it is reasonable to use machine learning algorithm for learning Turing machines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Samu is a disembodied developmental robotic experiment to develop a family chatterbot agent who will be able to talk in a natural language like humans do <ref type="bibr" target="#b2">[Bát15a]</ref>. At this moment it is only an utopian idea of the project Samu. The practical purpose of Samu projects is to develop computational mental organs that can support software agents to acquire higher-order knowledge from their input <ref type="bibr" target="#b9">[BB16]</ref>. The activities have been conducted during the development of such mental organs may be considered as first efforts to create on demand the Asimovian profession called robopsychology<ref type="foot" target="#foot_0">foot_0</ref>  <ref type="bibr" target="#b4">[Bát16a]</ref>.</p><p>The roots of this paper lie in the two new software experiments Samu Turing <ref type="bibr" target="#b6">[Bát16c]</ref> and Samu C. Turing <ref type="bibr" target="#b5">[Bát16b]</ref>. These are very simplified versions of the former habituation-sensitization <ref type="bibr" target="#b10">[CS14]</ref> based (like for example SamuBrain <ref type="bibr" target="#b7">[Bát16d]</ref> or SamuKnows <ref type="bibr" target="#b8">[Bát16e]</ref>) learning projects of Samu. Their common feature is that they use the same COP-based Q-learning engine that the chatbot Samu does. To be more precise the mental organs use the same code (to see this compare <ref type="url" target="https://github.com/nbatfai/SamuLife/blob/master/SamuQl.h">https://github.com/nbatfai/SamuLife/blob/master/  SamuQl.h</ref> with <ref type="url" target="https://github.com/nbatfai/nahshon/blob/master/ql.hpp">https://github.com/nbatfai/nahshon/blob/master/ql.hpp</ref>) as the chatbot does. The term "COP-based" (Consciousness Oriented Programming <ref type="bibr" target="#b1">[Bát11]</ref>) means that the engine predicts its future input. The engine itself is based on the Q-learning that receives positive reinforcement if the chatbot (or a mental organ) can successfully forecast the next input of Q-learning in the actual step. In this case the previous output (the previous prediction) is the same as the actual input, for precise details see <ref type="bibr" target="#b2">[Bát15a]</ref> and <ref type="bibr" target="#b9">[BB16]</ref>). In to write this paper stems from the last paragraph of the work of Neumann on the general theory of automata <ref type="bibr">[vN51]</ref> where Neumann had suggested that there is a complexity level above which the machines can reproduce themselves and even more complicated ones. Neumann investigated the self-reproducing automata <ref type="bibr">[vN51]</ref> roughly a decade after Alan Turing had published his work on universal simulation theorem <ref type="bibr" target="#b16">[Tur36]</ref>. The Turing machine is a precise form of the informal notion of the algorithm to describe algorithms. If this description algorithm has been applied to describe itself then this brings us to Turing's notion of the universal machine. In an intuitive sense we can say that Neumann replaced Turing's notion of simulation with the notion of reproduction. In this work we would like to replace the reproduction with the learning. To be more precise we investigate algorithms to write algorithms. For simplicity of our discussion the scope of this paper is constrained to Turing machines. It should be noticed that we could have used other universal computing models such as the Cellular Automata. For example, the first mental organs had learned the Conway's Game of Life <ref type="bibr" target="#b9">[BB16]</ref> (or see the YouTube video at <ref type="url" target="https://youtu.be/_W0Ep2HpJSQ">https://youtu.be/_W0Ep2HpJSQ</ref>). But in spite of this, we chose Turing machines because they are closer to the programmers' intuition.</p><p>The structure of this paper is as follows: the next section introduces the basic notations. Then, in Sect. 3 we present the results of two Samu-based developmental robotic software experiments to learn how Turing machines operate. Here we investigate some specific TMs. It should be noticed that some of them, such as the machines of Schult and Uhing or the Marxen and Buntrock's BB5 champion machine are famous in the field of the Radó Tibor's Busy Beaver problem <ref type="bibr" target="#b14">[LV08]</ref>. It is worth noting that despite that this problem is a very interesting theoretical computer science problem we do not address it in this paper. We introduce of the learning problem and give the basic notions of this subject. Finally we present a new complexity measure called self-reproduction complexity and we show in Subsect. 3.2.3 that it is reasonable to use machine learning algorithm for learning Turing machines. The paper is closed by a short conclusion in which some possible directions for further work are pointed out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notations and Technical Background</head><p>Throughout both this article and our software experiments we use the definition of the Turing machine (TM) that was introduced in [Bát09] and also used in <ref type="bibr" target="#b3">[Bát15b]</ref> where the Turing machine was defined by a quadruple T = (Q, 0, {0, 1}, f ) where f : Q × {0, 1} → Q × {0, 1} × {←, ↑, →} is a partial transition function and 0 ∈ Q ⊂ N is the starting state. As usual a configuration determines the actual state of the head, the position of the head and the contents of the tape. With the notation of <ref type="bibr" target="#b0">[Bát09]</ref> a configuration can be written in the form w bef ore [q &gt; w af ter , where w bef ore , w af ter ∈ {0, 1} * and q ∈ Q.</p><p>In some proofs for simplicity's sake we use multitape Turing machines or the blank symbol on the tape (that is the tape alphabet is extended by the symbol ). In addition, without limiting the generality, we may assume that halting Turing machines (with a given input) do not contain unused transition rules. The notation T (x) &lt; ∞ denotes that the machine T with the input x halts.</p><formula xml:id="formula_0">Definition 2.0.1 (configN). The word b N . . . b 1 [q &gt; a 0 a 1 . . . a N over the alpha- bet {0, 1, [, &gt;} ∪ Q where a i , b j ∈ {0, 1} is referred to as a configN configuration if there is a configuration w bef ore [q &gt; w af ter such that w bef ore [q &gt; w af ter = w , bef ore b N . . . b 1 [q &gt; a 0 a 1 . . . a N w , af ter . Remark 2.0.1 (config∞).</formula><p>In some cases, see for example Remark 3.2.1, we extend the definition of the configuration as follows ∞ w bef ore [q &gt; w af ter ∞ . In this sense a usual configuration corresponds to a config∞ configuration where w , bef ore = w , af ter = λ the empty word. We may note that the release of the project Samu C. Turing used in Fig. <ref type="figure" target="#fig_1">2</ref> uses config4 configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning by Listening and Doing</head><p>In the aforementioned projects Samu Turing and Samu C. Turing we programmed the Samu agent to work in a similar way as, for example, Professor James Harland did in his work <ref type="bibr" target="#b11">[Har16]</ref> where he observed and studied the configurations of Marxen and Buntrock's Busy Beaver champion machines <ref type="bibr" target="#b15">[MB90]</ref>.</p><p>In our experiments the agent Samu observes (listening) the consecutive subconfigurations of a given investigated Turing machine and try to predict (doing) the next rule of the machine that will be applied. From this viewpoint this whole learning process can be seen as a way of learning by listening and doing where the listening part is the sensation of the agent and doing is the prediction of the agent. But the question may naturally be raised why should we use agent technology and machine learning algorithms to learn Turing machines? Our explicit answer is based on the following intuitive results and it will be found in Sect. 3.2.3. q q q q q q q q q 1e+03 1e+04 1e+05 1e+06 1e+07</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some running and learning times</head><p>The number of ones</p><p>The number of steps 26 14 21 32 160 501 1915 1471 4097</p><p>Figure <ref type="figure" target="#fig_1">2</ref>: This figure shows the usual running time (time complexity) of some given machines and the learning time of these investigated machines. The blue curve is the usual time complexities and the red one is the running times of the learning. The x-axis labeled with the number of ones printed by the Turing machines "26", "14", "21", "32", "160", Schult ("501"), Uhing ("1915"), Uhing ("1471") and Marxen-Buntrock ("4097"). For more precise details see <ref type="url" target="https://github.com/nbatfai/SamuCTuring/releases/tag/vPaperTheorRobopsy">https://github.com/nbatfai/SamuCTuring/releases/tag/vPaperTheorRobopsy</ref> and Table <ref type="table" target="#tab_1">1</ref>. The exact values can be found in Table <ref type="table" target="#tab_1">1</ref>. One of the notions of cognitive complexity defined in Subsect. 3.2.3 will be based on this intuitive "learning complexity". In Fig. <ref type="figure" target="#fig_1">2</ref>, it seems that the growth rate of the learning time is related to the running time. It is worth to compare this with Fig. <ref type="figure" target="#fig_4">6</ref> where the growth rate of an another (the "self-reproducing") complexity has already been separated from the running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Some Intuitive Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Basic Notions of the Subject</head><p>From the observations of the two experiments above, we can build the abstract model of learning that is referred to as the learning problem. The learning problem of learning TMs is divided into two parts. The first is a simulation of the TM to be learned. The second is the actual learning problem itself. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">The Running Problem</head><p>It is obvious that the running problem trivially contains the halting problem. Therefore we may notice that similar undecidable statements can be made for this case as well but in this paper we only focus on halting machines. Lemma 3.2.1. Apart from the trivial case of the empty tapes, the transition rule between two consecutive configurations c i and c i+1 is uniquely determined by the configurations c i and c i+1 .</p><p>Proof. Suppose that there are two transition rules (q, r) → (q 1 , w 1 , d 1 ) and (q, r) → (q 2 , w 2 , d 2 ) where q, q 1 , q 2 ∈ Q, r, w 1 , w 2 ∈ {0, 1, }, d 1 , d 2 ∈ {←, ↑, →} and then we show that q 1 = q 2 , w 1 = w 2 and d 1 = d 2 .</p><p>Let c i = Ll[q &gt; rR where l ∈ {0, 1, }, L, R ∈ {0, 1, } ∞ , Then the following cases are possible</p><formula xml:id="formula_1">c i+1 = Ll[q 1 &gt; w 1 R, (d 1 =↑)                              = Ll[q 2 &gt; w 2 R, (d 2 =↑) ⇔ (q 1 = q 2 , w 1 = w 2 ) = L[q 2 &gt; lw 2 R, (←) ⇔ (q 1 = q 2 , Ll = L, w 1 R = lw 2 R that is, iff l, w 1 , w 2 = and R, L = ∞ ) = Llw 2 [q 2 &gt; R, (→) ⇔ (q 1 = q 2 , Ll = Llw 2 , w 1 R = R that is, iff l, w 1 , w 2 = and R, L = ∞ ) c i+1 = L[q 1 &gt; lw 1 R, (←)                = L[q 2 &gt; lw 2 R, (←) ⇔ (q 1 = q 2 , w 1 = w 2 ) = Llw 2 [q 2 &gt; R, (→)⇔ (q 1 = q 2 , L = Llw 2 , lw 1 R = R that is, iff l, w 1 , w 2 = and R, L = ∞ ) c i+1 = Llw 1 [q 1 &gt; R, (→) = Llw 2 [q 2 &gt; R, (→) ⇔ (q 1 = q 2 , w 1 = w 2 ) Remark 3.2.1.</formula><p>It is noted that we may give an even more simpler lemma and proof using the usual * and {0, 1, } * instead of ∞ and {0, 1, } ∞ . We use the latter because they are closer to the programmers' intuition.</p><p>Theorem 3.2.2 (Universal Learning). There exist an universal running machine R and a learning machine S such that, for all halting Turing machines T , it holds that S(R(T, x)) = T .</p><p>Proof. The proof is divided into two parts: in the first one, we modify the usual proof of Turing's universal simulation theorem (see for example the textbook <ref type="bibr" target="#b13">[ISR00]</ref>) to produce the sequence of configurations of T by the universal machine R. In the other part we focus the learning of S by using the previous lemma. We provide only an outline of the first part. We use a multitape TM for the implementation of R. But from the point of view of R they may be "interpreted" as ∞ from left and from right.</p><p>Then the theorem follows from Lemma 3.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">The Learning Problem</head><p>The previous theorem shows that there is no problem with learning if we use config∞ (or the usual) configurations. But otherwise, as shown in the following two simple examples of config2 configurations (Example 3.2.1 and 3.2.2) the applied transition rule between two consecutive configN configurations may be encoded T and x . . .</p><formula xml:id="formula_2">c i c 1 c 2 . . . c i</formula><p>Figure <ref type="figure">5</ref>: This figure shows that the denoted configuration c i is copied (and collected) to the output tape after the simulation of the i-th step of T .</p><p>not uniquely determined by the configN configurations. If we use configN configurations instead of the usual or config∞ configurations then the Lemma 3.2.1 does not hold. In the next subsection a notion of complexity will be exactly based on this property.</p><p>Example 3.2.1. Let c i = ∞ 11111[q &gt; 11111 ∞ be a config∞ configuration and c , i be a corresponded config2 configuration. Then the rules (q, 1) → (q, 1, ←), (q, 1) → (q, 1, →), and (q, 1) → (q, 1, ↑) yield the same c , i+1 = 11[q &gt; 11 config2 configuration.</p><p>Example 3.2.2. Let c i = ∞ 0101[q &gt; 1101 ∞ be a config∞ configuration and c , i be a corresponded config2 configuration. Then the rules (q, 1) → (q, 0, ←) and (q, 1) → (q, 0, →) yield the same c , i+1 = 10[q &gt; 10 config2 configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Cognitive Complexities</head><p>As has already been mentioned in Sect. 3.1 we intuitively use the running time of the learning machines as a complexity measure that may be formulated as follows cc(T, x) = min{t S ( c i x T ) |T (x) &lt; ∞, S(R(T, x)) = T } but it does not seem very helpful because it is probably correlated with the usual time complexity of T as it is suggested by Fig. <ref type="figure" target="#fig_1">2</ref>. The next type of complexity tells what is the first finite N for which Lemma 3.2.1 holds with using the configurations conf igN . To be more precise, it is defined as cc * (T, x) = min{N |T (x) &lt; ∞, S(R(T, x)) = T and for configN the lemma 3.2.1 holds} that has shown different behavior than the previous one as it can be seen in Fig. <ref type="figure" target="#fig_4">6</ref> The growth rate of the investigated cc * values not related to the number of ones rather than to the running time (see "14", "21" and "1471").</p><p>The results shown in Fig. <ref type="figure" target="#fig_4">6</ref> also suggest that it is hopeless to handle the learning problem with the universal learning machine S of Lemma 3.2.1. This justifies the using of agent technology (an agent observes the operation of the investigated TMs) and machine learning algorithms (such as Q-learning) to learn Turing machines instead of searching for suitable configNs for any universal learning machine S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we started with two developmental robotic software experiments Samu Turing <ref type="bibr" target="#b6">[Bát16c]</ref> and Samu C. Turing <ref type="bibr" target="#b5">[Bát16b]</ref> to learn how Turing ma- The values are computed by the version of the project Samu C. Turing that tagged by self-reproducing complexity, see <ref type="url" target="https://github.com/nbatfai/SamuCTuring/tree/self-reproducing">https://github.com/nbatfai/SamuCTuring/tree/self-reproducing</ref> complexity where a manual binary search was also used to determine the last three cc * values. The x-axis is exactly the same as in Fig. <ref type="figure" target="#fig_1">2</ref>. chines operate. This subject of the experiments itself enabled us to investigate the theoretical properties of learning. First, we have eliminated from our software experiments the developmental robotic processes (for example the habituation-sensitization parts) and then we introduced the problem of learning and some complexity measures based on it. For some cases of given TMs we also determine these complexities. The cc * of machines of greater sophistication cannot easily be computed by the universal learning machine S of Theorem 3.2.2. This justifies the usage of agent technology and machine learning for learning Turing machines. We have provided only an outline of the proof of Theorem 3.2.2. To complete it may be a further theoretical computer science work. Further work of a practical robopsychological nature is also needed. For example, we are going to investigate using Samu's neural architecture <ref type="bibr" target="#b2">[Bát15a]</ref>, Samu mental organs (like MPUs) <ref type="bibr" target="#b9">[BB16]</ref> and deep learning to learn how TMs operate.</p><p>To return to Neumann's train of thought mentioned in the introduction it seems to be interesting to study when the learning algorithm has been applied to write itself. Let's start from a machine T that halts with x. It follows from Theorem 3.2.2 that R(T, x) = c i x T and S(R(T, x)) = T . But then we can also learn this learning of T , that is R(S, c i   c i c i x T S</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S</head><p>then we can easily write that cc(S, y j ) &lt; cc(S, y j+1 ) because t S (y j ) &lt; t S (y j+1 ) but the similar relation between cc * (S, y j ) and cc * (S, y j+1 ) is an open question at this moment.</p><p>It is clear, of course, that further work of a theoretical robopsychological nature is required as well. For example, we are going to find possible relations among the time, space, Kolmogorov and cognitive complexities. We believe that this is a necessary step towards achieving the situation that has been defined as "Programs hacking programs" by Neo in the movie "The Matrix Reloaded". In the framework of Turing machines and Busy Beaver problem this quotation has a special meaning namely that can we program a computer program not only to discover a BB machine but to build it from scratch?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure 1: This is a screenshot from the project Samu Turing. The reality shown in the left side is generated by the operation of a given Turing machine. The right side shows the predicted configurations of the investigated Turing machine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig.2summarizes and compares some running results produced by the project Samu C. Turing. The numbers of two kinds of running times (usual time complexity and "learning complexity", see the caption of the figure for details) are not directly comparable because they use different scales to compute the y-axis values. One of the two curves is computed by the number of steps of a Turing machine and the other by the number of sensory-action pairs of the reinforcement learning agent Samu C. Turing. The exact values can be found in Table1. One of the notions of cognitive complexity defined in Subsect. 3.2.3 will be based on this intuitive "learning complexity". In Fig.2, it seems that the growth rate of the learning time is related to the running time. It is worth to compare this with Fig.6where the growth rate of an another (the "self-reproducing") complexity has already been separated from the running time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: This figure shows the schematic of the learning problem. The universal machine R takes two input parameters the description T of a TM and the input x of the machine T . The machine R computes the sequence c i x T of configurations occurred during the execution of the machine T with its input x. Then the learning machine S takes this sequence and finally S has to figure out from this input sequence what was actually simulated by the machine R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: This figure shows the preparation of the tapes of R. On the second last tape R denotes the used cells with the symbols and . From the point of view of T these symbols are interpreted as the blank symbol on the tape. But from the point of view of R they may be "interpreted" as ∞ from left and from right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: This figure shows the cc * values of machines of Fig. 2.The values are computed by the version of the project Samu C. Turing that tagged by self-reproducing complexity, see https://github.com/nbatfai/SamuCTuring/tree/self-reproducing complexity where a manual binary search was also used to determine the last three cc * values. The x-axis is exactly the same as in Fig.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>And then we can learn again the learning of learning of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>S</head><figDesc>and so on. If we introduce the notationy j = c i . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>T , that is, to be moret T 1s of T (λ) cc(T, λ)This table numerically shows the cc * values of the investigated machines. The combine columns show the given TM in the form of rule-index notation<ref type="bibr" target="#b3">[Bát15b]</ref>.</figDesc><table><row><cell>cc * (T, λ)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://en.wikipedia.org/wiki/Robopsychology</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgment</head><p>The author would like to thank his students in "High Level Programming Languages" course in the spring semester of 2015/2016 at the University of Debrecen for testing the Samu projects. He would also like to thank the members of some AI-specific communities on Facebook, Google+ and Linkedin and especially his group called DevRob2Psy at <ref type="url" target="https://www.facebook.com/groups/devrob2psy/">https://www.facebook.com/groups/  devrob2psy/</ref> for their interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the Running Time of the Shortest Programs</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
		</author>
		<idno>CoRR, abs/0908.1159</idno>
		<ptr target="http://arxiv.org/abs/0908.1159" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Conscious Machines and Consciousness Oriented Programming</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
		</author>
		<idno>CoRR, abs/1108.2865</idno>
		<ptr target="http://arxiv.org/abs/1108.2865" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A disembodied developmental robotic agent called Samu Bátfai</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
		</author>
		<idno>CoRR, abs/1511.02889</idno>
		<ptr target="http://arxiv.org/abs/1511.02889" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Are there intelligent Turing machines?</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
		</author>
		<idno>CoRR, abs/1503.03787</idno>
		<ptr target="http://arxiv.org/abs/1503.03787" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How to Become a Robopsychologist</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
		</author>
		<ptr target="https://github.com/nbatfai/Robopsychology/files/169195/robopsychology.pdf" />
	</analytic>
	<monogr>
		<title level="j">GitHub Project</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Samu</surname></persName>
		</author>
		<author>
			<persName><surname>Turing</surname></persName>
		</author>
		<ptr target="https://github.com/nbatfai/SamuCTuring" />
	</analytic>
	<monogr>
		<title level="j">GitHub Project</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>visited: 2016-06-04</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
		</author>
		<ptr target="https://github.com/nbatfai/SamuTuring" />
		<title level="m">Samu Turing. GitHub Project</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>visited: 2016-06-04</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
		</author>
		<author>
			<persName><surname>Samubrain</surname></persName>
		</author>
		<ptr target="https://github.com/nbatfai/SamuBrain" />
		<title level="m">GitHub Project</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>visited: 2016-06-04</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
		</author>
		<ptr target="https://github.com/nbatfai/SamuKnows" />
		<title level="m">SamuKnows. GitHub Project</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>visited: 2016-06-04</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Robopsychology Manifesto: Samu in His Prenatal Development</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Bátfai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renátó</forename><surname>Besenczi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>submitted manuscript</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Developmental Robotics: From Babies to Robots</title>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Cangelosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Schlesinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Busy Beaver Machines and the Observant Otter Heuristic (or How to Tame Dreadful Dragons)</title>
		<author>
			<persName><forename type="first">James</forename><surname>Harland</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<idno>abs/1602.03228</idno>
		<ptr target="http://arxiv.org/abs/1602.03228" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Ivanyos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réka</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lajos</forename><surname>Rónyai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algoritmusok. Typotex</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An Introduction to Kolmogorov Complexity and Its Applications</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Vitányi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated, 3 edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attacking the busy beaver 5</title>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Marxen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Buntrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull EATCS</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="247" to="251" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On computable numbers with an application to the Entscheidungsproblem</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Turing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proceeding of the London Mathematical Society</title>
		<imprint>
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The general and logical theory of automata</title>
		<author>
			<persName><surname>John Von Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cerebral Mechanisms in Behaviour -The Hixon Symposium</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeffress</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="1" to="31" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
