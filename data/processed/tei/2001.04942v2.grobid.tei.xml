<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PRIVATE MACHINE LEARNING VIA RANDOMISED RE-SPONSE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-02-24">24 Feb 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Barber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PRIVATE MACHINE LEARNING VIA RANDOMISED RE-SPONSE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-02-24">24 Feb 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">52FC1BFFA436BC72ECA5088008AF313D</idno>
					<idno type="arXiv">arXiv:2001.04942v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a general learning framework for private machine learning based on randomised response. Our assumption is that all actors are potentially adversarial and as such we trust only to release a single noisy version of an individual's datapoint. Our approach forms a consistent way to estimate the true underlying machine learning model and we demonstrate this in the case of logistic regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">PRIVATE MACHINE LEARNING</head><p>Our desire is to develop a strategy for machine learning driven by the requirement that private data should be shared as little as possible and that no-one can be trusted with an individual's data, neither a data collector/aggregator, nor the machine learner that tries to fit a model. Randomised Response, see for example <ref type="bibr">Warner (1965)</ref>, is relevant in this context in which a datapoint x n is replaced with a randomised 'noisy' version xn . A classical example is voting in an election in which an individual voter votes for one of two candidates A or B and is asked to lie (with probability p) about whom they voted for . This results in noisy data and estimating the fraction f A of voters that voted for candidate A based on this noisy data</p><formula xml:id="formula_0">fA = 1 N N n=1 I (x n = A)<label>(1)</label></formula><p>can give a potentially significantly incorrect estimate. As <ref type="bibr">Warner (1965)</ref> showed, since we know the probabilistic mechanism that generated the noisy data, a better estimate of the fraction of voters voting for candidate A is given by</p><formula xml:id="formula_1">f A = fA + p 1 -2p<label>(2)</label></formula><p>In a machine learning context, the kind of scenario we envisage is that users may have labelled face images as "happy" or "sad" on their mobile phones and the company MugTome wishes to train a "happy/sad" face classifier; however, users do not wish to send the raw face images to MugTome and also wish to be able to plausibly deny which label they gave any training image. To preserve privacy, each user will send to MugTome only a single corrupted datapoint -a single corrupted image and a single corrupted label.</p><p>It is straightforward to extend our approach to deal with users sending multiple corrupted datapoints. However, since MugTome will potentially then know which corrupted datapoints belong to each user, they will have more information to help reveal the underlying clean datapoint. Since we assume we cannot trust MugTome, MugTome may attempt to recover the underlying true datapoint. For example, if a user sends three class labels c 1 , c 2 , c 3 , c i ∈ {0, 1}, then MugTome can have a good guess of the underlying true class label by simple taking the majority class c = I (c 1 + c 2 + c 3 &gt; 2). Indeed, in general, if M corrupted datapoints are independently generated for a user, then MugTome's ability to reveal the true class (or attribute) increases dramatically. </p><p>where p(c true ) is the prior belief on the true class. This posterior distribution concentrates exponentially quickly (in M ) around the true value c true . Similarly, if a pollster asks each voter three times what they voted, then the questioner would have a very good idea of the true vote of each voter; to protect the voter's privacy, the voter would then have to trust that the pollster either does not pass on any information that states that the three votes came from the same person or that the pollster doesn't attempt themselves to figure out what the voter voted for.</p><p>Similarly, in a medical setting in which a patient privately owns a datapoint, releasing M synthetic versions (corruptions) of that datapoint can compromise privacy if which synthetic datapoints belong to each person is also known. To guarantee that privacy is retained would require patients to trust people with their data, namely that any data aggregation process will remove their patient ID. However, this is something out of the control of the patient and as such we do not consider generating multiple synthetic datapoints (see for example <ref type="bibr" target="#b1">Bindschaedler et al. (2017)</ref>) a 'safe' mechanism.</p><p>For these reasons, we wish to make a process in which an individual only reveals a single corrupted datapoint; from that point onwards in the machine learning training process, no other trust in that process is required. To motivate our general approach to private machine learning we discuss the voting example in more detail in section(3). Connections to other forms of privacy preserving machine learning are discussed in section( <ref type="formula">7</ref>). The justification for our approach hinges on the properties of the Spread Divergence, which we review in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SPREAD DIVERGENCE</head><p>Throughout we use the notation p(X = x) for a random variable X in state x. However, to reduce notational overhead, where unambiguous, we write simply p(x).</p><p>A divergence D(p||q) (see, for example <ref type="bibr" target="#b2">Dragomir (2005)</ref>) is a measure of the difference between two distributions p and q with the property D(p||q) ≥ 0 and D(p||q) = 0 ⇔ p = q (4) An important class is the f -divergence, defined as</p><formula xml:id="formula_3">D f (p||q) = E q(x) f p(x) q(x)<label>(5)</label></formula><p>where f (x) is a convex function with f (1) = 0. A special case of an f -divergence is the well-known Kullback-Leibler divergence KL(p||q) = E p(x) log p(x) q(x) which is widely used to train models using maximum likelihood. For the Spread Divergence, from q(x) and p(x) we define new distributions q(x) and p(x) that have the same support. Using the notation x to denote integration (•) dx for continuous x, and x∈X for discrete x with domain X , we define a random variable x with the same domain as x and distributions</p><formula xml:id="formula_4">p(x) = x p(x|x)p(x), q(x) = x p(x|x)q(x)<label>(6)</label></formula><p>where p(x|x) 'spreads' the mass of p and q such that p(x) and q(x) have the same support. For example, if we use a Gaussian p(x|x) = N x x, σ 2 , then p and q both have support R. The spread divergence has a requirement on the noise p(x|x), namely that D(p||q) = 0 ⇔ p = q; that is, if the divergence of the spreaded distributions is zero, then the original non-spreaded distribution will match. As shown in <ref type="bibr">Zhang et al. (2018)</ref> this is guaranteed for certain 'spread noise' distributions.</p><p>In particular, for continuous x and x of the same dimension and injective function f , a sufficient condition for a valid spread noise p(x|x) = K(x -f (x)) is that the kernel K(x) has strictly positive Fourier Transform. For discrete variables, a sufficient condition is that p(x = i|x = j) = P ij is that P ij &gt; 0 and the matrix P is square and invertible.</p><p>Spread divergences have a natural connection to privacy preservation and Randomised Response <ref type="bibr">(Warner, 1965)</ref>. The spread divergence suggests a general strategy to perform private machine learning. We first express the machine learning problem as min θ D f (p(X)||p θ (X)) for a specified model p θ (X). Then, given only noisy data X, we fit the model by min θ D f p( X)||p θ ( X) . To explain in more detail how this works, we first describe randomised response in a classical voting context and then justify how to generalise this to principled training of machine learning models based on corrupted data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A CLASSICAL VOTING EXAMPLE</head><p>There are two candidates in an election, candidate "one" and candidate "zero" and Alice would like to know the fraction of voters that voted for candidate "one". We write the dataset of voting as a collection of binary values {x 1 , . . . , x N }, x n ∈ {0, 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LEARNING θ USING CLEAN DATA</head><p>If we assume that Alice has full knowledge of which candidate each voter voted for, then clearly Alice may simply count the fraction of people that voted for "one" and set</p><formula xml:id="formula_5">θ = 1 N N n=1</formula><p>x n (7)</p><p>It will be useful to first consider how to arrive at the same result from a modelling perspective. We can consider an independent Bernoulli model</p><formula xml:id="formula_6">p θ (X 1 = x 1 , . . . , X N = x N ) = N n=1 p θ (X n = x n )<label>(8)</label></formula><p>where</p><formula xml:id="formula_7">p θ (X = 1) = θ (9) so that p θ (X = x) = θ x (1 -θ) 1-x<label>(10)</label></formula><p>We also construct an empirical data distribution that places mass only on the observed joint state, namely</p><formula xml:id="formula_8">p(X 1 , . . . , X N ) = N n=1 δ(X n , x n ) (11)</formula><p>where δ(x, x ) is the Kronecker delta function. Then</p><formula xml:id="formula_9">1 N KL(p(X 1 , . . . , X N )||p θ (X 1 , . . . , X N )) = L N (θ) + const.<label>(12)</label></formula><p>where</p><formula xml:id="formula_10">L N (θ) = 1 N N n=1 log p θ (X n = x n ) (13) = 1 N N n=1 (x n log θ + (1 -x n ) log (1 -θ))<label>(14)</label></formula><p>and minimising KL(p||p θ ) (or maximising L N (θ)) with respect to θ recovers the fraction of votes that are 1, equation( <ref type="formula">7</ref>). This shows how we can frame estimating the quantity θ from uncorrupted private data as a divergence minimisation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LEARNING θ USING CORRUPTED DATA</head><p>Returning to the privacy setting, Bob would also like to know the fraction of votes that are 1. However, Alice does not want to send to Bob the raw data x 1 , . . . , x N since the votes of any individual should not be explicitly revealed.</p><p>To preserve privacy, Alice sends noisy data x1 , . . . , xN to Bob. In this case we draw a single joint sample x1 , . . . , xN from the distribution p( X1 , . . . , XN |X 1 , . . . , X N ) = N n=1 p( Xn |X n ) (15) where the 'spread noise' model is p( Xn = i|X n = j) = P ij . Hence, if x n = 0 Alice draws a sample xn = 0 with probability P 00 and xn = 1 with probability P 10 . Given a sampled noisy dataset x1 , . . . , xN we form an empirical spreaded data distribution p( X1 , . . . , XN ) = N n=1 δ Xn , xn</p><p>Similarly, the corrupted joint model is given by pθ ( X1 , . . . , XN ) =</p><formula xml:id="formula_12">N n=1 pθ ( Xn )<label>(17)</label></formula><p>where</p><formula xml:id="formula_13">pθ ( X = j) = j∈{0,1} p( X = j|X = j)p θ (X = j)<label>(18)</label></formula><p>On receiving the noisy dataset x1 , . . . , xN , Bob can try to estimate θ by minimising</p><formula xml:id="formula_14">1 N KL p( X1 , . . . , XN )||p θ ( X1 , . . . , XN ) = - 1 N N n=1 log pθ (x n ) + const. (<label>19</label></formula><formula xml:id="formula_15">)</formula><p>with respect to θ. Equivalently, he may maximise the scaled spread log likelihood</p><formula xml:id="formula_16">LN (θ) = 1 N N n=1 log pθ (x n )<label>(20)</label></formula><p>For this simple model, Bob can easily explicitly calculate</p><formula xml:id="formula_17">pθ (x = 1) = p(x = 1|x = 1)p θ (x = 1)+p(x = 1|x = 0)p θ (x = 0) = P 11 θ +P 10 (1 -θ) (21)</formula><p>Similarly, pθ (x = 0) = P 01 θ + P 00 (1 -θ). In this case, equation(20) becomes f0 log (P 00 (1 -θ) + P 01 θ) + f1 log (P 10 (1 -θ) + P 11 θ)</p><p>where</p><formula xml:id="formula_19">f1 = 1 N N n=1 xn<label>(23)</label></formula><p>Using f0 + f1 = 1, P 00 + P 10 = 1, P 01 + P 11 = 1, the maximum of the spread log likelihood is at</p><formula xml:id="formula_20">θ = f1 + P 10 1 -P 10 -P 01<label>(24)</label></formula><p>which forms Bob's estimate of the underlying fraction of voters that voted for candidate "one".</p><p>For example, if there were no noise P 10 = P 01 = 0, Bob would estimate θ = f1 , simply recovering the fraction of votes that are 1 in the original data. In the limit of a large number of votes N → ∞ and true probability θ 0 of a voter voting for candidate "one", then f0 tends to P 00 (1 -θ 0 ) + P 01 θ 0 and Bob's estimate recovers the true underlying voting probability θ = θ 0 . Hence, even though Bob only receives a corrupted set of votes, in the limit of a large number of votes, he can nevertheless estimate the true fraction of people that voted for candidate "one".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PRIVATE MACHINE LEARNING USING RANDOMISED RESPONSE</head><p>The above example suggests a general strategy to perform private machine learning:</p><p>1. Phrase problem as likelihood maximisation: We first assume that a machine learning task for private data x 1 , . . . , x N can be expressed as learning a data model p θ (X) by optimising an objective</p><formula xml:id="formula_21">L N (θ) = 1 N N n=1 log p θ (X n = x n ) (25)</formula><p>2. Form a corrupted dataset: Draw a single joint sample x1 , . . . , xN from the distribution</p><formula xml:id="formula_22">p( X1 , . . . , XN |X 1 , . . . , X N ) = N n=1 p( Xn |X n ) (26)</formula><p>where p( X|X) is a defined spread noise distribution and known by both the owner of the private data and the receiver of the corrupted data. To do this, we go through each element of the dataset x n and replace it with a corruption xn sampled from p( Xn = xn |X n = x n ).</p><p>3. Send data to learner: We then send to the learner the corrupted dataset x1 , . . . , xN , the model to be learned p θ (X) and the corruption probability p( X|X).</p><p>4. Estimate θ from corrupted data: Having received the corrupted data x1 , . . . , xN , the learner fits θ by maximising the objective</p><formula xml:id="formula_23">LN (θ) = 1 N N n=1 log pθ (x n )<label>(27)</label></formula><p>where</p><formula xml:id="formula_24">pθ (x) = x p(x|x)p θ (x)<label>(28)</label></formula><p>4.1 JUSTIFICATION If we assume that each element x n of the training data x 1 , . . . , x N is identically and independently sampled from a model p θ0 (X n = x n ), then each corrupted observation xn is a sample from the same distribution given by</p><formula xml:id="formula_25">pθ0 ( XN = y n ) = x p( Xn = y n |X = x)p θ0 (X = x)<label>(29)</label></formula><p>By the law of large numbers the objective equation( <ref type="formula" target="#formula_23">27</ref>) approaches its average over the data generating mechanism</p><formula xml:id="formula_26">lim N →∞ L N (θ) a.s. --→ x pθ0 ( X = x) log pθ ( X = x)<label>(30)</label></formula><p>and maximising the spread likelihood objective LN (θ) becomes equivalent to minimising</p><formula xml:id="formula_27">KL pθ0 ( X)||p θ ( X)<label>(31)</label></formula><p>Provided that the spread noise is valid (see section(2)), then</p><formula xml:id="formula_28">KL pθ0 ( X)||p θ ( X) = 0 ⇒ θ = θ 0 (32)</formula><p>for an identifiable model p θ . Thus</p><formula xml:id="formula_29">θ est = argmax θ LN (θ)<label>(33)</label></formula><p>is a consistent estimator.</p><p>This means that (in the large data limit and assuming the training data is generated from the model), even though we only train on corrupted data, we are optimising an objective LN (θ) which has a global minimum close to that of the objective on uncorrupted data L N (θ). Indeed, the estimator is consistent in the sense that as the amount of training data increases, we will recover the true clean data generating mechanism. Hence, provided that the corruption process is based on spread noise, then we can still learn the model parameters θ even by training on only corrupted data. In our motivating voting scenario in section(3), we saw explicitly that the estimate θ of the true underlying voting fraction is consistent and indeed, this is a general property of our approach. 3) for the model with binary variable p(x = 1) = θ, p(x = 0) = 1 -θ. In each case we plot along the x-axis the true θ 0 from 0 to 1 and on the y-axis the value of θ that maximises J ∞ (θ). In each plot we use a different flip probability. For a consistent estimator we would require that each plot is a straight x = y line, which only occurs in the case of no noise, p f = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TRAINING ON NOISE ONLY</head><p>A common approach in private machine learning is to form synthetic (noisy, corrupted) data and then simply train the standard model on this noisy data -see for example <ref type="bibr" target="#b4">Li et al. (2019)</ref>. In our notation, this would be equivalent to maximising the likelihood</p><formula xml:id="formula_30">L N (θ) ≡ 1 N N n=1 log p θ (X = xn )<label>(34)</label></formula><p>As above, assuming that the training data is generated from an underlying model p θ0 (X n = x n ), by the law of large numbers,</p><formula xml:id="formula_31">lim N →∞ L N (θ) a.s. --→ x pθ0 ( X = x) log p θ (X = x)<label>(35)</label></formula><p>In general, the optimum of this objective does not occur when θ = θ 0 and therefore training on noisy data alone does not form a consistent estimator of the true underlying model.</p><p>We discuss learning with noisy labels more extensively in the context of logistic regression in section(B) in which we show that provided the label flip noise is not too high p 0→1 + p 1→0 &lt; 1, and for zero mean isotropically Gaussian distributed inputs, maximum likelihood training with corrupted class labels does form a consistent estimator. Hence, whilst one cannot guarantee that maximum likelihood training of logistic regression on noisy data will result in a consistent estimator, there are special situations in which this may work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RECONSTRUCTION APPROACH</head><p>A seemingly natural alternative to our method is to attempt to reconstruct the clean datapoint from the noisy datapoint and use that within a standard learning framework. This approach would give an objective</p><formula xml:id="formula_32">J N (θ) = 1 N N n=1 xn p(x n |x n ) log p θ (x n )<label>(36)</label></formula><p>Here we need to define a posterior distribution p(x n |x n ) to reconstruct the clean datapoint. Since the learner only has knowledge of the prior p θ (x) it is natural to set</p><formula xml:id="formula_33">p(x n |x n ) = p θ (x n |x n ) ≡ p(x n |x n )p θ (x n ) xn p(x n |x n )p θ (x n ) (37)</formula><p>By the law of large numbers J N converges to its expectation with respect to the true data generating mechanism p θ0 (x) = p(x|x)p θ0 (x), so that</p><formula xml:id="formula_34">lim N →∞ J N (θ) a.s. --→ x,x p θ0 (x)p θ (x|x) log p θ (x) ≡ J ∞ (θ)<label>(38)</label></formula><p>In general, the optimum of J ∞ (θ) is not at θ = θ 0 . To demonstrate this, we plot in figure(1) the optimal θ for a simple Bernoulli model for which we can calculate J ∞ (θ) exactly. As we see, for all but zero flip noise, p f = 0, the estimator does not correctly identify the underlying probabilty generating mechanism. For this reason, we do not pursue this approach further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">OTHER DIVERGENCES</head><p>An extension of the above is to learn θ by minimising other f -divergences</p><formula xml:id="formula_35">D f (p θ (Y )||p(Y |y)) = E p(Y |y) f pθ (Y ) p(Y |y)<label>(39)</label></formula><p>However, this generalisation to any f -divergence is harder to justify since the expectation of this objective (by averaging over the noise realisations)</p><formula xml:id="formula_36">p(y)D f (p θ (Y )||p(Y |y))<label>(40)</label></formula><p>will not in general give a divergence between spreaded distributions. This means that in the limit of a large number of datapoints, it is not guaranteed to recover the true data generating process, except for special choices of the f -divergence, such as the KL divergence. We leave a discussion of this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PRIVATE LOGISTIC REGRESSION</head><p>As an application of the above framework to a standard machine learning model, we now discuss how to form a private version of logistic regression. Returning to our motivating example, users may have labelled face images as "happy" or "sad" on their mobile phones and the company MugTome wishes to train a "happy/sad" face classifier; however, users do not wish to send the raw face images to MugTome and also wish to be able to plausibly deny which label they gave any training image.</p><p>In this case we have a set of training data x 1 , . . . , x N , x n ∈ R D and corresponding binary class labels c 1 , . . . , c N , c n ∈ {0, 1}. We wish to fit a logistic regression model</p><formula xml:id="formula_37">p θ (c|x) = φ((2c -1)θ T c x)<label>(41</label></formula><p>) where φ(x) = 1/(1 + e -x ) is the logistic function. We follow the general approach outlined in section(4).</p><p>1. The model: For observation (x, c) and parameter θ p θ (c, x) = p θc (c|x)p θx (x) (42) where p θc (c|x) is the standard logistic regression model above and p θx (x) is a model of the input x. The training objective is</p><formula xml:id="formula_38">L N (θ) = 1 N N n=1 log p θ (c n , x n ) (43) = 1 N N n=1 log p θc (c n |x n ) + 1 N N n=1 log p θx (x n ) (44)</formula><p>We note that this is a separable objective for  </p><formula xml:id="formula_39">L N (θ) = L c N (θ c ) + L x N (θ x ),</formula><formula xml:id="formula_40">L(θ) = 1 N N n=1 log pθ (c n , xn ) (46) = 1 N N n=1 log xn,cn p(c n |c n )p(x n |x n )p θc (c n |x n )p θx (x n ) (47)</formula><p>Unfortunately, in all but special cases, the integral (for continuous x) or sum (for discrete x) required to evaluate L is not tractable and numerical approximation is required. For this stage, there are many options available and we present below the approach taken in the experiments.</p><p>Interestingly, we note that, unlike training on clean data, the objective L(θ) is not separable into a function of θ c plus a function of θ x , meaning that learning the class prediction parameter θ c is coupled with learning the input distribution parameter θ x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">IMPLEMENTATION</head><p>In general, the spread noise defines a distribution on a pair of spread variables p(c, x|c, x) and the full joint distribution, including the original model is</p><formula xml:id="formula_41">p(c, x, c, x) = p(c, x|c, x)p θc (c|x)p θx (x)<label>(48</label></formula><p>) For continuous x, the spread likelihood is then obtained from</p><formula xml:id="formula_42">p(c, x) = c x p(c, x, c, x)<label>(49)</label></formula><p>In general, this sum/integral over x is intractable due to the high-dimensionality of x. We use a standard approach to lower bound the log likelihood (for a single datapoint) by</p><formula xml:id="formula_43">log p(c, x) ≥ -E q(c,x|c,x) [log q(c, x|c, x)] + E q(c,x|c,x) [log p(c, x|c, x)p θc (c|x)p θx (x)] (<label>50</label></formula><formula xml:id="formula_44">)</formula><p>where q is a distribution chosen to make the bound tight, see for example <ref type="bibr" target="#b0">Barber (2012)</ref>. This allows us to use an EM-style procedure in which we iterate between the two steps : (M-step) fix q and optimise θ and (E-step) fix θ and update q.</p><p>1. Iteration k M-step: Update θ to increase the "energy"</p><formula xml:id="formula_45">θ k+1 = argmax θ E(θ; q k )<label>(51)</label></formula><p>where (for multiple datapoints)</p><formula xml:id="formula_46">E(θ; q) ≡ N n=1 E q(cn,xn|cn,xn) [log p θc (c n |x n )] + N n=1 E q(xn|cn,xn) [log p θx (x n )] (52)</formula><p>An advantage of this approach is that E(θ; q) is separable and we can update the class prediction parameter θ c independently of the input distribution parameter θ x .</p><p>In practice we will typically only do a partial optimisation (gradient ascent step) over θ to guarantee an increase in the energy. 2. Iteration k E-step: The bound is tightest when q is set to the posterior (see for example <ref type="bibr" target="#b0">Barber (2012)</ref>),</p><formula xml:id="formula_47">q k+1 (c, x|c, x) = p(c, x|c, x) = p(c|c)p(x|x)p(c|x)p(x) Z(c, x)<label>(53)</label></formula><p>where p(c|x) = p θ k c (c|x), p(x) = p θ k x (c|x) and the normaliser is given by</p><formula xml:id="formula_48">Z(c, x) ≡ c p(c|c)p(x|x)p(c|x)p(x)dx<label>(54)</label></formula><p>To implement the M-step, Equation( <ref type="formula">52</ref>) requires expectations of the form</p><formula xml:id="formula_49">c p(c, x|c, x)f (x, c)dx (55)</formula><p>for some function f (x, c). Assuming that the posterior will be reasonably peaked around the noisy data we use sampling with an importance distribution</p><formula xml:id="formula_50">ρ(c, x|c, x) = ρ(c|c)ρ(x|x)<label>(56)</label></formula><p>The expectation is then motivated by</p><formula xml:id="formula_51">c x p(c, x|c, x)f (x, c) = c x ρ(c|c)ρ(x|x) p(c|c)p(x|x)p(c|x)p(x) ρ(c|c)ρ(x|x)Z(c, x) f (x, c)<label>(57)</label></formula><p>Choosing</p><formula xml:id="formula_52">ρ(c|c) = p(c|c) Z ρ (c) , ρ(x|x) = p(x|x)p(x) Z ρ (x)<label>(58)</label></formula><p>for normalising functions Z ρ (c), Z ρ (x) we then run a standard importance sampling approximation (see section(A)). For a given noisy datapoint (c n , xn ) we generate a set of S samples c 1 n , . . . , c S n from ρ(c|c n ) and samples x 1 n , . . . , x S n from ρ(x|x n ) and compute the importance weights</p><formula xml:id="formula_53">w(s|n) = φ (2c s n -1)θ T c x s n s φ ((2c s n -1)θ T c x s n )<label>(59)</label></formula><p>The energy equation( <ref type="formula">52</ref>) separates into two independent terms (see section(A))</p><formula xml:id="formula_54">E(θ c ; q) ≈ N n=1 S s=1 w(s|n) log φ (2c s n -1)θ T c x s n<label>(60)</label></formula><p>and</p><formula xml:id="formula_55">E(θ x ; q) ≈ N n=1 S s=1 w(s|n) log p θx (x s n )<label>(61)</label></formula><p>Equation( <ref type="formula" target="#formula_54">60</ref>) is a weighted version of the standard logistic regression log likelihood, L c (θ c ) in equation( <ref type="formula">44</ref>); similarly equation( <ref type="formula" target="#formula_55">61</ref>) is a weighted version of L x (θ x ). The advantage therefore is that, given the importance samples, the learning procedure for θ requires only a minor modification of the standard maximum likelihood training procedure on clean data.</p><p>The full procedure is that we randomly initialise θ and then, for each datapoint n, draw samples and accumulate the gradient across samples and datapoints. After doing a gradient ascent step in θ, we update the importance distributions and repeat until convergence.</p><p>The Importance Sampling approximation is a convenient approach, motivated by the assumption that corrupted datapoints will be close to their uncorrupted counterparts. Whilst we used a bound as part of the approximation, this is not equivalent to using a parametric q distribution; by sampling we form a consistent estimator of the tightest possible lower bound. In other words, we are simply using Importance Sampling to estimate the expectations required within a standard Expectation Maximisation algorithm, see for example <ref type="bibr" target="#b0">Barber (2012)</ref>. We also tried learning a parametric q, similar to standard variational approaches to log likelihood maximisation, but didn't find any improvement on the Importance Sampling approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LEARNING THE PRIOR</head><p>If we have access to clean data, the optimal input model p θx (x) can be learned from maximising the likelihood L x (θ x ). However, our general assumption is that we will never have access to clean data. There may be situations in which the learner has a good model of p θx (x), without compromising privacy (for example a publicly available dataset for a similar prediction problem might be available) in which case it makes sense to set the prior to this known model.</p><p>In the absence of a suitable prior we can attempt to learn p θx (x) from the corrupted data by maximising L(θ). For simplicity we assume a factorised model and for a D-dimensional input vector x = (x[1], . . . , x[D]) write</p><formula xml:id="formula_56">p θx (x) = D d=1 p(x[d]|d) (62)</formula><p>for a collection of learnable univariate distributions p(x[d]|d), d = 1, . . . , D. Under this assumption, and using the Importance Sampling approach in equation( <ref type="formula" target="#formula_55">61</ref>), this means that p(x[d]|d) can be learned by maximising</p><formula xml:id="formula_57">E x = N n=1 S s=1 w(s|n) D d=1 log p(x n s [d]|d) (<label>63</label></formula><formula xml:id="formula_58">)</formula><p>Since this is a separable objective, we can learn each p(x n s [d]|d) independently. For simplicity, we assume a discrete distribution for x[d] that contains K states (or bins). Then</p><formula xml:id="formula_59">E x [d] = K k=1 N n=1 S s=1 w(s|n)I (x n s [d] ∈ k) log p(k|d)<label>(64)</label></formula><p>where</p><formula xml:id="formula_60">I (x n s [d] ∈ k) is 1 if the sample x n s [d]</formula><p>is in the k th state and 0 otherwise. Optimising with respect to p(k|d) we obtain</p><formula xml:id="formula_61">p(k|d) = N n=1 S s=1 w(s|n)I (x n s [d] ∈ k) K k=1 N n=1 S s=1 w(s|n)I (x n s [d] ∈ k)<label>(65)</label></formula><p>For the M-step of the algorithm we then make a gradient update for θ c and update the prior using equation( <ref type="formula" target="#formula_61">65</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We implemented our approach in section(5) to train a logistic regression classifier to distinguish between the MNIST digits 7 and 9 based on noisy data (250 train and 900 test examples from each class). We chose to train on a small dataset since this constitutes the most challenging scenario and helps highlight potential differences between rival approaches. The MNIST images x have pixesl with 256 states and we used a discrete distribution to model x.</p><p>For our experiments we assume a corruption model p(c|c) that flips the label 0 → 1 and 1 → 0 with probability p f with probability p f . We also assume here for simplicity assume a factorised input corruption model</p><formula xml:id="formula_62">p(x|x) = D d=1 p(x[d]|x[d]</formula><p>) in which with probability 1 -p f and uniformly from the other states of that pixel with probability p f . In this case, computing the Importance Sampling distribution is straightforward since the posterior is factorised over the image pixels. We considered three settings for the prior (required to compute the Importance Sampling distribution) : (i) flat prior, (ii) learned prior using EM, (iii) true factorised prior based on computing the marginal distribution of each pixel on the training dataset. In the 'true prior' case we assumed that we know the true marginal distribution of each pixel p(x[d]|d) -in general, this information would be private, but it is interesting to consider how much improvement is obtained by knowing this quantity.</p><p>We compare the following approaches:</p><p>Log Reg Clean Data We trained logistic regression on clean data. This sets an upper limit on the expected performance.</p><p>Log Reg on Noisy Data We trained a standard logistic regression model but using the corrupted data. This forms a simple baseline comparison.</p><p>Spread Log Reg with Learned Prior We used our Spread Likelihood approach to learn the prior.</p><p>(a) Spread Log with 'True Prior' In general our assumption is that the true prior will not be known (since this requires users to release their private data to the prior learner). However, this forms an interesting comparison and expected upper bound on the performance of the spread approach. Spread Log with Flat Prior In this case we used an informative, flat prior on all pixel states. We ran 10 experiments for each level of flip noise p f from 0.1, 0.2, 0.3, 0.4 and then tested the prediction accuracy of the learned logistic classifiers on clean hold out data, see figure(3).</p><formula xml:id="formula_63">(b) (c) (d) (e) (f) (g) (h) (i) (j)</formula><p>For all but small noise levels, the results show the superiority of the spread learning approach over simply training on noisy data, consistent with our theory that training the standard model on noisy data does not in general form a consistent estimator. The best performing spread approach is that which uses the true prior -however, in general this true prior will not be available. For this experiment, there appears to be little difference between using a flat prior and a learned prior.</p><p>The performance of standard logistic regression training but using corrupted data is surprisingly effective, at least at low noise levels. However the performance degrades quickly for higher noise levels. A partial explanation for why logistic regression may give good results simply trained on noisy data is given in section(B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">GAUSSIAN INPUT PRIOR p(x)</head><p>We also demonstrate here training logistic regression treating the pixels as continuous. If we an independent have (per pixel) a Gaussian prior</p><formula xml:id="formula_64">p(x i ) = N x i µ i , σ2 i (66)</formula><p>and independent Gaussian spread noise</p><formula xml:id="formula_65">p(x i |x i ) = N xi x i , σ 2 i (67)</formula><p>then using the Importance Sampling posterior is</p><formula xml:id="formula_66">ρ(x i |x i ) = N x i mean = b i a i , var = 1 a i<label>(68)</label></formula><p>where</p><formula xml:id="formula_67">a i = 1 σ 2 i + 1 σ2 i , b i = xi σ 2 i + µ i σ2 i<label>(69)</label></formula><p>We also used Gaussian spread noise to corrupt the images and train a binary classifier to distinguish between the MNIST digits 7 and 9 based on noisy data (4500 train and 900 test examples from each class). For simplicity, we assumed factorised distributions with prior p(</p><formula xml:id="formula_68">x i ) = N x i µ i , σ2 i , 0.</formula><p>1 0.2 0.3 0.4 0.70 0.75 0.80 0.85 0.90 0.95 log reg SD true prior SD learn prior SD flat prior log reg noise training approach with true prior; "SD learn prior": spread approach with learned prior; "SD flat prior": spread approach with flat prior; "log reg noise": standard logistic regression training but trained on noisy data.</p><formula xml:id="formula_69">p(x i |x i ) = N xi x i , σ 2 i .</formula><p>We chose spread flip noise p f = 0.2 for the class labels and uniform spread noise with variance σ 2 i = 0.1; the prior p(x) was set to be quite uninformative with mean µ i = 0 and variance σ2 i = 10. This level of noise means that approximately 20% of the class labels are incorrect in the data passed to MugTome and the associated image is significantly blurred, see figure(4). For standard logistic regression we found that for a learning rate of 0.2, 400 iterations gave the best performance, with 95.5% train accuracy and 95.7% test accuracy. Using our Importance Sampling scheme with S = 2 samples per noisy datapoint, the trained θ when tested on clean images had 94.4% test accuracy. This shows that despite the high level of class label and image noise, MugTome are able to learn an effective classifier, preserving the privacy of the users. The loss in test and training accuracy, despite this high noise level is around a modest 1%. When using higher spread noise with variance σ 2 = 0.5, the θ learned on the noisy data had a clean data test accuracy 93%, which is also a modest decrease in accuracy for a significant increase in privacy.</p><p>For future work it would be interesting to consider other forms of noise, for example downsampling images. However, downsampling does not form an injective mapping and as such we cannot guarantee that we can find a consistent estimator for the underlying model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>There are many forms of private machine learning. Some attempt to transform a datapoint x to a form x such that a protected attribute a (such as gender) cannot be recovered from x , yet the prediction of an output y (for example using p(y|x )) is retained. For example this could be achieved by using a loss function such as (see for example <ref type="bibr" target="#b4">Li et al. (2019)</ref>)</p><formula xml:id="formula_70">(b) (c) (d) (e)<label>(f)</label></formula><formula xml:id="formula_71">L(θ, φ, ψ) = n [L y (y n , y(x n ; θ)) -L a (a n , a(x n ; φ))] (<label>70</label></formula><formula xml:id="formula_72">)</formula><p>where n is the data index, y(x ; θ) is a function that takes input x and outputs a prediction y; a(x ; φ) is a function that takes input x and outputs an attribute prediction a and x = f (x; ψ) gives a representation of the input; L y , L a are loss functions. In this protected attribute setting, typically some form of the clean dataset is required to learn the parameters θ, φ, ψ.</p><p>Another common form of private machine learning is based on differential privacy <ref type="bibr" target="#b3">(Dwork &amp; Roth, 2014)</ref>, with the aim to make it difficult to discern whether a datapoint x n was used to train the predictor y(x; θ). That is, given a trained model, differential privacy attempts to restrict the ability to differentiate whether any individual's datum was used to train the model.</p><p>A closely related concept to randomised response is that of plausible deniability, namely privately corrupting a datapoint x n such that no-one (except the datapoint provider) can confidently state what the original (private) value of x n is. Recently <ref type="bibr" target="#b1">Bindschaedler et al. (2017)</ref> used this to create synthetic datapoints, which were subsequently used with a standard machine learning training approach. The authors showed that generating synthetic data x from a distribution p(x|x) that takes dependency amongst the elements of the vector x results in better machine learning predictors than sampling from a factorised distribution. In synthetic data generation approaches the assumption is that the statistical characteristics are similar to the real data. However, care is required since if the generating mechanism is powerful, it may generate data which is very similar to the private data.</p><p>In general these synthetic data generating approaches do not take into consideration when learning the parameters of the machine learning model what that synthetic data generation mechanism is. This is analogous to simply using the corrupted votes to directly estimate the fraction of voters that voted for a candidate, equation(1), rather than using knowledge of the data generation approach, equation(2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">SUMMARY</head><p>We discussed a general privacy preserving mechanism based on random response in a datapoint is replaced by a corrupted versions. We showed that, provided the corruption process is a valid spread noise, then a maximum likelihood approach forms a consistent estimator. That is, even though the model is only trained on corrupted, synthetic data, it is possible to recover the true underlying data genering mechnanism on the clean data. We applied this approach to a simple logistic regression model, showing that the approach can work well, even with high levels of noise. The approach is readily applicable to a large class of much more complex models and other divergences.</p><p>S. L. Warner. Randomised response: a survey technique for eliminating evasive answer bias. Journal of the American Statistical Association, 60(309):63-69, 1965. Mingtian Zhang, Peter Hayes, Tom Bird, Raza Habib, and David Barber. Spread divergences, 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PRIVACY PRESERVING LOGISTIC REGRESSION</head><p>The posterior is given by</p><formula xml:id="formula_73">p(c, x|c, x) = p(c|c)p(x|x)p(c|x)p(x) c x p(c|c)p(x|x)p(c|x)p(x) = p(c|c)p(x|x)p(c|x)p(x) Z(c, x)<label>(71)</label></formula><p>For the learning, we need to take expectations</p><formula xml:id="formula_74">c x p(c, x|c, x)f (x, c)s<label>(72)</label></formula><p>We use importance sampling to approximate this expectation</p><formula xml:id="formula_75">c x p(c, x|c, x)f (x, c) = c x ρ(c|c)ρ(x|x) p(c, x|c, x) ρ(c|c)ρ(x|x) f (x, c) (73) = c x ρ(c|c)ρ(x|x) p(c|c)p(x|x)p(c|x)p(x) ρ(c|c)ρ(x|x)Z(c, x) f (x, c)<label>(74)</label></formula><p>Choosing</p><formula xml:id="formula_76">ρ(c|c) = p(c|c) c p(c|c) = p(c|c) Z ρ (c) , ρ(x|x) = p(x|x)p(x) x p(x|x)p(x) = p(x|x)p(x) Z ρ (x)<label>(75)</label></formula><p>we have</p><formula xml:id="formula_77">c x p(c, x|c, x)f (x, c) = Z ρ (c)Z ρ (x) c x ρ(c|c)ρ(x|x) p(c|x) Z(c, x) f (x, c)<label>(76)</label></formula><p>Here</p><formula xml:id="formula_78">Z(c, x) = c x ρ(c|c)ρ(x|x) p(c|c)p(x|x)p(c|x)p(x) ρ(c|c)ρ(x|x) (77) = c x ρ(c|c)ρ(x|x) p(c|c)p(x|x)p(c|x)p(x) ρ(c|c)ρ(x|x) (78) = Z ρ (c)Z ρ (x) c x ρ(c|c)ρ(x|x)p(c|x)<label>(79)</label></formula><p>Putting this together and using the same samples to estimate the numerator and denominator expectations,</p><formula xml:id="formula_79">c x p(c, x|c, x)f (x, c) = c x ρ(c|c)ρ(x|x) p(c|x) c x ρ(c|c)ρ(x|x)p(c|x) f (x, c) (80) ≈ s p(c s |x s ) s p(c s |x s ) f (x s , c s ) (81) = s w(s)f (x s , c s )<label>(82)</label></formula><p>for importance weight</p><formula xml:id="formula_80">w(s) = p(c s |x s ) s p(c s |x s ) (83) E [z 1 z 2 ] -E [z 1 ]E [z 2 ] = θ T Σθ 0 (97) E z 2 1 -E [z 1 ] 2 = θ T 0 Σθ 0 (98) E z 2 2 -E [z 2 ] 2 = θ T Σθ (99)</formula><p>We can then write the large data limit log likelihood as a two dimensional expectation</p><formula xml:id="formula_81">L ∞ (θ) = E [(p 1→1 φ(z 1 ) + p 0→1 (1 -φ(z 1 ))) log φ(z 2 )] + E [1 -p 1→1 φ(z 1 ) -p 0→1 (1 -φ(z 1 ))) log (1 -φ(z 2 )))] (100)</formula><p>For simplicity, consider µ = 0, Σ = s 2 I, θ T 0 θ 0 = 1, θ T θ = 1, θ T 0 θ = cos(α). It is straightforward to show that in this case the gradient with respect to α is zero when α = 0, namely when θ = θ 0 . However, in general, for non-isotropic data covariance Σ, the gradient is non-zero at θ = θ 0 .</p><p>To derive the above result, we note that the covariance for z in this case is simply where the functions are defined as</p><formula xml:id="formula_82">C ≡ s 2 1 cos α cos α 1<label>(</label></formula><formula xml:id="formula_83">Z 1 ( 1 ) = s 1 , Z 2 ( 1 , 2 ) = s ( 1 cos α + 2 sin α)<label>(105)</label></formula><p>Differentiating L ∞ (α) with respect to α, we obtain</p><formula xml:id="formula_84">sE N ( 0,I) [(γ(Z 1 ( 1 )) -φ(Z 2 ( 1 , 2 ))) ( 2 cos α -1 sin α)]<label>(106)</label></formula><p>When α = 0 we note that Z 2 ( 1 , 2 ) is independent of 2 and that the above is therefore is zero. Hence L ∞ (α) has zero gradient at α = 0. It is straightforward to show that the second derivative of L ∞ (α) (evaluated at α = 0) is sE N ( 1 0,1) [-1 γ(s 1 )] -s 2 E N ( 1 0,1) [φ(s 1 )(1 -φ(s 1 ))]</p><p>(107)</p><p>The second term in equation( <ref type="formula">107</ref>) above is clearly negative. Using integration by parts (and noting that we may assume s &gt; 0), one may easily show that the first term is also negative provided that p 1→1 &gt; p 0→1 .</p><p>Hence we arrive at the (perhaps surprising) result that for zero mean isotropic Gaussian distributed input data, training on noisy data (c, x) in which the class labels have been flipped with some probability, results in a consistent estimator for θ 0 , provided the flip noise is not too high, namely p 1→1 &gt; p 0→1 , or equivalently, p 0→1 + p 1→0 &lt; 1. This result holds even in the case of asymmetric flip noise p 0→1 = p 1→0 .</p><p>More generally, even if the data p(x) is not Gaussian distributed, from the Central Limit Theorem, p(z) is likely to be close to Gaussian distributed for high dimensional inputs. Hence, for input data x that is roughly isotropically distributed, we can expect that training using maximum likelihood for any classifier of the form p(c = 1|x) = φ θ T x will likely be close to recovering the true θ 0 that generated the data (in the limit of a large number of datapoints).</p><p>The above analysis considered only noise on the class label. If, independently of the class label we add isotropic Gaussian noise to the observations, then the projection z will still be isotropic Gaussian distributed for Gaussian inputs p(x) and the above argument trivially extends to this case as well.</p><p>Hence, one can expect training (using standard logistic regression but with corrupted inputs and flipped labels) to be partially successful at recovering the true data generating process provided that the input data is close to isotropically distributed, motivating a whitening pre-processing step of the input data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>For example, if MugTome know the corruption mechanism p(c m |c true ) the posterior of the class is given by p(c true |c 1 , . . . , c M ) ∝ p(c true ) M m=1 p(c m |c true )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Training based on the reconstruction approach, section(4.3) for the model with binary variable p(x = 1) = θ, p(x = 0) = 1 -θ. In each case we plot along the x-axis the true θ 0 from 0 to 1 and on the y-axis the value of θ that maximises J ∞ (θ). In each plot we use a different flip probability. For a consistent estimator we would require that each plot is a straight x = y line, which only occurs in the case of no noise, p f = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>in which the logistic regression parameters θ c are conditionally independent (conditioned on the training data) of the input parameters θ x . 2. Form the corrupted dataset: We wish to send noisy data x1 , . . . , xN , c1 , . . . , cN to the learner. To do so we need to define a corruption model p(c, x|c, x). For simplicity, we consider a corruption model of the form p(c, x|c, x) = p(c|c)p(x|x) (45) The corruption processes of p(c|c) and p(x|x) are problem specific; see the experiments section(6) for some examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3.</head><figDesc>Send to learner corrupted data and model: The corrupted labels and inputs are sent to the learner (c 1 , x1 ), . . . , (c N , xN ) along with the model p θc (c|x), p θx (x) and corruption process p(c|c), p(x|x). 4. Learn the model parameters θ: The spread log likelihood is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) An example MNIST "7" alongside its noisy examples (b) p f = 0.1, (c) p f = 0.2, (d) p f = 0.3, (e) p f = 0.4) which is sent to Mugshot.com noise. Each pixel remains in state 1 -p f and is otherwise sampled uniformly from the available states of that pixel. The bottom row shows an example of a clean "9" (f) and corruptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The test accuracy (on clean data) of the trained logistic regression models, averaged over 10 different randomly chosen training datasets of 500 datapoints. The x-axis is the corruption probability p f . "log reg": Standard logistic regression training on clean data; "SD true prior": spread divergence training approach with true prior; "SD learn prior": spread approach with learned prior; "SD flat prior": spread approach with flat prior; "log reg noise": standard logistic regression training but trained on noisy data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) An example MNIST "7" alongside its noisy example which is sent to MugTome with Gaussian noise variance (b) σ 2 = 0.1 and (c) σ 2 = 0.5 ; (d,e,f) similarly for an MNIST "9".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>101)We now use the decomposition C = M M T , with Cholesky factor sample from z is equivalent to z ∼ M for ∼ N ( 0, I). Definingγ(x) = p 1→1 φ(x) + p 0→1 (1 -φ(x))(103)we can then write the expected log likelihood as a function of φ:L ∞ (α) = E N ( 0,I) [γ(Z 1 ( 1 )) log φ(Z 2 ( 1 , 2 )) + (1 -γ(Z 1 ( 1 ))) log (1 -φ(Z 2 ( 1 , 2 )))](104)</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>I would like to thank <rs type="person">Xijie Hou</rs> for useful discussions.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the logistic regression case, we have f (x, c) = log p(c|x) = log φ (2c -1)θ T x (84)</p><p>where φ(x) = 1/(1 + exp(-x)).</p><p>The variational lower bound then becomes</p><p>where, for a given noisy datapoint cn , xn we generate a set of S samples c 1 n , . . . , c S n from ρ(c|c n ) and samples x 1 n , . . . , x S n from ρ(x|x n )</p><p>B TRAINING ON NOISY DATA A common approach in private machine learning is to train the standard model based on noisy alone, corresponding to maximising L N (θ), equation( <ref type="formula">35</ref>). As we discussed, this does not in general give a consistent estimator of the true underlying model. To show this, we consider a logistic regression in which only the class labels c are corrupted with probabilities p 0→1 ≡ p(c = 1|c = 0) and in the same state with p 1→1 ≡ p(c = 1|c = 1), leaving the inputs x uncorrupted. In this case</p><p>If we assume that the true labels are drawn from an underlying model</p><p>then the probability of a corrupted label is given by</p><p>and by the law of large numbers, L N tends to</p><p>Taking the gradient wrt θ c , we obtain</p><p>A sufficient condition for the gradient to be zero is</p><p>That is</p><p>which is</p><p>In general, equation( <ref type="formula">94</ref>) does not have a solution at θ c = θ 0 .</p><p>To understand when the objective equation( <ref type="formula">90</ref>) has an optimum, we assume the data is drawn from p(x) = N (x µ, Σ), then, defining</p><p>Since x is Gaussian distributed, z is also Gaussian distributed with</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<title level="m">Bayesian Reasoning and Machine Learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2012">2012. ISBN 0521518148</date>
			<biblScope unit="page">9780521518147</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Plausible deniability for privacypreserving data synthesis</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Bindschaedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
		<idno>CoRR, abs/1708.07975</idno>
		<ptr target="http://arxiv.org/abs/1708.07975" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Some general divergence measures for probability distributions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Dragomir</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10474-005-0251-6</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Mathematica Hungarica</title>
		<idno type="ISSN">1588-2632</idno>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="331" to="345" />
			<date type="published" when="2005-11">Nov 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Algorithmic Foundations of Differential Privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1561/0400000042</idno>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deepobfuscator: Adversarial training framework for privacy-preserving image classification</title>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanrui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
