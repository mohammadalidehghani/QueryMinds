<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POWER CONSUMPTION VARIATION OVER ACTIVATION FUNCTIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-06-12">12 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
							<email>leod@itu.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">IT University of Copenhagen</orgName>
								<address>
									<addrLine>2300 Denmark</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">POWER CONSUMPTION VARIATION OVER ACTIVATION FUNCTIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-12">12 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">018125D60D6B7CFE1452E4FBE12E7F18</idno>
					<idno type="arXiv">arXiv:2006.07237v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The power machine learning models consume when making predictions can be affected by a model's architecture. This paper presents various estimates of power consumption for a range of different activation functions, a core factor in neural network model architecture design. Substantial differences in hardware performance exist between activation functions. This difference informs how power consumption in machine learning models can be reduced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The field of deep neural networks has reported strong progress in many problem areas, including natural language processing (NLP), image recognition, and game playing. Many of the advances in these areas have been the fruit of using larger and thus more computationally demanding neural networks. <ref type="bibr">Amodei &amp; Hernandez (2018)</ref> find that the cost of training doubled every few months between the releases of AlexNet <ref type="bibr" target="#b9">(Krizhevsky et al., 2012)</ref> and AlphaZero <ref type="bibr" target="#b15">Silver et al. (2018)</ref>. In NLP, power consumption has also risen: <ref type="bibr" target="#b16">Strubell et al. (2019)</ref> determine the carbon footprint of a contemporary machine translation architecure search to be in the order of hundreds of intercontinental flights, for models that offer only marginal performance improvement. This paper examines activation functions, a core part of neural networks. The activation function is the non-linearity at the core of each network node. It is applied over the input and bias parameters at a given node for each inference that a model makes. This makes for a potentially large number of computation being required to make predictions, predicated on network structure and size. When it comes to individual calculations, there is also broad variance. The complexity of low-level instructions for each these functions also varies widely, from the simple rectified linear unit to the transcendental hyperbolic tangent. This variance has the potential to lead to differences in power consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>The constraints on choice of a neural network activation function are (a) that it must be differentiable, and (b) that it must have a continuous domain. These are required in order to train networks through backpropagation. A large range of functions fit these constraints, and as such, currently popular machine learning frameworks implement a broad range of activation functions.</p><p>The machine code for executing these functions can vary in complexity significantly. Figure <ref type="figure" target="#fig_0">1</ref> compares toy x86 code for rectified linear unit activation with code for a hyperbolic tangent (tanh) function. The tanh code is not only more complex, but also requires use of special resources such as an FPU (floating point unit). In practice, these functions are often run in a SIMD/SIMT structure, where a single instruction is performed over many data points at a time. However, x86 and CUDA SIMD instruction sets have similar restrictions: there is no direct tanh function for either, and instead a sequence of secondary operations have to be performed. While inference requires many other operations beyond calculating activation functions, the difference in scale of these functions' computation still leaves room for optimisation.</p><p>1 relu: push eax 2 rol eax, 1 3 xor eax, eax 4 and eax, 1 5 pop ebx 6 imul eax, ebx 7 ret (a) ReLU in x86-like code, with EAX holding a 32-bit float on entry. No floating point stack required; the function is applied bitwise with no branching. Grey instructions take one micro-op. Timings from Fog (2019). 1 tanh: fst dword [tmp1] 2 call exp 3 fst dword [tmp2] 4 fld dword [tmp1] 5 fchs 6 call exp 7 fst dword [tmp1] 8 fld dword [tmp2] 9 fsubr 10 fld dword [tmp2] 11 fld dword [tmp1] 12 fadd 13 fdiv 14 ret 15 exp: fldl2e 16 fmulp st1,st0 17 fld1 18 fscale 19 fxch 20 fld1 21 fxch 22 fprem 23 f2xm1 24 faddp st1,st0 25 fmulp st1,st0 26 ret (b) tanh in x86-like code; floating-point operations here begin 'f', which need FPUs and have higher execution times. Red instructions take more than ten micro-ops. A further bottleneck is presented by hardware structure. The operations needed to compose some activation functions can require special hardware. This hardware can be scarce and therefore highly contended. For example, an NVIDIA V100 card's streaming multiprocessor (SM) has just one single special function unit (SFU) to sixteen 32-bit floating point cores, eight 64-bit floating point cores, and sixteen 32-bit integer cores <ref type="bibr" target="#b2">(Durant et al., 2017;</ref><ref type="bibr" target="#b11">Markidis et al., 2018)</ref>. When computing an activation function requires special hardware, such as an SFU, the rest of the hardware unit (e.g. a CUDA warp) may be left idle until computation completes. Due to this bottleneck, use of these activation functions could lead to both increased power consumption (through fixed overheads incurred in the background as threads wait) and also slow models.</p><p>Current research increasingly addresses the energy impact of machine learning <ref type="bibr" target="#b14">(Schwartz et al., 2019;</ref><ref type="bibr" target="#b3">Fan et al., 2020)</ref>. Large amounts of work has been done on reducing the training time of machine learning models <ref type="bibr" target="#b5">(Girosi et al., 1995;</ref><ref type="bibr" target="#b13">Prechelt, 1998)</ref>; on reducing precision to afford bandwidth <ref type="bibr" target="#b18">(Woodland, 1989;</ref><ref type="bibr" target="#b17">Wang et al., 2018)</ref>; and increasing efficiency through parameter reduction <ref type="bibr" target="#b7">(Kim &amp; Rush, 2016;</ref><ref type="bibr" target="#b0">Alvarez &amp; Park, 2019)</ref>. Toolkits for measuring emissions impact have started to appear with particular detailed results in some geographic regions, that work on a limited range of hardware <ref type="bibr" target="#b6">(Henderson et al., 2020)</ref>. However, the specific impact that activation function choice has on power consumption has not been previously investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL SETUP</head><p>The experiment goal is to gauge the power consumption impact of varying activation function type in a neural network. Although activation function choice can impact the length of the training phase, predicated on both architecture and training data, the resources needed to label one instance at inference time are predicated only on architecture. Thus, experiments do not need to consider training data in order to estimate impacts on inference-time power consumption.</p><p>0 1 2 3 4 5 6 7 8 number of instances, 10^n 10 5 10 4 10 3 seconds per instance Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz (a) Consumer CPU 0 1 2 3 4 5 6 7 8 number of instances, 10^n 10 5 10 4 10 3 seconds per instance Intel(R) Xeon(R) CPU E5-2660 v3 @ 2.60GHz CELU ELU GELU Hardshrink Hardtanh LeakyReLU LogSigmoid LogSoftmax PReLU RReLU ReLU ReLU6 SELU Sigmoid Softmax Softmin Softplus Softshrink Softsign Tanh Tanhshrink AlphaDropout Dropout Dropout2d Dropout3d Identity (b) Datacentre CPU 0 1 2 3 4 5 6 7 8 number of instances, 10^n 10 9 10 8 10 7 10 6 10 5 10 4 seconds per instance GeForce GTX 1080 Ti Activation functions Implementations of AlphaDropout, CELU, Dropout, Dropout2d, Dropout3d, ELU, GELU, Hardshrink, Hardtanh, Identity, LeakyReLU, LogSigmoid, LogSoftmax, PReLU, ReLU, ReLU6, RReLU, SELU, Sigmoid, Softmax, Softmin, Softplus, Softshrink, Softsign, Tanh, and Tanhshrink in PyTorch 1.5.0 <ref type="bibr" target="#b12">(Paszke et al., 2019)</ref> are evaluated.</p><p>Network architecture The test neural network had a 64-unit linear input layer, four hidden layers of 1024 units, and a sixteen unit linear output layer. The activation functions in the 4096 hidden nodes were varied as an experiment parameter. Each model was trained for each activation function for 2000 epochs using random data, randomly initialised weights, and Adam optimisation.</p><p>Evaluation The metric is power use, measured for different activation functions. Power consumption was proxied through wall clock time via Python's time.perf counter(). This metric has known deficiencies: while a process consumes time it may impose a range of power overheads. Nevertheless, we expect that the specific variation in real power consumption is low enough across these similar experimental workloads, and spurious loads will be cushioned through the use of multiple runs. Experiments were run three times and means taken. Experiments had a one day completion time limit. Workload Inference workloads are a pre-generated amount of random data. Scales are chosen to resemble the scales of real prediction workloads, especially for on-demand services. Inference set workload sizes range from 10 0 to 10 8 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Platform</head><p>Code is available at <ref type="url" target="https://github.com/leondz/inferencepower">https://github.com/leondz/inferencepower</ref> (including full graphs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND DISCUSSION</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the inference time taken per-instance for a range of activation functions and inference set sizes. There are differences in the time taken by activation functions, and therefore power consumed. Function performance indicates that dropout functions have a lower power consumption, and that the identity function the lowest. The net time per instance is higher for smaller inference sets, which can be explained by the impact of fixed costs.</p><p>Of the activation functions, tanh, logSigmoid, tanhshrink, and softmin are the slowest to run at scale. This indicates that, due to their increased estimated emissions impact, use of these functions should be considered carefully before models using them are deployed broadly or deployed in an application with a long lifetime.</p><p>As the size of the inference set increases, so does the proportion of runtime spent running these functions. While the "v"-shaped part of the curve suggests some caching/batching effects for the GPUs, the relative difference between activation functions is the phenomenon of interest, and that persists. The spread between the fastest and slowest activation functions is roughly a factor of two, and is present across workload sizes and platform. There is a slight suggestion that the functionbased efficiency spread may close gradually on GPUs with even larger inference sets.</p><p>Performance varies between functions. The scale of difference between fast and slow activation functions is shown in Figure <ref type="figure" target="#fig_3">3</ref>. Dropout functions perform roughly as well as each other. The performance difference between activation functions varies, but the spread persists. CPU activation workload spreads are fairly consistent regardless of the size of the inference set. GPU spread varies depending on inference set size, with some smaller instances workload sizes presenting high variance, and a generally decreasing spread as inference set sizes rise. This suggests that, for GPUs, activation function choice has more effect in situations where inference is performed over smaller sets at a time.</p><p>Inference workload size is important. There are spikes in GPU spread at certain inference set scales. For example, the difference between activation functions on consumer GPU hardware was a factor of 11 when doing inference on a set of 10 4 values, i.e. an order of magnitude. The datacentre GPU platform did inference on the workload of 10 5 values seven times slower on the slowest activation function than on the fastest. The magnitude of the scale of variation indicates: (a) that applications should be analysed and tuned on the target hardware if one is to avoid particularly costly activation functions; (b) applications with high-frequency workloads of smaller inference sets may be particularly prone to raised power consumption and emissions due to activation function choice.</p><p>Consumption spreads are consistently present. Figure <ref type="figure" target="#fig_3">3b</ref> shows the mean and standard deviation in activation function performance across inference set sizes and platforms, normalised relative to the identity function's performance. Higher spreads and variations indicate greater potential im- pact from activation function tuning. GPUs processing time over different activation functions varies more than for CPUs, depending on inference set size. The size of the spread in absolute GPU timings seen at 10 4 and 10 5 (Figure <ref type="figure" target="#fig_3">3a</ref> is echoed here. On the other hand, the consumer CPU platform experiences relatively little variation across activation function performances as inference set size increases. This suggests that tuning function choice in larger-scale machine learning environments, e.g. datacentres and GPU hardware, can lead to the greatest relative emission reductions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TRAINING EFFECTIVENESS</head><p>Low power activations are less useful if one needs to use them more often to do the same thing. This is especially important when training machine learning models. The number of iterations required is predicated upon not only network architecture, but also the training data, the hyperparameters, and the starting conditions. Further, depending on a model's usage scenario, the part of its total power consumption represented by training can be between everything (if one never predicts) and asymptotic to zero (if one does many predictions). Nevertheless, it is helpful to estimate the demands that different activations place during this phase.</p><p>To work out how many iterations are needed, a dummy workload and performance target can be set up. In this case, we used MNIST data <ref type="bibr" target="#b10">(LeCun et al., 1998)</ref>. Th evaluation network was similar to that in Section 3, but instead using the MNIST training data with an input dimension of 784, a final sigmoid output layer with ten nodes (one per digit), and optimised with stochastic gradient descent. The activation function of the middle four layers of 1024 nodes each is varied as the experiment parameter. The hardware is a server CPU, platform (b) from Section 3. Training was stopped after the epoch when validation accuracy exceeded 0.90, or after 100 epochs. Time was only accumulated during training and not evaluation. Note that networks with hidden layers composed of regularising functions (i.e. dropout) and the identity function are still able to learn the target function in this setup due to the presence of sigmoid output function.  Figure <ref type="figure" target="#fig_5">4</ref> shows the total time consumed to reach the target validation accuracy. Not all functions reached the required accuracy within the given number of epochs. If a function reached the maximum epoch count in any of its runs without achieving 90% on the validation set, its bar is marked in grey. Scaled Exponential Linear Units <ref type="bibr" target="#b8">(Klambauer et al., 2017)</ref> performed particularly well on this problem over multiple random initialisations. From a learning perspective, the networks have not performed particularly well: identity performed quickest, suggesting it was simpler to have a narrow sigmoid layer learn the problem than multiple broad hidden ones. Of the functions, linear units generally performed well; only one of these did not complete the problem in the required number of epochs. It is also possible that the chosen optimiser is not equally suitable for all activation functions. However, there is an indication that many of the functions that are efficient in earlier experiments evaluating inference-time performance can also perform well during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper estimated the power consumption of neural network activation functions. The range over activation functions was often a factor of two or more, with larger spreads for different platformdependent workloads. This result was consistent across device type (CPU and GPU), on both consumer and datacenter hardware, and for various scales of dataset. The scale of spread indicates that choice of neural network activation function affects machine learning model power consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 TIMINGS FOR GPUS These are the mean absolute prediction times for various activation functions in seconds, on CUDA GPUs.   Table 4: For Intel Xeon E5-2660, 10 n instances</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: x86 style versions of ReLU vs. tanh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Per-instance function run time against inference data volume. Activation functions in blue-green colours, dropout in autumn colours, identity in grey.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>Test platforms were: (a) CPU on a commodity laptop (2017 MacBook Pro); (b) CPU on a server (Xeon E5-2660, 40 hyperthreaded cores; 256GB RAM) with consumer GPU (NVIDIA GeForce GTX 1080 Ti); (c) datacentre GPU (NVIDIA Tesla P100 16GB). Platforms (b) and (c) both ran CUDA 10.1. SIMT CPU extensions such as AVX and AVX512 were left enabled or disabled according to pip distribution defaults.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Spread between high-and low-running time functions, over dataset scale and platforms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>reach 90% accuracy on MNIST data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Time for functions to train on MNIST. Functions whose models do not reach the the required accuracy within 100 epochs have gray bars. Error bars are time standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>For GTX-1080i, 10 n instances</figDesc><table><row><cell>Function</cell><cell>n=0</cell><cell>n=1</cell><cell>n=2</cell><cell>n=3</cell><cell>n=4</cell><cell>n=5</cell><cell>n=6</cell><cell>n=7</cell><cell>n=8</cell></row><row><cell>CELU</cell><cell>2.522e-04</cell><cell>2.389e-04</cell><cell>2.697e-04</cell><cell>2.696e-04</cell><cell>2.411e-04</cell><cell>8.489e-04</cell><cell>2.221e-01</cell><cell>2.981e+00</cell><cell>3.349e+01</cell></row><row><cell>ELU</cell><cell>2.626e-04</cell><cell>2.643e-04</cell><cell>2.794e-04</cell><cell>2.758e-04</cell><cell>2.460e-04</cell><cell>2.495e-04</cell><cell>2.213e-01</cell><cell>3.708e+00</cell><cell>3.400e+01</cell></row><row><cell>GELU</cell><cell>2.486e-04</cell><cell>2.346e-04</cell><cell>2.564e-04</cell><cell>2.617e-04</cell><cell>2.322e-04</cell><cell>2.391e-04</cell><cell>3.061e-01</cell><cell>3.110e+00</cell><cell>3.360e+01</cell></row><row><cell>Hardshrink</cell><cell>2.525e-04</cell><cell>2.418e-04</cell><cell>2.640e-04</cell><cell>2.739e-04</cell><cell>2.504e-04</cell><cell>2.846e-04</cell><cell>2.162e-01</cell><cell>3.490e+00</cell><cell>3.449e+01</cell></row><row><cell>Hardtanh</cell><cell>2.464e-04</cell><cell>2.401e-04</cell><cell>2.889e-04</cell><cell>2.690e-04</cell><cell>2.701e-04</cell><cell>2.954e-04</cell><cell>3.430e-01</cell><cell>3.522e+00</cell><cell>3.346e+01</cell></row><row><cell>LeakyReLU</cell><cell>2.570e-04</cell><cell>2.363e-04</cell><cell>2.633e-04</cell><cell>3.440e-04</cell><cell>2.557e-04</cell><cell>2.636e-04</cell><cell>1.999e-01</cell><cell>3.291e+00</cell><cell>3.352e+01</cell></row><row><cell>LogSigmoid</cell><cell>3.479e-04</cell><cell>2.349e-04</cell><cell>2.803e-04</cell><cell>2.611e-04</cell><cell>2.519e-04</cell><cell>1.192e-03</cell><cell>3.085e-01</cell><cell>3.303e+00</cell><cell>3.455e+01</cell></row><row><cell>LogSoftmax</cell><cell>2.970e-04</cell><cell>2.879e-04</cell><cell>3.233e-04</cell><cell>3.091e-04</cell><cell>3.112e-04</cell><cell>3.135e-04</cell><cell>2.302e-01</cell><cell>3.401e+00</cell><cell>3.369e+01</cell></row><row><cell>PReLU</cell><cell>2.814e-04</cell><cell>3.402e-04</cell><cell>3.737e-04</cell><cell>3.069e-04</cell><cell>2.715e-04</cell><cell>2.951e-04</cell><cell>2.341e-01</cell><cell>3.217e+00</cell><cell>3.420e+01</cell></row><row><cell>RReLU</cell><cell>2.920e-04</cell><cell>2.853e-04</cell><cell>3.370e-04</cell><cell>3.126e-04</cell><cell>2.532e-03</cell><cell>3.035e-04</cell><cell>3.447e-01</cell><cell>3.350e+00</cell><cell>3.354e+01</cell></row><row><cell>ReLU</cell><cell>2.656e-04</cell><cell>2.424e-04</cell><cell>2.884e-04</cell><cell>2.692e-04</cell><cell>5.717e-04</cell><cell>2.654e-04</cell><cell>2.855e-01</cell><cell>3.585e+00</cell><cell>3.333e+01</cell></row><row><cell>ReLU6</cell><cell>2.590e-04</cell><cell>3.109e-04</cell><cell>2.555e-04</cell><cell>2.986e-04</cell><cell>2.356e-04</cell><cell>2.506e-04</cell><cell>3.785e-01</cell><cell>3.422e+00</cell><cell>3.409e+01</cell></row><row><cell>SELU</cell><cell>2.460e-04</cell><cell>2.335e-04</cell><cell>2.554e-04</cell><cell>2.617e-04</cell><cell>2.360e-04</cell><cell>2.543e-04</cell><cell>2.298e-01</cell><cell>3.304e+00</cell><cell>3.357e+01</cell></row><row><cell>Sigmoid</cell><cell>2.478e-04</cell><cell>2.457e-04</cell><cell>2.628e-04</cell><cell>2.725e-04</cell><cell>2.576e-04</cell><cell>2.758e-04</cell><cell>3.208e-01</cell><cell>3.307e+00</cell><cell>3.215e+01</cell></row><row><cell>Softmax</cell><cell>3.186e-04</cell><cell>2.774e-04</cell><cell>4.445e-04</cell><cell>3.273e-04</cell><cell>3.189e-04</cell><cell>2.874e-04</cell><cell>3.098e-01</cell><cell>3.742e+00</cell><cell>3.409e+01</cell></row><row><cell>Softmin</cell><cell>3.745e-04</cell><cell>3.840e-04</cell><cell>4.121e-04</cell><cell>3.955e-04</cell><cell>3.710e-04</cell><cell>9.869e-04</cell><cell>5.005e-01</cell><cell>3.764e+00</cell><cell>4.382e+01</cell></row><row><cell>Softplus</cell><cell>2.444e-04</cell><cell>2.448e-04</cell><cell>2.671e-04</cell><cell>2.652e-04</cell><cell>2.412e-04</cell><cell>4.425e-04</cell><cell>2.144e-01</cell><cell>3.332e+00</cell><cell>3.343e+01</cell></row><row><cell>Softshrink</cell><cell>2.396e-04</cell><cell>2.373e-04</cell><cell>2.554e-04</cell><cell>2.510e-04</cell><cell>2.736e-04</cell><cell>2.700e-04</cell><cell>3.378e-01</cell><cell>3.232e+00</cell><cell>3.488e+01</cell></row><row><cell>Softsign</cell><cell>4.635e-04</cell><cell>4.544e-04</cell><cell>4.967e-04</cell><cell>5.473e-04</cell><cell>5.158e-04</cell><cell>1.079e-03</cell><cell>4.194e-01</cell><cell>6.273e+00</cell><cell>5.721e+01</cell></row><row><cell>Tanh</cell><cell>2.360e-04</cell><cell>2.298e-04</cell><cell>2.766e-04</cell><cell>2.810e-04</cell><cell>2.385e-04</cell><cell>2.621e-04</cell><cell>3.345e-01</cell><cell>3.458e+00</cell><cell>3.495e+01</cell></row><row><cell>Tanhshrink</cell><cell>3.942e-04</cell><cell>3.417e-04</cell><cell>3.621e-04</cell><cell>5.387e-04</cell><cell>3.222e-04</cell><cell>5.335e-04</cell><cell>5.413e-01</cell><cell>5.447e+00</cell><cell>4.826e+01</cell></row><row><cell>AlphaDropout</cell><cell>1.613e-04</cell><cell>1.677e-04</cell><cell>1.754e-04</cell><cell>1.774e-04</cell><cell>2.967e-04</cell><cell>4.021e-04</cell><cell>1.002e-01</cell><cell>1.574e+00</cell><cell>2.337e+01</cell></row><row><cell>Dropout</cell><cell>1.791e-04</cell><cell>1.584e-04</cell><cell>1.988e-04</cell><cell>1.833e-04</cell><cell>1.641e-04</cell><cell>1.693e-04</cell><cell>1.192e-01</cell><cell>1.815e+00</cell><cell>2.353e+01</cell></row><row><cell>Dropout2d</cell><cell>1.713e-04</cell><cell>1.585e-04</cell><cell>1.695e-04</cell><cell>1.728e-04</cell><cell>1.672e-04</cell><cell>1.706e-04</cell><cell>1.079e-01</cell><cell>1.675e+00</cell><cell>2.500e+01</cell></row><row><cell>Dropout3d</cell><cell>1.535e-04</cell><cell>1.529e-04</cell><cell>1.567e-04</cell><cell>1.940e-04</cell><cell>1.531e-04</cell><cell>1.582e-04</cell><cell>1.093e-01</cell><cell>1.831e+00</cell><cell>2.283e+01</cell></row><row><cell>Identity</cell><cell>1.432e-04</cell><cell>1.346e-04</cell><cell>1.398e-04</cell><cell>1.538e-04</cell><cell>1.325e-04</cell><cell>1.371e-04</cell><cell>1.634e-01</cell><cell>1.791e+00</cell><cell>2.271e+01</cell></row><row><cell>Function</cell><cell>n=0</cell><cell>n=1</cell><cell>n=2</cell><cell>n=3</cell><cell>n=4</cell><cell>n=5</cell><cell>n=6</cell><cell>n=7</cell><cell>n=8</cell></row><row><cell>CELU</cell><cell>1.543e-04</cell><cell>1.835e-04</cell><cell>1.909e-04</cell><cell>2.285e-04</cell><cell>1.789e-04</cell><cell>1.758e-04</cell><cell>2.672e-01</cell><cell>3.320e+00</cell><cell>3.397e+01</cell></row><row><cell>ELU</cell><cell>1.815e-04</cell><cell>1.756e-04</cell><cell>1.637e-04</cell><cell>2.072e-04</cell><cell>1.733e-04</cell><cell>1.797e-04</cell><cell>2.155e-01</cell><cell>3.171e+00</cell><cell>3.289e+01</cell></row><row><cell>GELU</cell><cell>1.538e-04</cell><cell>2.726e-04</cell><cell>1.931e-04</cell><cell>2.135e-04</cell><cell>1.700e-04</cell><cell>1.666e-04</cell><cell>3.252e-01</cell><cell>3.635e+00</cell><cell>3.303e+01</cell></row><row><cell>Hardshrink</cell><cell>1.842e-04</cell><cell>1.683e-04</cell><cell>1.848e-04</cell><cell>2.626e-04</cell><cell>1.724e-04</cell><cell>1.008e-03</cell><cell>3.703e-01</cell><cell>3.278e+00</cell><cell>3.311e+01</cell></row><row><cell>Hardtanh</cell><cell>1.595e-04</cell><cell>4.051e-04</cell><cell>2.345e-04</cell><cell>2.061e-04</cell><cell>2.063e-04</cell><cell>2.024e-04</cell><cell>3.164e-01</cell><cell>3.479e+00</cell><cell>3.363e+01</cell></row><row><cell>LeakyReLU</cell><cell>1.539e-04</cell><cell>1.505e-04</cell><cell>2.635e-04</cell><cell>2.060e-04</cell><cell>2.474e-04</cell><cell>1.916e-04</cell><cell>2.553e-01</cell><cell>3.624e+00</cell><cell>3.378e+01</cell></row><row><cell>LogSigmoid</cell><cell>1.635e-04</cell><cell>1.654e-04</cell><cell>2.347e-04</cell><cell>3.102e-04</cell><cell>5.318e-04</cell><cell>1.731e-04</cell><cell>3.101e-01</cell><cell>3.234e+00</cell><cell>3.693e+01</cell></row><row><cell>LogSoftmax</cell><cell>2.482e-04</cell><cell>3.350e-04</cell><cell>2.560e-04</cell><cell>3.167e-04</cell><cell>2.648e-04</cell><cell>3.161e-04</cell><cell>3.525e-01</cell><cell>3.134e+00</cell><cell>3.424e+01</cell></row><row><cell>PReLU</cell><cell>4.236e-04</cell><cell>1.844e-04</cell><cell>1.808e-04</cell><cell>3.335e-04</cell><cell>2.307e-04</cell><cell>2.096e-04</cell><cell>2.138e-01</cell><cell>3.060e+00</cell><cell>3.284e+01</cell></row><row><cell>RReLU</cell><cell>2.388e-04</cell><cell>2.179e-04</cell><cell>2.544e-04</cell><cell>2.689e-04</cell><cell>1.894e-04</cell><cell>2.273e-04</cell><cell>3.394e-01</cell><cell>3.127e+00</cell><cell>3.404e+01</cell></row><row><cell>ReLU</cell><cell>1.643e-04</cell><cell>1.906e-04</cell><cell>1.971e-04</cell><cell>2.359e-04</cell><cell>1.841e-04</cell><cell>1.953e-04</cell><cell>3.507e-01</cell><cell>3.804e+00</cell><cell>3.280e+01</cell></row><row><cell>ReLU6</cell><cell>1.858e-04</cell><cell>1.632e-04</cell><cell>1.709e-04</cell><cell>2.246e-04</cell><cell>5.023e-04</cell><cell>5.198e-04</cell><cell>3.258e-01</cell><cell>3.304e+00</cell><cell>3.410e+01</cell></row><row><cell>SELU</cell><cell>1.775e-04</cell><cell>1.608e-04</cell><cell>2.979e-04</cell><cell>2.239e-04</cell><cell>1.946e-04</cell><cell>1.946e-04</cell><cell>2.156e-01</cell><cell>3.562e+00</cell><cell>3.309e+01</cell></row><row><cell>Sigmoid</cell><cell>1.618e-04</cell><cell>1.677e-04</cell><cell>2.544e-04</cell><cell>2.363e-04</cell><cell>1.690e-04</cell><cell>1.746e-04</cell><cell>3.489e-01</cell><cell>3.851e+00</cell><cell>3.487e+01</cell></row><row><cell>Softmax</cell><cell>1.917e-04</cell><cell>2.327e-04</cell><cell>2.146e-04</cell><cell>2.772e-04</cell><cell>2.345e-04</cell><cell>2.459e-04</cell><cell>1.993e-01</cell><cell>3.438e+00</cell><cell>3.517e+01</cell></row><row><cell>Softmin</cell><cell>2.812e-04</cell><cell>3.000e-04</cell><cell>7.762e-04</cell><cell>3.937e-04</cell><cell>5.543e-04</cell><cell>7.737e-04</cell><cell>4.968e-01</cell><cell>4.876e+00</cell><cell>4.034e+01</cell></row><row><cell>Softplus</cell><cell>1.779e-04</cell><cell>1.845e-04</cell><cell>1.612e-04</cell><cell>2.331e-04</cell><cell>1.634e-04</cell><cell>1.702e-04</cell><cell>3.565e-01</cell><cell>3.095e+00</cell><cell>3.366e+01</cell></row><row><cell>Softshrink</cell><cell>1.719e-04</cell><cell>1.843e-04</cell><cell>2.281e-04</cell><cell>3.273e-04</cell><cell>1.721e-04</cell><cell>1.537e-04</cell><cell>2.228e-01</cell><cell>3.688e+00</cell><cell>3.308e+01</cell></row><row><cell>Softsign</cell><cell>3.077e-04</cell><cell>3.466e-04</cell><cell>5.812e-04</cell><cell>4.464e-04</cell><cell>6.471e-04</cell><cell>1.061e-03</cell><cell>4.625e-01</cell><cell>6.055e+00</cell><cell>5.002e+01</cell></row><row><cell>Tanh</cell><cell>1.671e-04</cell><cell>2.035e-04</cell><cell>1.817e-04</cell><cell>2.137e-04</cell><cell>1.838e-04</cell><cell>1.041e-03</cell><cell>2.259e-01</cell><cell>3.637e+00</cell><cell>3.433e+01</cell></row><row><cell>Tanhshrink</cell><cell>2.387e-04</cell><cell>3.910e-04</cell><cell>2.380e-04</cell><cell>2.706e-04</cell><cell>2.918e-04</cell><cell>3.118e-04</cell><cell>3.549e-01</cell><cell>4.865e+00</cell><cell>4.383e+01</cell></row><row><cell>AlphaDropout</cell><cell>1.179e-04</cell><cell>1.262e-04</cell><cell>1.541e-04</cell><cell>1.613e-04</cell><cell>1.184e-04</cell><cell>1.276e-04</cell><cell>1.579e-01</cell><cell>1.553e+00</cell><cell>2.434e+01</cell></row><row><cell>Dropout</cell><cell>1.062e-04</cell><cell>1.445e-04</cell><cell>1.803e-04</cell><cell>1.607e-04</cell><cell>1.267e-04</cell><cell>1.424e-04</cell><cell>1.147e-01</cell><cell>1.035e+00</cell><cell>2.419e+01</cell></row><row><cell>Dropout2d</cell><cell>1.251e-04</cell><cell>1.247e-04</cell><cell>1.090e-04</cell><cell>2.141e-04</cell><cell>1.218e-04</cell><cell>1.338e-04</cell><cell>1.341e-01</cell><cell>1.119e+00</cell><cell>2.441e+01</cell></row><row><cell>Dropout3d</cell><cell>1.319e-04</cell><cell>1.565e-04</cell><cell>3.613e-04</cell><cell>1.842e-04</cell><cell>1.296e-04</cell><cell>4.115e-04</cell><cell>1.113e-01</cell><cell>1.382e+00</cell><cell>2.522e+01</cell></row><row><cell>Identity</cell><cell>1.380e-04</cell><cell>8.792e-05</cell><cell>9.750e-05</cell><cell>1.179e-04</cell><cell>1.074e-04</cell><cell>9.531e-05</cell><cell>1.671e-01</cell><cell>1.710e+00</cell><cell>2.386e+01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>For Tesla P100, 10 n instances A.2 TIMINGS FOR CPUS These are the mean absolute prediction times for various activation functions in seconds, on CPUs.</figDesc><table><row><cell>Function</cell><cell>n=0</cell><cell>n=1</cell><cell>n=2</cell><cell>n=3</cell><cell>n=4</cell><cell>n=5</cell><cell>n=6</cell><cell>n=7</cell><cell>n=8</cell></row><row><cell>CELU</cell><cell>1.405e-03</cell><cell>4.911e-03</cell><cell>8.225e-03</cell><cell>2.975e-02</cell><cell>3.330e-01</cell><cell>2.553e+00</cell><cell>2.626e+01</cell><cell>2.969e+02</cell><cell>0.000e+00</cell></row><row><cell>ELU</cell><cell>6.384e-03</cell><cell>6.949e-03</cell><cell>1.421e-02</cell><cell>3.112e-02</cell><cell>3.414e-01</cell><cell>2.527e+00</cell><cell>2.664e+01</cell><cell>3.030e+02</cell><cell>0.000e+00</cell></row><row><cell>GELU</cell><cell>2.163e-03</cell><cell>8.301e-03</cell><cell>6.664e-03</cell><cell>5.240e-02</cell><cell>2.892e-01</cell><cell>2.206e+00</cell><cell>2.309e+01</cell><cell>2.645e+02</cell><cell>0.000e+00</cell></row><row><cell>Hardshrink</cell><cell>6.289e-03</cell><cell>3.041e-03</cell><cell>9.742e-03</cell><cell>2.602e-02</cell><cell>2.575e-01</cell><cell>2.383e+00</cell><cell>2.158e+01</cell><cell>2.537e+02</cell><cell>0.000e+00</cell></row><row><cell>Hardtanh</cell><cell>3.353e-03</cell><cell>2.221e-03</cell><cell>1.117e-02</cell><cell>4.544e-02</cell><cell>2.571e-01</cell><cell>2.318e+00</cell><cell>2.202e+01</cell><cell>2.683e+02</cell><cell>0.000e+00</cell></row><row><cell>LeakyReLU</cell><cell>2.511e-03</cell><cell>7.106e-03</cell><cell>7.090e-03</cell><cell>4.036e-02</cell><cell>2.896e-01</cell><cell>2.116e+00</cell><cell>2.146e+01</cell><cell>2.593e+02</cell><cell>0.000e+00</cell></row><row><cell>LogSigmoid</cell><cell>1.243e-03</cell><cell>3.914e-03</cell><cell>1.446e-02</cell><cell>4.962e-02</cell><cell>5.080e-01</cell><cell>4.431e+00</cell><cell>4.517e+01</cell><cell>4.945e+02</cell><cell>0.000e+00</cell></row><row><cell>LogSoftmax</cell><cell>2.719e-03</cell><cell>2.893e-03</cell><cell>1.333e-02</cell><cell>3.773e-02</cell><cell>3.329e-01</cell><cell>2.436e+00</cell><cell>2.557e+01</cell><cell>2.985e+02</cell><cell>0.000e+00</cell></row><row><cell></cell><cell>2.453e-03</cell><cell>4.674e-03</cell><cell>5.918e-03</cell><cell>2.744e-02</cell><cell>3.002e-01</cell><cell>2.072e+00</cell><cell>2.201e+01</cell><cell>2.721e+02</cell><cell>0.000e+00</cell></row><row><cell>RReLU</cell><cell>7.116e-03</cell><cell>2.999e-03</cell><cell>1.075e-02</cell><cell>2.264e-02</cell><cell>2.514e-01</cell><cell>2.135e+00</cell><cell>2.223e+01</cell><cell>2.685e+02</cell><cell>0.000e+00</cell></row><row><cell>ReLU</cell><cell>1.113e-03</cell><cell>3.330e-03</cell><cell>7.654e-03</cell><cell>2.626e-02</cell><cell>2.948e-01</cell><cell>2.087e+00</cell><cell>2.165e+01</cell><cell>2.564e+02</cell><cell>0.000e+00</cell></row><row><cell>ReLU6</cell><cell>6.967e-03</cell><cell>3.698e-03</cell><cell>6.990e-03</cell><cell>2.638e-02</cell><cell>2.631e-01</cell><cell>2.117e+00</cell><cell>2.148e+01</cell><cell>2.451e+02</cell><cell>0.000e+00</cell></row><row><cell>SELU</cell><cell>6.106e-03</cell><cell>2.751e-03</cell><cell>1.178e-02</cell><cell>3.055e-02</cell><cell>3.165e-01</cell><cell>2.414e+00</cell><cell>2.629e+01</cell><cell>2.985e+02</cell><cell>0.000e+00</cell></row><row><cell>Sigmoid</cell><cell>3.347e-03</cell><cell>1.035e-02</cell><cell>7.452e-03</cell><cell>2.728e-02</cell><cell>3.080e-01</cell><cell>2.338e+00</cell><cell>2.456e+01</cell><cell>2.757e+02</cell><cell>0.000e+00</cell></row><row><cell>Softmax</cell><cell>1.927e-03</cell><cell>2.174e-03</cell><cell>1.547e-02</cell><cell>3.365e-02</cell><cell>3.083e-01</cell><cell>2.462e+00</cell><cell>2.610e+01</cell><cell>2.850e+02</cell><cell>0.000e+00</cell></row><row><cell>Softmin</cell><cell>3.691e-03</cell><cell>3.531e-03</cell><cell>1.200e-02</cell><cell>5.189e-02</cell><cell>4.026e-01</cell><cell>3.259e+00</cell><cell>3.337e+01</cell><cell>3.755e+02</cell><cell>0.000e+00</cell></row><row><cell>Softplus</cell><cell>4.992e-03</cell><cell>9.673e-03</cell><cell>1.355e-02</cell><cell>8.653e-02</cell><cell>4.558e-01</cell><cell>3.379e+00</cell><cell>3.382e+01</cell><cell>3.799e+02</cell><cell>0.000e+00</cell></row><row><cell>Softshrink</cell><cell>1.781e-03</cell><cell>3.413e-03</cell><cell>7.560e-03</cell><cell>2.436e-02</cell><cell>2.457e-01</cell><cell>2.052e+00</cell><cell>2.192e+01</cell><cell>2.582e+02</cell><cell>0.000e+00</cell></row><row><cell>Softsign</cell><cell>3.399e-03</cell><cell>7.535e-03</cell><cell>1.170e-02</cell><cell>2.544e-02</cell><cell>5.449e-01</cell><cell>3.742e+00</cell><cell>3.815e+01</cell><cell>4.451e+02</cell><cell>0.000e+00</cell></row><row><cell>Tanh</cell><cell>2.939e-03</cell><cell>6.905e-03</cell><cell>1.137e-02</cell><cell>3.721e-02</cell><cell>4.553e-01</cell><cell>3.319e+00</cell><cell>3.366e+01</cell><cell>3.707e+02</cell><cell>0.000e+00</cell></row><row><cell>Tanhshrink</cell><cell>4.985e-03</cell><cell>4.995e-03</cell><cell>1.345e-02</cell><cell>7.797e-02</cell><cell>4.744e-01</cell><cell>4.112e+00</cell><cell>4.218e+01</cell><cell>4.508e+02</cell><cell>0.000e+00</cell></row><row><cell>AlphaDropout</cell><cell>4.242e-03</cell><cell>1.146e-02</cell><cell>5.761e-03</cell><cell>2.243e-02</cell><cell>1.929e-01</cell><cell>1.329e+00</cell><cell>1.424e+01</cell><cell>1.672e+02</cell><cell>0.000e+00</cell></row><row><cell>Dropout</cell><cell>3.823e-03</cell><cell>2.265e-03</cell><cell>1.571e-02</cell><cell>2.163e-02</cell><cell>2.436e-01</cell><cell>1.503e+00</cell><cell>1.445e+01</cell><cell>1.667e+02</cell><cell>0.000e+00</cell></row><row><cell>Dropout2d</cell><cell>3.012e-03</cell><cell>3.630e-03</cell><cell>4.178e-03</cell><cell>1.913e-02</cell><cell>2.140e-01</cell><cell>1.439e+00</cell><cell>1.440e+01</cell><cell>1.665e+02</cell><cell>0.000e+00</cell></row><row><cell>Dropout3d</cell><cell>5.751e-03</cell><cell>4.988e-03</cell><cell>1.025e-02</cell><cell>2.829e-02</cell><cell>2.323e-01</cell><cell>1.331e+00</cell><cell>1.458e+01</cell><cell>1.768e+02</cell><cell>0.000e+00</cell></row><row><cell>Identity</cell><cell>1.838e-03</cell><cell>2.890e-03</cell><cell>1.530e-02</cell><cell>2.755e-02</cell><cell>2.290e-01</cell><cell>1.413e+00</cell><cell>1.409e+01</cell><cell>1.818e+02</cell><cell>0.000e+00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>For MacBook Pro 2017 / i5-7360U, 10 n instances (n = 8 did not complete in time)</figDesc><table><row><cell>Function</cell><cell>n=0</cell><cell>n=1</cell><cell>n=2</cell><cell>n=3</cell><cell>n=4</cell><cell>n=5</cell><cell>n=6</cell><cell>n=7</cell><cell>n=8</cell></row><row><cell>CELU</cell><cell>8.040e-04</cell><cell>5.504e-04</cell><cell>9.299e-04</cell><cell>4.652e-03</cell><cell>6.080e-02</cell><cell>5.491e-01</cell><cell>5.843e+00</cell><cell>5.390e+01</cell><cell>5.333e+02</cell></row><row><cell>ELU</cell><cell>5.142e-04</cell><cell>5.420e-04</cell><cell>9.370e-04</cell><cell>4.503e-03</cell><cell>6.701e-02</cell><cell>5.052e-01</cell><cell>5.739e+00</cell><cell>5.195e+01</cell><cell>5.303e+02</cell></row><row><cell>GELU</cell><cell>4.563e-04</cell><cell>2.212e-03</cell><cell>1.090e-03</cell><cell>9.235e-03</cell><cell>6.776e-02</cell><cell>5.033e-01</cell><cell>5.956e+00</cell><cell>5.824e+01</cell><cell>5.713e+02</cell></row><row><cell>Hardshrink</cell><cell>4.909e-04</cell><cell>4.580e-04</cell><cell>9.830e-04</cell><cell>4.254e-03</cell><cell>6.083e-02</cell><cell>5.435e-01</cell><cell>5.480e+00</cell><cell>5.065e+01</cell><cell>5.231e+02</cell></row><row><cell>Hardtanh</cell><cell>4.804e-04</cell><cell>4.295e-04</cell><cell>7.947e-04</cell><cell>4.607e-03</cell><cell>6.192e-02</cell><cell>5.038e-01</cell><cell>6.082e+00</cell><cell>5.125e+01</cell><cell>5.213e+02</cell></row><row><cell>LeakyReLU</cell><cell>4.435e-04</cell><cell>4.814e-04</cell><cell>1.130e-03</cell><cell>4.219e-03</cell><cell>5.889e-02</cell><cell>4.327e-01</cell><cell>5.147e+00</cell><cell>5.265e+01</cell><cell>5.146e+02</cell></row><row><cell>LogSigmoid</cell><cell>5.823e-04</cell><cell>6.052e-04</cell><cell>1.124e-03</cell><cell>6.629e-03</cell><cell>1.026e-01</cell><cell>8.386e-01</cell><cell>9.849e+00</cell><cell>9.301e+01</cell><cell>9.348e+02</cell></row><row><cell>LogSoftmax</cell><cell>4.839e-04</cell><cell>5.797e-04</cell><cell>1.719e-03</cell><cell>9.518e-03</cell><cell>5.974e-02</cell><cell>4.708e-01</cell><cell>5.720e+00</cell><cell>5.131e+01</cell><cell>5.260e+02</cell></row><row><cell>PReLU</cell><cell>5.169e-04</cell><cell>4.822e-04</cell><cell>8.476e-04</cell><cell>5.408e-03</cell><cell>6.506e-02</cell><cell>5.080e-01</cell><cell>5.241e+00</cell><cell>5.224e+01</cell><cell>5.137e+02</cell></row><row><cell>RReLU</cell><cell>9.946e-04</cell><cell>6.080e-04</cell><cell>9.066e-04</cell><cell>5.379e-03</cell><cell>6.046e-02</cell><cell>4.700e-01</cell><cell>5.331e+00</cell><cell>5.063e+01</cell><cell>5.138e+02</cell></row><row><cell>ReLU</cell><cell>5.820e-04</cell><cell>4.295e-04</cell><cell>9.473e-04</cell><cell>4.295e-03</cell><cell>5.799e-02</cell><cell>4.943e-01</cell><cell>5.329e+00</cell><cell>5.182e+01</cell><cell>5.199e+02</cell></row><row><cell>ReLU6</cell><cell>5.297e-04</cell><cell>5.645e-04</cell><cell>7.616e-04</cell><cell>4.971e-03</cell><cell>5.682e-02</cell><cell>4.900e-01</cell><cell>5.288e+00</cell><cell>5.230e+01</cell><cell>5.143e+02</cell></row><row><cell>SELU</cell><cell>5.307e-04</cell><cell>5.419e-04</cell><cell>9.343e-04</cell><cell>4.726e-03</cell><cell>6.620e-02</cell><cell>5.824e-01</cell><cell>5.878e+00</cell><cell>5.397e+01</cell><cell>5.294e+02</cell></row><row><cell>Sigmoid</cell><cell>5.802e-04</cell><cell>5.382e-04</cell><cell>8.845e-04</cell><cell>4.689e-03</cell><cell>6.233e-02</cell><cell>5.550e-01</cell><cell>5.756e+00</cell><cell>5.209e+01</cell><cell>5.282e+02</cell></row><row><cell>Softmax</cell><cell>8.080e-04</cell><cell>6.075e-04</cell><cell>1.183e-03</cell><cell>4.950e-03</cell><cell>5.313e-02</cell><cell>5.086e-01</cell><cell>5.270e+00</cell><cell>5.189e+01</cell><cell>5.189e+02</cell></row><row><cell>Softmin</cell><cell>9.904e-04</cell><cell>6.343e-04</cell><cell>1.002e-03</cell><cell>5.897e-03</cell><cell>8.061e-02</cell><cell>7.023e-01</cell><cell>8.580e+00</cell><cell>8.063e+01</cell><cell>7.957e+02</cell></row><row><cell>Softplus</cell><cell>4.135e-04</cell><cell>6.887e-04</cell><cell>1.290e-03</cell><cell>5.570e-03</cell><cell>6.268e-02</cell><cell>5.921e-01</cell><cell>6.075e+00</cell><cell>5.461e+01</cell><cell>5.993e+02</cell></row><row><cell>Softshrink</cell><cell>5.241e-04</cell><cell>4.692e-04</cell><cell>7.834e-04</cell><cell>4.706e-03</cell><cell>6.308e-02</cell><cell>5.442e-01</cell><cell>5.429e+00</cell><cell>5.219e+01</cell><cell>5.183e+02</cell></row><row><cell>Softsign</cell><cell>7.190e-04</cell><cell>6.517e-04</cell><cell>1.181e-03</cell><cell>6.631e-03</cell><cell>1.107e-01</cell><cell>9.482e-01</cell><cell>1.129e+01</cell><cell>1.087e+02</cell><cell>1.089e+03</cell></row><row><cell>Tanh</cell><cell>5.437e-04</cell><cell>1.747e-03</cell><cell>1.270e-03</cell><cell>5.356e-03</cell><cell>6.823e-02</cell><cell>5.610e-01</cell><cell>5.432e+00</cell><cell>5.245e+01</cell><cell>5.342e+02</cell></row><row><cell>Tanhshrink</cell><cell>4.899e-04</cell><cell>2.443e-03</cell><cell>1.129e-03</cell><cell>5.651e-03</cell><cell>1.030e-01</cell><cell>7.718e-01</cell><cell>8.912e+00</cell><cell>8.803e+01</cell><cell>8.879e+02</cell></row><row><cell>AlphaDropout</cell><cell>2.436e-04</cell><cell>4.216e-04</cell><cell>6.650e-04</cell><cell>3.773e-03</cell><cell>4.560e-02</cell><cell>3.195e-01</cell><cell>3.119e+00</cell><cell>2.281e+01</cell><cell>2.353e+02</cell></row><row><cell>Dropout</cell><cell>4.084e-04</cell><cell>4.113e-04</cell><cell>8.083e-04</cell><cell>4.251e-03</cell><cell>4.448e-02</cell><cell>3.329e-01</cell><cell>2.524e+00</cell><cell>2.351e+01</cell><cell>2.342e+02</cell></row><row><cell>Dropout2d</cell><cell>7.918e-04</cell><cell>4.341e-04</cell><cell>8.644e-04</cell><cell>3.793e-03</cell><cell>3.861e-02</cell><cell>4.170e-01</cell><cell>2.883e+00</cell><cell>2.364e+01</cell><cell>2.349e+02</cell></row><row><cell>Dropout3d</cell><cell>2.893e-04</cell><cell>4.250e-04</cell><cell>6.257e-04</cell><cell>3.737e-03</cell><cell>4.131e-02</cell><cell>3.767e-01</cell><cell>2.803e+00</cell><cell>2.363e+01</cell><cell>2.309e+02</cell></row><row><cell>Identity</cell><cell>7.799e-04</cell><cell>3.222e-04</cell><cell>6.066e-04</cell><cell>3.761e-03</cell><cell>3.314e-02</cell><cell>3.727e-01</cell><cell>2.917e+00</cell><cell>2.386e+01</cell><cell>2.387e+02</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>Thanks to <rs type="person">Peter Sestoft</rs> for suggestions regarding hardware constraints.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end streaming keyword spotting</title>
		<author>
			<persName><forename type="first">Raziel</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun-Jin</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6336" to="6340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Compute</forename><surname>Ai</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/ai-and-compute/,2018.OpenAI" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inside Volta: The worlds most advanced data center GPU</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Durant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Giroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Stam</surname></persName>
		</author>
		<ptr target="https://devblogs.nvidia.com/parallelforall/inside-volta" />
	</analytic>
	<monogr>
		<title level="m">NVidia Parallel for All Blog</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Proceedings of the first workshop on simple and efficient natural language processing</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nafise</forename><surname>Sadat Moosav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops of the Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Agner</forename><surname>Fog</surname></persName>
		</author>
		<ptr target="https://www.agner.org/optimize/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Danmarks Tekniske Universitet</orgName>
		</respStmt>
	</monogr>
	<note>Instruction tables</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regularization theory and neural networks architectures</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="269" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards the systematic reporting of the energy and carbon footprints of machine learning</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Romoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno>arXiv, abs/2002.05651</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">NVIDIA tensor core programmability, performance &amp; precision</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Markidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Wei Der Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Laure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Parallel and Distributed Processing Symposium Workshops (IPDPS)</title>
		<meeting>the International Parallel and Distributed Processing Symposium Workshops (IPDPS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic early stopping using cross validation: quantifying the criteria</title>
		<author>
			<persName><forename type="first">Lutz</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><forename type="middle">Etzioni</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename></persName>
		</author>
		<idno>arXiv, abs/1907.10597</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and go through self-play</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training deep neural networks with 8-bit floating point numbers</title>
		<author>
			<persName><forename type="first">Naigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7675" to="7684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weight limiting, weight quantisation and generalisation in multi-layer perceptrons</title>
		<author>
			<persName><surname>Pc Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEE International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="297" to="300" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
