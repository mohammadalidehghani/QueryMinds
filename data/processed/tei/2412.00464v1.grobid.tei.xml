<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Conditions for Domain Stability for Machine Learning: a Mathematical Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-30">30 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Gabriel</forename><surname>Pedroza</surname></persName>
							<email>gabriel.pedroza@ansys.com</email>
						</author>
						<title level="a" type="main">On the Conditions for Domain Stability for Machine Learning: a Mathematical Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-30">30 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">4155C1AAF579D2E24035D632DF4788DC</idno>
					<idno type="arXiv">arXiv:2412.00464v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Classifiers</term>
					<term>Stability</term>
					<term>Operational Design Domain</term>
					<term>Metric/Topological Spaces</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes a mathematical approach that (re)defines a property of Machine Learning models named stability and determines sufficient conditions to validate it. Machine Learning models are represented as functions, and the characteristics in scope depend upon the domain of the function, what allows us to adopt topological and metric spaces theory as a basis. Finally, this work provides some equivalences useful to prove and test stability in Machine Learning models. The results suggest that whenever stability is aligned with the notion of function smoothness, then the stability of Machine Learning models primarily depends upon certain topological, measurable properties of the classification sets within the ML model domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the study of Artificial Intelligence and related techniques, like Machine Learning, has attracted considerable attention from practitioners, engineers and researchers from many sectors. Such interest has been, in many respects, driven by questions and concerns raised and shared by the involved communities. In particular, it can be mentioned the need for safety of systems integrating AI algorithms and the possibility to achieve acceptable means for compliance, necessary for regulation and certification <ref type="bibr" target="#b2">[3]</ref>[2] <ref type="bibr" target="#b0">[1]</ref>. Despite autonomy is a well-known notion within the Systems, SW and HW engineering arenas, the usage of AI technology, (GenAI, LLMs, ML/DL) to carry out functions traditionally conducted under human supervision and control, induces new challenges <ref type="bibr" target="#b3">[4]</ref>. Discussing referred challenges is out of the scope of this paper: notwithstanding the relevance of AI challenges and risks, they are currently under inspection and study by a variety of experts, following a plethora of approaches, ranging from conceptual to empirical/pragmatical <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b9">[10]</ref>. From the variety and heterogeneity of those studies, a consensus seems to emerge: despite the significant advances in AI technology, a considerable amount of requirements and properties still need to be ensured for the AI systems to be trustworthy and aligned with requirements. It is known that the lack of properties like robustness, generalization, and stability in ML models can have impacts at different system levels <ref type="bibr" target="#b10">[11]</ref>. Several authors even highlight a lack of methods to validate and ensure referred properties <ref type="bibr" target="#b6">[7]</ref>. In addition, the observed uncertainty of ML performance raise questions on the foundations of the AI algorithms. A basis for their sound design and validation seems necessary but still missing. This work aims to provide a preliminary mathematical basis to analyze ML models abstracted as functions. It mainly introduces a property named stability, which appears to be fundamental for ML classification. This preliminary work provides some equivalences which can be useful to prove stability. Topological and metric spaces <ref type="bibr" target="#b5">[6]</ref>[9] were adopted as foundation to conduct this work which also relies on the theory of functional analysis <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Defining Stability</head><p>Definition 1. Let (S, d) a metric space, D i ⊂ S, i = 1 . . . m, a finite sequence of sets. M is a classifier on S for the sets</p><formula xml:id="formula_0">{D i } if: i) M is a function defined on m i=1 D i ii) D i ∩ D j = ∅, ∀i = j iii) ∃y i ∈ M (D i ), ∀x ∈ D i ⇒ M (x) = y i , i = 1 . . . m iv)</formula><p>For y i , y j as in previous point, if i = j ⇒ y i = y j Note 1. Given a metric space (S, d) the following notation is used for the neighborhood of a point x o ∈ S:</p><formula xml:id="formula_1">B(x o , δ) = {x ∈ S | d(x o , x) &lt; δ} Note 2. In Definition 1, a point x ∈ D i , M (x) = y i is denoted by x y i .</formula><p>The assignation M (x y i ) = y i can also be denoted by</p><formula xml:id="formula_2">x y i M --→ y i . Definition 2. Let (S, d) a metric space, D ⊂ S and M a classifier for D and D c . x y ∈ D is said a stable point of M in D if it is satisfied: i) M (x y ) = y ii) ∃δ &gt; 0, ∀x ∈ B(x y , δ) ⇒ x ∈ D, M (x) = y iii) For δ as in point ii), ∀δ α ≤ δ ∃x ∈ B(x y , δ α ), x = x y</formula><p>Practically, the existence of stable points for a classifier M implemented in a programming language, relies upon the limited/finite precision of computers. Indeed, the precision of a machine can be approximated, for instance, by an iterative algorithm taking ε o &gt; 0 as input and computing ε n+1 = ε n /2 n at each iteration n. The stopping criterion is when 1 + ε n = 1. Then, a candidate for δ is any number k-times bigger than ε n . i.e. δ := kε n . The Definition 2 helps to identify cases where classifiers are unable to smoothly classify subsets within its domain, as can be seen in the following example.</p><p>Example 1. Let M be a classifier in the interval S = [0, 1], with the 1euclidean metrics for the sets D 1 = I ∩ S and D 2 = Q ∩ S. Given that D 1 and D 2 are dense, then the set of stable points of M in D i is empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dense sets and Implications for Stability</head><p>The implications of dense sets regarding the existence (absence) of stable points is formalized in this subsection. <ref type="bibr" target="#b5">[6]</ref>. Let's assume there is x j ∈ S such that x j ∈ D j , j = k, is a stable point for M . Then M (x j ) = j and ∃δ &gt; 0 such that B(x j , δ) ⊂ D j . However, since D k is dense in S, then B(x j , δ) ∩ D k = ∅. Then for any point x ∈ B(x j , δ) ∩ D k , it occurs that M (x) = j and M (x) = k, and x ∈ D j ∩ D k which contradicts Definition 1 i), ii), iv) and Definition 2, ii). QED.</p><formula xml:id="formula_3">Definition 3. Let (S, d) a metric space. A set D ⊂ S is said dense in S if ∀x ∈ S, ∀δ &gt; 0, B(x, δ) ∩ D = ∅ [9],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Alternatives to Prove Stability</head><p>Some equivalences are provided as alternatives to prove stability. [⇒] Let's assume x y ∈ D is a stable point of M . It will be proved that x y is an accumulation point of D. Let's consider ε &gt; 0 arbitrary but fixed. Then since x y ∈ D is a stable point of M , M (x y ) = y and ∃δ &gt; 0 such that B(x y , δ) ⊂ D and ∀δ α ≤ δ, ∃x ∈ B(x y , δ α ), x = x y . The following cases exist: </p><formula xml:id="formula_4">If ε &lt; δ: then B(x y , ε) ⊂ B(x y , δ) ⊂ D, since ∃x ∈ B(x y , ε), x = x y , it follows that x ∈ D, which leads directly to B(x y , ε) ∩ D\{x y } = ∅. If ε &gt; δ: since x y ∈ D is stable point of M , then ∃x ∈ B(x y , δ) ⊂ B(x y , ε), x = x</formula><formula xml:id="formula_5">If ε &lt; δ: then B(x y , ε) ⊂ D and B(x y , ε) ∩ D c = ∅ what is a contra- diction If ε &gt; δ: then B(x y , δ) ⊂ B(x y , ε) ⊂ D, what conflicts with the as- sumption ∀δ α &lt; δ, B(x y , δ α ) ∩ D c = ∅ Therefore ∃δ α &lt; δ such that B(x y , δ α )∩D c = ∅ what implies B(x y , δ α ) ⊂ D, thus ∀x ∈ B(x y , δ α ) ⇒ M (x) = y. QED.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Accumulation series</head><p>As shown in previous subsection, a classifier M defined over an open set is stable for every accumulation point therein. The equivalence provided in the following lemma provides further means to prove stability. </p><formula xml:id="formula_6">if ∀{x n } series, such that lim n→∞ x n = x y , x n = x y , ∃{s k } sub-series of {x n } such that ∃k o , k ≥ k o , ⇒ s k ∈ D, s k = x y , lim k→∞ s k = x y . [⇒] Let's assume x y is a stable point of M . Let {x n } be a series such that lim n→∞ x n = x y , x n = x y , then ∃δ &gt; 0 such that B(x y , δ) ⊂ D. For such δ, ∃n o such that ∀n &gt; n o , d(x y , x n ) &lt; δ, what means that ∀n &gt; n o , x n ∈ B(x y , δ) ⊂ D, then M (x n ) = y, x n = x y . Then, we can define the sub-series of {x n } as {s k } = {x n } ∩ B(x y , δ) ∩ D\{x y } = ∅. Thus, by its construction, {s k } satisfies ∃k o , k ≥ k o , ⇒ s k ∈ D, s k = x y , lim k→∞ s k = x y .</formula><p>[⇐] Let δ &gt; 0. Then, if lim n→∞ x n = x y , x n = x y , and ∃{s k } a sub-series of</p><formula xml:id="formula_7">{x n } such that ∃k o , k ≥ k o , ⇒ s k ∈ D, s k = x y , lim k→∞ s k = x y , let ε = d(s ko , x y ).</formula><p>The following cases exist:</p><formula xml:id="formula_8">If ε &lt; δ: ∀k &gt; k o , ⇒ s k ∈ B(x y , ε) ⊂ B(x y , δ) and s k ∈ D, s k = x y , therefore ∀k &gt; k o , s k ∈ D ∩ B(x y , δ)\{x y } = ∅. If ε &gt; δ: since lim k→∞ s k = x y , then for the given δ, ∃k 1 such that ∀k ≥ k 1 , d(x y , s k ) &lt; δ. If k 2 = max{k o , k 1 }, then ∀k &gt; k 2 , s k ∈ B(x y , δ) ⊂ B(x y , ε) and s k ∈ D, s k = x y , therefore ∀k &gt; k 2 , s k ∈ D ∩ B(x y , δ)\{x y } = ∅.</formula><p>Thus, in any case x y is an accumulation point. Then by lemma 3, the conclusion follows. QED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Machine Learning models and their respective implementations include some uncertainties due to singularity regions. As such, the abstraction provided in this paper named classifier (Definition 1) is indeed a rough approximation of ML models. Nonetheless, the results obtained mostly rely upon properties of the domain of the ML model and not in the classifier itself. An important claim in this approach is that the conditions for ML stability depend, in a first place, upon certain topological and measurable properties of the space, namely density, accumulation and openness. Overall, open and bounded sets are suitable candidates sets for stable classification. As it is shown, if such topological/metric properties are not ensured, classifiers cannot ensure a stable operation. This approach shall be leveraged, by adapting the notion of classifier so as to consider the uncertainty of classifiers. This is left as a future work. The equivalence between stability and the so named accumulation series is intended to facilitate the specification of algorithms to test (absence of) stability. Whereas accumulation points and the density of sets are simple notions, they can be hard to verify on complex, high dimension domains. Then the notion of accumulation series seeks for a discrete solution, more adapted to finite/limited precision of computers. A description of such discrete algorithms was barely sketched but not formally defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This work aims to provide further understanding regarding foundational aspects of Machine Learning models. The first part of this paper introduces a definition for stability, a property that appears fundamental for the proper operation of classifiers which are defined as a function that abstracts the uncertainty of real Machine Learning models. Secondly, some limits of classifiers were explained which appear whenever one or more classification sets are dense. Indeed, it was proved that the stability of classifiers demands the absence of dense sets in the domain of classification (also called Operational Design Domain) (Lemmas 1, 2). Some basic but representative examples were provided to illustrate such limitations. The last part of this work provides alternatives to prove stability in the form of equivalences: Lemma 3 provides conditions for equivalence between stability and accumulation points, then, Lemma 4 proves the equivalence between accumulation points and so named accumulation series, also introduced in this paper. The resulting equivalence between stability and accumulation series provides the possibility to define finite, discrete algorithms to prove stability. The definition of referred algorithms is not included in this paper and will be addressed as a continuation of this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Lemma 1 .</head><label>1</label><figDesc>Let (S, d) a metric space and M a classifier for D ⊂ S and D c . If D c is dense in S then the set of stable points in D is empty. First, S = D ∪ D c . Let's assume x o ∈ D a stable point of M , then ∃δ &gt; 0 such that B(x o , δ) ⊂ D. Since D c is dense in S, then B(x o , δ) ∩ D c = ∅. However, if x ∈ B(x o , δ) ∩ D c then x ∈ D c and by definition 2, ii) x ∈ D what contradicts the assumption. QED. By exchanging roles between D and D c in previous lemma, the following corollary is proved. Corollary 1. Let (S, d) a metric space and M a classifier for D ⊂ S and D c . If D is dense, then D c does not have any stable point. Corollary 2. Let S ⊂ R a (non-empty) interval, then there is no classifier M able to classify S ∩ I and S ∩ Q. Indeed, since (S ∩ Q) = (S ∩ I) c and S ∩ I = (S ∩ Q) c are both dense, by lemma 1 and Corollary 1 the conclusion follows. QED. The previous results, which hold particularly for intervals in R, are generalized in the following lemma.Lemma 2. Let (S, d) a metric space and M a classifier for a collection of sets D 1 . . . D n ⊂ S such that S = ∪ n i=1 D i . If there is D k dense in S then there is no stable point for M in any of the sets D i for i = k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1</head><label>1</label><figDesc>Accumulation Points Definition 4. Let (S, d) a metric space. Given a set D ⊂ S, a point x ∈ S is an accumulation point of D if ∀δ &gt; 0 then B(x, δ) ∩ D\{x} = ∅ [9] [6]. Lemma 3. Let (S, d) a metric space and M a classifier for D and D c , with both D and D c not dense in S and D an open set. Then x y ∈ D is a stable point for M if and only if x y is an accumulation point of D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>y and x ∈ D, what also leads to B(x y , ε) ∩ D\{x y } = ∅. [⇐] Now, let's assume x y is an accumulation point of D, an open set. It will be proved that x y ∈ D is a stable point of M . By definition, ∀δ &gt; 0, B(x y , δ) ∩ D\{x y } = ∅. Let δ &gt; 0 arbitrary but fixed. Then, since ∀δ α ≤ δ, B(x y , δ α ) ∩ D\{x y } = ∅, this implies that ∃x α ∈ B(x y , δ α ) ∩ D, x α = x y . Therefore, M (x α ) = y since x α ∈ D and M classifier for D and D c . Previous statement holds for any δ α ≤ δ. Let's assume ∀δ α &lt; δ, B(x y , δ α ) ∩ D c = ∅. Since D is open and x y ∈ D, then ∃ε &gt; 0 such that B(x y , ε) ⊂ D. The following cases exist:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 4 .</head><label>4</label><figDesc>Let (S, d) a metric space and M a classifier for D and D c , with both D and D c not dense in S and D an open set. x y is a stable point of M if and only</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AI Project: The Confiance</title>
		<author>
			<persName><surname>Confiance</surname></persName>
		</author>
		<ptr target="https://www.confiance.ai/" />
	</analytic>
	<monogr>
		<title level="j">AI Project</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://www.deel.ai/" />
		<title level="m">DEEL Project: Dependable, Certifiable &amp; Explainable Artificial Intelligence for Critical Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts</title>
		<author>
			<orgName type="collaboration">European Commission</orgName>
		</author>
		<ptr target="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ai system engineering-key challenges and lessons learned</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ehrlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sobieczky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moser</surname></persName>
		</author>
		<idno type="DOI">10.3390/make3010004</idno>
		<ptr target="https://doi.org/10.3390/make3010004" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning and Knowledge Extraction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="56" to="83" />
			<date type="published" when="2020">12 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Artificial intelligence trust, risk and security management (ai trism): Frameworks, applications, challenges and future research directions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Habbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abuzaraida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="page">122442</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Halmos</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4684-9440-2</idno>
		<ptr target="https://link.springer.com/book/10.1007/978-1-4684-9440-2" />
		<title level="m">Measure Theory</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autonomics: In search of a foundation for nextgeneration autonomous systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sifakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page" from="17491" to="17498" />
			<date type="published" when="2020-07">jul 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Real and Functional Analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4612-0897-6</idno>
		<ptr target="https://link.springer.com/book/10.1007/978-1-4612-0897-6" />
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Introduction to Topological Manifolds</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4419-7940-7</idno>
		<ptr target="https://link.springer.com/book/10.1007/978-1-4419-7940-7" />
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Piorkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richards</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.06317</idno>
		<title level="m">Quantitative ai risk assessments: Opportunities and challenges</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust machine learning modeling for predictive control using lipschitz-constrained neural networks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Chemical Engineering</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page">108466</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
