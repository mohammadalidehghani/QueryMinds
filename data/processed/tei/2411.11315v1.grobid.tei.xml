<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Review on Machine Unlearning</title>
				<funder>
					<orgName type="full">Japan Science and Technology Agency (JST) Strategic International Collaborative Research Program (SICORP)</orgName>
				</funder>
				<funder ref="#_peCUQCX">
					<orgName type="full">JST SPRING</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-18">18 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haibo</forename><surname>Zhang</surname></persName>
							<email>zhang.haibo892@s.kyushu-u.ac.jp</email>
						</author>
						<author>
							<persName><forename type="first">Toru</forename><surname>Nakamura</surname></persName>
							<email>tr-nakamura@kddi-research.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">KDDI Research Inc</orgName>
								<address>
									<postCode>356-8502</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takamasa</forename><surname>Isohara</surname></persName>
							<email>ta-isohara@kddi-research.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">KDDI Research Inc</orgName>
								<address>
									<postCode>356-8502</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kouichi</forename><surname>Sakurai</surname></persName>
							<email>sakurai@inf.kyushu-u.ac.jp</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Faculty of Information Science and Electrical Engineering</orgName>
								<orgName type="institution">Kyushu University</orgName>
								<address>
									<postCode>819-0395</postCode>
									<settlement>Japan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Graduate School of Information Science and Electrical Engineering</orgName>
								<orgName type="institution">Kyushu University</orgName>
								<address>
									<postCode>819-0395</postCode>
									<settlement>Japan</settlement>
									<country></country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Review on Machine Unlearning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-18">18 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">EBD6AB9083B2BE46D237D392B700C627</idno>
					<idno type="arXiv">arXiv:2411.11315v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine learning</term>
					<term>Security</term>
					<term>Privacy</term>
					<term>Machine unlearning</term>
					<term>Data lineage</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, an increasing number of laws have governed the useability of users' privacy. For example, Article 17 of the General Data Protection Regulation (GDPR), the right to be forgotten, requires machine learning applications to remove a portion of data from a dataset and retrain it if the user makes such a request. Furthermore, from the security perspective, training data for machine learning models, i.e., data that may contain user privacy, should be effectively protected, including appropriate erasure. Therefore, researchers propose various privacy-preserving methods to deal with such issues as machine unlearning. This paper provides an in-depth review of the security and privacy concerns in machine learning models. First, we present how machine learning can use users' private data in daily life and the role that the GDPR plays in this problem. Then, we introduce the concept of machine unlearning by describing the security threats in machine learning models and how to protect users' privacy from being violated using machine learning platforms. As the core content of the paper, we introduce and analyze current machine unlearning approaches and several representative</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Background</head><p>Privacy protection has been a concern for researchers for a long time. In today's big data environment, users interact with data on various web platforms, such as sending and receiving emails, and browsing news, almost every day. For users, once they have provided their information in an application, it is difficult to remove it from the root. When machine learning is widely used today, most advanced features are obtained based on understanding and training data. As a result, users' privacy has been spread in every corner of the application, makings it more accessible for attackers to steal users' private data.</p><p>From the security perspective, if an attacker compromises the machine learning model by injecting some pollution data into its dataset, it is also necessary to remove such data from the dataset and retrain it <ref type="bibr" target="#b0">[1]</ref>. For example, an attacker can open a backdoor in a machine learning model by injecting malicious data into the dataset used for training <ref type="bibr" target="#b1">[2]</ref>. As a result, the attacker can steal all the private data in the model, shown in Figure <ref type="figure">1</ref>.</p><p>Fig. <ref type="figure">1</ref> The necessity of machine unlearning. The red arrow indicates that the attacker can access the training data or parameters of the machine learning model through malicious data injection or information stealing to obtain user privacy or even reconstruct the machine learning model. In this case, according to the orange arrow, the data owner will request to delete specific sensitive data, and the model owner needs to apply machine unlearning methods to remove the requested data.</p><p>For solving the above problems, it is necessary to retrain the machine learning model. However, the existing retraining methods cause a large amount of computational power and time consumption. Therefore, researchers propose machine unlearning as a more efficient research method <ref type="bibr" target="#b2">[3]</ref>.</p><p>The word "unlearning" means that the machine learning model is re-trained to generate a new predictive model with a portion of the data forgotten. There are two ways to perform unlearning on machine learning models. One is to retrain the new dataset from scratch after data removal (i.e., exact unlearning mentioned in Section 4.4. The other is to modify the machine learning model and dataset to achieve an approximate unlearning effect (i.e., approximate unlearning mentioned in Section 4.5). The ultimate goal of either unlearning approach is to improve the accuracy of unlearning methods while being as efficient as possible.</p><p>This paper provides an in-depth analysis of machine learning models' security and privacy concerns, which also refers to the privacy-preserving machine learning <ref type="bibr" target="#b3">[4]</ref>. This paper aims to provide a comprehensive analysis and summary of current machine unlearning techniques and future research potential.</p><p>First, we present how machine learning can use users' private data in daily life and the role that the GDPR plays in this problem. Then, we introduce the concept of machine unlearning by describing the security threats in machine learning models and how to protect users' privacy from being violated using machine learning platforms. In the next section, we introduce and analyze current machine unlearning approaches and several representative research results and discuss them in the context of the data lineage. Furthermore, we also discuss the future research challenges in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">How Machine Learning can use users' data?</head><p>Since the idea of simulating human intelligence was first proposed in the 1960s, artificial intelligence (AI) received widespread attention in both academical and industry fields. As the primary component of AI, machine learning also gained unprecedented development in recent years. Moreover, its application has spread to various fields of AI. For example, we can use machine learning to classify and locate objects in the field of computer vision, and we can also use deep neural networks to design and implement a high-accuracy face recognition system. In addition, we can also use machine learning in natural language processing to design and implement an intelligent question and answer system.</p><p>In the modern Big Data environment, Internet users interact with various applications almost every day. Enterprises and developers use data mining, big data analytics, and machine learning techniques to extract useful information from the vast database. This data contains more or less sensitive information about users, such as their identity and passwords. Hence, machine learning plays an important role.</p><p>Machine learning is a branch of artificial intelligence that automatically enables computers to learn from experience through human intervention. The whole concept of machine learning starts around determining the answer to an obstacle without human interference, which begins with understanding data from examples or direct experience, analyzing data patterns and making better decisions based on inferences. It is best used for problem-solving when large amounts of data and variables exist without using existing algorithms. For example, Google tends to optimize search results and pop-up ads for products similar to users' tastes or websites they have visited before. It studies the user's behavior and displays the results accordingly.</p><p>Machine learning is an integral part of big data analytics. Big data analytics includes big data, data learning, statistical information, etc. Machine learning uses programming and computational algorithms to conclude, while big data analytics uses numbers and statistics to draw results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">The General Data Protection Regulation</head><p>Recently, an increasing number of laws have governed the usability of users' privacy. For example, Article 17 of the General Data Protection Regulation (GDPR), the right to be forgotten, requires entities to remove a portion of data from a dataset if the user makes such a request <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Furthermore, it maintains the user's right to use their privacy from a privacy protection perspective <ref type="bibr" target="#b3">[4]</ref>.</p><p>The GDPR is a new EU privacy and data protection regulation. It requires more granular privacy protections in company systems, more detailed data protection agreements, and more user-friendly and detailed disclosures about company privacy and data protection practices.</p><p>The GDPR has direct legal implications for all EU member states, i.e., it is binding without having to be transposed into the national laws of EU member states. This will enhance the consistency and harmonization of the implementation of EU law.</p><p>From its initial draft in 2012 to becoming official EU law in 2016, the right to be forgotten was initially intended to bind Internet search engines, such as Google and Yahoo, in their use of users' privacy. Under the Article 17, if a user requests the deletion of any private data, the search engine shall immediately execute and is not allowed to refuse. However, implementing this law also raises considerations about the current hot topic, machine learning technology, and overusing users' private data. For example, how should machine learning platforms respond if data holders request to delete specific data used for training purposes?</p><p>In the context of machine learning, the right to be forgotten requires the machine learning applications to be able to readily accommodate requests from data owners who wish to delete any data <ref type="bibr" target="#b6">[7]</ref>. This process is called machine unlearning. The machine learning application needs to remove the requested data from the training dataset and retrain the machine learning model from scratch.</p><p>The appearance of the Article 17 has primarily limited the undesirable phenomenon of misused and unprotected user privacy in the current fast-growing Internet and big data environment. However, privacy protection should be carried out from both the perspective of the data controller and the data holder.</p><p>The right to be forgotten can be regulated from the perspective of data controllers, but it does not work from the perspective of data holders. That is, how data holders become aware of the violation of their privacy and when they request the deletion of their private data. These cannot be regulated by the regulation and require data holders to raise their awareness of privacy protection under the guidance of social engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Security Concerns 2.1 Machine Learning is Still Weak</head><p>In the era of big data and artificial intelligence, people can access information more quickly and efficiently. However, while gaining convenience, our behavior is being recorded, learned and used all the time. If we ignore privacy protection in the application, it will be challenging to prevent personal information from being used for illegal purposes.</p><p>Due to the vulnerability problem of machine learning models themselves, attackers can attack machine learning models by sending many malicious requests, exposing machine learning services to various potential security risks <ref type="bibr" target="#b7">[8]</ref>.</p><p>• Data privacy leakage risk: Attackers can exploit the model vulnerability to obtain data information for training models by invoking machine learning services. • Model theft risk: model parameter information in machine learning services due to the model's vulnerability issues, making it risky for attackers to speculate and restore model parameter information by frequently invoking the service. • Data Poisoning: The attacker can mix specific malicious data in the request process, which can affect the model training and subsequent model inference through the feedback of the service process to achieve the effect of interfering with the model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. • Evasion: Attackers can make machine learning services make wrong judgments by adding a small amount of noise and perturbation to typical requests.</p><p>Usually, when designing machine learning systems, developers consider specific threat models to ensure that the designed system is secure and trustworthy. So far, most of the existing machine learning models have been designed and implemented for a fragile threat model without much consideration of the attackers <ref type="bibr" target="#b10">[11]</ref>. Although these models can perform very well in the face of natural inputs, in a realistic setting, these machine learning models encounter many malicious users and even attackers.</p><p>Toreini et al. <ref type="bibr" target="#b11">[12]</ref> provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. For example, attackers have different degrees of ability to maliciously modify the inputs and outputs during the model's training and prediction phases. Even they can access the internal structure of the model by some means and steal the parameters, thus destroying the confidentiality, integrity and usability of the models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CIA Triad in Machine Learning</head><p>The CIA triad is a common assessment model that forms the basis for developing security systems and policies. The CIA refers to confidentiality, integrity and availability. The CIA triad identifies system vulnerabilities and methods to address problems and create effective solutions.</p><p>Attacks against machine learning models can impact the Confidentiality, Integrity, and Availability <ref type="bibr" target="#b12">[13]</ref>. Figure <ref type="figure" target="#fig_0">2</ref> describes how the CIA triad can be applied to the machine learning model.</p><p>• Confidentiality attacks means that machine learning systems must ensure that unauthorized users do not have access to the information. While most machine learning platforms are professional and secure, the algorithms provided by machine learning model providers are not necessarily reliable <ref type="bibr" target="#b13">[14]</ref>.</p><p>When data holders use MLaaS (Machine Learning as a Service) to train their predictive models, they may select a malicious model carefully constructed by an attacker. In such models, the attacker encodes the data holder's private data into the parameters of the model and finally steals the user's private data by decoding the parameters of the model <ref type="bibr" target="#b14">[15]</ref>. • Machine learning models are most vulnerable to integrity attacks, occurring both in the learning and prediction phases. If the attacker disrupts the model's integrity, then the model's prediction results will deviate from expectations. The attacker can modify the existing training set or add additional malicious data to compromise the integrity of the model in order to reduce the accuracy in the prediction phase <ref type="bibr" target="#b15">[16]</ref>. When the model is trained and used for prediction, the attacker only needs to add a small perturbation to the sample to be predicted, which is unrecognizable to the human eye but sufficient to make the model classification wrong. • The availability of machine learning models can also be a target of attack.</p><p>For example, in a driverless scenario, if an attacker places something complicated to identify on the side of the road where a vehicle would pass, it could potentially force a self-driving car to go into safety protection mode and then stop at the side of the road.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Privacy-preserving</head><p>In recent years, more and more people have started to pay attention to data privacy and pay more attention to privacy terms when choosing to use client software (apps). Some studies have shown that the protection of privacy can increase the usage rate of users <ref type="bibr" target="#b16">[17]</ref>.</p><p>As research evolves, machine learning models become more powerful and require more training data. For example, some training models in the industry need to use hundreds of gigabytes of data to train billions of parameters. Unfortunately, in many professional fields such as healthcare and financial fraud prevention, data is divided into silos due to privacy or interests, making machine learning face the problem of insufficient valid data. Therefore, information flow and machine learning cannot be achieved without providing guarantees for data privacy.</p><p>For privacy-preserving approaches in machine learning, they can be divided into confidential computing, model privacy, and distributed learning <ref type="bibr" target="#b3">[4]</ref>.</p><p>• Confidential computing means that the transmission of data and the computation process is confidential. Current approaches to achieving confidential computing include Multi-party Secure Computation, Homomorphic Encryption and Trusted Executive Environment. Confidential computing can be done to protect data privacy during the training process. So can the trained model cause the leakage of private training data? The answer is yes because machine learning models are overfitted to some extent. The models themselves remember part of the training data, leading to private training data leakage by the published models. • For model privacy, this includes differential private machine learning and machine unlearning algorithms. A common practice to achieve differential privacy is to add noise. Adding noise entails performance loss of the model, and differential privacy machine learning studies how to add noise more economically and how to add the least amount of noise to achieve the best performance for a given privacy loss requirement. Another hot topic of model privacy research is machine unlearning. If implementing differential privacy is viewed as actively designing algorithms to make the output model satisfy the privacy requirements, then machine forgetting is a passive solution to model privacy. It aims to implement the user's "the right to be forgotten" in machine learning models.</p><p>• The vision of federated learning is to perform multi-party federated machine learning without sharing data, which is essentially a distributed machine learning framework with restricted data access. Compared to classical distributed machine learning, the first layer of constraints in federated learning is data isolation -data is not shared across endpoints, is not balanced, and interactive communication is kept to a minimum.</p><p>Research on privacy-preserving machine learning has never stopped. Among the many approaches, machine unlearning is emerging and closely related to machine learning algorithms themselves. Current studies on machine unlearning cover various research approaches, such as model privacy, differential privacy, and federation learning. It also demonstrates the importance of machine unlearning in studying privacy-preserving machine learning. Therefore, this paper focuses on machine unlearning as a privacy-preserving approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Machine Unlearning</head><p>In this section, we explain the definitions of machine learning and machine unlearning and introduce the two primary approaches of machine unlearning, i.e., exact unlearning and approximate unlearning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Defining Machine Learning</head><p>Machine learning is a technique that makes judgments by predicting possible outcomes. The programmer designs an initial model, trains it on a specific data set, and continuously optimizes the parameters in the model based on the prediction results obtained, which eventually leads to a mature model. Figure <ref type="figure" target="#fig_1">3</ref> shows the general process of a machine learning system. The task to be learned in machine learning can be defined in a space Z of the form X × Y, where X is named the sample space and Y is named the output space <ref type="bibr" target="#b2">[3]</ref>. Taking supervised machine learning as an example, in the image classification problem, for a given data set D of the input-output pairs (x, y) ∈ X × Y, the learning aims to find a model function satisfying F: X → Y in a continuous optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Machine Retraining vs. Machine Unlearning</head><p>The most intuitive approach to machine unlearning is to retrain the model on the training data set after deleting the specified data. However, this approach is computationally expensive, so the primary goal of machine unlearning is to reduce the computational cost. One approach is to post-process the trained model so that the results of the machine unlearning algorithm are statistically indistinguishable from the retrained model <ref type="bibr" target="#b17">[18]</ref>. Another approach is to design new training methods to reduce the cost of retraining. For instance, dividing the data into different blocks, training a separate sub-model for each block, and aggregating the results of the sub-models so that only one sub-model needs to be retrained to remove a data point <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>. Figure 4 explains the difference between machine retraining and machine unlearning methods. As opposed to removing data from the data set and retraining the entire model, the purpose of machine unlearning is to minimize the cost of time and computational power associated with retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Defining Machine Unlearning</head><p>The purpose of machine unlearning is that when the user requests to delete a part of the data, the model that has been learned needs to be retrained in order to generate a model distribution as if that part of the data had not been learned from the beginning.</p><p>The unlearning problem is defined as a kind of game between two parties, the service provider S, and the user population U by Bourtoule et al. <ref type="bibr" target="#b2">[3]</ref>. The service provider S can be an organization that can collect various users' information, and the collected information is stored in the form of a dataset D. The service provider S uses this data to train and test a machine learning model M as a way to provide a intelligent service to the user U. Then according to the GDPR, any user u ∈ U has the right to request the removal of part of the data d u from D, and the service provider S must execute it. Thus, the service provider S must modify the model M to generate a new model M¬d u , which represents a model without trained data d u .</p><p>Guo et al. <ref type="bibr" target="#b20">[21]</ref> propose a similar concept, certified removal, from an accuracy perspective. D is assumed to be a training dataset and A is the learning algorithm used to train D, resulting in model h ∈ H, that is, A: D → H. When a request is made to remove sample x from D, this results in a data removal mechanism M, one that can be applied to A(D) and removes the effects of x. If the removal is successful, the output of M should be much close to the output of A applied on D¬x. Given ϵ &gt; 0, the removal mechanism M is said to perform ϵ-certified removal for learning algorithm A if ∀T ⊆ H, D ⊆ X , x ∈ D:</p><formula xml:id="formula_0">e -ϵ ≤ P(M(A(D), D, x) ∈ T) P(A(D¬x) ∈ T) ≤ e ϵ .</formula><p>The above definition states that the ratio between the likelihood of a model after the removal of sample x and a model that was never trained on sample x is close to one for all models, all possible data sets, and all removed samples. However, some researchers have also proposed different views on defining machine unlearning. Thudi et al. <ref type="bibr" target="#b21">[22]</ref> argue that machine unlearning should be divided into exact unlearning <ref type="bibr" target="#b22">[23]</ref> and approximate unlearning <ref type="bibr" target="#b19">[20]</ref>. Exact unlearning means the model outputs after removing the sample x is the same as the one that was never trained on the removed sample x ; approximate unlearning means the model and dataset are adjusted so that it does not need to be retrained from scratch. Current definitions of machine unlearning seek to make the output of approximate unlearning as close as possible to the output of exact unlearning. They suggest this definition is incorrect because the same model can be obtained even when trained on a different data set. Moreover, this definition only applies at the algorithmic level.</p><p>Figure <ref type="figure" target="#fig_3">5</ref> illustrates how machine unlearning algorithms can be applied to machine learning models and the essential difference between exact unlearning and approximate unlearning by defining a typical machine learning pipeline <ref type="bibr" target="#b19">[20]</ref> with the three phases of model training, inference and data unlearning. Finally, we summarize reviewed studies relatively to exact and approximate unlearning, as shown in Table <ref type="table" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Exact Unlearning</head><p>Exact unlearning <ref type="bibr" target="#b22">[23]</ref> means that in the case of direct use of user data to build a machine learning model, such as a prediction task, a reasonable criterion is that the state of the system is adjusted to what it would be in the complete absence of user data. Ullah et al. proposed an efficient machine unlearning algorithm, total variation stability, for the convex risk minimization problem, provided that the following three properties are satisfied.</p><p>• in the stream, at every time point, the output model should be indistinguishable from what we would have obtained if trained on the updated dataset; • the run-time of unlearning method should be small; • the output model should be effective in terms of the accuracy.</p><p>There exist several exact unlearning approaches, for example, in support vector machines <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref>, naive bayes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>, collaborative filtering and ridge regression. This subsection will introduce and analyze several representative exact unlearning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Machine Unlearning's First Proposed</head><p>Cao and Yang first introduced the concept of machine unlearning in <ref type="bibr" target="#b23">[24]</ref>. They present an unlearning method by transforming the model learning algorithm into a summation form that follows the statistical query (SQ) learning <ref type="bibr" target="#b45">[46]</ref>. The unlearning method is performed by simply updating a small number of summations from the training dataset. The small number of summations is set in a layer between the machine learning algorithm and the model's training data to break down the dependencies. The learning algorithm only depends on the summations.</p><p>The authors implemented the unlearning method based on non-adaptive SQ learning (i.e., all SQs are determined upfront before the algorithm starts) and adaptive SQ learning (i.e., the later SQs may depend on the earlier SQ results). In this case, their summation forms can be implemented in many machine learning models and all stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">SISA Training Approach</head><p>Bourtoule et al. <ref type="bibr" target="#b2">[3]</ref> introduce SISA training approach, short for Sharded, Isolated, Sliced, and Aggregated training. This framework expedites the unlearning process by strategically limiting the influence of a data point in the training procedure, which is illustrated in Figure <ref type="figure">6</ref>.</p><p>In this model, the authors slice the original dataset D into s sub-datasets D 1 to D s , and the machine learning network into s sub-networks M 1 to M s . Each sub-dataset D k is trained by the corresponding sub-network M k , and the final training results are integrated by the aggregation algorithm. In this series, if a portion of data is requested to be deleted, it is only necessary to remove it from the sub-dataset and retrain it. Finally, the training results are reintegrated to obtain the new training results. This approach reduces the unnecessary data and model training process and dramatically reduces the time and computational power consumed by machine retraining.</p><p>Fig. <ref type="figure">6</ref> The SISA approach is presented in the form of federated learning. In the SISA structure, separated machine learning models are trained on separated data blocks and the outputs are aggregated in the final inference stage. <ref type="bibr" target="#b2">[3]</ref>.</p><p>In the paper, the authors illustrate that for simple learning tasks, the SISA training approach can accomplish the unlearning requests quickly without affecting the accuracy of the model. However, for complex learning tasks, the SISA training approach needs to be combined with other learning methods, such as transfer learning, to reduce the impact on model accuracy and to complete the unlearning requests quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Approximate Unlearning</head><p>Approximate unlearning is a method for approximating the effect of model retraining by adjusting machine learning models and data sets. Mahadevan et al. <ref type="bibr" target="#b19">[20]</ref> summarize that approximate unlearning methods can be roughly divided into three groups, and this subsection will introduce several methods on this basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">First Group</head><p>The first group updates the machine learning model by retraining it with the remaining data and injecting optimal noise based on the principle of Fisher information matrix <ref type="bibr" target="#b46">[47]</ref> to control the certifiability.</p><p>Differential privacy <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref> can guarantee that the parameters of the trained model do not leak any individual information. Golatkar et al. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref> proposed a method to selectively unlearn the dataset and update machine learning models based on differential privacy methods. They propose a method for "scrubbing" the weights to remove specific training data used to train deep neural networks. This method does not require retraining from scratch or accessing the data initially used for training. Instead, this method modifies the weights of the model so that any probe function of the weights approximates the same function as the weights of a network that has not been trained on these particular data.</p><p>In <ref type="bibr" target="#b34">[35]</ref>, Golatkar et al. introduce a new concept for machine unlearning, mixed-privacy setting, based on their previous research. According to this method, a "core" subset of the training samples need not be unlearned. Similar to <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref>, this method allows to effectively remove all the information contained in the non-core data by simply setting a subset of the weights to zero with minimal performance loss. They demonstrate that this method yields significant improvements of unlearning in accuracy and guarantees for a large-scale vision classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Second Group</head><p>The second group updates the machine learning model with the deleted data during the unlearning, they perform a Newton's method <ref type="bibr" target="#b50">[51]</ref> to estimate the impact of the deleted data on the model and removing it. The work <ref type="bibr" target="#b20">[21]</ref> attempted approximate retraining by taking a single Newton's step. This can be formed as</p><formula xml:id="formula_1">θ N ewton = θ f ull -[∇ 2 θ L \k (θ f ull )] -1 ∇ θ L \k (θ f ull ).</formula><p>Where θ ∈ R d denotes the model parameters, k denotes the number of data points to be deleted from the model, θ f ull = argmin θL f ull (θ) are the model parameters when fitted to the full dataset, L \k (θ) is the loss on the LKO dataset. When the loss function is quadratic in θ, the approximation to L \k is just L \k itself which means Newton's method is effective for solving this issue. Izzo et al. <ref type="bibr" target="#b35">[36]</ref> introduces the Influence method <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> to estimate the influence of a particular training point on the model's predictions. The influence method can be formed under suitable assumptions on the loss function l : θ(w) ≡ argmin θ n i=1 w i l(x i , y i ; θ), where n denotes the total number of training points, X = [x i , ..., x n ] ⊤ ∈ R n×d is the data matrix for full set of training data D f ull , Y = [y i , ..., y n ] ⊤ ∈ R n is the response vector for D f ull , d denotes the data dimension. In this setting, θ f ull = θ(1) where 1 is the all 1s vector and θ \k = θ((θ, ..., k 1, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n-k</head><p>) ⊤ ). The influence function approach uses the linear approximation <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>:</p><formula xml:id="formula_2">θ inf = θ f ull -[∇ 2 θ L f ull (θ f ull )] -1 ∇ θ L \k (θ f ull ).</formula><p>to θ(w) about w = 1 to estimate θ \k . Therefore, they propose a unlearning method based on the influence method principle that the computational cost is linearly related to the feature dimension d, i.e., O(d 2 ), and is independent of the number of training data n. And this method is applicable to both linear regression and logistic regression models. The influence method explains the principle of machine unlearning at a higher level. In other words, the influence method-based unlearning can compute the impact of the deleted data relatively to the parameters of the trained model for removing the influence and updates the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Third Group</head><p>The third group stores the data and related information during the machine learning model training and use them to update the model when a request to delete the data is made <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Graves et al. <ref type="bibr" target="#b5">[6]</ref> proposed the concept of Amnesiac Unlearning, where the model owner stores the sensitive data and parameters in the form of batches during the training process. When a request for deleting the data is made, the model owner does not perform the parameter update of the batches containing the deleted data. This process can also be interpreted as selectively undoing specific machine learning steps containing sensitive data. The model training can be regarded as a series of parameter updates to the initial model parameters. The model parameters can be expressed as:</p><formula xml:id="formula_3">θ M = θ initial + E e=1 B b=1 ∆ θ e,b</formula><p>where θ initial is the initial model parameters, model M is trained for E epochs, each epoch consists of B batches. The model parameters are updated after each batch by an amount ∆ θ e,b . During training, the model owner stores a list SB, which refers to the batches contain the sensitive data. When the request of removing data s belonging to the batch b, where sb ∈ SB, is received, the amnesiac unlearning method can simply remove the parameter updates from the learning parameters θ M to get the θ M ′ :</p><formula xml:id="formula_4">θ M ′ = θ initial + E e=1 B b=1 ∆ θ e,b - SB sb=1 ∆ θ sb = θ M - SB sb=1 ∆ θ sb .</formula><p>This approach has a potential drawback in that the model owner needs a large amount of storage space for storing sensitive data and related parameters. However, the authors argue that this space cost is much less than the computational and time cost of the exact unlearning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Evaluation Metrics</head><p>For approximate unlearning, in addition to designing an effective and fast algorithm for data deletion, it is a significant challenge to evaluate the quality of an approximate unlearning method properly. As a result, many researchers proposed effective evaluation metrics for their algorithms.</p><p>In <ref type="bibr" target="#b19">[20]</ref>, the authors defined three evaluation metrics to measure the performance of different unlearning methods in terms of effectiveness, certifiability and efficiency on the basis of the Symmetric Absolute Percentage Error (SAPE) defined as:</p><formula xml:id="formula_5">SAP E(a, b) = | b -a | | b | + | a | • 100%.</formula><p>• Effectiveness is used to measure the prediction accuracy of a machine learning model.</p><p>The error in test accuracy Acc Err of the updated model w u is defined as: Acc Err = SAP E(Acc * test , Acc u test ) where Acc u test denotes the accuracy of the updated model w u on the test dataset D test , Acc *</p><p>test denotes the optimal accuracy of the regression model on the same dataset. The lower value of Acc Err means that the prediction accuracy of w u is closer to the accuracy of the initial model (in which the noise value σ = 0), i.e., w u is more effective.</p><p>• Certifiability is used to measure how well the updated model w u has unlearned the delated data. For the certifiability, both the updated model and the fully retrained model are considered, the disparity in accuracy of the two models Acc Dis is defined as:</p><formula xml:id="formula_6">Acc Dis = SAP E(Acc * del , Acc u del )</formula><p>where Acc u del denotes the accuracy on the deleted data D del for the updated model w u , and Acc * del denotes the accuracy on the deleted data for the fully retrained model. The lower value of Acc Dis means that the updated model is more similar to the fully retrained model, i.e. the updated model w u has higher certifiability.</p><p>• Efficiency is used to measure the speed-up performance of running the algorithm to obtain the updated model w u and the fully retrained model w * :</p><p>speed -up = time taken to obtain w * time taken to obtain w u • x.</p><p>Izzo et al. <ref type="bibr" target="#b35">[36]</ref> introduced two metrics, L 2 distance and Feature injection test, to evaluate the effectiveness of an approximate data deletion method.</p><p>• L 2 distance between the updated model and the fully retrained model is a relatively common method used to measure the accuracy of approximate unlearning. A lower value of L 2 distance indicates that the predictive ability of the updated model is closer to that of the fully retrained model. • Feature injection test is injected as a strong signal (an extra feature) into the remaining dataset, which the model (updated and fully retrained model) expects to learn. The authors measure the effectiveness of the approximate deletion method by observing the performance of the model's learned parameters before and after the removal of this particular feature.</p><p>5 Discussion on Data Lineage Fig. <ref type="figure">7</ref> Data lineage management for machine learning data flows recording and machine unlearning updating. For each step in the machine learning system, from the original dataset to optimizing each parameter in the training process to getting the final training results and analyzing the results, all data and changes will be recorded in the data lineage management system. This series of records of data flow characteristics and changes allow developers to control and track any subtle differences in the model learning process at any time.</p><p>In the process of protecting the privacy of machine learning models, the tracking of data flow is an essential part of the process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref>. Therefore, this section discusses the role of data lineage management techniques in the privacy protection of machine learning models.</p><p>Data lineage tracks data movement over time from the source system to different forms of persistence and transformations and ultimately to data consumption by an application or analytics model. The data lineage management system can monitor any data changes in the machine learning model that occur at any point in time <ref type="bibr" target="#b55">[56]</ref>. Therefore, the combination with the data lineage management system can effectively enhance the security protection of machine learning models.</p><p>Data lineage management can be applied to defend against particular cyberattacks, such as data poisoning attacks, which can be viewed as integrity attacks. The attacker affects the model's prediction of the correct output by tampering with the training data. Even the attacker's goal is to have their input accepted as the model's training data.</p><p>Baracaldo et al. <ref type="bibr" target="#b0">[1]</ref> proposed to identify poisonous data by using the sytem's lineage about the sources, transformation and destinations of data points in the training dataset of a machine learning model as part of a filtering algorithm, which is also known as a method for detecting causative attacks. With this approach, online and periodically retrained machine learning systems can discriminate between data sources in a potentially adversarial environment. Subsequently, they applied this approach to identify poisonous data injection in the Internet of Things environment as well <ref type="bibr" target="#b9">[10]</ref>.</p><p>The Tensorflow team of Google developed a version control platform for machine learning data lineage management, Machine Learning Metadata (MLMD). MLMD can be viewed as a library to track the complete data lineage of the entire machine learning workflow, including the metadata, data preprocessing, feature selection, model training, prediction, evaluation, deployment and so on. This work aims to answer questions like,</p><p>• What hyperparameters were this model used?</p><p>• Which dataset was this model trained on? • Which version of libraries were used to build this model? • Which pipeline was used to build this model? • Which version of this model was last deployed? • What is the reason for this model's failure?</p><p>MLMD can be implemented in various machine learning pipelines to record and control all the data generated during model training. It can help developers analyze all model data transformations, including parameter updates and debugging of errors. Furthermore, from the security perspective, this metadata platform also provides ideas for the research of combining machine unlearning with data lineage.</p><p>Figure <ref type="figure">7</ref> explains how the data lineage management technique works in a machine learning system. With the machine unlearning approach, data lineage can still play an important role. For example, developers use machine unlearning when a user wants to withdraw sensitive personal information used to train a machine learning model or when a malicious data injection attack is detected. A part of the training data needs to be removed from the dataset to retrain the model. This process is also recorded in the data lineage management system without reservation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Challenges</head><p>This paper describes the security risks and privacy protection issues associated with machine learning models. Both machine unlearning and data lineage management systems can play a role in addressing these issues. However, research in this area is just beginning, and researchers still face many challenges. For example, how machine unlearning can efficiently handle large amounts of data deletion tasks in a big data environment; how to quickly respond to privacy theft when machine learning platforms encounter it; and how data lineage management systems can make the most of the privacy-preserving aspects of machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Machine Unlearning Algorithms</head><p>There are not many machine unlearning algorithms designed for the privacy preservation of machine learning models. Instead, algorithms with high adaptability are necessary for different user needs or data diversity. For example, the superiority of the SISA algorithm can be demonstrated when the amount of data requested for deletion is small. However, when the amount of data requested for deletion is large, the retraining approach becomes more applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Active and Passive Unlearning</head><p>The machine unlearning methods we are discussing are all based on the active unlearning at the will of the data holder. However, passive unlearning is also a good option for the CIA property of machine learning models. When an attacker performs a CIA attack on a machine learning model, the data holder or the machine learning platform does not discover this attacker's behavior in time, which leads to the private data being compromised before taking countermeasures (such as machine unlearning methods). In this case, the passive model unlearning method can delete the data in time when the machine learning system is attacked, thus minimizing the loss of the data holder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Privacy Risks of Machine Unlearning</head><p>The original intent of machine unlearning was to prevent privacy leaks caused by machine learning. However, some researchers have questioned the privacypreserving effects of machine unlearning in recent years. Chen et al. <ref type="bibr" target="#b6">[7]</ref> propose that machine unlearning methods can also be attacked and leak the privacy of models in specific scenarios, such as membership inference attacks <ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref>. They designed novel membership attacks and conducted experimental evaluations against two machine unlearning approaches, retraining the machine learning model from scratch and the SISA approach. The experimental results show that their attack methods significantly impact unlearning methods that handle tedious tasks, i.e., retraining from scratch. In contrast, they have less impact on distributed unlearning models like SISA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Working with Data Lineage</head><p>At the level of security and privacy protection for machine learning, the data lineage management system can trace all data and changes in the model. With the introduction of the machine unlearning approach as a protection mechanism for machine learning models, it is imperative to use it in conjunction with a data lineage management system. The machine unlearning approach can be considered another model independent of the machine learning model used for training in the same environment. Any data changes in the machine unlearning model directly affect the security of the machine learning model used for training and, therefore, should be recorded by the data lineage management system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper starts with the right to be forgotten of the GDPR regulations. Then, it discusses the security concerns in machine learning models and the possible privacy breaches to the data holders used for training. In this process, machine unlearning methods and data lineage management play an essential role in machine learning privacy protection. Furthermore, the challenges this research area may encounter in the future are elaborated. More and more machine learning models appear in our lives at a swift pace. While we enjoy the convenience of technological development, we cannot let down our guard on the potential security threats that may exist.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 CIA triad in machine learning.</figDesc><graphic coords="6,52.53,109.11,334.32,147.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 The general machine learning system is consists of three stages, i.e. feature selection, model training and prediction.</figDesc><graphic coords="8,54.69,411.80,330.00,116.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Machine Retraining vs. Machine Unlearning</figDesc><graphic coords="9,48.35,261.59,342.67,137.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 A</head><label>5</label><figDesc>Fig.5A typical machine learning pipeline consists of three primary stages, i.e., training, inference, and unlearning. First, the initial model W * is trained on the initial dataset D init , and the output is used in the inference stage; afterward, once a request to delete the data Dm is received, the updated model W u can be obtained through the unlearning stage, when the data set becomes D \ Dm. The process pointed by the red arrow is to apply the updated model W u directly to the inference stage, i.e., approximate unlearning; the process pointed by the green arrow is to start retraining the initial model W * on the new data set D \ Dm from scratch, i.e., exact unlearning<ref type="bibr" target="#b19">[20]</ref>.</figDesc><graphic coords="10,55.72,55.28,327.93,172.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="2,65.97,353.90,307.44,172.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="17,47.23,146.89,344.90,176.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Summary of reviewed studies relatively to exact and approximate unlearning.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Unlearning type</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Exact</cell><cell>Approximate</cell><cell></cell></row><row><cell>Author</cell><cell cols="2">Year unlearning</cell><cell>unlearning</cell><cell>Approach</cell><cell>Ref.</cell></row><row><cell>Cao &amp; Yang</cell><cell>2015</cell><cell>✓</cell><cell></cell><cell>Summations following SQ learning</cell><cell>[24]</cell></row><row><cell>Cao et al.</cell><cell>2018</cell><cell>✓</cell><cell></cell><cell>Causal unlearning</cell><cell>[25]</cell></row><row><cell>Ullah et al.</cell><cell>2021</cell><cell>✓</cell><cell></cell><cell>Total variation stability</cell><cell>[23]</cell></row><row><cell>Kashef</cell><cell>2021</cell><cell>✓</cell><cell></cell><cell>Decremental unlearning</cell><cell>[26]</cell></row><row><cell>Schelter</cell><cell>2021</cell><cell>✓</cell><cell></cell><cell>Incremental maintenance</cell><cell>[5]</cell></row><row><cell>Jose &amp; Simeone</cell><cell>2021</cell><cell>✓</cell><cell></cell><cell>Pac-bayesian</cell><cell>[27]</cell></row><row><cell>Bourtoule</cell><cell>2021</cell><cell>✓</cell><cell></cell><cell>Federated learning (SISA)</cell><cell>[3]</cell></row><row><cell>Liu et al.</cell><cell>2021</cell><cell>✓</cell><cell></cell><cell>Federated unlearning</cell><cell>[28]</cell></row><row><cell>Brophy &amp; Lowd</cell><cell>2021</cell><cell>✓</cell><cell></cell><cell>Random forests</cell><cell>[29]</cell></row><row><cell>Wu et al.</cell><cell>2022</cell><cell>✓</cell><cell></cell><cell>Federated unlearning</cell><cell>[30]</cell></row><row><cell>Guo et al.</cell><cell>2019</cell><cell></cell><cell>✓</cell><cell>Newton method</cell><cell>[21]</cell></row><row><cell>Du et al.</cell><cell>2019</cell><cell></cell><cell>✓</cell><cell>Exploding loss and catastrophic forgetting</cell><cell>[31]</cell></row><row><cell cols="2">Baumhauer et al. 2020</cell><cell></cell><cell>✓</cell><cell>Linear filtration</cell><cell>[32]</cell></row><row><cell>Golatkar et al.</cell><cell>2020</cell><cell></cell><cell>✓</cell><cell>Differential privacy</cell><cell>[33]</cell></row><row><cell>Wu et al.</cell><cell>2020</cell><cell></cell><cell>✓</cell><cell>Rapid retraining by storing training data</cell><cell>[34]</cell></row><row><cell>Graves et al.</cell><cell>2020</cell><cell></cell><cell>✓</cell><cell>Amnesiac unlearning</cell><cell>[6]</cell></row><row><cell>Golatkar et al.</cell><cell>2021</cell><cell></cell><cell>✓</cell><cell>Mixed-privacy setting</cell><cell>[35]</cell></row><row><cell>Izzo et al.</cell><cell>2021</cell><cell></cell><cell>✓</cell><cell>Influence method</cell><cell>[36]</cell></row><row><cell>Neel et al.</cell><cell>2021</cell><cell></cell><cell>✓</cell><cell>Gradient-based method</cell><cell>[37]</cell></row><row><cell>Thudi et al.</cell><cell>2021</cell><cell></cell><cell>✓</cell><cell>Verification unlearn-error</cell><cell>[38]</cell></row><row><cell>Warnecke et al.</cell><cell>2021</cell><cell></cell><cell>✓</cell><cell>Parameters updates</cell><cell>[39]</cell></row><row><cell>He et al.</cell><cell>2021</cell><cell></cell><cell>✓</cell><cell>Intermeidate models</cell><cell>[40]</cell></row><row><cell>Gong et al.</cell><cell>2021</cell><cell></cell><cell>✓</cell><cell>Particle-Based Bayesian Federated Unlearning</cell><cell>[41]</cell></row><row><cell>Guo et al.</cell><cell>2022</cell><cell></cell><cell>✓</cell><cell>Vertical unlearning</cell><cell>[42]</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research was partially supported by the <rs type="funder">Japan Science and Technology Agency (JST) Strategic International Collaborative Research Program (SICORP)</rs>.</p><p>The first author was supported by <rs type="funder">JST SPRING</rs>, Grant Number <rs type="grantNumber">JPMJSP2136</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_peCUQCX">
					<idno type="grant-number">JPMJSP2136</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conflict of Interest. On behalf of all the authors, the corresponding author states that there is no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mitigating poisoning attacks on machine learning models: A data provenance based approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Safavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09538</idno>
		<title level="m">Backdoor defense with machine unlearning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Machine unlearning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourtoule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Travers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="141" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Privacy-preserving machine learning: Threats and solutions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Rubaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security &amp; Privacy</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Towards efficient machine unlearning via incremental view maintenance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schelter</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10981</idno>
		<title level="m">Amnesiac machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">When machine unlearning jeopardizes privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2021 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="896" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deletion inference, reconstruction, and compliance in machine (un) learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahmoody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03460</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hard to forget: Poisoning attacks on certified machine unlearning</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Marchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alfeld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08266</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting poisoning attacks on machine learning in iot environments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Congress on Internet of Things (ICIOT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Chundawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Tarun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05629</idno>
		<title level="m">Zero-shot machine unlearning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The relationship between trust in ai and trustworthy machine learning technologies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Toreini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Coopamootoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Zelaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Moorsel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="272" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hacking machine learning: towards the comprehensive taxonomy of attacks against machine learning systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Surma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 the 4th International Conference on Innovation in Artificial Intelligence</title>
		<meeting>the 2020 the 4th International Conference on Innovation in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stealing machine learning models via prediction apis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th {USENIX} Security Symposium ({USENIX} Security 16)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="601" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Machine learning models that remember too much</title>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="587" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auror: Defending against poisoning attacks in collaborative deep learning systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tople</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Conference on Computer Security Applications</title>
		<meeting>the 32nd Annual Conference on Computer Security Applications</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="508" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Alsdurf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Belliveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Janda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jarvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kolody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krastev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08502</idno>
		<title level="m">Covi white paper</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Making ai forget you: Data deletion in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ginart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05012</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 22nd ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1322" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Certifiable machine unlearning for linear models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathioudakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15093</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Certified data removal from machine learning models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03030</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the necessity of auditable algorithmic definitions for machine unlearning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shumailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11891</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Machine unlearning via algorithmic stability</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4126" to="4142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards making systems forget with machine unlearning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Symposium on Security and Privacy</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="463" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient repair of polluted machine learning systems via causal unlearning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merwine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 on Asia Conference on Computer and Communications Security</title>
		<meeting>the 2018 on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="735" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A boosted svm classifier trained by incremental learning and decremental unlearning approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kashef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page">114154</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A unified pac-bayesian framework for machine unlearning via information risk minimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simeone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 31st International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Federaser: Enabling efficient client-level data removal from federated learning models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM 29th International Symposium on Quality of Service (IWQOS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Machine unlearning for random forests</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brophy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1092" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09441</idno>
		<title level="m">Federated unlearning with knowledge distillation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lifelong anomaly detection through unlearning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2019 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1283" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Baumhauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schöttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeppelzauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02730</idno>
		<title level="m">Machine unlearning: Linear filtration for logit-based classifiers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Eternal sunshine of the spotless net: Selective forgetting in deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9304" to="9312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deltagrad: Rapid retraining of machine learning models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dobriban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davidson</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10355" to="10366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mixedprivacy forgetting in deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Polito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="792" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Approximate data deletion from machine learning models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Izzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2008" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Descent-to-delete: Gradientbased methods for machine unlearning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharifi-Malvajerdi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="931" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unrolling sgd: Understanding factors influencing machine unlearning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13398</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Warnecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pirch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wressnegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11577</idno>
		<title level="m">Machine unlearning of features and labels</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deepobliviate: A powerful charm for erasing data residual memory in deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06209</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simeone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kassab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12056</idno>
		<title level="m">Forget-svgd: Particle-based bayesian federated unlearning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Vertical machine unlearning: Selectively removing sensitive information from latent feature space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13295</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Incremental and decremental support vector machine learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Incremental and decremental training for linear classification</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="343" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multiple incremental decremental learning of support vector machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1048" to="1059" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient noise-tolerant learning from statistical queries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="983" to="1006" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1193</idno>
		<title level="m">New insights and perspectives on the natural gradient method</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Privacy-preserving logistic regression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Forgetting outside the box: Scrubbing deep networks of information accessible from input-output observations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="383" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A swiss army infinitesimal jackknife</title>
		<author>
			<persName><forename type="first">R</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Broderick</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Diagnosing machine learning pipelines with fine-grained lineage</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing</title>
		<meeting>the 26th International Symposium on High-Performance Parallel and Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="143" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A roadmap for automating lineage tracing to aid automatically explaining machine learning predictions for clinical decision support</title>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">27778</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Managing data lineage of o&amp;g machine learning models: the sweet spot for shale use case</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F D S</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Bayser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cerqueira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First EAGE Digitalization Conference and Exhibition</title>
		<imprint>
			<publisher>European Association of Geoscientists &amp; Engineers</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Making recommender systems forget: Learning and unlearning for erasable recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11491</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Privacy risk in machine learning: Analyzing the connection to overfitting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yeom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Giacomelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 31st Computer Security Foundations Symposium (CSF)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="268" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Whitebox vs black-box: Bayes optimal strategies for membership inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5558" to="5567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Logan: Membership inference attacks against generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Danezis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Cristofaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on Privacy Enhancing Technologies (PoPETs)</title>
		<meeting>on Privacy Enhancing Technologies (PoPETs)</meeting>
		<imprint>
			<publisher>De Gruyter</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="133" to="152" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
