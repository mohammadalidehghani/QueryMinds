<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Financial Time Series Data Processing for Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-06">June 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Fabrice</forename><surname>Daniel</surname></persName>
							<email>fabrice.daniel@lusis.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Department of Lusis</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Financial Time Series Data Processing for Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06">June 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">52B1EA0890DB55F59445F6A2BFCB22EB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Financial Time Series</term>
					<term>Data Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article studies the financial time series data processing for machine learning. It introduces the most frequent scaling methods, then compares the resulting stationarity and preservation of useful information for trend forecasting. It proposes an empirical test based on the capability to learn simple data relationship with simple models. It also speaks about the data split method specific to time series, avoiding unwanted overfitting and proposes various labelling for classification and regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the field of machine learning Time Series are very special data needed their specific processing and methods <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>.</p><p>On top of that, Financial data adds a big challenge due to their proportion of randomness and their non-stationary nature <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b2">[3]</ref>.</p><p>There is a lot of research relative to the Financial Market forecast with Machine Learning <ref type="bibr" target="#b4">[5]</ref>.</p><p>However, many studies only cover one type of data scaling or labelling while the decisions made on this step can have a huge impact on the results. Not only in term of pure model performances metrics but in term of capabilities to really implement a profitable trading strategy based on the model.</p><p>This study covers the following points:</p><p>• Pre-processing and Stationarity</p><p>• Pre-processing and preservation of useful prices relationships</p><p>• Labelling for classifiers and regressors</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">STATIONARITY</head><p>Before to work on any price forecast model we need to preprocess our historical prices then we have to make sure the resulting data are stationary.</p><p>We evaluate three of the most frequent pre-processing, starting by the price returns, then two scaling methods: MinMax and Standardization.</p><p>For this purpose, we use the SPY daily closing prices between 1993 and 2019. Let's first apply an Augmented Dickey-Fuller test[6][7] to the raw data as reference.</p><p>For such a sample size, the ADF of a dataset with trend must be below -3.96 to reject the null hypothesis with a 1% confidence <ref type="bibr" target="#b7">[8]</ref>.</p><p>• ADF Statistic: -0.226901</p><p>• p-value: 0.991042 As expected with an ADF of -0.22 p-value of 0.99 the process is not stationary.</p><p>The most common way to make a time series stationary is differencing. In the case of financial data we can simply compute the returns. So, when applied on returns the ADF test gives: • ADF Statistic: -14.954070 • p-value: 0.000000 This confirms the time series of returns is stationary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SCALING</head><p>The returns data could be a good baseline, especially because the resulting very small values, near from 0 makes them directly compatible with a deep learning approach.</p><p>While this is very suitable for a single feature time series, if we use multiple features like the High/Low prices, the Volume or any technical indicator we can face an issue by simply using such a return-based method as it does not preserve the information of the position of a feature relative to each other.</p><p>In this situations some other scaling methods must be used like MinMax or Standardization. They are widely used in Machine Learning and enable to keep the relative position of each feature.</p><p>Before to evaluate these scaling methods, let's first introduce another aspect of data processing for time series in Machine Learning context, the slicing.</p><p>When doing Machine Learning on Financial Time Series, the model generally takes a time window as input, for instance 20 consecutive closing prices. The number of prices used for this time window is defined as the lookback period.</p><p>A frequent label found in many papers is the next price change, but we will see later this can be more sophisticated. So let's assume a time series of T consecutive stock returns {r 0 , . . . , r T -1 } Building a training set S consists on creating as series of K slices S = {S 0 , . . . , S K-1 }, each of size n &gt; 1, where S t = {r t-n , . . . , r t-1 }.</p><p>For a model predicting the next return the label is defined by y t-n = r t .</p><p>Each S t slice is created by incrementing t by steps of 1 or more. As an example, for an increment of 1, and a slice size of 20 returns, the first two sets are:</p><p>(S 0 = {r 0 , . . . , r 19 }, y 0 = r 20 ) (S 1 = {r 1 , . . . , r 20 }, y 1 = r 21 )</p><p>After the slicing was done, we have K slices than can be scaled independently from each other.</p><p>We expect our Machine Learning model to identify price patterns leading to up or down move. Scaling each slice independently can make the training easier by removing the global range effect due to the long-term market trend.</p><p>For instance, here are the first and last slices when we scale first with a minmax then slice (Figures <ref type="figure" target="#fig_2">3a</ref>, <ref type="figure" target="#fig_2">3b</ref>) and when we slice first then scale (Figures <ref type="figure" target="#fig_2">3c</ref>, <ref type="figure" target="#fig_2">3d</ref>). (a) First slice (b) Last slice (c) First slice (d) Last slice It clearly appears Figure 5 that slicing first then scaling enables getting each training example being within the same range, so making easier the model training. Now let's find the best scaling method in a more formal way.</p><p>For this purpose, we scale the training set then do the following:</p><p>• Check stationarity with ADF</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Check if the information is preserved for machine learning context by training a simple model identifying simple prices relationship into slices</head><p>We assume that if the model cannot learn a very simple price relationship inside each slice, like for instance identifying if the last close of the slice is above the close 5 bars ago, then there is nearly no chance for the model to be able to learn any price pattern leading to the future price changes.</p><p>If a simple price relationship is preserved, we expect the model to learn it nearly perfectly.</p><p>So, the scaling method we select is the one enabling a model to learn simple prices relationship inside a slice with the best efficiency.</p><p>We test two common scaling methods, the MinMax and the Standardization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MinMax</head><p>Scale each slice into a [0, 1] or [-1, 1] range Assume x min and x max the smallest and highest x values. And assume min and max the feature range, so [0, 1] for instance.</p><formula xml:id="formula_0">z = x -x min x max -x min (max -min) + min</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standardization</head><p>Scale each slice by removing the mean and scaling to unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>z =</head><p>x -µ σ</p><p>• µ : mean</p><p>• σ : standard deviation After scaling and reshaping the SPY prices we apply an ADF test. Scaling ADF p-value MinMax -41.72 0.000 Standardization -56.90 0.000 Table 1: ADF Test per scaling Each Scaling method results in a non-stationary dataset.</p><p>If prices relationship is preserved, we assume a simple model can be trained to identify conditions like: The third one is the most complex as the highest close over the past 10 bars is not always at the same position.</p><formula xml:id="formula_1">• C t &gt; C t-5 1 • C t &gt; EM A5 t</formula><p>We create a binary label for each of these conditions then train the following LSTM model. Using 2 units output with softmax instead of a single unit binary output with sigmoid enables making the models more generic when increasing the number of classes. (a) Ct &gt; Ct-5 minmax (b) Ct &gt; Ct-5 std (c) Ct &gt; EM A5t minmax (d) Ct &gt; EM A5t std (e) Ct &gt; HC10t minmax (f) Ct &gt; HC10t std</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lookback (Slices</head><p>The precisions on validation set are the following: The two first cases are learned with a nearly perfect generalization.</p><formula xml:id="formula_2">Scaling MinMax Standardization C t &gt; C t-</formula><p>The last case shows that validation loss is slightly not as good as the training but seems to be able to continue to improve if we increase the number of epochs.</p><p>By training the same LSTM on 150 epochs with 0.2 dropout and 0.2 recurrent dropout to improve the generalization, we get the following loss. (a) Ct &gt; HC10t minmax (b) Ct &gt; HC10t std The precisions on validation set are the following: Scaling MinMax Standardization C t &gt; HC10 t 0.970 0.968 Table 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>: HC10 Precision per scaling</head><p>These scaling methods preserve the prices relationship. It's also the case if we add new features like High, Low, or any overlaid indicator <ref type="foot" target="#foot_1">4</ref> . So, for instance the relative position of a closing price and a moving average is preserved.</p><p>The two scaling methods gives similar results but Standardization looks a little bit better , especially when looking at the validation loss shape in the last case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATA SPLIT</head><p>In machine learning, splitting data into a training, validation and test set is often performed by using the following process:</p><p>1. Shuffle the full data set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Split into training/validation/test sets</head><p>On time series this process leads to a risk of partially fitting to validation and test set during training so getting very good metrics while getting poor results when applying the resulting trading strategy to real-time data.</p><p>Remind the training set S = {S 0 , . . . , S K-1 } has K overlapped slices S t = {r t-n , . . . , r t-1 }.</p><p>Assume we have these two first slices:</p><formula xml:id="formula_3">(S 0 = {r 0 , . . . , r 19 }, y 0 = r 20 ) (S 1 = {r 1 , . . . , r 20 }, y 1 = r 21 )</formula><p>If we shuffle before to split, we can have S 0 in the training set while S 1 can be part of the validation set.</p><p>In this case S 0 and S 1 have {r 1 , . . . , r 19 }, so 95% of their data, in common.</p><p>If the shuffle is uniform, the biggest proportion of data in the validation set slices will also be part of the training set. In the example above for instance, up to 95% of the validation set data can be part of the training set slices.</p><p>So, depending on the model type, while fitting the training set, we also have a chance to partially fit the validation set. This can result in unreliable validation loss showing good results without overfitting while its real metrics on very new data taken independently can be poor with a strong overfitting.</p><p>The proper way to split and shuffle the training set for Financial Time Series is the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Split into training/validation/test sets</head><p>The split is performed over S such as:</p><formula xml:id="formula_4">train = {S 0 , . . . , S ts-1 } (1) val = {S ts , . . . , S ts+vs-1 } (2) test = {S ts+vs , . . . , S K-1 }<label>(3)</label></formula><p>Where:</p><p>• ts : training set size</p><p>• vs : validation set size</p><p>• K : total number of slices</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Shuffle the training set</head><p>So only train = {S 0 , . . . , S ts-1 } is shuffled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FEATURES</head><p>In Financial time series the raw features are the Open, High, Low, Close and the Volume<ref type="foot" target="#foot_2">foot_2</ref> .</p><p>To take a decision, traders often use technical indicators calculated from these features. These indicators can be used as additional features to help the model.</p><p>When used as input in a Neural Network model, these indicators must be scaled but the method to use differs depending on their nature. The overlaid indicators must be scaled but the separated indicators are generally moving in a narrow range of values, no matter if the price of the instrument is around 10 or 1000, so they need to be scaled separately with specific methods.</p><p>The overlaid indicators must be scaled all together with the prices to preserve their relationship. For instance this enable the model to use the relation between the closing price and one or several indicators like moving averages.</p><p>The Volume and the separated indicators must have their own independent scaling.</p><p>The not bounded indicators must be scaled with MinMax or Standardization.</p><p>The bounded indicators can benefit from being divided by their maximum value (100 for the RSI). This preserve fixed values that can make sense for traders, like the overbought/oversold levels. For instance, 70/30 for the RSI becomes 0.7/0.3 in the scaled version.</p><p>Note that when using another type of model like Random Forest or XGBoost, the scaling is not always required. Everything depends on the nature of the data. For instance the RSI is bounded between 0 and 100, so it can directly be used in a Random Forest without any scaling.</p><p>When using a single feature, like the closing prices, the input shape is simply defined by (m, s) When using multiple features, like the high, low, close and volume, the input shape becomes (m, s, i)</p><p>• m : samples When a model does not accept three dimensional inputs, a reshape could be necessary. Making the previous example flattened with a shape of (1000, 100)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LABELING</head><p>Labelling can have a major influence on a model training to Financial Time Series. Some labels can also results in non-tradable strategies. For instance, predicting a technical indicator created from lagged prices can be not usable for real trading, even when getting very accurate predictions.</p><p>Models can be of two type, classifier or regressor. A classifier generally attempts to predict the probability for the market to go Up or Down for a given time horizon while a regressor tries to predict the future price.</p><p>By far, the most frequent label found in the literature is the next return. However, many other labels can be used. So %Q is interpreted as following:</p><p>• %Q = 1 when we have a perfect up move without any drawdown during the next n bars</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This article covered the specificity of Financial Time Series data and the processing practice that can be applied to start a research project in good conditions.</p><p>It explains how to properly scale, slice and test the stationarity of the dataset.</p><p>It also speaks about features and proposed different labeling methods. Time Series are not like any other data. The classical pre-processing methods generally used cannot directly be applied.</p><p>On top of that, their Financial nature means they follow a stochastic process, which is even adding another level of complexity.</p><p>The methods shown in this document can be applied to any financial instrument (Equities, Commodities, Forex, ...) and to any timescales (daily, hourly, 5 minutes, ...).</p><p>Selecting features, scaling and labelling is part the whole Machine Learning Research process. Comparing different combinations of features and labels can sometimes have more impacts on the results than hyperparameters tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SPY daily closing prices</figDesc><graphic coords="1,324.31,411.15,224.38,150.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SPY daily returns</figDesc><graphic coords="2,62.07,126.48,224.37,143.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Compare scale then slice, with slice then scale</figDesc><graphic coords="2,441.48,372.92,99.72,64.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 1</head><label>2</label><figDesc>Ct: closing price at time t, C t-5 closing price 5 periods (days) ago 2 EMA5 : 5 periods Exponential Moving Average • C t &gt; HC10 t 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Loss of each model per scaling</figDesc><graphic coords="3,441.48,386.42,99.72,71.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: HC10 model on 150 Epochs with dropouts</figDesc><graphic coords="4,70.80,303.09,99.72,70.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>•</head><figDesc>s : timesteps • i : features If we build a training data set with 1000 slices of 20 bars, each with open, high, low, close and volume, the training set shape is : (1000, 20, 5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Model Detail and Hyperparameters</figDesc><table><row><cell>)</cell><cell>20 bars</cell></row><row><cell>Features</cell><cell>Close</cell></row><row><cell>1 LSTM</cell><cell>64 units (tanh)</cell></row><row><cell>1 Dense output</cell><cell>2 units (softmax)</cell></row><row><cell>Bias</cell><cell>No</cell></row><row><cell>Dropout</cell><cell>No</cell></row><row><cell>Recurrent Dropout</cell><cell>No</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Epochs</cell><cell>100</cell></row><row><cell>Batch Size</cell><cell>64</cell></row><row><cell>Training/Validation</cell><cell>80/20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Precision of each model per scaling</figDesc><table><row><cell>5</cell><cell>0.920</cell><cell>0.978</cell></row><row><cell>C t &gt; EM A5 t</cell><cell>0.993</cell><cell>0.998</cell></row><row><cell>C t &gt; HC10 t</cell><cell>0.910</cell><cell>0.904</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>shows some examples of the labels we often use in our research at Lusis.</figDesc><table><row><cell>Label</cell><cell>Description</cell></row><row><cell>N bars Up/Down</cell><cell>Classifier on C t+n &gt; C t</cell></row><row><cell>N bars price change</cell><cell>Regressor on C t+n -C t</cell></row><row><cell>N bars log returns</cell><cell>Regressor on log( Ct+n Ct )</cell></row><row><cell cols="2">N bars Moving Average Classifier on M A t+n &gt; M A t</cell></row><row><cell>N bars trend Strength</cell><cell>Regressor on Trend</cell></row><row><cell>N bars trend Direction</cell><cell>Classifier on Trend</cell></row><row><cell>%Q after N bars</cell><cell>Regressor on %Q</cell></row><row><cell>QClass after N bars</cell><cell>Classifier on QClass</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Labels often used in our research %Q6 is a specific metric we especially created for focusing on the most tradable models.IfC t is the closing price at time t, assume the corresponding time series slice of size m ending by C t is defined by S t = C t-m+1 , . . . , C t %Q t+n t+1 : %Q between t + 1 and t + n • HH t+n t+1 : Highest High price between t + 1 and t + n • LL t+n t+1 : Lowest Low price between t + 1 and t + n</figDesc><table><row><cell>Then</cell><cell></cell></row><row><cell>%Q t+n t+1 =</cell><cell>HH t+n t+1 -C t HH t+n t+1 -LL t+n t+1</cell></row><row><cell>Where</cell><cell></cell></row><row><cell cols="2">• n : time horizon in number of bars</cell></row><row><cell>•</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>HC10: Highest close of the 10 last bars</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>An indicator plotted to the same chart than the prices (e.g. moving averages, Bollinger Bands)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>As an OTC Market, the Forex does not includes the Volume. When present, it's only a synthetic indicator built from the number of ticks during the period. Many people use it as a proxy for the market activity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>Q stands for Quality</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• %Q = 0 when we have a perfect down move without any drawup during the next n bars</p><p>• %Q = 0.5 when we have an equally Up and Down move during the next n bars So the nearest from 1 or 0 %Q is, the more tradable the prediction is. It corresponds to conditions where the MAE 7 is lower than the MFE 8 between t and t + n.</p><p>%Q corresponds to a risk/reward ratio. For instance, %Q = .75 is a 1:3 risk/reward.</p><p>The QClass label derivates from %Q by using thresholds as class separators.     </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Brockwell</surname></persName>
		</author>
		<author>
			<persName><surname>Davis</surname></persName>
		</author>
		<title level="m">Introduction to Time Series and Forecasting</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Forecasting: Principles and Practice</title>
		<author>
			<persName><forename type="first">Rob</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Athanasopoulos</surname></persName>
		</author>
		<ptr target="https://otexts.com/fpp2/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Kjersti</forename><surname>Aas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Xeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dimakos</surname></persName>
		</author>
		<title level="m">Statistical modelling of financial time series: An introduction</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Ruey</surname></persName>
		</author>
		<author>
			<persName><surname>Tsay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Analysis of Financial Time Series</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>De Prado</surname></persName>
		</author>
		<title level="m">Advances in Financial Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Dickey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Fuller</surname></persName>
		</author>
		<title level="m">Distribution of the Estimators for Autoregressive Time Series with a Unit Root</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Fuller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>Introduction to Statistical Time Series</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Augmented_Dickey-Fuller_test" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
