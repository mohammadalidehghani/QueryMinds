<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Position Paper: Towards Transparent Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-11-12">12 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dustin</forename><forename type="middle">Juliano</forename><surname>Nov</surname></persName>
						</author>
						<title level="a" type="main">Position Paper: Towards Transparent Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-12">12 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">CACDA9C5CC30309BE8468523CBFC67ED</idno>
					<idno type="arXiv">arXiv:1911.06612v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current machine learning (ML) systems produce models that are difficult or impossible to understand. This poses clear challenges with security, safety, and bias in these deployments. Opaque models also make it difficult to gain insight into the automated decision-making process.</p><p>As a result of this, interpretable or explainable ML have become active areas of research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, with a notable interest from DARPA <ref type="bibr" target="#b2">[3]</ref>. However, those approaches are focused on analyzing models that are inherently resistant to human understanding due to the way in which they are represented.</p><p>Transparent machine learning (TML) is intended to solve these problems by producing models and data that we can understand. It would do this by representing and modifying source code representations. This, in turn, would result in a potentially self-contained executable that could be deployed directly.</p><p>In addition to the source code model, the TML system itself may be embedded into the output program. This would give it the ability to continuously update itself. Embedding, however, is not a requirement; the learner may be suppressed so that it is not emitted with the final program. This could be for safety purposes or to ensure stability of the deployment.</p><p>A complex program may require auxiliary information, such as labels, lookup tables, or a database of some kind. Transparent machine learning systems will need to produce such data to accurately represent the possible programs in its search space. While the data can and should be made part of the source code model, large quantities of information should be stored externally. This should be in a format that is appropriate to its size and intended usage.</p><p>It it crucial that TML systems target commonly known programming languages and data formats that can be easily understood. Further, the source code and data it produces in those languages and formats must be of sufficient clarity so that it can be easily understood and modified by an engineer of reasonable skill. This is a central tenant that should take priority over all other considerations, even at cost of model efficiency. Later on, suggestions will be given on how both efficiency and legibility might be accomplished, without permanently sacrificing either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Long-Term Objectives</head><p>The ultimate goal of TML is for it to target one or more dependently typed programming languages that feature support for rich specifications. This would necessarily entail concepts from the work being done on Deep Specification <ref type="bibr" target="#b3">[4]</ref>, which is an ambitious project<ref type="foot" target="#foot_0">foot_0</ref> that seeks to formally verify the entire development pipeline, end-to-end, from application to OS, right down to the hardware.</p><p>Then there are the long-term quality goals for the generated source code itself:</p><p>• Dense commenting • Use of high-level abstractions • Minimization of complexity • Use of accelerated hardware • Multiple language targets Fully solving these goals may require complete or partial general AI. This may put it into the classification of being a potentially AI-complete or AI-hard problem <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Short-Term Objectives</head><p>The immediate goals are to:</p><p>1. Create a working transparent machine learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Have that produce legible source code.</head><p>The immediate challenge is just getting a working proof-of-concept. An early strategy would be to find TML systems that equal or rival the best ML. The rationale for this is that it would increase research interest. Legibility, however, must eventually become the highest priority, otherwise it defeats the purpose of the project; incomprehensible source code is just another type of opaque model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>One practical benefit of TML is that we would unlock the capability to produce explicit AI implementations. This could potentially help solve one of the largest technical challenges of AI safety and security, known as the control problem <ref type="bibr" target="#b5">[6]</ref>.</p><p>Verifying that an opaque ML model adheres to its specification is an impressive research effort <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, but it is not equivalent to having human-readable source code. Transparent machine learning, in its most advanced form, would not only give us an explicit program, but it would be "correct by construction". This would be a result of it having been generated in programming languages with the most advanced type systems.</p><p>By studying the source code produced by TML we could also gain knowledge about how it implemented its solution. Instead of creating systems that just give predictions, we could explore the space of programs that provide these models of intelligence. This could lead to unique insights into both the problem domain it is addressing and our understanding of artificial intelligence itself.</p><p>Transparent machine learning can be seen as a form of automated programming. This could greatly accelerate software and hardware development. And, because TML must understand source code in order to modify it, this gives it the potential to audit the software and hardware that we have created. It could be used to find defects, improve efficiency, and even port projects to modern languages and platforms. Thus, it is not just something that will only help us going forward, but could be used to retroactively upgrade existing software.</p><p>Having explicit AI implementations will help us understand how to build better automation. It may even help us find new research directions to pursue general AI. As will be discussed ahead, these two research directions will eventually converge as the most significant challenges of TML are addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Challenges</head><p>The greatest initial challenge for TML will be in making it as effective and efficient as current ML systems. It may be the case that computing TML models will require significantly more resources. The author believes, however, that the resulting TML model, once found, will run significantly faster than traditional machine learning deployments. The rationale for this prediction is that TML source models will be capable of being natively compiled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Labeling Problem</head><p>After the initial proof-of-concept is realized there will be another problem, and it will likely hold sway over TML until general AI is discovered: the labeling problem. This is the challenge of how to create the most legible source code.</p><p>It is highly likely that the first TML models will be incomprehensible to us. An early example may end up looking something like the following:</p><p>// Begin prototypes double _EB29F654701535CC( double _0, double _1, double _2, double _3); double _A141F416696C035A( double _0, double _1, double _2, double _3, double _4, double _5, double _6, double _7, double _8); double _D1773B8053A78241( double _0, double _1, double _2, double _4, double _5, double _6, double _7);</p><p>That is clearly not a desirable final outcome, but it would be acceptable for an initial proof-of-concept.</p><p>It is important to remember, however, that the labeling problem is not just about the naming of identifiers. It is also about the patterns that are used to realize the source code model.</p><p>We need to move away from an abstract function space and into the logic and conventions of computer programs. This means the use of algorithms and data structures that are ubiquitous in software engineering.</p><p>As computer programs are extremely flexible with what they can entail, the challenge will be in confining program search to patterns that we would be likely to use if we knew what the TML system knew about the data.</p><p>The labeling problem also includes the issue of commenting. This implies that the concepts being used by the transparent learner will have to be represented in natural language.</p><p>There really is no upper-bound on the complexity of comments, as this is equivalent to the TML system describing its "thoughts" or intent in a way that may be missing from the source code itself. This capability puts this part of the labeling problem well into the range of problems only solvable by general AI. It would be sufficient, however, that early comments were simple enough to just help organize the structure of the source, and, perhaps, make references to particular features or subsets of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Program Search</head><p>Program search is fundamentally hard. The space of programs is what TML must operate over, and to do so it must intelligently iterate over the well-formed statements of the language generated by some formal grammar. For detailed information on formal grammars and languages see <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>How will search be conducted? Will it be directed or random? A combination of both? What is the measure of fitness? What are the parameters of the search? These are just some of the questions that will have to be answered. Should one use algorithmic complexity <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, Shannon entropy <ref type="bibr" target="#b15">[16]</ref>, minimum message length <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, or some other method?</p><p>An important early task for program search will likely be in reducing the search space itself. Opportunities may be found just by ensuring the generated source code is syntactically and semantically correct for the target language.</p><p>There is work related to this under the name of Universal Search <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, which uses iterative search algorithms that seek solutions to inversion problems by exhaustively enumerating program descriptions for a universal Turing machine.</p><p>Informally, for a given function φ and value y, an inversion problem is concerned with finding a value x such that φ(x) = y. More information on inversion problems can be found in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Program Generation</head><p>Super-optimization <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> will be of interest to researchers undertaking transparent machine learning. This is because TML may need to make many millions of program permutations in order to approach meaningful solutions, and it will likely need to be able to do that extremely quickly to be practical.</p><p>However, the problem is not merely one of speed. Reducing the number of times the learner has to iterate is of greater benefit than reducing the cost of generating the next permutation. While probably not realizable in practice, a perfect TML system would only require a single iteration. This allows us to think of the number of iterations required to reach a solution as a kind of cost function.</p><p>What is so interesting about super-optimization is that it has already been demonstrated to be effective. It proves a partial technical result towards fully realized TML without ever having been designed for this purpose. The challenge is in scaling it up and utilizing it on the entire source model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Interfacing</head><p>Most of the knowledge about the system calls and the various APIs that are used in programming are expressed in disparate collections of natural language. Worse, they are often of uneven quality.</p><p>How would the TML system know about these interfaces? It is not sufficient for it to simply generate correct syntax for the target programming language.</p><p>It must be able to entail the semantics of these programming interfaces.</p><p>The only plausible answer seems to be that we would have to formally specify the semantics and interdependencies between all of the APIs and system calls that the TML system could possibly rely upon.</p><p>That is extremely non-trivial, as it implies the use of dependently typed programming languages or specification languages specially suited to the task of interface-level interdependencies. Being dependently typed does not necessarily make a programming language suitable for productively writing complex specifications. There is early support being developed to entail complex state with dependent types in Idris <ref type="bibr" target="#b30">[31]</ref>, and it provides a clear case for just how difficult it is to make dependent types as productive as conventional programming language constructs.</p><p>Related to this challenge is the act of specifying a system so that it is internally consistent according to a certain design. This is different from the above problem of entailing the state and interdependencies between calls to a programming interface. Both are a form of specification, but the former has the challenge of having to interface with existing data and foreign functions.</p><p>It can take teams of individuals to verify a complex system. One prominent example is Project Everest <ref type="bibr" target="#b31">[32]</ref>, which seeks to create a formally verified dropin replacement for HTTPS. There have been others, such as CompCert <ref type="bibr" target="#b32">[33]</ref> and seL4 <ref type="bibr" target="#b33">[34]</ref>.</p><p>The purpose of canvasing these projects is to show how complicated and costly it is to do formal verification. The author strongly believes that the focus needs to be on making the tools easier to use. This would make verification more practical and open up the field to a wider audience.</p><p>Whether or not the details of specifying such complex rules and logic can be simplified is an open question. But it is one that must be investigated if we are to have ubiquitous, formally verified software and hardware.</p><p>The problem of interfacing for TML is also complicated by the need to target multiple languages. Data formats are often used to communicate complex information between many of these systems, some of which are far more involved than just defining a new data type. A good example of this challenge would be in how TML would generate compute shaders and the relevant data to perform general-purpose GPU computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Language Choice</head><p>Should the target language be functional or imperative? Object-oriented or procedural? These are important questions and there probably is no one right answer. However, there are two qualities that any TML target language should have: simplicity and efficiency. In terms of feature complexity and expressiveness, less may be more when it comes to the choice of target language.</p><p>It could be useful to target a total programming language, though it need not be functional, as with <ref type="bibr" target="#b34">[35]</ref>. While this would help ensure that every program terminates, it does not resolve the problem entirely. Consider a program that calls the Ackermann function <ref type="bibr" target="#b35">[36]</ref> with large positive integer values for each of its arguments. This would pass a termination check, but the running time of the program would have no practical end because of its asymptotic behavior.</p><p>This ties in with a counter to one of the misnomers of using a total language, which is that one may believe that it can not entail long-running processes, servers, simulations, or other such programs. The answer to this is that it is sufficient to have termination be contingent upon external input, time, or a certain number of steps. These are all more or less admissible for a flexible termination checking step. Like the above example, the question of whether a program terminates is generally interpreted in a syntactic and structural sense, and becomes a lot more involved when considering "external" effects.</p><p>Whatever paradigm is chosen, it is important to remember that legible source code is paramount. This does not equate with terseness. It is going to be a lot more helpful to have source code that some would consider to be verbose, especially in the early stages where the labeling problem remains unsolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Program Complexity</head><p>Another major concern is how to reduce program complexity. While related to program search in the previous subsection, it has several distinctions. Evaluation of fitness during program search does not necessarily correspond with source code legibility. And that is what separates it so cleanly from the research into iterative or sequential program search. While a shorter program is preferable in many cases, that has to be reconciled with the requirement for human understanding.</p><p>There are several dimensions to program complexity, and each of them will be discussed. They are listed below by order of preference:</p><p>1. The impact of program complexity on human understanding and program legibility.</p><p>2. The relationship between program complexity and model effectiveness, including fitness and length of descriptions in an information theoretic interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Program complexity in terms of runtime performance.</head><p>The space of models is quite vast in machine learning, and it is often the case that there are multiple models to choose from for a given set of training data. The question of effectiveness then has to be determined by a comparison with the other competing priorities of complexity. And it should be noted that, apart from the first rule of legibility, the other priorities should be taken not as a rigid ordering, but as a set of guidelines; it is the specific use case of each TML model that must ultimately determine the ordering of these priorities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Complexity and Legibility</head><p>To maximize our understanding, we should first think about how we develop high quality source code and then incorporate those ideas into the TML algorithm. This could begin with an analysis of the structure of programs, which should include function, block, statement, and expressions each as independent levels of organization. The use of multiple files or "translation units" is also a major consideration, especially with regard to how it resolves dependencies, both internally and externally, in the source model. A related question is whether the system should involve the use of incremental compilation and linking. It may be simpler for early TML systems to just focus on standalone programs with a single translation unit.</p><p>Will we allow the TML system to optimize its own style or should it be restricted? Care must be taken with this, as any and all limitations on the expression of programs by the TML system could significantly impact program search and model quality. This is especially difficult because of the competing priorities to reduce program complexity. It may require a lot of experimentation.</p><p>Worse yet is that these checks and balances may not be transferable between implementations; a set of priorities that works for one TML system may not be beneficial for another.</p><p>Another aspect to program legibility is the question of which algorithms and data structures we allow it to use to construct its model. Consider this as a set of programming patterns. Do we open up the search space to include exploration of new patterns or should we provide this set based on standard programming practice?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Complexity and Model Fitness</head><p>All things being equal, descriptive complexity should be minimized, but not at cost to legibility or runtime performance.</p><p>Operating over source code presents challenges for the process of model search if it performs that search in a representation that is different from the target languages of the TML model. While this additional layer of abstraction is temporarily admissible, it brings with it the additional requirement that the mapping it uses is provably invertible. This is a consequence of ensuring TML is verifiable.</p><p>It is tempting to think that the shortest program or subroutine is the best, but this will not always be the case. The target architecture in which the model will be deployed must be considered. Cache sizes, branch prediction, memory constraints, and other factors play a role in the balance between program complexity, size, and speed.</p><p>How, though, do we even approach the issue of balancing model fitness with the complexity of its description? And how can we find or create an algorithm that does this while giving us the ability to understand what it produces?</p><p>One might begin by thinking that the TML algorithm must find a model before it can explain it to us. But the way in which it constructs the model will necessarily curtail the space of models in which it conducts its search. And the optimization, measures, and metrics used during search may be antagonistic to the simplest and most easily understood programs. It has to know, in advance, what we consider to be legibile, and that has to be used in tandem with any possible notion of fitness for the model. They are inseparable.</p><p>So, the process of model search and program generation must be one in the same, or at least highly interdependent. One does not, and can not, come before the other. This is why a purely reductive approach using data compression, information theory, and algorthmic complexity theory could be extremely misleading to a researcher who is trying to design a TML algorithm. Care must be taken to keep these measures in mind, but not to be bound to them in a way that defeats their purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Complexity and Performance</head><p>One simple example of a major runtime impact is the calling conventions that are used in the target language. For example, the System V AMD64 ABI provides that the first six arguments are passed in registers <ref type="bibr" target="#b36">[37]</ref>.</p><p>This places a premium on the complexity of the function signatures themselves, tending towards six or less parameters on most consumer 64-bit hardware. On other target platforms, the calling conventions may be completely different, and this could have a dramatic effect on not only runtime performance, but the descriptive complexity of the model. This would be the result of changes to the structure of function signatures and the use of auxilary data structures to reduce register pressure, just as a human programmer would do if faced with similar constraints.</p><p>Earlier, in the Introduction section, the possibility to have both legibility and efficacy without sacrificing either was mentioned. How might that be achieved?</p><p>One way is that we utilize multiple modes. The default mode of the TML system would be to produce programs that are less efficient, but easier for us to understand. Then, once we understand the relevant parts of the source model, we could instruct it to conduct a (super-)optimization pass.</p><p>The TML would perform a translation between the original program and its more efficient and effective representation, which we may understand substantially less. Nothing would be lost, however, as it would be required to establish a rigorous mapping between the unoptimized version and the optimized one. This would be done by using the so-called de Bruijn criterion, which would see it emit a full trace of "proof objects" in a format that could be externally verified with a simple proof checker that we verify by hand and implicitly trust <ref type="bibr" target="#b37">[38]</ref>. This would, in effect, give us the best of both worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>How might transparent machine learning be implemented? The purpose of this paper is to begin an investigation to answer that very question. All that can be provided right now is a list of suggested directions we might take. It is expected that a great deal of research will be required to find even a modest implementation of TML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Early Stages</head><p>The first recommended direction would be to develop a flexible grammar engine based on term-rewriting with capture-avoiding substitution. This would need to be extremely efficient. A type system could be developed within it. Dependent types are not considered a requirement for an initial proof-of-concept.</p><p>It is not currently known whether or not it would be better to base such a system on the simply-typed Lambda calculus or if it is best to have the ability to arbitrarily specify formal grammars in a more open-ended fashion.</p><p>That is an open question likely to draw strong opinions from various researchers.</p><p>After the grammar engine, the next step might be to see how this could be used for program search and program generation. Of particular importance is the need to confine the search space by ensuring that only the correct forms for the language are generated. This sounds simple in theory, but it is confounded by context in practice. This is because there is a distinction between the syntax of a programming language and its semantics.</p><p>One possible aid to this may come from something called the Morphological Approach <ref type="bibr" target="#b38">[39]</ref>. This, however, is more of a way of thinking about the problem space than a specific engineering solution. The usefulness of that approach is in using it to find the internal consistencies of a particular grammar and the corresponding semantics of its programs. This would then be used to enhance the grammar engine so that it always remained correct with respect to the particular programming language it was generating.</p><p>A more conventional approach would see the use of operational semantics <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, but how that would be efficiently integrated with the proposed grammar engine is unclear.</p><p>After those steps, the greatest initial obstacle will be the labeling problem. This could be addressed early on with the use of labeled training data. Research into "semi-weak" supervised learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> may also be used to help with the process. Alternatively, there is the prospect of creating a website where labels for training data could be crowdsourced with help from the general public. Of course, that brings its own challenges, as it would have to be filtered for spam, checked for accuracy, and normalized against bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Research Directions</head><p>Below are some suggested research directions. While certainly not exhaustive, it should provide more than enough for experimentation and analysis in the early stages of developing transparent machine learning algorithms. Each subsection will include a number of linked references and will be listed, more or less, in ascending order of difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Intermediate Representations</head><p>This approach could be helpful in providing a way to manipulate and structure various programming languages and data formats. Regardless of which one is chosen, it is highly likely that some form of intermediate representation (IR) is going to be necessary to organize the structure of the source models for TML.</p><p>The s-expressions <ref type="bibr" target="#b45">[46]</ref> found in the Lisp programming language, and its many variants, are a significant potential candidate for an IR, and would have the added benefit of being directly accessible as part of an interactive programming environment. However, this should not discourage the use of other formats, as the ability to read and write interchange formats is ubiquitous in practice, and is supported in some capacity by most general-purpose programming languages.</p><p>Care, however, must be taken in the choice of the IR. There will be overhead in its translation and processing. And, because of the burden of proof requirements on translation, it will need to be representative of the target languages without loss of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Interpreted Languages</head><p>Early stage TML may be far simpler to explore with an interpreted language, especially if it has a homoiconic structure or metacircular <ref type="bibr" target="#b46">[47]</ref> capability. Lisp dialects come to mind, but the use of an interpreted language need not be restricted to those forms. The intent is to provide universal support for selfmodification, regardless of hardware or operating system support.</p><p>It may be useful to create a universal or open framework for the manipulation and representation of programming languages. The goal of such a project would be to allow programs in one language to analyze and transform the source code of itself or another program, regardless of the programming language.</p><p>The main drawback to using interpreted languages, compared to native compilation, is the penalty to runtime performance and memory utilization. While memory use could be reduced, the cost of interpretation is unavoidable. Anything other than the native instruction set is going to incur a penalty. One technique to mitigate this is the construction of threaded interpreters <ref type="bibr" target="#b47">[48]</ref>, which essentially use a computed form of goto when evaluating instructions. Both GCC and LLVM have extensions that support taking the address of a label, which can be used for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Bit-Level Languages</head><p>A bit-level language should not be confused with the notion of interpretable byte code, though they are highly related. The main difference is that bit-level languages are programming languages with an ultra-compact representation. They are typically used to study information theoretic and algorithmic complexity properties of program descriptions. Notable examples include binary representations for the lambda calculus <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>The parsimony of these languages could be of benefit to super-optimization and model search, but at cost of having to translate them into representations that we can understand. They are also arguably less efficient than equivalent byte code due to the need to unpack bits. On x86, for example, byte-aligned accesses for opcodes would allow directly indexing an instruction table for the purposes of implementing a threaded interpreter. To do this in a bit-level language the program would have to perform several operations before an array index or table offset could be calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Just-In-Time Compilation</head><p>The primary benefit of JIT compilation is that it combines the benefits of (self-)interpretation with efficiency. This would be especially useful if a TML system needed to perform real-time updates during deployment. Generally speaking, the faster a TML system runs, the more effective it will be at approaching optimal solutions. Speed, however, is not the only consideration. Work towards the acceleration of program search and program generation is ultimately going to be a losing battle.</p><p>While early efforts should be made to improve the operational efficiency of TML systems, we must not lose sight of the fact that it is the asymptotic behavior of these system that matters; for n iterations in program search, O(log n) is vastly superior to O(n) when n is large. And the (micro-)optimization afforded by faster interpreters and program execution only represents a reduction of k in O(n + k) for the same n.</p><p>It should be seen as a measure of "intelligence" that one TML system requires fewer n than another to obtain a source model. This would remain true even if the models it produced were less effective. Even more so if the reduction in iterations was significant. Such a TML system would need to be studied in isolation to ascertain how it was able to move through the program search space in such a way.</p><p>This also hints at the possibility of using TML to improve itself. This is related to the field of meta-optimization <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Metamorphic Code</head><p>Metamorphic code is a program that is capable of rewriting itself in a potentially different representation <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>This would make a TML system capable of recognizing, disassembling, and rewriting itself in pure machine code. The target language for the source code model becomes the instruction-set architecture of the CPU.</p><p>At first glance, it would seem that having such a low-level representation would defeat the purpose of legibility, but this is not necessarily the case. An IR could be used, with a provable inverse, that carried the additional information about the machine code so that the representations could be presented side-by-side, with no loss of fidelity.</p><p>A metamorphic program shares some overlap with JIT compilation in the sense that it combines runtime efficiency with the ability to update itself continuously. However, a distinction needs to be made between online and offline updates. An online update is what a JIT performs for a language with some form selfinterpreting capability, such as eval(). By contrast, an offline update is where a metamorphic program modifies its own executable in the file system. This is not an activity that a JIT would traditionally be expected to perform.</p><p>A standard program could be made to incorporate self-updates by loading and unloading shared libraries while it is running, but this is no where near the capability of a metamorphic program.</p><p>The most important consequence of a metamorphic engine is that it can manipulate other programs in machine code form. This capability, when combined with TML, would give it the ability to modify compiled programs, regardless of the language they were originally written in. And, if the labeling problem were in its late stages of being solved, it would prove to be a most effective disassembly and reverse engineering tool. In its full form, TML could be used to forcibly "open source" executables. This would have a significant impact on cybersecurity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.6">Simulated Annealing</head><p>Simulated annealing is a powerful metaheuristic for global optimization <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>. It may be especially applicable to the development of TML systems, as program generation is discrete and covers a large search space. Parallel simulated annealing, as discussed in <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67]</ref>, may be of special interest to accelerating TML program search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.7">Accelerated Processing</head><p>One of the greatest catalysts for modern ML is likely the fact that these algorithms are so readily parallelized. This gives them the opportunity to exploit the tremendous speedup from operating over general-purpose graphics processing (GPGPU) architectures. That is a very important property, as not all algorithms are created equally in this regard.</p><p>The question must be asked: Does this same advantage apply to transparent machine learning? Can the grammar engine at the core of TML, or something equivalent to it, be implemented in a compute shader? And, in general, can program search and program generation be done in a massively parallel fashion?</p><p>If L-systems are any indication, then the answer to this question might be in the affirmative, as demonstrated in <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>. An L-system, or Lindenmayer system, is a type of rewrite system that can be used to produce complex, fractal-like images and animations for computer graphics <ref type="bibr" target="#b69">[70]</ref>. They are naturally parallelizable, however, and may be insufficient in power to iterate over a general program space. Regardless, studying how these algorithms are used in graphics hardware could result in a transfer of knowledge to the TML domain.</p><p>If it turns out to be too difficult to write a grammar engine of sufficient complexity in shader form, then the next best option is to use a CPU-based method to target graphics accelerated hardware. There has been some valuable work done using rewrite systems in this way to generate OpenCL and compute shaders for GPGPU purposes <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref>. There is also PyCUDA and PyOpenCL <ref type="bibr" target="#b72">[73]</ref>. And of relevance may be automatic termination analysis for GPU kernels <ref type="bibr" target="#b73">[74]</ref>.</p><p>The problem with generating code for accelerated hardware is that it limits updates and throughput. It also significantly complicates verification. The ideal realization would be a "pure" GPU TML engine. In principle, such a system would run entirely within the graphics pipeline, and it would not require recompilation, except in the most extreme cases where the TML engine was being changed itself.</p><p>One approach to a pure GPU solution may be to find mappings between rewrite operations and data parallel instructions in the shader language.</p><p>Another approach, though highly speculative, would be to emulate a distributed virtual machine entirely on the GPU. The TML system would then be executed in that environment. It should be noted that this is distinct from the research on making GPU-accelerated hardware accessible to a virtual machine instance. By contrast, this approach would implement the TML system within the virtual instruction set being emulated by one or more shaders.</p><p>Even if the above methods succeed, it will still present challenges for systems level access and general input-output. In all likelihood, a hyrbid CPU-GPU approach will be required, and this will, unfortunately, complicate the source models generated by TML; it implies multiple target languages and data formats, along with the additional overhead that brings. Optimizing for both the CPU and GPU portion of a TML source model could be an extremely complex challenge. It may be far simpler in the early stages to create CPU-based TML models and then treat the GPU target as an optimization pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.8">Machine Consciousness</head><p>The largest obstacle to full transparent machine learning is going to be the labeling problem.</p><p>While various methods for labeling and classification are likely to be developed, the author believes that they will reach a hard limit without a fundamental advance in our understanding of artificial intelligence.</p><p>A basic component of that limit is related to something called the grounding problem, which is concerned with how the symbols in any mental or cognitive model could be given meaning <ref type="bibr" target="#b74">[75]</ref>. Instead of symbols, however, in the context of TML, we have the inhabitants, elements, or objects of one or more data types. And instead of a connectionist architecture, we are constrained to source code we can understand in one or more programming languages.</p><p>A full solution to the labeling problem must include an answer to the grounding problem. There will be many other challenges for legibility, but this is the most important first step towards a comprehensive solution.</p><p>One way of approaching this problem would be to mimic consciousness. In particular, the phenomenology of our subjective experience. This would mean creating one or more streams of information for sensory perception and then combining them into a unified whole. These could then be used to build episodic memories that can be referred to by the learning system, effectively grounding its representations in a form of artificial sentience.</p><p>It may be helpful to think of the primitives for this sensory data as fragments of experience, not unlike the texture fragments manipulated in graphics shaders.</p><p>In fact, that may be a most useful analogy for thinking about how such data would be combined and processed in a real implementation, especially given the distributed and parallel nature of our biology.</p><p>There is another aspect to this future direction that should be discussed as well. It is the importance of time, and how it is consistently overlooked in how we currently design, build, and think about AI.</p><p>The author strongly believes that any possible approach to resolving machine consciousness must be done with respect to time. It is not sufficient to merely involve agents in some perception-feedback loop. It has to be much more fundamental than that; the very meaning of the fragments of experience must co-vary with the relative timeframe in which they are interpreted.</p><p>While this may all seem philosophical, it has an important connection to any technical realization of machine consciousness: it demands the use of a hard realtime system. Whatever perceptual processing is done, it will need to be done under strict deadlines, otherwise it will change the interpretation and meaning of the experience. This is contrasted to a soft real-time system, where missing some deadlines is not considered to be a failure. See <ref type="bibr" target="#b75">[76]</ref> for more information on real-time computing.</p><p>The reason artificial sentience must be developed under hard real-time constraints is because its meaning co-varies with time. Consider an audio recording of someone saying "one, two, three". Imagine playing this back at one-quarter speed. While we could eventually make out what it is saying, and recover the number sequence, that sequence would not represent the totality of the information about the recording. The experience of listening to it comes first, and our interpretation of that experience is secondary.</p><p>All of this would make machine consciousness, and the transparent machine learning systems built over it, akin to a simulation. This approach would form the basis for a cognitive architecture, which opens up the possibility of building a more general-purpose framework for studying artificial intelligence as subjects of experience. And there is even more to discuss on this topic<ref type="foot" target="#foot_1">foot_1</ref> , but, unfortunately, it would be far beyond the scope of this article. This was presented only as a potential research direction towards solving the labeling problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Theoretical Limits</head><p>Will human understanding in TML scale as the overall complexity of its models increase? Is there a threshold of program complexity beyond which transparent ML necessarily degenerates to opaque ML?</p><p>A distinction must be made between program legibility and the ease in which it can be understood. This is not to contradict the standard test of reasonable competence set previously in the definition of TML. It is meant to facilitate discussion about the theoretical limitations of TML source models at the boundary of maximum model complexity and complete human understanding.</p><p>Like natural language, source code can entail complex logic, mathematics, algorithms, and patterns that one can recognize as correct statements of the language, but would otherwise require years of study to fully understand.</p><p>It is useful then to imagine a TML system that, while fulfilling its stated obligations, reaches a point where it starts to produce models that are so sophisticated that its burden of satisfying human legibility begins to weaken its expressive power.</p><p>To help discuss this, two complementary definitions from Yampolskiy <ref type="bibr" target="#b76">[77]</ref> on the foundations of AI will be considered:</p><p>1. Unexplainability: For certain decisions made by an intelligent system there will be no explanation that is both 100% accurate and comprehensible to humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Incomprehensibility:</head><p>Certain decisions made by an intelligent system will have a 100% accurate explanation for which no human can completely understand.</p><p>The first step in addressing these claims is that we must consider the distinction between opaque ML and transparent ML. In TML, the explanation is inseparable from the model, because it is the model. And that model may also include the description of the TML system itself.</p><p>A separate description that explains an opaque ML model may be of low accuracy and it will not affect the deployment of that model. This is not the case with TML, as the description and the model are not just equivalent to each other, they are the same object. So, any description of a TML model is of maximum accuracy by definition.</p><p>This simplifies at least one part of the analysis: there are no inaccurate descriptions of models while remaining within the framework of TML. Every TML system must produce models that are valid programs. This is because every TML model has a source code component that must be recognized by a compiler or interpreter for the target languages, which, by the TML requirements, must include at least a syntax and type check. The TML system then performs additional checks during program search, program generation, and runtime evaluation based on a balance of priorities for program complexity and model fitness.</p><p>While the author acknowledges the possibility of there being written documentation, scientific theories, or mathematical theorems resulting from knowledge gained from a TML model, these must be seen as supplemental. The definition given in (1) for unexplainability entails a spectrum of comprehensibility that co-varies with the accuracy of descriptions for machine learning models. As the description for a TML model is inseparable and exact by definition, this makes accuracy a constant factor, leaving only comprehensibility. Under such an interpretation, this effectively reduces claim (1) to claim (2), which should not be surprising, as they are complementary to begin with. Under TML they are simply equivalent. This is a necessary first step towards addressing incomprehensibility. The rest of the subsections will proceed based on this clarification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Legibility</head><p>In TML, program legibility is intended to be synonymous with human comprehensibility, which was given in terms of reasonable standards of skill.</p><p>The system has to take this legibility requirement into account when performing its search. This means that not only is a model's description or explanation inseparable from its construction, the effect of legibility on program complexity is necessarily a part of the search for the model. The model does not come first; legibility is not evaluated, but likely a fundamental part of the generative process. That is another way in which it is so distinct from opaque ML.</p><p>Legibility places a constraint on the space of all possible programs that TML can generate, limiting it to only those programs that necessarily satisfy userdefined legibility. That will exclude otherwise valid programs from the search space, and is highly relevant when interpreting claim (2) of incomprehensibility.</p><p>We will not get incomprehensible models if a TML system is operating correctly. This can be seen as ( <ref type="formula">2</ref>) placing a maximum upper-bound on TML model effectiveness in the limit of program legibility. This would be caused by a TML system not being able to adequately express programs because they do not combine in such a way as to satisfy expected results from the input training data.</p><p>It was suggested in the section on program complexity and model fitness that we utilize multiple representations, provided that we have a provable inverse. The problem with this is that it shifts the burden of legibility to finding highly expressive instruction sets and programming languages with inverses that produce models we can reverse engineer into legible data structures, algorithms, and programming patterns. While probably easier than interpreting deep neural networks <ref type="bibr" target="#b77">[78]</ref>, it would still be an oblique approach to the problem of legibility.</p><p>One might try to have a TML system optimize its ability to make legibile programs in an effort to strengthen its expressive power. That would be difficult, as TML relies upon a user-defined notion of what it must consider to be legible, which was previously referred to as a set of programming patterns. It has no general criterion for what any human would consider more or less preferable in terms of legibility, and this limitation would still apply even if general intelligence were to be involved. This can be observed in the fact that even different human programmers have widely varying standards program quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Groups</head><p>Consider the billions of lines of code in all of the actively maintained and publicly available free and open source software projects that currently exist. These are developed and maintained by one or more human programmers, some with the aid of automation in varying capacities. Each of these individuals have their specific talents, knowledge, and areas of expertise.</p><p>It is inconceivable that any one of us could ever fully understand all of these projects in totality. This is direct evidence for claim <ref type="bibr" target="#b1">(2)</ref> in the more general sense.</p><p>Even though the individual parts of this collective work are in fact understood by those working on it, the sheer volume of all these projects in combination would necessarily exceed human ability to follow.</p><p>That evidence, however, also contains a strong counter-claim. By adding one more human we have been able to scale with the growing complexity in software and hardware. We specialize and work in teams. Even if we are not directly communicating or collaborating, our combined efforts are a form of collective human understanding. This is not an implicit argument for emergence, but a statement of fact that we do overcome immense complexity through group effort.</p><p>These same arguments apply to the totality of human knowledge in general, but the specific case of source code projects was used because it exemplifies the kind of complexity we might anticipate from TML models of extraordinary sophistication.</p><p>Consider a TML model that has grown in complexity to the extent that it is in the hundreds of billions of lines of code. If each of the parts of the model satisfied the legibility requirements of the TML definition then there is no reason, in principle, that we could not scale to the challenge by adding one more human to the problem, and repeating that step until we have collectively understood it well enough.</p><p>Unfortunately, if it turns out to be the case that a full-spectrum general AI, or superintelligence, must necessarily involve a model of such size, then we may find ourselves unable to respond quickly enough to anticipate it, despite being able to comprehend its description through a concerted group effort.</p><p>In such a case, the author suggests that we construct a trusted sequence of artificial intelligence where each element has a strictly increasing ability to comprehend its successor better than its predecessor. This would still incur delays in prediction and analysis, but at possibly shorter timescales than that of a comparative group effort made by humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Data</head><p>It was anticipated that there might be a need for data to accompany the programs representing complex TML models. It is for this reason that the ability to target data formats and markup languages was explicitly included in the definition.</p><p>But how does that relate to incomprehensibility?</p><p>Consider a TML model that was equal or better than us at recognizing human faces. It is reasonable to expect that it would have both a data and source code component. This would mean that the TML system would need to target at least two distinct languages, with one of them being a data or markup language and the other being a programming language.</p><p>The data part of the TML model might be a listing of the numeric values for the symmetries and structures it had found for the faces in the training data. That might include composite models or topological information in its own unique encoding. The author argues that this would be analyzable by data scientists, even if they had to first find a more meaningful representation or data visualization technique.</p><p>On the source code side, the model would use that data in queries or lookups to simulate, project, or manipulate incoming image data through the use of one or more algorithms and data structures.</p><p>While the goal of TML is to produce legible programs, this does not necessarily apply to the content of the targeted data languages. In other words, while the data languages used must be legible to us, and their statements well-formed, this does not mean that the data will be anything other than just a direct representation of the information that is vital to the operation of the source model.</p><p>It is the design intent of TML that the data structures, algorithms, and programming patterns ultimately determine legibility, even though what is legible is user-defined. While data will play a role in the construction of the particular models, it should not be taken as a limit on model effectiveness, legibility, or complexity.</p><p>On the other hand, that design intent must not be used as an excuse to make space versus time trade-offs that reduce program complexity while sacrificing overall clarity. This could be taken as a qualification on the legibility condition. The purpose of including data language targets was to make a practical distinction between programs and data, even though they are both a form of information.</p><p>A TML system should minimize the data portion of its model while simultaneously maximizing program legibility, without sacrificing the other competing priorities on complexity and model fitness.</p><p>Following the example given, it is reasonable to accept that the geometric symmetries in millions of human faces might be represented best in a numeric interpretation, even though an algorithm could be presented that generates it with a much shorter description. The TML model that produces that data would, in fact, be one example of such an algorithm, for the simple fact of having generated it. That, however, does not mean that the TML model and its training data should be admitted as an explanation of human facial recognition.</p><p>What we seek in practice is a generalization of that training data in the form of a model, without the need for its specific examples again in the future. The data portion should be taken as the information portion of the TML model that could not be effectively represented in source code form, or was otherwise classified as the input to the source code portion of the model.</p><p>The TML approach makes an intentional distinction between data and source code for this purpose. It reduces the length of programs and enables alternative constructions to accelerate them.</p><p>It is the data side of TML that can be used to spare it from claim (2), which would permit the source code to remain legibile as overall model complexity increases.</p><p>A data set may very well be incomprehensible to us, but would not prevent us from analyzing, visualizing, and studying it. There is nothing, in principle, that prevents such data from being the useful input to algorithms we do understand. Additionally, the quantity of the data need not affect our understanding of the algorithms that use it, but it could add time to an analysis of the model as a form of knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Output</head><p>As TML models are just computer programs, the study of algorithms and computation directly apply to every model produced by any possible TML system.</p><p>The following analysis will focus on the output of these models by approaching it from the perspective of algorithms and computation. In this context, and unless noted otherwise, the word "model" should be considered as a TML model, which makes it synonymous with the word "program". Where it makes sense, these terms will be used interchangeably for the remainder of this section.</p><p>It is possible to understand the description of a program while not being able to easily predict its runtime behavior <ref type="bibr" target="#b78">[79]</ref>. Such runtime behavior can further be divided into the internal state of the program and its output, with the latter being most relevant to this analysis.</p><p>This brings up an important point about program legibility in the definition of TML. The goal of this approach is to produce models that we understand, and every model that is generated by TML should produce behavior that is useful, according to some user-defined criteria for model fitness. The definition of TML does not specify any requirement on the predictability or comprehensibility of the runtime behavior of these programs, and there are several reasons for this.</p><p>The comprehensibility of model output is not contingent upon our ability to predict that output in advance. Safety notwithstanding, we can still benefit from models we can not predict, just as we do with natural intelligence; we do not need to predict the wording of the next scientific paper for that result to be both comprehensible and useful to us.</p><p>It may even be the case that the unpredictability of model output is impossible to avoid <ref type="bibr" target="#b79">[80]</ref> or is an otherwise necessary condition for the most effective models. Instead of a limitation, however, the author argues that such a property of model output could be interpreted as novelty or creativity, which would be a highly desirable outcome. This should be possible, at least in principle, as TML has the ability to sample from the space of all possible programs. But how would that enable such output characteristics?</p><p>Consider a model with one or more nondeterministic algorithms <ref type="bibr" target="#b80">[81]</ref>, each of which are using external input in a stream of unbounded processing. Now give it sufficient memory and storage so that the program can refer to previous external inputs and runtime states. Such a model would have the potential to generate an unbounded amount of new information from an otherwise finite initial program description. And there is no reason to believe that TML would not be able to produce such models in theory.</p><p>So, if unpredictability of output is unavoidable for the most effective models, and even a desirable indicator of novelty, then that leaves only the comprehensibility of that output to consider.</p><p>Suppose it is possible for full-spectrum general intelligence or artificial superintelligence <ref type="bibr" target="#b5">[6]</ref> to be represented as programs. These kinds of AI would then form a subset of the set of all possible programs. Now devise a TML system that is instructed to include that subset within its search space. This would lead to one of the following outcomes:</p><p>1. These advanced models would not be found by any TML system, as the human legibility requirements would preclude it from expressing models at that level of sophistication.</p><p>2. It would find one or more models at that level of sophistication, and we would understand them, but they would produce output that we can not understand, even with a group effort.</p><p>The author believes that the second scenario is the most likely outcome. This represents a much worse result, however, as it strengthens the case for a different kind of incomprehensibility. We would, in principle, have the ability to understand some of the most sophisticated AI models possible, but the price would be that its outputs would be incomprehensible to us.</p><p>There are a couple of ways in which the output of such models might become incomprehensible. It could be at a level of abstraction or sophistication that is beyond human ability to follow. Even if we could eventually find one or more people to specialize in the knowledge it produced, it may take us too long, and, by then the model might have produced something even more complex. This ties in with the next concern, which is the rate at which these models could produce novel output. If done at scale, it could preclude human understanding even under a collective interpretation.</p><p>One can imagine humanity falling behind an exponential curve of incomprehensibility from such models of intelligence. Even if our entire population perfectly coordinated and specialized in the knowledge it was amassing, we might never hope to keep up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Closing</head><p>Transparent machine learning has been introduced as a possible alternative direction in machine learning. It would give us the ability to produce explicit AI that we can study, verify, and refine. This would transform the way we integrate automation with our technology by leveraging the existing hardware and software development processes that are commonplace today.</p><p>More importantly, TML answers the question of how we might control AI, both now and in the future. Having the source code to the implementation is a common sense prerequisite, and one that TML would provide. This would allow us to ensure that these systems behave exactly as we expect and enable us to program them to exacting specifications, in the most transparent and verifiable way possible. That includes codifying our ethical norms and laws within these systems. While TML does not address how we decide or translate our values into that form, it does ensure that it will be an explicit part of the implementation. And it will be present in a form that we can directly check.</p><p>The benefits of learning from TML source models can not be overstated. This could be of aid to an early form of automated science, where working theories are generated directly from data. Domain-specific languages could be used to translate source code to and from the language used by specific scientific disciplines. This may be one of the most direct routes to constructing automated research assistants until the discovery of general AI has been made. It only takes a willingness to see source code as a form of knowledge.</p><p>To balance the benefits of TML, a warning and disclaimer must also be given. As with any new technology, there is a significant potential for misuse of TML. By definition, TML has the ability to read and write source code. If combined with a metamorphic engine, this would also give it the ability to read and write any program it can access, without the need to execute it. This includes updatable hardware as well.</p><p>Metamorphic software with AI capability has to be treated as its own class of malware. The very same creative tools that will advance TML research can also be used to destroy. The upper limit on what this kind of malware can do should be treated as equivalent to what a human operator would be able to do with physical access to a compromised system. While that full threat potential is not realizable now, it may become reality later when TML becomes more sophisticated.</p><p>The suggested defensive action with TML is to utilize it to harden individual programs and networks. An API could be developed that exposes the network directly to the target language. The TML system could then be instructed to make program search and generation with that API a part of its model. This could, in principle, enable it to manipulate and explore that network through the use of the API, finding vulnerabilities and other flaws in security.</p><p>The true power of TML is that it can be used to search, iterate, and generate sequences over arbitrary formal grammars. And it could be employed for any problem domain that can be described in such a way. It is distinct from conventional ML because TML could do this with little or no training data whatsoever. This is because it exploits the formal structure of the grammar. Consider the case where that grammar entails genetic, chemical, or physical models. As long as a reasonable measure of fitness could be developed, then TML could be used to investigate the space of sequences for the respective target domain without it having to necessarily be a computer program.</p><p>Lastly, the pursuit of full TML will likely converge with research directions for general AI. These technologies would be complementary to each other, especially in terms of trust and security. The labeling problem will be the greatest challenge going forward, and it may require research into machine consciousness to resolve. That, in turn, could help unlock new ways of thinking about general AI, which could and should be considered the ultimate objective of TML related research.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>See also: deepspec.org.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>See also: "Machine Consciousness", AI Security (Juliano, 2016).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08296</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards better analysis of machine learning models: A visual analytics perspective</title>
		<author>
			<persName><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence (xai)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Gunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Defense Advanced Research Projects Agency (DARPA), nd Web</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Position paper: the science of deep specification</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Appel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="page">20160331</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AI-complete, AI-hard, or AI-easy-classification of problems in AI</title>
		<author>
			<persName><surname>Roman V Yampolskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 23rd Midwest Artificial Intelligence and Cognitive Science Conference</title>
		<meeting><address><addrLine>Cincinnati, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Nick</forename><surname>Bostrom</surname></persName>
		</author>
		<title level="m">Superintelligence: Paths, Dangers, Strategies</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Reprint ed</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Dual Approach to Scalable Verification of Deep Networks</title>
		<author>
			<persName><forename type="first">Krishnamurthy</forename><surname>Dvijotham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>UAI</publisher>
			<biblScope unit="page" from="550" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and effective robustness certification</title>
		<author>
			<persName><forename type="first">Gagandeep</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10802" to="10813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards fast computation of certified robustness for relu networks</title>
		<author>
			<persName><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09699</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards verified artificial intelligence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanjit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Seshia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S Shankar</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><surname>Sastry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08514</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to Languages and the Theory of Computation</title>
		<author>
			<persName><surname>John C Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>McGraw-Hill NY</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On certain formal properties of grammars</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and control</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="167" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Programming languages, natural languages, and mathematics</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Naur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="676" to="683" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Three approaches to the quantitative definition of information</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">N</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problems of information transmission</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the length of programs for computing finite binary sequences</title>
		<author>
			<persName><surname>Gregory J Chaitin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="547" to="569" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Elwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An information measure for classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><surname>Boulton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="185" to="194" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Minimum message length and Kolmogorov complexity</title>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Dowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="270" to="283" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness</title>
		<author>
			<persName><surname>David L Dowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of statistics</title>
		<imprint>
			<biblScope unit="page" from="901" to="982" />
			<date type="published" when="2011">2011</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Universal sequential search problems</title>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Anatolevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problemy peredachi informatsii</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="115" to="116" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimum sequential search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><surname>Solomonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memorandum, Oxbridge Research</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progress in incremental machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Solomonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Universal Learning Algorithms and Optimal Search</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Whistler, BC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gagliolo</surname></persName>
		</author>
		<idno type="DOI">10.4249/scholarpedia.2575</idno>
	</analytic>
	<monogr>
		<title level="m">revision #152144</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">2575</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Superoptimizer: a look at the smallest program</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Massalin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="122" to="126" />
			<date type="published" when="1987">1987</date>
			<publisher>IEEE Computer Society Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Eliminating branches using a superoptimizer and the GNU C compiler</title>
		<author>
			<persName><forename type="first">Granlund</forename><surname>Torbjo Rn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Kenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">cal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shade: A fast instruction-set simulator for execution profiling</title>
		<author>
			<persName><forename type="first">Bob</forename><surname>Cmelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Keppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fast simulation of computer architectures</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="5" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Denali: a goal-directed superoptimizer</title>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Randall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TOAST: Applying answer set programming to superoptimisation</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Logic Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="270" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic generation of peephole superoptimizers</title>
		<author>
			<persName><forename type="first">Sorav</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="394" to="403" />
			<date type="published" when="2006">2006</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic superoptimization</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Schkufza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="2013">2013</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">State Machines All The Way Down</title>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Brady</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Everest: Towards a verified, drop-in replacement of HTTPS</title>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Bhargavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>2nd Summit on Advances in Programming Languages</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The CompCert verified compiler</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Leroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Documentation and user&apos;s manual</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">seL4: Formal verification of an OS kernel</title>
		<author>
			<persName><forename type="first">Gerwin</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles</title>
		<meeting>the ACM SIGOPS 22nd symposium on Operating systems principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="207" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Total Functional Programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. UCS</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="751" to="768" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Ackermann function. a theoretical, computational, and formula manipulative study</title>
		<author>
			<persName><forename type="first">Yngve</forename><surname>Sundblad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="119" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">System V application binary interface</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">K1OM Architecture Processor Supplement</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">0</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Proof assistants: History, ideas and future</title>
		<author>
			<persName><forename type="first">Herman</forename><surname>Geuvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sadhana</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="25" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The morphological approach to discovery, invention, research and construction</title>
		<author>
			<persName><forename type="first">Fritz</forename><surname>Zwicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New methods of thought and procedure</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A structural approach to operational semantics</title>
		<author>
			<persName><surname>Gordon D Plotkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Abstract interpretation of small-step semantics</title>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LOMAPS workshop on Analysis and Verification of Multiple-Agent Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="76" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The origins of structural operational semantics</title>
		<author>
			<persName><surname>Gordon D Plotkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Logic and Algebraic Programming</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coinductive big-step operational semantics</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Grall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Computation</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page" from="284" to="304" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large-scale weaklysupervised pre-training for video action recognition</title>
		<author>
			<persName><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12046" to="12055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName><forename type="first">Yalniz</forename><surname>Zeki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Recursive functions of symbolic expressions and their computation by machine</title>
		<author>
			<persName><forename type="first">John</forename><surname>Mccarthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Definitional interpreters for higher-order programming languages</title>
		<author>
			<persName><forename type="first">Reynolds</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM annual conference</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="717" to="740" />
			<date type="published" when="1972">1972</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Threaded code</title>
		<author>
			<persName><forename type="first">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="370" to="372" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Binary lambda calculus and combinatory logic</title>
		<author>
			<persName><forename type="first">John</forename><surname>Tromp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Randomness and Complexity, from Leibniz to Chaitin. World Scientific</title>
		<imprint>
			<biblScope unit="page" from="237" to="260" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Counting and generating terms in the binary lambda calculus</title>
		<author>
			<persName><forename type="first">Katarzyna</forename><surname>Grygiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lescanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Programming</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Meta optimization: improving compiler heuristics with machine learning</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Stephenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="77" to="90" />
			<date type="published" when="2003">2003</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Meta-optimization for parameter tuning with a flexible computing budget</title>
		<author>
			<persName><forename type="first">Juergen</forename><surname>Branke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jawad</forename><surname>Asem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elomari</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th annual conference on Genetic and evolutionary computation</title>
		<meeting>the 14th annual conference on Genetic and evolutionary computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1245" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Parameter meta-optimization of metaheuristic optimization algorithms</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Neumüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Systems Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Meta-optimization based on selforganizing map and genetic algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Karpenko</surname></persName>
		</author>
		<author>
			<persName><surname>Svianadze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Memory and Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="279" to="283" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Performance index and metaoptimization of a direct search optimization method</title>
		<author>
			<persName><forename type="first">Petter</forename><surname>Krus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Ölvander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering optimization</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1167" to="1185" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hunting for metamorphic</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ferrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Virus bulletin conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Prague</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mechanisms of polymorphic and metamorphic viruses</title>
		<author>
			<persName><forename type="first">Xufang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter Kk</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freddy</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 European intelligence and security informatics conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="149" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Detection of metamorphic viruses: A survey</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bist</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1559" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel Gelatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Černy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of optimization theory and applications</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="41" to="51" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stochastic versus deterministic update in simulated annealing</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Moscato</surname></persName>
		</author>
		<author>
			<persName><surname>Fontanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="204" to="208" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Simulated annealing: A proof of convergence</title>
		<author>
			<persName><forename type="first">Mirko</forename><surname>Vincent Granville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-P</forename><surname>Krivánek</surname></persName>
		</author>
		<author>
			<persName><surname>Rasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="652" to="656" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Simulated annealing and Boltzmann machines</title>
		<author>
			<persName><forename type="first">Emile</forename><surname>Aarts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Korst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Parallel simulated annealing techniques</title>
		<author>
			<persName><surname>Daniel R Greening</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="293" to="306" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Parallel simulated annealing using speculative computation</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">E</forename><surname>Witte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">D</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel &amp; Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="483" to="494" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Parallel search for combinatorial optimization: Genetic algorithms, simulated annealing, tabu search and GRASP</title>
		<author>
			<persName><surname>Panos M Pardalos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Parallel Algorithms for Irregularly Structured Problems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="317" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Parallel simulated annealing algorithms</title>
		<author>
			<persName><forename type="first">Ram</forename><surname>Janaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sreenivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subramaniam</forename><surname>Ganapathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of parallel and distributed computing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="207" to="212" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Generating subdivision curves with L-systems on a GPU</title>
		<author>
			<persName><forename type="first">Radomir</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemyslaw</forename><surname>Prusinkiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>SIGGRAPH</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Parallel generation of multiple L-systems</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="585" to="593" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Interactive evolution of L-system grammars for computer graphics modelling</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Mccormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complex Systems: from biology to computation</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Generating performance portable code using rewrite rules: from high-level functional expressions to high-performance OpenCL code</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Steuwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="205" to="217" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Matrix multiplication beyond auto-tuning: rewrite-based GPU code generation</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Steuwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toomas</forename><surname>Remmelg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Dubach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Compilers, Architectures and Synthesis for Embedded Systems</title>
		<meeting>the International Conference on Compilers, Architectures and Synthesis for Embedded Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">PyCUDA and PyOpenCL: A scripting-based approach to GPU run-time code generation</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Klöckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="157" to="174" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Automatic termination analysis for GPU kernels</title>
		<author>
			<persName><forename type="first">Jeroen</forename><surname>Ketema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><forename type="middle">F</forename><surname>Donaldson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Fuhs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Termination</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The symbol grounding problem</title>
		<author>
			<persName><forename type="first">Stevan</forename><surname>Harnad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Real-time computing: A new discipline of computer science and engineering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parameswaran</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><surname>Ramanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="6" to="24" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Unexplainability and Incomprehensibility of Artificial Intelligence</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Yampolskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Methods for interpreting and understanding deep neural networks</title>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unpredictability and computational irreducibility</title>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Zwirn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Paul</forename><surname>Delahaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Irreducibility and Computational Equivalence</title>
		<imprint>
			<biblScope unit="page" from="273" to="295" />
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Unpredictability of AI</title>
		<author>
			<persName><surname>Roman V Yampolskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Nondeterministic algorithms</title>
		<author>
			<persName><forename type="first">Floyd</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="636" to="644" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
