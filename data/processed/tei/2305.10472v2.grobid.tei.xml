<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nine tips for ecologists using machine learning</title>
				<funder ref="#_XwYSQXb">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-05-26">26 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marine</forename><surname>Desprez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CEFE</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">EPHE, IRD</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Vincent</forename><surname>Miele</surname></persName>
							<email>vincent.miele@univ-lyon1.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<postCode>F-69000</postCode>
									<settlement>Lyon</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Gimenez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CEFE</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">EPHE, IRD</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">UMR5558</orgName>
								<orgName type="laboratory" key="lab2">Laboratoire de Biométrie et Biologie Évolutive</orgName>
								<orgName type="institution" key="instit1">Université Lyon 1</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-69622</postCode>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Nine tips for ecologists using machine learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-26">26 May 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">1D912832A7365A929883981AF34E87A4</idno>
					<idno type="arXiv">arXiv:2305.10472v2[q-bio.PE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to their high predictive performance and flexibility, machine learning models are an appropriate and efficient tool for ecologists. However, implementing a machine learning model is not yet a trivial task and may seem intimidating to ecologists with no previous experience in this area. Here we provide a series of tips to help ecologists in implementing machine learning models. We focus on classification problems as many ecological studies aim to assign data into predefined classes such as ecological states or biological entities. Each of the nine tips identifies a common error, trap or challenge in developing machine learning models and provides recommendations to facilitate their use in ecological studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Ecological datasets are generally characterised by complex interactions between variables, nonlinearity, missing values, dependence in the observations and/or a continuously expanding size <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>, especially since the recent increase in the use of remote sensing and automatic recorders <ref type="bibr" target="#b3">[4]</ref>. A growing number of those datasets cannot be effectively processed by humans anymore and require methods that can deal with high number of variables and complex data structures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Because of their ability to process large and complicated datasets, machine learning models are expected to become a standard framework in the analysis of ecological data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Over the last few years, machine learning algorithms have become increasingly popular due to their high performance and flexibility <ref type="bibr" target="#b7">[8]</ref>. In ecology, they have been successfully applied to perform various tasks such as identifying species from images or sounds <ref type="bibr" target="#b8">[9]</ref>, monitoring animal behaviour <ref type="bibr" target="#b9">[10]</ref> or modelling species distribution <ref type="bibr" target="#b10">[11]</ref> and new innovative studies and perspectives keep being regularly documented <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>However, implementing a machine learning model is not yet a trivial task and may seem intimidating to ecologists with no previous experience in this area. In this paper, we aim to share nine tips to help ecologists avoid some of the most common errors and incorrect practices in machine learning. We focused our tips on classification problems as a substantial number of ecological studies aim to assign data into predefined classes such as ecological states or biological entities. Some typical examples of classification include species identification through pictures <ref type="bibr" target="#b8">[9]</ref> or sound recordings <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>, distinction of different phenological phases in plant life cycle <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, description of animal behaviour <ref type="bibr" target="#b17">[18]</ref> and detection of disease in plants <ref type="bibr" target="#b18">[19]</ref>. Each tip presented in this paper identifies a common error or challenge in developing machine learning models and provides recommendations to facilitate the use of machine learning methods in ecological studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tip 1: Adopt the machine learning mindset</head><p>The concept behind machine learning refers to the use of a certain type of model that can discover and learn patterns in data to generate predictions or detect patterns automatically without having to follow explicit instructions. Without machine learning, humans have to provide data and instructions; with machine learning, humans have to provide data.</p><p>The learning phase can happen in two different ways, with or without supervision (see <ref type="bibr" target="#b7">[8]</ref> for an introductory review). In unsupervised learning, the model automatically discovers patterns and similarities in unlabelled data (e.g. data for which we do not have a label indicating the associated class, given by the user). Unsupervised learning is often used in data exploration to find the underlying structure of the dataset, reduce its dimensions or cluster/group similar data together. The state-of-the-art methods include PCA, k-means and hierarchical clustering, or the more recent methods t-SNE <ref type="bibr" target="#b19">[20]</ref> and UMAP <ref type="bibr" target="#b20">[21]</ref> that are particularily popular in this context. The latter was for instance used to compare soundscapes from a variety of ecosystems <ref type="bibr" target="#b21">[22]</ref>. In supervised learning, a labelled dataset is initially provided to the model: those labelled data include input variables and output variables (e.g. class labels). These data play the role of a supervisor that teaches the model how to correctly predict the output by finding a function that maps the explanatory input variables with the output. Depending on whether the output is a discrete category or a quantity, the problem is called a classification or a regression problem respectively. Random forests, XGBoost and neural networks are leading options in this framework. In ecology, a classical example of a supervised learning task is the classification of individuals in different categories based on a set of explanatory variables. This problem can be solved using a wide range of models from logistic models to complex deep learning models.</p><p>The machine learning mindset can be presented by revisiting a classification problem (see a concrete example in <ref type="bibr">Box 1)</ref>. Before being able to predict to which category a new individual belongs, the model (logistic regression in Box 1) has to enter a learning phase that consists in finding the optimal parameters describing the relationship between the variables and the labelled output. How does the model find those optimal parameters? By minimizing a loss function. This function (e.g. mean squared error function), the keystone of a supervised learning approach, evaluates how far from the correct answers/outputs are the model predictions. The learning phase consists in minimizing this loss function numerically, such that the optimal parameters (i.e., those that lead to the lowest predictive error) are the ones chosen for the final model. This final model can then be used to predict the labels of a new set of individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tip 2: Create your data sets (very) carefully</head><p>Here, we focus on classification tasks but the following principles are also applicable to regression problems. The general approach to solve a classification problem in machine learning is to: (i) develop different versions of a method of classification (a classifier) and train them on a dataset; (ii) evaluate and compare the models' predictive performance using an evaluation metric (see Tip <ref type="formula">6</ref>) and (iii) select the best performing model to carry out the final predictions on a sample of new unseen dataset (Fig. <ref type="figure" target="#fig_0">1</ref>). Before entering the learning process, the collected data should be composed of three separate sets: the training set, the validation set and the test set. This partitioning is necessary because the data used to train and evaluate the model need to remain independent in order to obtain reliable performance measure <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. The three separate data sets. Training. Only the training data set is used to train the model (Fig. <ref type="figure" target="#fig_0">1</ref>). Ideally, this training set should include a various set of inputs to train the model under as many situations as possible in order to predict any unseen data sample that may appear in the future. Validation. The trained model is simultaneously used to predict the classes from the observations in the separate validation set to evaluate the model predictive performance (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Evaluating the model on a separate validation set prevents the model from overfitting, i.e., when the model memorizes the pattern in the training data to such an extent that it fails to generalize and to make accurate predictions on unseen data <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. As an example, imagine a model developed to detect the presence of dogs in pictures. The model performance obtained on the training set is high, indicating the model is doing a good job classifying the pictures with and without dogs. However, when tested on the separate validation set, its performance drops. This indicates that the model was overfitting; it memorized specific patterns of the dogs from the training set instead of learning general patterns common to dogs. This issue would not have been detected if the model was evaluated on the same data it was trained on. During the training (or tuning) phase, different models or set of variables may be tested and hyperparameters optimised (e.g., the number of trees in a random forest or the number of layers in a neural network). The simultaneous validation phases provide information about how those different model configurations affect the model predictive performance. Training and validation phases are repeated until a desirable predictive performance is reached on the validation set. Test. In a final step, the best performing model is run on the test set to obtain an unbiased measure of the model predictive performance (Fig. <ref type="figure" target="#fig_0">1</ref>). It is critical that the test set (sometimes called out-of-sample, <ref type="bibr" target="#b26">[27]</ref>) remains out of the development phase until the final predictions are made for the performance measure to be reliable <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>The good, the bad and the ugly data sets ? The validation set is used to confirm the training process is correct, and that the model choice is satisfying. The main issue with this validation set is to avoid data leakage (see Tip 4) that can lead to over-optimistic model evaluation. However, it should not be used to claim that the trained model will perform well when deployed in real scenarios. This is the role of the test dataset. Test sets must be representative of the target data, i.e. data in real life for which the model was built. The key recommendation to obtain a reliable measure of model performance, is to keep the test data set as independent as possible. For example, using camera trap data from different locations independently, or continuous sections of longitudinal data from dates beyond the end of the training set <ref type="bibr" target="#b1">[2]</ref>. A random splitting of dataset into train/validation/test sets is therefore strongly discouraged.</p><p>If the training data is an unbiased sample of the underlying distribution, then the learned classification function will generalize well and will make accurate predictions for new samples <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>If not, the distribution of the target data may differ from the distribution of the training data and the classification function will perform poorly <ref type="bibr" target="#b29">[30]</ref>. This issue, called distribution or domain shift <ref type="bibr" target="#b30">[31]</ref>, is a common cause of a well-known scenario in machine learning: a seemingly impressive model (as evaluated on validation data) that completely fails when used on a test set of new data <ref type="bibr" target="#b7">[8]</ref>. In species identification for example, a model trained on pictures with clean weather conditions will likely fail on a test set of pictures with adverse weather conditions (e.g., rain, fog, snow <ref type="bibr" target="#b30">[31]</ref>). The solution often consists in enlarging the training set with new data covering the complete distribution expected for the data in real conditions.</p><p>Choosing appropriate validation and test sets is one of the most important step in a machine learning project. We cannot stress this enough. A poor choice of validation and test sets will lead to a disconnect between the results in development and the ones obtained when deploying the model on new data <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tip 3: Get the right amount of data</head><p>The amount of data needed to capture the relationship between the explanatory variables and the output data varies depending on the complexity of the problem and model. While there is no general rule for determining the quantity of data required in machine learning models, three concepts should be kept in mind when gathering data for the training, validation and test steps.</p><p>First, machine learning models, especially deep learning models <ref type="bibr" target="#b31">[32]</ref>, often need a significant amount of training data. This is because the number of parameters in those models can be tremendous (tens of millions for most convolutional neural networks (CNN)). Therefore, complex models will need significantly more data than simpler ones (e.g. thousands or millions of pictures to train a model for species identification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref>). Second, a model can only capture what it is trained on. For instance, a model trained on daytime images will not work on night images; a species distribution model trained on mountains will not work on wetlands. Therefore, the training set should include as much diversity/variability and edge cases as possible to enable the model to learn and predict various scenarios. Note that a large dataset does not necessarily include sufficient variability in the data to guarantee good model performance <ref type="bibr" target="#b33">[34]</ref>. For instance, a dataset might contain millions of pictures of a given species, but only in sunny conditions. Using this dataset to identify that species during rainy or snowy days would likely produce poor predictions (see <ref type="bibr">Tip 7)</ref>. Therefore, we recommend to pay a greater attention to the variability/diversity available, not to only focus on the raw amount of data. Third, if there is not enough data to build correct validation and test sets, the model evaluation metric will have a greater variance which will (i) prevent a proper tuning of the model and (ii) make it hard to assess how well the model will perform on new data and generalize.</p><p>If more data is needed, we suggest to make use of existing methods to increase the sample size. This includes data augmentation techniques to generate heterogeneous data from existing training data <ref type="bibr" target="#b34">[35]</ref>, crowd sourcing to maximize data sources <ref type="bibr" target="#b35">[36]</ref> or creation of large consortium to gather more data <ref type="bibr" target="#b32">[33]</ref>). It can also consist in going back to the field to collect more labelled data (when possible).</p><p>In deep learning specifically, an alternative approach is to limit the amount of training data required by (re-)using existing pre-trained models as a starting point for new models using a transfer learning approach <ref type="bibr" target="#b36">[37]</ref>. Recently developed self-supervised learning methods <ref type="bibr" target="#b37">[38]</ref> are also another option to solve the challenges posed by the needs of large labelled data. They consists in learning from the similarity between close images (e.g. two sub-parts of the same image, two successive camera trap images <ref type="bibr" target="#b37">[38]</ref>) and can cope with the limited availability of some categories (e.g. rare species).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tip 4: Be mindful of data leakage</head><p>Data leakage is one of the leading machine learning errors ( <ref type="bibr" target="#b28">[29]</ref>). It happens when a model is trained using data that contain some information that would not be available at the time of predictions <ref type="bibr" target="#b38">[39]</ref>. It is a serious problem as it can create overly optimistic, if not completely invalid, predictive models with very poor generalization.</p><p>In practice, data leakage often occurs subtly and inadvertently, making it hard to detect and eliminate <ref type="bibr" target="#b38">[39]</ref>. A common example of data leakage is when the pre-processing of data is done on the whole combination of training, validation and test sets. Knowledge of the full data distribution is included in the processed data and used by the model (see Fig. <ref type="figure" target="#fig_2">2</ref> for an example on standardisation). A non-leaky way of processing the data would be to create the training, validation and test sets first and process the data within each set. However, it is important to keep in mind that only parameters computed from the training set can be used for transforming the data in the validation and test sets (Fig. <ref type="figure" target="#fig_2">2</ref>). In time series data, a temporal cutoff may be useful in preventing leaking any information about the future, to ensure that any data used for training does not include records with a timestamp later than the cutoff value.</p><p>X X train 1 X train 2 ...</p><formula xml:id="formula_0">X train i X val 1 ... X val i X test 1 ... X test i X' X train' 1 X train' 2 ... X train' i X val' 1 ... X val' i X test' 1 ... X</formula><p>test' i Compute mean and standard deviation from all data Standardisation ... X test' i Compute mean and standard deviation from training set Standardisation Standardisation with data leakage Standardisation with no data leakage Training Val Test Training Val Test Training Training Val Val Test Test Another common error leading to leakage is data duplication, when the dataset contains identical or near identical data ( <ref type="bibr" target="#b28">[29]</ref>). For example, when working on sequences of pictures from camera traps, duplicates may correspond to images from the same temporal sequence (Figure <ref type="figure" target="#fig_3">3</ref>). In this case, data leakage may happen because the training and validation sets contain the same information even though they correspond to different observations, i.e., different pictures from the same temporal sequence. As a general rule, if the model is "too good to be true" (e.g. with over-excellent evaluation metrics), we should get suspicious and check for potential data leakage. Again, we advise using a holdout dataset (i.e., test set) as a final sanity check for model performance and generality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tip 5: Treat imbalanced datasets with care</head><p>In ecological data, it is common for one or several classes to be more frequent than other(s) (e.g. sporadic distribution vegetation types, apex predators in camera trap images). Imbalance distribution in data can lead to poor predictive performance in minority classes as conventional classification models tend to be biased towards majority classes (e.g. in deep learning <ref type="bibr" target="#b39">[40]</ref>). In those cases, it is challenging for models to learn the characteristics of the observations from the minority class and to differentiate those observations from the others <ref type="bibr" target="#b40">[41]</ref>. Indeed, by construction, the rarest classes are under-represented in the loss function and its numerical minimization tend to be driven by the frequent classes. Ignoring data imbalance while building a classification model generally lead to poor predictive performance on the minority class (see Box 2). This is problematic as minority class(es) are often the class(es) of interest (e.g. rare species <ref type="bibr" target="#b41">[42]</ref>) or rare habitats) and reliable model performance in predicting those instances is therefore particularly critical.</p><p>Often, collecting more data will not solve the issue as minority classes are by nature difficult to sample (rare species, rare habitat, rare event) and data imbalance will persist. In these cases, multiple methods have been developed to handle the imbalanced data problem <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref>. A common technique is to use resampling approaches (see Box 2). These techniques work at the data level by modifying the number of instances in majority and minority classes to balance the data distribution independently of the learning algorithm <ref type="bibr" target="#b43">[44]</ref>. With undersampling, the most abundant classes are down-sampled. On the contrary, with oversampling, the rarest classes are over-sampled. In the later case, data augmentation is the leading method (instead of duplication). It consists of generating new data from existing observations by using some disruptions and changes (e.g. changing orientation or colors in images; drawing a small subset of variables from random distributions in ecological studies). Another option is to assign different weights in the loss function to the observations belonging to the majority or minority class <ref type="bibr" target="#b44">[45]</ref>. However, this approach is very empirical since it remains challenging to select the optimal weights. Finally, it is also possible to calibrate the classification scores given by a model: the user can try to rescale the scores of each class to improve the classification performance <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tip 6: Choose evaluation metrics carefully</head><p>An important step before making any prediction on a sample of unseen data is to make sure the model consistently achieves a desirable performance. Various metrics exist to do so and the choice of the metric(s) to use depends on the type of model considered and the problem to solve. Using the wrong metric may lead to select poorly performing models ultimately altering the predictions <ref type="bibr" target="#b46">[47]</ref>. Evaluation metrics can also provide deeper insights into the results as they weight the importance of different characteristics in the predictions (Box 2). The confusion matrix, while not an evaluation metric, is also a useful tool that compares the model predictions to the actual classes and provide valuable information about the type of errors the model is making (see Box 3 for an example). Imbalanced classification problems also complicate the evaluation of predictive performance as popular classification metrics generally assume a balanced class distribution and may be misleading when data are imbalanced <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>Therefore, which metrics should the user choose? Top-k accuracy? Sensitivity/specificity? Precision/recall? There is no general answer here. Depending on the problem, some predictive errors may be more serious than others. For instance, in some applications, it could be more important to reduce the number of false positives to zero, while a trade-off between a small amount of false positives and false negatives could be preferable in other cases. We recommend to move beyond textbook examples and take the time to convert the objectives of the machine learning approach into the appropriate metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tip 7: Look out for shortcut learning</head><p>Due to the black box nature of some algorithms (e.g., neural networks, see Tip 8), it is often difficult to understand why those models are successful and, in particular, which part of the data and decision rules they choose to focus on when making predictions <ref type="bibr" target="#b5">[6]</ref>. Shortcuts is a particular group of decision rules based on unintended correlations and other biases in data that the model uses to make predictions. While superficially successful (i.e. perform well on standard benchmarks), these shortcut strategies typically lack generalisation and cause the model to fail unexpectedly (i.e., make inaccurate predictions) when transferring to slightly different data <ref type="bibr" target="#b5">[6]</ref>. In Figure <ref type="figure" target="#fig_4">4</ref>, we show an example of data that could lead to a shortcut opportunity. In this example, the data used by the model to learn how to differentiate two species, the wild boar and the white-tailed deer, included only nocturnal pictures of wild boar and diurnal pictures of deer. In that scenario, the model may learn to recognize species by focusing on image timestamps rather than by learning more complex shapes and patterns of the animals themselves. The variable day/night may be used as an unintended predictor for the species identification.</p><p>Several approaches can be used to limit shortcut opportunities. First, many shortcuts are a consequence of natural relationships (e.g., between a species and its typical surrounding landscapes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b49">50]</ref>) and can be avoided by modifying the training data to restrict the model's access to shortcut features. In our example in Figure <ref type="figure" target="#fig_4">4</ref>, including both nocturnal and diurnal pictures of each species would block the model from learning the shortcut feature day/night as a predictor to recognize the species. Adding noise to the training data with data augmentation may also be a solution to discourage the model from learning unintended relationships with the output. In photo-identification, a common practice is to crop the picture around the area of interest (e.g., face or individual pattern) in order to promote the training of the model on the zone of interests only <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b50">51]</ref>. Another recommendation is, again, to use on an out-of-sample test set to evaluate the model and test its generalisation beyond the narrowly learned settings <ref type="bibr" target="#b5">[6]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tip 8: Add some transparency in your black box models</head><p>Machine learning models lie on a continuum of interpretability and complexity and high predictive performance may come with a loss in interpretability <ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref>. Highly performing but complex models generally offer less visibility on how predictions are made, how the explanatory input variables impact the output and what the relationships between variables are <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b55">56]</ref>. They are therefore often considered to be black box models, hard to understand and communicate to a target audience. On the other side of the continuum, models like regressions or shallow decision trees (i.e., small trees with low depth) are simple and easy to understand when used with a few variables but may not be optimal for prediction. Random forests also offer the possibility to inspect variables importance, to enhance interpretability. Ecologists should be aware of this continuum before choosing which machine learning model to use. A model that is appropriate for a study focusing on prediction is unlikely to be optimal if the aim of the study is to understand the impact and directionality of the relationships between the explanatory input variables and the output.</p><p>However, regardless of the end goal of the study, some level of interpretability remains indispensable to validate and improve models <ref type="bibr" target="#b56">[57]</ref> and to avoid dangerous traps (e.g., shortcut learning, see <ref type="bibr">Tip 7)</ref>. Increasing research has been aiming at helping to explain predictions made by complex models <ref type="bibr" target="#b52">[53]</ref> and various methods are now available (e.g., SHAP <ref type="bibr" target="#b57">[58]</ref>, LIME <ref type="bibr" target="#b58">[59]</ref>). If the selected model is not interpretable per se (e.g. a neural network), there is still a path to gain transparency on the underlying process. For example, in deep learning for images classification, heatmaps have been used to highlight the image zones that were selected/activated by the model <ref type="bibr" target="#b59">[60]</ref> (e.g. the curved tail for baboons identification <ref type="bibr" target="#b60">[61]</ref>). In Box 3, we show how two other methods could improve the transparency of models usually considered to be black box models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tip 9: Make sure you do not learn from errors</head><p>Real-world data contain redundancy, duplicates and mislabelled classes that can significantly reduce machine learning efficiency <ref type="bibr" target="#b61">[62]</ref>. This is a particularly known issue in data collected from various citizen science programs <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref> or in datasets that have been merged from different sources before being fed into a model. To avoid a loss in model performance, an important effort must be made to curate, clean and prepare the data before training any model. This task may be time-consuming and hectic but often provides a greater payoff than experimenting with any advanced modelling approaches.</p><p>We suggest to consider machine learning as a tool to facilitate and automate the data cleaning process. In Box 4, we showed how a cross-validation approach could be used to flag observations in the training set that were likely to be mislabelled. In this example, we only used the model prediction probabilities to identify the observations that required our attention. However, more elaborate tools that implement a family of theory and algorithms called confident learning have recently been developed <ref type="bibr" target="#b64">[65]</ref>. They can detect flaws in datasets, characterize label noises, find label errors, fix datasets and improve the model performance by training on cleaned data with just a few lines of code (see open-source package cleanlab <ref type="bibr" target="#b64">[65]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>To face pressing challenges such as climate change and biodiversity loss, precise ecological predictions are critically needed by policy makers and ecosystems managers <ref type="bibr" target="#b65">[66]</ref>. Due to their high predictive performance and flexibility, machine learning models are an appropriate and efficient tool for ecologists. In this paper, we shared a few tips to help ecologists that are getting started with machine learning to avoid the common mistakes and traps and overcome some of the known challenges of those models. We believe that the use of machine learning by ecologists could result in important advances in ecology. Machine learning approaches also have the potential to be used for more than just model building and prediction, e.g. data cleaning, hypothesis creation and testing and discovery of new patterns in unlabelled data, making these approaches a powerful and valuable tool in the ecologist's toolbox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Box 1: Logistic regression in the machine learning mindset</head><p>We tackle here a binary classification problem using a logistic regression. We want to estimate the parameter of the model using a machine learning approach.</p><p># # Sepal / Petal length as predictors , # # binary class ( being or not Setosa ) as response # # coded with 0=" non setosa " and 1=" setosa " &gt; data ( iris ) &gt; full . set &lt;-data . frame ( Sepal . Length = iris $ Sepal . Length , Petal . Length = iris $ Petal . Length , Setosa = as . integer ( iris $ Species == " setosa " ) )</p><p>Sepal . Length Petal . Length Setosa  This dataset is characterised by a strong imbalance in the response variable (Table <ref type="table" target="#tab_1">1</ref>). We modelled the data using random forest. Our results showed that the classifier not accounting for the imbalance in data (first row in Table2) was unable to recognize any instance from the minority class (i.e. high risk individuals). We therefore used resampling methods: 1) undersampling, 2) oversampling, 3) combined under and oversampling and 4) SMOTE <ref type="bibr" target="#b66">[67]</ref>. However, implementing resampling methods in the random forest barely improved the model predictive power in the minority class (Table <ref type="table" target="#tab_2">2</ref> and Fig. <ref type="figure">5</ref>). This study also highlighted the importance of choosing an appropriate evaluation metric as the classification accuracy suggested outstanding model performance in most models despite their inability to predict the class of interest. While the undersampling model predicted significantly more positive instances (i.e., higher recall), the number of false positives increased substantially (i.e. very low precision, low F-scores). Therefore, none of those models seemed to be informative for the end-user (i.e., low F-Score for all models). To improve the model predictive performance and handle the imbalance in data, other options should be considered in this particular case (see tip 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>0.993 0.993 0.342 0.992 0.992 0.25 0.01 0.143 0.2 NA 0 0.014 0.913 0.014 0.014 0.027 0.02 0.026 0.027 NA 0.059 0.012 0.052 0.056 NA 0.018 0.048 0.018 0.018 NA Accuracy Precision Recall F1_score F0.5_score F2_score No sampling Oversampling Undersampling Combined SMOTE Fig 5. Accuracy, precision, recall and F-scores for the model without and with resampling methods R scripts to reproduce this study are provided in Supplementary Material. Box 3: Explaining predictions in species distribution modeling.</p><p>We used the dataset available in the supplementary material of Zurell et al. <ref type="bibr" target="#b67">[68]</ref> that recorded information about the presence or absence of the Ring Ouzel associated with 52 environmental predictors and investigated the drivers of the predictions in each model. We used two different machine learning models, a random forest and an artificial neural network, to predict the presence of the Ring Ouzel (Turdus torquatus) in Switzerland. Both model predicted the presence or absence of the Ring Ouzel with a high accuracy, 0.90 for the neural network and 0.92 for the random forest respectively.</p><p>To investigate which variables played an important role in those predictions, we generated the features importance for each model. In random forests, features importance are directly provided during the training and validation step. The variable ranks are based on the Gini importance score <ref type="bibr" target="#b68">[69]</ref> (Fig. <ref type="figure" target="#fig_6">6</ref>) or permutation importance measure <ref type="bibr" target="#b69">[70]</ref> (not shown). With neural networks, generating feature importance is not as straightforward. Feature importance can be determined by calculating the permutation importance but the implementation needs to be done by the user himself. In our example, we used another technique, the LIME approach <ref type="bibr" target="#b58">[59]</ref>, to provide a local model interpretability instead of interpretability from the perspective of the entire dataset. The output of LIME explains the contribution of each feature to the prediction of a data sample (Fig. <ref type="figure" target="#fig_7">7</ref>). For example, in our study, low values of the variable 'ddeg5' correlated with the presence (positive cases in Fig. <ref type="figure" target="#fig_7">7</ref>) of Ring Ouzel. R scripts to reproduce this study are provided in Supplementary Material. We trained an XGBoost model <ref type="bibr" target="#b70">[71]</ref> to classify surveys in classes A,B or C. We observed the predictive probabilities obtained from the XGBoost model on the training set (see Fig. <ref type="figure">8</ref>) using a 3-fold cross-validation approach. Survey 251 was labelled as A but the predictive probabilities obtained from the model were close to 0.0 for that particular class. However, the probability was close to 1.0 for class C (Fig. <ref type="figure">8</ref>). This suggested that survey 251 was wrongly labelled as class A and actually belonged to class C. Similarly, the model successfully flagged the class of surveys 1 and 101 as labelling errors (survey 1 being labelled B while actually belonging to class A and survey 101 classified in class C while being an instance from class C).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 1 .</head><label>1</label><figDesc>Fig 1. Illustration of the different steps of developing a machine learning model involving three separate sets of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 2 .</head><label>2</label><figDesc>Fig 2. Data standardisation computed with and without data leakage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig 3 .</head><label>3</label><figDesc>Fig 3. Example of data leakage due to data duplication. In case A, pictures in training and validation sets are from the same temporal sequence and look near identical. In that case, the model will work artifactually well on the validation set. In case B however, the validation set is informative since training and validation sets are independent (no duplication). Source: Vincent Miele</figDesc><graphic coords="7,141.56,95.04,328.88,235.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 4 .</head><label>4</label><figDesc>Fig 4. Example of shortcut learning opportunity in a neural network that aims to classify species in images. During training, white-tailed deer pictures were always taken in daylight; wild boar pictures in the nightime. This pattern is still present in test set 1 (middle row) but not in test set 2 (bottom row), exposing the shortcut: the model has learned to associate the picture timestamp to the species. On test set 2, the predictions are therefore erroneous.</figDesc><graphic coords="9,158.65,299.78,294.76,208.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>a train and a validation dataset # # with a random split ( for simplicity ; see drawbacks in Tip 2) &gt; idx &lt;-sample (1:150) &gt; train . set &lt;-full . set [ idx [1:120] ,] &gt; val . set &lt;-full . set [ idx [121:150] ,] # # Loss function measuring the gap between true class and prediction &gt; loss &lt;-function ( par ) { a &lt;-par [1] b &lt;-par [2] c &lt;-par [3] proba &lt;-1 / (1+ exp ( -( a + b * train . set $ Petal . Length + c * train . set $ Sepal . Length ) ) ) loss &lt;-sum ( -train . set $ Setosa * log ( proba ) -(1 -train . set $ Setosa ) * log (1 -proba ) ) / length ( train . set $ Setosa ) print ( loss ) return ( loss ) } # # Training the model by minimizing the loss # # to find the optimal parameters using train data set &gt; bestpar &lt;-optim ( par = c ( a = 0 , b = 0 , d =0) , " Best params are : " , bestpar , " \ n " ) Best params are : 2.407035 -23.63244 10.78046 &gt; cat ( " Minimal train loss is : " , loss ( bestpar ) ," \ n " ) Best train loss is : 4.742235 e -08 # # Building a predictor # # that predicts a binary class ( being or not Setosa ) &gt; predict &lt;-function ( data . set ) { a = bestpar [1] b = bestpar [2] c = bestpar [3] proba &lt;-1 / (1+ exp ( -( a + b * data . set $ Petal . Length + c * data . set $ Sepal . Length ) ) ) return ( as . integer ( proba &gt;0.5) ) } # # Checking prediction on validation data set &gt; head ( data . frame ( prediction = predict ( val . set ) , truth = val . set $ Setosa ) ) on a new test data set &gt; test . set &lt;-data . frame ( Petal . Length = c (2 ,3.5) , Sepal . Length = c (5.5 ,6) , Setosa = c ( NA , NA ) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 6 .</head><label>6</label><figDesc>Fig 6. Variable importance according to the Gini importance measure generated by the random forest model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 7 .</head><label>7</label><figDesc>Fig 7. Visualization of the 5 most important variables driving the predictions in the neural network model for 10 data points (the case number) using the LIME method . Features that have positive correlations with the output are shown in blue, negatively correlated features are shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Box 4 :</head><label>4</label><figDesc>Flagging label errors in vegetation surveysWe used a simulated set of vegetation surveys reporting the abundance of 100 species for 300 locations. Each survey was simulated accorded to three vegetation classes, using three different species assembly schemes that we called A, B and C (response variable). Surveys 1-100 were simulated with type A, 101-250 with type B and 251-300 with type C. The classification information for three surveys (1, 101 and 251) was intentionally modified to introduce labelling errors: survey 1 was labelled as belonging to class B instead of true class A, survey 101 to class C instead of B, and survey 251 to class A instead of C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Box 2: Unbalanced data and metrics to estimate risk of collision with vehicles in LynxWe used animal-vehicle collision data collected on the Eurasian Lynx in the French Jura Mountains to predict animals at high risk of collision in the Swiss Jura Mountains. Data on collisions were collected from 1982 to 2018, in France by OFB (the French Biodiversity Agency https://www.ofb.gouv.fr/en) and in Switzerland by KORA Carnivore Ecology and Wildlife Management (https://www.kora.ch/en/). A grid of 1 km 2 cells was overlaid on the Swiss-French road network taken from Open Street Map. Explanatory variables were urban land use cover (from the Corine Land Cover 2012 data base https://land.copernicus.eu/pan-european/corine-land-cover/clc-2012), distance to major road segments, human density, road class (proximal measure of traffic intensity, split into highways, main, local and regional roads), and total length of road segments. We also used lynx presence that we summarised over the study period by the cumulated number of times a cell was occupied (data were provided by the French Biodiversity Agency https://carmen.carmencarto.fr/38/Lynx.map).Number of individuals in each class for French and Swiss datasets.</figDesc><table><row><cell></cell><cell cols="3">Petal . Length Sepal . Length Setosa</cell></row><row><cell>1</cell><cell>2.0</cell><cell>5.5</cell><cell>NA</cell></row><row><cell>2</cell><cell>3.5</cell><cell>6.0</cell><cell>NA</cell></row><row><cell cols="4">&gt; test . set $ Setosa &lt;-predict ( test . set )</cell></row><row><cell></cell><cell cols="3">Petal . Length Sepal . Length Setosa</cell></row><row><cell>1</cell><cell>2.0</cell><cell>5.5</cell><cell>1</cell></row><row><cell>2</cell><cell>3.5</cell><cell>6.0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Predictive performance computed from the confusion matrix obtained for each model. Abbreviations: TP: True Positive; TN: True Negative; FP: False Positive; FN: False Negative</figDesc><table><row><cell></cell><cell>TP</cell><cell>TN</cell><cell>FP</cell><cell>FN</cell></row><row><cell>No sampling</cell><cell>0</cell><cell>9472</cell><cell>0</cell><cell>69</cell></row><row><cell>Oversampling</cell><cell>1</cell><cell>9469</cell><cell>3</cell><cell>68</cell></row><row><cell>Undersampling</cell><cell cols="3">63 3198 6274</cell><cell>6</cell></row><row><cell>Combined</cell><cell>1</cell><cell>9466</cell><cell>6</cell><cell>68</cell></row><row><cell>SMOTE</cell><cell>1</cell><cell>9468</cell><cell>4</cell><cell>68</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>X X train 1 X train 2 ... X train i X val 1 ... X val i X test 1 ... X test i X' X train' 1 X train' 2 ... X train' i X val' 1 ... Xval' i X test' 1</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by a grant from the <rs type="funder">French National Research Agency</rs> (grant <rs type="grantNumber">ANR-16-CE02-0007</rs>). We warmly thank <rs type="person">Christophe Duchamp</rs> (OFB, France) and <rs type="person">Fridolin Zimmermann</rs> (<rs type="affiliation">KORA, Switzerland</rs>) for sharing the data on lynx collisions with vehicles.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XwYSQXb">
					<idno type="grant-number">ANR-16-CE02-0007</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>R scripts to reproduce this study are provided in Supplementary Material.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random forests for classification in ecology</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Edwards</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><forename type="middle">H</forename><surname>Beard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adele</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">T</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">J</forename><surname>Lawler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2783" to="2792" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure</title>
		<author>
			<persName><surname>David R Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Volker Bahn</surname></persName>
		</author>
		<author>
			<persName><surname>Ciuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Boyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurutzeta</forename><surname>Elith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Severin</forename><surname>Guillera-Arroita</surname></persName>
		</author>
		<author>
			<persName><surname>Hauenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Lahoz-Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfried</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName><surname>Thuiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecography</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="913" to="929" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Applications for deep learning in ecology</title>
		<author>
			<persName><forename type="first">Éric</forename><surname>Sylvain Christin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hervet</surname></persName>
		</author>
		<author>
			<persName><surname>Lecomte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1632" to="1644" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">which camera trap type and how many do i need?&quot; a review of camera features and study designs for a range of wildlife research applications</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Rovero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fridolin</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duccio</forename><surname>Berzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hystrix</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three critical factors affecting automated image species recognition performance for camera traps</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saul</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><forename type="middle">C</forename><surname>Kremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology and evolution</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3503" to="3517" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Machine learning for ecology and sustainable natural resource management</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><forename type="middle">R</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Falk</forename><surname>Magness</surname></persName>
		</author>
		<author>
			<persName><surname>Huettmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machine learning and deep learning-a review for ecologists</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Pichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Hartig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Machine learning for image based species identification</title>
		<author>
			<persName><forename type="first">Jana</forename><surname>Wäldchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mäder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2216" to="2225" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Applications of machine learning in animal behaviour studies</title>
		<author>
			<persName><forename type="first">John</forename><surname>Joseph Valletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Torney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joah</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Behaviour</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="203" to="220" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evolutionary algorithms for species distribution modelling: A review in the context of machine learning</title>
		<author>
			<persName><forename type="first">Sacha</forename><surname>Gobeyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">F</forename><surname>Ans M Mouton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter Lm</forename><surname>Volk</surname></persName>
		</author>
		<author>
			<persName><surname>Goethals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Modelling</title>
		<imprint>
			<biblScope unit="volume">392</biblScope>
			<biblScope unit="page" from="179" to="195" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perspectives in machine learning for wildlife conservation</title>
		<author>
			<persName><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Blair R Costelloe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Risse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mackenzie</forename><forename type="middle">W</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tilo</forename><surname>Van Langevelde</surname></persName>
		</author>
		<author>
			<persName><surname>Burghardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dcl system using deep learning approaches for land-based or ship-based real time recognition and localization of marine mammals</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">W</forename><surname>Peter J Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofie</forename><forename type="middle">M</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Van Parijs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Bioacoustics Research Program, Cornell University Ithaca United States</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification of large acoustic datasets using machine learning and crowdsourcing: Application to whale calls</title>
		<author>
			<persName><forename type="first">Lior</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><surname>Yerby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander M Von</forename><surname>Benda-Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tyack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filipa</forename><surname>Samarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wallin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="953" to="962" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated classification of bird and amphibian calls using machine learning: A comparison of methods</title>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Acevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">J</forename><surname>Corrada-Bravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Héctor</forename><surname>Corrada-Bravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">J</forename><surname>Villanueva-Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aide</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="206" to="214" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Jakub Nowosad, and Katarzyna Jab lońska. Machine learning modeling of plant phenology based on coupling satellite and gridded meteorological dataset</title>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Czernecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of biometeorology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1297" to="1309" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluations and comparisons of rule-based and machine-learning-based methods to retrieve satellite-based vegetation phenology using modis and usa national phenology network data</title>
		<author>
			<persName><forename type="first">Qinchuan</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuewen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Earth Observation and Geoinformation</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">102189</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Sadegh Norouzzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Kosmala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">S</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>25):E5716-E5725</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using deep learning for image-based plant disease detection</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Sharada P Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><surname>Salathé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in plant science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1419</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<title level="m">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Characterizing soundscapes across diverse ecosystems using a universal acoustic feature set</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><forename type="middle">S</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Fulcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dena</forename><forename type="middle">Jane</forename><surname>Picinali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Clink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">H</forename><surname>David L Orme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Wrege</surname></persName>
		</author>
		<author>
			<persName><surname>Ewers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="17049" to="17055" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An introduction to statistical learning: with applications in R</title>
		<author>
			<persName><forename type="first">James</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Witten</forename><surname>Daniela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tibshirani</forename><surname>Robert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Spinger</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going further with model verification and deep learning</title>
		<author>
			<persName><forename type="first">Éric</forename><surname>Sylvain Christin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hervet</surname></persName>
		</author>
		<author>
			<persName><surname>Lecomte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="130" to="134" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An overview of overfitting and its solutions</title>
		<author>
			<persName><forename type="first">Xue</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of physics: Conference series</title>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1168</biblScope>
			<biblScope unit="page">22022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overfitting and undercomputing in machine learning</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="327" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust ecological analysis of camera trap data labelled by a machine learning model</title>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">C</forename><surname>Whytock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jedrzej</forename><surname>Świeżewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadeusz</forename><surname>Zwerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélie</forename><surname>Bara-S Lupski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flore</forename><forename type="middle">Koumba</forename><surname>Pambo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marek</forename><surname>Rogala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laila</forename><surname>Bahaa-El Din</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Boekee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Brittain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anabelle</forename><forename type="middle">W</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1080" to="1092" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Applied predictive modeling</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kjell</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Leakage and the reproducibility crisis in ml-based science</title>
		<author>
			<persName><forename type="first">Sayash</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.07048</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An introduction to domain adaptation and transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wouter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Kouw</surname></persName>
		</author>
		<author>
			<persName><surname>Loog</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11806</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Continual active adaptation to evolving distributional shifts</title>
		<author>
			<persName><forename type="first">Amrutha</forename><surname>Machireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranganath</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilesh</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omesh</forename><surname>Tickoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3444" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The deepfaune initiative: a collaborative effort towards the automatic identification of the french fauna in camera-trap images</title>
		<author>
			<persName><forename type="first">Noa</forename><surname>Rigoudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelbaki</forename><surname>Benyoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Besnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole</forename><surname>Birck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoann</forename><surname>Bollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoann</forename><surname>Bunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><forename type="middle">De</forename><surname>Backer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Caussimont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Delestrade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucie</forename><surname>Dispan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards robust cnn-based object detection through augmentation with synthetic rain variations</title>
		<author>
			<persName><forename type="first">Georg</forename><surname>Volk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Von Bernuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Hospach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="285" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Data augmentation: A comprehensive survey of modern approaches. Array</title>
		<author>
			<persName><forename type="first">Alhassan</forename><surname>Mumuni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuseini</forename><surname>Mumuni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">100258</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scientific utopia iii: Crowdsourcing science</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luis Uhlmann</surname></persName>
		</author>
		<author>
			<persName><surname>Charles R Ebersole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Christopher R Chartier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mallory</forename><forename type="middle">C</forename><surname>Errington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><forename type="middle">K</forename><surname>Kidwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><forename type="middle">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Riegelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">A</forename><surname>Silberzahn</surname></persName>
		</author>
		<author>
			<persName><surname>Nosek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="711" to="733" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">small data&apos;for big insights in ecology</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lindsay C Todman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia Sc</forename><surname>Bush</surname></persName>
		</author>
		<author>
			<persName><surname>Hood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Ecology &amp; Evolution</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Focus on the positives: Self-supervised learning for biodiversity monitoring</title>
		<author>
			<persName><forename type="first">Omiros</forename><surname>Pantazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><forename type="middle">E</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aodha</forename><surname>Mac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10583" to="10592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Leakage in data mining: Formulation, detection, and avoidance</title>
		<author>
			<persName><forename type="first">Shachar</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saharon</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Perlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Stitelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning from imbalanced data sets</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvador</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Galar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ronaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName><surname>Herrera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Responsible ai for conservation</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Oliver R Wearn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David Mp</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><surname>Jacoby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="73" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey on addressing high-class imbalance in big data</title>
		<author>
			<persName><surname>Joffrey L Leevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naeem</forename><surname>Bauder</surname></persName>
		</author>
		<author>
			<persName><surname>Seliya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Using random forest to learn imbalanced data</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression</title>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Van Den Goorbergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Van Smeden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Timmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Van Calster</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09101</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An experimental comparison of performance measures for classification</title>
		<author>
			<persName><forename type="first">César</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<author>
			<persName><surname>Modroiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Using auc and accuracy in evaluating learning algorithms</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="310" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Study of the impact of resampling methods for contrast pattern based classifiers in imbalanced databases</title>
		<author>
			<persName><forename type="first">Octavio</forename><surname>Loyola-González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Fco Martínez-Trinidad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesús</forename><surname>Ariel Carrasco-Ochoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milton</forename><surname>García-Borroto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="935" to="947" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="456" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting animal photo-identification using deep metric learning and network analysis</title>
		<author>
			<persName><forename type="first">Gaspard</forename><surname>Vincent Miele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Dussert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Spataro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Chamaillé-Jammes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Allainé</surname></persName>
		</author>
		<author>
			<persName><surname>Bonenfant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="863" to="873" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Data science vs. statistics: two cultures?</title>
		<author>
			<persName><forename type="first">Iain</forename><surname>Carmichael</surname></persName>
		</author>
		<author>
			<persName><surname>Marron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Japanese Journal of Statistics and Data Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="138" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A translucent box: interpretable machine learning in ecology</title>
		<author>
			<persName><forename type="first">Tim Cd</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Monographs</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1422</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Comprehensible classification models: a position paper</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Techniques for interpretable machine learning</title>
		<author>
			<persName><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Interpretability of deep learning models: A survey of results</title>
		<author>
			<persName><forename type="first">Supriyo</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tomsett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramya</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Harborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alun</forename><surname>Preece</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghuveer</surname></persName>
		</author>
		<author>
			<persName><surname>Rao</surname></persName>
		</author>
		<ptr target="smartworld/SCALCOM/UIC/ATC/CBDcom/IOP/SCI" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE smartworld, ubiquitous intelligence &amp; computing, advanced &amp; trusted computed, scalable computing &amp; communications, cloud &amp; big data computing, Internet of people and smart city innovation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Explainable ai: A review of machine learning interpretability methods</title>
		<author>
			<persName><forename type="first">Pantelis</forename><surname>Linardatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasilis</forename><surname>Papastefanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotiris</forename><surname>Kotsiantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Model-agnostic interpretability of machine learning</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05386</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Explainable deep learning: A field guide for the uninitiated</title>
		<author>
			<persName><forename type="first">Gabrielle</forename><surname>Ras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Van Gerven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Doran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="329" to="397" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Insights and approaches using deep learning to classify wildlife</title>
		<author>
			<persName><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitlyn</forename><forename type="middle">M</forename><surname>Gaynor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Muellerklein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Norouzzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Mcinturff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Rauri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Bowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8137</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Mohammad Sadegh</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Data preprocessing for supervised leaning</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Sotiris B Kotsiantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><forename type="middle">E</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><surname>Pintelas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="117" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Assessing data quality in citizen science</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Kosmala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Wiggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Ecology and the Environment</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="551" to="560" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Reliability of data collected by volunteers: A nine-year citizen science study in the red sea</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Meschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><forename type="middle">Machado</forename><surname>Toffolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Marchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Caroselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiorella</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Mancuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Franzellitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Locci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Davoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Trittoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="page">395</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">G</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
		<title level="m">Confident learning: Estimating uncertainty in dataset labels</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ecological forecasts: an emerging imperative</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">R</forename><surname>James S Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>Lodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Pielke</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><surname>Pizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="issue">5530</biblScope>
			<biblScope unit="page" from="657" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Nitesh V Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Testing species assemblage predictions from stacked and joint species distribution models</title>
		<author>
			<persName><forename type="first">Damaris</forename><surname>Zurell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helge</forename><surname>Niklaus E Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andri</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Baltensweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><forename type="middle">O</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><surname>Wüest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biogeography</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<title level="m">Classification and regression trees</title>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>Case: 40 Label: Positive Probability: 0.94 Explanation Fit: 0.25</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
