<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Techniques for Interpretable Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-05-19">19 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mengnan</forename><surname>Du</surname></persName>
							<email>dumengnan@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
							<email>xiahu@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Techniques for Interpretable Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-19">19 May 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">F57AEF007263A7DEF208BF1577CCF57B</idno>
					<idno type="arXiv">arXiv:1808.00033v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Post-hoc Global Explanation Post-hoc Local Explanation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interpretable machine learning tackles the important problem that humans cannot understand the behaviors of complex machine learning models and how these models arrive at a particular decision. Although many approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. We provide a survey covering existing techniques to increase the interpretability of machine learning models. We also discuss crucial issues that the community should consider in future work such as designing user-friendly explanations and developing comprehensive evaluation metrics to further push forward the area of interpretable machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Machine learning is progressing at an astounding rate, powered by complex models such as ensemble models and deep neural networks (DNNs). These models have a wide range of real-world applications, such as movie recommendations of Netflix, neural machine translation of Google, speech recognition of Amazon Alexa. Despite the successes, machine learning has its own limitations and drawbacks. The most significant one is the lack of transparency behind their behaviors, which leaves users with little understanding of how particular decisions are made by these models. Consider, for instance, an advanced self-driving car equipped with various machine learning algorithms doesn't brake or decelerate when confronting a stopped firetruck. This unexpected behavior may frustrate and confuse users, making them wonder why. Even worse, the wrong decisions could cause severe consequences if the car is driving at highway speeds and might finally crash the firetruck. The concerns about the black-box nature of complex models have hampered their further applications in our society, especially in those critical decision-making domains like self-driving cars.</p><p>Interpretable machine learning would be an effective tool to mitigate these problems. It gives machine learning models the ability to explain or to present their behaviors in understandable terms to humans <ref type="bibr" target="#b10">[10]</ref>, which is named interpretability or explainability and we use them interchange-ably in this paper. Interpretability would be an indispensable part for machine learning models in order to better serve human beings and bring benefits to society. For endusers, explanation will increase their trust and encourage them to adopt machine learning systems. From the perspective of machine learning system developers and researchers, the provided explanation can help them better understand the problem, the data and why a model might fail, and eventually increase the system safety. Thus there is a growing interest among the academic and industrial community in interpreting machine learning models and gaining insights into their working mechanisms.</p><p>Interpretable machine learning techniques can generally be grouped into two categories: intrinsic interpretability and post-hoc interpretability, depending on the time when the interpretability is obtained <ref type="bibr" target="#b23">[23]</ref>. Intrinsic interpretability is achieved by constructing self-explanatory models which incorporate interpretability directly to their structures. The family of this category includes decision tree, rule-based model, linear model, attention model, etc. In contrast, the post-hoc one requires creating a second model to provide explanations for an existing model. The main difference between these two groups lies in the trade-off between model accuracy and explanation fidelity. Inherently interpretable models could provide accurate and undistorted explanation but may sacrifice prediction performance to some extent. The post-hoc ones are limited in their approximate nature while keeping the underlying model accuracy intact.</p><p>Based on the above categorization, we further differentiate two types of interpretability: global interpretability, and local interpretability. Global interpretability means that users can understand how the model works globally by inspecting the structures and parameters of a complex model, while local interpretability locally examines an individual prediction of a model, trying to figure out why the model makes the decision it makes. Using the DNN in Figure <ref type="figure" target="#fig_0">1</ref> as an example, global interpretability is achieved by understanding the representations captured by the neurons at an intermediate layer, while local interpretability is obtained by identifying the contributions of each feature in a specific input to the prediction made by DNN. These two types bring different benefits. Global interpretability could illuminate the inner working mechanisms of machine learning models and thus can increase their transparency. Local interpretability will help uncover the causal relations between a specific input and its corresponding model prediction. Those two help users trust a model and trust a prediction, respectively.</p><p>In this article, we first summarize current progress of three lines of research for interpretable machine learning: designing inherently interpretable models (including globally and locally), post-hoc global explanation, and post-hoc local explanation. We proceed by introducing applications and challenges of current techniques. Finally, we present limitations of current explanations and propose directions towards more human-friendly explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">INTRINSIC INTERPRETABLE MODEL</head><p>Intrinsic interpretability can be achieved by designing selfexplanatory models which incorporate interpretability directly into the model structures. These constructed interpretable models either are globally interpretable or could provide explanations when they make individual predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Globally Interpretable Model</head><p>Globally interpretable models can be constructed in two ways: directly trained from data as usual but with interpretability constraints, and being extracted from a complex and opaque model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Adding Interpretability Constraints</head><p>The interpretability of a model could be promoted by incorporating interpretability constraints. Some representative examples include enforcing sparsity terms or imposing semantic monotonicity constraints in classification models <ref type="bibr" target="#b14">[14]</ref>. Here sparsity means that a model is encouraged to use relatively fewer features for prediction, while monotonicity enables the features to have monotonic relations with the prediction. Similarly, decision trees are pruned by replacing subtrees with leaves to encourage long and deep trees rather than wide and more balanced trees <ref type="bibr" target="#b29">[29]</ref>. These constraints make a model simpler and could increase the model's comprehensibility by users.</p><p>Besides, more semantically meaningful constraints could be added to a model to further improve interpretability. For instance, interpretable convolutional neural networks (CNN) add a regularization loss to higher convolutional layers of CNN to learn disentangled representations, resulting in filters that could detect semantically meaningful natural objects <ref type="bibr" target="#b39">[39]</ref>. Another work combines novel neural units, called capsules, to construct a capsule network <ref type="bibr" target="#b32">[32]</ref>. The activation vectors of an active capsule can represent various semantic-aware concepts like position and pose of a particular object. This nice property makes capsule network more comprehensible for humans.</p><p>However, there are often trade-offs between prediction accuracy and interpretability when constraints are directly incorporated into models. The more interpretable models may result in reduced prediction accuracy comparing the less interpretable ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Interpretable Model Extraction</head><p>An alternative is to apply interpretable model extraction, also referred as mimic learning <ref type="bibr" target="#b36">[36]</ref>, which may not have to sacrifice the model performance too much. The motivation behind mimic learning is to approximate a complex model using an easily interpretable model such as a decision tree, rule-based model, or linear model. As long as the approximation is sufficiently close, the statistical properties of the complex model will be reflected in the interpretable model. Eventually, we obtain a model with comparable prediction performance, and the behavior of which is much easier to understand. For instance, the tree ensemble model is transformed into a single decision tree <ref type="bibr" target="#b36">[36]</ref>. Moreover, a DNN is utilized to train a decision tree which mimics the inputoutput function captured by the neural network so that the knowledge encoded in DNN is transferred to the decision tree <ref type="bibr" target="#b5">[5]</ref>. To avoid the overfitting of the decision tree, active learning is applied for training. These techniques convert the original model to a decision tree with better interpretability and maintain comparable predictive performance at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Locally Interpretable Model</head><p>Locally interpretable models are usually achieved by designing more justified model architectures that could explain why a specific decision is made. Different from the globally interpretable models that offer a certain extent of transparency about what is going on inside a model, locally interpretable models provide users understandable rationale for a specific prediction.</p><p>A representative scheme is employing attention mechanism <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b4">4]</ref>, which is widely utilized to explain predictions made by sequential models, e.g., Recurrent Neural Networks (RNNs). Attention mechanism is advantageous in that it gives users the ability to interpret which parts of the input are attended by the model through visualizing the attention weight matrix for individual predictions. Attention mechanism has been used to solve the problem of generating image caption <ref type="bibr" target="#b38">[38]</ref>. In this case, a CNN is adopted to encode an input image to a vector, and an RNN with attention mechanisms is utilized to generate descriptions. When generating each word, the model changes its attention to reflect the relevant parts of the image. The final visualization of the attention weights could tell human what the model is looking at when generating a word. Similarly, attention mechanism has been incorporated in machine translation <ref type="bibr" target="#b4">[4]</ref>. At decoding stage, the neural attention module added to neural machine translation (NMT) model assigns different weights to the hidden states of the decoder, which allows the decoder to selectively focus on different parts of the input sentence at each step of the output generation. Through visualizing the attention scores, users could understand how words in one language depend on words in another language for correct translation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">POST-HOC GLOBAL EXPLANATION</head><p>Machine learning models automatically learn useful patterns from a huge amount of training data and retain the learned knowledge into model structures and parameters. Post-hoc global explanation aims to provide a global understanding about what knowledge has been acquired by these pre-trained models, and illuminate the parameters or learned representations in an intuitive manner to humans. We classify existing models into two categories: traditional machine learning and deep learning pipelines (see Figure <ref type="figure" target="#fig_1">2</ref>), since we are capable of extracting some similar explanation paradigms from each category. We introduce below how to provide explanation for these two types of pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Traditional ML Explanation</head><p>Traditional machine learning pipelines mostly rely on feature engineering, which transforms raw data into features that better represent the predictive task, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. The features are generally interpretable and the role of machine learning is to map the representation to output. We consider a simple yet effective explanation measure which is applicable to most of the models belonging to traditional pipeline, called feature importance, which indicates statistical contribution of each feature to the underlying model when making decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Model-agnostic Explanation</head><p>Model-agnostic feature importance is broadly applicable to various machine learning models. It treats a model as a black-box and does not inspect internal model parameters.</p><p>A representative approach is Permutation Feature Importance <ref type="bibr" target="#b1">[1]</ref>. The key idea is that the importance of a specific feature to the overall performance of a model can be determined by calculating how the model prediction accuracy deviates after permuting the values of that feature. More specifically, given a pre-trained model with n features and a test set, the average prediction score of the model on the test set is p, which is also the baseline accuracy. We shuffle the values of a feature on the test set and compute the average prediction score of the model on the modified dataset. This process is iteratively performed for each feature and eventually n prediction scores are obtained for n features respectively. We then rank the importance of the n features according to the reductions of their score comparing to baseline accuracy p. There are several advantages for this approach. First, we do not need to normalize the values of the hand-crafted features. Second, it can be generalized to nearly any machine learning models with hand-crafted features as input. Third, this strategy has been proved to be robust and efficient of implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Model-specific Explanation</head><p>There also exists explanation methods specifically designed for different models. Model-specific methods usually derive explanations by examining internal model structures and parameters. We introduce below how to provide feature importance for two families of machine learning models. Generalized linear models GLM is constituted of a series of models which are linear combination of input features and model parameters followed by feeding to some transformation function (often nonlinear) <ref type="bibr" target="#b21">[21]</ref>. Examples of GLM includes linear regression, logistic regression, etc. The weights of a GLM directly reflect feature importance, so users can understand how the model works by checking their weights and visualizing them. However, the weights may not be reliable when different features are not appropriately normalized and vary in their scale of measurement. Besides, the interpretability of an explanation will decrease when the feature dimensions become too large, which may be beyond the comprehension ability of humans. Tree-based ensemble models Tree-based ensemble models, such as gradient boosting machines, random forests and XGBoost <ref type="bibr" target="#b7">[7]</ref>, are typically inscrutable to humans. There are several ways to measure the contribution of each feature. The first approach is to calculate the accuracy gain when a feature is used in tree branches. The rationale behind is that without adding a new split to a branch for a feature, there may be some misclassified elements, while after adding the new branch, there are two branches and each one is more accurate. The second approach measures the feature coverage, i.e., calculating the relative quantity of observations related to a feature. The third approach is to count the number of times that a feature is used to split the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DNN Representation Explanation</head><p>DNNs, in contrast to traditional models, not only discover the mapping from representation to output, but also learn representations from raw data <ref type="bibr" target="#b15">[15]</ref>, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. The learned deep representations are usually not human interpretable <ref type="bibr" target="#b19">[19]</ref>, hence the explanation for DNNs mainly focuses on understanding the representations captured by the neurons at intermediate layers of DNNs. In the following, we introduce explanation methods for two major categories of DNN, i.e., CNN and RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Explanation of CNN Representation</head><p>There has been a growing interest to understand the inscrutable representations at different layers of CNN. Among different strategies to understand CNN representations, the most effective and widely utilized one is through finding the preferred inputs for neurons at a specific layer. This is generally formulated in the activation maximization (AM) framework <ref type="bibr" target="#b33">[33]</ref>, which can be formulated as:</p><formula xml:id="formula_0">x * = argmax x f l (x) -R(x),<label>(1)</label></formula><p>where f l (x) is the activation value of a neuron at layer l for input x, and R(x) is a regularizer. Starting from random initialization, we optimize an image to maximally activate a neuron. Through iterative optimization, the derivatives of the neuron activation value with respect to the image is utilized to tweak the image. Eventually, the visualization of the generated image could tell what individual neuron is looking for in its receptive field. We can in fact do this for arbitrary neurons, ranging from neurons at the first layer all the way to the output neurons at the last layer, to understand what is encoded as representations at different layers. While the framework is simple, getting it to work faces some challenges, among which the most significant one is the surprising artifact. The optimization process may produce unrealistic images containing noise and high-frequency patterns. Due to the large searching space for images, if without proper regularization, it is possible to produce images that satisfy the the optimization objective to activate the neuron but are still unrecognizable. To tackle this problem, the optimization should be constrained using natural image priors so as to produce synthetic images which resemble natural images. Some researchers heuristically propose hand-crafted priors, including total variation norm, α-norm, Gaussian blur, etc. In addition, the optimization could be regularized using stronger natural image priors produced by a generative model, such as GAN or VAE, which maps codes in the latent space to the image spaces <ref type="bibr" target="#b25">[25]</ref>. Instead of directly optimizing the image, these methods optimize the latent space codes to find an image which can activate a given neuron. Experimental results have shown that the priors produced by generative models lead to significant improvements in visualization.</p><p>The visualization results provide several interesting observations about CNN representations. First, the network learns representations at several levels of abstraction, transiting from general to task-specific from the first layer to the last layer. Take the CNN trained with the ImageNet dataset for example. Lower-layer neurons detect small and simple patterns, such as object corners and textures. Mid-layer neurons detect object parts, such as faces, legs. Higher-layer neurons respond to whole objects or even scenes. Interestingly, the visualization of the last layer neurons illustrates that CNN exhibits a remarkable property to capture global structure, local details, and contexts of an object. Second, a neuron could respond to different images that are related to a semantic concept, revealing the multifaceted nature of neurons <ref type="bibr" target="#b27">[27]</ref>. For instance, a face detection neuron can fire in response to both human faces and animal faces. Note that this phenomenon is not confined to high layer neurons, all layers of neurons are multifaceted. The neurons at higher layers are more multifaceted than the ones at lower layers, indicating that higher-layer neurons become more invariant to large changes within a class of inputs, such as colors and poses. Third, CNN learns distributed code for objects <ref type="bibr" target="#b40">[40]</ref>. Objects can be described using part-based representations and these parts can be shared across different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Explanation of RNN Representation</head><p>Following numerous efforts to interpret CNN, uncovering the abstract knowledge encoded by RNN representations (including GRUs and LSTMs) has also attracted increasing interest in recent years. Language modeling, which targets to predict the next token given its previous tokens, is usually utilized to analyze the representations learned by RNN. The studies indicate that RNN indeed learns useful representations <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b28">28]</ref>.</p><p>First, some work examines the representations of the last hidden layer of RNN and study the function of different units at that layer, by analyzing the real input tokens that maximally activate a unit. The studies demonstrate that some units of RNN representations are able to capture complex language characteristics, e.g., syntax, semantics and long-term dependencies. For instance, a study analyzes the interpretability of RNN activation patterns using characterlevel language modeling <ref type="bibr" target="#b18">[18]</ref>. This work finds that although most of the neural units are hard to find particular meanings, there indeed exist certain dimensions in RNN hidden representations that are able to focus on specific language structures such as quotation marks, brackets, and line lengths in a text. In another work, a word-level language model is utilized to analyze the linguistic features encoded by individual hidden units of RNN <ref type="bibr" target="#b17">[17]</ref>. The visualizations illustrate that some units are mostly activated by certain semantic category, while some others could capture a particular syntactic class or dependency function. More interestingly, some hidden units could carry the activation values over to subsequent time steps, which explains why RNN can learn longterm dependencies and complex linguistic features.</p><p>Second, the research finds that RNN is able to learn hierarchical representations by inspecting representations at different hidden layers <ref type="bibr" target="#b28">[28]</ref>. This observation indicates that RNN representations bear some resemblance to their CNN counterpart. For instance, a bidirectional language model is constructed using a multi-layer LSTM <ref type="bibr" target="#b28">[28]</ref>. The analysis of representations at different layers of this model shows that the lower-layer representation captures context-independent syntactic information. In contrast, higher-layer LSTM representations encode context-dependent semantic information. The deep contextualized representations can disambiguate the meanings of words by utilizing their context, and thus could be employed to perform tasks which require contextaware understanding of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">POST-HOC LOCAL EXPLANATION</head><p>After understanding the model globally, we zoom in to the local behavior of the model and provide local explanations for individual predictions. Local explanations target to identify the contributions of each feature in the input towards a specific model prediction. As local methods usually attribute a model's decision to its input features, they are also called attribution methods. In this section, we first introduce model-agnostic attribution methods and then discuss attribution methods specific to DNN-based predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model-agnostic Explanation</head><p>Model-agnostic methods allow explaining predictions of arbitrary machine learning models independent of the implementation. They provide a way to explain predictions by treating the models as black-boxes, where explanations could be generated even without access to the internal model parameters. They bring some risks at the same time, since we cannot guarantee that the explanation faithfully reflects the decision making process of a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Local Approximation Based Explanation</head><p>Local approximation based explanation is based on the assumption that the machine learning predictions around the neighborhood of a given input can be approximated by an interpretable white-box model. The interpretable model does not have to work well globally, but it must approximate the black-box model well in a small neighborhood near the original input. Then the contribution score for each feature can be obtained by examining the parameters of the whitebox model. Some studies assume that the prediction around the neighborhood of an instance could be formulated as the linearly weighted combination of its input features <ref type="bibr" target="#b30">[30]</ref>. Attribution methods based on this principle first sample the feature space in the neighborhood of the instance to constitute an additional training set. A sparse linear model, such as Lasso, is then trained using the generated samples and labels. This approximation model works the same as a black-box model locally but is much easier to inspect. Finally, the prediction of the original model can be explained by examining the weights of this sparse linear model instead.</p><p>Sometimes, even the local behavior of a model may be extremely non-linear, linear explanations could lead to poor performance. Models which could characterize non-linear relationship are thus utilized as the local approximation. For instance, a local approximation based explanation framework can be constructed using if-then rules <ref type="bibr" target="#b31">[31]</ref>. Experiments on a series of tasks show that this framework is effective at capturing non-linear behaviors. More importantly, the produced rules are not confined merely to the instance being explained and often generalize to other instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Perturbation Based Explanation</head><p>This line of work follows the philosophy that the contribution of a feature can be determined by measuring how prediction score changes when the feature is altered. It tries to answer the question: which parts of the input, if they were not seen by the model, would most change its prediction? Thus, the results may be called counterfactual explanations. The perturbation is performed across features sequentially to determine their contributions, and can be implemented in two ways: omission and occlusion. For omission, a feature is directly removed from the input, but this is impractical in practice since few models allow setting features as unknown. As for occlusion, the feature is replaced with a reference value, such as zero for word embeddings or specific gray value for image pixels. Nevertheless, occlusion raises a new concern that new evidence may be introduced and that can be used by the model as a side effect <ref type="bibr" target="#b8">[8]</ref>. For instance, if we occlude part of an image using green color and then we may provide undesirable evidence for the grass class. Thus we should be particularly cautious when selecting reference values to avoid introducing extra pieces of evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model-specific Explanation</head><p>There are also explanation approaches exclusively designed for a specific type of model. Below we introduce DNNspecific methods, which treat the networks as white-boxes and explicitly utilize the interior structure to derive explanations. We divide them into three major categories: backpropagation based methods in a top-down manner; perturbation based methods in a bottom-up manner; investigation of deep representations in intermediate layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Back-propagation</head><p>Back-propagation based methods calculate the gradient, or its variants, of a particular output with respect to the input using back-propagation to derive the contribution of features. In the simplest case, we can back-propagate the</p><p>Ski Cardoon (a) Input (b) Gradient (c) Perturbation (d) Representation gradient <ref type="bibr" target="#b33">[33]</ref>. The underlying hypothesis is that larger gradient magnitude represents a more substantial relevance of a feature to a prediction. Other approaches back-propagate different forms of signals to the input, such as discarding negative gradient values at the back-propagation process <ref type="bibr" target="#b34">[34]</ref>, or back-propagating the relevance of the final prediction score to the input layer <ref type="bibr" target="#b3">[3]</ref>. These methods are integrated into a unified framework where all methods are reformulated as a modified gradient function <ref type="bibr" target="#b2">[2]</ref>. This unification enables comprehensive comparison between different methods and facilitates effective implementation under modern deep learning libraries, such as TensorFlow and PyTorch. Backpropagation based methods are efficient in terms of implementation, as they usually need a few forward and backward calculations. On the other hand, they are limited in their heuristic nature and may generate explanations of unsatisfactory quality, which are noisy and highlight some irrelevant features, as shown in Figure <ref type="figure" target="#fig_3">3</ref> (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Mask Perturbation</head><p>Model-agnostic perturbation mentioned in the previous section could be computationally very expensive when handling an instance with high dimensions, since they need to sequentially perturb the input. In contrast, DNN-specific perturbation could be implemented efficiently through mask perturbation and gradient descent optimization. One representative work formulates the perturbation in an optimization framework to learn a perturbation mask, which explicitly preserves the contribution values of each feature <ref type="bibr" target="#b13">[13]</ref>. Note that this framework generally needs to impose various regularizations to the mask to produce meaningful explanation rather than surprising artifacts <ref type="bibr" target="#b13">[13]</ref>. Although the optimization based framework has drastically boosted the efficiency, generating an explanation still needs hundreds of forward and backward operations. To enable more computationally efficient implementation, a DNN model can be trained to predict the attribution mask <ref type="bibr" target="#b8">[8]</ref>. Once the mask neural network model is obtained, it only requires a single forward pass to yield attribution scores for an input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Investigation of Deep Representations</head><p>Either perturbation or back-propagation based explanations ignore the intermediate layers of the DNN that might contain rich information for interpretation. To bridge the gap, some studies explicitly utilize the deep representations of the input to perform attribution.</p><p>Based on the observation that deep CNN representations capture the high-level content of input images as well as their spatial arrangement, a guided feature inversion framework is proposed to provide local explanations <ref type="bibr" target="#b11">[11]</ref>. This framework inverts the representations at higher layers of CNN to a synthesized image, while simultaneously encodes the location information of the target object in a mask. Decomposition is another perspective to take advantage of deep DNN representations. For instance, through modeling the information flowing process of the hidden representation vectors in RNN models, the RNN prediction is decomposed into additive contribution of each word in the input text <ref type="bibr" target="#b12">[12]</ref>. The decomposition result could quantify the contribution of each individual word to a RNN prediction. These two explanation paradigms achieve promising results on a variety of DNN architectures, indicating that the intermediate information indeed contributes significantly to the attribution. Besides, deep representations serve as a strong regularizer, increasing the possibility that the explanations faithfully characterize the behaviors of DNN under normal operating conditions. Thus it reduces the risks of generating surprising artifacts and leads to more meaningful explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">APPLICATIONS</head><p>Interpretable machine learning has numerous applications. We introduce three representative ones: model validation, model debugging, and knowledge discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Validation</head><p>Explanations could help to examine whether a machine learning model has employed the true evidences instead of biases which widely exist among training data. A post-hoc attribution approach, for instance, analyzes three question answering models <ref type="bibr" target="#b24">[24]</ref>. The attribution heatmaps show that these models often ignore important part of the questions and rely on irrelevant words to make decisions. They further indicate that the weakness of the models is caused by the inadequacies of training data. Possible solutions to fix this problem include modifying training data or introducing inductive bias when training the model. More seriously, machine learning models may rely on gender and ethnic biases to make decisions <ref type="bibr">[9]</ref>. Interpretability could be exploited to identify whether models have utilized these biases to ensure models don't violate ethical and legal requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Debugging</head><p>Explanations also can be employed to debug and analyze the misbehavior of models when models give wrong and unexpected predictions. A representative example is adversarial learning <ref type="bibr" target="#b26">[26]</ref>. Recent work demonstrated that machine learning models, such as DNNs, can be guided into making erroneous predictions with high confidence, when processing accidentally or deliberately crafted inputs <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b20">20]</ref>. However, these inputs are quite easy to be recognized by humans. In this case, explanation facilitates humans to identify the possible model deficiencies and analyze why these models may fail. More importantly, we may further take advantage of human knowledge to figure out possible solutions to promote the performances and reasonability of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Knowledge Discovery</head><p>The derived explanations also allow humans to obtain new insights from machine learning model through comprehending their decision making process. With explanation, the area experts and the end-users could provide realistic feedbacks. Eventually, new science and new knowledge which are originally hidden in the data could be extracted. For instance, a rule-based interpretable model has been utilized to predict the mortality risk for patients with pneumonia <ref type="bibr" target="#b6">[6]</ref>. One of the rules from the model suggests that having asthma could lower a patient's risk of dying from pneumonia. It turns out to be true since patients with asthma were given more aggressive treatments which led to better outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESEARCH CHALLENGES</head><p>Despite recent progresses in interpretable machine learning, there are still some urgent challenges, especially on explanation method design as well as evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Explanation Method Design</head><p>The first challenge is related to the method design, especially for post-hoc explanation. We argue that an explanation method should be restricted to truly reflect the model behavior under normal operation conditions. This criterion has two meanings. Firstly, the explanations should be faithful to the mechanism of the underlying machine learning model <ref type="bibr" target="#b12">[12]</ref>. Post-hoc explanation methods propose to approximate the behavior of models. Sometimes, the approximation is not sufficiently accurate, and the explanation may fail to precisely reflect the actual operation status of the original model. For instance, an explanation method may give an explanation that makes sense to humans, while actually, the machine learning model works in an entirely different way. Second, even when explanations are of high fidelity to the underlying models, they may fail to represent the model behavior under normal conditions. Model explanation and surprising artifacts are often two sides of the same coin. The explanation process could generate examples which are out of distribution from the statistics in the training dataset, including nonsensical inputs and adversarial examples <ref type="bibr" target="#b16">[16]</ref>, which are beyond the capability of current machine learning models. Without careful design, both global and local explanations may trigger the artifacts of machine learning models, rather than produce meaningful explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Explanation Method Evaluation</head><p>The second challenge involves the method evaluation. We introduce below the evaluation challenges for intrinsic explanation and post-hoc explanation.</p><p>The challenge for intrinsic explanation mainly lies in how to quantify the interpretability. There are broad sets of interpretable models which are designed according to distinct principles and have various forms of implementations. Take the recommender system as an example, both interpretable latent topic model and attention mechanism could provide some extent of interpretability. Nevertheless, how can we compare the interpretability between globally interpretable model and locally interpretable model? There is still no consensus on what interpretability means and how to measure the interpretability. Finale and Been propose three types of metrics: application-grounded metrics, human-grounded metrics, and functionally-grounded metrics <ref type="bibr" target="#b10">[10]</ref>. These metrics are complementary to each other and bring their own pros and cons regarding the degree of validity and the cost to perform evaluations. Adopting what metrics heavily depends on the tasks so as to make more informed evaluations.</p><p>For post-hoc explanation, comparing to evaluate its interpretability, it is equally important to assess the faithfulness of explanation to the original model, which is often omitted by existing literature. As mentioned before, generated explanations for a machine learning model are not always reasonable to humans. It is extremely hard to tell whether the unexpected explanation is caused by misbehavior of the model or limitation of the explanation method. Therefore, better metrics to measure the faithfulness of explanations are needed, in order to complement existing evaluation metrics. The degree of faithfulness can determine how confident we can trust a explanation. Nevertheless, the design of appropriate faithfulness metric remains an open problem and deserves further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION</head><p>We briefly introduce limitations of explanation methods that we have surveyed and then present explanation formats that might be more understandable and friendly to users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Limitations of Current Explanations</head><p>A major limitation of existing work on interpretable machine learning is that the explanations are designed based on the intuition of researchers rather than focusing on the demands of end-users. Current local explanations are usually given in the format of feature importance vectors, which are a complete causal attribution and a low-level explanation <ref type="bibr" target="#b23">[23]</ref>. This format would be satisfactory if the explanation audiences are developers and researchers, since they can utilize the statistic analysis of the feature importance distribution to debug the models. Nevertheless, this format is less friendly if the explanation receivers are lay-users of machine learning. It describes the full decision logic of a model, which contains huge amount of redundant information and will be overwhelming to users. The presentation formats could be further enhanced to better promote user satisfaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Towards Human-friendly Explanations</head><p>Based on findings in social sciences and human behavioural studies <ref type="bibr" target="#b22">[22]</ref>, we provide some directions towards user-oriented explanations, which might be more satisfying to humans as a means of communication.</p><p>Contrastive Explanations They are also referred as differential explanations <ref type="bibr" target="#b22">[22]</ref>. They do not tell why a specific prediction was made, but rather explain why this prediction was made instead of another, so as to answer questions like "Why Q rather than R?". Here Q is the fact which requires explanation, and R is the comparing case, which could be a real one or virtual one. Consider, for instance, a user is declined mortgage. The user may compare with another real case and raise question "why didn't I get a mortgage when my neighborhood did?". On the other hand, the user may ask "Why was my mortgage rejected?". Here is an implicit contrast case, and actually the user is requesting explanation for a virtual case "How to get my mortgage loan approved?". Since it is compared to an event which has not happened, thus the desirable explanation here can also be called counterfactual explanation <ref type="bibr" target="#b37">[37]</ref>.</p><p>To provide contrastive explanations for a model prediction, similar strategy could be used for both above-mentioned comparisons. We first produce feature importance attri-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretable Machine Learning</head><p>More faithful &amp; accurate Researcher-oriented explanation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current stage</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difficulty of Tasks</head><p>Improve ML generalization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User-friendly Explanation</head><p>Figure <ref type="figure">4</ref>: Progress of interpretable ML. Current stage is researcher-oriented explanations. We can make it more faithful and accurate, which can be further utilized to promote model generalization ability, and then develop user-friendly explanations.</p><p>bution for two instances: not-accepted case for the user, has-accepted case of a neighbor (or would-be-accepted case of the user), and then compare the two attribution vectors. Note that we could resort to adversarial perturbation to find the would-be-accepted case. Besides, it is recommended to provide a diverse set of reasons, i.e., to find multiple contrast cases, to make the explanation more informative. Ultimately, we generate explanations of the form "Your mortgage is rejected because your income is lower than your neighbor's, your credit history is not as strong as your neighbor's, etc" or "Your mortgage would be accepted if your income is raised from x to y".</p><p>Selective Explanations Usually, users do not expect an explanation can cover the complete cause of a decision. Instead, they wish the explanation could convey the most important information that contributes to the decision <ref type="bibr" target="#b22">[22]</ref>. A sparse explanation, which includes a minimal set of features that help justify the prediction is preferred, although incompletely. Still use the mortgage case for example. One good explanation could be presenting users the top 2 reasons contributing to the decision, such as poor credit history, low income to debt ratio.</p><p>Credible Explanations Good explanation might be consistent with prior knowledge of general users <ref type="bibr" target="#b23">[23]</ref>. Suppose the generated top reasons for the mortgage case include marital status is single and education status is high school graduate, then it would be less trustable than an explanation outputting poor credit history and low income to debt ratio, since the latter two are more reasonable causes leading to rejection. Low credibility could be caused by the poor fidelity of explanation to the original model. On the other hand, the explanations maybe faithful, however, the machine learning model does not adopt correct evidences to make decisions.</p><p>Conversational Explanations Explanations might be delivered as a conversation between the explainer and explanation receivers <ref type="bibr" target="#b22">[22]</ref>. It means that we need to consider the social context, i.e., to whom an explanation is provided <ref type="bibr" target="#b35">[35]</ref>, in order to determine the content and formats of explanations. For instance, a preferred format is verbal explanation if it is explaining to lay-users.</p><p>Note that there are many other paths to user-friendly explanations. We refer interested readers to the survey by Miller <ref type="bibr" target="#b22">[22]</ref> for a comprehensive list of directions. All the aforementioned directions serve an identical purpose that explanation should tell users why a decision was reached in a concise and friendly manner. More importantly, the expla-nation could inform users what could be possibly changed to receive a desired decision next time. Granted, there is still a long way to go to render explanations promote user's satisfaction. In future, researchers from different disciplines, including machine learning, human-computer interaction, and social science, are encouraged to closely cooperate to design really user-oriented and human-friendly explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>Interpretable machine learning is an open and active field of research, with numerous interpretation approaches continuously emerging every year. We present a clear categorization and comprehensive overview of existing techniques for interpretable machine learning, aiming to help the community to better understand the capabilities and weaknesses of different interpretation approaches. Although techniques for interpretable machine learning are advancing quickly, some key challenges remain unsolved, and future solutions are needed to further promote the progress of this field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of three lines of interpretable machine learning techniques, taking DNN for example: Intrinsic explanation, Post-hoc global explanation of a model, and Post-hoc local explanation of a prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A traditional machine learning pipeline using feature engineering, and a deep learning pipeline using DNN based representation learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Local explanation heatmaps produced by (b) Back-propagation, (c) Mask perturbation, (d) Investigation of representations.</figDesc><graphic coords="5,497.18,99.58,50.09,50.09" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Permutation importance: a corrected feature importance measure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Toloşi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lengauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1340" to="1347" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards better understanding of gradient-based attribution methods for deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interpretability via model extraction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bastani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fairness, Accountability, and Transparency in Machine Learning Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real time image saliency for black box classifiers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dabkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6970" to="6979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human issues in the use of pattern recognition techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks and Pattern Recognition in Human Computer Interaction</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="429" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards a rigorous science of interpretable machine learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards explanation of dnn-based prediction with guided feature inversion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On attribution of recurrent neural network predictions via additive decomposition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comprehensible classification models: a position paper</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation of linguistic form and function in recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chrupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alishahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="780" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representation interpretation with spatial encoding and multimodal analytics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial detection with model interpretation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Generalized linear models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>CRC press</publisher>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="https://christophm.github.io/interpretable-ml-book/" />
		<title level="m">Interpretable Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Mudrakarta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dhamdhere</surname></persName>
		</author>
		<title level="m">Did the model understand the question? 56th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ICLR Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations. Proceedings of NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simplifying decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of man-machine studies</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="234" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anchors: High-precision model-agnostic explanations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<title level="m">Striving for simplicity: The all convolutional net. ICLR workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Interpretable to whom? a role-based model for analyzing interpretable machine learning systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tomsett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Braines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Preece</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Human Interpretability in Machine Learning (WHI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Genesim: genetic extraction of a single</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vandewiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Janssens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ongenae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">De</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Hoecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">interpretable model. NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Counterfactual explanations without opening the black box: Automated decisions and the gdpr</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interpretable convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
