<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Industrial practitioners&apos; mental models of adversarial machine learning</title>
				<funder ref="#_fgsnXV3">
					<orgName type="full">BMDW</orgName>
				</funder>
				<funder ref="#_ZphGAmx">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF) through funding for the Center for IT-Security, Privacy and Accountability</orgName>
					<orgName type="abbreviated">CISPA</orgName>
				</funder>
				<funder>
					<orgName type="full">BMK</orgName>
				</funder>
				<funder>
					<orgName type="full">FFG</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lukas</forename><surname>Bieringer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Pluribus One</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="institution" key="instit1">University of Cagliari</orgName>
								<orgName type="institution" key="instit2">University of Cagliari</orgName>
								<address>
									<addrLine>7-9</addrLine>
									<postCode>2022</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kathrin</forename><surname>Quantpi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Pluribus One</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="institution" key="instit1">University of Cagliari</orgName>
								<orgName type="institution" key="instit2">University of Cagliari</orgName>
								<address>
									<addrLine>7-9</addrLine>
									<postCode>2022</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Grosse</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Pluribus One</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="institution" key="instit1">University of Cagliari</orgName>
								<orgName type="institution" key="instit2">University of Cagliari</orgName>
								<address>
									<addrLine>7-9</addrLine>
									<postCode>2022</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Pluribus One</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="institution" key="instit1">University of Cagliari</orgName>
								<orgName type="institution" key="instit2">University of Cagliari</orgName>
								<address>
									<addrLine>7-9</addrLine>
									<postCode>2022</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Pluribus One</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="institution" key="instit1">University of Cagliari</orgName>
								<orgName type="institution" key="instit2">University of Cagliari</orgName>
								<address>
									<addrLine>7-9</addrLine>
									<postCode>2022</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Katharina</forename><surname>Krombholz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Pluribus One</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="institution" key="instit1">University of Cagliari</orgName>
								<orgName type="institution" key="instit2">University of Cagliari</orgName>
								<address>
									<addrLine>7-9</addrLine>
									<postCode>2022</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Industrial practitioners&apos; mental models of adversarial machine learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">68E07DC9E7E2C547450EB8992BA2F80A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although machine learning is widely used in practice, little is known about practitioners' understanding of potential security challenges. In this work, we close this substantial gap and contribute a qualitative study focusing on developers' mental models of the machine learning pipeline and potentially vulnerable components. Similar studies have helped in other security fields to discover root causes or improve risk communication. Our study reveals two facets of practitioners' mental models of machine learning security. Firstly, practitioners often confuse machine learning security with threats and defences that are not directly related to machine learning. Secondly, in contrast to most academic research, our participants perceive security of machine learning as not solely related to individual models, but rather in the context of entire workflows that consist of multiple components. Jointly with our additional findings, these two facets provide a foundation to substantiate mental models for machine learning security and have implications for the integration of adversarial machine learning into corporate workflows, decreasing practitioners' reported uncertainty, and appropriate regulatory frameworks for machine learning security.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Adversarial machine learning (AML) studies the reliability of learning based systems in the context of an adversary <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b68">69]</ref>. For example, tampering with some features often suffices to change the classifier's outputs to a class chosen by the adversary <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b79">80]</ref>. Analogously, slightly altering the training data enables the attacker to decrease performance of the classifier <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b71">72]</ref>. Another change in the training data allows the attacker to enforce a particular output class when a specified stimulus is present <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>. Most state-of-the-art attacks and mitigations are in an ongoing arms race <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b81">82]</ref>.</p><p>Although machine learning (ML) is increasingly used in industry, very little is known about ML security in practice. At the same time, previous works show that practitioners are concerned about AML <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b59">60]</ref>, and failures already occur <ref type="bibr" target="#b52">[53]</ref>, very little is known about ML security in practice. To tackle this question, we conduct a first study to explore mental models of AML. Mental models are relatively enduring, internal conceptual representations of external systems that originated in cognitive science <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>. In other security related areas, correct mental models have been found to ease the communication of security warnings <ref type="bibr" target="#b15">[16]</ref> or enable users to implement security best-practices <ref type="bibr" target="#b80">[81]</ref>. Mental models also serve to enable better interactions with a given system <ref type="bibr" target="#b86">[87]</ref>, or to design better user interfaces <ref type="bibr" target="#b28">[29]</ref>.</p><p>Our methodology builds upon these previous works by using qualitative methods to investigate the perception of vulnerabilities in ML applications. More concretely, we conducted 15 semi-structured interviews and drawing tasks with industrial practitioners from European start-ups and coded both drawings and the transcripts of the interviews. As the first work in this direction, we lay the foundations for practitioners' mental models of AML by describing two facets of these models. The first concerns the separation of ML related security (AML) and security unrelated to ML (non-AML security). In many cases, the borders between these two fields are blurry: a participant may start talking about evasion and finish the sentence with a reference to cryptographic keys. The second facet concerns the view of the ML model within a project. In contrast to the focus on an isolated model in AML research <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b68">69]</ref>, our practitioners often describe one or more pipelines with potentially several applications of ML. Finally, we found more facets which are left for an in-depth arXiv:2105.03726v4 [cs.CR] 29 Jun 2022 data design training model deployment poisoning <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b71">72]</ref>, backdooring <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref> model stealing <ref type="bibr" target="#b82">[83]</ref> model reverse engineering <ref type="bibr" target="#b66">[67]</ref> membership inference <ref type="bibr" target="#b73">[74]</ref> evasion <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b79">80]</ref> adversarial reprogramming <ref type="bibr" target="#b26">[27]</ref> adversarial initialization <ref type="bibr" target="#b31">[32]</ref> weight perturbations <ref type="bibr" target="#b20">[21]</ref> sponge attacks <ref type="bibr" target="#b74">[75]</ref> Figure <ref type="figure">1</ref>: AML threats within the ML pipeline. Each attack is visualized as an arrow pointing from the step controlled to the point where the attack affects the pipeline.</p><p>investigation by future work. These include the application setting, prior education, and the perceived relevance of AML.</p><p>Our interviews showed that most of our participants lack an adequate and differentiated understanding to secure ML systems in production. At the same time, more than a third of our participants feels insecure about AML. These concerns seem justified as we found evidence for semi-automated fraud on ML systems in the wild. However, our findings have more practical implications. Our results allow us to address the current lack of understanding by (I) increasing awareness for AML and decreasing uncertainty about AML, (II) developing tools that help practitioners to assess and evaluate security of ML applications, and (III) drafting regulations that contain adequate security assessments and reduce insecurity about AML. However, more work is needed to understand the individual and shared mental models of practitioners and assess the real world security risks when applying ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and related work</head><p>In this section, we review related work on AML and recall different attacks that have recently been discussed. We also review literature on mental models with regard to humancomputer interaction, usable security and ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adversarial machine learning</head><p>AML studies the security of ML algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b68">69]</ref>. We attempt to give an informal overview of all attacks in AML, and additionally illustrate them in Figure <ref type="figure">1</ref>.</p><p>Poisoning/backdooring. Early works in poisoning altered the training data <ref type="bibr" target="#b71">[72]</ref> or labels <ref type="bibr" target="#b10">[11]</ref> to decrease accuracy of the resulting classifier, for example SVM. For deep learning, due to the flexibility of the models, introducing backdoors is more common <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>. Backdoors are chosen input patterns that reliably trigger a specified classification output. Defending such backdoors has lead to an arms race <ref type="bibr" target="#b81">[82]</ref>.</p><p>Evasion/adversarial examples. Early work in evasion decreased the test-time accuracy of spam classification <ref type="bibr" target="#b23">[24]</ref>. It was later shown that also more complex models change their output for small, malicious input perturbations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b79">80]</ref>. Albeit all classifiers are principally vulnerable towards evasion, recent works focus on the arms race in deep learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Membership inference. After first inferring attributes <ref type="bibr" target="#b3">[4]</ref> of the training data, research later showed that entire points can be leaked from a model <ref type="bibr" target="#b73">[74]</ref>. More concretely, the attacker deduces, given the output of a trained ML model, whether a data record was part of the training data or not. As for other attacks, numerous defenses are being proposed <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>Model stealing. Tramèr et al. <ref type="bibr" target="#b82">[83]</ref> recently introduced model stealing. During this attack, the attacker copies the ML model functionality without consent of the model's owner. The attacker, given black box access to the original model, tries to reproduce a model with similar performance. As for the previous attacks, mitigations have been proposed <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b67">68]</ref>.</p><p>Weight perturbations. Fault tolerance of neural networks has long been studied in the ML community <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b64">65]</ref>. Recently, maliciously altered weights are used to introduce a specific backdoor <ref type="bibr" target="#b34">[35]</ref>. Few works exist to defend malicious change to the weights in general, not only related to backdoor introduction <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b87">88]</ref>.</p><p>For the sake of completeness, we conclude with a description of additional, recent attacks, some of which are part of our questionnaires (see Appendix D.3). In adversarial initialization, the initial weights of a neural network<ref type="foot" target="#foot_0">foot_0</ref> are targeted to harm convergence or accuracy during training <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b53">54]</ref>. In adversarial reprogramming, an input perturbation mask forces the classifier at test time to perform another classification task than originally intended <ref type="bibr" target="#b26">[27]</ref>. For example, a cat/dog classifier is reprogrammed to classify digits. In model reverse engineering, crafted inputs allow to deduce from a trained model the usage of dropout and other architectural choices <ref type="bibr" target="#b66">[67]</ref>. Fi-nally, sponge attacks aim to increase energy consumption of the classifier at test time <ref type="bibr" target="#b74">[75]</ref>.</p><p>Practical Relevance of AML. In general, AML research has been criticized for the limited practical relevance of its threat models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref>. A possible reason is our lack of knowledge about AI security in practice. Few works attempt to tackle this gap, including for example Lin and Biggio <ref type="bibr" target="#b52">[53]</ref>. They give an overview about AI attacks that were carried out in practice based on AI related incidents covered in newspapers. Furthermore, Boenisch et al. <ref type="bibr" target="#b13">[14]</ref> conducted a survey and developed an awareness score, which however encompasses AML, privacy, and non-AML security. Concerning which threats are relevant in practice in industry, Kumar et al. <ref type="bibr" target="#b46">[47]</ref> and Mirsky et al. <ref type="bibr" target="#b59">[60]</ref> found that practitioners are most concerned about model theft and poisoning. Yet, in academia, most work focused on evasion so far. To shed more light on AML in practice, we interview industrial practitioners and take a first step towards a theory of mental models of AML. To this end, we now introduce and review mental models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mental models</head><p>Mental models are relatively enduring and accessible, but limited, internal conceptual representations of external systems <ref type="bibr" target="#b25">[26]</ref> that enable people to interact with given systems. Hence, the field of human computer interaction (HCI) studied this concept quite early <ref type="bibr" target="#b72">[73]</ref>. Mental models, most recently, saw an increasing relevance in usable security. We now recall prior application scenarios and highlight relevant conceptual contributions in the context of security and ML.</p><p>Mental models in HCI and usable security. The relevance of mental models has been subject to a lengthy debate in HCI research <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b84">85]</ref>. In many cases, the focus was to capture, depict and analyze mental models of specific objects of investigation. Examples of topics include, but are not limited to, the design of online search applications <ref type="bibr" target="#b6">[7]</ref>, interface design <ref type="bibr" target="#b43">[44]</ref>, and interfaces for blind people <ref type="bibr" target="#b24">[25]</ref>. Research in usable security has recently focused on mental models of security in general <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b86">87]</ref>, privacy in general <ref type="bibr" target="#b70">[71]</ref>, security warnings <ref type="bibr" target="#b15">[16]</ref>, incident response <ref type="bibr" target="#b69">[70]</ref>, the internet <ref type="bibr" target="#b40">[41]</ref>, the design of security dashboards <ref type="bibr" target="#b57">[58]</ref>, the Tor anonymity network <ref type="bibr" target="#b28">[29]</ref>, privacy and security in smart homes <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b89">90]</ref>, encryption <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b88">89]</ref>, HTTPS <ref type="bibr" target="#b44">[45]</ref>, and cryptocurrency systems <ref type="bibr" target="#b56">[57]</ref>.</p><p>With regard to the respective object of investigation, these contributions paved the way for improvements of user interface designs <ref type="bibr" target="#b28">[29]</ref>, adequate security communication <ref type="bibr" target="#b15">[16]</ref>, as well as the development of security policies and implementation of best-practices <ref type="bibr" target="#b80">[81]</ref>. It has been argued that security mental models contain structural and functional properties <ref type="bibr" target="#b88">[89]</ref>. For each application, users develop a cognitive representation of its inherent components, their interconnection and correspondingly possible security threats. This representation helps them to understand where threats could emerge and how they could take effect. Mental models evolve dynamically upon individual interaction with a given application <ref type="bibr" target="#b12">[13]</ref>.</p><p>Mental models in ML. In order to interact with an ML application, humans need a mental model of how it combines evidence for prediction <ref type="bibr" target="#b65">[66]</ref>. This is all the more important for ML-based applications which often inherit a certain opacity. As Lage et al. <ref type="bibr" target="#b47">[48]</ref> pointed out, the number of necessary cognitive chunks is the most important type of complexity in order to understand applications. During interaction with black-box processes, humans strive for reduced complexity which may lead to the development of inaccurate or oversimplified mental models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>A dedicated line of research therefore elaborates on the relevance and nature of mental models in the context of explainable artificial intelligence. Mental models have been found to serve as scaffolds not only for a given ML application <ref type="bibr" target="#b83">[84]</ref>, but also for its embedding in organizational practices <ref type="bibr" target="#b90">[91]</ref>. For data science teams, these workflows usually consist of predefined steps (Figure <ref type="figure">1</ref>) and necessitate interpersonal collaboration <ref type="bibr" target="#b61">[62]</ref>. Following Arrieta et al. <ref type="bibr" target="#b2">[3]</ref>, we argue that individual collaborators within these teams (e.g., ML engineers, software engineers) develop separate internal representations of a given workflow or application. The need for appropriate mental models thereby increases with the enlarged scope of ML applications <ref type="bibr" target="#b48">[49]</ref> and involved stakeholders <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b78">79]</ref>.</p><p>Recent work in this line of research called for qualitative studies at the intersection of the HCI and ML communities, to better understand the cognitive expectations practitioners have on ML systems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>. Suchlike studies seem all the more relevant as various industry initiatives propagate a humancentric approach to AI, explicitly referring to mental models. <ref type="foot" target="#foot_1">2</ref>However, the current scientific discourse lacks a dedicated consideration of cognition in AML. In order to fill this gap, we present the first qualitative study to elicit mental models of adversarial aspects in ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section describes the design of our semi-structured interviews, the drawing task, our recruiting strategy, the participants, and the data analysis. Our methodology was designed to investigate the perception of ML security and is, to the best of our knowledge, the first mental model study of AML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Study design and procedure</head><p>To assess participants' perceptions, we conducted semistructured interviews enriched with drawing tasks. We draw inspiration from recent work in usable security which also investigated mental models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b88">89]</ref>.</p><p>Before the interview, participants were informed about the general purpose of our study and the applied privacy measures. We further assured each participant that their answers would not be judged. Participants were then instructed to complete a questionnaire on demographics, organizational background and a self-reflected familiarity with field-related concepts (Appendix D) before the interview. This questionnaire was filled with or without the authors' presence. The answers have later been used to put participants' perceptions in context to their organizational and individual background.</p><p>The threefold structure of our interviews covered 1) a specification of a given ML project a participant was involved in, 2) the underlying ML pipeline of this project and 3) possible security threats within the project. We chose this approach as the different attack vectors form part of the ML-pipeline as shown in Section 2.1. The detailed interview guideline can be found in Appendix C. As a last step of our interviews, we confronted the participants with exemplary attacker models for some of the threats considered relevant in industrial application of ML <ref type="bibr" target="#b46">[47]</ref>. To assess practitioners' understandings of these threats, study participants had to elaborate on these attack vectors within their specific setup (Appendix D.2).</p><p>To assess the participants' knowledge about (A)ML in general, participants were asked to fill an additional questionnaire after the interview (Appendix D.3). In this questionnaire, we tested general knowledge in ML and independently asked for a self-reflected familiarity rating with some of the attacks we discussed in Section 2.1. This questionnaire was handed to the participants after the interview as to avoid priming.</p><p>We conducted one pilot interview to evaluate our study design. This first participant met all criteria of our target population in terms of employment, education and prior knowledge. As his explanations and drawings matched our expectations, we only added a specific question regarding the collaborators within a given ML-based project.</p><p>The average interview lasted 40 minutes and was jointly conducted by the first two authors of this paper between April and July 2019. To minimize interviewer biases, we equally distributed the interviews, where one author was the lead interviewer and the other took notes. Due to the COVID-19 pandemic, interviews were conducted remotely and relied on a freely available digital whiteboard<ref type="foot" target="#foot_2">foot_2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recruitment</head><p>Recruitment for a study on applied ML in corporate environments presents a challenge, as only a small proportion of the overall population works with ML. Furthermore, the topic touches compliance and intellectual property of participating organizations. Hence, many companies are skeptical about the exchange with third parties. Consequently, many current contributions with industrial practitioners as study participants are conducted by corporate research groups (e.g., <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref>).</p><p>We tried to initiate interviews with two large multinational companies. Unfortunately, both denied our request after internal risk assessments. Therefore, we focused on smaller companies where we could present our research project directly to decision-makers and convince them to participate in our study. We relied on the authors' networks (pilot participant, P11) and public databases for start-ups (more details in Appendix A) to find potential participants and used directmessaging on LinkedIn and emails to get in contact.</p><p>Recruitment of study participants happened in parallel to interview conduction. Some participants forwarded our interview request to internal colleagues, so that we talked to multiple employees of some participating companies (see Table <ref type="table" target="#tab_0">1</ref>). We aimed to recruit experienced and knowledgeable participants and hence our requirements were a background in ML or computer science and positions such as data scientists, software engineers, product managers, or tech leads. We did not require any prior knowledge in security. After 8 interviews, no new topics (in our case for example new pipeline elements, whether defenses were mentioned, or how attacks were depicted in drawings) emerged. The research team thus agreed after 15 interviews that saturation was reached <ref type="bibr" target="#b14">[15]</ref>, and we stopped recruiting. The participants were randomly assigned an ID (a number between 1 and 20) which was used throughout our analysis. All participants were offered an euro 20 voucher as compensation for their time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Participants</head><p>We summarize demographic information in Table <ref type="table" target="#tab_0">1</ref>. One participant, P10, did not hand in the questionnaire and is consequently not included in the following statistics. 14 participants identified as male, one identified as female, our sample is thus skewed towards males when considering ML practitioners <ref type="bibr" target="#b39">[40]</ref>. As previous work found security perception of women and men to exhibit only some differences <ref type="bibr" target="#b58">[59]</ref>, this bias is acceptable for a first exploration but should be studied in depth in future work. Our participants had an average age of 34 years (standard deviation (STD) 4.27). As intended for a first exploration of practitioners' perception of AML, our sample covered various application domains and organizational roles which we now describe in detail.</p><p>Education and prior knowledge. The majority of participants (9 of 14) has a PhD, with all participants holding some academic degree. While our sample skews towards PhDs compared to the overall population of ML practitioners <ref type="bibr" target="#b39">[40]</ref>, previous work reports no correlation between overall education and security awareness <ref type="bibr" target="#b13">[14]</ref>. Most participants (12 of 14) reported that they had attended lectures or seminars on ML. Roughly half (6 of 14) reported to have a similar background in security. To obtain a more objective measure we conducted a test about ML knowledge and asked participants to rate their familiarity with AML attacks (details in Appendix B). While we found that all participants were indeed knowledgeable in ML, we found that few attacks were well known to them.</p><p>Employment. Regarding the size of the companies, four participants worked in companies with less than ten employees, five in companies with less than 50 and the remaining six participants in companies with less than 200 employees. The companies' application areas were as diverse as healthcare, security, human resources, and others. Most participants were working in their current positions 6 years (STD 4.9). Their roles were diverse: Most (8 of 15) were in managing positions. Three were software or ML engineers, three more researchers. One of the participants stated to be both a researcher and a founder. One participant did not report his role.</p><p>Finally, we asked participants to report which goals were part of their companies' AI/ML checklist. Almost all participants (13 of 14) reported that performance mattered in their company. Half (7 of 14) stated that privacy was important. Slightly less than half (6 of 14) focused on explainability and security. Least participants (4 of 14) listed fairness as a goal in their products. To conclude, when interpreting these numbers, one should keep in mind that not all five goals apply equally to all application domains. Furthermore, our sample is too small to derive per area or per company insights, and we thus leave a detailed analysis for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data analysis</head><p>We adopted an inductive approach, where we followed recent work in social sciences and usable security that constructed theories based on qualitative data <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b62">63]</ref>. To distill observable patterns in interview transcripts and drawings, we applied two rounds of open coding, e.g. we assigned one or several codes to sentences, words, or parts of the drawings. We then performed Strauss and Corbin's descriptive axial coding to group our data into categories and selective coding to relate these categories to our research questions <ref type="bibr" target="#b76">[77]</ref>. Throughout the coding process, we used analytic memos to keep track of thoughts about emerging themes. The final set of codes for interview transcripts and drawings is listed in Appendix E.</p><p>As a first step, the first two authors independently conducted open coding sentence by sentence and sketch by sketch. This allowed for the generation of new codes without predefined hypotheses. Afterwards, the resulting codes were discussed and the research team agreed on adding specific codes for text snippets relating to the confusion of standard security and AML. As a second step, two coders independently coded the data again. After all iterations of coding, conflicts were resolved and the codebook was adapted accordingly.</p><p>During axial coding, the obtained codes were grouped into categories. The first two authors independently came up with proposed categories which have then been discussed within an in-person meeting. While the grouping was undisputed for some of the categories presented in Appendix E (e.g. AML attacks, pipeline elements), for others the research team decided for (e.g. confusion, relevance) or against (e.g. type of ML model applied) the inclusion of a corresponding category only after detailed discussion. In addition, dedicated codes for the perception of participants (e.g. perceives AML as a feature, not a bug or security issue) were added to the codebook. Once the research team agreed on a final codebook, all transcripts and drawings were coded again using corresponding software. <ref type="foot" target="#foot_3">4</ref> In doing so, we aimed for inferring contextual statements instead of singular entities.</p><p>The codes and categories served as a baseline for selective coding. Independently, the researchers came up with observations and proposals for specific mental models. Every proposal included a definition of the observation, related codes, exemplary quotes and drawings. The first two authors then met multiple times to discuss the observations and the corresponding relations of codes and categories. The resulting code tree contains 77 interview codes in 12 groups, 44 for drawings (in 5 groups), as depicted in Appendix E.</p><p>Over all interviews, the coders agreed on 989 codes while disagreeing on 136. Analogously, there were 275 codes on drawings in total, with 42 disagreements. We further calculated Cohen's kappa <ref type="bibr" target="#b22">[23]</ref> to measure the level of agreement among the coders. For interview transcripts, we reached κ = 0.71; for the codes assigned to drawings κ = 0.85. These values indicate a good level of coding agreement since both values are greater than 0.61 <ref type="bibr" target="#b49">[50]</ref>. Given the semi-technical nature of our codebook, we consider these values as substantial inter-coder agreement. Irrespective of this and in line with best practices in qualitative research, we believe that it is important to elaborate how and why disagreements in coding arose and disclose the insights gained from discussions about them. Each coder brought a unique perspective on the topic that contributed to a more complete picture. Due to the diverse background of our research team in AML, usable security and economic geography, most conflicts arose regarding the relevance of technical and organizational elements of transcripts and drawings. These were resolved during conceptual and on-the-spot discussions within the research team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Expectations of mental models</head><p>Given previous work on mental models and ML, we designed our study in a way that participants would first visualize their pipeline and later add corresponding attacks and defenses. For the pipeline, we expected that participants would name basic steps or components, such as data (collection), training, and testing. In general, we assumed participants' descriptions would vary in technical detail. Regarding AML, one of our motivations to conduct this study was to learn which knowledge our participants had. As a recent phenomenon, AML might not be known at all in practice, although practitioners might be aware of attacks relevant to their specific application. In particular, we did not expect practitioners to depict attacks using a starting and target point, as done in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ethical considerations</head><p>The ethical review board of our university reviewed and approved our study design. We limited the collection of personal data as much as possible and used ID's for participants throughout the analysis. Since all participants were employed at existing companies and partially shared business-critical information, we aimed to avoid company-specific disclosures in this paper. Finally, we complied with both local privacy regulations and the general data protection regulation (GDPR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical results</head><p>In this section, we discuss our findings from the interviews and drawings. Given the unexplored nature of mental models of AML, we focus on two main facets, and discuss additional findings that require a more in depth analysis (in the sense of future work) at the end of this section.</p><p>The first of the two main facets is the (mingled) relationship between ML security (AML) and security unrelated to ML (non-AML security). We found that our participants, while not referring to AML and non-AML security interchangeably, still exhibited an often vague boundary between the two topics. The second facet concerns the view on ML as part of a larger workflow or product in industry, as opposed to the focus on an isolated model in academia. As a description of a high level workflow requires a high level perspective, we investigate whether it is equivalent to one, which we find not to be true. Afterwards, we then discuss potential facets requiring a more in depth investigation: the application setting, prior knowledge of the participant, and the perceived relevance of AML. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Non-AML security and AML</head><p>Non-AML security deals with the protection against digital attacks in general. In our case, it encompasses topics like access control, cryptography, malicious code execution, etc. Non-AML security provides sound solutions by deploying defenses or implementing design choices. In AML, threats are much more connected with the functioning of ML. For many AML attacks, it is unclear which defenses work due to the ongoing arms-race. Although both topics are conceptually different, we found that our participants did not distinguish between security unrelated to ML and AML, as visualized in Figure <ref type="figure" target="#fig_0">2</ref>. In our interviews, on the one hand, the boundary between non-AML security and AML often appeared blurry or unclear, with the corresponding concepts intertwined. On the other hand, there were crucial differences in the perception between non-AML security and AML threats. One difference is that whereas security defenses were often clearly stated as such, AML mitigations <ref type="foot" target="#foot_4">5</ref> were often applied without security incentives. Finally, we find a tendency to not believe in AML threats. Many participants denied responsibility, doubted an attacker would benefit, or stated the attack does not exist in the wild. There was no such tendency in non-AML security.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Mingling AML and non-AML security</head><p>We first provide examples showing that non-AML security and AML were not distinguished by our participants. Afterwards, we investigate if non-AML security and AML are used interchangeably, by investigating the co-occurrence of codes.</p><p>Vagueness of the boundary between security and AML. There are many examples for a vague boundary between non-AML security and AML. For example P20 reasoned about evasion: "this would require someone to exactly know how we deploy, right? and, where we deploy to, and which keys we use." At the beginning, the scenario seems unclear, but the reference to (cryptographic) keys or access tokens shows that the participant has moved to classical security. Analo-gously, when P18 reasoned about membership inference: "but that could be only if you break in [...] if you login in to our computer and then do some data manipulation." Again, this participant was reasoning about failed access control as opposed to an AML attack via an API. Sometimes, ambiguity in naming confused our participants. For example, P11 thought aloud: "poisoning [...] the only way to install a backdoor into our models would be that we use python modules that are somewhat wicked or have a backdoor." In this case, the term 'backdoor' in our questionnaire caused a non-AML security mindset involving libraries in contrary to our original intention to query participants about neural network backdoors. The same reasoning can also be seen in P11's drawing (compare Figure <ref type="figure" target="#fig_1">3</ref>), where 'backdoor' points to python modules. Finally, P12 stated: "maybe the poisoning will be for the neural network. From our point of view you would have to get through the Google cloud infrastructure." From an AML perspective, the attack is carried out via data which is uploaded from the user. Yet, the infrastructure is perceived as an obstacle for the attack.</p><p>Correlations between non-AML security and AML attacks. In the previous paragraph, we showed that the boundaries between AML and non-AML security are blurred in our interviews. Another example is P6 reasoning about IP loss: "we are very much concerned I'd say the models themselves and the training data we have that is a concern if people steal that would be bad." In this case, it is left out how the attack is performed. Analogously, P9 remarked: "We could of course deploy our models on the Android phones but we don't want anybody to steal our models." To investigate whether our participants are more concerned about some property or feature (data, IP, the model functionality) than about how it is stolen or harmed, we examined the co-occurrence of AML and non-AML security codes that refer to similar properties in our interviews. For example, the codes 'model stealing' and 'code breach' both describe a potential loss of the model (albeit the security version is broader). Both codes occur together six times, with 'code breach' being tagged one additional time. Furthermore, the code 'model reverse engineering', listed only two times, occurs both times with both 'model stealing' and 'code breach'. However, not all cases are that clear. For example 'membership inference' and 'data breach' only occur together two times. The individual codes are more frequent, and were mentioned by three ('membership inference') and eleven ('data breach') participants. Analogously, attacks on availability (such as DDoS) in ML and non-AML security were only mentioned once together. Such availability attacks were brought up in an ML context twice, in non-AML security four times. Codes like 'evasion' and 'poisoning', in contrast, are not particularly related to any non-AML security concern. We conclude that AML and security are not interchangeable in our participants' mental models to refer to attacks with a shared goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Differences between AML and non-AML security</head><p>In the previous subsection, we found that our participants did not distinguish non-AML security and AML. To show that this is not true in general, we now focus on the differences between the two topics. To this end, we start with the perception of defenses and then consider the overall perception of threats in AML and security. We conclude with a brief remark on the practical relevance of AML.</p><p>Defenses. Out of fifteen interviews, in thirteen some kind of defense or mitigation was mentioned; whereas all corresponding interviewees mentioned a non-AML security defense (encryption, passwords, sand-boxing, etc). An AML mitigation appeared in eight. In contrast to security defenses, however, AML defenses were often implemented as part of the pipeline, and not seen in relation to security or AML. As an example, P9, P15, and P18 reported to have humans in the loop, however not for defensive purposes. P10 and P16 were aware that this makes an attack more difficult. For example, P16 stated: "maybe this poisoning of the data [...] is potentially more possible. There, we would have to manually check the data itself. We don't [...] blindly trust feedback from the user." Analogous observations hold techniques like explainable models (3 participants apply, 1 on purpose) or retraining (2 apply, additional 2 as mitigation). For example, P14 said: "when we find high entropy in the confidences of the data [...] for those kind of specific ranges we send them back to the data sets to train a second version of the algorithm." In this case, retraining was used to improve the algorithm, not as a mitigation. We conclude that albeit no definite solution to vulnerability exists, many techniques that increase the difficulty for an attacker are implemented by our participants. At the same time, many practitioners are unaware which techniques potentially make an attack harder.</p><p>Perception of threats. There is also a huge difference in the perception of threats in non-AML security and AML. In security, threats were somewhat taken for granted. For example, P9 was concerned about security of the server's passwords "because anybody can reverse-engineer or sniff it or something." Analogously, P6 said to pay attention to "the infrastructure so that means that the network the machines but also the application layer we need to look at libraries." On the other hand, almost a third of our participants (4 of 15) externalized responsibility for AML threats. For example, P3 said their "main vulnerability from that perspective would probably be more the client would be compromised." Analogously, P1 remarked that ML security was a "concern of the other teams." In both cases, the participants referred to another entity, and reasoned that they were not in charge to alleviate risks. Other reasons not to act include participants not having encountered an AML threat yet, and concluded AML was not relevant. More concretely, P9 remarked: "we also have a community feature where people can upload images. And there could be some issues where people could try to upload not safe or try to get around something. But we have not observed that much yet. So it's not really a concern, poisoning." Roughly half of the participants (7 of 15) reported to doubt the attackers' motivation or capabilities in the real world. For example, P1 said: "I have a hard time imagining right now in our use-cases what an attacker might gain from deploying such attacks." P20, who worked in the medical domain, stated: "I'm left thinking, like, why, what could you, achieve from that, by fooling our model. I'm not sure what the benefit is for whoever is trying to do that." Finally, many participants (9 of 15) believed that they have techniques in place which function as defenses. As an in-depth evaluation of which mitigations are effective in which setting is beyond the scope of this paper, we leave it for future work.</p><p>Practical relevance of AML. The fact that most participants did not consider AML threats relevant might be an expression of these threats being academic and not occurring in practice. Yet, our interviews showed that there are already variants of AML attacks in the wild. More concretely, P10 stated: "What we found is [...] common criminals doing semiautomated fraud using gaps in the AI or the processes, but they probably don't know what AML, like adversarial machine learning is and that they are doing that. So we have seen plenty of cases are intentional circumventions, we haven't quite seen like systematic scientific approaches to crime." Our participants lack of concern might then be an indicator that harmful AML attacks are (still) rare in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Summary</head><p>We found that non-AML security and AML were mingled in our participants' mental models: the boundaries between the corresponding threats were often unclear. Yet, security and AML were not interchangeably used to refer to attacks with a shared goal. Furthermore, non-AML security threats were treated differently than AML threats: the latter were often considered less relevant. Whereas it remains an open question whether AML and non-AML security should be treated differently in practice, the fact that they are currently Model M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AML Research Our Findings</head><p>Figure <ref type="figure">4</ref>: High-level intuition of Section 4.2. While AML research studies individual models, our participants often describe workflows with potentially several models, sometimes even the embedding system of the ML project.</p><p>poorly distinguished might due to low exposure to AML. At the same time, our interviews provided evidence for AML attacks in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ML models and ML workflows</head><p>Many of our participants did not only refer to an ML model, but discussed a workflow or an entire system. This is in stark contrast to AML research, where models are often studied in isolation, possibly due to a lack of available data. This finding is visualized in Figure <ref type="figure">4</ref>. In this subsection, we first discuss our participants view on ML models and the described systems. We then investigate whether such views are equivalent to a high level view on ML related projects, and conclude the section with a short discussion on some of our participants' struggles to assess threats at a high level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Model versus system view</head><p>We first focus on the description of the ML model itself. Afterwards, we describe practitioners' views of ML models within larger systems and conclude the section with relating both findings to the technical level of abstraction. ML model perspective. The general perception of the ML pipeline (Figure <ref type="figure">1</ref>) seems to affect mainly the relevance of ML-models as such within the pipeline. More concretely, participants talked about models as pipeline components. Many (11 of 15) of our participants presented their projects in chronological order or with an implicit flow. Examples are visible in Figure <ref type="figure" target="#fig_1">3</ref> or Figure <ref type="figure" target="#fig_3">6</ref>. Moreover, 6 out of 15 participants explained a pipeline not only as being composed by several steps, but remarked potentially several applications of ML within, or that several (different) pipelines exist. For example, P14 reported that "the models are chained one after the other," and P7 stated that "we have both like unsupervised training and unsupervised training." We conclude that often there is not a single model deployed, but data may be processed by several models, potentially in sequential order. System perspective. Moreover, participants showed a strong focus on the surrounding or embedding of their MLbased project. In other words, not only the pipeline around the model was important, but also the surrounding infrastructure of the project. Out of 15 participants, 5 described their ML pipeline as a classifier as embedded into the larger project context (for example visible in Figure <ref type="figure" target="#fig_1">3</ref> or Figure <ref type="figure" target="#fig_2">5</ref>). Related to this embedding, in two of the interviews, the topic of technical debt (or long-term maintenance) arose. In this context, P6 stated: "how [...] we can also have to something that is maintainable in the long term."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Technical abstraction level</head><p>The previous findings suggest a high level of technical abstraction in our interviews. While this is true on average, some (5 of 15) participants described their project minutely. For example P12 described their application almost at the code level: "[...] we want to have for each node, that is basically the union of those two columns [...]." However, whereas the same participants also described their project as a workflow, they did not talk about the embedding of the project. On the other hand, P18 remarked on their "supervising" (e.g., high level) perspective, yet provided no context. We conclude that our sample does not allow conclusions about the level of technical abstraction and perspective on ML model, which is thus left for future work. We did find, however, that a high level perspective seemed to make threat assessment harder for at least some participants. Asked to specify a certain threat model, P19 stated for example: "It's like everywhere. Internal threats, external threats. Trying to mess with the communication, trying to mess if we model something." In a similar manner, P14 explained that an adversary could "try to put some pythons in non conforming ways to trigger networks." Both descriptions are hard to interpret in technical terms, although both participants seemed aware of security threats in general. The same problem persists for defenses that our participants apply to encounter AML-specific security threats. P18, for example, first explained that "the countermeasures are all in the API." After rechecking the documentation, the participant was able to provide further details on the applied defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Summary</head><p>Our findings illustrate an important point which at the same time is very intuitive. Whereas most research papers focus on a single model when investigating ML security, in practice, models are trained and deployed in the context of other models or as components of larger workflows. At the same time, one pipeline may also contain several applications of ML. These views are not to be confused with the technical detail of a projects' description. We furthermore find evidence that the right level of detail is crucial to providing useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Additional facets of mental models</head><p>Eliciting mental models with only fifteen interviews seems ambitious, in particular in the context of a technique so versatile as ML. In the following, we thus discuss potential aspects of mental models that have to be studied in more depth in future work. These aspects include, but are not limited to the application setting, the effect of prior knowledge, and the perceived relevance of AML. We also found evidence of structural and functional components in our participant's mental models. As the occurrence of these in AML mental models can be anticipated from prior work in mental models <ref type="bibr" target="#b88">[89]</ref>, we leave the corresponding discussion to Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Application setting</head><p>Our sample is too small to make general statements about the application area. However, since almost a third (4 of 15) participants work in cybersecurity, we attempt to investigate whether working in security affects sensitivity to AML. Hence, we first divide the participants into security and nonsecurity groups, starting with participants working in securityrelated fields. P10, who worked in a setting with cybersecurity reported: "there is some standard AML attacks on ML you can use, but we design our system knowing that very well; on the other hand, we know that there is no perfect security, so, again defense is in monitoring and vigilance, but it's not something that can be fully automated in our opinion." P10 was in general very sensitive towards AML. P4, also from a cybersecurity setting, was less concerned about evasion: "I can't imagine yet how it can be applied for real life, for example [...] since we are pretty close on our development." Yet, P4 also stated the need to gather more information about AML. Hence, also participants who worked in security-related areas had diverse mental models with respect to concrete attacks.</p><p>Participants from non-security fields have similarly diverse mental models. This diversity is also reflected in the drawings. P11 (Figure <ref type="figure" target="#fig_1">3</ref>) added some attacks (in red) before we provided explanations of evasion, backdooring and membership inference (added in blue). P18 (Figure <ref type="figure" target="#fig_3">6</ref>), on the other hand, did not add any threats in their drawing. Analogously, opinions also differ in the interviews; e.g., P15 who worked in an non-security setting, was aware of security issues: "one interesting thing of course is that the solution is in some ways constraint by adversarial security considerations so for example you cannot use natural language generation very much because of potential adversarial behavior." On the other hand, and confirming the drawing, P18 reported that "we do not really protect the machine learning part." Investigating the diversity of mental models induced by the application area in more depth is thus left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Prior knowledge</head><p>Another potential factor on a practitioner's mental model is knowledge about or exposure to the topic at hand. However, we find no strong relation between education and capability or knowledge about AML in our sample. For example, one participant self-reported high knowledge in AML, but also stated: "maybe the poisoning will be for the neural network." Here, a general attack, poisoning, is related to a specific model (neural networks). On the other end of the spectrum, P9 did not self-report any knowledge about security or AML, but correctly remarked: "Somebody could send us 100.000 images and collect all the results and try to build a model from that." We conclude that in our sample, self-reported prior knowledge is not related to AML knowledge. Yet, more work is needed to understand more in depth the complex relationship between exposure, education, and mental models of AML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Perceived relevance of AML</head><p>Last but not least, we found little awareness of AML in our sample. As already discussed in section 4.1.2, this might be a consequence of little exposure to AML attacks in the wild. On the other hand, we found all levels of concern about AML in our sample. More concretely, a third of our the practitioners (5 of 15) did not mention AML at all before we explicitly asked. Another third reported that they were not very concerned about AML. For example, P1 stated that evasion, or "injecting malicious data to basically make the model [...] predict the wrong things" was "a concern that is not as high on my priority list." P15, analogously, said: "mainly the machine learning pipeline this is the less critical security problem," reasoning that "simply a performance would be unexpected." Yet, over a third (6 of 15) of the participants reported to feel insecure about AML when confronted with the topic. Of these six participants, two previously showed low priority on AML, and three did not mention AML at all. An example of insecurity is P4, who stated they needed "some more research on it." Some participants, like P19, were concerned about specific attacks: "I maybe need to learn more about this membership." In summary, some practitioners consider AML threats important, whereas some participants did not know AML well, and yet others did not consider it an important threat. From each of these three groups, there was at least one participant that felt not well informed. After the interviews (e.g., off the record) some participants stated that their awareness for AML had increased due to the interview. Many also inquired about defenses against specific threats, further confirming that they were indeed concerned about specific attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future work</head><p>Our findings expose the lack of knowledge about AML in practice, and thus show the need for additional research at the intersection of AML and cognitive science. In this section, we summarize these potential directions of future work. We first discuss theoretical research on mental models of AML and secondly more practical research that applies findings derived from mental models to AML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">A theory on mental models of AML</head><p>Our work is a first step to describe mental models of AML. For well-grounded mental models, more research is needed to investigate different aspects, as discussed in the previous section about the technical detail, application area and prior knowledge, for example. However, more research is also required concerning the development of mental models, and how a user based threat taxonomy (as opposed to a research based taxonomy) could look like.</p><p>Temporal evolvement of mental models of AML. A better understanding about the development of individual mental models could help to assess necessary steps to make practitioners take into account AML. In addition, research on how mental models are shared between various AI practitioners might help to implement adequate defenses within and across corporate workflows. Corresponding starting points can be found in cognitive science <ref type="bibr" target="#b60">[61]</ref>, where the convergence of mental models has been studied as a three-phase process of orientation, differentiation and integration <ref type="bibr" target="#b42">[43]</ref>.</p><p>Inherent threat taxonomies of mental models. Whereas academia has proposed clear threat models in ML security, it is unclear whether or to which degree these are also used or useful in practice. In this context, it could be interesting to consider existing taxonomies by Biggio et al. <ref type="bibr" target="#b9">[10]</ref> and Barreno et al. <ref type="bibr" target="#b5">[6]</ref>. These frameworks seem promising to investigate which specific structural elements practitioners consider relevant for specific attack vectors and how they perceive the causal evolution of these attacks. In line with recent work by Wang et al. <ref type="bibr" target="#b85">[86]</ref>, such user-centric attack taxonomies might help to understand practitioners' reasoning on AML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Applying mental models to AML</head><p>Secondly, but not less important, is the question how AML research can benefit from the study of mental models and which problems could be tackled in this context. Examples include the usability of AML tools and libraries, a more realistic threat modelling in AML research as well as a general assessment of AML attacks in the wild.</p><p>Utility and usability of AML tools and libraries. We found that practitioners' mental models depend on available and provided information. Future research should therefore elaborate on the needed specificity of the available information. Furthermore, an evaluation of the available AML tools and libraries with regards to capabilities and needs of industrial practitioners might ease their usage across application domains. In line with recent work on fairness <ref type="bibr" target="#b51">[52]</ref> and ethics <ref type="bibr" target="#b21">[22]</ref>, we consider this crucial for designing usable and accessible tools, corporate guidelines and regulations.</p><p>Practical threat modelling for AML research. As stated in Section 2, AML research has been criticized for the limited practical relevance of its threat models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref>. Mental models could alleviate this issue in two ways. On the one hand, understanding which threats occur in which applications and how they are perceived helps to shift research towards designing practical and usable defenses. On the other hand, a deeper understanding of why non-AML security and AML are mingled allows us to adapt and improve current threat modelling. To this end, however, it is also important to know which threats need to be studied in the first place.</p><p>AML in the wild. Given the previous insight and evidence of semi-automated, ML-related fraud, a more detailed assessment of which attacks are conducted in the wild would be beneficial. Future work could investigate this with a focus on different groups of ML practitioners, including for example ML engineers, auditors, and researchers, or dependant on the application. Furthermore, our work outlines that the model perspective usually taken in AML is of limited use in practice. More work is needed to study AML in the context of entire ML pipelines and end-to-end workflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Practical implications</head><p>Similar to Kumar et al. <ref type="bibr" target="#b46">[47]</ref>, we find that most of our participants lack an adequate and differentiated understanding to secure ML systems in production. Given that we found only reports of semi-automated fraud in our sample in Section 4.1.2, the absence of strong AML in practice might explain this lack of knowledge. Yet, as discussed in Section 4.3.3, 6 of 15 participants felt insecure about ML security. We thus now discuss the diverse implications of our study on how to tackle these insecurities and the overall lack of knowledge. We start with the question how to raise awareness for AML. Afterwards, discuss the implications of our findings for the the embedding of AML in corporate workflows and finish with implications for regulatory frameworks of AML.</p><p>Raising awareness of and increasing confidence about AML. Although we did not ask about privacy specifically, the general data protection regulation was often mentioned by our participants. For example, P6 stated: "we are also subject to GDPR so we cannot just ignore the security aspects of the process." Like other participants (P12, P18), P6 mentioned GDPR before we had asked about membership inference and thus privacy. Legislation might thus be a tool to increase awareness of AML. Independently, a third of our participants felt insecure about AML (Section 4.3.3). Given that several participants reported used software (P9, P14, for example "TensorFlow"), infrastructure (P14) or service provider (P3, P12, P20, for example "Google"), advertising tools to assess AML risks might be helpful for our participants. In particular as AML libraries <ref type="foot" target="#foot_5">6</ref> , but also overviews like the Adversarial ML Threat Matrix<ref type="foot" target="#foot_6">foot_6</ref> already exist. Our findings on the confusion between AML and non-AML security (Section 4.1.2) suggest these tools need to either enforce dedicated audits for both AML and non-AML security or combined countermeasures to address both areas jointly. Another solution to the feeling of insecurity, reported by our participants themselves (Section 4.3.3, P19: "I maybe need to learn more about this membership"), could be to provide materials for education.</p><p>Embedding AML into corporate workflows. Whereas academia generally studies AML with the perspective of an individual model, in practice, the entire ML pipeline and broader AI workflow need to be considered. As discussed in Section 4.2, in our interviews, for example P6 and P16 (see Figure <ref type="figure" target="#fig_2">5</ref>) described the entire workflow of their AI application, whereas other participants focused on the ML pipelines (for example P18, as visible in Figure <ref type="figure" target="#fig_3">6</ref>). To successfully integrate AML into corporate workflows, however, more effort is needed. All actors working on an ML product need to be able to identify relevant and possible attacks and implementable defenses. Potential factors to consider here are for example different applications areas, as discussed in Section 4.3.1. Also the existing knowledge of the target audience should be considered, as the in Section 4.3.2 discussed variation of knowledge in our sample shows.</p><p>Creating appropriate regulatory and standardization frameworks for AML. Lastly, our study has implications for regulatory approaches that enable appropriate security assessments. The differences in application (Section 4.3.1) and prior knowledge (Section 4.3.2) we found imply that regulatory frameworks need to find a way to formally encompass these differences with regards to necessary security measures. The currently proposed 'Legal Framework for AI' by the European Commission, for example, differentiates certain types of ML applications of which some are prohibited or classified as high-risk and thus require a certain risk management. Furthermore, as discussed in Section 4.2, our results indicate that it is essential to communicate such frameworks at the right technical abstraction level to encompass both technical ML practitioners and non-technical stakeholders. Standardization efforts could incorporate this requirement by providing adequate information at multiple mental abstraction levels <ref type="bibr" target="#b17">[18]</ref>. For example, recently proposed frameworks like the NIST Taxonomy and Terminology of AML 8 explicitly lists references that might help practitioners develop more complex mental models. As mentioned above, a similar regulatory approach to privacy, the European general data protection regulation, had served as a scaffold for their privacy perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>We followed an inductive approach to investigate mental models through qualitative analysis. Hence, the data collected is self-reported and subjected to a coding process. We continued coding and refining codes until a good level of inter-coder agreement was reached. Nonetheless, all our findings are subject to interpretation and do not generalize beyond the sample, both of which is inherent to qualitative analyses. Finally, due to the COVID-19 pandemic, all interviews were conducted remotely and the interface limitations of the digital whiteboard might have impacted the participants' sketches.</p><p>Given the qualitative approach and reached saturation, the small sample size of 15 is indeed acceptable <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b88">89]</ref>. Due to the small sample size, however, several factors cannot be addresses in depth, as discussed in Section 4.3. Examples include, but are not limited to, the application setting and the perceived relevance. Ideally, future work provides a more in depth analysis of these topics in a larger quantitative study. 8 <ref type="url" target="https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8269-draft.pdf">https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8269-draft.pdf</ref> All participants were employed at European organizations with &lt;200 employees. This is due to the fact that while several multinational companies stated great interest in our research, they denied participation after internal risk assessments. As mental models of ML systems are always embedded in organizational practices <ref type="bibr" target="#b90">[91]</ref>, we strongly encourage future research to assess our findings within larger samples including more variety, for example academics, small and large companies. Given that previous work found differences in general security behavior depending on gender <ref type="bibr" target="#b58">[59]</ref>, and cultural background <ref type="bibr" target="#b45">[46]</ref>, we also strongly encourage a more in depth analysis of these aspects.</p><p>Furthermore, AML itself is a subject of study of which the perception evolves continuously. With an increasing awareness for security within applied machine learning, the findings presented can only be valid temporarily. Machine learning is applied in a wide range of settings. Consequently, not all attacks are relevant within each application domain. For example, a healthcare setting is subjected to other threats than a cybersecurity setting. For the sake of studying abstract facets of mental models, we did not consider the application in the present work. Yet, we would like to point out the necessity to study this aspect of AML in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Based on our semi-structured interviews with industrial practitioners, we take a first step towards a theory of mental models of AML. We described two facets of practitioners' mental models and sketched more facets as an anchor for in-depth investigation by future work. These include the technical abstraction level, application setting, prior education, and the perceived relevance of AML. We provided more details on the first facet, or the blurry relationship between AML and non-AML security. These two topics were often mingled, yet not used interchangeably by our participants. The second facet can be understood as a first step to refined threat models in AML research. As apposed to a single model, our participants instead described workflows and relationships between potentially several ML models in a larger system context.</p><p>A clear understanding of the elicited mental models allows to improve information for practitioners and adjustments of corporate workflows. More concretely, our results help to raise awareness for AML, thus making practitioners feel less insecure. We further suggest that both application area and prior knowledge are considered when embedding AML into corporate workflows. Finally, regulatory frameworks might reduce uncertainty about AML and increase the awareness for possible AML threats. However, a wide range of subsequent research towards an encompassing theory of mental models in AML is still required. Last but nor least, we are convinced that the AML community will benefit from further practical assessment of attacks in practice, as our work already provides evidence of semi-automated fraud in the wild.</p><p>we added two rather unknown terms, adversarial initialization <ref type="bibr" target="#b31">[32]</ref> and neural trojans <ref type="bibr" target="#b55">[56]</ref> (similar to backdoors). The results are depicted in Figure <ref type="figure" target="#fig_5">7</ref>. Only one participant reported to be familiar with one attack (evasion). In general, most participants reported to have heard of most common attacks (evasion, poisoning, membership inference, and model stealing). As expected for the sanity check, adversarial initialization and neural trojans were largely unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Interview protocol</head><p>Thank you so much for taking the time to give us your perspective on security in machine learning. This study consists in III parts. Part I aims at exploring your role in ML-projects. Part II addresses the underlying machine learning pipeline. In part III, we want to know how you perceive the security of machine learning. In part II and III, please visualize the topics (and relationships between them) that we ask you about. There are no rules, no wrong way to do it, and don't worry about spelling things perfectly. Nothing is off limits and you can use any feature of the digital whiteboard. After this last part, we will ask you about your knowledge about security of machine learning before this study. • Can you tell us a bit more about the goal of this project?</p><p>• Who else is involved in this project?</p><p>• What is your collaborators role in the project?</p><p>Part II: Machine learning pipeline • What kind of pipeline do you currently apply within this machine learning based project?</p><p>• Which part of this pipeline is crucial for your business, or identical to your product? • Have you encountered any issues relating to security in the projects you described?</p><p>• Where in the pipeline did these security-related issues originate?</p><p>• Can you specify the cause of the security-related issues?</p><p>• Can you specify how these security-related issues evolve in your pipeline?</p><p>• Which goal pursues an adversary with a such a threat?</p><p>• What is the security violation of the threat?</p><p>• How specific is the depicted threat?</p><p>• Are you aware of any further possible security threats in the scope of your project or pipeline?</p><p>• Which countermeasures do you implement against any of the aforementioned threats?</p><p>Thank you so much for taking the time to give us your perspective on security in machine learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: High-level intuition Section 4.1. While in research, non-AML security and AML are rather distinct, our participants do not always clearly distinguish the two fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Drawing of P11. Red markings were added by the participant before, blue after being confronted with selected attacks.</figDesc><graphic coords="8,54.00,72.00,493.92,142.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Drawing of P16. Colors were added after selected attack were presented to the participant. Red refers to evasion, purple to reverse engineering, blue to membership inference.</figDesc><graphic coords="9,54.00,72.00,235.32,123.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Drawing of P18. Red star indicates the most important component of the pipeline, not an attack.</figDesc><graphic coords="9,317.88,72.00,235.31,68.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Part I: Machine learning project • Can you briefly describe what AI-or machine learningbased project you are currently involved in?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Self-reported familiarity of interviewed participants with different attacks on ML. Total of participants is 14, as one participant did not hand in questionnaire.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="20,77.89,223.92,456.22,174.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Participants with their random IDs. Capital letters denote that participants work in the same company. We denote the application domain and the working experience (Exp.) in years. Knowledge in ML, Security and AML is encoded as completed lectures (++), seminar/self-study (+) or none ().</figDesc><table><row><cell></cell><cell></cell><cell>Company</cell><cell></cell><cell></cell><cell cols="2">Education</cell></row><row><cell>ID</cell><cell></cell><cell cols="6">Application domain Exp. ML Sec. AML Degree</cell></row><row><cell>1</cell><cell></cell><cell>Human resources</cell><cell>7</cell><cell>++</cell><cell>+</cell><cell></cell><cell>PhD</cell></row><row><cell>3</cell><cell>A</cell><cell>Healthcare</cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell>PhD</cell></row><row><cell>4</cell><cell>B</cell><cell>Cybsersecurity</cell><cell>8</cell><cell>++</cell><cell>+</cell><cell></cell><cell>PhD</cell></row><row><cell>6</cell><cell cols="2">C Business intelligence</cell><cell>15</cell><cell>++</cell><cell>++</cell><cell>+</cell><cell>PhD</cell></row><row><cell>7</cell><cell></cell><cell>Computer vision</cell><cell>12</cell><cell>++</cell><cell></cell><cell></cell><cell>BSc</cell></row><row><cell>9</cell><cell></cell><cell>Computer vision</cell><cell>9</cell><cell>++</cell><cell></cell><cell></cell><cell>MSc</cell></row><row><cell>10</cell><cell></cell><cell>Cybersecurity</cell><cell></cell><cell cols="3">no questionnaire handed in</cell></row><row><cell>11</cell><cell></cell><cell>Business intelligence</cell><cell>1</cell><cell>++</cell><cell></cell><cell></cell><cell>PhD</cell></row><row><cell>12</cell><cell></cell><cell>Retail and commerce</cell><cell>1.4</cell><cell></cell><cell></cell><cell>++</cell><cell>PhD</cell></row><row><cell>14</cell><cell></cell><cell>AI as a service</cell><cell>5</cell><cell>++</cell><cell></cell><cell>+</cell><cell>PhD</cell></row><row><cell>15</cell><cell></cell><cell>Computer linguistics</cell><cell>5</cell><cell>+</cell><cell>+</cell><cell></cell><cell>MSc</cell></row><row><cell cols="3">16 C Business intelligence</cell><cell>3</cell><cell>++</cell><cell>+</cell><cell>+</cell><cell>PhD</cell></row><row><cell cols="2">18 A</cell><cell>Healthcare</cell><cell>1.5</cell><cell>++</cell><cell></cell><cell></cell><cell>PhD</cell></row><row><cell cols="2">19 B</cell><cell>Cybersecurity</cell><cell>15</cell><cell>++</cell><cell>++</cell><cell>+</cell><cell>MSc</cell></row><row><cell cols="2">20 A</cell><cell>Healthcare</cell><cell>1.2</cell><cell>++</cell><cell></cell><cell></cell><cell>MSc</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Classifiers with convex optimization problems (for example SVM) cannot be targeted, as the mathematical solution to the learning problem does not depend on the initial weights.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>e.g., https://pair.withgoogle.com/chapter/mental-models/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://awwapp.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Available at https://www.taguette.org/ and https://www.maxqda.com/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We are aware that AML is far from being solved, and communicated this to our participants if required. In this study, we define defenses as techniques which increase the difficulty for an attacker, like retraining or explainability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>For example the Adversarial Robustness Toolbox, CleverHans, Robust-Bench, or the SecML library, just to name a few.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/mitre/advmlthreatmatrix</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://www.crunchbase.com/ for European companies operating in AI and having raised more than 1 million dollar funding</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="person">Antoine Gautier</rs>, <rs type="person">Michael Schilling</rs>, the anonymous reviewers and the shepherd for the insightful feedback. This work was supported by the <rs type="funder">German Federal Ministry of Education and Research (BMBF) through funding for the Center for IT-Security, Privacy and Accountability (CISPA)</rs> (FKZ: <rs type="grantNumber">16KIS0753</rs>) and by <rs type="funder">BMK</rs>, <rs type="funder">BMDW</rs> and the <rs type="institution">Province of Upper Austria</rs> within the <rs type="programName">COMET program</rs> managed by <rs type="funder">FFG</rs> in the COMET S3AI module.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZphGAmx">
					<idno type="grant-number">16KIS0753</idno>
				</org>
				<org type="funding" xml:id="_fgsnXV3">
					<orgName type="program" subtype="full">COMET program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details on recruiting</head><p>We searched online databases like crunchbase 9 , AIhubs 10 , and lists with promising AI start-ups (for example the list by Forbes 11 ) to find potential participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Participants' prior knowledge in (A)ML</head><p>To measure our participants' knowledge in ML, we constructed a questionnaire based on ML job interview questions 12  <ref type="bibr">(Appendix D.3)</ref>. Given that participants were not informed they had to take a test, we aimed to select a broad range of topics easy to query with multiple choice answers that were not too hard. The questionnaire had 8 questions, with the participants correctly answering on average 6.64 questions (STD 1.14). Guessing would yield an average of 2.66 correct questions. Thus, while we do not know how reliable our questionnaire estimates ML knowledge, we conclude that our participants are indeed knowledgeable in ML.</p><p>We also investigated the familiarity of our participants with AML attacks. To avoid priming, we asked participants to rate their familiarity after the interview. As sanity checks,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Questionnaires</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Demographics questionnaire</head><p>Thank you for participating in our research study about security in machine learning. Please take a couple of minutes to respond to the following questions. Poisoning. This attack targets the training or optimization phase of the model. The goal of the attacker is to either decrease accuracy significantly, or to install a backdoor. An example is a cat-dog classifier that always classifies images containing a smiley as cat.</p><p>Privacy/ Membership Inference. This attack targets a model at test-time. The attacker's goal is to identify individual samples from or even the whole training set. An example is to measure the confidence on an input, as some algorithms tend to be more confident on data they have seen during training. Also over-fitting eases to determine what a classifier was trained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 ML quiz</head><p>Please answer the following questions about ML. For each question, please tick at least one box.</p><p>Question 1. Which loss is used to train DNN? 0/1-loss. Cross-entropy loss. Hinge-loss. Question 2. What is the difference between classification and regression?</p><p>The kind of labels we fit: reals vs discrete classes. Regression is the name of classification in psychology / medical science.</p><p>Regression is for discrete labels, classification for real valued ones.</p><p>Question 3. What is the between L 1 and L 2 regularization?</p><p>L 1 yields sparser solutions. L 2 yields sparser solutions. none -they differ only in few practical applications. Question 4. In the bias-variance trade-off, what does high variance imply?</p><p>The analyzed data shows high variance.</p><p>The clf is overly complex and potentially overfits. The data is likely to be classified fair (e.g., low bias). Question 5. Why is Naive Bayes naive?</p><p>Due to historic reasons.</p><p>Due to the assumption that all features are independent.</p><p>Because the application is simple and straightforward.</p><p>Question 6. What is cross-validation? Training on one task and then transferring the model to another task.</p><p>Splitting the dataset and training/evaluating on different subsets.</p><p>A method to reduce overfitting or choosing hyperparameters.</p><p>Question 7. What are kernels in machine learning? Essentially similarity functions.</p><p>A part of SVM, potentially yielding non-linear SVM.</p><p>A specific instance of a similarity function used in SVM.</p><p>Question 8. What is pruning? Deletion of for example weights in a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deletion of specific points of the data.</head><p>A technique to get a smaller from a large model with similar performance.</p><p>To conclude the study, we will ask you to rate your background knowledge on attacks before this study according to the following four classes:</p><p>Familiar. Your are familiar with this concept, and can write down the mathematical formulation.</p><p>Dabbled in. You could explain in a five minute talk what the concept is about.</p><p>Heard of. You have heard of the concept and you could put it into context if necessary.</p><p>Never heard. You did not know about this concept before this survey.</p><p>For each concept, please tick one box. The original questionnaire was formatted as table . To ease readability, we list them as questions here.</p><p>Evasion </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Final set of codes</head><p>The final set of codes for the interviews is depicted in Table <ref type="table">2</ref>, the codes for the drawings in Table <ref type="table">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Structural and functional components</head><p>We found structural and functional components in our participants' mental models. Structural components cover multiple, constituting entities that an individual perceives as relevant within a given application. In interaction with an ML system, functional components describe an individual's perception of the relations between the structural elements. As intended, the structure of our interview and drawing task (Appendix C) allowed to investigate these properties on the level of the ML pipeline, of the attack vectors as well as of the defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 ML pipeline</head><p>All participants distinguish clearly separable elements within their ML workflow. The specific composition of these steps defines the structure of a certain ML pipeline. For two participants, this structure reflects the ML pipeline that we introduced in Figure <ref type="figure">1</ref>. When asked to sketch the kind of pipeline applied, P4 talked about "data", "training", "testing", and "visualization". We argue that these structural components serve as a scaffold for an individual's mental model. Interestingly, the mental models of 12 out of 15 participants covered additional components that we did not expect prior to the study. The sketches of P3, P7, and P11 (Figure <ref type="figure">3</ref>), for example, contain explicit elements for data capturing. P1, P9, P12, as well as P20 included dedicated elements representing a specific database to their drawing. Five participants also highlighted structural elements within the deployment environment during the interviews. P14, for example, specified on an API for deployment "on several kinds of hardware architectures".</p><p>Analogously, P1 described an API that "can be used to allow the user to interact with the models" Hence, these structural elements concerning data and deployment seem to be of importance for the corresponding mental models. However, the perception of industrial practitioners does no only focus on these structural components but also covers functional aspects. P6 for instance stated that his ML pipeline "forks into a number of different directions and there are also interactions between the different components". In the corresponding sketch, multiple arrows within and across specific ML models indicate this interconnection of single components. Other drawings include this functional perspective through straight lines connecting the structural components, arrows connecting some of the structural components in a subsequent manner (e.g. P14), and arrows connecting all structural components in a subsequent manner (P18 in Figure <ref type="figure">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.1 Attack vectors</head><p>The identified structural and functional components seem to be similarly relevant for mental models on attack vectors. For any kind of ML-specific threat, participants were able to precisely locate where they situated the corresponding, structural starting point. These have been specifically named during the interview and sketched via labelled arrows (e.g. Figure <ref type="figure">3</ref>, P11), additional annotations (P11, P15), highlighted parts of potentially vulnerable pipeline components (e.g. Figure <ref type="figure">8</ref>, P10) or as entire steps within a given ML workflow that have been marked as vulnerable (P9, P20). Strikingly, we saw a wide overlap in the perception of potential focal starting points for attack vectors. Study participants consid-  ered the model itself, the input of their ML pipeline, or the deployment environment to be particularly vulnerable. <ref type="bibr">Figure 5 (P16)</ref> shows this for the latter. When confronted with poisoning and reverse engineering attacks, P16 marked the input and output of his pipeline as possible starting points for threats (purple rectangles) and talked about how a competitor could "screw our labeled dataset" or a customer might "ask a lot of questions to the API". However, the perception of attack vectors did also cover functional components. P1, for example, depicted the causal sequence of a "data injection attack" as three consecutive red arrows connecting different components of his ML pipeline. This is all the more relevant, as P1 provided such a functional explanation and drawing for each of the attack vectors we presented to him. His mental models, hence, clearly seem to contain functional components. This is also the case for P16, who similarly provided explanations on the functional evolvement of certain attacks within his workflow and even added corresponding functional elements to his sketch (blue and red arrows in Figure <ref type="figure">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.2 Defenses</head><p>Although we found participants' defenses explanations and sketches to be rather sparse, structural and functional properties are also relevant for the corresponding mental models.</p><p>As visible in the sketch of P18, defenses are often thought of as structurally bound to specific components of a workflow/pipeline (Figure <ref type="figure">6</ref>, P18). Data (P14), training (P6) and the models themselves (P10) have been specifically named as focal points for implementing defenses. In the case of defenses implemented at the model component, P14 stated to "regularize in a way that makes it less sensitive to an adversary". Hence, these implemented defenses are cognitively attached to the classifier as a focal pipeline component. However, security mental models also contain functional properties. In the case of human-in-the-loop-defenses, for example, P14 stated to send certain classifications "back to the data sets to train a second version of the algorithm" if the output confidence for certain data exhibited high entropy. This is depicted in the corresponding sketch by an arrow pointing from a rectangle with the caption "CPU" at the end of the pipeline to "raw data" (initial step of the pipeline). Similarly, P7, a participant working in video surveillance, explained the defense they had implemented to secure the transfer of input data (from cameras and on-site computers) into their pipeline: "This can only go out, never go in. [...] Nothing from the internet can connect to that server". Industrial practitioners, hence, perceive defenses as containing functional components to unfold their full effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.3 Summary</head><p>We conclude that mental models in AML contain of structural components which are cognitively put into (internal) relation.</p><p>However, the specific unfolding of these internal conceptual representations seems to depend on the corresponding application and its underlying ML pipeline.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring user mental models of endto-end encrypted communication tools</title>
		<author>
			<persName><forename type="first">Ruba</forename><surname>Abu-Salma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elissa</forename><forename type="middle">M</forename><surname>Redmiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blase</forename><surname>Ur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miranda</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End user and expert perceptions of threats and potential countermeasures</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Anell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lea</forename><surname>Gröber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Krombholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroS&amp;P Workshops</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Barredo Arrieta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Del Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siham</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvador</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Gil-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Benjamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><forename type="middle">V</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Spognardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Villani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenico</forename><surname>Vitali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Felici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Security and Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Can machine learning be secure</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The design of browsing and berrypicking techniques for the online search interface</title>
		<author>
			<persName><forename type="first">Marcia</forename><forename type="middle">J</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online review</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explainable machine learning in deployment</title>
		<author>
			<persName><forename type="first">Umang</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruchir</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><surname>Eckersley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAccT</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Srndic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pattern recognition systems under attack: Design issues and research challenges</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and AI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Support vector machines under adversarial label noise</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wild patterns: Ten years after the rise of adversarial machine learning</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">&apos;it&apos;s reducing a human being to a percentage&apos; perceptions of justice in algorithmic decisions</title>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Binns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Van Kleek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrik</forename><surname>Lyngs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Shadbolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">i never thought about securing my machine learning systems&quot;: A study of security and privacy awareness of machine learning practitioners</title>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Boenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Battis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Buchmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maija</forename><surname>Poikela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mensch und Computer</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Naturalistic inquiry and the saturation concept: a research note</title>
		<author>
			<persName><forename type="first">Glenn</forename><forename type="middle">A</forename><surname>Bowen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Qualitative research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bridging the gap in computer security warnings: A mental model approach</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bravo-Lillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorrie</forename><forename type="middle">Faith</forename><surname>Cranor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saranga</forename><surname>Komanduri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&amp;P</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Practical fault attack on deep neural networks</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Breier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirmanto</forename><surname>Jap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Bhasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Psychological foundations of explainability and interpretability in artificial intelligence</title>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Broniatowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>US Department of Commerce</publisher>
		</imprint>
		<respStmt>
			<orgName>NIST: National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Workshop on Artificial Intelligence and Security</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Targeted backdoor attacks on deep learning systems using data poisoning</title>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05526</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the robustness of convolutional neural networks to internal architecture and weight perturbations</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08245</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Shruthi</forename><surname>Sai Chivukula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">C</forename><surname>Pivonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08909</idno>
		<title level="m">Surveying the landscape of ethics-focused design methods</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and psychological measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial classification</title>
		<author>
			<persName><forename type="first">Nilesh</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The design of auditory user interfaces for blind users</title>
		<author>
			<persName><forename type="first">Hilko</forename><surname>Donker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Palle</forename><surname>Klante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Gorny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NordiCHI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mental models concepts for system dynamics research</title>
		<author>
			<persName><forename type="first">K</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">N</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><surname>Ford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">System dynamics review</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Gamaleldin F Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.11146</idno>
		<title level="m">Adversarial reprogramming of neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Security and machine learning in the real world</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07205</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">New me: Understanding expert and non-expert perceptions and usage of the tor anonymity network</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasir</forename><surname>Memon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOUPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Dedre</forename><surname>Gentner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">L</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mental Models. Erlbaum</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06732</idno>
		<title level="m">Motivating the rules of the game for adversarial example research</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On the security relevance of initial weights in deep neural networks</title>
		<author>
			<persName><forename type="first">Kathrin</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Trost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>In ICANN</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Can children understand machine learning concepts? the effect of uncovering black boxes</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hitron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Orlev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iddo</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadas</forename><surname>Erel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Zuckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving fairness in machine learning systems: What do industry practitioners need</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Holstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Miro</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model-reuse attacks on deep learning systems</title>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiapu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Backdoor attacks against learning systems</title>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CNS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Memguard: Defending against black-box membership inference attacks via adversarial examples</title>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Johnson-Laird</surname></persName>
		</author>
		<title level="m">Mental Models: Towards a Cognitive Science of Language, Inference, and Consciousness</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Harvard University Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Prada: protecting against dnn model stealing attacks</title>
		<author>
			<persName><forename type="first">Mika</forename><surname>Juuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Szyller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Marchal</surname></persName>
		</author>
		<author>
			<persName><surname>Asokan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EuroS&amp;P</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">State of machine learning and data science</title>
		<author>
			<persName><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/kaggle-media/surveys/Kaggle" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>s%20State% 20of%20Machine%20Learning%20and%20Data% 20Science%202021.pdf</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">my data just goes everywhere:&quot; user mental models of the internet and implications for privacy and security</title>
		<author>
			<persName><forename type="first">Ruogu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Dabbish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Fruchter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Kiesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOUPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interpreting interpretability: understanding data scientists&apos; use of interpretability tools for machine learning</title>
		<author>
			<persName><forename type="first">Harmanpreet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Merging internal and external processes: Examining the mental model convergence process through team communication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Deanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><surname>Mccomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Issues in Ergonomics Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Integrating culture into interface design</title>
		<author>
			<persName><forename type="first">Julie</forename><surname>Khaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">if https were secure, i wouldn&apos;t need 2fa&quot;-end user and administrator mental models of https</title>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Krombholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karoline</forename><surname>Busse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Pfeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zezschwitz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&amp;P</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An assessment of the role of cultural factors in information security awareness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drevin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tjaart</forename><surname>Flowerday</surname></persName>
		</author>
		<author>
			<persName><surname>Steyn</surname></persName>
		</author>
		<editor>ISSA</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry perspectives</title>
		<author>
			<persName><forename type="first">Ram</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Nyström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Goertzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&amp;P Workshops</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Human evaluation of models built for interpretability</title>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Lage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menaka</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finale</forename><surname>Samuel J Gershman</surname></persName>
		</author>
		<author>
			<persName><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Interpretable decision sets: A joint framework for description and prediction</title>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">biometrics</title>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">What do we want from explainable artificial intelligence (xai)?-a stakeholder perspective on xai and a conceptual model guiding interdisciplinary xai research</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Speith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Hermanns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Kästner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The landscape and gaps in open source fairness toolkits</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jatinder</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adversarial machine learning: Attacks from laboratories to the real world</title>
		<author>
			<persName><forename type="first">Hsiao-Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bad global minima exist and sgd can reach them</title>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Trojaning attack on neural networks</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousra</forename><surname>Aafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NDSS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural trojans. In ICCD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">User mental models of cryptocurrency systems -a grounded theory approach</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Pfeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Gusenbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Weippl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Krombholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOUPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Influence of mental models on the design of cyber security dashboards</title>
		<author>
			<persName><forename type="first">Janosch</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arne</forename><surname>Padmos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mortaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Bargh</surname></persName>
		</author>
		<author>
			<persName><surname>Wörndl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In VISIGRAPP (3: IVAPP</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exploring potential gender differences in information security and privacy</title>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nik</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information &amp; Computer Security</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Yisroel</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambra</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaidip</forename><surname>Kotak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Gelei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Elovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15764</idno>
		<title level="m">The threat of offensive ai to organizations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Metaphor no more: A 15-year review of the team mental model construct</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Ferzandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Collaboration challenges in building mlenabled systems: Communication, documentation, engineering, and process</title>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Nahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shurui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Kästner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Why do developers get password storage wrong? a qualitative usability study</title>
		<author>
			<persName><forename type="first">Alena</forename><surname>Naiakshina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Danilova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Tiefenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergej</forename><surname>Dechand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Machine learning with membership privacy using adversarial regularization</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Houmansadr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Maximally fault tolerant neural networks</title>
		<author>
			<persName><forename type="first">Chalapathy</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">H</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">D</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Believe it or not: Designing a human-ai partnership for mixed-initiative fact-checking</title>
		<author>
			<persName><forename type="first">An</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kharosekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saumyaa</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhesh</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Towards reverse-engineering black-box neural networks</title>
		<author>
			<persName><forename type="first">Joon</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Prediction poisoning: Towards defenses against model stealing attacks</title>
		<author>
			<persName><forename type="first">Tribhuvanesh</forename><surname>Orekondy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A marauder&apos;s map of security and privacy in machine learning: An overview of current and future research directions for making machine learning secure and private</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Artificial Intelligence and Security</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">should i worry?&quot; a cross-cultural examination of account security incident response</title>
		<author>
			<persName><forename type="first">Elissa</forename><forename type="middle">M</forename><surname>Redmiles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&amp;P</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Why doesn&apos;t jane protect her privacy?</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Renaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Volkamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arne</forename><surname>Renkema-Padmos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PETS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Antidote: understanding and defending against poisoning of anomaly detectors</title>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Benjamin Ip Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shing-Hon</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Taft</surname></persName>
		</author>
		<author>
			<persName><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IMC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">How to t (r) ap users&apos; mental models</title>
		<author>
			<persName><forename type="first">Martina-Angela</forename><surname>Sasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Factors in Information Technology</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&amp;P</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Shumailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Mullins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03463</idno>
		<title level="m">Sponge examples: Energy-latency attacks on neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mental models: concepts for human-computer interaction research</title>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Staggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">F</forename><surname>Norcio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Man-machine studies</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Basics of qualitative research</title>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliet</forename><surname>Corbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Sage publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Bit error robustness for energyefficient dnn accelerators</title>
		<author>
			<persName><forename type="first">David</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandhini</forename><surname>Chandramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<editor>MLSys</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs</title>
		<author>
			<persName><forename type="first">Harini</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">K</forename><surname>Steven R Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><surname>Satyanarayan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09824</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">i don&apos;t own the data&quot;: End user perceptions of smart home device data practices and risks</title>
		<author>
			<persName><forename type="first">Madiha</forename><surname>Tabassum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lipford</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOUPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Bypassing backdoor detection algorithms in deep learning</title>
		<author>
			<persName><forename type="first">Te</forename><surname>Juin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lester</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroS&amp;P</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Stealing machine learning models via prediction apis</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Understanding mental models of ai through player-ai interaction</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Villareale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichen</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Mental modelsgeneral introduction and review of their application to human-centred security</title>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Volkamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Renaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Number Theory and Cryptography</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Designing theory-driven user-centric explainable ai</title>
		<author>
			<persName><forename type="first">Danding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashraf</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Influencing mental models of security: a research agenda</title>
		<author>
			<persName><forename type="first">Rick</forename><surname>Wash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilee</forename><surname>Rader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Security Paradigms Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Towards certificated model robustness against weight perturbations</title>
		<author>
			<persName><forename type="first">Pu</forename><surname>Tsui-Wei Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">When is a tree really a truck? exploring mental models of encryption</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zappala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOUPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">End user security and privacy concerns with smart homes</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrirang</forename><surname>Mare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOUPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">How do data science workers collaborate? roles, workflows, and tools</title>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dakuo</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ACM on Human-Computer Interaction</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
