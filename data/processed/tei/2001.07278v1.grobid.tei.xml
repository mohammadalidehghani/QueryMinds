<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixed integer programming formulation of unsupervised learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-01-22">January 22, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Arturo</forename><surname>Berrones-Santos</surname></persName>
							<email>arturo.berronessn@uanl.edu.mx</email>
							<affiliation key="aff0">
								<orgName type="department">Facultad de Ingeniería Mecánica y Eléctrica Posgrado en Ingeniería de Sistemas Facultad de Ciencias Físico Matemáticas Posgrado en Ciencias con Orientación en Matemáticas AP 126</orgName>
								<orgName type="institution">Universidad Autónoma de Nuevo León</orgName>
								<address>
									<addrLine>Cd. Universitaria San Nicolás de los Garza</addrLine>
									<postCode>66450</postCode>
									<region>NL</region>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mixed integer programming formulation of unsupervised learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-01-22">January 22, 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">F0A6CE3E29682A17B383FCDB6AAA53C8</idno>
					<idno type="arXiv">arXiv:2001.07278v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Boltzamnn machines</term>
					<term>Mixed integer programming</term>
					<term>Unsupervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel formulation and training procedure for full Boltzmann machines in terms of a mixed binary quadratic feasibility problem is given. As a proof of concept, the theory is analytically and numerically tested on XOR patterns.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A central open question in machine learning is the effective handling of unlabeled data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The construction of balanced representative datasets for supervised machine learning for the most part still requires a very close and time consuming human direction, so the development of efficient learning from data algorithms in an unsupervised fashion is a very active area of research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. A general framework to deal with unlabeled data is the Boltzmann machine paradigm, in which is attempted to learn a probability distribution for the patterns in the data without any previous identification of input and output variables. In its most general setups however, the training of Blotzmann machines is computationally intractable <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. In this contribution is established a relation, which to the best of my knowledge was previously unknown, between Mixed Integer Programing (MIP) and the full Boltzmann machine in binary variables. Is hoped that this novel formulation opens the road to more efficient learning algorithms by taking advantage of the great variety of techniques available for MIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Full Boltzmann machine with data as constraints</head><p>Consider a network of units with binary state space. Each unit depends on all the others by a logistic-type response function,</p><formula xml:id="formula_0">x i = round   1 1 + exp -j =i q j,i x j -b i   ≡ f i ,<label>(1)</label></formula><p>where the "round" indicates the nearest integer function, the q's are pairwise interactions between units and the b's are shift parameters. As will later be clear, the proposed model supports both supervised and unsupervised learning and leads to a full Boltzmann machine in its classical sense. </p><formula xml:id="formula_1">(-1) v d,i   I j =i q i,j v d,j + b i   ≤ 0,<label>(2)</label></formula><formula xml:id="formula_2">(-1) u d,m   M j =m q m,j u d,j + b m   ≤ 0, d = 1, 2, ..., D; i = 1, 2, ..., I; m = 1, 2, ..., M.</formula><p>A posterior distribution for the parameters P ({q i,j , b i }|D), can be constructed by the maximum entropy principle, which gives the less biased distribution that is consistent with a set of constraints <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. This is done by the minimization of the Lagrangian</p><formula xml:id="formula_3">L = P ln P d w + N D r=1 λ r constraint(r) ,<label>(3)</label></formula><p>where the brackets represent average under the posterior, w is a vector that contains the connectivity and shift parameters and the λ's are positive Lagrange multipliers. Due to the linearity of the system of inequalities (2), the average of the constraints under P with fixed unit values is simply given by the same set of inequalities with the coefficients q j,i 's and b i 's substituted by their averages q j,i 's, b i 's. The maximum entropy distribution for the parameters is therefore given by</p><formula xml:id="formula_4">P ({q i,j , b i }|D) = 1 Z exp   - {d,i} λ {d,i}   (-1) x d,i   N j =i q i,j x d,j + b i       , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where Z is a normalization factor. So due to the linearity of the constraints, P is a tractable (i. e. an easy to sample) product of independent two parameter exponential distributions:</p><formula xml:id="formula_6">P ({q i,j , b i }|D) = P ( w|D) = N n=1</formula><p>α n e -αn(wn-βn) ,</p><p>where w n = 1 αn + β n <ref type="bibr" target="#b6">[7]</ref>. Therefore a necessary and sufficient condition for the existence of the above distribution is the existence of the averages w n , which is determined by the satisfaction of the inequalities <ref type="bibr" target="#b1">(2)</ref>.</p><p>The representation of the posterior by its two parameter exponential form Eq. ( <ref type="formula" target="#formula_7">5</ref>) gives a codification of the training data in terms of a tractable distribution for the parameters that in conjunction with Eq. ( <ref type="formula" target="#formula_0">1</ref>) is in fact a distribution for new unlabeled binary strings of data. For fully connected topologies, this is what is usually understood by an equilibrium distribution of a full Boltzmann machine <ref type="bibr" target="#b1">[2]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Illustrative example 1: Supervised XOR</head><p>The theoretical soundness of the proposed approach is now shown through the XOR logical table, D = {(0, 0, 0), (1, 0, 1), (0, 1, 1), (1, 1, 0)}. Let's consider first a restricted architecture with only two directed arcs that connect two inputs with an output unit, as represented in Figure <ref type="figure" target="#fig_1">2A</ref>. The inequalities (2) in this case read,</p><formula xml:id="formula_8">b 2 ≤ 0, -q 0,2 -b 2 ≤ 0,<label>(6)</label></formula><p>-q 1,2 -b 2 ≤ 0, q 0,2 + q 1,2 + b 2 ≤ 0.</p><p>There are no values for b 2 , q 0,2 and q 1,2 that satisfy all the inequalities. This is reflected in the maximum entropy distribution,</p><formula xml:id="formula_9">P = 1 Z e -λ1(b2) e -λ2(-q0,2-b2) e -λ3(-q1,2-b2) e -λ4(q0,2+q1,2+b2)<label>(7)</label></formula><p>which to be a properly normalized product of two-parameter exponential distributions must satisfy the contradictory conditions b 2 &lt; 0, q 0,2 &gt; 0, q 1,2 &gt; 0,</p><formula xml:id="formula_10">|b 2 | &lt; q 0,2 , |b 2 | &lt; q 1,2 , |b 2 | &gt; q 0,2 + q 1,2 .</formula><p>A valid model is however attainable by the addition of a single hidden unit. Consider the architecture represented in Figure <ref type="figure" target="#fig_1">2B</ref>. This leads to a two stage constraint satisfaction problem. The first stage is given by the data evaluated on the visible units,</p><formula xml:id="formula_11">q 3,2 f 3 (0, 0) + b 2 ≤ 0, (<label>8</label></formula><formula xml:id="formula_12">)</formula><formula xml:id="formula_13">-q 0,2 -q 3,2 f 3 (1, 0) -b 2 ≤ 0, -q 1,2 -q 3,2 f 3 (0, 1) -b 2 ≤ 0, q 0,2 + q 1,2 + q 3,2 f 3 (1, 1) + b 2 ≤ 0,</formula><p>for which solutions certainly exist. Take for instance,</p><formula xml:id="formula_14">|b 2 | &lt; q 0,2 , |b 2 | &lt; q 1,2 , |b 2 + q 3,2 | &gt; q 0,2 + q 1,2 ,<label>(9)</label></formula><formula xml:id="formula_15">f 3 (0, 0) = f 3 (1, 0) = f 3 (0, 1) = 0, f 3 (1, 1) = 1.</formula><p>The second stage is consequently given by,</p><formula xml:id="formula_16">b 3 ≤ 0, (<label>10</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">q 0,3 + b 3 ≤ 0, q 1,3 + b 3 ≤ 0, -q 0,3 -q 1,3 -b 3 ≤ 0,</formula><p>for which solutions exist under the conditions b</p><formula xml:id="formula_19">3 &lt; -C, |b 3 | &gt; |q 0,3 |, |b 3 | &gt; |q 1,3 |, |b 3 | &lt; |q 0,3 + q 1,3 |</formula><p>, where C is a positive constant. Therefore, the maximum entropy distribution for the parameters of the model represented in the Figure <ref type="figure" target="#fig_1">2B</ref> exists. Equivalently, this result shows that the classical XOR supervised learning problem can be solved by the proposed MIP feasibility formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Illustrative example 2: Unsupervised XOR</head><p>A model capable of unsupervised learning is sketched in Figure <ref type="figure" target="#fig_1">2C</ref>. The system of inequalities should be now extended to consider inputs to nodes x 0 and x 1 ,</p><formula xml:id="formula_20">q 3,0 f 3 (0, 0) + b 0 ≤ 0,<label>(11)</label></formula><p>-q 2,0 -q 3,0 f 3 (1, 0) -b 0 ≤ 0,</p><formula xml:id="formula_21">q 1,0 + q 2,0 + q 3,0 f 3 (0, 1) + b 0 ≤ 0, -q 3,0 f 3 (1, 1) -q 1,0 -b 0 ≤ 0, q 3,1 f 3 (0, 0) + b 1 ≤ 0, q 0,1 + q 2,1 + q 3,1 f 3 (1, 0) + b 1 ≤ 0, -q 2,1 -q 3,1 f 3 (0, 1) -b 1 ≤ 0, -q 0,1 -q 3,1 f 3 (1, 1) -b 1 ≤ 0,</formula><p>which has indeed solutions, as discussed in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sampling from the posterior distribution</head><p>The equilibrium posterior distribution of patterns can be sampled by taking an arbitrary solution of the MIP feasibility problem and using it to define the averages w n = 1 αn + β n . The standard deviation of each two-parameter exponential distribution is given by σ n = 1 αn , which can be at first instance assigned to some positive value related to the constant C. If y is an uniform random deviate in the interval [0, 1], then w n = -1 αn ln(1 -y) + β n is a deviate from the two-parameter exponential distribution associated to w n . In this way, the vector of visible units can be sampled in a computation time that is quadratic in the number of total (visible and invisible) units.</p><p>Algorithm 1 (Pseudo-code for sampling from the maximum entropy posterior.)</p><p>1: Initialize: β n from a solution of the MIP feasibility problem and 1 αn ← C ( arbitrary positive real number). 2: Asign value to size (desired number of samples). 3: Generate y τ , (τ = 1, ..., size×N ) uniform and independent random deviates in the [0, 1] interval. 4: for s = 1 to size do 5: w n,s = -1 αn,s ln(1 -y) + β n,s , n = 1, ..., N 6:</p><p>Generate x s by inserting w s in Eq. (1) 7: end for The step 6 of the algorithm above is made by starting with an inital random binary vector x at each s. The self-consistent system Eq. ( <ref type="formula" target="#formula_0">1</ref>) is then iterated. No more than 10 iterations are needed to achieve convergence.</p><p>The sampling procedure Algorithm 1 is now shown through the XOR example. Take an arbitrary solution of the MIP feasibility problem, say f 3 (0, 0) = f 3 (1, 0) = f 3 (0, 1) = 0, f 3 (1, 1) = 1, (</p><p>-b 0 = -b 1 = -b 2 = -b 3 = C, q 0,3 = q 1,3 = 3 4 C, q 2,0 = q 0,2 = q 1,2 = q 2,1 = -q 0,1 = -q 1,0 = 2C, q 3,0 = q 3,1 = -q 3,2 = 4C.</p><p>Due to the rounding operator in Eq. ( <ref type="formula" target="#formula_0">1</ref>), any C &gt; 0 can work. In the following experiments the value C = 100 is used with sample sizes of 1500. Some of the samples drawn for each C are shown. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Full Boltzmann machine with five units.</figDesc><graphic coords="2,219.70,302.33,171.85,222.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Three different architectures for the XOR problem.</figDesc><graphic coords="4,150.96,124.80,309.34,239.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>, [000], [110], [101], [000], [011], [000], [000], [110], [110], [000], [011], [000], [000], [110]...</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The resulting ratios of XOR patterns relative to non-XOR patterns over the entire 1500 samples for each case are, (a) : 1, (b) : 0.9 and (c) : 0.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In the author's view, this paper presents a formalism that has the potential not only to give more efficient learning algorithms but to improve the understanding of the learning from data itself. Particularly, datasets explicitly constrain the parameters of the learning model by a set of feasiblity mixed binary inequalities. For fixed binary values, the system is linear and continuous. For fixed model parameters, it's a linear constraint satisfaction problem in binary variables. The author together with collaborators is now working in different ways to exploit these structures in order to scale the framework to solve realistic large scale unsupervised learning problems. In such problems, a measure proportional to the number of satisfied constraints might be used to gide the learning procedure and to assign sensible values to the 1 αn hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>acknowledgements</head><p>The author acknowledge partial financial support from UANL and CONACyT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>The author declares that he have no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training restricted Boltzmann machines: An introduction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="39" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improved Boltzmann machines with error corrected quantum annealing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Albash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lidar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01283</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information theory and statistical mechanics</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="37" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reliability engineering handbook</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kececioglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>DEStech Publications, Inc</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
