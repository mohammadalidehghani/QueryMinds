<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automated Graph Machine Learning: Approaches, Libraries, Benchmarks and Directions</title>
				<funder ref="#_ujSeyRX">
					<orgName type="full">Beijing Key Lab of Networked Multimedia</orgName>
				</funder>
				<funder ref="#_vSyPbJQ #_TgMx5tH #_xzmZ9uA">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_wqSNtFB #_aBgRt8s #_eAqXptd #_DsaMPGn #_Mq6Jpge #_a7rHaTw">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_mrUZjNC #_k7s958e">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-02">2 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>Ziwei Zhang Haoyang Li</addrLine>
									<settlement>Xin Wang Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>Ziwei Zhang Haoyang Li</addrLine>
									<settlement>Xin Wang Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>Ziwei Zhang Haoyang Li</addrLine>
									<settlement>Xin Wang Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cn.lihy218@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>Ziwei Zhang Haoyang Li</addrLine>
									<settlement>Xin Wang Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automated Graph Machine Learning: Approaches, Libraries, Benchmarks and Directions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-02">2 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">F99EBD9A53AA00D80356BA75463F50D6</idno>
					<idno type="arXiv">arXiv:2201.01288v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Machine Learning</term>
					<term>Graph Neural Network</term>
					<term>Automated Machine Learning</term>
					<term>AutoML</term>
					<term>Neural Architecture Search</term>
					<term>Hyper-parameter Optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph machine learning has been extensively studied in both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To tackle the challenge, automated graph machine learning, which aims at discovering the best hyper-parameter and neural architecture configuration for different graph tasks/data without manual design, is gaining an increasing number of attentions from the research community. In this paper, we extensively discuss automated graph machine learning approaches, covering hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We briefly overview existing libraries designed for either graph machine learning or automated machine learning respectively, and further in depth introduce AutoGL, our dedicated and the world's first open-source library for automated graph machine learning. Also, we describe a tailored benchmark that supports unified, reproducible, and efficient evaluations. Last but not least, we share our insights on future research directions for automated graph machine learning. This paper is the first systematic and comprehensive discussion of approaches, libraries as well as directions for automated graph machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>G RAPH data is ubiquitous in our daily life. We can use graphs to model the complex relationships and dependencies between entities ranging from small molecules in proteins and particles in physical simulations to large national-wide power grids and global airlines. Therefore, graph machine learning, i.e., machine learning on graphs, has long been an important research direction for both academics and industry <ref type="bibr" target="#b0">[1]</ref>. In particular, network embedding <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and graph neural networks (GNNs) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> have drawn increasing attention in the last decade. They are successfully applied to recommendation systems <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, information retrieval <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, fraud detection <ref type="bibr" target="#b16">[17]</ref>, bioinformatics <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, physical simulation <ref type="bibr" target="#b19">[20]</ref>, traffic forecasting <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, knowledge representation <ref type="bibr" target="#b22">[23]</ref>, drug re-purposing <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> and pandemic prediction <ref type="bibr" target="#b25">[26]</ref> for Covid-19.</p><p>Despite the popularity of graph machine learning algorithms, the existing literature heavily relies on manual hyper-parameter or architecture design to achieve the best performance, resulting in costly human efforts when a vast number of models emerge for various graph tasks. Take GNNs as an example, at least one hundred new general-purpose architectures have been published in top-tier machine learning and data mining conferences in the year of 2021 alone, not to mention cross-disciplinary researches of task-specific designs. More and more human efforts are inevitably needed if we stick to the manual try-and-error paradigm in designing the optimal algorithms for targeted tasks.</p><p>On the other hand, automated machine learning (AutoML) has been extensively studied to reduce human efforts in developing and deploying machine learning models <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Complete AutoML pipelines have the potential to automate every step of machine learning, including auto data collection and cleaning, auto feature engineering, and auto model selection and optimization, etc. Due to the popularity of deep learning models, hyperparameter optimization (HPO) <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> and neural architecture search (NAS) <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> are most widely studied. AutoML has achieved or surpassed human-level performance <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> with little human guidance in areas such as computer vision <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>.</p><p>Automated graph machine learning, combining advantages of AutoML and graph machine learning, naturally serves as a promising research direction to further boost the model performance, which has attracted an increasing number of interests from the community. In this paper, we provide a systematic overview of approaches for automated graph machine learning 1 , introduce related public libraries as well as our AutoGL, the world's first open-source library for automated graph machine learning, describe a tailored benchmark that supports unified, reproducible, and efficient evaluations, and share our insights on challenges and future research directions.</p><p>Particularly, we focus on two major topics: HPO and NAS of graph machine learning. For HPO, we focus on how to de-velop scalable methods. For NAS, we follow the literature and compare different methods from search spaces, search strategies, and performance estimation strategies. We also briefly discuss several recent automated graph learning works that feature in different aspects such as architecture pooling, structure learning, accelerator and joint software-hardware design etc. Besides, how different methods tackle the challenges of AutoML on graphs are discussed along the way as well. Then, we review libraries related to automated graph machine learning and discuss AutoGL, the first dedicated framework and open-source library for automated graph machine learning. We highlight the design principles of AutoGL and briefly introduce its usages, which are all specially designed for AutoML on graphs. Last but not least, we point out the potential research directions for both graph HPO and graph NAS, including but not limited to Scalability, Explainability, Outof-distribution generalization, Robustness, and Hardware-aware design etc. We believe this paper will greatly facilitate and further promote the studies and applications of automated graph machine learning in both academia and industry.</p><p>The rest of the paper is organized as follows. In Section 2, we intoduce the fundamentals and preliminaries for automated graph machine learning by briefly introducing basic formulations of graph machine learning and AutoML. We comprehensively discuss HPO based approaches on graph machine learning in Section 3 and NAS based methods for graph machine learning in Section 4. Then, in Section 5.1, we overview related libraries for graph machine learning and automated machine learning and in depth introduce AutoGL, our dedicated and the world's first open-source library tailored for automated graph machine learning. We discuss the tailored benchmark that enables fair, fully reproducible, and efficient empirical comparisons in Section 6. Last but not least, we outline future research opportunities in Section 7 and conclude the whole paper in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FUNDAMENTALS AND PRELIMINARIES OF AUTO-MATED GRAPH MACHINE LEARNING</head><p>We briefly present basic problem formulations for graph machine learning, automated machine learning as well as unique characteristics for automated graph machine learning before moving to the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Machine Learning</head><p>Consider a graph G = (V, E) where V = v 1 , v 2 , ..., v |V| is a set of nodes and E ⊆ V ×V is a set of edges. The neighborhood of node v i is denoted as N (i) = {v j : (v i , v j ) ∈ E}. The nodes can also have features denoted as F ∈ R |V|×f , where f is the number of features. We use bold uppercases (e.g., X) and bold lowercases (e.g., x) to represent matrices and vectors, respectively.</p><p>Most tasks of graph machine learning can be divided into the following two categories:</p><p>• Node-level tasks: the tasks are associated with individual nodes or pairs of nodes. Typical examples include node classification and link prediction. • Graph-level tasks: the tasks are associated with the whole graph, such as graph classification and graph generation. For node-level tasks, graph machine learning models usually learn a node representation H ∈ R |V|×d and then adopt a classifier or predictor on the node representation to solve the task. For graphlevel tasks, a representation for the whole graph is learned and fed into a classifier/predictor. GNNs are the current state-of-the-art in learning node and graph representations. The message-passing framework of GNNs <ref type="bibr" target="#b39">[40]</ref> is formulated as follows.</p><formula xml:id="formula_0">m (l) i = AGG (l) a (l) ij W (l) h (l) i , ∀j ∈ N (i)<label>(1)</label></formula><formula xml:id="formula_1">h (l+1) i = σ COMBINE (l) m (l) i , h (l) i ,<label>(2)</label></formula><p>where h (l) i denotes the node representation of node v i in the l th layer, m (l) is the message for node v i , AGG (l) (•) is the aggregation function, a (l) ij denotes the weights from node v j to node v i , COMBINE (l) (•) is the combining function, W (l) are learnable weights, and σ(•) is an activation function. The node representation is usually initialized as node features H (0) = F, and the final representation is obtained after L message-passing layers H = H (L) .</p><p>For the graph-level representation, pooling methods (also called readout) are applied to the node representations</p><formula xml:id="formula_2">h G = POOL (H) ,<label>(3)</label></formula><p>i.e., h G is the representation of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Automated Machine Learning (AutoML)</head><p>Many AutoML algorithms such as HPO and NAS can be formulated as the following bi-level optimization problem:</p><formula xml:id="formula_3">min α∈A L val (W * (α), α) s.t. W * (α) = arg min W (L train (W, α)) ,<label>(4)</label></formula><p>where α is the optimization objective of the AutoML algorithm, e.g., hyper-parameters in HPO and neural architectures in NAS, A is the feasible space for the objective, and W(α) are trainable weights in the graph machine learning models. Essentially, we aim to optimize the objective in the feasible space so that the model achieves the best results in terms of a validation function, and W * indicates that the weights are fully optimized in terms of a training function. Different AutoML methods differ in how the feasible space is designed and how the objective functions are instantiated and optimized since directly optimizing Eq. ( <ref type="formula" target="#formula_3">4</ref>) requires enumerating and training every feasible objective, which is prohibitive in practice. Typical formulations of automated graph machine learning need to properly integrate the above formulations in Section 2.1 and Section 2.2 to form a new optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Automated Graph Machine Learning</head><p>Automated graph machine learning, which non-trivially combines the strength of AutoML and graph machine learning, faces the following challenges.</p><p>• The uniqueness of graph machine learning: Unlike audio, image, or text, which has a grid structure, graph data lies in a non-Euclidean space <ref type="bibr" target="#b40">[41]</ref>. Thus, graph machine learning usually has unique architectures and designs. For example, typical NAS methods focus on the search space for convolution and recurrent operations, which is distinct from the building blocks of GNNs <ref type="bibr" target="#b41">[42]</ref>. • Complexity and diversity of graph tasks: As aforementioned, graph tasks per se are complex and diverse, ranging from node-level to graph-level problems, and with different settings, objectives, and constraints <ref type="bibr" target="#b42">[43]</ref>. How to impose proper inductive bias and integrate domain knowledge into a graph AutoML method is indispensable. • Scalability: Many real graphs such as social networks or the Web are incredibly large-scale with billions of nodes and edges <ref type="bibr" target="#b43">[44]</ref>. Besides, the nodes in the graph are interconnected and cannot be treated as independent samples. Designing scalable AutoML algorithms for graphs poses significant challenges since both graph machine learning and AutoML are already notorious for being compute-intensive. Approaches with HPO or NAS for graph machine learning reviewed in later sections target at handling at least one of these three challenges. As such, we will discuss approaches for automated graph machine learning from two aspects: i) HPO for graph machine learning and ii) NAS for graph machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HPO FOR GRAPH MACHINE LEARNING</head><p>In this section, we review HPO for graph machine learning. The main challenge here is scalability, i.e., a real graph can have billions of nodes and edges, and each trial on the graph is computationally expensive. Next, we elaborate on how different methods tackle the efficiency challenge. Notice that we omit some straightforward HPO methods such as random search and grid search <ref type="bibr" target="#b28">[29]</ref> since they are applied to graphs without any modification.</p><p>Tu et al. <ref type="bibr" target="#b44">[45]</ref> propose AutoNE, the first HPO method specially designed to tackle the efficiency problem of graphs, to facilitate the graph hyper-parameter optimization for large-scale graph representation learning. AutoNE proposes a transfer paradigm that samples subgraphs as proxies for the large graph. Specifically, AutoNE has three modules: the sampling module, the signature extraction module, and the meta-learning module. In the sampling module, multiple representative subgraphs are sampled from the large graph using a multi-start random walk strategy. Each subgraph learns a representation by the signature extraction module. Then, AutoNE conducts HPO on the sampled subgraphs using Bayesian optimization <ref type="bibr" target="#b30">[31]</ref> and records the results. Finally, using the HPO results and representation of subgraphs to extract metaknowledges, AutoNE fine-tunes hyper-parameters on the large graph using the meta-learning module. In this way, AutoNE achieves satisfactory results while maintaining scalability since the knowledge of multiple HPO trials on the sampled subgraphs and a few HPO trails on the large graph are properly integrated.</p><p>Wang et al. <ref type="bibr" target="#b45">[46]</ref> propose e-AutoGR to further increase the explainability of hyper-parameter optimization for automated graph representation learning, with the help of hyper-parameter importance decorrelation. e-AutoGR employs six fully explainable graph features, i.e., number of nodes, number of edges, number of triangles, global clustering coefficient, maximum total degree value and number of components, as measures for similarity between different graphs. A hyper-parameter decorrelation algorithm (HyperDeco) is proposed to decorrelate the mixed relations among different hyper-parameters given various graph features so that more accurate importance of different hyper-parameters towards model performances can be estimated through any regression approaches. The authors theoretically validate the correctness of the proposed hyper-parameter decorrelation algorithm and empirically discover that first-order proximity is most important for AROPE <ref type="bibr" target="#b46">[47]</ref>, number of walks together with window size is of great importance for DeepWalk <ref type="bibr" target="#b47">[48]</ref>, and dropout is particularly important for GCN <ref type="bibr" target="#b48">[49]</ref>. <ref type="bibr">Guo et al. [50]</ref> propose ITuNE to replace the sampling process of AutoNE with graph coarsening to generate a hierarchical graph synopsis. A similarity measurement module is also proposed to ensure that the coarsened graph shares sufficient similarity with the large graph. Compared with sampling, such graph synopsis can better preserve graph structural information. Therefore, JITuNE argues that the best hyper-parameters in the graph synopsis can be directly transferred to the large graph. Besides, since the graph synopsis is generated in a hierarchy, the granularity can be more easily adjusted to meet the time constraints of downstream tasks.</p><p>Yuan et al. <ref type="bibr" target="#b50">[51]</ref> propose HESGA as another strategy to improve efficiency using a hierarchical evaluation strategy together with evolutionary algorithms. Specifically, HESGA proposes to evaluate the potential of hyper-parameters by interrupting training after a few epochs and calculating the performance gap with respect to the initial performance with random model weights. This gap is used as a fast score to filter out unpromising hyperparameters. Then, the standard full evaluation, i.e., training until convergence, is adopted as the final assessor to select the best hyper-parameters to be stored in the population of the evolutionary algorithm.</p><p>Besides efficiency, Yoon et al. <ref type="bibr" target="#b51">[52]</ref> propose AutoGM to focuses on studying a unified framework for various graph machine learning algorithms. Specifically, AutoGM finds that many popular GNNs and PageRank can be characterized in a framework similar to Eq. ( <ref type="formula" target="#formula_0">1</ref>) with five hyper-parameters: the number of messagepassing neighbors, the number of message-passing steps, the aggregation function, the dimensionality, and the non-linearity. AutoGM also adopts Bayesian optimization to optimize these hyper-parameters.</p><p>Yuan et al. <ref type="bibr" target="#b52">[53]</ref> focus on the impact of selecting two types of GNN hyper-parameters (i.e., graph-related layers and task-specific layers) on the performance of GNN for molecular property prediction. They employed CMA-ES for HPO, which is a derivativefree and evolutionary black-box optimization method. The results reveal that optimizing the two types of hyper-parameters separately can result in improvement on GNN performance, and removing any of the two types of hyper-parameters may result in deteriorated performance. Even doing this means a larger search space, which seems to be more challenging given the same number of trials (limited computational resources), such a strategy can surprisingly achieve better performance. Meanwhile, their study further confirms the importance of HPO for GNNs in molecular property prediction problems.</p><p>Many molecular datasets are far smaller than other datasets in typical deep learning applications. Most HPO methods have not been explored in terms of their performances on these small datasets in molecular domain. Yuan et al. <ref type="bibr" target="#b53">[54]</ref> conduct a theoretical analysis of common and specific features for two stateof-the-art HPO algorithms: i.e., TPE and CMA-ES, and they compare them with random search (RS). Experimental studies are carried out on several benchmarks in MoleculeNet, from different perspectives to investigate the impact of RS, TPE, and CMA-ES on HPO of GNNs for molecular property prediction. Their experimental results indicate that TPE is the most suited HPO method for GNN under molecular property prediction problems with limited computational resources. Meanwhile, RS is the simplest method capable of achieving comparable performance with TPE and CMA-ES.</p><p>GCN models are sensitive to the choice of hyper-parameters such as dropout rate and learning weight decay <ref type="bibr" target="#b45">[46]</ref>, especially for deep GCN models. Zhu et al. <ref type="bibr" target="#b54">[55]</ref> therefore target at automating the training of GCN models through hyper-parameter optimization. To be specific, they propose a self-tuning GCN (ST-GCN) approach by incorporating hypernets in each graph convolutional layer, enabling the joint optimization over GCN model parameters and hyper-parameters. They further extend the approach through incorporating the population based training scheme and adopt a population based training framework to self-tuning GCN, thus alleviating local minima problem via exploring hyper-parameter space globally. Experimental results on three benchmark datasets demonstrate the effectiveness of their approaches in terms of optimizing multi-layer GCNs.</p><p>Bu et al. <ref type="bibr" target="#b55">[56]</ref> analyze the performance of different evolutionary algorithms on automated graph machine learning through experimental study. The experimental results show that evolutionary algorithms can serve as an effective alternative to the traditional hyper-parameter optimization algorithms such as random search, grid search and Bayesian Optimization for GNN.</p><p>Sun et al. <ref type="bibr" target="#b56">[57]</ref> propose AutoGRL, an automated graph representation learning framework for node classification task. Au-toGRL consists of an appropriate search space with four components: data augmentation, feature engineering, hyper-parameter optimization, and architecture search. Given graph data, AutoGRL searches for the best graph representation learning model in the search space using an efficient searching algorithm. Extensive experiments are conducted on four real-world node classification datasets to demonstrate that AutoGRL can automatically find competitive graph representation learning models on specific graph data effectively and efficiently.</p><p>Yang et al. <ref type="bibr" target="#b57">[58]</ref> address the underexplored issue of obtaining reliable and trustworthy predictions using automated Graph Neural Networks (GNNs). It integrates uncertainty estimation into the Hyperparameter Optimization (HPO) problem through a bilevel formulation in a novel model named HyperU-GCN. The upperlevel problem focuses on reasoning uncertainties by developing a probabilistic hypernetwork through a variational Bayesian approach. The lower-level problem targets how the weights in the Graph Convolutional Network (GCN) respond to a distribution of hyperparameters. By incorporating model uncertainty into the hyperparameter space, HyperU-GCN is able to achieve calibrated predictions, similar to Bayesian model averaging over hyperparameters. Experimental results on six public datasets indicate that this approach outperforms several state-of-the-art methods in terms of node classification accuracy and expected calibration error (ECE).</p><p>Lloyd et al. <ref type="bibr" target="#b58">[59]</ref> focus on the challenges of embedding knowledge graphs into low-dimensional spaces, a process that is computationally expensive largely due to hyperparameter optimization. They introduce a novel approach using Sobol sensitivity analysis to evaluate the significance of different hyperparameters in affecting the quality of the embeddings. Through thousands of trials and subsequent regression analysis, they identify considerable variability in the importance of different hyperparameters across various knowledge graphs. This variability is attributed to differences in dataset characteristics. Additionally, the paper makes a unique contribution by identifying data leakage issues in the UMLS knowledge graph and presenting a leakage-robust variant, termed UMLS-43.</p><p>Yoon et al. <ref type="bibr" target="#b59">[60]</ref> address the challenge of selecting the most suitable graph algorithm for specific real-world applications due to the proliferation of algorithms with different problem formu-lations, computational times, and memory footprints. To resolve this, they propose AUTOGM, an automated system for graph mining algorithm development. The paper introduces a unified framework, UNIFIEDGM, which simplifies the search space for graph algorithms by requiring only five parameters for algorithm determination. AUTOGM then uses Bayesian Optimization to find the optimal parameter set for UNIFIEDGM. To assess algorithmic efficacy within a given computational budget, the authors introduce a novel budget-aware objective function. Tests on various real-world datasets show that AUTOGM generates novel graph algorithms that offer the best speed-accuracy trade-off compared to existing models.</p><p>Zhang et al. <ref type="bibr" target="#b60">[61]</ref> address the issue of inefficient hyperparameter (HP) tuning in the context of knowledge graph (KG) learning. The authors first conduct a thorough analysis of different hyper-parameters and their transferability from smaller subgraphs to full graphs. Based on these insights, they introduce a two-stage search algorithm called KGTuner. In the first stage, the algorithm efficiently explores hyper-parameter configurations using small subgraphs. In the second stage, the best-performing configurations are fine-tuned on the full, large-scale graph. Experimental results demonstrate that KGTuner outperforms baseline algorithms, achieving an average relative improvement of 9.1% across four different embedding models when applied to large-scale KGs in the open graph benchmark.</p><p>Yang et al. <ref type="bibr" target="#b61">[62]</ref> present a systematic analysis of the impact of hyperparameters on both factorization-based and graph-samplingbased graph embedding techniques for homogeneous graphs. The authors design generalized techniques that include a wide range of hyperparameters and conduct an exhaustive experimental study with over 3,000 trained embedding models per dataset. The findings reveal that optimal hyperparameter settings, rather than the complexity of the embedding models, largely account for performance gains. The study shows that well-tuned hyperparameters can outperform a collection of 18 state-of-the-art graph embedding models by a margin of 0.30-35.41% across various tasks. Importantly, the paper notes that there is no universal set of hyperparameters that are optimal for all tasks, but offers taskspecific recommendations for hyperparameter settings, which can serve as valuable guidelines for future research in embeddingbased graph analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NAS FOR GRAPH MACHINE LEARNING</head><p>NAS methods can be compared in three aspects <ref type="bibr" target="#b32">[33]</ref>: search space, search strategy, and performance estimation strategy. Next, we review NAS methods for graph machine learning from these three aspects and discuss some designs uniquely for graphs. We mainly review NAS for GNNs fitting Eq. ( <ref type="formula" target="#formula_0">1</ref>), which is the focus of the literature. We summarize the characteristics of different methods in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Search Space</head><p>The first challenge of NAS on graphs is the search space design since the building blocks of graph machine learning are usually distinct from other deep learning models such as CNNs or RNNs. For GNNs, the search space can be divided into the following five categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Micro search space</head><p>Following the message-passing framework in Eq. ( <ref type="formula" target="#formula_0">1</ref>), the micro search space defines how nodes exchange messages with ✓ ✓ ✗ ✗ Fixed ✓ ✗ RNN controller + RL --AGNN [63] ✓ ✗ ✗ ✗ Fixed ✓ ✗ Self-designed controller + RL Inherit Weights -SNAG [64] ✓ ✓ ✗ ✗ Fixed ✓ ✗ RNN controller + RL Inherit Weights Simplify the micro search space PDNAS [65] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot -NAS-GNN [66] ✓ ✗ ✗ ✓ Fixed ✓ ✗ Evolutionary algorithm --AutoGraph [67] ✓ ✓ ✗ ✓ Various ✓ ✗ Evolutionary algorithm --GeneticGNN [68] ✓ ✗ ✗ ✓ Fixed ✓ ✗ Evolutionary algorithm --EGAN [69] ✓ ✓ ✗ ✗ Fixed ✓ ✓ Differentiable One-shot Sample small graphs for efficiency NAS-GCN [70] ✓ ✓ ✓ ✗ Fixed ✗ ✓ Evolutionary algorithm -Handle edge features LPGNAS [71] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Search for quantization options GraphGym [72] ✓ ✓ ✗ ✓ Various ✓ ✓ Random search -Transfer across datasets and tasks SGAS [73] ✓ ✗ ✗ ✗ Fixed ✓ ✓ Self-designed algorithm --Peng et al.</p><p>[74] ✓ ✗ ✗ ✗ Fixed ✗ ✓ CEM-RL [75] -Search spatial-temporal modules GNAS [76] ✓ ✓ ✗ ✗ Various ✓ ✓ Differentiable One-shot -AutoSTG [77] ✗ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot+meta learning Search spatial-temporal modules DSS [78] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Dynamically update search space SANE [79] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot -AutoAttend [80] ✓ ✓ ✗ ✗ Fixed ✓ ✓ Evolutionary algorithm One-shot Cross-layer attention DiffMG [81] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Support heterogeneous graphs DeepGNAS [82] ✓ ✓ ✗ ✗ Various ✓ ✗ Controller +RL -Alleviate over-smoothing LLC [83] ✗ ✓ ✗ ✗ Various ✓ ✗ Differentiable One-shot -FL-AGCNS [84] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Evolutionary algorithm One-shot Federated learning setting G-Cos [85] ✓ ✗ ✗ ✗ Fixed ✓ ✗ Evolutionary algorithm One-shot Accelerator search PAS [86] ✓ ✓ ✓ ✗ Fixed ✗ ✓ Differentiable One-shot -FGNAS [87] ✓ ✗ ✗ ✗ Fixed ✓ ✗ RNN controller +RL -Software-hardware co-design GraphPAS [88] ✓ ✗ ✗ ✗ Fixed ✓ ✗ Evolutionary algorithm Sharing population Parallel search ALGNN [89] ✓ ✓ ✗ ✓ Various ✓ ✗ MOPSO [90] -Consider consumption cost EGNAS [91] ✓ ✓ ✗ ✗ Fixed ✓ ✓ Differentiable One-shot Handle edge features AutoGEL [92] ✓ ✓ ✓ ✗ Fixed ✓ ✓ SNAS [93] One-shot Handle edge features GASSO [94] ✓ ✗ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Graph structure learning G-RNA [95] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Evolutionary algorithm One-shot Search with robustness metrics GRACES [96] ✓ ✗ ✓ ✗ Fixed ✗ ✓ Differentiable One-shot Handle distribution shifts GAUSS [97] ✓ ✗ ✗ ✗ Fixed ✓ ✗ Evolutionary algorithm One-shot Handle large-scale graphs PasCa [98] ✓ ✗ ✗ ✗ Various ✓ ✗ Bayesian Optimization -Decouple neural message passing PMMM [99] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Search meta paths DHGAS [100] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Search spatial-temporal modules </p><formula xml:id="formula_4">a const ij = 1 GCN a gcn ij = 1 √ |N (i)||N (j)| GAT a gat ij = LeakyReLU (ATT (Wa [h i , h j ])) SYM-GAT a sym ij = a gat ij + a gat ji COS a cos ij = cos (Wah i , Wah j ) LINEAR a lin ij = tanh (sum (Wah i + Wah j )) GENE-LINEAR a gene ij = tanh (sum (Wah i + Wah j )) W ′ a</formula><p>others in each layer. Commonly adopted micro search spaces <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b62">[63]</ref> compose the following components:</p><p>• Aggregation function AGG(•): SUM, MEAN, MAX, and MLP. • Aggregation weights a ij : common choices are listed in Table 2. • Number of heads when using attentions: 1, 2, 4, 6, 8, 16, etc. • Combining function COMBINE(•): CONCAT, ADD, and MLP. • Dimensionality of h l : 8, 16, 32, 64, 128, 256, 512, etc.</p><p>• Non-linear activation function σ(•): Sigmoid, Tanh, ReLU, Identity, Softplus, Leaky ReLU, ReLU6, and ELU. However, directly searching all these components results in thousands of possible choices in a single message-passing layer. Thus, it may be beneficial to prune the space to focus on a few crucial components depending on applications and domain knowledge <ref type="bibr" target="#b63">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Macro search space</head><p>Similar to residual connections and dense connections in CNNs, node representations in one layer of GNNs do not necessarily solely depend on the immediate previous layer <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b101">[102]</ref>. These connectivity patterns between layers form the macro search space. Formally, such designs are formulated as</p><formula xml:id="formula_5">H (l) = j&lt;l F jl H (j) ,<label>(5)</label></formula><p>where F jl (•) can be the message-passing layer in Eq. ( <ref type="formula" target="#formula_0">1</ref>), ZERO (i.e., not connecting), IDENTITY (e.g., residual connections), or an MLP. Since the dimensionality of H (j) can vary, IDENTITY can only be adopted if the dimensionality of each layer matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Pooling methods</head><p>To handle graph-level tasks, information from all the nodes are aggregated to form graph-level representations using the pooling operation in Eq. (3). Jiang et al. <ref type="bibr" target="#b69">[70]</ref> propose a pooling search space including row-or column-wise sum, mean, or maximum, attention pooling, attention sum, and flatten. More advanced methods such as hierarchical pooling <ref type="bibr" target="#b102">[103]</ref> could also be added to the search space with careful designs. For example, PAS <ref type="bibr" target="#b85">[86]</ref> further proposes to search for adaptive pooling architectures. Firstly they design a unified framework consisting of four modules: Aggregation, Pooling, Read out and Merge, which can cover existing human-designed pooling methods (global and hierarchical) for graph classification. Based on this framework, a novel search space is designed by incorporating popular operations in humandesigned architectures. To further enable efficient search, a coarsening strategy is proposed to continuously relax the search space, with the utilization of differentiable search methods. Extensive experiments on six real-world datasets from three domains are conducted, and the results demonstrate the effectiveness and efficiency of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Hyper-parameters</head><p>Besides architectures, other training hyper-parameters can be incorporated into the search space, i.e., similar to jointly conducting NAS and HPO. Typical hyper-parameters include the learning rate, the number of epochs, the batch size, the optimizer, the dropout rate, and the regularization strengths such as the weight decay. These hyper-parameters can be jointly optimized with architectures or separately optimized after the best-architectures are found. HPO methods in Section 3 can also be combined here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Layers</head><p>Another critical model choice not incorporated in the above four categories is the number of message-passing layers. Unlike CNNs, most currently successful GNNs are shallow, e.g., with no more than three layers, possibly due to the over-smoothing problem <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b100">[101]</ref>. Limited by this problem, the existing NAS methods for GNNs preset the number of layers as a fixed small number. Except for a recent attempt DeepGNAS <ref type="bibr" target="#b81">[82]</ref>, how to automatically design deep GNNs while integrating techniques to alleviate over-smoothing remains mostly unexplored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Search Strategy</head><p>The search strategy can be broadly divided into three categories: architecture controllers trained with reinforcement learning (RL), differentiable methods, and evolutionary algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Controller + RL</head><p>A widely adopted NAS search strategy is to use a controller to generate the neural architecture descriptions and train the controller with reinforcement learning to maximize the model performance as rewards. For example, if we consider neural architecture descriptions as a sequence, we can use RNNs as the controller <ref type="bibr" target="#b34">[35]</ref>. Such methods can be directly applied to GNNs with a suitable search space and performance evaluation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Differentiable</head><p>Differentiable NAS methods such as DARTS <ref type="bibr" target="#b35">[36]</ref> and SNAS <ref type="bibr" target="#b92">[93]</ref> have gained popularity in recent years. Instead of optimizing different operations separately, differentiable methods construct a single super-network (known as the one-shot model) containing all possible operations. Formally, we denote</p><formula xml:id="formula_6">y = o (x,y) (x) = o∈O exp(z (x,y) o ) o ′ ∈O exp(z (x,y) o ′ ) o(x),<label>(6)</label></formula><p>where o (x,y) (x) is an operation in the GNN with input x and output y, O are all candidate operations, and z (x,y) are learnable vectors to control which operation is selected. Briefly speaking, each operation is regarded as a probability distribution of all possible operations. In this way, the architecture and model weights can be jointly optimized via gradient-based algorithms. The main challenges lie in making the NAS algorithm differentiable, where several techniques such as Gumbel-softmax <ref type="bibr" target="#b104">[105]</ref> and concrete distribution <ref type="bibr" target="#b105">[106]</ref> are resorted to. When applied to GNNs, slight modification may be needed to incorporate the specific operations defined in the search space, but the general idea of differentiable methods remains unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Evolutionary Algorithms</head><p>Evolutionary algorithms are a class of optimization algorithms inspired by biological evolution. For NAS, randomly generated architectures are considered initial individuals in a population. Then, new architectures are generated using mutations and crossover operations based on the population. The architectures are evaluated and selected to form the new population, and the same process is repeated. The best architectures are recorded while updating the population, and the final solutions are obtained after sufficient updating steps. For GNNs, regularized evolution (RE) NAS <ref type="bibr" target="#b38">[39]</ref> has been widely adopted. RE's core idea is an aging mechanism, i.e., in the selection process, the oldest individuals in the population are removed. Genetic-GNN <ref type="bibr" target="#b106">[107]</ref> also proposes an evolution process to alternatively update the GNN architecture and the learning hyper-parameters to find the best fit of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Combinations</head><p>It is also feasible to combine these three types of search strategies mentioned above. For example, AGNN <ref type="bibr" target="#b62">[63]</ref> proposes a reinforced conservative search strategy by adopting both RNNs and evolutionary algorithms in the controller and train the controller with RL. By only generating slightly different architectures, the controller can find well-performing GNNs more efficiently. Peng et al. <ref type="bibr" target="#b73">[74]</ref> adopt CEM-RL <ref type="bibr" target="#b74">[75]</ref>, which combines evolutionary and differentiable methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Estimation Strategy</head><p>Due to the large number of possible architectures, it is infeasible to fully train each architecture independently. Next, we review some performance estimation strategies.</p><p>A commonly adopted "trick" to speed up performance estimation is to reduce fidelity <ref type="bibr" target="#b32">[33]</ref>, e.g., by reducing the number of epochs or the number of data points. This strategy can be directly generalized to GNNs.</p><p>Another strategy successfully applied to CNNs is sharing weights among different models, known as parameter sharing or weight sharing <ref type="bibr" target="#b36">[37]</ref>. For differentiable NAS with a large oneshot model, parameter sharing is naturally achieved since the architectures and weights are jointly trained. However, training the one-shot model may be difficult since it contains all possible operations. To further speed up the training process, single-path one-shot model <ref type="bibr" target="#b107">[108]</ref> has been proposed where only one operation between an input and output pair is activated during each pass.</p><p>For NAS without a one-shot model, sharing weights among different architecture is more difficult but not entirely impossible. For example, since it is known that some convolutional filters are common feature extractors, inheriting weights from previous architectures is feasible and reasonable in CNNs <ref type="bibr" target="#b108">[109]</ref>. However, since there is still a lack of understandings of what weights in GNNs represent, we need to be more cautious about inheriting weights <ref type="bibr" target="#b63">[64]</ref>. AGNN <ref type="bibr" target="#b62">[63]</ref> proposes three constraints for parameter inheritance: same weight shapes, same attention and activation functions, and no parameter sharing in batch normalization and skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Recent Featured Works</head><p>In this section, we discuss several recent advances in automated graph machine learning that feature in taking topological structure learning, robustness and generalization, scalability, data heterogeneity, efficient architecture search or software-hardware codesign into considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Architecture Search with Graph Structure Learning</head><p>Qin et al. <ref type="bibr" target="#b93">[94]</ref> investigate the important question that how NAS is able to select the desired GNN architectures by conducting a measurement study with experiments, which discovers that gradient based NAS methods tend to select proper architectures based on the usefulness of different types of information with respect to the target task. The explorations further show that gradient based NAS also suffers from noises hidden in the graph, resulting in searching suboptimal GNN architectures. Based on these findings, they propose a Graph differentiable Architecture Search model with Structure Optimization (GASSO), which allows differentiable search of the architecture with gradient descent and is able to discover graph neural architectures with better performance through employing graph structure learning as a denoising process in the search procedure. The proposed GASSO model is capable of simultaneously searching the optimal architecture and adaptively adjusting graph structure by jointly optimizing graph architecture search and graph structure denoising. Extensive experiments on real-world graph datasets demonstrate that the proposed GASSO model is able to achieve state-of-the-art performance compared with existing baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Robust and Generalizable Graph NAS</head><p>G-RNA <ref type="bibr" target="#b94">[95]</ref> enhances the robustness of graph NAS methods against adversarial attacks. G-RNA designs a robust search space for the message-passing mechanism by incorporating graph structure mask operations. The graph structure mask operations cover important robust essences of graph structure and could recover various existing defense methods as well. The framework also defines a robustness metric to guide the search process and filter robust architectures. Specifically, G-RNA uses an attack proxy to produce several adversarial samples based on the clean graph, and it searches robust GNNs using the robustness metric with clean and generated adversarial samples.</p><p>Besides the robustness, GRACES <ref type="bibr" target="#b95">[96]</ref> addresses the limitations of existing graph neural architecture search methods in dealing with distribution shifts between training and test graphs. GRACES tailors a customized GNN architecture suitable for each graph instance to handle the distribution shifts. GRACES uses a self-supervised disentangled graph encoder <ref type="bibr" target="#b109">[110]</ref>, <ref type="bibr" target="#b110">[111]</ref> to characterize invariant factors hidden in diverse graph structures. It further proposes a prototype based architecture self-customization strategy to tailor specialized GNN architectures for graphs based on the similarities of their representations with operation prototypes vectors in the latent space. Finally, the customized supernet provides differentiable weights on the mixture of different operations. GRACES model can be easily optimized in an end-to-end fashion through gradient based methods 4.4.3 Large-scale Graph NAS Guan et al. <ref type="bibr" target="#b96">[97]</ref> presents the graph architecture search at scale (GAUSS) method, designed to address the limitations of existing graph NAS approaches in handling large-scale graphs. Traditional graph NAS methods are computationally intensive and suffer from the consistency collapse issues, making them unsuitable for large graphs. GAUSS tackles these problems by introducing an efficient light-weight supernet and a joint architecture-graph sampling technique. Specifically, it proposes a graph sampling-based singlepath one-shot supernet to minimize computational load. To handle consistency issues, the method incorporates a unique architecture peer learning mechanism on sampled sub-graphs, as well as an architecture importance sampling algorithm. These innovations aim to smooth the highly non-convex optimization objective and stabilize the architecture sampling process. Theoretical analyses and empirical tests on five different datasets, ranging from 10 4 to 10 8 vertices, show that GAUSS outperforms existing GNAS methods, marking it as the first framework capable of efficiently handling large-scale graphs with billions of edges within a single GPU day.</p><p>Zhang et al. <ref type="bibr" target="#b97">[98]</ref> further introduces PasCa, a new system aimed at systematically exploring the design space for scalable graph neural networks. It addresses the limitations of current GNNs that are not well-suited for scalability. Through the deconstruction of the neural message-passing mechanism, the authors propose a novel scalable graph neural architecture paradigm (SGAP) that includes a design space with up to 150,000 different architectures. To navigate this expansive design space, it also presents an autosearch engine capable of multi-objective optimization to find GNN architectures that are both efficient and accurate. Empirical studies across ten benchmark datasets reveal that the architectures discovered by PasCa, specifically PasCa-V3, not only offer competitive predictive accuracy but also achieve up to 28.3 times faster training speeds compared to state-of-the-art methods like JK-Net <ref type="bibr" target="#b101">[102]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Heterogeneous Graph NAS</head><p>Heterogeneous information networks (HINs) are used to describe real-world data with intricate entities and relationships. Several recent works <ref type="bibr" target="#b111">[112]</ref>, <ref type="bibr" target="#b112">[113]</ref>, <ref type="bibr" target="#b98">[99]</ref> study the neural architecture search on HINs to handle the heterogeneous node types and relationships. Specifically, Li et al. <ref type="bibr" target="#b98">[99]</ref> proposes a method called Partial Message Meta Multigraph search (PMMM) for automatically optimizing the neural architecture design on Heterogeneous Information Networks (HINs), aiming at better stability and flexibility. It adopts an efficient differentiable framework to search for a meaningful meta multigraph, which captures more flexible and complex semantic relations than a meta graph. The authors also propose a stable algorithm called partial message search to ensure that the searched meta multigraph consistently outperforms manually designed meta-structures. Experimental results on six benchmark datasets for node classification and recommendation tasks demonstrate the effectiveness of PMMM. The proposed method outperforms state-of-the-art heterogeneous GNNs, discovers meaningful meta multigraphs, and exhibits significantly improved stability.</p><p>Zhang et al. <ref type="bibr" target="#b99">[100]</ref> first consider the temporal information on heterogeneous graphs, and propose a method called Dynamic Heterogeneous Graph Attention Search (DHGAS) for automating the design of Dynamic Heterogeneous Graph Neural Networks (DHGNNs). The existing DHGNNs are manually designed and lack adaptability to diverse dynamic heterogeneous graph scenarios. To overcome this limitation, the authors introduce a search space that considers spatial-temporal dependencies and heterogeneous interactions in graphs and develop an efficient search algorithm. The proposed DHGAS method can automatically discover optimal DHGNN architectures without human guidance. The authors introduce a unified dynamic heterogeneous graph attention (DHGA) framework that enables nodes to attend to their heterogeneous and dynamic neighbors. They also design a localization space to determine attention application and a parameter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Graph Transformer Architecture Search</head><p>Besides searching the architecture of graph neural networks, AutoGT <ref type="bibr" target="#b113">[114]</ref> is proposed to search the architecture of graph transformer, which is another type of strong graph encoder <ref type="bibr" target="#b114">[115]</ref>. However, unlike their applications in text and image data, using Transformers for graph data involves additional complexities due to the non-euclidean nature of graphs. The authors identify two main challenges. The first challenge is how to creat a unified search space for graph Transformers. The second challenge is how to handle the complex relationship between the architecture of each Transformer layer and graph encoding strategies. To address these challenges, they introduce Automated Graph Transformer (AutoGT), a neural architecture search framework designed for graphs. AutoGT uses a unified graph Transformer formulation and includes a comprehensive search space that considers both architectural and encoding options. To manage the coupling between architecture and graph encodings, the authors propose an encoding-aware performance estimation strategy. Through rigorous experiments, they demonstrate that AutoGT outperforms stateof-the-art hand-crafted models, establishing its effectiveness and broad applicability. <ref type="bibr" target="#b84">[85]</ref> is a GNN and accelerator co-search framework that can automatically search for matched GNN structures and accelerators to maximize both task accuracy and acceleration efficiency. Specifically, G-CoS integrates two major components: i) a generic GNN accelerator search space which is applicable to various GNN structures and ii) a one-shot GNN and accelerator co-search algorithm that enables simultaneous and efficient search for optimal GNN structures as well as their matched accelerators. Extensive experiments and ablation studies show that the GNNs together with accelerators generated by G-CoS consistently outperforms state-of-the-art GNNs and GNN accelerators in terms of both task accuracy and hardware efficiency, while only requiring a few hours for the end-to-end generation of the best matched GNNs and their accelerators. Similarly, LPGNAS <ref type="bibr" target="#b70">[71]</ref> jointly searches for architectures and quantisation choices so that both model and buffer sizes can be greatly reduced while keeping similar accuracy as other methods. Their empirical results show that 4-bit weights, 8-bit activations quantisation strategy might be the key for GNNs. ALGNN <ref type="bibr" target="#b88">[89]</ref> considers the computation cost and complexity of the searched model using a multi-objective optimization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.6">Search of Efficient Architectures G-Cos</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.7">Co-design of Software and Hardware</head><p>Lu et al. <ref type="bibr" target="#b86">[87]</ref> propose FGNAS as the first software-hardware codesign framework for automating the search and deployment of GNNs. Using FPGA as the target platform, the FGNAS framework is able to perform the FPGA-aware graph neural architecture search. FPGA is employed as the vehicle for illustration and implementation of the methods. Specific hardware constraints are considered so that quantization is adopted to compress the model. Under specific hardware constraints, they show the FGNAS framework can successfully identify a solution of higher accuracy while using shorter time than random search and the traditional twostep tuning. To evaluate the design, they conduct experiments on benchmark datasets, i.e., Cora, CiteSeer and PubMed, and the results show that the proposed FGNAS framework has better capability in optimizing the accuracy of GNNs when the hardware implementation is specifically constrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussions</head><p>In this section, we will discuss other unique NAS designs for graphs in terms of search space, transferability and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">The search space</head><p>Besides the basic search space presented in Section 4.1, different graph tasks may require other search spaces. For example, metapaths are critical for heterogeneous graphs <ref type="bibr" target="#b80">[81]</ref>, edge features are essential in modeling molecular graphs <ref type="bibr" target="#b69">[70]</ref> and many graph tasks <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b90">[91]</ref>, and spatial-temporal modules are needed in skeleton-based recognition <ref type="bibr" target="#b73">[74]</ref> and traffic forcasting <ref type="bibr" target="#b76">[77]</ref>. A suitable search space usually requires careful designs and domain knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Transferability</head><p>It is non-trivial to transfer GNN architectures across different datasets and tasks due to the complexity and diversity of graph tasks. GraphGym <ref type="bibr" target="#b115">[116]</ref> propose to adopt a fixed set of GNNs as anchors and rank the performance of these GNNs on different tasks and datasets. Then, the rank correlation serves as a metric to measure the similarities between different datasets and tasks. The best-performing GNNs of the most similar tasks are transferred to solve the target tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">The scalability challenge of large-scale graphs</head><p>Similar to AutoNE introduced in Section 3, EGAN <ref type="bibr" target="#b68">[69]</ref> proposes to sample small graphs as proxies and conduct NAS on the sampled subgraphs to improve the efficiency of NAS. While achieving some progress, more advanced and principle approaches are further needed to handle billion-scale graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIBRARIES FOR AUTOMATED GRAPH MACHINE LEARNING</head><p>Although there have been quite a few libraries for both graph machine learning and automated machine learning, there is no but one library for automated graph machine learning. Therefore, we will briefly overview libraries for graph machine learning and automated machine learning, followed by the in-depth introduction of the world's first dedicated open-source automated graph machine learning library, AutoGL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Libraries for Graph Machine Learning and Automated Machine Learning</head><p>Publicly available libraries are important to facilitate and advance the research and applications of AutoML on graphs. First, we briefly list libraries for graph machine learning and automated machine learning, respectively.</p><p>Graph Machine Learning Libraries Popular libraries for graph machine learning include PyTorch Geometric <ref type="bibr" target="#b116">[117]</ref>, Deep Graph Library <ref type="bibr" target="#b117">[118]</ref>, GraphNets <ref type="bibr" target="#b118">[119]</ref>, AliGraph <ref type="bibr" target="#b119">[120]</ref>, Euler <ref type="bibr" target="#b120">[121]</ref>, PBG <ref type="bibr" target="#b121">[122]</ref>, PGL <ref type="bibr" target="#b122">[123]</ref>, TF-GNN <ref type="bibr" target="#b123">[124]</ref>, Stellar Graph <ref type="bibr" target="#b124">[125]</ref>, Spektral <ref type="bibr" target="#b125">[126]</ref>, CodDL <ref type="bibr" target="#b126">[127]</ref>, OpenNE <ref type="bibr" target="#b127">[128]</ref>, OpenHGNN <ref type="bibr" target="#b128">[129]</ref>, GEM <ref type="bibr" target="#b129">[130]</ref>, Karateclub <ref type="bibr" target="#b130">[131]</ref> and classical NetworkX <ref type="bibr" target="#b131">[132]</ref>. However, these libraries do not support AutoML.</p><p>Automated Machine Learning Libraries On the other hand, AutoML libraries such as NNI <ref type="bibr" target="#b132">[133]</ref>, AutoKeras <ref type="bibr" target="#b133">[134]</ref>, Au-toSklearn <ref type="bibr" target="#b134">[135]</ref>, Hyperopt <ref type="bibr" target="#b135">[136]</ref>, TPOT <ref type="bibr" target="#b136">[137]</ref>, AutoGluon <ref type="bibr" target="#b137">[138]</ref>, MLBox <ref type="bibr" target="#b138">[139]</ref>, and MLJAR <ref type="bibr" target="#b139">[140]</ref> are widely adopted. Unfortunately, because of the uniqueness and complexity of graph tasks, they cannot be directly applied to automate graph machine learning.</p><p>Despite their successes, integrating these libraries to fully support automated graph machine learning is non-trivial. This motivates us to design a specific library tailored for automated graph machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">AutoGL: A Library for Automated Graph Machine Learning</head><p>To fill this gap, we present Automated Graph Learning (AutoGL) <ref type="foot" target="#foot_0">2</ref> , the first dedicated framework and library for automated graph machine learning. The overall framework of AutoGL is shown in Figure <ref type="figure" target="#fig_2">1</ref>. We summarize and abstract the pipeline of AutoML on graphs into five modules: auto feature engineering, neural architecture search, model training, hyper-parameter optimization, and auto ensemble. For each module, we provide plenty of stateof-the-art algorithms, standardized base classes, and high-level APIs for easy and flexible customization. The AutoGL library is built upon PyTorch Geometric (PyG) <ref type="bibr" target="#b116">[117]</ref>, a widely adopted graph machine learning library. AutoGL has the following key characteristics:</p><p>• Open source: The code <ref type="foot" target="#foot_1">3</ref> and detailed documentation<ref type="foot" target="#foot_2">foot_2</ref> are available online. • Easy to use: AutoGL is designed to be user-friendly. Users can conduct quick AutoGL experiments with less than ten lines of code. • Flexible to be extended: The modular design, high-level base class APIs, and extensive documentation of AutoGL allow flexible and easy customized extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Detailed Designs of AutoGL</head><p>In this section, we introduce AutoGL designs in detail. AutoGL is designed in a modular and object-oriented fashion to enable clear logic flows, easy usages, and flexible extensions. All the APIs exposed to users are abstracted in a high-level fashion to avoid redundant re-implementation of models, algorithms, and train/evaluation protocols. All the five main modules, i.e., auto feature engineering, neural architecture search, model training, hyper-parameter optimization and auto ensemble, have taken into account the unique characteristics of graph machine learning. Next, we elaborate on the detailed designs for each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">AutoGL Dataset</head><p>We first briefly introduce our dataset management. AutoGL Dataset is currently based on Dataset from PyTorch Geometric and supports common benchmarks for node and graph classification, including the recent Open Graph Benchmark <ref type="bibr" target="#b42">[43]</ref>. We present the complete list of datasets in Table <ref type="table" target="#tab_4">3</ref>, and users can also easily customize datasets following our documentation. Specifically, we provide widely adopted node classification datasets including Cora, CiteSeer, PubMed <ref type="bibr" target="#b140">[141]</ref>, Amazon Computers, Amazon Photo, Coauthor CS, Coauthor Physics <ref type="bibr" target="#b141">[142]</ref>, Reddit <ref type="bibr" target="#b142">[143]</ref>, and graph classification datasets such as MU-TAG <ref type="bibr" target="#b143">[144]</ref>, PROTEINS <ref type="bibr" target="#b144">[145]</ref>, IMDB-B, IMDB-M, COL-LAB <ref type="bibr" target="#b145">[146]</ref>, etc. Datasets from Open Graph Benchmark <ref type="bibr" target="#b42">[43]</ref> are also supported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Auto Feature Engineering</head><p>The graph data is first processed by the auto feature engineering module, where various nodes, edges, and graph-level features can be automatically added, compressed, or deleted to help boost the graph learning process after. Graph topological features can also be extracted to utilize graph structures better.</p><p>Currently, we support 24 feature engineering operations abstracted into three categories: generators, selectors, and graph features. The generators aim to create new node and edge features based on the current node features and graph structures. The selectors automatically filter out and compress features to ensure they are compact and informative. Graph features focus on generating graph-level features.</p><p>We summarize the supported generators in Table <ref type="table" target="#tab_5">4</ref>, including Graphlets <ref type="bibr" target="#b146">[147]</ref>, EigenGNN <ref type="bibr" target="#b147">[148]</ref>, PageRank <ref type="bibr" target="#b148">[149]</ref>, local degree profile, normalization, one-hot degrees, and one-hot node IDs. For selectors, GBDT <ref type="bibr" target="#b149">[150]</ref> and FilterConstant are supported. An automated feature engineering method DeepGL <ref type="bibr" target="#b150">[151]</ref> is also supported, functioning as both a generator and a selector. For graph feature, Netlsd <ref type="bibr" target="#b151">[152]</ref> and a set of graph feature extractors implemented in NetworkX <ref type="bibr" target="#b131">[132]</ref> are wrapped, e.g., NxTransitivity, NxAverageClustering, etc.</p><p>We also provide convenient wrappers that support feature engineering operations in PyTorch Geometric <ref type="bibr" target="#b116">[117]</ref> and NetworkX <ref type="bibr" target="#b131">[132]</ref>. Users can easily customize feature engineering methods by inheriting from the class BaseGenerator, BaseSelector, and BaseGraph, or BaseFeatureEngineer if the methods do not fit in our categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Neural Architecture Search</head><p>In AutoGL, Neural Architecture Search (NAS) aims to automate the construction of Graph Neural Networks. The best GNN model will be searched using various NAS methods to fit the current datasets. In Neural Architecture Search module, Algorithm, GNNSpace, and Estimator submodule is developed to further solve the search problem. GNNSpace defines the whole search range where we explore the best models. Algorithms are used to determine which architectures should be evaluated next, and the Estimators are used for deriving the performances of target architectures.</p><p>We have supported various NAS models, including algorithms specified for graph data like AutoNE <ref type="bibr" target="#b44">[45]</ref> and AutoGR <ref type="bibr" target="#b45">[46]</ref> and general-purpose algorithms like random search <ref type="bibr" target="#b28">[29]</ref>, Tree Parzen Estimator <ref type="bibr" target="#b29">[30]</ref>, etc. Users can customize HPO algorithms by inheriting from the BaseHPOptimizer class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Model Training</head><p>This module handles the training and evaluation process of graph machine learning tasks with two functional sub-modules: Model and Trainer. Model handles the construction of graph machine learning models, e.g., GNNs, by defining learnable parameters and the forward pass. Trainer controls the optimization process for the given model.  GAT <ref type="bibr" target="#b152">[153]</ref>, and GraphSAGE <ref type="bibr" target="#b142">[143]</ref>, GIN <ref type="bibr" target="#b153">[154]</ref>, and pooling methods such as Top-K Pooling <ref type="bibr" target="#b154">[155]</ref> are supported. Users can quickly implement their own graph models by inheriting from the BaseModel class and add customized tasks or optimization methods by inheriting from BaseTrainer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Hyper-Parameter Optimization</head><p>The Hyper-Parameter Optimization (HPO) module aims to automatically search for the best hyper-parameters of a specified model and training process, including but not limited to architecture hyper-parameters such as the number of layers, the dimensionality of node representations, the dropout rate, the activation function, and training hyper-parameters such as the learning rate, the weight decay, the number of epochs. The hyper-parameters, their types (e.g., integer, numerical, or categorical), and feasible ranges can be easily set.</p><p>We have supported various HPO algorithms, including algorithms specified for graph data like AutoNE <ref type="bibr" target="#b44">[45]</ref> and AutoGR <ref type="bibr" target="#b45">[46]</ref> and general-purpose algorithms like random search <ref type="bibr" target="#b28">[29]</ref>, Tree Parzen Estimator <ref type="bibr" target="#b29">[30]</ref>, etc. Users can customize HPO algorithms by inheriting from the BaseHPOptimizer class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.6">Auto Ensemble</head><p>This module can automatically integrate the optimized individual models to form a more powerful final model. Currently, we have adopted two kinds of ensemble methods: voting and stacking. Voting is a simple yet powerful ensemble method that directly averages the output of individual models. Stacking trains another meta-model to combine the output of models. We have supported general linear models (GLM) and gradient boosting machines (GBM) as meta-models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.7">AutoGL Solver</head><p>On top of the modules mentioned above, we provide another highlevel API Solver to control the overall pipeline. In Solver, the five modules are integrated systematically to form the final model. Solver receives the feature engineering module, a model list, the HPO module, and the ensemble module as initialization arguments to build an Auto Graph Learning pipeline. Given a dataset and a task, Solver first perform auto feature engineering to clean and augment the input data, then optimize all the given models using the model training and HPO module. At last, the optimized best models will be combined by the Auto Ensemble module to form the final model.</p><p>Solver also provides global controls of the AutoGL pipeline. For example, the time budget can be explicitly set to restrict the maximum time cost, and the training/evaluation protocols can be selected from plain dataset splits or cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation of AutoGL</head><p>In this section, we provide experimental results. Note that we mainly want to showcase the usage of AutoGL and its main functional modules rather than aiming to achieve the new stateof-the-art on benchmarks or compare different algorithms. For node classification, we use Cora, CiteSeer, and PubMed with the standard dataset splits from <ref type="bibr" target="#b48">[49]</ref>. For graph classification, we follow the setting in <ref type="bibr" target="#b155">[156]</ref> and report the average accuracy of 10fold cross-validation on MUTAG, PROTEINS, and IMDB-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AutoGL Results</head><p>We turn on all the functional modules in AutoGL, and report the fully automated results in Table <ref type="table" target="#tab_6">5</ref> and Table <ref type="table" target="#tab_7">6</ref>. We use the best single model for graph classification under the cross-validation setting. We observe that in all the benchmark datasets, AutoGL achieves better results than vanilla models, demonstrating the importance of AutoML on graphs and the effectiveness of the proposed pipeline in the released library.  Hyper-Parameter Optimization Table <ref type="table" target="#tab_8">7</ref> reports the results of two implemented HPO methods, i.e., random search and TPE <ref type="bibr" target="#b29">[30]</ref>, for the semi-supervised node classification task. As shown in the table, as the number of trials increases, both HPO methods tend to achieve better results. Besides, both methods outperform vanilla models without HPO. Note that a larger number of trials do not guarantee better results because of the potential overfitting problem. We further test these HPO methods with ten trials for the graph classification task and report the results in Table <ref type="table" target="#tab_9">8</ref>. The results generally show improvements over the default hand-picked parameters on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auto Ensemble</head><p>Table <ref type="table" target="#tab_10">9</ref> reports the performance of the ensemble module as well as its base learners for the node classification task. We use voting as the example ensemble method and choose GCN and GAT as the base learners. The table shows that the ensemble module achieves better performance than both the base learners, demonstrating the effectiveness of the implemented ensemble module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Advanced Functions of AutoGL</head><p>We have presented AutoGL, the first library for automated graph machine learning, which is open-source, easy to use, and flexible to be extended. Currently, we are actively developing AutoGL and have supported the following advanced functionalities:</p><p>• Support more graph models. We have supported selfsupervised graph models and robust graph models now. • Handle more graph tasks. We have supported heterogeneous node classification tasks now and plan to support more complex spatial-temporal graphs.</p><p>• Support more graph library backends. We have supported Deep Graph Library (DGL) <ref type="bibr" target="#b117">[118]</ref> backend including homogeneous node classification, link prediction, and graph classification tasks. AutoGL is also compatible with PyG 2.0 <ref type="bibr" target="#b116">[117]</ref> now. All kinds of inputs and suggestions are also warmly welcomed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">BENCHMARK</head><p>To promote the further development of automated graph machine learning, NAS-Bench-Graph <ref type="bibr" target="#b156">[157]</ref> is proposed as one benchmark tailored to enable unified, reproducible, and efficient evaluations for graph NAS. Two key issues are identified: i) the lack of a standard experimental setting, making comparisons across research unreliable and non-reproducible, and ii) the computational inefficiency of graph NAS methods. To resolve these issues, NAS-Bench-Graph constructs a unified search space, encompassing 26,206 unique GNN architectures, and offers a standardized evaluation protocol. All architectures have been trained and evaluated on nine representative graph datasets, with metrics such as training, validation, and test performance, latency, and the number of parameters recorded. This results in a look-up table that allows for fair and efficient comparisons without additional computational costs. The authors also showcase the benchmark's compatibility with existing GraphNAS libraries like AutoGL and NNI. The work claims to be the first of its kind to offer a benchmark in the domain of GraphNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benchmark Design</head><p>Here we elucidates the methodology employed in the development of our benchmark for graph NAS, encapsulating search space design (Section 6.1.1), utilized datasets (Section 6.1.2), and experimental configurations (Section 6.1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Search Space Design</head><p>To achieve a judicious equilibrium between effectiveness and efficiency, we propose an intricately crafted, yet computationally tractable search space. In particular, we conceptualize the macro search space governing Graph Neural Network (GNN) architectures as a Directed Acyclic Graph (DAG), thereby providing a formal structure for the computational paradigm 5 . In this DAG, each computational node signifies a vertex representation, while each edge symbolizes a specific operation. The DAG is composed of six nodes, including the input and output nodes. Notably, each intermediate node is restricted to having a single incoming edge. As a result, the aforementioned DAG encompasses nine selectable configurations, as delineated in Figure <ref type="figure">2</ref>. Any intermediate nodes devoid of successor nodes are concatenated to the output node.</p><p>In addition to this overarching macro space, we also incorporate optional fully-connected layers in the pre-processing and post-processing layers, following the design principles established in GraphGym <ref type="bibr" target="#b71">[72]</ref> and PasCa <ref type="bibr" target="#b97">[98]</ref>. To circumvent an exponential increase in the complexity of the search space, the number of layers in both pre-processing and post-processing phases are treated as hyperparameters, a point that will be elaborated upon in Section 6.1.3. To culminate the architecture, a task-specific fullyconnected layer is employed to generate the model's prediction. <ref type="bibr" target="#b4">5</ref>. For disambiguation, we refer to nodes and edges in the computational graph, as opposed to vertices and links that constitute the graph data.  </p><p>Fig. <ref type="figure">2</ref>: Nine different choices of our macro search space. Each node is a representation of vertices and each edge is an operation <ref type="bibr" target="#b156">[157]</ref>. For the set of admissible operations, we focus on seven GNN layers that have garnered widespread acceptance in the literature: GCN <ref type="bibr" target="#b48">[49]</ref>, GAT <ref type="bibr" target="#b152">[153]</ref>, GraphSAGE <ref type="bibr" target="#b142">[143]</ref>, GIN <ref type="bibr" target="#b153">[154]</ref>, ChebNet <ref type="bibr" target="#b157">[158]</ref>, ARMA <ref type="bibr" target="#b158">[159]</ref>, and k-GNN <ref type="bibr" target="#b159">[160]</ref>. Furthermore, we introduce the Identity operation to facilitate residual connections and a fully-connected layer that does not exploit graph structures.</p><p>In summation, the search space we designed is remarkably expansive, consisting of 26,206 unique architectures after accounting for isomorphic structures (i.e., structures that may exhibit different topological characteristics yet perform identical functionalities, further elaborated in Appendix A.5). Importantly, this search space encapsulates a myriad of GNN variants, including but not limited to the previously mentioned methods, and extends to more advanced architectures such as JK-Net <ref type="bibr" target="#b101">[102]</ref> and residual-and dense-like GNNs <ref type="bibr" target="#b100">[101]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Datasets</head><p>In the course of this study, we employ nine diverse, publicly available datasets extensively utilized in the realm of Graph Neural Architecture Search (GraphNAS). Specifically, these datasets include Cora, CiteSeer, and PubMed <ref type="bibr" target="#b140">[141]</ref>, alongside Coauthor-CS, Coauthor-Physics, Amazon-Photo, and Amazon-Computer <ref type="bibr" target="#b160">[161]</ref>, as well as ogbn-arXiv and ogbn-proteins <ref type="bibr" target="#b161">[162]</ref>. These datasets span an array of sizes, ranging from several thousands to millions of edges, and encompass diverse application domains such as citation networks, e-commerce graphs, and protein interaction networks.</p><p>Regarding dataset partitioning strategies, we adhere to the publicly available semi-supervised settings for Cora, CiteSeer, and PubMed as delineated by <ref type="bibr" target="#b162">[163]</ref>. This involves utilizing 20 labeled nodes per class for training and 500 nodes for validation purposes. For the Amazon and Coauthor datasets, we employ a random partitioning scheme for train/validation/test splits in a semi-supervised fashion, as recommended by <ref type="bibr" target="#b160">[161]</ref>. Specifically, each class is represented by 20 nodes for training, 30 nodes for validation, and the remaining nodes are used for testing. In the case of ogbn-arXiv and ogbn-proteins, we abide by the official dataset splits.</p><p>Pertaining to the ogbn-proteins dataset, preliminary experiments indicated that the utilization of Graph Isomorphism Network (GIN) and k-Graph Neural Network (k-GNN) operations resulted in parameter explosion, thereby yielding uninterpretable outcomes. Additionally, employing Graph Attention Networks (GAT) and Chebyshev Spectral Graph Convolution Networks (ChebNet) led to out-of-memory errors on our high-capacity GPUs equipped with 32GB of memory. To mitigate these computa-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Settings</head><p>Hyper-parameters: For fair and reproducible comparisons, we propose a unified evaluation protocol and consider the following hyper-parameters with tailored ranges:</p><p>• Number of pre-process layers: 0 or 1.</p><p>• Number of post-process layers: 0 or 1.</p><p>• Dimensionality of hidden units: 64, 128, or 256.</p><p>• Dropout rate: 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8.</p><p>• Optimizer: SGD or Adam.</p><p>• Learning Rate (LR): 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001. • Weight Decay: 0 or 0.0005.</p><p>• Number of training epochs: 200, 300, 400, 500. For each dataset, we establish fixed hyper-parameters across all architectures in order to ensure a fair comparison. It should be noted that exhaustively enumerating combinations of architectures and hyper-parameters would lead to an impractical number of architecture hyper-parameter pairs in the order of billions. Hence, we adopt a two-step approach, first optimizing the hyperparameters to a suitable value that can accommodate various Graph Neural Network (GNN) architectures, and subsequently focusing on the GNN architectures themselves. Specifically, we select 30 GNN architectures from our search space as "anchors" and employ random search for hyper-parameter optimization <ref type="bibr" target="#b28">[29]</ref>. The set of 30 anchor architectures comprises 20 randomly chosen architectures from our search space, along with 10 classic GNN architectures including Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), Graph Isomorphism Networks (GIN), GraphSAGE, and ARMA with 2 and 3 layers. We optimize the hyper-parameters by maximizing the average performance of these anchor architectures. The specific hyper-parameters selected for each dataset are shown in Table <ref type="table" target="#tab_11">10</ref>.</p><p>Metrics: During the training of each architecture, we record a comprehensive set of metrics covering both model effec-tiveness and efficiency. These metrics include the loss values and evaluation metric at each epoch for both the training, validation, and testing sets, as well as the model latency and the number of parameters. The hardware and software configurations utilized in our experiments are provided as follows:</p><p>• Operating System: Ubuntu 18.04.6 LTS for PubMed, ogbn-arXiv, and CentOS Linux release 7.6.1810 for the others. • CPU: Intel(R) Xeon(R) Gold 6129 CPU @ 2.30GHz for PubMed, ogbn-arXiv, and Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz for the others. • GPU: NVIDIA GeForce RTX 3090 with 24GB of memories for PubMed, ogbn-arXiv, and NVIDIA Tesla V100 with 16GB of memories for the others. • Software: Python 3.9.12, PyTorch 1.11.0+cu113, PyTorch-Geometric 2.0.4 <ref type="bibr" target="#b116">[117]</ref>.</p><p>Moreover, to account for the inherent variability in the training process, all experiments are repeated three times using different random seeds. The average training time of architectures on each dataset is reported in Table <ref type="table" target="#tab_12">11</ref>. The total computational cost incurred in creating our benchmark dataset amounts to approximately 8,000 GPU hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Example Usages</head><p>This section demonstrates the utilization of NAS-Bench-Graph in conjunction with established open-source libraries such as AutoGL <ref type="bibr" target="#b79">[80]</ref> and NNI <ref type="bibr" target="#b163">[164]</ref>. Specifically, we employ two NAS algorithms, namely GNAS <ref type="bibr" target="#b164">[165]</ref> and Auto-GNN <ref type="bibr" target="#b62">[63]</ref>, within the AutoGL framework. Additionally, we employ Random Search <ref type="bibr" target="#b165">[166]</ref>, Evolutionary Algorithm (EA), and Policy-based Reinforcement Learning (RL) algorithms within NNI. To ensure equitable comparisons, we restrict each algorithm's access to the performance data of only 2% of the total architectures within the search space. The obtained results are presented in Table <ref type="table" target="#tab_13">12</ref>. Furthermore, we include the performance metrics of the top 5% architectures, representing the 20-quantiles for each dataset, within the aforementioned table.</p><p>The detailed example codes are provided as follows. All the codes and recorded metrics for the trained models are available at <ref type="url" target="https://github.com/THUMNLab/NAS-Bench-Graph">https://github.com/THUMNLab/NAS-Bench-Graph</ref>. Next, we provide some example usages. At first, the benchmark of a certain dataset, e.g., Cora, can be read as: The data is stored as a Python dictionary. In order to capture the metrics of interest, it is imperative to define an architecture by specifying its macro space and corresponding operations. In our approach, we impose a restriction on the directed acyclic graph (DAG) of the computation graph, dictating that each intermediate node may have only one input node. This constraint allows us to represent the macro space using a list of integers, wherein each integer denotes the index of the input node for a given computing node. Specifically, the value of 0 corresponds to the raw input, 1 corresponds to the first computing node, and so forth. Additionally, the operations associated with the architecture can be described using a list of strings of equal length, providing a concise representation of the architectural choices made. For example, to specify the architecture shown in Figure <ref type="figure" target="#fig_4">3</ref>, we can use the following code: We assume all leaf nodes (i.e., nodes without descendants) are connected to the output, so there is no need to specific the output node. Besides, the list can be specified in any order, e.g., the following code can specific the same architecture:  We have also provided the source codes of using our benchmark together with two public libraries for GraphNAS, Au-toGL and NNI. See <ref type="url" target="https://github.com/THUMNLab/AutoGL/tree/agnn">https://github.com/THUMNLab/AutoGL/tree/  agnn</ref> and <ref type="url" target="https://github.com/THUMNLab/NAS-Bench-Graph/blob/main/runnni.py">https://github.com/THUMNLab/NAS-Bench-Graph/  blob/main/runnni.py</ref> for details.</p><p>From the experimental findings, it is evident that all algorithms exhibit superior performance compared to the top 5% benchmark, indicating their ability to acquire meaningful patterns in the NAS-Bench-Graph. Nevertheless, no algorithm demonstrates consistent superiority across all datasets. Notably, Random Search remains a robust baseline in comparison to alternative methods and even achieves the highest performance on two datasets, partially supporting the findings of Li et al. <ref type="bibr" target="#b165">[166]</ref> in the context of general NAS. These results underscore the pressing need for further research in the domain of graph NAS.</p><p>In order to delve into the learning behaviors of diverse graph NAS methods, we present the curves depicting optimal performance as a function of the number of architectures, as illustrated in Figure <ref type="figure" target="#fig_7">4</ref>. It is evident that different algorithms exhibit distinct characteristics. For instance, EA and AGNN demonstrate sporadic "jumps" in performance, indicating substantial performance improvements, while RL exhibits more gradual and consistent performance enhancements. A meticulous examination of these learning curves may serve as a source of inspiration for the development of novel algorithms for graph NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FUTURE DIRECTIONS</head><p>We have discussed existing literature in automated graph machine learning approaches and libraries. Our discussion in detail contains how HPO and NAS can be applied to graph machine learning to handle problems in automated graph machine learning. We also introduce AutoGL, a dedicated framework and library for automated graph machine learning. In this section, we will suggest future directions deserving further investigations from both academia and industry. There exist plenty of challenges and opportunities worthy of future explorations.</p><p>• Scalability: AutoML has been successfully applied to various graph scenarios, however, there are still lots of future directions deserving further investigation regarding scalability to large-scale graphs. On the one hand, although HPO for large-scale graph machine learning has been preliminarily explored in literature <ref type="bibr" target="#b44">[45]</ref>, the Bayesian Optimization utilized in the model suffers from limited efficiency. Thus it will be interesting and challenging to explore how we can reduce the computational costs to realize fast hyper-parameter optimization. On the other hand, the scalability of NAS for graph machine learning has drawn few attentions from the researchers despite applications involving large-scale graphs are very common in real world, leaving a large space for further explorations. • Explainability: Existing automated graph machine learning approaches are mainly based on black-box optimizations. For example, it is unclear why certain NAS models can perform better compared with others, and the explainability of NAS algorithms still lack systematic research efforts. There have been some preliminary studies on explainability of graph machine learning <ref type="bibr" target="#b166">[167]</ref>, and on explainable graph hyperparameter optimization <ref type="bibr" target="#b45">[46]</ref> via hyper-parameter importance decorrelation. However, further and deeper investigations on the explainability of automated graph machine learning are still of great importance. • Out-of-distribution generalization: When applied to new graph datasets and tasks, there still need huge human ef-forts to construct task-specific graph HPO configurations and graph NAS frameworks, e.g., spaces and algorithms. The generalization of current graph HPO configurations and NAS frameworks are limited, especially training and testing data come from different distributions <ref type="bibr" target="#b167">[168]</ref>, <ref type="bibr" target="#b168">[169]</ref>, <ref type="bibr" target="#b169">[170]</ref>, <ref type="bibr" target="#b170">[171]</ref>. It will be a promising direction to study the out-of-distribution generalization abilities for both graph HPO and graph NAS algorithms which are capable of handling continuously and rapidly changing tasks. • Robustness: Since many applications of AutoML on graphs are risk-sensitive, e.g., finance and healthcare, the robustness of the models is indispensable for actual usages. Though there exist some initial studies on the robustness <ref type="bibr" target="#b171">[172]</ref> of graph machine learning, how to generalize these techniques into automated graph machine learning has not been explored. • Graph models for AutoML: In this paper, we mainly focus on how AutoML methods are extended to graphs. The other direction, i.e., using graphs to help AutoML, is also feasible and promising. For example, we can model neural networks as a directed acyclic graph (DAG) to analyze their structures <ref type="bibr" target="#b172">[173]</ref>, <ref type="bibr" target="#b115">[116]</ref> or adopt GNNs to facilitate NAS <ref type="bibr" target="#b106">[107]</ref>, <ref type="bibr" target="#b173">[174]</ref>, <ref type="bibr" target="#b174">[175]</ref>, <ref type="bibr" target="#b175">[176]</ref>. Ultimately, we expect graphs and AutoML to form tighter connections and further facilitate each other. • Hardware-aware models: To further improve the scalability of automated graph machine learning, hardware-aware models may be a critical step, especially in real industrial environments. Both hardware-aware graph models <ref type="bibr" target="#b176">[177]</ref> and hardware-aware AutoML models <ref type="bibr" target="#b177">[178]</ref>, <ref type="bibr" target="#b178">[179]</ref>, <ref type="bibr" target="#b179">[180]</ref> have been studied, but integrating these techniques is still in the early stage and poses significant challenges. • Comprehensive evaluation protocols: Currently, most Au-toML on graphs are tested on small traditional benchmarks such as three citations graphs, i.e., Cora, CiteSeer, and PubMed <ref type="bibr" target="#b140">[141]</ref>. However, these benchmarks have been identified as insufficient to compare different graph machine learning models <ref type="bibr" target="#b141">[142]</ref>, not to mention AutoML on graphs. More comprehensive evaluation protocols are needed, e.g., on recently proposed graph machine learning benchmarks <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b180">[181]</ref>, or new dedicated graph AutoML benchmarks <ref type="bibr" target="#b156">[157]</ref> similar to the NAS-bench series <ref type="bibr" target="#b181">[182]</ref> are needed. • Broader Scope of Applications: While automated graph machine learning techniques have been applied to a range of practical use-cases, there's considerable potential for using these newly-developed techniques in the information retrieval (such as search engines, recommender systems) for achieving effective, reliable, and user-friendly predictions. A viable approach could involve using this specialized domain knowledge as a form of prior to guide both the hyperparameter optimization and architecture search strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we discuss the current state-of-the-art automated graph machine learning approaches, libraries. In particular, we in depth elaborate how graph hyperparameter optimization (HPO) and graph neural architecture search (NAS) have been developed to facilitate automated graph machine learning. We also introduce AutoGL, our dedicated framework and open source library for automated graph machine learning, and NAS-Bench-Graph, our tailored benchmark that enables fair, fully reproducible, and ef-ficient empirical comparisons. Last but not least, we point out challenges and suggest promising directions deserving further investigations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The overall framework of AutoGL.</figDesc><graphic coords="10,50.58,43.70,510.83,262.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>from r e a d b e n c h i m p o r t l i g h t r e a d b e n c h = l i g h t r e a d ( ' c o r a ' )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: An example architecture.</figDesc><graphic coords="15,48.00,425.75,252.00,50.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>a r c h = Arch ( [ 0 , 1 , 1 , 2 ] , [ ' gcn ' , ' cheb ' , ' g i n ' , ' f c ' ] )Then, four recorded metrics in the benchmark including the validation and test performance, the latency, and the number of parameters, can be obtained by a look-up table:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>i n f o = b e n c h [ a r c h . v a l i d h a s h ( ) ] i n f o [ ' v a l i d p e r f ' ] # v a l i d a t i o n p e r f o r m a n c e i n f o [ ' p e r f ' ] # t e s t p e r f o r m a n c e i n f o [ ' l a t e n c y ' ] # l a t e n c y i n f o [ ' p a r a ' ] # number o f p a r a m e t e r s We provide the full data, including the training/validation/testing performance at each epoch at: https://figshare.com/articles/ dataset/NAS-bench-Graph/20070371. Since we run each dataset with three random seeds, each dataset has 3 files. The full metric can be obtained similarly as follows: from r e a d b e n c h i m p o r t r e a d b e n c h = r e a d ( ' c o r a 0 . bench ' ) # d a t a s e t and s e e d i n f o = b e n c h [ a r c h . v a l i d h a s h ( ) ] e p o c h = 50 i n f o [ ' dur ' ] [ e p o c h ] [ 0 ] # t r a i n i n g p e r f o r m a n c e i n f o [ ' dur ' ] [ e p o c h ] [ 1 ] # v a l i d a t i o n p e r f o r m a n c e i n f o [ ' dur ' ] [ e p o c h ] [ 2 ] # t e s t i n g p e r f o r m a n c e i n f o [ ' dur ' ] [ e p o c h ] [ 3 ] # t r a i n i n g l o s s i n f o [ ' dur ' ] [ e p o c h ] [ 4 ] # v a l i d a t i o n l o s s i n f o [ ' dur ' ] [ e p o c h ] [ 5 ] # t e s t i n g l o s s i n f o [ ' dur ' ] [ e p o c h ] [ 6 ] # b e s t p e r f o r m a n c e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: The learning curve depicting the optimal performance as a function of the number of searched architectures is presented herein. The reported results are obtained by averaging measurements from five independent experiments, each conducted with distinct random seeds. The background of the figure displays the standard errors associated with the reported values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>A summary of different graph neural architecture search (NAS) methods for automated graph machine learning.</figDesc><table><row><cell cols="2">Other Characteristics -</cell></row><row><cell>Performance</cell><cell>Estimation</cell></row><row><cell cols="2">Search Strategy</cell></row><row><cell>Search space Tasks</cell><cell>Micro Macro Pooling HP Layers Node Graph</cell></row><row><cell cols="2">Method</cell><cell>GraphNAS [42]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>The typical search space of different types of aggregation weights aij. We omit the layer superscript for brevity.</figDesc><table><row><cell>Type</cell><cell>Formulation</cell></row><row><cell>CONST</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Common optimization methods are packaged as high-level APIs to provide neat and clean interfaces. More advanced training controls and regularization methods in graph tasks like early stopping and weight decay are also supported.The model training module supports both node-level and graph-level tasks, e.g., node classification and graph classification. Commonly used models for node classification such as GCN<ref type="bibr" target="#b48">[49]</ref>,The statistics of the supported datasets. For datasets with more than one graph, #Nodes and #Edges are the average numbers of all the graphs. #Features correspond to node features by default, and edge features are specified.</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell cols="2">#Graphs #Nodes</cell><cell>#Edges</cell><cell>#Features</cell><cell>#Classes</cell></row><row><cell>Cora</cell><cell>Node</cell><cell>1</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell></row><row><cell>CiteSeer</cell><cell>Node</cell><cell>1</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell></row><row><cell>PubMed</cell><cell>Node</cell><cell>1</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell></row><row><cell>Reddit</cell><cell>Node</cell><cell>1</cell><cell>232,965</cell><cell>11,606,919</cell><cell>602</cell><cell>41</cell></row><row><cell>Amazon Computers</cell><cell>Node</cell><cell>1</cell><cell>13,381</cell><cell>245,778</cell><cell>767</cell><cell>10</cell></row><row><cell>Amazon Photo</cell><cell>Node</cell><cell>1</cell><cell>7,487</cell><cell>119,043</cell><cell>745</cell><cell>8</cell></row><row><cell>Coauthor CS</cell><cell>Node</cell><cell>1</cell><cell>18,333</cell><cell>81,894</cell><cell>6,805</cell><cell>15</cell></row><row><cell>Coauthor Physics</cell><cell>Node</cell><cell>1</cell><cell>34,493</cell><cell>247,962</cell><cell>8,415</cell><cell>5</cell></row><row><cell>ogbn-products</cell><cell>Node</cell><cell>1</cell><cell>2,449,029</cell><cell>61,859,140</cell><cell>100</cell><cell>47</cell></row><row><cell>ogbn-proteins</cell><cell>Node</cell><cell>1</cell><cell>132,534</cell><cell>39,561,252</cell><cell>8(edge)</cell><cell>112</cell></row><row><cell>ogbn-arxiv</cell><cell>Node</cell><cell>1</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell></row><row><cell>ogbn-papers100M</cell><cell>Node</cell><cell>1</cell><cell cols="3">111,059,956 1,615,685,872 128</cell><cell>172</cell></row><row><cell>Mutag</cell><cell cols="2">Graph 188</cell><cell>17.9</cell><cell>19.8</cell><cell>7s</cell><cell>2</cell></row><row><cell>PTC</cell><cell cols="2">Graph 344</cell><cell>14.3</cell><cell>14.7</cell><cell>18</cell><cell>2</cell></row><row><cell>ENZYMES</cell><cell cols="2">Graph 600</cell><cell>32.6</cell><cell>62.1</cell><cell>3</cell><cell>6</cell></row><row><cell>PROTEINS</cell><cell cols="2">Graph 1,113</cell><cell>39.1</cell><cell>72.8</cell><cell>3</cell><cell>2</cell></row><row><cell>NCI1</cell><cell cols="2">Graph 4,110</cell><cell>29.8</cell><cell>32.3</cell><cell>37</cell><cell>2</cell></row><row><cell>COLLAB</cell><cell cols="2">Graph 5,000</cell><cell>74.5</cell><cell>2,457.8</cell><cell>-</cell><cell>3</cell></row><row><cell>IMDB-B</cell><cell cols="2">Graph 1,000</cell><cell>19.8</cell><cell>96.5</cell><cell>-</cell><cell>2</cell></row><row><cell>IMDB-M</cell><cell cols="2">Graph 1,500</cell><cell>13.0</cell><cell>65.9</cell><cell>-</cell><cell>3</cell></row><row><cell>REDDIT-B</cell><cell cols="2">Graph 2,000</cell><cell>429.6</cell><cell>497.8</cell><cell>-</cell><cell>2</cell></row><row><cell>REDDIT-MULTI5K</cell><cell cols="2">Graph 5,000</cell><cell>508.5</cell><cell>594.9</cell><cell>-</cell><cell>5</cell></row><row><cell cols="3">REDDIT-MULTI12K Graph 11,929</cell><cell>391.4</cell><cell>456.9</cell><cell>-</cell><cell>11</cell></row><row><cell>ogbg-molhiv</cell><cell cols="2">Graph 41,127</cell><cell>25.5</cell><cell>27.5</cell><cell cols="2">9, 3(edge) 2</cell></row><row><cell>ogbg-molpcba</cell><cell cols="2">Graph 437,929</cell><cell>26.0</cell><cell>28.1</cell><cell cols="2">9, 3(edge) 128</cell></row><row><cell>ogbg-ppa</cell><cell cols="2">Graph 158,100</cell><cell>243.4</cell><cell>2,266.1</cell><cell>7(edge)</cell><cell>37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>Supported generators in the auto feature engineering module.</figDesc><table><row><cell>Name</cell><cell>Description</cell></row><row><cell>graphlet</cell><cell>Local graphlet numbers</cell></row><row><cell>eigen</cell><cell>EigenGNN features.</cell></row><row><cell>pagerank</cell><cell>PageRank scores.</cell></row><row><cell>PYGLocalDegreeProfile</cell><cell>Local Degree Profile features</cell></row><row><cell>PYGNormalizeFeatures</cell><cell>Row-normalize all node features</cell></row><row><cell>PYGOneHotDegree</cell><cell>One-hot encoding of node degrees.</cell></row><row><cell>onehot</cell><cell>One-hot encoding of node IDs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 :</head><label>5</label><figDesc>The results of node classification</figDesc><table><row><cell>Model</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>GCN</cell><cell>80.9 ± 0.7</cell><cell>70.9 ± 0.7</cell><cell>78.7 ± 0.6</cell></row><row><cell>GAT</cell><cell>82.3 ± 0.7</cell><cell>71.9 ± 0.6</cell><cell>77.9 ± 0.4</cell></row><row><cell>GraphSAGE</cell><cell>74.5 ± 1.8</cell><cell>67.2 ± 0.9</cell><cell>76.8 ± 0.6</cell></row><row><cell>AutoGL</cell><cell cols="3">83.2 ± 0.6 72.4 ± 0.6 79.3 ± 0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>The results of graph classification</figDesc><table><row><cell>Model</cell><cell>MUTAG</cell><cell>PROTEINS</cell><cell>IMDB-B</cell></row><row><cell>Top-K Pooling</cell><cell>80.8 ± 7.1</cell><cell>69.5 ± 4.4</cell><cell>71.0 ± 5.5</cell></row><row><cell>GIN</cell><cell>82.7 ± 6.9</cell><cell>66.5 ± 3.9</cell><cell>69.1 ± 3.7</cell></row><row><cell>AutoGL</cell><cell cols="3">87.6 ± 6.0 73.3 ± 4.4 72.1 ± 5.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc>The results of different HPO methods for node classification</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Cora</cell><cell cols="2">CiteSeer</cell><cell cols="2">PubMed</cell></row><row><cell cols="2">Method Trials</cell><cell>GCN</cell><cell>GAT</cell><cell>GCN</cell><cell>GAT</cell><cell>GCN</cell><cell>GAT</cell></row><row><cell>None</cell><cell></cell><cell>80.9 ± 0.7</cell><cell>82.3 ± 0.7</cell><cell>70.9 ± 0.7</cell><cell>71.9 ± 0.6</cell><cell>78.7 ± 0.6</cell><cell>77.9 ± 0.4</cell></row><row><cell></cell><cell>1</cell><cell>81.0 ± 0.6</cell><cell>81.4 ± 1.1</cell><cell>70.4 ± 0.7</cell><cell>70.1 ± 1.1</cell><cell>78.3 ± 0.8</cell><cell>76.9 ± 0.8</cell></row><row><cell>random</cell><cell>10</cell><cell>82.0 ± 0.6</cell><cell>82.5 ± 0.7</cell><cell>71.5 ± 0.6</cell><cell>72.2 ± 0.7</cell><cell>79.1 ± 0.3</cell><cell>78.2 ± 0.3</cell></row><row><cell></cell><cell>50</cell><cell>81.8 ± 1.1</cell><cell>83.2 ± 0.7</cell><cell>71.1 ± 1.0</cell><cell>72.1 ± 1.0</cell><cell>79.2 ± 0.4</cell><cell>78.2 ± 0.4</cell></row><row><cell></cell><cell>1</cell><cell>81.8 ± 0.6</cell><cell>81.9 ± 1.0</cell><cell>70.1 ± 1.2</cell><cell>71.0 ± 1.2</cell><cell>78.7 ± 0.6</cell><cell>77.7 ± 0.6</cell></row><row><cell>TPE</cell><cell>10</cell><cell>82.0 ± 0.7</cell><cell>82.3 ± 1.2</cell><cell>71.2 ± 0.6</cell><cell>72.1 ± 0.7</cell><cell>79.0 ± 0.4</cell><cell>78.3 ± 0.4</cell></row><row><cell></cell><cell>50</cell><cell>82.1 ± 1.0</cell><cell>83.2 ± 0.8</cell><cell>72.4 ± 0.6</cell><cell>71.6 ± 0.8</cell><cell>79.1 ± 0.6</cell><cell>78.1 ± 0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>The results of different HPO methods for graph classification</figDesc><table><row><cell>HPO</cell><cell cols="2">MUTAG Top-K Pooling</cell><cell>GIN</cell><cell cols="2">PROTEINS Top-K Pooling</cell><cell>GIN</cell><cell>IMDB-B Top-K Pooling</cell><cell>GIN</cell></row><row><cell>None</cell><cell>76.3 ± 7.5</cell><cell cols="2">82.7 ± 6.9</cell><cell>69.5 ± 4.4</cell><cell cols="2">66.5 ± 3.9</cell><cell>71.0 ± 5.5</cell><cell>69.1 ± 3.7</cell></row><row><cell>random</cell><cell>82.7 ± 6.8</cell><cell cols="2">87.6 ± 6.0</cell><cell>73.3 ± 4.4</cell><cell cols="2">71.0 ± 5.9</cell><cell>71.5 ± 4.1</cell><cell>71.3 ± 4.0</cell></row><row><cell>TPE</cell><cell>83.9 ± 10.1</cell><cell cols="2">86.7 ± 6.2</cell><cell>72.3 ± 5.5</cell><cell cols="2">71.0 ± 7.2</cell><cell>71.6 ± 2.5</cell><cell>70.2 ± 3.7</cell></row><row><cell>In</cell><cell></cell><cell>In</cell><cell></cell><cell>In</cell><cell></cell><cell></cell><cell>In</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>In</cell></row><row><cell>In</cell><cell></cell><cell>In</cell><cell></cell><cell>In</cell><cell></cell><cell></cell><cell>In</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9 :</head><label>9</label><figDesc>The performance of the ensemble module of AutoGL for the node classification task.</figDesc><table><row><cell>Base Model</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>GCN</cell><cell>81.1 ± 0.9</cell><cell>69.6 ± 1.1</cell><cell>78.5 ± 0.4</cell></row><row><cell>GAT</cell><cell>82.0 ± 0.5</cell><cell>70.4 ± 0.6</cell><cell>77.7 ± 0.5</cell></row><row><cell>Ensemble</cell><cell cols="3">82.2 ± 0.4 70.8 ± 0.5 78.5 ± 0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 10 :</head><label>10</label><figDesc>The hyper-parameters and hardware used for each dataset. #Pre and #Post denotes the number of pre-process and post-process layers, respectively.</figDesc><table><row><cell>Dataset</cell><cell cols="5">#Pre #Post Dimension Dropout Optimizer LR</cell><cell>WD</cell><cell># Epoch</cell></row><row><cell>Cora</cell><cell>0</cell><cell>1</cell><cell>256 0.7</cell><cell>SGD</cell><cell>0.1</cell><cell>0.0005</cell><cell>400</cell></row><row><cell>CiteSeer</cell><cell>0</cell><cell>1</cell><cell>256 0.7</cell><cell>SGD</cell><cell>0.2</cell><cell>0.0005</cell><cell>400</cell></row><row><cell>PubMed</cell><cell>0</cell><cell>0</cell><cell>128 0.3</cell><cell>SGD</cell><cell>0.2</cell><cell>0.0005</cell><cell>500</cell></row><row><cell>Coauthor-CS</cell><cell>1</cell><cell>0</cell><cell>128 0.6</cell><cell>SGD</cell><cell>0.5</cell><cell>0.0005</cell><cell>400</cell></row><row><cell>Coauthor-Physics</cell><cell>1</cell><cell>1</cell><cell>256 0.4</cell><cell>SGD</cell><cell>0.01</cell><cell>0</cell><cell>200</cell></row><row><cell>Amazon-Photo</cell><cell>1</cell><cell>0</cell><cell>128 0.7</cell><cell>Adam</cell><cell cols="2">0.0002 0.0005</cell><cell>500</cell></row><row><cell>Amazon-Computers</cell><cell>1</cell><cell>1</cell><cell>64 0.1</cell><cell>Adam</cell><cell>0.005</cell><cell>0.0005</cell><cell>500</cell></row><row><cell>ogbn-arxiv</cell><cell>0</cell><cell>1</cell><cell>128 0.2</cell><cell>Adam</cell><cell>0.002</cell><cell>0</cell><cell>500</cell></row><row><cell>ogbn-proteins</cell><cell>1</cell><cell>1</cell><cell>256 0</cell><cell>Adam</cell><cell>0.01</cell><cell>0.0005</cell><cell>500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 11 :</head><label>11</label><figDesc>The average training time of architectures on each dataset.</figDesc><table><row><cell>Dataset</cell><cell>Time</cell><cell>Dataset</cell><cell>Time</cell><cell>Dataset</cell><cell>Time</cell></row><row><cell>Cora</cell><cell>5.8s</cell><cell>Coauthor-CS</cell><cell>8.6s</cell><cell>Amazon-Computers</cell><cell>9.8s</cell></row><row><cell>CiteSeer</cell><cell>6.2s</cell><cell cols="2">Coauthor-Physics 15.4s</cell><cell>ogbn-arXiv</cell><cell>71s</cell></row><row><cell>PubMed</cell><cell>7.8s</cell><cell>Amazon-Photo</cell><cell>8.8s</cell><cell>ogbn-proteins</cell><cell>50min</cell></row><row><cell cols="3">tional inefficiencies, we circumscribed the candidate operations for</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ogbn-proteins to Graph Convolution Networks (GCN), Attention-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">based Recurrent Multi-layer Average networks (ARMA), Graph-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">SAGE, Identity mappings, and fully connected layers. Conse-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">quently, this constraint yielded a feasible architecture space com-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">prising 2,021 candidate models for ogbn-proteins.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 12 :</head><label>12</label><figDesc>The performance of NAS methods in AutoGL and NNI using NAS-Bench-Graph. The best performance for each dataset is marked in bold. We also show the performance of the top 5% architecture (i.e., 20-quantiles) as a reference line. The results are averaged over five experiments with different random seeds and the standard errors are shown in the bottom right. 0.17 70.89 0.16 77.79 0.02 90.97 0.06 92.43 0.04 92.43 0.03 84.74 0.20 72.00 0.02 78.71 0.11 Auto-GNN 81.80 0.00 70.76 0.12 77.69 0.16 91.04 0.04 92.42 0.16 92.38 0.01 84.53 0.14 72.13 0.03 78.54 0.30 NNI Random 82.09 0.08 70.49 0.08 77.91 0.07 90.93 0.07 92.35 0.05 92.44 0.02 84.78 0.14 72.04 0.05 78.32 0.14 EA 81.85 0.20 70.48 0.12 77.96 0.12 90.60 0.07 92.22 0.08 92.43 0.02 84.29 0.29 71.91 0.06 77.93 0.21 RL 82.27 0.21 70.66 0.12 77.96 0.09 90.98 0.01 92.48 0.03 92.42 0.06 84.90 0.19 72.13 0.05 78.52 0.18</figDesc><table><row><cell>Library</cell><cell>Method</cell><cell>Cora</cell><cell cols="3">CiteSeer PubMed CS</cell><cell cols="2">Physics Photo</cell><cell cols="2">ComputersarXiv</cell><cell>proteins</cell></row><row><cell cols="3">AutoGL 82.04 The top 5% GNAS 80.63</cell><cell>69.07</cell><cell>76.60</cell><cell>90.01</cell><cell>91.67</cell><cell>91.57</cell><cell>82.77</cell><cell>71.69</cell><cell>78.37</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>This manuscript is based on AutoGL v0.2.0-pre released on 11st, July 2021. Pleases visit the website for the most up-to-the version.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/THUMNLab/AutoGL/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://autogl.readthedocs.io/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This work is supported by the <rs type="funder">National Key Research and Development Program of China</rs> No.<rs type="grantNumber">2020AAA0106300</rs> and <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62222209</rs>, <rs type="grantNumber">62250008</rs>, <rs type="grantNumber">62102222</rs>), BNRist under Grant No. <rs type="grantNumber">BNR2023RC01003</rs>, <rs type="grantNumber">BNR2023TD03006</rs>, and <rs type="funder">Beijing Key Lab of Networked Multimedia</rs>.</p></div>
			</div>
			<div type="funding">
<div><p>• This work is supported by the <rs type="funder">National Key Research and Development Program of China</rs> No.<rs type="grantNumber">2023YFF1205001</rs>, <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62222209</rs>, <rs type="grantNumber">62250008</rs>, <rs type="grantNumber">62102222</rs>). BNRist under Grant No. <rs type="grantNumber">BNR2023RC01003</rs>, <rs type="grantNumber">BNR2023TD03006</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mrUZjNC">
					<idno type="grant-number">2020AAA0106300</idno>
				</org>
				<org type="funding" xml:id="_wqSNtFB">
					<idno type="grant-number">62222209</idno>
				</org>
				<org type="funding" xml:id="_aBgRt8s">
					<idno type="grant-number">62250008</idno>
				</org>
				<org type="funding" xml:id="_eAqXptd">
					<idno type="grant-number">62102222</idno>
				</org>
				<org type="funding" xml:id="_vSyPbJQ">
					<idno type="grant-number">BNR2023RC01003</idno>
				</org>
				<org type="funding" xml:id="_ujSeyRX">
					<idno type="grant-number">BNR2023TD03006</idno>
				</org>
				<org type="funding" xml:id="_k7s958e">
					<idno type="grant-number">2023YFF1205001</idno>
				</org>
				<org type="funding" xml:id="_DsaMPGn">
					<idno type="grant-number">62222209</idno>
				</org>
				<org type="funding" xml:id="_Mq6Jpge">
					<idno type="grant-number">62250008</idno>
				</org>
				<org type="funding" xml:id="_a7rHaTw">
					<idno type="grant-number">62102222</idno>
				</org>
				<org type="funding" xml:id="_TgMx5tH">
					<idno type="grant-number">BNR2023RC01003</idno>
				</org>
				<org type="funding" xml:id="_xzmZ9uA">
					<idno type="grant-number">BNR2023TD03006</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Networks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="833" to="852" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE DEBU</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph embedding techniques, applications, and performance: A survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KBS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: Problems, techniques, and applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning disentangled representations for recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Intention-aware sequential recommendation with structured intent transition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5403" to="5414" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph neural pretraining for recommendation with side information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How can graph neural networks help document retrieval: A case study on cord19 with concept map generation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dcgnn: Decoupled graph neural networks for improving and accelerating large-scale e-commerce retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the Web Conference</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gnn-based retrieval and recommadation system: A semantic enhenced graph model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 5th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1823" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abslearn: a gnn-based framework for aliasing and buffer-size information retrieval</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Applications</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph based anomaly detection and description: a survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DMKD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Network embedding in biomedical data science</title>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Datadriven traffic forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Few-shot link prediction via graph neural networks for covid-19 drug-repurposing</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10261</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Network medicine framework for identifying drug repurposing opportunities for covid-19</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gysi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07229</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Examining covid-19 forecasting using spatiotemporal graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03113</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automl: A survey of the state-of-the-art</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KBS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Taking human out of learning applications: A survey on automated machine learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13306</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta hyperparameter optimization with adversarial proxy subsets sampling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1109" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Autoias: Automatic integrated architecture searcher for click-trough rate prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2101" to="2110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On power law growth of social networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Autone: Hyperparameter optimization for massive network embedding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explainable automated graph representation learning with hyperparameter importance</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Arbitrary-order proximity preserved network embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Jitune: Just-in-time hyperparameter tuning for network embedding algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06427</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A novel genetic algorithm with hierarchical evaluation strategy for hyperparameter optimisation of graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09300</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Autonomous graph mining algorithm search with best speed/accuracy trade-off</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICDM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Which hyperparameters to optimise? an investigation of evolutionary hyperparameter optimisation in graph neural network for molecular property prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>GECCO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A systematic comparison study on hyperparameter optimisation of graph neural networks for molecular property prediction</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>GECCO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Automated graph learning via population based self-tuning gcn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2096" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automatic graph learning with evolutionary algorithms: An experimental study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRICAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Automated graph representation learning for node classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCNN</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Calibrate automated graph neural network via hyperparameter uncertainty</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4640" to="4644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Assessing the effects of hyperparameters on knowledge graph embedding quality</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of big Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Autonomous graph mining algorithm search with best performance trade-off</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1571" to="1602" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Kgtuner: Efficient hyperparameter search for knowledge graph learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.02460</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Revisiting embedding based graph analyses: Hyperparameters matter</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cudré-Mauroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Auto-gnn: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Simplifying architecture search for graph neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11652</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Probabilistic dual network architecture search on graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09676</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Neural architecture search in graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian Conference on Intelligent Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Autograph: Automated graph neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICONIP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Evolutionary architecture search for graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10199</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Efficient graph neural architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=IjIzIOkK2D6" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Graph neural network architecture search for molecular property prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balaprakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Big Data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learned low precision graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EuroMLSys</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning graph convolutional network for skeletonbased human action recognition by neural searching</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">CEM-RL: Combining evolutionary and gradientbased methods for policy search</title>
		<author>
			<persName><forename type="first">Sigaud</forename><surname>Pourchot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Rethinking graph neural architecture search from message-passing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Autostg: Neural architecture search for predictions of spatio-temporal graphs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">One-shot graph neural architecture search with dynamic search space</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Search to aggregate neighborhood for graph neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDE</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Autoattend: Automated attention representation search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Diffmg: Differentiable meta graph search for heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Search for deep graph neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10047</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Learn layer-wise connections in graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13585</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Fl-agcns: Federated learning framework for automatic graph convolutional network search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04141</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">G-cos: Gnnaccelerator co-search towards both better accuracy and efficiency</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Pooling architecture search for graph classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Fgnas: Fpga-aware graph neural architecture search</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Graphpas: Parallel architecture search for graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Algnn: Auto-designed lightweight graph neural network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRICAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Mopso: A proposal for multiple objective particle swarm optimization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Coello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lechuga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CEC</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Edge-featured graph neural architecture search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01356</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Autogel: An automated graph neural network with explicit link information</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Snas: stochastic neural architecture search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Graph differentiable architecture search with structure learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16" to="860" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Adversarially robust neural architecture search for graph neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8143" to="8152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Graph neural architecture search under distribution shifts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">95</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Large-scale graph neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7968" to="7981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Pasca: A graph neural architecture search system under the scalable paradigm</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1817" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Differentiable meta multigraph search with partial message propagation on heterogeneous information networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="8518" to="8526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Dynamic heterogeneous graph attention neural architecture search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Deepgcns: Can gcns go as deep as cnns?&quot; in ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Bridging the gap between sample-based and one-shot neural architecture search with bonas</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Disentangled contrastive learning on graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21" to="872" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Disentangled graph contrastive learning with independence promotion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Diffmg: Differentiable meta graph search for heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Hgnas++: efficient architecture search for heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">AutoGT: Automated graph transformer architecture search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=GcM7qfl5zY" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OeWooOxFwDa" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Graph structure of neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Aligraph: A comprehensive graph neural network platform</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Euler: A distributed graph deep learning framework</title>
		<author>
			<persName><surname>Alibaba</surname></persName>
		</author>
		<ptr target="https://github.com/alibaba/euler" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">PyTorch-BigGraph: A Large-scale Graph Embedding System</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SysML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Paddle graph learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Nlp</surname></persName>
		</author>
		<author>
			<persName><surname>Baidu</surname></persName>
		</author>
		<ptr target="https://github.com/PaddlePaddle/PGL,2021" />
		<imprint>
			<date type="published" when="2021-12">Dec-2021</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Tensorflow gnn</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Sibon</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/gnn" />
		<imprint>
			<date type="published" when="2021-12">2021. Dec-2021</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Stellargraph machine learning library</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<ptr target="https://github.com/stellargraph/stellargraph" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Data61</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Graph neural networks in tensorflow and keras with spektral</title>
		<author>
			<persName><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Cogdl: An extensive research toolkit for deep learning on graphs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">U</forename></persName>
		</author>
		<ptr target="https://github.com/THUDM/cogdl" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Knowledge Engineering Group</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Openne: An open source toolkit for network embedding</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L P</forename></persName>
		</author>
		<ptr target="https://github.com/thunlp/OpenNE" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>at Tsinghua University</note>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">T</forename><surname>Lab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/BUPT-GAMMA/OpenHGNN" />
		<imprint>
			<date type="published" when="2021-12">2021. -Dec-2021</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Gem: A python package for graph embedding methods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Exploring network structure, dynamics, and function using networkx</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hagberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Los Alamos National Lab.(LANL), Tech. Rep</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Retiarii: A deep learning exploratory-training framework</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Auto-keras: An efficient neural architecture search system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Auto-sklearn: efficient and robust automated machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Evaluation of a tree-based pipeline optimization tool for automating data science</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Olson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>GECCO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Autogluon-tabular: Robust and accurate automl for structured data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Erickson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06505</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Mlbox, machine learning box</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Romblay</surname></persName>
		</author>
		<ptr target="https://github.com/AxeldeRomblay/MLBox" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Mljar automated machine learning</title>
		<author>
			<persName><surname>Mljar</surname></persName>
		</author>
		<ptr target="https://github.com/mljar/mljar-supervised" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relational Representation Learning Workshop</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Eigen-gnn: A graph structure preserving plug-in for gnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer networks and ISDN systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Deep inductive graph representation learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Netlsd: hearing the shape of a graph</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2347" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Nas-bench-graph: Benchmarking graph neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="54" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Graph neural networks with convolutional arma filters</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Relational Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Neural Network Intelligence</title>
		<author>
			<persName><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://github.com/microsoft/nni" />
		<imprint>
			<date type="published" when="2021">1 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Graph neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1403" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in artificial intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Explainability in graph neural networks: A taxonomic survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15445</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Ood-gnn: Out-of-distribution generalized graph neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03806</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization on graphs: A survey</title>
		<idno type="arXiv">arXiv:2202.07987</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Learning invariant graph representations for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Invariant node representation learning under distribution shifts with multiple latent environments</title>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Adversarial attack and defense on graph data: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10528</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Graph hypernetworks for neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Brp-nas: Prediction-based nas using gcns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dudziak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Gqnas: Graph q network for neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Hardware acceleration of graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Auten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Hardware-aware transformable architecture search with efficient search space</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICME</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
