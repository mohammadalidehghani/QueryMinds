<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Information Theory and its Relation to Machine Learning</title>
				<funder ref="#_JNB32c4">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2015-01-18">18 Jan 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Bao-Gang</forename><surname>Hu</surname></persName>
							<email>hubg@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Information Theory and its Relation to Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-01-18">18 Jan 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">30B60B807BAE6D0BA784972C6DE1C930</idno>
					<idno type="arXiv">arXiv:1501.04309v1[cs.IT]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine learning</term>
					<term>learning target selection</term>
					<term>entropy</term>
					<term>information theory</term>
					<term>similarity</term>
					<term>conjecture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this position paper, I first describe a new perspective on machine learning (ML) by four basic problems (or levels), namely, "What to learn?", "How to learn?", "What to evaluate?", and "What to adjust?". The paper stresses more on the first level of "What to learn?", or "Learning Target Selection". Towards this primary problem within the four levels, I briefly review the existing studies about the connection between information theoretical learning (ITL [1]) and machine learning. A theorem is given on the relation between the empirically-defined similarity measure and information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning is the study and construction of systems that can learn from data. The systems are called learning machines. When Big Data emerges increasingly, more learning machines are developed and applied in different domains. However, the ultimate goal of machine learning study is insight, not machine itself. By the term insight I mean learning mechanisms in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the "Tao (道)" reflects the most fundamental of the universe by Lao Tzu (老子), Einstein suggests that we should pursue the simplest mathematical interpretations to the nature. Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles. It is the author's belief that "mathematical-principle-based machine" might be more important and critical than "brain-inspired machine" in the study of machine learning.</p><p>The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning. In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed. A theorem between the empirically-defined similarity measures and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Four basic problems (or levels) in machine learning</head><p>For information processing by a machine, in the 1980's, Marr <ref type="bibr" target="#b3">[4]</ref> proposed a novel methodology by three distinct yet complementary levels, namely, "Computational theory", "Representation and algorithm", and "Hardware implementation", respectively. Although the three levels are "coupled" loosely, the distinction is of great necessity to isolate and solve problems properly and efficiently. In 2007, Poggio <ref type="bibr" target="#b4">[5]</ref> described another set of three levels on learning, namely, "Learning theory and algorithms", "Engineering applications", and "Neuroscience: models and experiments", respectively. In apart from showing a new perspective, one of important contributions of this methodology is on adding a closed loop between the levels. These studies are enlightening because they show that complex objects or systems should be addressed by decompositions with different, yet basic, problems. The methodology is considered to be reductionism philosophically.</p><p>In this paper, I propose a novel perspective on machine learning by four levels shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The levels correspond to four basic problems. The definition of each level is given below. Definition 1: "What to learn" is a study on identifying learning target(s) to the given problem(s), which will generally involve two distinct sets of representations (Fig. <ref type="figure" target="#fig_1">2</ref>) defined below.</p><p>Definition 1a: "Linguistic representation" reflects a high-level description in a natural language about the expected learning information. This study is more related to linguistics, psychology, and cognitive science.</p><p>Definition 1b: "Computational representation" is to define the expected learning information based on mathematical notations. It is relatively a lowlevel representation which generally includes objective functions, constraints, and optimization formations.</p><p>Definition 2: "How to learn?" is a study on learning process design and implementations. Probability, statistics, utility, optimization, and computational theories will be the central subjects. The main concerns are generalization performance, robustness, model complexity, computational complexity/cost, etc. The study may include physically realized system(s).</p><p>Definition 3: "What to evaluate?" is a study on "evaluation measure selection" where evaluation measure is a mathematical function. This function can be the same or different with the objective function defined in the first level.</p><p>Definition 4: "What to adjust?" is a study on dynamic behaviors of a machine from adjusting its component(s). This level will enable a machine with a functionality of "evolution of intelligence". The first level is also called "learning target selection". The four levels above are neither mutually exclusive, nor collectively exhaustive to every problems in machine learning. We call them basic so that the extra problems can be merged within one of levels. Figs. 1 and 2 illustrate the relations between each level in different contexts, respectively. The problems within four levels are all interrelated, particularly for "What to learn?" and "What to evaluate?" (Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>"How to learn?" may influence to "What to learn?", such as convexity of the objective function or scalability to learning algorithms <ref type="bibr" target="#b5">[6]</ref> from a computational cost consideration. Structurally, "What to adjust?" level is applied to provide the multiple closed loops for describing the interrelations (Fig. <ref type="figure" target="#fig_0">1</ref>). Artificial intelligence will play a critical role via this level. In the "knowledge driven and data driven" model <ref type="bibr" target="#b6">[7]</ref>, the benefits of utilizing this level are shown from the given examples by removable singularity hypothesis to "Sinc" function and prior updating to Mackey-Glass dataset, respectively. Philosophically, "What to adjust?" level remedies the intrinsic problems in the methodology of reductionism and offers the functionality power for being holism. However, this level receives even less attention while learning process holds a self organization property.</p><p>I expect that the four levels show a novel perspective about the basic problems in machine learning. Take an example shown in Fig. <ref type="figure" target="#fig_2">3</ref> [after Duda, et al, <ref type="bibr" target="#b7">[8]</ref>, Fig. <ref type="figure">5</ref><ref type="figure" target="#fig_4">6</ref><ref type="figure">7</ref><ref type="figure">8</ref><ref type="figure">9</ref><ref type="figure" target="#fig_0">10</ref><ref type="figure" target="#fig_0">11</ref><ref type="figure" target="#fig_1">12</ref><ref type="figure" target="#fig_2">13</ref><ref type="figure" target="#fig_3">14</ref><ref type="figure" target="#fig_0">15</ref><ref type="figure" target="#fig_4">16</ref><ref type="figure" target="#fig_0">17</ref>] . Even for the linearly separable dataset, the learning function using least mean square (LMS) does not guarantee a "minimum-error" classification. This example demonstrates two points. First, the computational representation of LMS is not compatible with the linguistic representation of "minimum-error" classification. Second, whenever a learning target is wrong in the computational representation, one is unable to reach the goal from Levels 2 and 3. Another example in Fig. <ref type="figure" target="#fig_3">4</ref> shows why we need two sub-levels in learning target selection. For the given character (here is Albert Einstein), one does need a linguistic representation to describe "(un)likeness" <ref type="bibr" target="#b8">[9]</ref> between the original image and caricature image. Only when a linguistic representation is well defined, is a computational measure of similarity possibly proper in caricature learning. The meaning of possibly proper is due to the difficulty in the following definition. Definition 5: "Semantic gap" is a difference between the two sets of representations. The gap can be linked by two ways, namely, a direct way for describing a connection from linguistic representation to computational representation, and an inverse way for a connection opposite to the direct one.</p><p>In this paper, I extend the definition of the gap in <ref type="bibr" target="#b9">[10]</ref> by distinguishing two ways. The gap reflects one of the critical difficulties in machine learning. For the direct-way study, the difficulty source mostly comes from ambiguity and subjectivity of the linguistic representation (say, on mental entity), which will lead to an ill-defined problem. While sharing the same problem, an inverse-way study will introduce an extra challenge called ill-posed problem, in which there is no unique solution (say, from a to 3D objects).</p><p>Up to now, we have missed much studies on learning target selection if comparing with a study of feature selection. When "What to learn?" is the most primary problem in machine learning, we do need a systematic, or comparative, study on this subject. The investigations from <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> into discriminative and generative models confirm the importance of learning target selection in the vein of computational representation. From the investigations, one can identify the advantages and disadvantages of each model for applications. A better machine gaining the benefits from both models is developed <ref type="bibr" target="#b12">[13]</ref>. Furthermore, the subject of "What to learn?" will provide a strong driving force to machine learning study in seeking "the fundamental laws that govern all learning processes" <ref type="bibr" target="#b13">[14]</ref>. Take a decision rule about "Less costs more"<ref type="foot" target="#foot_0">foot_0</ref> example. Generally, Chinese people classify object's values according to this rule. In Big Data processing, the useful information, that often belongs to a minority class, is extracted from massive datasets. While an English idiom describes it as "Finding a needle in a haystack", the Chinese saying refers to "Searching a needle in a sea (大海撈針)". Users may consider that an error from a minority class will cost heavier than that from a majority class in their searching practices. This consideration will derive a decision rule like "Less costs more" . The rule will be one of the important strategies in Big Data processing. Two questions can be given to the example. What is the mathematical principle (or fundamental law) for supporting the decision rule of "Less costs more"? Is it a Bayesian rule? Machine learning study does need to answer the questions.</p><p>Shannon introduced "entropy" concept as the basis of information theory <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_0">H(Y ) = - y p(y) log 2 p(y),<label>(1)</label></formula><p>where Y is a discrete random variable with probability mass function p(y). Entropy is an expression of disorder to the information. From this basic concept, the other information measures (or entropy functions) can (Table <ref type="table" target="#tab_0">1</ref>), where p(t, y) is the joint distribution for the target random variable T and prediction random variable Y , and p(t) and p(y) are called marginal distributions.</p><p>We call them measures because some of them do not satisfy the metric properties fully, like KL divergence (asymmetric). More other measures from information theory can be listed as learning criteria, but the measures in Table <ref type="table" target="#tab_0">1</ref> are more common and sufficiently meaningful for the present discussion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dissimilarity Asymmetry</head><p>We can divid the learning machines, in view of "mathematical principles", within two groups. One group is designed based on the empirical formulas, like error rate or bound, cost (or risk), utility, or classification margins. The other is on information theory <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>. Therefore, a systematic study seems necessary to answer the two basic questions below <ref type="bibr" target="#b16">[17]</ref>: Q1: When one of the principal tasks in machine learning is to process data, can we apply entropy or information measures as a generic learning target for dealing with uncertainty of data in machine learning? Q2: What are the relations between information learning criteria and empirical learning criteria, and the advantages and limitations in using information learning criteria?</p><p>Regarding to the first question, Watanabe <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> proposed that "learning is an entropy-decreasing process" and pattern recognition is "a quest for minimum entropy". The principle behind entropy criteria is to transform disordered data into ordered one (or pattern). Watanabe seems to be the first "to cast the problems of learning in terms of minimizing properly defined entropy functions" <ref type="bibr" target="#b19">[20]</ref>, and throws a brilliant light on the learning target selection in machine learning.</p><p>In 1988, Zellner theoretically proved that Bayesian theorem can be derived from the optimal information processing rule <ref type="bibr" target="#b20">[21]</ref>. This study presents a novel, yet important, finding that Bayesian theory is rooted on information and optimization concepts. Another significant contribution is given by Principe and his collaborators <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b0">1]</ref> for the proposal of Information Theoretical Learning (ITL) as a generic learning target in machine learning. We consider ITL will stimulate us to develop new learning machines as well as "theoretical interpretations" of learning mechanisms. Take again the example of the decision rule about "Less costs more". Hu <ref type="bibr" target="#b22">[23]</ref> demonstrates theoretically that Bayesian principle is unable to support the rule. When a minority class approximates to a zero population, Bayesian classifiers will tend to misclassify the minority class completely. The numerical studies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> show that mutual information provides positive examples to the rule. The classifiers based on mutual information are able to protect a minority class and automatically balance the error types and reject types in terms of population ratios of classes. Theses studies reveal a possible mathematical interpretation of learning mechanism behind the rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">(Dis)similarity Measures in Machine Learning</head><p>When mutual information describes similarity between two variables, the other information measures in Table <ref type="table" target="#tab_0">1</ref> are applied in a sense of dissimilarity. For a better understanding of them, their graphic relations are shown in Fig. <ref type="figure">5</ref>. If we consider the variable T provides a ground truth statistically (that is, p(t) = (p 1 , ..., p m ) with the population rate p i (i = 1, ..., m) is known and fixed), its entropy H(T ) will be the baseline in learning. In other words, when the following relations hold,</p><formula xml:id="formula_1">I(T, Y ) = H(T ; Y ) = H(Y ; T ) = H(Y ) = H(T ), or KL(T, Y ) = KL(Y, T ) = H(T |Y ) = H(Y |T ) = 0,<label>(2)</label></formula><p>we call the measures reach the baseline of H(T ).</p><p>Based on the study in <ref type="bibr" target="#b25">[26]</ref>, further relations are illustrated in Fig. <ref type="figure" target="#fig_4">6</ref> between exact classifications and the information measures. We apply the notations of E, Rej, A, CR for the error, reject, accuracy, and correct recognition rates, respectively. Their relations are given by:</p><formula xml:id="formula_2">CR + E + Rej = 1, A = CR CR + E .<label>(3)</label></formula><p>The form of {y k } = {t k } in Fig. <ref type="figure" target="#fig_4">6</ref> describes an equality between the label variables in every samples. For a finite dataset, the empirical forms should be Fig. <ref type="figure">5</ref>. Graphic relations among joint information, mutual information, marginal information, conditional entropy, cross entropy and KL divergences (modified based on <ref type="bibr" target="#b24">[25]</ref> by including cross entropy and KL divergences).</p><p>used for representing the distributions and measures <ref type="bibr" target="#b25">[26]</ref>. Note that the link using "↔" indicates a two-way connection for equivalent relations, and "→" for a one-way connection. Three important aspects can be observed from Fig. <ref type="figure" target="#fig_4">6</ref>:</p><p>I. The necessary condition of exact classifications is that all the information measures reach the baseline of H(T ). II. When an information measure reaches the baseline of H(T ), it does not sufficiently indicate an exact classification. III. The different locations of one-way connections result in the interpretations why and where the sufficient condition exists. Although Fig. <ref type="figure" target="#fig_4">6</ref> only shows the relations to the information measures listed in Table <ref type="table" target="#tab_0">1</ref> for the classification problems, its observations may extend to other information measures as well as to the other problems, like clustering, feature selection/extraction, image registrations, etc. When we consider machine learning or pattern recognition to be a process of data in a similarity sense (any dissimilarity measure can be transformed into similarity one <ref type="bibr" target="#b25">[26]</ref>), one important theorem exists to describe their relations. Theorem 1. Generally, there is no one-to-one correspondence between the empirically-defined similarity measures and information measures.</p><p>The proof is neglected in this paper, but it can be given based the study of bounds between entropy and error (cf. <ref type="bibr" target="#b26">[27]</ref> and references therein). The significance of Theorem 1 implies that an optimization of information measure may not guarantee to achieve an optimization of the empirically-defined similarity measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Final remarks</head><p>Machine learning can be exploited with different perspectives depending on study goals of researchers. For a deep understanding of ourself on the learning mechanisms mathematically, we can take learning machines as human's extended sensory perception. This paper stresses on identifying the primary problem in machine learning from a novel perspective. I define it as "What to learn?" or "learning target selection". Furthermore, two sets of representations are specified, namely, "linguistic representation" and "computational representation". While a wide variety of computational representations have been reported in learning targets, we can argue that if there exists a unified, yet fundamental, principle behind them. Towards this purpose, this paper extends the Watanabe's proposal <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and the studies from Zellner <ref type="bibr" target="#b20">[21]</ref> and Principe <ref type="bibr" target="#b0">[1]</ref> to a "conjecture of learning target selection" in the following descriptions.</p><p>Conjecture 1. In a machine learning study, all computational representations of learning target(s) can be interpreted, or described, by optimization of entropy function(s).</p><p>I expect that the proposal of the conjecture above will provide a new driving force not only for seeking fundamental laws governing all learning processes <ref type="bibr" target="#b13">[14]</ref> but also for developing improved machines <ref type="bibr" target="#b27">[28]</ref> in various applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Four basic problems (or levels) in machine learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Design flow according to the basic problems in machine learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Learning target selection within linearly separated dataset. (after [8] on Fig. 5-17). Black Circle = Class 1, Ruby Square = Class 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of "What to learn?" and a need of defining a linguistic representation of similarity for the given character. a) Original image (http://en.wikipedia.org/wiki/Albert_Einstein). b) Caricature image drawn by A. Hirschfeld (http://www.georgejgoodstadt.com/goodstadt/hirschfeld.dca).</figDesc><graphic coords="5,321.20,116.78,81.89,94.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Relations between exact classifications and mutual information, conditional entropy, cross entropy and KL divergences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Some information formulas and their properties as learning measures.</figDesc><table><row><cell>Name</cell><cell>Formula</cell><cell></cell><cell></cell><cell>(Dis)similarity (A)symmetry</cell></row><row><cell>Joint Information</cell><cell>H(T, Y ) = -</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">p(t, y) log 2</cell><cell>p(t,y) p(t)p(y)</cell><cell>Similarity</cell><cell>Symmetry</cell></row><row><cell></cell><cell>t y</cell><cell></cell><cell></cell></row><row><cell cols="2">Conditional Entropy H(Y |T ) = -</cell><cell cols="3">p(t, y) log 2 p(y|t) Dissimilarity Asymmetry</cell></row><row><cell></cell><cell cols="2">t y</cell><cell></cell></row><row><cell>Cross Entropy</cell><cell>H(T ; Y ) = -</cell><cell cols="3">pt(z) log 2 py(z)</cell><cell>Dissimilarity Asymmetry</cell></row><row><cell></cell><cell>z</cell><cell></cell><cell></cell></row><row><cell>KL Divergence</cell><cell>KL(T, Y ) =</cell><cell>pt(z) log 2</cell><cell cols="2">pt(z) py (z)</cell></row><row><cell></cell><cell>z</cell><cell></cell><cell></cell></row></table><note><p>t y p(t, y) log 2 p(t, y) Inapplicable Symmetry Mutual Information I(T, Y ) =</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This rule is translated from Chinese saying, " 物以稀為貴" in Pinyin "Wu Yi Xi Wei Gui". The translation is modified from the English phase "Less is more" which usually describes simplicity in design.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work is supported in part by <rs type="funder">NSFC</rs>(No. <rs type="grantNumber">61273196</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JNB32c4">
					<idno type="grant-number">61273196</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Information theoretic learning: Renyi&apos;s entropy and kernel perspectives</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Lao Tzu (ca. 500 BCE) Tao Te Ching《道德經》</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nature is the realization of the simplest conceivable mathematical ideas: Einstein and the canon of mathematical simplicity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Norton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in History and Philosophy of Modern Physics</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="135" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vision. A computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How the brain might work: The role of information and learning in understanding and replicating intelligence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information: Science and technology for the new century</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Jacovitt</surname></persName>
		</editor>
		<imprint>
			<publisher>Lateran University Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="45" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Scaling learning algorithms towards AI. Large-scale kernel machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">generalized constraint neural networks model: Associating partially known relationships for nonlinear regressions</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="1929" to="1943" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pattern classification, 2nd edn</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Caricature generator: The dynamic exaggeration of faces by computer</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="392" to="400" />
			<pubPlace>Leonardo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative vs informative learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="49" to="53" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative or discriminative? Getting the best of both worlds</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lasserre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Statistics 8</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bernardo</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="3" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The discipline of machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno>CMU- ML-06-108</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication. Bell System Technical</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Information-theoretic measures for knowledge discovery and data mining</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Karmeshu (ed) Entropy Measures, Maximum Entropy Principle and Emerging Applications</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="115" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluation criteria based on mutual information for classifications including rejected class</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1396" to="1403" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pattern recognition as a quest for minimum entropy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="381" to="387" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pattern recognition as conceptual morphogenesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="161" to="165" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey of decision tree classifier methodology</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Safavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Sys Man Cyber</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="660" to="674" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Optimal information processing and Bayes&apos;s theorem. The American Statistician</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zellner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="278" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Information theoretic learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><surname>Jw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Unsupervised Adaptive Filtering</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="265" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What are the differences between Bayesian classifiers and mutualinformation classifiers?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Networks Learning Sys</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="249" to="264" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A new strategy of cost-free learning in the class imbalance problem IEEE Trans on Knowledge Data Eng</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Bg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2872" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Information theory, inference, and learning algorithms</title>
		<author>
			<persName><forename type="first">Djc</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Information-theoretic measures for objective evaluation of classifications</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">T</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1170" to="1182" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A new approach of deriving bounds between entropy and error from joint distribution: Case study for binary classifications</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.6602v1[cs.IT</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Robust recognition via information theoretic learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
