<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Tribes of Machine Learning and the Realm of Computer Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ayaz</forename><surname>Akram</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Lowe-Power</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Tribes of Machine Learning and the Realm of Computer Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">461B9FA8652C5BE609CF235E848040AE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine learning (ML) refers to the process in which computers learn to make decisions based on the given data set without being explicitly programmed to do so <ref type="bibr" target="#b7">[8]</ref>. There are various classifications of the ML algorithms. One of the more insightful classifications has been done by Pedro Domingos in his book The Master Algorithm <ref type="bibr" target="#b38">[39]</ref>. Domingos presents five fundamental tribes of ML: the symbolists, the connectionists, the evolutionaries, the bayesians and the analogizers. Each of these believe in a different strategy to go through the learning process. These tribes or schools of thought of ML along-with their primary algorithms and origins are shown in Table <ref type="table" target="#tab_0">1</ref>. There are existing proofs that given the enough amount of data, each of these algorithms can fundamentally learn anything. Most of the well known ML techniques/algorithms<ref type="foot" target="#foot_0">foot_0</ref> belong to one of these tribes of ML. In this paper, we look at these five school of thoughts of ML and identify how each of them can be fundamentally used to solve different research problems related to computer architecture. ML techniques have already influenced many domains of computer architecture. Figure <ref type="figure">1</ref> shows the number of research works using ML each year since 1995. It is observed that most of the work employing ML techniques (approximately 65% of the studied work) is done in last 5-6 years indicating increasing popularity of ML models in computer architecture research. Findings also indicate that Neural Networks are the most used ML technique in computer architecture research as shown in Figure <ref type="figure">2</ref>. Decision Trees Genetic Algo. KNN K-Means Linear Regress. Logistic Regress. Naive Bayes Neural Nets. Random Forest Reinforc. Learn. SVM Others 0 10 20 30 40 Number of Works Fig. 2. Number of works for each ML Algorithm</p><p>We also present an extensive survey of the research works employing these techniques in computer architecture. Most of the insights discussed in this paper are based on a review of more than a hundred papers 2 which use ML for computer architecture.</p><p>The specific questions we answer in this paper include:</p><p>• What are the fundamental features of different ML algorithms which make them suitable for particular architecture problems compared to the others? 2 Detailed summaries of most of these papers are available in: <ref type="url" target="https://github.com/ayaz91/Literature-Review/blob/main/ML_In_CompArch/ML_CompArch.md">https://github.com/ayaz91/Literature-Review/blob/main/ML_   In_CompArch/ML_CompArch.md</ref> </p><p>The Tribes of Machine Learning and the Realm of Computer Architecture 0:3</p><p>• How has ML impacted computer architecture research so far?</p><p>• What are the most important challenges that need to be addressed to fully utilize ML potential in computer architecture research? Microarchitecture <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b129">130,</ref><ref type="bibr" target="#b143">144]</ref>, Performance Estimation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b78">79]</ref>, Scheduling <ref type="bibr" target="#b55">[56]</ref>, Energy <ref type="bibr" target="#b121">[122]</ref>, instruction scheduling <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b123">124]</ref> Connectionists Credit Assignment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SURVEY OF USE OF MACHINE LEARNING IN COMPUTER ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep-Neural Networks, Perceptrons</head><p>Microarchitecture <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b157">158,</ref><ref type="bibr" target="#b172">173,</ref><ref type="bibr" target="#b184">185,</ref><ref type="bibr" target="#b192">193]</ref>, Performance Estimation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b188">189]</ref>, scheduling <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b185">186,</ref><ref type="bibr" target="#b186">187]</ref>, DSE <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b84">85]</ref>, Energy <ref type="bibr" target="#b187">[188]</ref>, Security <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b134">135,</ref><ref type="bibr" target="#b136">137]</ref> Evolutionaries Structure Discovery</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Genetic Algorithm</head><p>Performance estimation <ref type="bibr" target="#b58">[59]</ref> , Design Space Exploration <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b144">145,</ref><ref type="bibr" target="#b166">167]</ref>, instruction scheduling <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b94">95]</ref> Bayesians Uncertainty Reduction</p><p>Naive-Bayes, LDA Microarchitecture <ref type="bibr" target="#b12">[13]</ref>, DSE <ref type="bibr" target="#b144">[145]</ref> Analogizers Similarity Discovery</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM, KNN</head><p>Microarchitecture <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b104">105]</ref>, Performance <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">59]</ref>, Scheduling <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b185">186,</ref><ref type="bibr" target="#b186">187]</ref>, Security <ref type="bibr" target="#b136">[137]</ref> In this section, we will discuss how each of the previously mentioned paradigms of ML can be (or have been) applied to the field of computer architecture. Table <ref type="table" target="#tab_1">2</ref> enlists the five tribes of ML along-with their targeted problem and some examples of the relevant research works. Of course, many architectural problems can be solved by more than one of these families of ML algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Symbolists:</head><p>This tribe of ML relies on symbol manipulation to produce intelligent algorithms. The fundamental problem that Symbolists are trying to solve is knowledge composition. Their insight is to use initial knowledge to learn quicker than learning from scratch. More specific examples of this group of ML algorithms include inverse deduction and decision trees. Algorithms belonging to Symbolists seem to be ideal to be used for cases where a cause and effect relationship needs to be established between events. For example, the architecture problems where we need this type of learning include: finding reasons for hardware security flaws and consistency bugs.</p><p>The more interpretable nature of these algorithms make them a good candidate to understand the impact of certain design features (input events) on the performance metric of the entire system. As a result, certain parameters can be fine tuned to produce desirable results. For example, this kind of learning can be applied to understand which pipeline structures consume the most amount of energy in a given configuration. However, it might be hard to map these algorithms directly to the hardware (if desired to be used at run-time) due to their symbolic nature.</p><p>There exist some examples of usage of these algorithms in computer architecture research. Fern et al. <ref type="bibr" target="#b44">[45]</ref> introduced decision tree based branch predictors. Decision trees allowed the proposed branch predictor to be controlled by different processor state features. The relevant features could change at run-time without increasing linearly in size with the addition of new features (compared to table based predictors), providing significant benefits over conventional table based predictors.</p><p>Decision tree based models have been used in many cases to understand the impact of different architectural events on systems' performance. Yount et al. <ref type="bibr" target="#b42">[43]</ref> compared various machine learning (ML) algorithms with respect to their ability to analyze architectural performance of different workloads and found tree-based ML models to be the most interpretable and almost as accurate as Artificial Neural Networks (ANNs). Jundt et al. <ref type="bibr" target="#b78">[79]</ref> used a ML model named Cubist <ref type="bibr">[1]</ref>, which uses a tree of linear regression models to observe the importance of architectural blocks that have an effect on performance and power of high performance computing applications on Intel's Sandy Bridge and ARMv8 XGene processors. Mariani et al. <ref type="bibr" target="#b114">[115]</ref> used Random Forest to estimate HPC (High Performance Computing) applications' performance on cloud systems using hardware independent profiling of applications.</p><p>Rahman et al. <ref type="bibr" target="#b143">[144]</ref> used decision trees and logistic regression to build framework to identify the best prefetcher configuration for given multithreaded code (in contrast to focus on serial code as in <ref type="bibr" target="#b104">[105]</ref>). Hardware prefetcher configuration guided by the presented machine learning framework achieved close to 96% speed-up of optimum configuration speed-up.</p><p>Moeng and Melhem <ref type="bibr" target="#b121">[122]</ref> used decision trees (implemented in hardware) to propose a DVFS (dynamic voltage and frequency scaling) policy that will be able to predict clock frequency resulting into the least amount of energy consumption in a multicore processor. They used simple measurement metrics like cycles, user instructions, total instructions, L1 accesses and misses, L2 accesses, misses and stalls for a DVFS interval during execution as inputs to the decision trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Connectionists:</head><p>The connectionists rely on brain structure to develop their learning algorithms and try to learn connections between different building blocks (neurons) of the brain. The main problem they are trying to solve is of credit assignment i.e. figure out which connections are responsible for errors and what should be the actual strength of those connections. The most common example of this class of ML algorithms is (deep) neural networks. The algorithms belonging to the Connectionists are good at learning complex patterns specially at run time (as evident by many microarchitecture design examples). This type of learning can be useful in many microarchitectural predictive structures (like branch predictors, cache prefetchers and value predictors) which try to forecast an event based on similar events of the past, where different past events might have different weights in determining if a certain event will happen in the future (the problem of credit assignment). In contrast to symbolists, the algorithms belonging to this group might not need any initial knowledge, however they are not very interpretable. So, they might not be very useful to understand the importance of different architectural events/components in overall performance/output for static analysis. This kind of algorithms have found uses in the architecture research, specially simple neural networks like perceptrons <ref type="bibr" target="#b18">[19]</ref> (owing to their simple structure and being more amenable to be implemented in the hardware).</p><p>Calder et al. <ref type="bibr" target="#b26">[27]</ref> introduced the use of neural networks for static branch prediction in the late 1990's. One of the earliest works to use ML for dynamic branch prediction was done by Vinton and Iridon <ref type="bibr" target="#b178">[179]</ref>. They used neural networks with Learning Vector Quantization (LVQ) <ref type="bibr" target="#b93">[94]</ref> as a learning algorithm for neural networks and were able to achieve around 3% improvement in misprediction rate compared to conventional table based branch predictors. Later, Egan et al. <ref type="bibr" target="#b41">[42]</ref> and Wang and Chen <ref type="bibr" target="#b184">[185]</ref> also used LVQ for branch prediction. The complex hardware implementation of LVQ due to computations involving floating point numbers, could significantly increase the latency of the predictor <ref type="bibr" target="#b75">[76]</ref>. Jimenez et al. <ref type="bibr" target="#b75">[76]</ref>, working independently, also used neural network based components, perceptrons <ref type="bibr" target="#b18">[19]</ref>, to perform dynamic branch prediction. In their work, each single branch is allocated a perceptron. The inputs to the perceptron are the weighted bits of the "global branch history shift register", and the output is the decision about branch direction. One big advantage of perceptron predictor is the fact that the size of perceptron grows linearly with the branch history size (input to the perceptron) in contrast the size of pattern history table (PHT) in PHT based branch predictors which grows exponentially with the size of the branch history. Therefore, within same hardware budget, perceptron based predictor is able to get a benefit from longer branch history register. One of the big problems with the use of perceptrons is their inability to learn linear separability 3 . This was later resolved by Jimenez <ref type="bibr" target="#b70">[71]</ref> using a set of piecewise linear functions to predict the outcomes for a single branch. These linear functions refer to a distinct historical path that lead to the particular branch instruction. Graphically, all these functions, when combined together, form a surface.</p><p>Since the introduction of neural network based branch predictors, there has been a lot of research done to optimize the design of these predictors. Different analog versions have been proposed to improve speed and power consumption of the neural network based branch predictors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b165">166]</ref>. Perceptron based predictors have also been combined with other perceptron or non-neural network based predictors to achieve better accuracy overall: <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b122">123,</ref><ref type="bibr" target="#b148">149,</ref><ref type="bibr" target="#b164">165,</ref><ref type="bibr" target="#b169">170]</ref>. Different optimizations/modifications to neural network branch predictors including use of different types of neural networks have been explored: <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b131">132,</ref><ref type="bibr" target="#b155">156,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b173">174]</ref>. These optimizations improve performance of the predictor and save power and hardware cost. Similar concepts are also used in development of other branch predictor related structures like confidence estimators and indirect branch predictors: <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b81">82]</ref>. The statistical corrector predictor used in TAGE-SC_L <ref type="bibr" target="#b157">[158]</ref> branch predictor 4 is also a perceptron based predictor. This statistical corrector predictor regresses TAGE's prediction if it statistically mispredicts in similar situations. Mao et al. <ref type="bibr" target="#b112">[113]</ref>, recently, applied deep learning (easy to train Deep Belief Networks (DBN) <ref type="bibr" target="#b56">[57]</ref> to the problem of branch prediction.</p><p>The popularity of perceptrons in branch prediction has also affected the design of other microarchitecture structures. For example, Wang and Luo <ref type="bibr" target="#b181">[182]</ref> proposed perceptron based data cache prefetching. The proposed prefetcher is a two level prefetcher which uses conventional table-based prefetcher at the first level. At the second level a perceptron is used to reduce unnecessary prefetches by relying on memory access patterns. Peled et al. <ref type="bibr" target="#b138">[139]</ref> used neural networks to capture semantic locality of programs. Memory access streams alongwith a machine state is used to train neural network at run-time which predicts the future memory accesses. Evaluation of the proposed neural prefetcher using SPEC2006 <ref type="bibr" target="#b162">[163]</ref>, Graph500 <ref type="bibr" target="#b126">[127]</ref> benchmarks and other hand-written kernels indicated an average speed-up of 22% on SPEC2006 benchmarks and 5x on other kernels. However, importantly, Peled et al. <ref type="bibr" target="#b138">[139]</ref> also performed a feasibility analysis of the proposed prefetcher which shows that the benefits of neural prefetcher are outweighed by other factors like learning overhead, power and area efficiency. Teran et al. <ref type="bibr" target="#b172">[173]</ref> applied perceptron learning algorithm (not actual perceptrons) to predict reuse of cache bkocks using features like addresses of recent memory instructions and portions of address of current block. Verma <ref type="bibr" target="#b176">[177]</ref> extended the work of Teran et al. <ref type="bibr" target="#b172">[173]</ref> and proposed CARP (coherence-aware reuse prediction). CARP uses cache coherence information as an additional feature in the alogrithm of Teran et al. <ref type="bibr" target="#b172">[173]</ref>. Seng and Hamerly 3 A boolean function is "linearly separable" if all false instances of the function can be separated from its all true instances using a hyperplane <ref type="bibr" target="#b120">[121]</ref>. As an example XOR is linearly inseparable and AND is linearly separable. 4 won the 2014 branch predictor championship and combines TAGE branch predictor with a loop predictor and a statistical corrector predictor <ref type="bibr" target="#b154">[155]</ref> did one of the earliest works to present perceptron based register value predictor, where each perceptron is fed with the global history of recently committed instructions. Later, Black and Franklin <ref type="bibr" target="#b17">[18]</ref> proposed perceptron based confidence estimator for a value predictor. Nemirovsky et al. <ref type="bibr" target="#b130">[131]</ref> proposed a neural network based scheduler design for heterogeneous processors.</p><p>These algorithms have also been used to build performance/power models. For example, Khan et al. <ref type="bibr" target="#b84">[85]</ref> proposed feed-forward neural network based predictive modeling technique to do design space exploration for chip multiprocessors. Wu et al. <ref type="bibr" target="#b188">[189]</ref> proposed a neural network based GPU performance and power prediction model to solve the problem of slow simulation speed of simulators to study performance/power of GPUs. The proposed model estimates performance and power consumption of applications with changes in GPU configurations.</p><p>Dai et al. <ref type="bibr" target="#b36">[37]</ref> exploited deep learning techniques to propose Block2Vec (which is inspired by Word2Vec <ref type="bibr" target="#b117">[118]</ref> used in word embeding), which can find out correlations among blocks in storage systems. Such information can be used to predict the next block accesses and used to make prefetching decisions. Khakhaeng and Chantrapornchai <ref type="bibr" target="#b83">[84]</ref> used perceptron based neural network to build model to predict ideal cache block size. Neural network is trained using features from address traces of benchmarks. The particular features used for training include: cache misses and size and frequency adjoining addresses which reflects temporal and spatial locality of the program.</p><p>Neural networks have also been used (both offline and online) to discover the optimal schedule of workloads or manage other aspects of shared hardware resources. For example, Bitirgen et al.. <ref type="bibr" target="#b16">[17]</ref> implemented Artificial Neural Network (ANN) in the hardware for the management of shared resources in multiprocessor systems. Each ANN (edge weights are multiplexed at the run time to achieve virtual ANNs using one hardware ANN) acts as a performance model and takes available resources and other factors describing the program behavior (e.g. read/write hits and misses in data cache and the portion of dirty cache ways allocated to the applications) as inputs and outputs the information which helps to decide which resource distiribution would result in the best overall performance. Li et al. <ref type="bibr" target="#b101">[102]</ref> used ANNs to predict the performance of parallel tasks at run-time to do an optimal scheduling. Nemirovsky et al. <ref type="bibr" target="#b130">[131]</ref> proposed an ANN based approach to perform scheduling on heterogeneous processors, which increases throughput by 25-31% compared to a RoundRobin scheduler on an ARM big.Little system <ref type="bibr" target="#b54">[55]</ref>. The proposed methodology relies on ANNs to predict thread's performance for a particular scheduling interval on different hardware core types and the scheduler picks the schedule of threads that would maximize the system performance.</p><p>Other examples of the use of neural networks include the work of Ipek et al. <ref type="bibr" target="#b59">[60]</ref> for design space exloration and the work of Chiappetta et al. <ref type="bibr" target="#b30">[31]</ref> to detect cache based side channel attacks (e.g. Flush+Reload <ref type="bibr" target="#b191">[192]</ref>) relying on the hardware performance counter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Evolutionaries:</head><p>This class of algorithms are based on the evolutionary process of nature and rely on learning structures instead of learning parameters. These algorithms keep on evolving and adapting to unspecified surroundings. The most obvious example are genetic algorithms, which potentially can be used for searching the best design in a large design space. Another problem these algorithms are a natural fit to solve is of an attacker which evolves to different versions once a preventive technique is deployed (e.g. different ML based side channel attack detection and mitigation solutions). These algorithms can help in building preventive techniques which can evolve with the evolution of the attacker. General purpose computer systems might be used for a diverse set of applications after being deployed. It seems reasonable to make these systems able to evolve their configuration at runtime depending on their use. Genetic algorithms seem to be the most suitable algorithms to learn how to adapt to the best possible system configuration depending on the workloads being executed.</p><p>There exist only a limited number of examples of the utilization of these algorithms by computer architects. For example, Joel et al. <ref type="bibr" target="#b43">[44]</ref> used genetic algorithms to design better branch predictors and Jimenez et al. <ref type="bibr" target="#b73">[74]</ref> took help of genetic algorithms to introduce a pseudo-LRU (least recently used) insertion and promotion mechanism for cache blocks in last level caches, which could result in 5% speedup compared to traditional LRU (least recently used) algorithm using much less overhead. The performance of the proposed mechanism matched other contemporary techniques like DRRIP (dynamic rereference interval prediction <ref type="bibr" target="#b62">[63]</ref>) and PDP (protecting distance policy <ref type="bibr" target="#b40">[41]</ref>) with less overhead. Mukundan and Martinez <ref type="bibr" target="#b124">[125]</ref> used genetic algorithms to propose MORSE (Multi-objective Reconfigurable Self-optimizing Memory Scheduler) extending Ipek et al's work <ref type="bibr" target="#b60">[61]</ref> which used reinforcement learning for memory scheduling. MORSE can target optimization of different metrics like performance, energy and throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The Bayesians:</head><p>The algorithms belonging to the Bayesians are usually concerned with incorporating new evidence into previous beliefs. The main problem they try to solve is of reducing uncertainty. They are specially good at predicting events when there is a known probability of occurrence of particular events. Naive Bayes and Linear Discriminant Analysis (LDA) are a couple of examples of this group of algorithms. These algorithms have a high potential to be used by the architects due to their inherent characterstics, as a lot of architecture problem solutions rely on the knowledge of existence of certain events. For instance, a bayesian based model to detect hardware side channel attacks can rely on the fact that the probability of system under no attack would be very high normally. Similarly, a model to predict memory events can depend on the average probability of an instruction being a memory operation. Beckman and Sanchez <ref type="bibr" target="#b12">[13]</ref> recognized that the problem of design of cache replacement policies have to deal with uncertainty as the time when the candidate cache blocks will be accessed is not known. Thus, bayesians have the potential to learn in such environments and develop smart cache replacement policies.</p><p>Overall, Bayesians also have not found a lot of use in the computer architecture community as our survey suggests. Jung and Pedram <ref type="bibr" target="#b79">[80]</ref> used Bayesian classification to tune voltage and frequency settings to reduce system's energy consumption in a multicore processor. Reagen et al. <ref type="bibr" target="#b144">[145]</ref> used Bayesian optimization <ref type="bibr" target="#b140">[141]</ref> to explore design space for a Deep Neural Network hardware accelerator. Different design parameters studied in this work, like neurons/layer for DNN, L2 regularization for DNN, learning rate of DNN, loop parallelism in hardware, and hardware pipelining, have "complex interactions" among them which the bayesian optimization tries to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">The Analogizers:</head><p>Analogizers believe in learning things from other examples. The primary problem they target is finding similarities between different situations. Few examples of algorithms belonging to this domain include: support vector machines (SVM), k-means clustering and k-nearest neighbor (KNN). Some of these algorithms (like k-means clustering and KNN) are used in un-supervised learning. Algorithms from this group can be good candidates to be applied in cutting down the search space for design space exploration especially when configuration space is large and there exists multiple designs with marginal differences. These algorithms can help to focus on designs which are sufficiently different from a baseline. Similarly, they can help in detecting similarities at the granularity of programs or at the granularity of instructions to guide better predictions at the run-time by different microarchitectural structures.</p><p>Analogizers have been pretty successful in the domain of computer architecture. For example, Culpepper and Gondree <ref type="bibr" target="#b35">[36]</ref> used support vector machines (SVM) to improve accuracy of branch prediction. This SVM based branch predictor performed better compared to fast-path based predictor and gshare predictor at high hardware budgets <ref type="bibr" target="#b35">[36]</ref>. Sherwood et al. <ref type="bibr" target="#b158">[159]</ref> used K-means clustering to form groups/clusters of similar basic block vectors (BBV 5 ). These groups then act as a few representative portions of the entire program and are known as Simpoints, which can be used to approximate the performance/power for the entire program. Hoste et al. <ref type="bibr" target="#b58">[59]</ref> detected similarity among programs using data from various microarchitecture-independent statistics (like instruction classes, instruction-level parallelism and register traffic, branch predictability and working set sizes) to predict performance of programs similar to reference benchmarks with known performance on specific microarchitecture. They used principal component analysis for this purpose alongwith other ML techniques.</p><p>Walker et al. <ref type="bibr" target="#b180">[181]</ref> relied on heirarchical cluster analysis <ref type="bibr" target="#b24">[25]</ref> to form groups of various performance monitoring counters which are then used to estimate power consumption for mobile and embedded devices and showed that their technique leads to lower percentage error compared to many other power estimation techniques <ref type="bibr" target="#b141">[142,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b179">180]</ref>.</p><p>Baldini et al. <ref type="bibr" target="#b11">[12]</ref> trained two binary classifiers Nearest Neighbor with Generalized exemplars (NNGE) and Support Vector Machine (SVMs) to predict possible GPU speed-up given a run of parallel code on CPU (using OpenMP implementation). Wang and Ipek <ref type="bibr" target="#b183">[184]</ref> proposed an online data clustering based technique to reduce energy of data transfers in memory by reducing the number of ones in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Reinforcement Learning: The Sixth Tribe 6</head><p>Reinforcement learning refers to the process in which an agent learns to take actions in a given environment through independent exploration of different possible actions and choosing the ones that increase the overall reward. Reinforcement learning does not require any prior data to learn to take the right action, rather it learns to do this on the fly. This makes it a good fit to be used for computer architecture problems where significant former knowledge or data is not available and the right action can only be learned dynamically. For example, hardware optimizations or hardware based resource management techniques which are totally transparent to the software can rely on reinforcement learning to decide the best action to take for the overall efficiency of the system.</p><p>One of the earliest examples of the use of reinforcement learning in computer architecture is the work of Ipek et al <ref type="bibr" target="#b60">[61]</ref>. Ipek et al <ref type="bibr" target="#b60">[61]</ref> introduced reinforcement learning based DRAM scheduling for an increased utilization of memory bandwidth. The DRAM scheduler which acts as a reinforcement learning agent utilizes system state defined by different factors like number of read/write requests residing in the transaction queue. The actions that the agent can take include all normal commands of DRAM controller like read, write, pre-charge and activate. Ipek et al <ref type="bibr" target="#b60">[61]</ref> implemented a five stage hardware pipeline to calculate q-values <ref type="bibr" target="#b168">[169]</ref> associated with the credit assignment which determine the eventual benefit for an action given the current state.</p><p>Peled et al <ref type="bibr" target="#b137">[138]</ref> used reinforcement learning to approximate program semantic locality, which was later used to anticipate data access patterns to improve prefetching decisions. Accesses are considered to have semantic locality if there exists a relationship between them via a series of actions. A subset of different attributes (e.g. program counter, accesses history, branch history, status of registers, types and offsets of objects in program data structures and types of reference operations) is used to represent the current context a program.</p><p>Juan and Marculescu <ref type="bibr" target="#b77">[78]</ref> proposed a reinforcement learning based DVFS technique which divides the central agent in many "distributed" agents and uses a supervisor for coordination 5 BBV contains the frequency of occurrence of all basic blocks during an interval of execution 6 We refer to reinforcement learning as the sixth tribe of ML as it is one of the most important algorithms belonging to the class of self learning algorithms. Reinforcement learning does not belong to the original ML tribes taxonomy of Pedro Domingos <ref type="bibr" target="#b38">[39]</ref>.</p><p>among other agents to maximize performance under given power budget. Similarly, Ye and Xu <ref type="bibr" target="#b193">[194]</ref> proposed a reinforcement learning based dynamic power management (DPM) technique for multi-core processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Regression Techniques -Statistics Meets Machine Learning 7</head><p>Regression techniques from applied statistics have been largely borrowed by machine learning. A detailed discussion on different regression techniques used in machine learning is provided in the Appendix section. In this section, we take a look at how these regression techniques have influenced the field of computer architecture.</p><p>Performance and power estimation of different workloads on a particular architecture is a critical step to design new architectures or to have a better understanding of the already existing architectures. There are numerous examples of research works where regression techniques are used to estimate performance or power consumption. Performance/power estimation of applications on a particular architecture is mostly done by simulation tools and analytical models. Regression techniques have also been used to build new and accurate models using empirical data. Sometimes they are also used to increase the accuracy of simulation techniques. For example, Lee et al. <ref type="bibr" target="#b98">[99]</ref> proposed regression analysis based calibration methodology called PowerTrain to improve the accuracy of McPAT <ref type="bibr" target="#b102">[103]</ref>. <ref type="bibr">McPAT [103]</ref> is a well-known power estimation simulator, but it is shown to have various inaccuracies due to different sources like un-modeled architectural components, discrepancy between the modeled and actual hardware components and vague configurational parameters <ref type="bibr" target="#b189">[190]</ref>. The proposed calibration methodology called PowerTrain uses power measurements on real hardware to train McPAT using regression analysis. Reddy et al. <ref type="bibr" target="#b145">[146]</ref> studied correlation among gem5 <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b109">110]</ref> statistics and hardware performance monitoring counters to build a model in gem5 to estimate power consumption of ARM Cortex-A15 processor. This work uses the same model built by Walker et al. <ref type="bibr" target="#b180">[181]</ref> but does not include all performance monitoring events as some gem5 equivalents would not be available. The used events in ML model are cycle counts, speculative instructions, L2 cache accesses, L1 instruction cache accesses and memory bus reads. Reddy et al. <ref type="bibr" target="#b145">[146]</ref> show that the differences between statistics of the simulator (gem5) and those of the real hardware only affect the estimated power consumption by approximately 10%.</p><p>There are also examples <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b141">142]</ref> of use of regression techniques to estimate performance of multi-core processors from single core processors. Lee et al. <ref type="bibr" target="#b96">[97]</ref> used spline-based regression to build model for multiprocessor performance estimation from uniprocessor models to mitigate the problems associated with cycle accurate simulation of multiprocessors. Pricopi et al. <ref type="bibr" target="#b141">[142]</ref> used regression analysis to propose an analytical model to estimate performance and power for heterogeneous multi-core processors (HMPs). During an application run, a cpi (cycles per instruction) stack is built for the application using different micro-architectural events that can impact the execution time of the application. Relying on some compiler analysis results alongwith the cpi stack, performance and power can be estimated for other cores in an HMP system. Experiments performed with an ARM big.Little system indicate an intra-core prediction error of below 15% and an inter-core prediction error of below 17%.</p><p>Regression techniques are also used in heterogeneous systems for cross platform performance estimation. Examples include <ref type="bibr" target="#b198">[199]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b132">[133]</ref>, and <ref type="bibr" target="#b9">[10]</ref>. Zheng et al. <ref type="bibr" target="#b198">[199]</ref> used Lasso Linear Regression and Constrained Locally Sparse Linear Regression (CLSLR) to explore correlation among performance of same programs on different platforms to perform cross-platform performance prediction. The test case used in their work is prediction of an ARM ISA platform based on performance on Intel and AMD x86 real host systems. The authors extended this work in <ref type="bibr" target="#b197">[198]</ref> by proposing LACross framework which applies similar methodology to predict performance and power at fine granularity of phases. LACross is shown to have an average error less than 2% in entire program's performance estimation, in contrast to more than 5% error in <ref type="bibr" target="#b197">[198]</ref> for SD-VBS benchmark suite <ref type="bibr" target="#b175">[176]</ref>. Boran et al. <ref type="bibr" target="#b19">[20]</ref> followed Pricopi et al.'s <ref type="bibr" target="#b141">[142]</ref> work and used regression techniques to estimate execution cycle count of a particular ISA core based on the performance statistics of another ISA core. This model is used to dynamically schedule programs in a heterogeneous-ISA multi-core system. ARMv8 and x86-64 based multi-core system is used to validate the model. Although this model shows above 98% accuracy to estimate performance on any particular ISA core, the inter-core performance estimation has high error (23% for estimation from ARM to x86 and 54% for estimation from x86 to ARM). O'Neal et al. <ref type="bibr" target="#b132">[133]</ref> used various linear/non-linear regression algorithms to propose GPU performance predicting model focusing on "pre-silicon design" of GPUs using DirectX 3D workloads. Ardalani et al. <ref type="bibr" target="#b9">[10]</ref> proposed XAPP (Cross Architecture Performance Prediction) technique to estimate GPU performance from CPU code using regression and bootstrap aggregating (discussed in the Appendix section). Different features associated with the program behavior are used to train machine learning models. These features include some basic features related to ILP (instruction level parallelism), floating point operations and memory operations and also some advanced features like shared memory bank utilization, memory coalescing and branch divergence.</p><p>Regression techniques have also been used in design space exploration (e.g. <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b95">96]</ref>). Lee et al. <ref type="bibr" target="#b95">[96]</ref> used regression modeling for fast design space exploration to avoid expensive cycle accurate simulations. Jia et al. <ref type="bibr" target="#b65">[66]</ref> proposed StarGazer, an automated framework which uses a stepwise regression algorithm to explore GPU design space by using only few samples out of all possible design points and estimates the performance of design points with only 1.1% error on average. Some regression techniques have also been used in the domain of hardware security, specifically to detect the malware or micro-architectural side channel attacks. For instance, Ozosoy et al. <ref type="bibr" target="#b134">[135]</ref> used neural network and logistic regression for detection of malware using hardware architectural features (like memory accesses and instruction mixes). They evaluated the FPGA based implementations of both ML models (logistic regression and neural networks). Khasawneh et al. <ref type="bibr" target="#b85">[86]</ref> used neural networks and logistic regression (LR) based hardware malware detectors to prove the possibility of evading malware detection. Availability of malware detector training data makes it possible for attackers to reverse engineer detectors and potentially modify malware to evade their detection. Khasawneh et al. <ref type="bibr" target="#b85">[86]</ref> also proposed randomization based technique to attain resilient malware detection and avoid reverse engineering. Many other works which utilize regression or other ML techniques for micro-architectural side channel attack detection are referred in <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SUMMARY TABLE</head><p>Table <ref type="table" target="#tab_2">3</ref> provides a summary of the previously discussed literature survey at a finer granularity of individual ML techniques. Each column in this table refer to a broader category (or field) of computer architecture that has relied on ML. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPPORTUNITIES AND CHALLENGES:</head><p>This section presents some opportunities to use ML for architecture that have not been explored in their full potential if explored at all. Moreover, we also enlist a number of challenges that need to be tackled to be able to fully exploit ML for architecture research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Opportunities:</head><p>• There exists an opportunity to borrow ideas from ML instead of using ML algorithms in their intact form. Since, implementing ML algorithms in the hardware can be costly, computer architects can rely on ML algorithms to optimize the classical learning techniques used in computer systems. For instance, the work done by Teran et al. <ref type="bibr" target="#b172">[173]</ref> to apply perceptron learning algorithm to predict reuse of cache blocks is a good example of this. • ML based system components can be used in synergy with traditional components. A good example of this collaboration is the work of Wang and Luo <ref type="bibr" target="#b181">[182]</ref> to perform data cache prefetching discussed in the previous section. • ML techniques can potentially be used to build customizable computer systems which will learn to change their behavior at different granularities. Take an example of an 'A' cache eviction policy which works better for workload type 'X' and another policy 'B' which works better for workload type 'Y'. Since, ML makes learning easier, same processor can learn to adapt to using policy 'A' or 'B' depending on the workloads run on it. • Computer architects have relied on heuristics to design different policies. One possiblity is to start using ML in place of these heuristics. Machine Learning can also help us to come up with new and better heuristics that are hard to ascertain otherwise. • For all the decisions taken inside a computer system, multiple policies are used. If single (or a small number of) ML algorithm(s) can learn all of these policies, the design of systems can be significantly simplified. • As we observed earlier, performance and power estimation of existing or new architectures is an area which can be highly influenced by ML. Architects have historically used simulation tools or mathematical models for this purpose. Future architectural innovations are expected to come from optimizations across the entire software/hardware computing stack <ref type="bibr" target="#b99">[100]</ref>. Current simulation techniques are not designed with this kind of use case in mind. Therefore, ML techniques can come to rescue and enable building simulation tools of the future as traditional cycle level modeling might be too slow for cross-stack studies. Not only that ML techniques can enable building new tools, they can help in improving the accuracy of current simulation tools (or performance/power models) which are the primary enabler of computer architecture research at the moment (for example the work by Renda et al. <ref type="bibr" target="#b146">[147]</ref>). • ML can specially prove useful to build systems/accelerators for ML.</p><p>• Design space explorations (which are extensively done in computer architecture research) should use more of ML techniques (especially the ones belonging to evolutionaries and analogizers). Techniques like genetic algorithms can help to evolve the design space itself as well, thus enabling the evaluation of significant configurations which might remain hidden otherwise. Since, all of this exploration is done statically (is part of pre-silicon design), there will be no issues of the cost of the algorithm here. • Computer architecture research often relies on combination of multiple (independently functioning) structures to optimize the system. For example, tournament branch predictors use multiple branch predictors and pick one of them for a given prediction. Similar tasks can be relegated to ML to learn the best of different independent techniques.</p><p>• Computer architects often run a number of benchmarks/workloads on a multitude of hardware configurations (and generate multiple statistics) when studying new or existing ideas. This leads to huge amounts of data which researchers often study in a limited fashion on ad hoc basis. In contrast, if ML based analysis is performed on this data, we might be able to see the true potential of that data and its ability to help us discover new findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Challenges:</head><p>• The resources needed by these ML techniques when they are implemented in hardware can be significant. Specially, if these techniques need to be used for microarchitectural decisions at a finer granularity (at the program phase level), the hardware implementations become more critical. There is a need to develop more optimized hardware implementations of these algorithms to make them more feasible to be applied towards architectural problems/design. • ML algorithms primarily rely on huge data sets for learning. In comparison to other fields, there does not exist standardized data sets in architecture domain to be used by ML algorithms. • Although there exist a number of research works which take help of ML algorithms for computer architecture problems, the use of a particular algorithm is not justified by the authors mostly. As we observed in this paper, there are ML algorithms which naturally fit certain problems. We think it will be sensible to emphasize the use of appropriate algorithms for a given problem, which can help in ensuring the robustness of the proposed ideas. • ML methods have found more use in branch prediction like problems where the decision to be made is a binary decision (taken/not-taken) as opposed to the problems like memory prefetching where the output has to be a complete memory address (which increases the complexity of the ML model). • Data selection, cleaning, and pre-processing and then interpreting results is a challenge.</p><p>• Formulation of architecture problems as machine learning problem is a general challenge.</p><p>• As we pointed out in the opportunities sub-section that ML can help improving the accuracy of simulation tools, that opportunity comes with a challenge of building new tools to embed ML algorithms into simulation techniques. Specifically, the question of how to make the simulation tools talk to ML algorithms needs to be addressed. • There exist a big disparity between the time that ML algorithms take to make a prediction and the time that often microarchitectural structures take for an estimation (known as time scale gap). This needs to be solved before practical ML based microarchitectural solutions can be designed. An example where this problem has been addressed is the design of branch predictors i.e. the proposal of analog versions of neural network based branch predictors (for example: <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b72">73]</ref>) to improve latency of prediction. • It is also not clear how (and if) the ML based architecture design will scale. Traditionally, in computer architecture, many microarchitectural/architectural techniques work the same way irrespective of the workloads used or size of the processor. For heuristics based designs, it is easier for humans to argue about their scalability. However, this might not be true for ML based design (specially if it is not very interpretable). Thus, the question arises if the optimal machine learning based design for a specific problem at hand will work if the workloads change significantly or the hardware resources scale to a bigger budget. • As ML is finding more and more uses in the real world, many security challenges have been raised. Before computer architecture design can be fully influenced by ML, the attacks (like adversarial ML attacks) possible on ML techniques need to be addressed in the context of hardware design. Moreover, these machine learning implementations might lead to new sidechannels and computer architects will need to be cognizant of this to avoid major security vulnerabilities in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A AN OVERVIEW OF ML TECHNIQUES</head><p>Machine Learning (ML) refers to the process in which computers learn to make decisions based on the given data set without being explicitly programmed to do so <ref type="bibr" target="#b7">[8]</ref>. There are numerous ML algorithms that can be grouped into three categories: supervised learning algorithms, unsupervised learning algorithms and other types of algorithms. Learning algorithms are usually trained on a set of samples called a "training set". A different set of data is usually used to test the accuracy of decisions made by the learning algorithm known as "test set". This section briefly discusses different ML techniques that are used by the research surveyed in this paper. Readers who wish to have detailed explanation of the discussed methodologies in this section can refer to <ref type="bibr" target="#b63">[64]</ref>. A common textbook classification of ML algorithms is shown in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Supervised Learning</head><p>The process of training machine learning models given a set of samples of input vectors with the corresponding output/target vectors is called Supervised learning <ref type="bibr" target="#b31">[32]</ref>. The trained model can then be used to predict ouput vectors given a new set of input vectors. The output/target vectors are also known as labels and input data as labelled data.</p><p>Machine learning models can operate on different types of data. If the target labels in a data set can be a set of discrete classes/categories, the modeling task is referred as classification task. On the other hand if the target labels in a data set are continous variables, the modeling task is called a</p><formula xml:id="formula_0">K=8 ? X X X X X X Y Y Y Y Y Y Y X X X Fig. 4. KNN X X X X X X Y Y Y Y Y Y Y Y X X X X Y Y Y Fig. 5. SVM</formula><p>make new predictions for test data. KNN has also been used for regression problems <ref type="bibr" target="#b31">[32]</ref>. More information on KNN can be found in <ref type="bibr" target="#b34">[35]</ref>.</p><p>A.1.8 Support Vector Machines. Support Vector Machine (SVM) is a non-probabilistic ML model that has been used for both classification and regression problems, but mostly for classification.</p><p>Assuming that we have n number of features or input variables, SVM classifier plots given data points in an n-dimensional space. The algorithm finds hyper-planes in this n-dimensional space which can distinguish given classes with the largest margin. This hyperplane can be a linear/nonlinear function of input variables resulting into linear/non-linear SVM. SVM based classifiers use "a few training points" (also known as support vectors) while classifying new data points. Theoretically, support vectors are the most difficult points to classify since they are the nearest to the "decision surface". Maximum margin between these support vectors ensures that the distance between classes is maximized. Figure <ref type="figure">5</ref> shows a linear SVM classifier, distinguishing between two classes of data. SVMs are effective for high number of features and are memory-efficient. However, these models are not very interpretable <ref type="bibr" target="#b88">[89]</ref>. More information on SVMs can be found in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b63">64]</ref> A.1.9 Decision Trees. Decision trees summarize the relationship between different values of given features (represented by branches) and the conlusions/results about the output variables's value (represnted by leaves) <ref type="bibr" target="#b1">[2]</ref>. Decision trees can be used for both classification and regression problems. Figure <ref type="figure">6</ref> shows an example of a decision tree used for a classification problem; is a person ready to run a marathon? Decision trees are easy to interpret because of their pictorial nature. Generally decision trees are considered to be less accuarate compared to other advanced ML techniques. Interested readers can obtain more information about decision trees in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>A.1.10 Random Forests. Random forests form a large number of decision trees based on the training data and then uses the combined behavior of those trees to make any predictions <ref type="bibr" target="#b2">[3]</ref>. Only a subset of the entire training data is used to train individual trees. The subset of the training data and input features are selected randomly. Random forests can be used for both classification and regression. Random forests can solve the decision trees' problem of overfitting as they use an ensemble of decision trees.</p><p>Random forests are known for their higher accuracy, however they are not very interpretable <ref type="bibr" target="#b27">[28]</ref>. More information on random forests can be found in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b63">64]</ref>. network connect to each other using weighted edges. Neural networks are composed of three different layers: input layer (neurons in this layer get input from outer world), output layer (neurons in this layer generate predicted output response) and hidden layer(this layer performs useful transformations on input data and feed it to the output layer). There can be any number of hidden layers in a neural network. Figure <ref type="figure">7</ref> shows an example of a basic neural network, with only one hidden layer. This type of neural networks in which connections between layers move in forward direction only are known as feed-forward neural networks. Neural networks can have variable number of nodes and hidden layers. Different architectures of neural networks like perceptron, multilayer perceptron, recurrent neural network and boltzman machine network have been proposed. Neural networks learn by configuring the weights of edges and thresholds of activation functions iteratively to achieve the required results. There are two main learning algorithms to train neural networks: Gradient Descent: In this learning algorithm the weights of the NN are modified to minimize the difference between the actual outputs and the predicted outputs. Back propagation: In this learning algorithm the dissimilarity of the predicted and actual outputs is computed at the output layer and transferred to the input layer using hidden layer.</p><p>Neural networks find their use in both regression and classification problems. Neural networks are known for their good accuracy. More information on neural networks can be found in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b107">108]</ref> A.2 Un-Supervised Learning</p><p>The process of training machine learning models given only sets of input vectors and no output/target vectors is called Un-supervised learning <ref type="bibr" target="#b31">[32]</ref>. In other words, the training data is not labelled in case of Unsupervised learning. This type of learning is used to attain a better understanding of the given data by identifying existing patterns in the data.</p><p>A.2.1 Prinicpal Component Analysis. Principal component analysis (PCA) is largely used to reduce the number of dimensions (dimensionality reduction). It allows to understand exisintg patterns in given data by using small number of "representative" variables (called principal components) from a larger number of given "correlated" variables. Basically, PCA transforms the given data to another space such that there is maximum variance among variables in new space. The components which have least variance are discarded as they do not contain much information. More information on PCA can be found in <ref type="bibr" target="#b76">[77]</ref>.</p><p>A.2.2 Clustering. Another popular un-supervised learning technique is clustering i.e. finding groups of data in given unlabelled data-set. Following are two main types of clustering algorithms: K-Means Clustering: This algorithm starts with specification of required clusters K. Random data points are chosen as centoids of K clusters. The algorithm then identifies the points nearest to the centroid by using some distance measure, calculates mean of all points and assign a new centre to the cluster. The algorithm keeps on identifying closest points and calculate new centres untill a convergence condition is met. More information on K-means clustering can be found in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>Heirarchical clustering: Heirarchical clustering does not require the specification of total number of clusters in advance. The algorithm builds a data binary tree that repeatedly combines similar data points. The mostly used form of heirachical clustering known as agglomerative clustering is performed as follows: Each single data point forms its own group. Two closest groups are combined iteratively, untill the time when all data points are contained in a single group/cluster.</p><p>More information on heirarchical clustering can be found in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>A.2.3 Anomaly Detection. Anamoly detection algorithms is a class of algorithms that identify data points with abnormal behavior in given data set. Many of the anamoly detection algorithms are unsupervised but they can be supervised and semisupervised as well <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Other Types of ML Techniques</head><p>A.3.1 Reinforcement Learning. Reinforcement learning is based on an "agent" attached/associated to an "environement" <ref type="bibr" target="#b80">[81]</ref>. Agent decides to take certain actions depending on the state of the environment. The changes in the state because of agent's actions are fed back to the agent using reinforcement signal. Depending on the consequences of earlier decision agent receives a reward or a penalty. Agent should always take actions that try to increase the overall reward <ref type="bibr" target="#b80">[81]</ref>, which it tries to learn by trial and error using different algorithms. Q-learning algorithm is one of the largely used Reinforcement Learning (RL) algorithms. Interested readers can read more about reinforcement learning in <ref type="bibr" target="#b168">[169]</ref>.</p><p>A.3.2 Heuristic Algorithms. These type of ML algorithms use rules or heuristics while making decisions. They work well if the solution to a problem is expensive <ref type="bibr" target="#b92">[93]</ref>. One of their famous types is Genetic Alorithms. These algorithms take their inspiration from nature. They use a process similar to evolution to find the best working solution to a problem. More information on genetic algorithms can be found in <ref type="bibr" target="#b163">[164]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Techniques Related to Model Assessment</head><p>There are various techniques used to assess a machine learning model. Following is a description of few of them: A.4.1 Validation Set Approach. This method involves partitioning the given data into two halves: training and validation/test sets. The training data set us used to train the Machine learning model. The trained model is then used for prediction of outputs using the available test set data. The error in test data is usually estimated through MSE (mean squared error). A potential problem with this approach is that the test error can have high variations <ref type="bibr" target="#b63">[64]</ref>.</p><p>A.4.2 Leave-One-Out Cross-Validation. Consider that there are n total observations, then in LOOCV one observation is excluded and the model is trained on the n-1 observations. The excluded observation is used to caluclate test error. All the observations are included in the test set one by one and as a result n test errors are calculated by using training on other observations. LOOCV has less bias and randomness compared to validation set approach <ref type="bibr" target="#b63">[64]</ref>. This method can take long time to process if the number of observations is large.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Number of works done in chronological order</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><figDesc>Fig. 6. Decision Tree Fig. 7. Neural Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Five Tribes of ML (taken from<ref type="bibr" target="#b38">[39]</ref>)</figDesc><table><row><cell>Tribe</cell><cell>Origins</cell><cell>Master Algorithms</cell></row><row><cell>Symbolists</cell><cell>Logic, philosophy</cell><cell>Inverse deduction</cell></row><row><cell cols="2">Connectionists Neuroscience</cell><cell>Backpropagation</cell></row><row><cell cols="3">Evolutionaries Evolutionary biology Genetic programming</cell></row><row><cell>Bayesians</cell><cell>Statistics</cell><cell>Probabilistic infernce</cell></row><row><cell>Analogizers</cell><cell>Psychology</cell><cell>Kernel machines</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>ML Tribes and Examples of Their Use in Computer Architecture</figDesc><table><row><cell>Tribe</cell><cell>Targeted Problem</cell><cell>Example rithms Algo-</cell><cell>Example Works</cell></row><row><cell></cell><cell></cell><cell>Inverse</cell><cell></cell></row><row><cell></cell><cell>Knowledge</cell><cell></cell><cell></cell></row><row><cell>Symbolists</cell><cell>Composi-</cell><cell>deduction, Decision</cell><cell></cell></row><row><cell></cell><cell>tion</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>trees</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Summary of the Literature Survey on Machine Learning in Computer Architecture</figDesc><table><row><cell>Technique Microarchitecture Power/Perform. Estimation Thread Scheduling Design Space Exploration Energy Improvement Instruction Scheduling Hardware Security</cell><cell>Decision Trees [45, 105, 130, 144] [29, 43, 79] [56, 143] [122, 154, 200] [112, 124] [128, 129]</cell><cell>Genetic Algo. [44] [59] [145, 167] [34, 95]</cell><cell>K-Nearest [105] Neighbor [12, 29] [56]</cell><cell>K-Means [79, 159, 189] [111, 184]</cell><cell>Linear/Non-[5, 10, 20, 43, 99,</cell><cell>Linear 133, 134, 136, 142, [120, 143] [66, 96] [191] [128, 129]</cell><cell>Regression 146, 181, 198, 199]</cell><cell>Logistic [105, 144] Regression [29] [33, 109] [86-88, 129, 135, 137],</cell><cell>Bayes Theory [130] [85, 145] [80] [137]</cell><cell>[14, 15, 27, 37, 40,</cell><cell>51, 69, 76, 84, 107, [22, 60, 83, 85, [31, 86, 88,</cell><cell>Neural Network 113, 114, 140, 158, [29, 43, 48, 65, 117, 134, 147, 189] [17, 53, 56, 102, 131, 186, 187] 104, 106, 109, [188] [62] 119, 129, 135,</cell><cell>161, 162, 171-173, 154] 137]</cell><cell>185, 193, 195, 196]</cell><cell>PCA [59]</cell><cell>Random [11, 115, 134, 136] Forest [143] [154]</cell><cell>Reinforcment [61, 91, 138, 152, [47] Learning 183, 197] [46, 78, 194] [116]</cell><cell>SVM [36, 105] [12, 43] [56, 186, 187] [154] [128, 129, 137]</cell><cell>Others [71, 130, 178] [43, 181] [120] [145] [101] [4] [137]</cell><cell>Thread Scheduling:</cell><cell>management of shared hardware resources, thread scheduling in heterogeneous systems. Design Space Exploration: design space exploration of</cell><cell>single or multi-cores, gpus, accelerators, network-on-chips. Energy Improvements: energy aware thread assignment or micro-architecture design,</cell><cell>dynamic voltage and frequency scaling. Hardware Security: micro-architectural Instruction Scheduling: static/dynamic instruction scheduling.</cell><cell>side-channel attack detection and evasion.</cell></row></table><note><p>Note:</p><p>The examples of sub-problems for each column in this table are following: Microarchitecture: branch prediction, cache replacement, cache reuse prediction, value prediction, memory scheduling, prefetching, network-on-chip design.</p><p>Power/Perform. Estimation: power/performance estimation for single or multi-cores, gpus, cross-platform (e.g. one ISA to other or cpu to gpu), simulation augmentation.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>See Appendix A for a detailed summary of many ML techniques referred in this paper.Authors' addresses: Ayaz Akram, University of California, Davis, yazakram@ucdavis.edu; Jason Lowe-Power, University of California, Davis, jlowepower@ucdavis.edu.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1"><p>Regression techniques are not a part of the original ML tribes taxonomy of Pedro Domingos<ref type="bibr" target="#b38">[39]</ref> </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">CONCLUSION</head><p>This paper surveys a wide set of representative work in computer architecture which utilize ML techniques. We identify the fundamental properties of different classes of <rs type="person">Machine Learning</rs> and their relation with the problems in the domain of computer architecture. It is observed that the usage of ML techniques is on the rise and future computer architecture research can further leverage the exciting developments from the field of <rs type="person">Machine Learning. However</rs>, a number of challenges need to be addressed to fully exploit the ML potential for computer architecture research.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Learning Un-Supervised Learning</head><p>Other Categories</p><p>Fig. <ref type="figure">3</ref>. Classification of common ML algorithms regression task <ref type="bibr" target="#b63">[64]</ref>. In other words, in classfication the built models predict qualitative response and in regression the built models predict quantitative response. Many of the ML algorithms can be applied to both regression and classification tasks, while there are others that can only be used for classification or regression tasks.</p><p>A.1.1 Linear Regression. Linear regression is used to relate different variables and approximates the effect of changing a variable on other variables. Assume that there exists a predictor variable X (also known as independent/input variable) and a corresponding quantitative response Y (also known as dependent/output variable). Linear regression, assuming that there exists a linear relation between dependent and independent variables, generates a model as shown in (1) <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b152">153]</ref>.</p><p>Here 𝛼 0 and 𝛼 1 are regression coefficients unknown before training the model using given input data.</p><p>A.1.2 Multiple Linear Regression. In case the ouput variable Y is dependent on more than one independent variables or predictors, the linear regression equation can be extended to include all those predictors. Assuming there are p independent variables the multiple linear regression model would be given as in <ref type="bibr" target="#b1">(2)</ref>  <ref type="bibr" target="#b63">[64]</ref>.</p><p>Here 𝛼 0 to 𝛼 𝑚 are regression coefficients unknown before training the model using given input/output data.</p><p>A.1.3 Polynomial Regression. If there exists a non-linear relation between response and predictor variables, polynomials of predictor variables can be added in the regression model resulting into Polynomial Regression. For example if a quadratic relation exists between input X and output Y the regression model would become as in (3) <ref type="bibr" target="#b63">[64]</ref>.</p><p>A.1.4 Logistic Regression. Logistic regression algorithm finds its use purely for classification problems. In case of logistic regression the built model predicts the probability that the response Y would be member of particular class. Given an input/independent variable X, logistic regression model can be represented by the logistic function of (4) <ref type="bibr" target="#b63">[64]</ref>.</p><p>This relation can also be extended to multiple logistic regression in case of more than one independent variables as in the case of multiple linear regression.</p><p>A.1.5 Piecewise Polynomial Regression. Piecewise polynomial regression tries to avoid use of high degree polynomials. Instead it uses models built by low-degree polynomials, each corresponding to a different region of the variable X. As an example, "piecewise cubic polynomial" will use cubic regression models shown in ( <ref type="formula">5</ref>) and ( <ref type="formula">6</ref>) for different ranges of x. The points of changes in values of coefficients are known as knots. The given example in ( <ref type="formula">5</ref>) and ( <ref type="formula">6</ref>) has a single knot at point c.</p><p>A.1.6 Naive Bayes'. Bayes' theorem is used significantly to compute conditional probability. Mathematical form of bayes' theorem is given in ( <ref type="formula">7</ref>) <ref type="bibr" target="#b174">[175]</ref>.</p><p>Here Pr(h|e) represents the probability that the hypothesis h will be true if some event e exists (posterior probability). Pr(e|h) represents the probability that the event e would occur in presence of an hypothesis h. Pr(h) is the proability of hypothesis by itself without the event (prior probability) and Pr(e) is the probability of the event e <ref type="bibr" target="#b174">[175]</ref>. Bayes' theorem is largely used for classification purposes in ML. Bayes' theorem based classifiers that assume that the given inputs/features are independent of each other are known as Naive Bayes' Classifiers. Usually, a small number of data points are sufficient to train Naive Bayes' classifiers. They are famous for their scalability and efficiency. A more detailed review of Bayes' classifiers can be found in <ref type="bibr" target="#b125">[126]</ref>.</p><p>A.1.7 K-Nearest Neighbors. K-Nearest Neighbors (KNN) is used to classify unseen data points by observing K already-classified data points that are closest to the unseen data point. Different distance measures can caclculate the distance of new/unseen data point from neighboring data points such as Euclidean, Manhattan, Hamming etc. A mathematical representation of KNN is given in <ref type="bibr" target="#b7">(8)</ref>. Assuming that there is a test observation that needs to be classified, and K closest points are represented by 𝑁 0 , then to find out if test point belongs to class j KNN relies on <ref type="bibr" target="#b7">(8)</ref>. In <ref type="bibr" target="#b7">(8)</ref>, a fraction of points in 𝑁 0 belonging to class j gives the conditional probability of class j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1/𝐾 ∑︁</head><p>Figure <ref type="figure">4</ref> shows an example of how KNN algorithm works. For K=8, the data point in question will be classified as 'X' as most of its neighbors belong to category 'X'. KNN is specifically useful for ML problems where "underlying joint distribution of the result and observation" is unknown. KNN does not scale well with larger data sets since it makes use of the complete training data to A.4.3 k-fold Cross-Validation. In this apprach the available data is randomly divided into k partitions/groups of equal size. The machine learning model under consideration is trained using k-1 groups of data and then tested on kth group of data by calculating MSE. The aforementioned process is then replicated k times using new/different validation data sets. The overall error is estimated by calculating average of k MSE values. This method is less expensive comapred to LOOCV <ref type="bibr" target="#b63">[64]</ref>.</p><p>A.4.4 Bootstrapping (Bootstrap aggregating). Assuming that there is an n-sized training set called D, bagging is a process of creating m training sets 𝐷 𝑖 each of size n'. These samples (also called bootstrap samples) are taken from D "uniformly with replacement". This means some data points can be repeated in 𝐷 𝑖 . If n'=n and n is large, 𝐷 𝑖 will have nearly 63% of unique samples of D and the others will be duplicates <ref type="bibr" target="#b167">[168]</ref>. These m samples are used to train m ML models whose outputs are averaged to combine all of them. Bootstrapping can reduce the variance in results, but results can be more biased <ref type="bibr" target="#b63">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Techniques Related to Fitting of ML Models</head><p>There are various techniques used to train a ML model. Some of them are discussed below: A.5.1 Subset Selection. This involves using only a subset of the available predictors/inputs/features to fit the ML model <ref type="bibr" target="#b63">[64]</ref>.</p><p>A.5.2 Best subset selection. In this method, every combination of input features is used to train the ML model and the best combination is identified. This method is resource intensive <ref type="bibr" target="#b63">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.3 Step-wise selection.</head><p>Step-wise selection is more efficient way for subset selection compared to best subset selection. Forward step-wise selection starts with no predictors and keeps on adding predictors one by one untill all predictors are included in the model. At every step, the predictor that leads to greatest improvement in the model is selected. Backward step-wise selection starts with a ML model with all predictors/input varibales and removes the predictors with least benefit one by one <ref type="bibr" target="#b63">[64]</ref>. Often hybrid versions of these two models are used.</p><p>A.5.4 Shrinkage methods. To reduce variance in a regression model and avoid overfitting, often the model is trained with constraining or regularization of regression cofficient estimates. These constraints move the estimates close to zero. Such methods are known as shrinkage methods.</p><p>A.5.5 Ridge Regression. In ridge regression the "coefficient estimates" are the values that minimize the expression in (9) <ref type="bibr" target="#b63">[64]</ref>.</p><p>Here 𝜆 is a "tuning parameter", p is the number of features and N is the number of data points. Ridge regression (like least sqaures) tries to find coefficients that make "RSS small" which is accounted by first term in <ref type="bibr" target="#b8">(9)</ref>. The second term in ( <ref type="formula">9</ref>) is known as shrinkage penalty which controls the effect of two terms of equation "on the regression cofficient estimates". This is small when 𝛽 cofficients are close to zero <ref type="bibr" target="#b63">[64]</ref>.</p><p>A.5.6 Lasso Regression. Lasso regression is an alternative to ridge regression. It tries to find coefficients that minimize <ref type="bibr" target="#b9">(10)</ref>  <ref type="bibr" target="#b63">[64]</ref>. The difference in ridge and lasso regression is in the second term, where lasso regression has 𝛽 coefficients of ridge regression with absolute signs. Lasso regression not only punishes high values of 𝛽 like ridge regression, but it makes them zero if they seem irrelevant. This gives lasso regression its variable selection property and makes it more interpretable <ref type="bibr" target="#b63">[64]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Akram</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lowe-Power</forename></persName>
		</author>
		<ptr target="https://www.rulequest.com/cubist-win.html" />
		<imprint>
			<date type="published" when="1997">1995 1996 1997. 1998 1999 2000 2001 2002 2003. 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020. June-2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Decision tree learning</title>
		<ptr target="https://en.wikipedia.org/wiki/Decision_tree_learning" />
		<imprint>
			<date type="published" when="2018-04-04">2018. April 4. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Random Forest</orgName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Random_forest" />
		<imprint>
			<date type="published" when="2018-04-04">2018. April 4. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using machine learning to focus iterative optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Agakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cavazos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fursin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Code Generation and Optimization</title>
		<meeting>the International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Performance prediction for multi-threaded applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zahran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on AI-assisted Design for Architecture (AIDArc), held in conjunction with ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perceptron-based branch confidence estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akkary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koltur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Refaai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software, IEE Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="265" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meet the sherlock holmes&apos; of side channel leakage: A survey of cache sca detection techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mushtaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Bhatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lapotre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gogniat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="70836" to="70860" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Introduction to machine learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mixed-signal approximate computation: A neural predictor case study</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Amant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE micro</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="104" to="115" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-architecture performance prediction (xapp) using cpu code to predict gpu performance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lestourgeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microarchitecture (MICRO), 2015 48th Annual IEEE/ACM International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="725" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A static analysis-based cross-architecture performance prediction using machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ardalani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albarghouthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07840</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting gpu performance from cpu runs using machine learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Baldini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture and High Performance Computing (SBAC-PAD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
	<note>IEEE 26th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximizing cache performance under uncertainty</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture (HPCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="109" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pecc: Prediction-error correcting cache</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Janardhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on ML for Systems at NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptron-based prefetch filtering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The gem5 Simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Comp. Arch. News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coordinated management of multiple interacting resources in chip multiprocessors: A machine learning approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bitirgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 41st annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural confidence estimation for more accurate value prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on High Performance Computing</title>
		<meeting>the 12th international conference on High Performance Computing</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The perceptron: A model for brain functioning</title>
		<author>
			<persName><forename type="first">H.-D</forename><surname>Block</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">i. Reviews of Modern Physics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">123</biblScope>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Performance modelling of heterogeneous isa multicore architectures</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Boran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Meghwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">East-West Design &amp; Test Symposium (EWDTS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth annual workshop on Computational learning theory</title>
		<meeting>the fifth annual workshop on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding memory access patterns for prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on AI-assisted Design for Architecture (AIDArc), held in conjunction with ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical cluster analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Bridges</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological reports</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="851" to="854" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new organization for a perceptron-based branch predictor and its fpga implementation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cadenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Megson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLSI</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="305" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evidence-based static branch prediction using machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems (TOPLAS)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="222" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An empirical comparison of supervised learning algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using machine learning techniques to analyze the performance of concurrent kernel execution on gpus</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Clua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M D</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="528" to="540" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real time detection of cache-based side-channel attacks using hardware performance counters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chiappetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Savas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1162" to="1174" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">PATTERN RECOGNITION AND MACHINE LEARNING</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Christopher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pack &amp; cap: adaptive dvfs and thread packing under power caps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cochran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hankendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Coskun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual IEEE/ACM international symposium on microarchitecture</title>
		<meeting>the 44th annual IEEE/ACM international symposium on microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimizing for reduced code space using genetic algorithms</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Schielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Svms for improved branch prediction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gondree</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>UCDavis, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
	<note>ECS201A Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Block2vec: A deep learning strategy on mining block correlations in storage systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Processing Workshops (ICPPW), 2016 45th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="230" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved composite confidence mechanisms for a perceptron branch predictor</title>
		<author>
			<persName><forename type="first">V</forename><surname>Desmet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>De Bosschere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems Architecture</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="143" to="151" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The master algorithm: How the quest for the ultimate learning machine will remake our world</title>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Basic Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Kleio: A hybrid memory page scheduler with machine intelligence</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Doudali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurumurthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gavrilovska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing</title>
		<meeting>the 28th International Symposium on High-Performance Parallel and Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving cache management policies using dynamic reuse distances</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microarchitecture (MICRO), 2012 45th Annual IEEE/ACM International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="389" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two-level branch prediction using neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Quick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vintan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems Architecture</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="557" to="570" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the comparison of regression algorithms for computer architecture performance analysis of software applications</title>
		<author>
			<persName><forename type="first">O</forename><surname>Elmoustapha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kshitij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Statistical and Machine learning approaches applied to ARchitectures and compilaTion</title>
		<meeting>the First Workshop on Statistical and Machine learning approaches applied to ARchitectures and compilaTion</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A language for describing predictors and its application to automatic synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gloy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="304" to="314" />
			<date type="published" when="1997">1997</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic feature selection for hardware prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Givan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECE Technical Reports</title>
		<imprint>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic voltage and frequency scaling in nocs with supervised and reinforcement learning techniques</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Louri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hardware-level thread migration to reduce on-chip data movement via reinforcement learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Louri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shiflett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3638" to="3649" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cpu hardware classification and performance prediction using neural networks and statistical learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Foots</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence and Applications (IJAIA)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer series in statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adaptive information processing: An effective way to improve perceptron branch predictors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bit-level perceptron prediction for indirect branches</title>
		<author>
			<persName><forename type="first">E</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirbagher-Ajorpaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Using binary neural networks for hardware branch prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gaudet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Appropriate allocation of workloads on performance asymmetric multicore architectures via deep learning algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gomatheeshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Selvakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocessors and Microsystems</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102996</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Bias-free neural predictor. The 4th Championship Branch Prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
		<ptr target="http://www.jilp.org/cbp2014" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Big. little processing with arm cortex-a15 &amp; cortex-a7</title>
		<author>
			<persName><forename type="first">P</forename><surname>Greenhalgh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>ARM White paper</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A machine learning-based approach to estimate the cpu-burst time for processes in the computational grids</title>
		<author>
			<persName><forename type="first">T</forename><surname>Helmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Azani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bin-Obaidellah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence, Modelling and Simulation (AIMS), 2015 3rd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning multiple layers of representation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="428" to="434" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Combining local and global history hashing in perceptron branch prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIS 2007. 6th IEEE/ACIS International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
	<note>Computer and Information Science</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Performance prediction based on inherent program similarity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>De Bosschere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Architectures and Compilation Techniques (PACT), 2006 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="114" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Efficiently exploring architectural design spaces via predictive modeling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ïpek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>De Supinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schulz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Self-optimizing memory controllers: A reinforcement learning approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture, 2008. ISCA&apos;08. 35th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning automatic schedulers with projective reparameterization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction (rrip)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="60" to="71" />
			<date type="published" when="2010">2010</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">An introduction to statistical learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Cache miss rate predictability via neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karuvally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E B</forename><surname>Moss</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Automated regression-based gpu design space exploration</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><surname>Stargazer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Performance Analysis of Systems and Software (ISPASS), 2012 IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Multiperspective perceptron predictor. Championship Branch Prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jiménez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Snip: Scaled neural indirect predictor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fast path-based neural branch prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 36th annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">243</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Improved latency and accuracy for neural branch prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="197" to="218" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Piecewise linear branch prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="382" to="393" />
			<date type="published" when="2005">2005</date>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Generalizing neural branch prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">An optimized scaled neural branch predictor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Design (ICCD), 2011 IEEE 29th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Insertion and promotion for tree-based pseudolru last-level caches</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="284" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Perceptron learning for predicting the behavior of conditional branches</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. IJCNN&apos;01. International Joint Conference on</title>
		<meeting>IJCNN&apos;01. International Joint Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2122" to="2127" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Neural methods for dynamic branch prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="369" to="397" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Principal components in regression analysis</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Principal component analysis</title>
		<imprint>
			<biblScope unit="page" from="167" to="198" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Power-aware performance increase via core/uncore reinforcement control for chip-multiprocessors</title>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM/IEEE international symposium on Low power electronics and design</title>
		<meeting>the 2012 ACM/IEEE international symposium on Low power electronics and design</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Compute bottlenecks on the new 64-bit arm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cauble-Chantrenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peraza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carrington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Energy Efficient Supercomputing</title>
		<meeting>the 3rd International Workshop on Energy Efficient Supercomputing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Supervised learning based power management for multicore processors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1395" to="1408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Branch and confidence prediction using perceptrons</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kelley</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jafanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zahran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muzahid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13074</idno>
		<title level="m">The case for learning application behavior to improve hardware energy efficiency</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">On the finding proper cache prediction model using neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khakhaeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chantrapornchai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge and Smart Technology (KST), 2016 8th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="146" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Using predictivemodeling for cross-program design space exploration in multicore systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xekalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cavazos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cintra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Parallel Architecture and Compilation Techniques</title>
		<meeting>the 16th International Conference on Parallel Architecture and Compilation Techniques</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="327" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Rhmd: evasion-resilient hardware malware detectors</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Khasawneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Abu-Ghazaleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ponomarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 50th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="315" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Ensemble learning for low-level hardware-supported malware detection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Khasawneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ozsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Donovick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Abu-Ghazaleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ponomarev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Recent Advances in Intrusion Detection</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Ensemblehmd: Accurate hardware malware detectors with specialized ensemble classifiers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Khasawneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ozsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Donovick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Ghazaleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Ponomarev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Dependable and Secure Computing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Constructing support vector machine ensemble</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Je</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Bang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2757" to="2767" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Branch prediction using advanced neural methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep., Technical Report</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Energy efficiency optimization for stochastic edge inference using reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Autoscale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1082" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Mixed-signal neural network branch prediction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirabbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A survey of machine learning techniques applied to self-organizing cellular networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Klaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Onireti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2392" to="2431" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning vector quantization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Self-Organizing Maps</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="175" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Automatic feature generation for machine learning based optimizing compilation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Leather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO 2009. International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
	<note>Code Generation and Optimization</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Illustrative design space studies with microarchitectural regression models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA 2007. IEEE 13th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="340" to="351" />
		</imprint>
	</monogr>
	<note>High Performance Computer Architecture</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Composable performance regression for scalable multiprocessor models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><surname>Cpr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 41st annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="270" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The multiple branch predictor using perceptrons</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Transactions of The Korean Institute of Electrical Engineers</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="621" to="626" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Powertrain: A learning-based calibration of mcpat power models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gerstlauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Low Power Electronics and Design (ISLPED)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">There&apos;s plenty of room at the top: What will drive computer performance after moore&apos;s law?</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kuszmaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Lampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Schardl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="page">6495</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Processor design space exploration via statistical sampling and semi-supervised ensemble learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="25495" to="25505" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Machine learning based online performance prediction for runtime parallelization and task scheduling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>De Supinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Performance Analysis of Systems and Software</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Mcpat: an integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 42nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Characterizing on-chip traffic patterns in general-purpose gpus: A deep learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Penney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 37th International Conference on Computer Design (ICCD</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Machine learning-based prefetch optimization for data center applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><surname>-W</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis</title>
		<meeting>the Conference on High Performance Computing Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">56</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Design space exploration of memory controller placement in throughput processors with deep learning</title>
		<author>
			<persName><forename type="first">T.-R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="51" to="54" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hafdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Neural-hardware architecture search</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">An introduction to computing with neural nets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Assp magazine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A machine learning approach to accelerating dse of reconfigurable accelerator systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S B</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 33rd Symposium on Integrated Circuits and Systems Design</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lowe-Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Amslinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Armejach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Asmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03152</idno>
		<title level="m">The gem5 simulator: Version 20.0+</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Automatically diagnosing abnormal battery drain issues on smartphones</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName><surname>Edoctor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="57" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Learning heuristics for basic block instruction scheduling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Beek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Heuristics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="549" to="569" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">A study deep belief net for branch prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gui</forename></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Virtual address translation via learned page table indexes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Margaritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Predicting cloud performance for hpc applications: a user-oriented approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anghel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jongerius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dittmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing</title>
		<meeting>the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="524" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Scheduling straight-line code using reinforcement learning and rollouts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E B</forename><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="903" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4505" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Detecting invariant footprints of microarchitectural attacks with perceptron</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirbagher-Ajorpaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mohammadian-Koruyeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Abu-Ghazaleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><surname>Perspectron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1124" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Esp: A machine learning approach to predicting application interference</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomic Computing (ICAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">A survey of techniques for dynamic branch prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Applying statistical machine learning to multicore voltage &amp; frequency scaling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Melhem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Computing frontiers</title>
		<meeting>the 7th ACM international conference on Computing frontiers</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">The combined perceptron branch predictor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Monchiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Palermo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Parallel Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A machine learning approach to automatic production of compiler heuristics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Monsifrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Quiniou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIMSA</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2002">2002</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Multi-objective reconfigurable self-optimizing memory scheduler</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mukundan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><surname>Morse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2012 IEEE 18th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naive bayes classifiers. University of British Columbia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Introducing the graph 500</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CUG)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="45" to="74" />
			<date type="published" when="2010">2010</date>
			<publisher>Cray Users Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Nights-watch: A cache-based side-channel intrusion detector using hardware performance counters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mushtaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Bhatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lapotre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gogniat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Hardware and Architectural Support for Security and Privacy</title>
		<meeting>the 7th International Workshop on Hardware and Architectural Support for Security and Privacy</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Whisper: A tool for run-time detection of side-channel attacks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mushtaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bricq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Bhatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lapotre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gogniat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Benoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="83871" to="83900" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A machine learning methodology for cache recommendation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stuckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hübner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Applied Reconfigurable Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="311" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">A machine learning approach for performance prediction and scheduling on heterogeneous cpus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arkose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Markovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Unsal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristal</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture and High Performance Computing (SBAC-PAD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
	<note>th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Path traced perceptron branch predictor using local history for weight selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ninomiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2nd JILP Championship Branch Prediction Competition (CBP-2) in conjunction with The 39th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Gpu performance estimation using software rasterization and machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Brisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abousamra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shriver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems (TECS)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Hlspredict: Cross platform performance prediction for fpga high-level synthesis</title>
		<author>
			<persName><forename type="first">K</forename><surname>O'neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Derenard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Hardware-based malware detection using low-level architectural features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ozsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Khasawneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Donovick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gorelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Abu-Ghazaleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ponomarev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="3332" to="3344" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Hardware-assisted cross-generation prediction of gpus under design</title>
		<author>
			<persName><forename type="first">K</forename><surname>O'neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shriver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kishinevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1133" to="1146" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Analyzing hardware based malware detectors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Homayoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Design Automation Conference</title>
		<meeting>the 54th Annual Design Automation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Semantic locality and context-based prefetching using reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Etsion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture (ISCA), 2015 ACM/IEEE 42nd Annual International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="285" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Etsion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00478</idno>
		<title level="m">Towards memory prefetching with neural networks: Challenges and insights</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A neural network prefetcher for arbitrary memory access patterns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Etsion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Boa: The bayesian optimization algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pelikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cantú-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation</title>
		<meeting>the 1st Annual Conference on Genetic and Evolutionary Computation</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="525" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Power-performance modeling on asymmetric multi-cores</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pricopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Muthukaruppan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compilers, Architecture and Synthesis for Embedded Systems (CASES), 2013 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Deciphering predictive schedulers for heterogeneous-isa multicore architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prodromou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Programming Models and Applications for Multicores and Manycores</title>
		<meeting>the 10th International Workshop on Programming Models and Applications for Multicores and Manycores</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Maximizing hardware prefetch effectiveness with machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burtscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qasem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing and Communications (HPCC), 2015 IEEE 7th International Symposium on Cyberspace Safety and Security (CSS), 2015 IEEE 12th International Conferen on Embedded Software and Systems (ICESS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="383" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">A case for efficient accelerator design space exploration via bayesian optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Low Power Electronics and Design</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Empirical cpu power modelling and estimation in the gem5 simulator</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balsamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diestelhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Al-Hashimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Merrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Power and Timing Modeling, Optimization and Simulation (PATMOS), 2017 27th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Optimizing cpu simulator parameters with learned differentiable surrogates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<author>
			<persName><surname>Difftune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="442" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">System-level power estimation tool for embedded processor based platforms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rethinagiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Palomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ben Atitallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Niar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Unsal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kestelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools</title>
		<meeting>the 6th Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Evaluating branch prediction using two-level perceptron table</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V M</forename><surname>Ribas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goncalves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PDP 2006. 14th Euromicro International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Parallel, Distributed, and Network-Based Processing</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">A study on the use of performance counters to estimate power in microprocessors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Annamalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems II: Express Briefs</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="882" to="886" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Memristors for neural branch prediction: a case study in strict latency and write endurance challenges</title>
		<author>
			<persName><forename type="first">H</forename><surname>Saadeldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Strukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Computing Frontiers</title>
		<meeting>the ACM International Conference on Computing Frontiers</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Cads</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07776</idno>
		<title level="m">Core-aware dynamic scheduler for multicore memory controllers</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">Linear regression analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Seber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">329</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Machine learning based design space exploration for hybrid main-memory design</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Imam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Memory Systems</title>
		<meeting>the International Symposium on Memory Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="480" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Exploring perceptron-based register value prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Value Prediction and Value-Based Optimization Workshop, held in conjuction with ASPLOS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">A neural net branch predictor to reduce power</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sethuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">I</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Venkatanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Bushnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Held jointly with 6th International Conference on Embedded Systems., 20th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="679" to="684" />
		</imprint>
	</monogr>
	<note>VLSI Design</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Revisiting the perceptron predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRISA</title>
		<imprint>
			<biblScope unit="volume">1620</biblScope>
			<date type="published" when="2004-05">May (2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Tage-sc-l branch predictors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JILP-Championship Branch Prediction</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="45" to="57" />
			<date type="published" when="2002">2002</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Perceptron branch prediction with separated taken/not-taken weight tables</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lipasti</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Applying deep learning to the cache replacement problem</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="413" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">A neural hierarchical sequence model for irregular data prefetching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">SPEC CPU</title>
		<author>
			<persName><surname>Spec</surname></persName>
		</author>
		<ptr target="https://www.spec.org/cpu2006/" />
		<imprint>
			<date type="published" when="2006-08-05">2006. 2006. August 5. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Genetic algorithms: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Patnaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">computer</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="17" to="26" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">An idealistic neuro-ppm branch predictor</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Low-power, high-performance analog neural branch prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>St Amant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 41st annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="447" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">A parallel genetic algorithm for multiobjective microprocessor design</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICGA</title>
		<imprint>
			<biblScope unit="page" from="597" to="604" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Data Analytics using Open-Source Tools</title>
		<author>
			<persName><forename type="first">J</forename><surname>Strickland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lulu. com</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Merging path and gshare indexing in perceptron branch prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on architecture and code optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="280" to="300" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Post-silicon cpu adaptation made practical using machine learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Tarsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B R</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sebot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chinya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="14" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Improving branch prediction by modeling global history with convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Tarsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chinya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09889</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Perceptron learning for reuse prediction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microarchitecture (MICRO), 2016 49th Annual IEEE/ACM International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Hardware efficient piecewise linear branch predictor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Held jointly with 6th International Conference on Embedded Systems., 20th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="673" to="678" />
		</imprint>
	</monogr>
	<note>VLSI Design</note>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Sd-vbs: The san diego vision benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Venkata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Louie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Characterization</title>
		<imprint>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="2009">2009. 2009. 2009</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Perceptron learning driven coherence aware reuse prediction for last-level caches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Driving cache replacement with ml-based lecar</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vietri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rangaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th {USENIX} Workshop on Hot Topics in Storage and File Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>HotStorage 18</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Towards a high performance neural branch predictor</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Vintan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iridon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 1999. IJCNN&apos;99. International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="868" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">Run-time power estimation for mobile and embedded asymmetric multi-core cpus</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Merrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hashimi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Accurate and stable run-time power modeling for mobile and embedded cpus</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diestelhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Al-Hashimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Merrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="106" to="119" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<monogr>
		<title level="m" type="main">Data cache prefetching with perceptron learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00905</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Cure: A high-performance, low-power, and reliable network-on-chip design using reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Louri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2125" to="2138" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Reducing data movement energy via online data clustering and encoding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microarchitecture (MICRO), 2016 49th Annual IEEE/ACM International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Dynamic branch prediction using machine learning. ECS-201A, Fall</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Mapping parallelism to multi-cores: a machine learning based approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Sigplan notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Integrating profile-driven parallelism detection and machine-learning-based mapping</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tournavitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>'boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Up by their bootstraps: Online learning in artificial neural networks for cmp uncore power management</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Soteriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="308" to="319" />
		</imprint>
	</monogr>
	<note>IEEE 20th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Gpgpu performance and power estimation using machine learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Greathouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lyashevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2015 IEEE 21st International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="564" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Quantifying sources of error in mcpat and potential impacts on architectural studies</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2015 IEEE 21st International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="577" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Adaptive energy minimization of embedded heterogeneous systems using regression-based learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Shafik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Merrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Al-Hashimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Power and Timing Modeling, Optimization and Simulation (PATMOS), 2015 25th International Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Flush+ reload: A high resolution, low noise, l3 cache side-channel attack</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Falkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="719" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Neural acceleration for gpu throughput processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microarchitecture (MICRO), 2015 48th Annual IEEE/ACM International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="482" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Learning-based power management for multicore processors via idle period manipulation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1043" to="1055" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Experiences with ml-driven design: A noc case study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sethumurugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="637" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">A dynamic branch predictor based on parallel structure of srnn</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Yahya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="86230" to="86237" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">An energy-efficient network-on-chip design using reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Louri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Design Automation Conference</title>
		<meeting>the 56th Annual Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Accurate phase-level cross-platform power and performance estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gerstlauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Design Automation Conference</title>
		<meeting>the 53rd Annual Design Automation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Learning-based analytical cross-platform performance prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gerstlauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS), 2015 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Bbs: Micro-architecture benchmarking blockchain systems through machine learning and fuzzy set</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="411" to="423" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
