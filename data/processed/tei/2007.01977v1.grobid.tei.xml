<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lale: Consistent Automated Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-07-04">4 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Baudart</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Hirzel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kiran</forename><surname>Kate</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parikshit</forename><surname>Ram</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Avraham</forename><surname>Shinnar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lale: Consistent Automated Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-04">4 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">D686A062531C387A9428A11310A1BD71</idno>
					<idno type="arXiv">arXiv:2007.01977v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated machine learning makes it easier for data scientists to develop pipelines by searching over possible choices for hyperparameters, algorithms, and even pipeline topologies. Unfortunately, the syntax for automated machine learning tools is inconsistent with manual machine learning, with each other, and with error checks. Furthermore, few tools support advanced features such as topology search or higher-order operators. This paper introduces Lale, a library of high-level Python interfaces that simplifies and unifies automated machine learning in a consistent way.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine learning (ML) is widely used in various data science problems. There are many ML operators for data preprocessing (scaling, missing imputation, categorical encoding), feature extraction (principal component analysis, non-negative matrix factorization), and modeling (boosted trees, neural networks). A machine learning pipeline consists of one or more operators that take the input data through a series of transformations to finally generate predictions. Given the plethora of ML operators (for example, in the widely used scikit-learn library <ref type="bibr" target="#b4">[5]</ref>), the task of finding a good ML pipeline for the data at hand (which involves not only selecting operators but also appropriately configuring their hyperparameters) can be tedious and time consuming if done manually. This has led to a wider adoption of automated machine learning (AutoML), with the development of novel algorithms (such as SMAC <ref type="bibr" target="#b11">[12]</ref>, hyperopt <ref type="bibr" target="#b2">[3]</ref>, and subsequent recent work <ref type="bibr" target="#b15">[16]</ref>), open source libraries (auto-sklearn <ref type="bibr" target="#b9">[10]</ref>, hyperopt-sklearn <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, TPOT <ref type="bibr" target="#b16">[17]</ref>), and even commercial tools.</p><p>Scikit-learn provides a consistent programming model for manual ML <ref type="bibr" target="#b4">[5]</ref>, and various other tools (such as XGBoost <ref type="bibr" target="#b5">[6]</ref>, Light-GBM <ref type="bibr" target="#b13">[14]</ref>, and Tensorflow's tf.keras.wrappers.scikit_learn) maintain consistency with this model whenever appropriate. AutoML tools usually feature two pieces -(i) a way to define the search space corresponding to the pipeline topology, the operator choices in that topology, and their respective possible hyperparameter configurations, and (ii) an algorithm that explores this search space to optimize for predictive performance. Many of these also try to maintain consistency with the scikit-learn model but only when the user gives up all control and lets the tool completely automate the ML pipeline configuration, eschewing item (i) above. Users often want to retain some control over the automation, for example, to comply with domain-specific requirements by specifying operators or hyperparameter ranges to search over. While fine-grained control over the automation is possible (we will discuss examples), users need to manually configure the search space. This search space specification differs for each of the different AutoML tools and differs from the manual programming model.</p><p>We believe that proper abstractions are necessary to provide a consistent programming model across the entire spectrum of controlled automation. To this end, we introduce Lale, an open-source Python library 1 , built upon mathematically grounded abstractions of the elements of the pipeline (for example, topology, operator, hyperparameter configurations), and designed around scikit-learn <ref type="bibr" target="#b4">[5]</ref> and JSON Schema <ref type="bibr" target="#b17">[18]</ref>. Lale provides a programming interface for specifying pipelines and search spaces in a consistent manner while providing fine-grained control across the automation spectrum. The abstractions also allow us to provide a consistent programming interface for capabilities for which no such interface currently exists: (i) search spaces with higher-order operators (operators, such as ensembling, that have other operators as hyperparameters), and (ii) search spaces that include search for the pipeline topology via context-free grammars <ref type="bibr" target="#b6">[7]</ref>. The contributions of this paper are:</p><p>1. A pipeline specification syntax that is consistent across the automation spectrum and grounded in established technologies. 2. Automatic search space generation from pipelines and schemas for a consistent experience across existing AutoML tools. 3. Higher-order operators (for ensembles, batching, etc.) with automatic AutoML search space generation for nested pipelines. 4. A grammar syntax for pipeline topology search that is a natural extension of the pipeline syntax.</p><p>Overall, we hope Lale will make data scientists more productive at finding pipelines that are consistent with their requirements and yield high predictive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM STATEMENT</head><p>Consistency is a core problem for AutoML and existing libraries fall short on this front. This section uses concrete examples from popular (Auto-)ML libraries to present four shortcomings in existing systems. We strive to do so in a factual and constructive way.  1 estim = AutoSklearnClassifier( 2 include_preprocessors=['pca'], 3 include_estimators=['liblinear_svc', 'random_forest'], 4 time_left_for_this_task=1800, per_run_time_limit=120) 5 estim.fit(train_X, train_y) Figure 5: Example for auto-sklearn based on SMAC.</p><p>users progress across this spectrum, the state-of-the-art libraries require them to learn and use different syntax and concepts. Figure <ref type="figure">2</ref> shows an example from the no-automation end of the spectrum using scikit-learn <ref type="bibr" target="#b4">[5]</ref>. The code assembles a two-step pipeline of a PCA transformer and a LinearSVC classifier and manually sets their hyperparameters, for example, n_components=4.</p><p>The example in Figure <ref type="figure" target="#fig_0">3</ref> automates hyperparameter tuning and operator selection using GridSearchCV from scikit-learn. Lines 1-3 resemble Figure <ref type="figure">2</ref>. Lines 4-11 specify a search space, consisting of a list of two dictionaries. In the first dictionary, Line 7 specifies the list of values to search over for the n_components hyperparameter of the PCA operator; Line 8 specifies the classify step of the pipeline to be a LinearSVC operator; and Line 9 specifies the list of values to search over for the C hyperparameter of the LinearSVC operator. The second dictionary is similar, but specifies the RandomForestClassifier.</p><p>The syntax for a pipeline (Figure <ref type="figure">2</ref> Lines 1-3) differs from that for a search space (Figure <ref type="figure" target="#fig_0">3</ref> Lines 4-11). The mental model is that operators and hyperparameters are pre-specified and then the search space selectively overwrites them with different choices. To do so, the code uses strings to name steps and hyperparameters, with a double underscore ('__') name mangling convention to connect the two. Relying on strings for names can cause hard-to-detect mistakes <ref type="bibr" target="#b0">[1]</ref>. In contrast, using a single syntax for both manual and automated pipelines would make them more consistent and would obviate the need for mangled strings to link between the two. P 2 : Provide a consistent programming model across different Au-toML tools. Compared to GridSearchCV, Bayesian optimizers such as hyperopt-sklearn <ref type="bibr" target="#b14">[15]</ref> and auto-sklearn <ref type="bibr" target="#b9">[10]</ref> speed up search using smarter search strategies. Unfortunately, each of these AutoML tools comes with its own syntax and concepts. Figure <ref type="figure">4</ref> shows an example using the hyperopt-sklearn <ref type="bibr" target="#b14">[15]</ref> wrapper for hyperopt <ref type="bibr" target="#b2">[3]</ref>. Line 1 specifies a discrete search space N with a logarithmic prior, a range from 2..8, and a quantization to multiples of 1. Line 2 specifies a continuous search space C with a logarithmic prior and a range from 1..1000. Line 4 sets the transform step of the pipeline to pca with hyperparameter n_components=N. Lines 5-7 set the classify step to a choice between linear_svc and random_forest.</p><p>Figure <ref type="figure">5</ref> shows the same example using auto-sklearn <ref type="bibr" target="#b9">[10]</ref>. While power users can use the ConfigurationSpace used by SMAC <ref type="bibr" target="#b11">[12]</ref> to adjust search spaces for individual hyperparameters, we elide this for brevity. Line 2 sets the preprocessor to 'pca' and Line 3 sets the classifier to a choice of 'linear_svc' or 'random_forest'.</p><p>The syntaxes for the three AutoML tools in Figures <ref type="figure" target="#fig_0">3</ref>, <ref type="figure">4</ref>, and 5 differ. There are three ways to refer to the same operator: PCA, pca(..), and 'pca'. There are three ways to specify an operator choice: a list of dictionaries, hp.choice, and a list of strings. The mental model varies from overwriting to nested configuration to string-based configuration. Users must learn new syntax and concepts for each tool and must rewrite code to switch tools. Moreover, as we consider more sophisticated pipelines (beyond the simple two-step one presented in the example), the search space specifications get even more convoluted and diverse between these existing specification schemes. A unified syntax would make these tools more consistent, easier to learn, and easier to switch. Furthermore, this syntax should unify not just AutoML tools but also be consistent with the manual end of the spectrum (P 1 ). More specifically, given that scikit-learn sets the de-facto standard for manual ML, the syntax should be scikit-learn compatible. P 3 : Support topology search and higher-order operators in Au-toML tools. The tools previously discussed search operators and hyperparameters but do not optimize the topology of the pipeline itself. There are some tools that do, including TPOT <ref type="bibr" target="#b16">[17]</ref>, Recipe <ref type="bibr" target="#b7">[8]</ref>, and AlphaD3M <ref type="bibr" target="#b8">[9]</ref>. Unfortunately, their methods for specifying the search space are inconsistent with manual machine learning and established tools. TPOT does not allow the user to specify the search space for pipeline topologies (the user can specify the set of operators and can fix the pipeline topology, disabling the topology search). Recipe and AlphaD3M use context-free grammars to specify the search space for the topology, but in a manner inconsistent with each other or with other levels of automation.</p><p>Some transformers (e.g. RFE in Figure <ref type="figure" target="#fig_3">6</ref> Line 7) and estimators (e.g. AdaBoostClassifier) are higher-order: they take other operators as arguments. Using the AutoML tools discussed so far to search inside their nested operators is not straightforward. The aforementioned TPOT, Recipe, and AlphaD3M do not handle higher-order operators in their search for pipeline topology.</p><p>A unified syntax for topology search and higher-order operators that is a natural extension of the syntax for manual machine learning, algorithm selection, and hyperparameter tuning would make AutoML more expressive while keeping it consistent. P 4 : Check for invalid configurations early and prune them out of search spaces. Even if the search for each hyperparameter uses a valid range in isolation, their combination can violate side constraints. Worse, these errors may be detected late, wasting time.</p><p>Figure <ref type="figure" target="#fig_3">6</ref> shows a misconfigured pipeline: the hyperparameters for LR in Line 7 are valid in isolation but invalid in combination. Unfortunately, this is not detected when the pipeline is created on Line 7. Instead, when Line 10 tries to fit the pipeline, it first fits the first step of the pipeline (see RFE in Line 6). Only then does it try to fit the LR and detect the mistake. This wastes 10 minutes (Line 15).</p><p>In contrast, a declarative specification of the side constraints would both speed up this manual example and enable AutoML search space generators to prune cases that violate constraints, thus speeding up the automated case too. Furthermore, in some situations, invalid configurations cause more harm than just wasted time, leading optimizers astray (Section 6). It is possible (with varying levels of difficulty) to incorporate these side constraints with the search space specification schemes used by the tools discussed earlier, but they each have inconsistent methods for doing this. Moreover, the complexity of these specifications make them error prone. Additionally, while these side constraints help the optimizer, they do not directly help detect misconfigurations (as in Figure <ref type="figure" target="#fig_3">6</ref>). Custom validators would need to be written for each tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROGRAMMING ABSTRACTIONS</head><p>This section shows Lale's abstractions for consistent AutoML, addressing the problem statements P 1 ∧ P 2 ∧ P 3 ∧ P 4 from Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Abstractions for Declarative AutoML</head><p>An individual operator is a data science algorithm (aka. a primitive or a model), which may be a transformer or an estimator such as classifier or a regressor. Individual operators are modular building blocks from libraries such as scikit-learn. Figure <ref type="figure" target="#fig_1">1</ref> Lines 2-6 contain several examples, e.g., import LinearSVC as SVM. Mathematically, Lale views an individual operator as a function of the form</p><formula xml:id="formula_0">indivOp : θ hyperparams → D fit → D in → D out</formula><p>This view uses currying: it views an operator as a sequence of functions each with a single argument and returning the next in the sequence. An individual operator (e.g., SVM) is a function from hyperparameters θ hyperparams (e.g., dual=False) to a function from training data D fit (e.g., train_X, train_y) to a function from input data D in (e.g., test_X) to output data D out (e.g., predicted in Figure <ref type="figure" target="#fig_1">1</ref> Line 17). In the beginning, all arguments are latent, and each step in the sequence captures the next argument as given. The scikitlearn terminology for the three curried sub-functions is init, fit, and predict. Viewing operators as mathematical functions avoids complications arising from in-place mutation. It lets us conceptualize bindings as lifecycle: each bound, or captured, argument unlocks the functionality of the next state in the lifecycle.</p><p>A pipeline is a directed acyclic graph (DAG) of operators and a pipeline is itself also an operator. Since a pipeline contains operators and is an operator, it is highly composable. Furthermore, viewing both individual operators and pipelines as special cases of operators makes the concepts more consistent. An example is make_pipeline(make_union(PCA, RFC), SVM), which is equivalent to</p><formula xml:id="formula_1">((PCA &amp; RFC) &gt;&gt; ConcatFeatures &gt;&gt; SVM).</formula><p>Here, &amp; is the and combinator and &gt;&gt; is the pipe combinator. Combinators make edges more explicit and code more concise. An expression x &amp; y composes x and y without introducing additional edges. An expression x &gt;&gt; y introduces edges from all sinks of subgraph x to all sources of y. Mathematically, Lale views a pipeline as a function of the form</p><formula xml:id="formula_2">pipeline : θ topology → θ hyperparams → D fit → D in → D out</formula><p>This uses currying just like individual operators, plus an additional θ topology at the start to capture the steps and edges. A pipeline is trainable if both θ topology and θ hyperparams are given, i.e., the hyperparameters of all steps have been captured. To fit a trainable pipeline, iterate over the steps in a topological order induced by the edges. For each step s, let D s fit be the training data for the step, which is either the pipeline's training data if s is a source or the predecessors' output data D s fit = [D p out ] p ∈preds(s) otherwise. Then, recalling that s is a curried function, calculate s trained = s(D s fit ) and D s out = s trained (D s fit ). The trained pipeline substitutes trained steps for trainable steps in θ topology . To make predictions with a trained pipeline, simply interpret &gt;&gt; as function composition •.</p><p>An operator choice is an exclusive disjunction of operators and is itself also an operator. Operator choice specifies algorithm selection, and by being an operator, addresses problem P 1 from Section 2. An example is (SVM | RFC), where | is the or combinator. Mathematically, Lale views an operator choice as a function of the form opChoice :</p><formula xml:id="formula_3">θ steps → θ hyperparams → D fit → D in → D out</formula><p>This again uses currying. Argument θ steps is the list of operators to choose from. The θ hyperparams of an operator choice consists of an indicator for which of its steps is being chosen, along with the hyperparameters for that chosen step. Once θ hyperparams is captured, the operator choice is equivalent to just the chosen step, as shown in the visualization after Figure <ref type="figure" target="#fig_1">1</ref> Line 20.</p><p>The combined schema of an operator specifies the valid values along with search guidance for its latent arguments. It addresses problem P 4 from Section 2, supporting automated search with a pruned search space and early error checking all from the same single source of truth. Consider the pipeline PCA &gt;&gt; (J48 | LR), where PCA and LR are the principal component analysis and logistic regression from scikit-learn and J48 is a decision tree with pruning from Weka <ref type="bibr" target="#b10">[11]</ref>. These operators have many hyperparameters and constraints and Lale handles all of them. For didactic purposes, this section discusses only a representative subset. Figure <ref type="figure" target="#fig_5">7</ref> shows the JSON Schema <ref type="bibr" target="#b17">[18]</ref> specification of that subset. The open-source Lale library includes JSON schemas for many operators, some handwritten and others auto-generated <ref type="bibr" target="#b1">[2]</ref>. The number of components for PCA is given by N , which can be a continuous value in (0..1) or the categorical value mle. The prior distribution helps AutoML tools search faster. J48 has a categorical hyperparameter R to enable reduced error pruning and a continuous confidence threshold C for pruning. Mathematically, we denote allOf as ∧, anyOf as ∨, and not as ¬.  ¬(R = true) ∨ (C = 0.25). LR has two categorical hyperparameters S (solver) and P (penalty), with a constraint that solvers sag and lbfgs only support penalty l2, which we already encountered in Figure <ref type="figure" target="#fig_3">6</ref>. Going forward, we will denote a JSON Schema object with properties as dict{} and a JSON Schema enum as [ ]. Eliding priors, this means Figure <ref type="figure" target="#fig_5">7</ref>  To auto-configure an operator means to automatically capture θ hyperparams , which involves jointly selecting algorithms (for operator choices) and tuning hyperparameters (for individual operators). We saw examples for doing this with scikit-learn's Grid-SearchCV <ref type="bibr" target="#b4">[5]</ref>, hyperopt-sklearn <ref type="bibr" target="#b14">[15]</ref>, and auto-sklearn <ref type="bibr" target="#b9">[10]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Abstractions for Controlled AutoML</head><p>AutoML users rarely want to completely surrender all decisions to the automation. Instead, users typically want to control certain</p><p>1 from xgboost import XGBRegressor as Forest 2 import lale.schemas as schemas 3 lale.wrap_imported_operators() 4 Grove = Forest.customize_schema( 5 n_estimators=schemas.Int(min=2, max=6), 6 booster=schemas.Enum(['gbtree'])) decisions based on their problem statement and expertise. This section discusses how Lale supports such controlled AutoML.</p><p>The lifecycle state of an operator is the number of curried arguments it has already captured that determines the functionality it supports. To enable controlled AutoML, Lale pipelines can mix operators with different states. The visualizations in Figure <ref type="figure" target="#fig_1">1</ref> indicate lifecycle states via colors. A planned operator, shown in dark blue , has captured only θ topology or θ steps , but θ hyperparams is still latent. Planned operators support auto_configure but not fit or predict. A trainable operator, shown in light blue , has also captured θ hyperparams , leaving D fit latent. Trainable operators support auto_configure and fit but not predict. A trained operator, shown in white, also captures D fit . Trained operators support auto_configure, fit, and predict. Later states subsume the functionality of earlier states, enabling user control over where automation is applied. The state of a pipeline is the least upper bound of the states of its steps.</p><p>Partially captured hyperparameters of an individual operator treat some of its hyperparameters as given while keeping the remaining ones latent. Specifying some hyperparameters by hand lets users control and hand-prune the search space while still searching other hyperparameters automatically. For example, Figure <ref type="figure" target="#fig_1">1</ref> Line 12 shows partially captured hyperparameters for SVM and RFC. The visualization after Line 13 reflects this in a tooltip for RFC. And the pretty-printed code after Line 20 shows that automation respected the given dual hyperparameter of SVM while capturing the latent C, penalty, and tol. Individual operators with partially captured hyperparameters are trainable: fit uses defaults for latents.</p><p>Freezing an operator turns future auto_configure or fit operations into an identity. PCA(n_components=4).freeze_trainable() &gt;&gt; SVM freezes all hyperparameters of PCA (using defaults for its latents), so auto_configure on this pipeline tunes only the hyperparameters of SVM. Similarly, on a trained operator, freeze_trained() freezes its learned coefficients, so any subsequent fit call will ignore its new D fit . Freezing part of a pipeline speeds up (Auto-)ML.</p><p>A custom schema derives a variant of an individual operator that differs only in its schema. Custom schemas can modify ranges or distributions for search. As an extreme case, users can attach custom schemas to non-Lale operators to enable hyperparameter tuning on them-the call to wrap_imported_operators in Figure <ref type="figure" target="#fig_1">1</ref> Line 8 implicitly does that. Figure <ref type="figure" target="#fig_8">8</ref> shows how to explicitly customize a schema. Line 2 imports helper functions for expressing JSON Schema in Python. XGBoost implements a forest of boosted trees, so to get a small forest (a Grove), Line 5 restricts the number of trees to <ref type="bibr">[2..6]</ref>. Line 6 restricts the booster to a constant, using a singleton enum. Afterwards, Grove is an individual operator like any other. Mathematically, we can view the ability to attach a schema to an   individual operator as extending its curried function on the left:</p><formula xml:id="formula_4">indivOp : θ schemas → θ hyperparams → D fit → D in → D out</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Abstractions for Expressive AutoML</head><p>A higher-order operator is an operator that takes another operator as an argument. Scikit-learn includes several higher-order operators including RFE, AdaBoostClassifier, and BaggingClassifier.</p><p>The nested operator is a hyperparameter of the higher-order operator and the nested operator can have hyperparameters of its own. Finding the best predictive performance requires AutoML to search both outside and inside higher-order operators. Figure <ref type="figure" target="#fig_10">9</ref> shows an example higher-order AdaBoostClassifier with a nested DecisionTreeClassifier. On the outside, AutoML should search the operator choice in Lines 2-3 and the hyperparameters of AdaBoost-Classifier such as n_estimators. On the inside, AutoML should tune the hyperparameters of DecisionTreeClassifier. Lale searches both jointly, helping solve problem P 3 from Section 2. The JSON schema of the base_estimator hyperparameter of AdaBoostClassifier is:</p><p>1 { description: "Base estimator from which the ensemble is built.",</p><p>2 anyOf: [ 3 {typeForOptimizer: "operator"}, 4 {enum: [null]}], 5 default: null}</p><p>A pipeline grammar is a context-free grammar that describes a possibly unbounded set of pipeline topologies. A grammar describes a search space for the θ topology and θ steps arguments needed to create planned pipelines and operator choices. Grammars formalize AutoML tools for topology search such as TPOT <ref type="bibr" target="#b16">[17]</ref>, Recipe <ref type="bibr" target="#b7">[8]</ref>, and AlphaD3M <ref type="bibr" target="#b8">[9]</ref> that capture θ topology and θ steps automatically. Lale provides a grammar syntax that is a natural extension of concepts described earlier, helping solve problem P 3 from Section 2.</p><p>Figure <ref type="figure" target="#fig_1">10</ref> shows a Lale grammar inspired by the AlphaD3M paper <ref type="bibr" target="#b8">[9]</ref>. It describes linear pipelines comprising zero or more data cleaning operators, followed by zero or more transformers, 1 g = Grammar()</p><p>2 3 g.start = g.process &gt;&gt; g.features &gt;&gt; g.model 4 g.process = g.process1 | ((g.process1 &amp; g.process) &gt;&gt; Concat) 5 g.features = g.feature1 \ 6 | (((g.feature1 | g.est) &amp; g.features) &gt;&gt; Concat) 7 g.model = g.est followed by exactly one estimator. This is implemented via recursive non-terminals: g.clean on Line 5 is a recursive definition, and so is g.tfm on Line 6, implementing a search space with sub-pipelines of unbounded length. While AlphaD3M uses reinforcement learning to search over this grammar, Figure <ref type="figure" target="#fig_1">10</ref> does something far less sophisticated. Line 13 unfolds the grammar to depth 3, obtaining a bounded planned pipeline, and Line 14 searches that using hyperopt, with no further modifications required.</p><p>Figure <ref type="figure" target="#fig_1">11</ref> shows a Lale grammar inspired by TPOT <ref type="bibr" target="#b16">[17]</ref>. It describes possibly non-linear pipelines, which use not only &gt;&gt; but also the &amp; combinator. Recall that (x &amp; y) &gt;&gt; Concat applies both x and y to the same data and then concatenates the features from both. Besides transformers, Line 6 also uses estimators with &amp;, turning their predictions into features for downstream operators. This is not supported in scikit-learn pipelines but is supported in Lale.</p><p>Progressive disclosure is a design technique that makes things easier to use by only requiring users to learn new features when and as they need them. The starting point for Lale is manual machine learning, and thus, the scikit-learn code in Figure <ref type="figure">2</ref> is also valid Lale code. User needs to learn zero new features if they do not use AutoML. To use algorithm selection, users only need to learn about the | combinator and the auto_configure function. To express pipelines more concisely, users can learn about the &gt;&gt; and &amp; combinators, but those are optional syntactic sugar for make_pipeline and make_union from scikit-learn. To use hyperparameter tuning, users only need to learn about wrap_imported_operators. To exercise more control over the search space, users can learn about freeze and custom schemas. While schemas are a non-trivial concept, Lale expresses them in JSON Schema <ref type="bibr" target="#b17">[18]</ref>, which is a widely-adopted and well-documented standard proposal. To use higher-order operators, users need not learn new syntax, as Lale supports scikit-learn syntax for them. Finally, to use grammars, users need to add 'g.' in front of their pipeline definitions; however, all the other features, such as the | combinator and the auto_configure function, continue to work the same with or without grammars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SEARCH SPACE GENERATION</head><p>This section describes how to map the programming model from Section 3 to work directly with three popular AutoML tools: scikitlearn's GridSearchCV <ref type="bibr" target="#b4">[5]</ref>, hyperopt <ref type="bibr" target="#b3">[4]</ref>, and SMAC <ref type="bibr" target="#b11">[12]</ref>, the library behind auto-sklearn <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">From Grammars to Planned Pipelines</head><p>Lale offers two approaches for using a grammar with GridSearch-CV, hyperopt, and SMAC: unfolding and sampling. Both approaches produce a planned pipeline, which can be directly used as the input for the compiler in Section 4.2. Unfolding and sampling are merely intended as proof-of-concept baseline implementations. In the future, we will also explore integrating Lale grammars directly with AutoML tools that support them, such as AlphaD3M <ref type="bibr" target="#b8">[9]</ref>.</p><p>Unfolding first expands the grammar to a given depth, such as 3 in the example from Figure <ref type="figure" target="#fig_1">10</ref> Line 13. Then, it prunes all disjuncts containing unresolved nonterminals, so that only planned Lale operators (individual, pipeline, or choice) remain.</p><p>Sampling traverses the grammar by following each nonterminal, picking a random step in each choice, and unfolding each pipeline. The result is a planned pipeline without any operator choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">From Planned Pipelines to Existing Tools</head><p>This section sketches how to map a planned pipeline (which includes a topology, steps for operator choices, and schemas for individual operator) to a search space in the format required by Grid-SearchCV, hyperopt, or SMAC. The running example for this section is the pipeline PCA &gt;&gt; (J48 | LR) with the individual operator schemas in Figure <ref type="figure" target="#fig_5">7</ref>.</p><p>Lale's search space generator has two phases: normalizer and backend. The normalizer translates the schemas of individual operators separately. The backend combines the schemas for the entire pipeline and finally generates a tool-specific search space.</p><p>The normalizer processes the schema for an individual operator in a bottom-up pass. The desired end result is a search space in Lale's normal form, which is ∨(dict{cat * , cont * } * ). At each level, the normalizer simplifies children and hoists disjunctions up. The backend starts by first combining the search spaces for all operators in the pipeline. Each pipeline becomes a 'dict' over its steps; each operator choice becomes an '∨' over its steps with added discriminants D to track what was chosen; and each individual operator simply comes from the normalizer. This yields an intermediate representation (IR) whose nesting structure reflects the operator nesting of the original pipeline. For our running example, this is:</p><formula xml:id="formula_5">dict                0: dict{N : (0..1)} ∨ dict{N : [mle]} 1: dict{D: [J48], R: [false], C: (0..0.5)} ∨ dict{D: [J48], R: [true, false], C: [0.25]} ∨ dict{D: [LR], S: [linear], P: [l1, l2]} ∨ dict{D: [LR], S: [linear, sag, lbfgs], P: [l2]}               </formula><p>The remainder of the backend is specific to the targeted AutoML tools, which the following text describes one by one.</p><p>The hyperopt backend of Lale is the simplest because hyperopt supports nested search space specifications that are conceptually similar to the Lale IR. For instance, an exclusive disjunction '∨' from the IR can be translated into a hyperopt hp.choice, an example for which occurs in Figure <ref type="figure">4</ref> Line 5. Similarly, a 'dict' from the IR can be translated into a Python dictionary that hyperopt understands.</p><p>For working with higher-order operators, Lale adds additional markers that enable it to reconstruct nested operators later.</p><p>The SMAC backend has to flatten Lale's nested IR into a grid of disjuncts with discriminants D. To do this, it internally uses a name mangling encoding that extends the __ mangling of scikit-learn, an example for which occurs in Figure <ref type="figure" target="#fig_0">3</ref> Line 7. Each element of the grid needs to be a simple 'dict' with no further nesting. For our running example, the result in mathematical notation is: Next, the SMAC backend adds conditionals that tell the Bayesian optimizer which variables are relevant for which disjunct, and finally outputs the search space in SMAC's PCS format.</p><p>The GridSearchCV backend starts from the same flattened grid representation that is also used by the SMAC backend. Then, it discretizes each continuous hyperparameter into a categorical by first including the default and then sampling a user-configurable number of additional values from its range and prior distribution (such as uniform in Figure <ref type="figure" target="#fig_5">7</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION</head><p>This section highlights some of the trickier parts of the Lale implementation, which is entirely in Python.</p><p>To implement lifecycle states, Lale uses Python subclassing. For example, the Trainable is a subclass of Planned, adding a fit method. Subclassing lets users treat an operator as also still belonging to an earlier state, e.g., in a mixed-state pipeline. The Lale implementation adds Python 3 type hints so users can get additional help from tools such as MyPy, PyCharm, or VSCode.</p><p>To implement the combinators &gt;&gt;, &amp;, and |, Lale uses Python's overloaded __rshift__, __and__, and __or__ methods. Python only supports overriding these as instance methods. Therefore, unlike in scikit-learn, Lale planned operators are object instances, not classes. This required emulating the scikit-learn __init__ with __call__.</p><p>The implementation carefully avoids in-place mutation of operators by methods such as auto_configure, fit, customize_schema, or unfold. This prevents unintended side effects and keeps the implementation consistent with the mathematical function abstractions from Section 3.1. Unfortunately, in scikit-learn, fit does in-place mutation, so for compatibility, Lale supports that but with a deprecation warning.  The implementation lets users import operators directly from their source packages. For example, see Figure <ref type="figure" target="#fig_1">1</ref> Lines 2-5. However, these operators need to then support the combinators and have attached schemas. Lale supports that via wrap_imported_operators(), which reflects over the symbol table and replaces any known non-Lale operators by an object that points to the non-Lale operator and augments it with Lale operator functionality.</p><p>The implementation supports interoperability with PyTorch, Weka, and R operators. This is demonstrated by Lale's operators from PyTorch (BERT, ResNet50), Weka (J48), and R (ARulesCBA). Supporting them requires, first, a class that is scikit-learn compatible. While this is easy for some cases (e.g., XGBoost), it is sometimes non-trivial. For instance, for Weka, Lale uses javabridge. Second, each operator needs a JSON schema for its hyperparameters. This is eased by Lale's customize_schema API.</p><p>The implementation of grammars had to overcome the core difficulty that recursive nonterminals require being able to use a name before it is defined. Python does not allow that for local variables. Therefore, Lale grammars implement it with object attributes instead. More specifically, Lale grammars use overloaded __getattr__ and __setattr__ methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>This section evaluates Lale on OpenML classification tasks and on different data modalities. It also experimentally demonstrates the importance of side constraints for the optimization process. For each experiment, we specified a Lale search space and then used auto_configure to run hyperopt on it. The value proposition of Lale is to leverage existing AutoML tools effectively and consistently; in general, we do not expect Lale to outperform them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benchmarks: OpenML Classification</head><p>To demonstrate the use of Lale, we designed four experiments that specify different search spaces for OpenML classification tasks. lale-pipe: The three-step planned lale_openml_pipeline in Figure <ref type="figure" target="#fig_18">12</ref>.</p><p>lale-ad3m: The AlphaD3M-inspired grammar of Figure <ref type="figure" target="#fig_1">10</ref> unfolded with a maximal depth of 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>lale-tpot:</head><p>The TPOT-inspired grammar of Figure <ref type="figure" target="#fig_1">11</ref> unfolded with a maximal depth of 3. lale-adb: The higher-order operator pipeline of Figure <ref type="figure" target="#fig_10">9</ref>.</p><p>For comparison, we used auto-sklearn <ref type="bibr" target="#b9">[10]</ref> -a popular scikitlearn based AutoML tool that has won two OpenML challengeswith its default setting as a baseline. We chose 15 OpenML datasets for which we could get meaningful results (more than 30 trials) Experiments were run on a 32 cores (2.0GHz) virtual machine with 128GB memory, and for each task, the total optimization time was set to 1 hour with a timeout of 6 minutes per trial. Table <ref type="table" target="#tab_2">1</ref> presents the results of our experiments. For each experiment, we report the test accuracy of the best pipeline found averaged over 5 runs. Note that for the shuttle dataset, 3 out of 5 runs of auto-sklearn resulted in a MyDummyClassifier being returned as the result. Since we were trying to evaluate the default settings, we did not attempt debugging it, but according to the tool's issue log, other users have encountered it before. The column askl-adb reports the accuracy of auto-sklearn when the set of classifiers is limited to AdaBoost with only data pre-processing. These results are presented for comparison with the lale-adb experiments, as the data preprocessing operators in Figure <ref type="figure" target="#fig_10">9</ref> were chosen to match those of auto-sklearn as much as possible. Also note that the default setting for auto-sklearn uses meta-learning.</p><p>The results show that carefully crafted search spaces (e.g., TPOTinspired grammar, or pipelines with higher-order operators) and offthe-shelf optimizers such as hyperopt can achieve accuracies that are competitive with state-of-the-art tools. These experiments thus validate Lale's controlled approach to AutoML as an alternative to black-box solutions. In addition, these experiments illustrate that Lale is modular enough to easily express and compare multiple search spaces for a given task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Case Studies: Other Modalities</head><p>While all the examples so far focused on tasks for tabular datasets, the core contribution of Lale is not limited to those. This section demonstrates Lale's versatility on three datasets from different modalities. Table <ref type="table" target="#tab_3">2</ref> summarizes the results.</p><p>Text. We used the Drug Review dataset for predicting a rating given by a patient to a drug. The Drug Review dataset has a text column called review, to which the pipeline applies either TfidfVectorizer  Image. We used the CIFAR-10 computer vision dataset. We picked the ResNet50 deep-learning model, since it has been shown to do well on CIFAR-10. Our experiments kept the architecture of ResNet50 fixed and tuned learning-procedure hyperparameters. Time-series. We used the Epilepsy dataset, a subset of the TUH Seizure Corpus, for classifying seizures by onset location (generalized or focal). We used a three-step pipeline:</p><formula xml:id="formula_6">1 planned = Window \ 2 &gt;&gt; (KNeighborsClassifier| XGBClassifier| LogisticRegression) \ 3 &gt;&gt; Voting</formula><p>We implemented a popular pre-processing method <ref type="bibr" target="#b18">[19]</ref> in a Window operator with three hyperparameters W , O, and T . Note that this transformer leads to multiple samples per seizure. Hence, during evaluation, each seizure is classified by taking a vote of the predictions made by each sample generated from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effect of Side Constraints on Convergence</head><p>Lale's search space compiler takes rich hyperparameter schemas including side constraints and translates them into semantically equivalent search spaces for different AutoML tools. This raises the question of how important those side constraints are in practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example for scikit-learn GridSearchCV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>P 1 :</head><label>1</label><figDesc>Provide a consistent programming model across the automation spectrum. There is a spectrum of AutoML ranging from manual machine learning (no automation) to hyperparameter tuning, operator selection, and pipeline topology search. Unfortunately, as 1 https://github.com/ibm/lale 1 N = scope.int(hp.qloguniform('N', 2, 8, 1)) 2 C = hp.lognormal('C', 1, 1000) 3 estim = HyperoptEstimator( 4 preprocessing=[pca('transform', n_components=N)], 5 classifier=hp.choice('classify', [ 6 liblinear_svc('classify.svc', dual=False, C=C), 7 random_forest('classify.rf', n_estimators=12)]), 8 algo=tpe.suggest, max_evals=100, trial_timeout=120) 9 estim.fit(train_X, train_y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Example for hyperopt-sklearn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of scikit-learn error checking.</figDesc><graphic coords="3,328.06,83.68,229.97,154.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 type 5 { 7 distribution: uniform}, 8 { 10 { 16 { 18 { 19 {</head><label>257810161819</label><figDesc>Even though the valid values for C are (0..1), search should only consider values C ∈ (0..0.5). The constraint in Lines 16-19 encodes a conditional (R = true) ⇒ (C = 0.25) by using the equivalent 1 PCA: { description: "Amount of variance to explain", 6 type: number, minimum: 0.0, maximum: 1.0, description: "Guess with Minka's MLE", enum: [mle]}]}}} 9 J48: { allOf: [ type: object, 11 properties: { 12 R: { description: "Use reduced error pruning", type: boolean }, 13 C: { description: "Pruning confidence threshold", 14 type: number, minimum: 0.0, maximum: 1.0, 15 maximumForOptimizer: 0.5, distribution: uniform }}}, description: "Setting confidence makes no sense for R", 17 anyOf: [ not: { type: object, properties: {R: {enum: [true]}}}}, type: object, properties: {C: {enum: [0.25]}}}]}]} 20 LR: { allOf: [ 21 { type: object, 22 properties: { 23 S: { description: "Optimization problem solver", 24 enum: [linear, sag, lbfgs], default: linear}, 25 P: { description: "Penalization norm", 26 enum: [l1, l2], default: l2}}}, 27 { description: "Solvers sag and lbfgs support only l2.", 28 anyOf: [ 29 { not: { type: object, properties: {S: {enum: [sag, lbfgs]}}}}, 30 { type: object, properties: {P: {enum: [l2]}}}]}]}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: JSON Schemas for hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>becomes: PCA : dict{N : (0..1) ∨ [mle])} J48 : dict{R: [true, false], C: (0..0.5)}∧ (dict{R: [true]} ⇒ dict{C: [0.25]}) LR : dict{S: [linear, sag, lbfgs], P: [l1, l2]}∧ (dict{S: [sag, lbfgs]} ⇒ dict{P: [l2]})</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>in Figures 3, 4, and 5. Lale offers a single unified syntax shown in Figure 1 Lines 15-16 to address problem P 2 from Section 2. Mathematically, let op be either an individual operator or a pipeline or choice whose θ topology or θ steps are already captured. That means op has the form θ hyperparams → D fit → D in → D out . Then auto_configure(op, D fit ) returns a function of the form D in → D out . Section 4 discusses how to implement auto_configure using schemas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example for custom schemas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 pipeline = make_pipeline( 2 MinMaxScaler | StandardScaler | Normalizer 3 |</head><label>23</label><figDesc>RobustScaler | QuantileTransformer, 4 AdaBoostClassifier(base_estimator=DecisionTreeClassifier))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Example for higher-order operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 g = Grammar() 2 3 4 | 7 8 10 |Figure 10 :</head><label>2471010</label><figDesc>Figure 10: Example grammar inspired by AlphaD3M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>8 9 g 12 || ExtraTreesClassifier \ 13 | QDA | PassiveAggressiveClassifier \ 14 |Figure 11 :</head><label>912131411</label><figDesc>Figure 11: Example grammar inspired by TPOT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>PCA: dict{N : (0..1)} ∨ dict{N : [mle]} J48 : dict{R: [false], C: (0..0.5)} ∨ dict{R: [true, false], C: [0.25]} LR : dict{S: [linear], P: [l1, l2]} ∨ dict{S: [linear, sag, lbfgs], P: [l2]}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><figDesc>dict{N : (0..1), D: [J48], R: [false], C: (0..0.5)} ∨ dict{N : (0..1), D: [J48], R: [true, false], C: [0.25] } ∨ dict{N : [mle], D: [J48], R: [false], C: (0..0.5)} ∨ dict{N : [mle], D: [J48], R: [true, false], C: [0.25] } ∨ dict{N : (0..1), D: [LR], S: [linear], P: [l1, l2] } ∨ dict{N : (0..1), D: [LR], S: [linear, sag, lbfgs], P: [l2] } ∨ dict{N : [mle], D: [LR], S: [linear], P: [l1, l2] } ∨ dict{N : [mle], D: [LR], S: [linear, sag, lbfgs], P: [l2] }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><figDesc>Line 7). The generated search space in mathematical notation is: dict{N : [0.50, 0.01], D: [J48], R: [false], C: [0.25, 0.01]} ∨ dict{N : [0.50, 0.01], D: [J48], R: [true, false], C: [0.25]} ∨ dict{N : [mle], D: [J48], R: [false], C: [0.25, 0.01]} ∨ dict{N : [mle], D: [J48], R: [true, false], C: [0.25]} ∨ dict{N : [0.50, 0.01], D: [LR], S: [linear], P: [l1, l2]} ∨ dict{N : [0.50, 0.01], D: [LR], S: [linear, sag, lbfgs], P: [l2]} ∨ dict{N : [mle], D: [LR], S: [linear], P: [l1, l2]} ∨ dict{N : [mle], D: [LR], S: [linear, sag, lbfgs], P: [l2]}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>1 2 | 5 || RandomForestClassifier 6 || QuadraticDiscriminantAnalysis 7 || DecisionTreeClassifier 8 |</head><label>25678</label><figDesc>prep = ( NoOp | MinMaxScaler | StandardScaler | Normalizer RobustScaler ) 3 feat = ( NoOp | PCA | PolynomialFeatures | Nystroem ) 4 clf = ( GaussianNB | GradientBoostingClassifier | SVC KNeighborsClassifier ExtraTreesClassifier PassiveAggressiveClassifier LogisticRegression | XGBClassifier | LGBMClassifier ) 9 lale_openml_pipeline = prep &gt;&gt; feat &gt;&gt; clf</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Pipeline for OpenML experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>1 planned_pipeline = ( 2 Project 3 &amp;</head><label>23</label><figDesc>(columns=['review']) &gt;&gt; (BERT | TfidfVectorizer) Project(columns={'type': 'number'}) 4 ) &gt;&gt; Cat &gt;&gt; (LinearRegression | XGBRegressor)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="1,66.30,152.69,237.97,216.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="1,325.40,151.83,232.80,227.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Accuracy for 15 OpenML classification tasks</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Absolute accuracy (mean and standard deviation over 5 runs)</cell><cell></cell><cell cols="2">100  *  (accuracy/autoskl -1)</cell></row><row><cell>Dataset</cell><cell>autoskl</cell><cell>lale-pipe</cell><cell>lale-tpot lale-ad3m</cell><cell>lale-adb</cell><cell>askl-adb</cell><cell cols="2">lale-pipe lale-tpot lale-ad3m lale-adb</cell></row><row><cell>australian</cell><cell cols="3">85.09 (0.44) 85.44 (0.72) 85.88 (0.57) 86.84 (0.00)</cell><cell cols="2">86.05 (1.62) 84.74 (3.11)</cell><cell>0.41</cell><cell>0.93</cell><cell>2.06</cell><cell>1.13</cell></row><row><cell>blood</cell><cell cols="3">77.89 (1.39) 76.28 (5.22) 77.49 (2.46) 74.74 (0.74)</cell><cell cols="2">77.09 (0.74) 74.74 (0.84)</cell><cell>-2.08</cell><cell>-0.52</cell><cell>-4.05</cell><cell>-1.04</cell></row><row><cell cols="4">breast-cancer 73.05 (0.58) 71.16 (1.20) 71.37 (1.15) 69.47 (3.33)</cell><cell cols="2">70.95 (2.05) 72.42 (0.47)</cell><cell>-2.59</cell><cell>-2.31</cell><cell>-4.90</cell><cell>-2.88</cell></row><row><cell>car</cell><cell cols="3">99.37 (0.10) 98.25 (1.16) 99.12 (0.12) 92.71 (0.63)</cell><cell cols="2">98.28 (0.26) 98.25 (0.25)</cell><cell>-1.13</cell><cell>-0.25</cell><cell>-6.70</cell><cell>-1.09</cell></row><row><cell>credit-g</cell><cell cols="3">76.61 (1.20) 74.85 (0.52) 74.12 (0.55) 74.79 (0.40)</cell><cell cols="2">76.06 (1.27) 76.24 (1.02)</cell><cell>-2.29</cell><cell>-3.24</cell><cell>-2.37</cell><cell>-0.71</cell></row><row><cell>diabetes</cell><cell cols="3">77.01 (1.32) 77.48 (1.51) 76.38 (1.11) 77.87 (0.18)</cell><cell cols="2">75.98 (0.48) 75.04 (1.03)</cell><cell>0.61</cell><cell>-0.82</cell><cell>1.12</cell><cell>-1.33</cell></row><row><cell>hill-valley</cell><cell cols="5">99.45 (0.97) 99.25 (1.15) 100.0 (0.00) 96.80 (0.21) 100.00 (0.00) 99.10 (0.52)</cell><cell>-0.20</cell><cell>0.55</cell><cell>-2.66</cell><cell>0.55</cell></row><row><cell>jungle-chess</cell><cell cols="3">88.06 (0.24) 90.29 (0.00) 88.90 (2.05) 74.14 (2.02)</cell><cell cols="2">89.41 (2.29) 86.87 (0.20)</cell><cell>2.54</cell><cell>0.96</cell><cell>-15.80</cell><cell>1.53</cell></row><row><cell>kc1</cell><cell cols="3">83.79 (0.31) 83.48 (0.75) 83.48 (0.54) 83.62 (0.23)</cell><cell cols="2">83.30 (0.36) 84.02 (0.31)</cell><cell>-0.38</cell><cell>-0.38</cell><cell>-0.21</cell><cell>-0.58</cell></row><row><cell>kr-vs-kp</cell><cell cols="3">99.70 (0.04) 99.34 (0.07) 99.43 (0.00) 96.83 (0.14)</cell><cell cols="2">99.51 (0.10) 99.47 (0.16)</cell><cell>-0.36</cell><cell>-0.27</cell><cell>-2.87</cell><cell>-0.19</cell></row><row><cell cols="4">mfeat-factors 98.70 (0.08) 97.58 (0.28) 97.18 (0.50) 97.55 (0.07)</cell><cell cols="2">97.52 (0.40) 97.94 (0.08)</cell><cell>-1.14</cell><cell>-1.54</cell><cell>-1.17</cell><cell>-1.20</cell></row><row><cell>phoneme</cell><cell cols="3">90.31 (0.39) 89.06 (0.67) 89.56 (0.36) 76.57 (0.00)</cell><cell cols="2">90.11 (0.45) 91.36 (0.21)</cell><cell>-1.39</cell><cell>-0.83</cell><cell>-15.20</cell><cell>-0.22</cell></row><row><cell>shuttle</cell><cell cols="3">87.27 (11.6) 99.94 (0.01) 99.93 (0.04) 99.89 (0.00)</cell><cell cols="2">99.98 (0.00) 99.97 (0.01)</cell><cell>14.51</cell><cell>14.50</cell><cell>14.45</cell><cell>14.56</cell></row><row><cell>spectf</cell><cell cols="3">87.93 (0.86) 87.24 (1.12) 88.45 (2.25) 83.62 (6.92)</cell><cell cols="2">88.45 (2.63) 89.66 (2.92)</cell><cell>-0.78</cell><cell>0.59</cell><cell>-4.90</cell><cell>0.59</cell></row><row><cell>sylvine</cell><cell cols="3">95.42 (0.21) 95.00 (0.61) 94.41 (0.75) 91.31 (0.12)</cell><cell cols="2">95.15 (0.20) 95.07 (0.14)</cell><cell>-0.45</cell><cell>-1.07</cell><cell>-4.31</cell><cell>-0.29</cell></row><row><cell cols="4">using the default settings of Auto-sklearn. The selected datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">comprise 5 simple classification tasks (test accuracy &gt; 90% in all</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">our experiments) and 10 harder tasks (test accuracy &lt; 90%). For</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">each experiment, we used a 66% -33% validation-test split, and a</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">5-fold cross validation on the validation split during optimization.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of the best pipeline using Lale with hyperopt. The mean and stdev are over 3 runs. learn or a pretrained BERT embeddings, which is a text embedding based on neural networks. The dataset also has numeric columns, which are concatenated with the result of the embedding.</figDesc><table><row><cell>modality</cell><cell>dataset</cell><cell>mean stdev</cell><cell>metric</cell></row><row><cell>Text</cell><cell cols="2">Drug Review 1.9237 0.06</cell><cell>test RMSE</cell></row><row><cell>Image</cell><cell>CIFAR-10</cell><cell>93.53% 0.11</cell><cell>test accuracy</cell></row><row><cell cols="2">Time-series Epilepsy</cell><cell>73.15% 8.2</cell><cell>test accuracy</cell></row><row><cell>from scikit-</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To explore this, we did an ablation study where we generated not just the constrained search spaces that are default with Lale but also unconstrained search spaces that drop side constraints. With hyperopt on the unconstrained search space, some iterations are unsuccessful due to exceptions, for which we reported np.float.max loss. Figure <ref type="figure">13</ref> plots the convergence for the Car dataset on the planned pipeline LR | KNN. Both of these operators have a few side constraints. Whereas the unconstrained search space causes some invalid points early in the search, the two curves more-or-less coincide after about two dozen iterations. The story looks very different in Figure <ref type="figure">14</ref> when adding a third operator J48 | LR | KNN. In the unconstrained case, J48 has many more invalid runs, causing hyperopt to see so many np.float.max loss values from J48 that it gives up on it. In the constrained case, on the other hand, J48 has no invalid runs, and hyperopt eventually realizes that it can configure J48 to obtain substantially better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Dataset Details</head><p>In order to be specific about the exact datasets for reproducibility, Table <ref type="table">3</ref> report the URLs for accessing those.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>There are various search tools, designed around scikit-learn <ref type="bibr" target="#b4">[5]</ref>, and each usually focused on a particular novel optimization algorithm. Auto-sklearn <ref type="bibr" target="#b9">[10]</ref> uses the search space specification and optimization algorithm of SMAC <ref type="bibr" target="#b11">[12]</ref>. Hyperopt-sklearn <ref type="bibr" target="#b14">[15]</ref> uses its own search space specification scheme and a novel optimizer based on Tree-structured Parzen Estimators (TPE) <ref type="bibr" target="#b2">[3]</ref>. Scikit-learn also comes with its own GridSearchCV and RandomizedSearchCV classes. Auto-Weka <ref type="bibr" target="#b19">[20]</ref> is a predecessor of auto-sklearn that also uses SMAC but operates on operators from Weka <ref type="bibr" target="#b10">[11]</ref> instead of scikit-learn. TPOT <ref type="bibr" target="#b16">[17]</ref> is designed around scikit-learn and uses genetic programming to search for pipeline topologies and operator choices. This usually leads to the generation of many misconfigured pipelines, wasting execution time. RECIPE <ref type="bibr" target="#b7">[8]</ref> prunes away the misconfigurations to save execution time by validating generated pipelines with a grammar for linear pipelines. AlphaD3M <ref type="bibr" target="#b8">[9]</ref> makes use of a grammar in a generative manner (instead of just for validation) with a deep reinforcement learning based algorithm. Katz et al. <ref type="bibr" target="#b12">[13]</ref> similarly use a grammar for Lale pipelines with AI planning tools. In contrast to these tools, the contribution of this paper is not a novel optimization algorithm but rather a more consistent programming model with search space generation targeting existing tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This paper describes Lale, a library for semi-automated data science. Lale contributes a syntax that is consistent with scikit-learn, but extends it to support a broad spectrum of automation including algorithm selection, hyperparameter tuning, and topology search. Lale works by automatically generating search spaces for established AutoML tools, extending their capabilities to grammar-based search and to search inside higher-order operators. The experiments show that search spaces crafted using Lale achieve results that are competitive with state-of-the-art tools while offering more versatility.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Baudart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avraham</forename><surname>Shinnar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3315508.3329972</idno>
		<ptr target="https://doi.org/10.1145/3315508.3329972" />
		<title level="m">Workshop on Machine Learning and Programming Languages (MAPL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Machine Learning in Python with No Strings Attached</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining Documentation to Extract Hyperparameter Schemas</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Baudart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Kate</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.16984" />
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Automated Machine Learning (AutoML@ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for Hyper-Parameter Optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Kégl</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimizat" />
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyperopt: a Python Library for Model Selection and Hyperparameter Optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="DOI">10.1088/1749-4699/8/1/014008</idno>
		<ptr target="http://dx.doi.org/10.1088/1749-4699/8/1/014008" />
	</analytic>
	<monogr>
		<title level="j">Computational Science &amp; Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Lars</forename><surname>Buitinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaques</forename><surname>Grobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Layton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1309.0238" />
		<title level="m">API Design for Machine Learning Software: Experiences from the scikit-learn Project</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="http://doi.acm.org/10.1145/2939672.2939785" />
	</analytic>
	<monogr>
		<title level="m">Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three models for the description of language</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="113" to="124" />
			<date type="published" when="1956">1956. 1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RECIPE: A Grammar-Based Framework for Automatically Evolving Classification Pipelines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>De Sá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Otavio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gisele</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><surname>Pappa</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-55696-3_16</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-319-55696-3_16" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Genetic Programming (EuroGP)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="246" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic Machine Learning by Pipeline Synthesis using Model-Based Reinforcement Learning and a Grammar</title>
		<author>
			<persName><forename type="first">Iddo</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yamuna</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raoni</forename><surname>Lourenco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Rampin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliana</forename><surname>Freire</surname></persName>
		</author>
		<ptr target="https://www.automl.org/wp-content/uploads/2019/06/automlws2019_Paper34.pdf" />
	</analytic>
	<monogr>
		<title level="m">Workshop on Automatic Machine Learning (AutoML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient and Robust Automated Machine Learning</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning" />
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS). 2962-2970</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<idno type="DOI">10.1145/1656274.1656278</idno>
		<ptr target="http://doi.acm.org/10.1145/1656274.1656278" />
	</analytic>
	<monogr>
		<title level="m">The WEKA Data Mining Software: An Update</title>
		<imprint>
			<date type="published" when="2009-11">2009. Nov. 2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequential Model-Based Optimization for General Algorithm Configuration</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><surname>Leyton-Brown</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-25566-3_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-25566-3_40" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning and Intelligent Optimization (LION)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring Context-Free Languages via Planning: The Case for Automating Machine Learning</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parikshit</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirin</forename><surname>Sohrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Udrea</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ojs/index.php/ICAPS/article/view/6686" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Automated Planning and Scheduling (ICAPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="403" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LightGBM: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf" />
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hyperopt-Sklearn: Automatic Hyperparameter Configuration for Scikit-Learn</title>
		<author>
			<persName><forename type="first">Brent</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<ptr target="http://conference.scipy.org/proceedings/scipy2014/komer.html" />
	</analytic>
	<monogr>
		<title level="m">Python in Science Conference (SciPy</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="32" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An ADMM Based Framework for AutoML Pipeline Configuration</title>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parikshit</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Vijaykeerthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djallel</forename><surname>Bouneffouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Bramble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Samulowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dakuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Conn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/5926" />
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4892" to="4899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automating Biomedical Data Science Through Tree-Based Pipeline Optimization</title>
		<author>
			<persName><forename type="first">Randal</forename><forename type="middle">S</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Urbanowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">C</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><forename type="middle">A</forename><surname>Lavender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><forename type="middle">Creis</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">H</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-31204-0_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-31204-0_9" />
	</analytic>
	<monogr>
		<title level="m">European Conference on the Applications of Evolutionary Computation (EvoApplications)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="123" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Foundations of JSON Schema</title>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Pezoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">L</forename><surname>Reutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín</forename><surname>Ugarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domagoj</forename><surname>Vrgoč</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872427.2883029</idno>
		<ptr target="https://doi.org/10.1145/2872427.2883029" />
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web (WWW)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Assessing seizure dynamics by analysing the correlation structure of multichannel intracranial EEG</title>
		<author>
			<persName><forename type="first">Kaspar</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howan</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">E</forename><surname>Elger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Lehnertz</surname></persName>
		</author>
		<idno type="DOI">10.1093/brain/awl304</idno>
		<ptr target="http://oup.prod.sis.lan/brain/article-pdf/130/1/65/992272/awl304.pdfhttps://doi.org/10.1093/brain/awl304" />
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><surname>Leyton-Brown</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487575.2487629</idno>
		<ptr target="https://doi.org/10.1145/2487575.2487629" />
	</analytic>
	<monogr>
		<title level="m">Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="847" to="855" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
