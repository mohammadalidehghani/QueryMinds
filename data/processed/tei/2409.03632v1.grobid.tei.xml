<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-09-05">5 Sep 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Andrew</forename><surname>Smart</surname></persName>
							<email>andrewsmart@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ATOOSA KASIRZADEH</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>San Francisco</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Atoosa Kasirzadeh</orgName>
								<orgName type="institution" key="instit2">Google Research</orgName>
								<address>
									<settlement>San Fran- cisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-09-05">5 Sep 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">B6185CE39001974AB9A4F478A113A487</idno>
					<idno type="arXiv">arXiv:2409.03632v1[cs.LG]</idno>
					<note type="submission">2024. Manuscript submitted to ACM Manuscript submitted to ACM 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>What is it to interpret the outputs of an opaque machine learning model? One approach is to develop interpretable machine learning techniques. These techniques aim to show how machine learning models function by providing either model-centric local or global explanations, which can be based on mechanistic interpretations (revealing the inner working mechanisms of models) or non-mechanistic approximations (showing input feature-output data relationships). In this paper, we draw on social philosophy to argue that interpreting machine learning outputs in certain normatively-salient domains could require appealing to a third type of explanation that we call "socio-structural" explanation. The relevance of this explanation type is motivated by the fact that machine learning models are not isolated entities but are embedded within and shaped by social structures. Socio-structural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models. We demonstrate the importance of socio-structural explanations by examining a racially biased healthcare allocation algorithm. Our proposal highlights the need for transparency beyond model interpretability: understanding the outputs of machine learning systems could require a broader analysis that extends beyond the understanding of the machine learning model itself.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In order to formulate a learning theory of machine learning, it may be necessary to move from seeing an inert model as the machine learner to seeing the human developer-along with, and not separate from, his or her model and surrounding social relations-as the machine learner.</p><p>-Reigeluth &amp; Castelle <ref type="bibr" target="#b54">[55]</ref> The past decade has seen massive research on interpretable machine learning (ML). <ref type="foot" target="#foot_0">1</ref> Here is a rough restatement of the goal of interpretable ML research program: many ML models are opaque in that even the expert humans cannot robustly understand, in non-mathematical terms, the reasons for why particular outputs are generated by these models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b65">66]</ref>. To overcome this opacity, various model-centric techniques have been developed to interpret their outputs. These techniques are diverse. They range from producing counterfactual explanations or heatmaps that offer insights into how changing inputs affect outputs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46]</ref>, to interpreting the inner workings of the model by probing patterns of neuron activations or attention mechanisms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b47">48]</ref>. <ref type="foot" target="#foot_1">2</ref>Despite these advancements, ML interpretability remains a contentious and ambiguous topic in the scientific community, lacking a universally accepted scope and definition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>. This ambiguity complicates the evaluation and regulation of opaque ML systems, raising questions about what constitutes sufficient interpretation and how it should be assessed. A pragmatic and pluralistic approach to interpretability has gained traction, viewing explanations as context-dependent responses to why-questions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. On this pluralistic approach, the adequacy of an explanation depends on the specific inquiry.</p><p>For simple classification tasks, techniques like saliency maps or feature importance may suffice. For instance, if a model is differentiating between images of cats and dogs, saliency maps could highlight the pixels most influential in the decision-making process. However, for complex and socially-embedded topics -such as biased healthcare algorithms -these model-centric explanations can fall short. Consider an algorithm that predicts hospital readmission risk but systematically underestimates it for certain racial groups. A model-centric explanation might highlight "total healthcare costs incurred in the past year" as an important feature. However, this alone might not fully reveal why the algorithm underestimates risk for a specific racial group. The algorithmic choice could come from the fact that this racial group, due to systemic inequities, have historically been unable to afford adequate healthcare and thus incurred lower costs. As a result, the low value for the "total healthcare costs incurred in the past year" feature does not necessarily indicate better health. Instead, it may suggest unmet healthcare needs, leading to higher readmission rates that the algorithm does not effectively account for. In such cases, interpretations that consider both model-specific details like feature importance and relevant social and structural factors like healthcare affordability disparities among racial groups are crucial for understanding ML predictions or decisions.</p><p>In this paper, we draw on social philosophy <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68]</ref> to advocate for a more comprehensive approach to ML interpretability research, expanding beyond model-centric explanations. We propose incorporating relevant socio-structural explanations to achieve a deeper understanding of ML outputs in domains with substantial societal impact. In the rest of the paper, we introduce the concept of socio-structural explanations and discuss their relevance to understanding ML outputs. We then examine how these explanations can enhance the interpretation of automated decision-making by ML systems in healthcare <ref type="bibr" target="#b48">[49]</ref>. Our paper expands the discourse on transparency in machine learning, arguing that it extends beyond model interpretability. We propose that in high-stake decision domains, a sociostructural analysis could be necessary to understand system outputs, uncover societal biases, ensure accountability, and guide policy decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INTERPRETABLE ML AND ITS DISCONTENTS</head><p>ML interpretability aims to generate human-understandable explanations for model predictions. This process requires the specification of two key components: the explanandum (the phenomenon requiring explanation) and the explanans (the elements providing the explanation). The model's prediction (or decision) typically serves as the explanandum, while visualizations or linguistic descriptions generated via interpretability techniques act as the explanans. To better understand the landscape of interpretability methods, we provide a broad classification of prominent approaches. <ref type="foot" target="#foot_3">3</ref>Model-centric interpretability approaches can be classified according to various criteria, with one fundamental distinction being between intrinsic and post-hoc interpretability <ref type="bibr" target="#b43">[44]</ref>. Intrinsic interpretability achieves transparency by restricting the complexity of the ML model itself, using approaches such as short decision trees or rule-based systems.</p><p>In contrast, post-hoc interpretability involves applying methods after model training. These methods include SHAP (SHapley Additive exPlanations) values <ref type="bibr" target="#b38">[39]</ref>, LIME (Local Interpretable Model-agnostic Explanations) <ref type="bibr" target="#b55">[56]</ref>, saliency maps for neural networks <ref type="bibr" target="#b0">[1]</ref>, and mechanistic interpretability tools <ref type="bibr" target="#b5">[6]</ref>. <ref type="foot" target="#foot_4">4</ref> Another popular classification criterion categorizes ML interpretability techniques into two main types: local and global. This categorization offers a complementary perspective by focusing on the scope and depth of the explanations they provide.</p><p>Local explanations focus on explaining individual (or a specific group of) predictions or decisions made by a model.</p><p>Local explanations often use techniques like feature attribution <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b55">56]</ref> or counterfactual instances <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47]</ref>. For example, for an image classification model that predicts "dog, " a pixel attribution method might highlight the pixels around the dog's ears and tail as being most influential in the prediction "dog. " The explanation could be "The model classified this image as a dog primarily because of the distinctive shapes in these highlighted areas (pointing to highlighted pixels in a visualization). The pointed ears here and the curved tail shape here were the most influential features in making this prediction. Other parts of the image, such as the background or the dog's body, had less impact on the classification. " For a loan approval ML model, a counterfactual explanation could be "If your income was 5,000 US dollars higher, your loan would have been approved. "</p><p>Global explanations shed light on the average behavior of the model and provide an overall understanding of how a model works across possible inputs. These methods are often expressed as expected values based on the distribution of the data. Global explanations aim to answer questions like "What features are generally most important for this model's predictions?" or "How does the model behave across different types of inputs?" Techniques for global explanations include partial dependence plots <ref type="bibr" target="#b17">[18]</ref> and accumulated local effects <ref type="bibr" target="#b3">[4]</ref>. For example, a partial dependence plot, a type of feature effect plot, can show the expected prediction when all other features are marginalized out. In a house price prediction model, a partial dependence plot might show how the predicted price changes as the house size increases, averaged across all other features like location, number of bedrooms, or age of the house. Since global interpretability methods describe average behavior, they are particularly useful when the modeler wants to understand the general mechanisms in the data, debug a model, or gain insights into its overall performance across various scenarios.</p><p>Mechanistic interpretations expand upon both local and global explanations. These interpretability tools seek to understand the internals of a model. In the case of neural networks, mechanistic interpretability tools reverse engineer the algorithms implemented by neural networks into concepts, often by examining the weights and activations of neural networks. This approach includes methods such as circuit analysis or dictionary learning for identifying specific subnetworks of neurons within larger models to understand the implementation of particular behavior <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b62">63]</ref>. 5   Mechanistic interpretability is an emerging and highly active area of research, with rapid developments in its neural analysis techniques. Each of the above-mentioned approaches offers different perspectives on model behavior, ranging from specific instance explanations to overarching principles of operation and fundamental computational mechanisms. The choice of method depends on the specific interpretability goals and the nature of the model being analyzed. There are several acknowledged limitations to existing interpretability approaches.</p><p>First, interpretability techniques can be brittle, sensitive to the target of interpretation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b64">65]</ref>, to minor perturbations in model parameters <ref type="bibr" target="#b19">[20]</ref> or input data <ref type="bibr" target="#b51">[52]</ref>. This fragility raises concerns about the reliability and robustness of generated explanations using interpretability methods, especially in real-world scenarios where models are subject to noisy data and evolving conditions. Recent work on mechanistic interpretability has begun to discover features 5 Neurons in neural networks can be monosemantic (representing a single concept) or polysemantic (representing multiple unrelated concepts). Monosemantic neurons activate for a single semantic concept, suggesting a one-to-one relationship between neurons and features <ref type="bibr" target="#b62">[63]</ref>. However, neurons are often polysemantic, activating for multiple unrelated concepts, complicating network interpretation. For instance, researchers have empirically shown that for a certain language model, a single neuron can correspond to a mixture of academic citations, English dialogue, HTTP requests, and Korean text <ref type="bibr" target="#b8">[9]</ref>. Polysemanticity makes it difficult to reason about the behavior of the network in terms of the activity of individual neurons.</p><p>of large language models that are more robust <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>. However, there is still significant progress to be made in developing consistently reliable interpretability methods <ref type="bibr" target="#b64">[65]</ref>.</p><p>Second, for a given model and input, there may be multiple valid explanations, each highlighting different aspects of the prediction-making or decision-making process <ref type="bibr" target="#b61">[62]</ref>. This multiplicity embodies both a feature and a bug: as a feature, it reflects the need for a multi-faceted understanding of the complexity of ML predictions; as a bug, it introduces potential confusion and conflicting interpretations, challenging efforts to identify the most relevant or meaningful explanation. Consider, for instance, an ML model used in hiring decisions that recommends not to hire a particular candidate. A feature importance analysis might indicate that the candidate's educational background was the primary factor in the decision. However, a counterfactual explanation might suggest that changing the candidate's gender would alter the outcome. Simultaneously, a SHAP analysis could show that a combination of factors including work experience, interview performance, and age contributed to the decision. Each of these explanations provides insight into the model's reasoning, but emphasizes different aspects, some of which may be more socially sensitive or legally problematic than others. This diversity of explanations challenges practitioners in determining which aspects are most crucial for the model's behavior. Moreover, different stakeholders -such as job applicants, hiring managers, and legal compliance officers -might prefer or trust certain types of explanations over others, further complicating the practical application of these interpretability methods <ref type="bibr" target="#b6">[7]</ref>. Despite the growing number of interpretability approaches, there is a lack of standardized benchmarks and evaluation frameworks to assess their legal and ethical relevance and compare their performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Third, we still have no provable guarantee that a post-hoc explanation accurately reflects the true reasoning behind a model's prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b64">65]</ref>. Explanations may be overly simplistic, highlight irrelevant features, or even be misleading, potentially leading to incorrect conclusions about the model's behavior. The potential lack of faithfulness is particularly problematic in high-stakes domains where decisions have significant consequences. For example, a counterfactual explanation for a loan denial might suggest that increasing income would lead to approval. However, the true cause might be a complex interaction of credit history and debt-to-income ratio, not captured by the explanation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b61">62]</ref>. Given these limitations, researchers are exploring novel methods to enhance our understanding of ML models and their predictions (or decisions).</p><p>A promising approach to enhance model transparency involves expanding the scope of interpretations beyond the internal mechanics of the model itself. This expanded perspective recognizes that ML models do not operate in isolation, but within complex social and institutional contexts that can significantly influence their behavior and impact. Here, we propose a new perspective to interpreting ML outputs that incorporates relevant social and structural factors into transparency demands. In particular, we think that in certain situations, the soundness and stability of ML explanations can be improved by appealing to what we call socio-structural explanations that are external to an ML model. Our thesis is that in some socially salient applications of ML models, perhaps the most important constraints on model behavior are external to the model itself. Extending the idea that the machine learner is not only the inert model, but includes the human developers, uses and surrounding social relations and practices <ref type="bibr" target="#b54">[55]</ref>, we propose to explain the behavior of a model, in such instances, given its place in a social structure. We call such explanations socio-structural explanations.</p><p>In order to understand socio-structural explanations, we first need to know what are social structures?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SOCIAL STRUCTURES AND SOCIO-STRUCTURAL EXPLANATIONS</head><p>Social structures are the underlying realities that shape our social lives, influencing our choices, opportunities, and experiences <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b66">67]</ref>. They are the invisible scaffolding of society, both constraining and enabling our individual and collective actions. They give rise to social hierarchies through institutions, policies, economic systems, and cultural or normative belief systems such as race or socioeconomic status <ref type="bibr" target="#b7">[8]</ref>. Social structures manifest in various forms, from the subtle influence of societal norms to the explicit impact of legal frameworks, creating a multilayered reality that shapes our experiences and opportunities.</p><p>Social and political philosopher, Iris Marion Young <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>, defines social structures as the interplay of institutional rules, interactive routines, resource mobilization, and physical infrastructure. These enduring elements shape the context within which individuals act, offering both opportunities and limitations. 6 These structures, while socially constructed, possess a reality for exerting tangible influences on individuals and institutions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b67">68]</ref>. They are powerful forces that can constrain and enable actions, cause the specific distribution of resources, and define social roles and expectations. Social structures can explain persistent patterns and circumstances in society, such as racial inequality or gender disparities. To get more concrete, let us analyze the notion of social structures in the context of a socio-structural explanation, borrowing a simple example from Garfinkel <ref type="bibr" target="#b18">[19]</ref>: Suppose that, in a class I am teaching, I announce that the course will be "graded on a curve, " that is, that I have decided beforehand what the overall distribution of grades is going to be. Let us say, for the sake of the example, that I decide that there will be one A, 24 Bs, and 25 Cs. The finals come in, and let us say Mary gets the A. She wrote an original and thoughtful final.</p><p>Garfinkel <ref type="bibr" target="#b18">[19]</ref> argues that the explanation "She wrote an original and thoughtful final" is inadequate to answer the explanation-seeking question "Why did Mary get an A?" In a curved grading system, achieving the sole A grade requires more than just quality work. For Mary to earn the only A in the class, her final would need to be the best. If the instructor had not implemented a grading curve, multiple students could have earned As by producing thoughtful and original finals. Garfinkel elaborates on this point, stating "So it is more accurate to answer the question by pointing to the relative fact that Mary wrote the best paper in the class" <ref type="bibr">[19, p. 41</ref>]. Mary's A grade was not solely a result of her individual performance, but also a consequence of her relative standing among peers, combined with the specific grading structure that emphasized this comparative aspect. The grading structure, in this case, serves as a crucial contextual element shaping the explanation. More precisely, the structural aspect of this explanation is "the predetermined grading curve that limited the number of As to one, " while the social aspect is "Mary's performance relative to her peers (she wrote the best paper in the class). " Here is a different example for further clarification of the notion of socio-structural explanation. Consider the following explanatory question: "Why do women continue to be economically disadvantaged relative to men (as opposed to reaching economic parity with men?)" <ref type="bibr" target="#b24">[25]</ref>. Haslanger <ref type="bibr" target="#b24">[25]</ref> argues that we can have (at least) three explanations for this question: biological, individualistic, and structural.</p><p>Biologistic explanation: Women are inherently less capable than men in biological qualities deemed necessary (such as intelligence or competitiveness) for success in high-paying jobs. 6 Social structures, according to Young <ref type="bibr">[67, p.111]</ref>, are defined as follows:</p><p>As I understand the concept, the confluence of institutional rules and interactive routines, mobilization of resources, as well as physical structures such as buildings and roads. These constitute the historical givens in relation to which individuals act, and which are relatively stable over time. Social structures serve as background conditions for individual actions by presenting actors with options; they provide "channels" that both enable action and constrain it.</p><p>Individualistic explanation: Women, to a greater extent than men, prioritize child-rearing over pursuing high-paying careers, thus voluntarily sacrificing economic success for the perceived rewards and satisfactions of motherhood.</p><p>Structural explanation: Women are embedded within a self-reinforcing economic structure that systematically disadvantages them through institutional practices, social norms, and power dynamics.</p><p>Each of these explanations refers to different causes, operating at distinct levels of analysis. The biologistic and individualistic explanations focus on factors intrinsic to individuals or groups, without considering the broader sociostructural context. In contrast, the structural explanation situates individual actions and outcomes within a larger system of interconnected social forces. If the social structure is in place, then we can view individuals as occupying specific "nodes" within a complex social network or structure. The socio-structural explanation posits that gender wage disparities arise from the complex interplay of societal, economic, and institutional factors that collectively shape opportunities and constraints. Given the socio-structural limitations in place, we can explain why women, at the population level, experience economic disadvantages compared to men based on their position within the social structure.</p><p>In this context, "social structure" refers to the complex network of institutions, relationships, and cultural norms that organize society. It includes economic systems that historically undervalue work traditionally performed by women, political institutions that may underrepresent women's interests, and educational structures that can reinforce gender stereotypes. Additionally, it includes cultural norms that influence career choices and work-life balance expectations, organizational hierarchies that often favor male leadership, and legal frameworks that may inadequately address gender discrimination.</p><p>Women's place within this multifaceted social structure often results in reduced access to resources, limited decisionmaking power, and fewer opportunities for advancement, collectively contributing to persistent economic disparities at the population level. The socio-structural approach to explanation, when rigorously applied, offers valuable insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It demonstrates how individual choices and actions can be profoundly shaped by the surrounding social structures.</head><p>By highlighting the influence of broader structural forces on seemingly personal decisions, it reveals patterns often operating beyond an individual's immediate awareness or control.</p><p>Let us draw a close analogy between the above instances of socio-structural explanations and a toy example of interpreting an ML model's output. Consider an ML-powered hiring model that consistently recommends male candidates over female candidates for senior executive positions in a tech company. An initial explanation of the recommendations generated by a SHAP interpretability method might say: "The model recommends male X over female Y because X's features contribute more positively to the model's output. Specifically, X's 10 years of tech leadership experience contributes +0.4 to the score, while Y's 7 years contributes only +0.2. " Let us assume similar explanations (relating years of tech leadership experience to the recommendation score) are generated for a population of females. These explanations might fail to capture the full picture.</p><p>Upon deeper investigation, an auditor team uncovers a more complex and nuanced reality. First, the auditors find that the ML model was trained on the company's historical hiring data from 2000-2020, which included 85% male executives. This data reflects the company's past hiring practices, which favored men for leadership roles. The sociostructural aspect here is the historical underrepresentation of women in executive positions, rooted in long-standing societal norms and institutional practices. A socio-structural explanation could look like: "The model's bias reflects decades of systemic exclusion of women from leadership roles in the tech industry, perpetuating a cycle where the Manuscript submitted to ACM lack of female representation in executive positions reinforces the perception that these roles are best suited for men. "</p><p>Second, the ML model places high importance on continuous work experience, with any gap longer than 6 months reducing a candidate's score by 0.1 per year. 40% of female candidates had career gaps averaging 2.5 years, compared to 10% of male candidates averaging 1 year, often coinciding with childbearing ages. This reflects the socio-structural reality of women bearing a disproportionate responsibility for child-rearing and family care, leading to more frequent and longer career interruptions. A socio-structural explanation could look like: "The model's penalty for career gaps disproportionately impacts women due to societal expectations and norms that place the primary burden of childcare and family responsibilities on women, resulting in more frequent and longer career interruptions that are then interpreted by the model as reduced qualifications. " Third, the model does not consider geographic location in its evaluation.</p><p>However, geographic disparities affect job availability and commute times, disproportionately impacting women with childbearing responsibilities. This reflects socio-structural expectations around family care that often limit women's job options to those closer to home or with flexible hours. A socio-structural explanation could look like: "The model's failure to account for geographic factors overlooks the societal expectations that often constrain women's job choices based on proximity to home and flexibility for family care. This oversight particularly disadvantages women who may be highly qualified but limited in their job options due to these socially imposed constraints. "</p><p>Producing rigorous socio-structural explanations can be challenging as it requires significant sociological understanding and interdisciplinary expertise. However, once obtained, these explanations enable novel forms of interventions. Here are some examples of possible interventions enabled by obtaining socio-structural explanations. The first is to modify the model to cap the maximum contribution of "continuous experience" at +0.2. The second is to introduce a new feature "diverse experience" that values varied career paths, including those with gaps. The third is to augment the training data with 500 profiles of successful executives who have had career gaps of 1-3 years, ensuring at least 50% are women. The fourth is to implement a company-wide policy requiring human review for any candidate the ML system ranks lower primarily due to career gaps (&gt;0.2 score difference). This toy example is supposed to highlight that integrating socio-structural explanations into the ML transparency toolkit enables us to transcend superficial model-centric solutions (when relevant) and address the fundamental causes underlying ML outputs.</p><p>Of course, the specific interventions depend on what we want to change and the particular context of the problem at hand. Socio-structural explanations are not always useful or applicable in every situation. The effectiveness of these explanations and subsequent interventions can vary based on the complexity of the social systems involved, the quality of available data, and the specific goals of the analysis. In some cases, other approaches might be more appropriate or effective.</p><p>In the following section, we examine a case study of algorithmic deployment in healthcare decision-making, highlighting the critical relevance of socio-structural explanations in this context. This analysis demonstrates how a deeper understanding of social structures can inform more effective strategies for developing and implementing algorithmic systems in high-stakes decision domains. While our original focus was on socio-structural explanations for ML systems, we recognize that the importance of these explanations generalizes to a broader range of automated decision systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SOCIO-STRUCTURAL EXPLANATIONS OF RACIAL BIAS IN HEALTH-CARE ALGORITHMS</head><p>A widely discussed example in the growing body of literature on algorithmic bias is the study by Obermeyer et al. <ref type="bibr" target="#b48">[49]</ref>. This research revealed that a commonly used US hospital predictive algorithm for allocating scarce healthcare resources systematically discriminated against Black patients. Specifically, the algorithm assigned lower risk scores to Black patients who were equally in need as their White counterparts. The root cause was the algorithm's use of healthcare costs as a proxy for "healthcare need" <ref type="bibr" target="#b34">[35]</ref>. This approach led to a significant underestimation of health risks for Black patients who, on average, incurred lower healthcare costs than White patients with similar chronic conditions due to systemic disparities in care access and quality <ref type="bibr" target="#b48">[49]</ref>.</p><p>Empirical investigations demonstrated that the care provided to Black patients cost an average of USD 1,800 less per year than the care given to a white person with the same number of chronic health problems. At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses <ref type="bibr" target="#b48">[49]</ref>. The algorithm predicted that this disparity in spending corresponded to a similar disparity in actual health-care needs and therefore risk score. Consequently, Black people had to be much sicker in order to be referred for treatment or other resources. The algorithm's prediction of health needs is, in fact, a prediction on health costs <ref type="bibr" target="#b48">[49]</ref>.</p><p>When algorithmic decision systems fail in consequential domains like health-care, the repercussions can be severe, potentially leading to patient deaths. It is crucial to understand the reasons and causes for such failures. Therefore, explaining the "why" behind these failures through the analysis of failed outputs is critical. One prevalent type of failure is algorithmic bias that perpetuates existing socio-structural inequalities, such as structural racism <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Structural racism refers to the complex ways in which historical and contemporary racial inequities are reproduced through interconnected societal systems like healthcare, education, housing, and the criminal justice system <ref type="bibr" target="#b50">[51]</ref>. Even when race is not explicitly considered, its influence can be deeply embedded in the data, shaping associations and outcomes <ref type="bibr" target="#b56">[57]</ref>. In the context of this instance, the following explanatory question demands a response: Why did this algorithm systematically discriminate against Black people?</p><p>To answer this question, we must consider both the interpretation in reference to the model and the broader sociostructural context in which it operates. <ref type="foot" target="#foot_6">7</ref> Obermeyer et al. <ref type="bibr" target="#b48">[49]</ref> show that this particular algorithm discriminated against Black patients due to its use of healthcare costs as a proxy for "healthcare need. " This choice reflects a fundamental misunderstanding of the relationship between costs and needs in a healthcare system marked by systemic racial disparities. Obermeyer et al. <ref type="bibr" target="#b48">[49]</ref> demonstrated that conditioning on healthcare costs is the mechanism by which the bias arises in this case, and we must change the data we feed the algorithm and use new labels that better reflect social reality, which in turn requires deep understanding of the domain, the ability to identify and extract relevant data elements, and the capacity to iterate and experiment <ref type="bibr" target="#b48">[49]</ref>. The socio-structural interpretation of the algorithm's behavior is as follows.</p><p>The algorithm discriminates against Black patients because it is designed and deployed in a healthcare system characterized by longstanding racial inequities. By encoding healthcare costs as a proxy for healthcare needs, the algorithm inadvertently encodes and perpetuates systemic disparities in care access and quality. Black patients, on average, incur lower healthcare costs not because they are healthier, but due to historical patterns of exclusion, lack of access to care, and underinvestment in healthcare resources for Black communities. The algorithm interprets these lower costs as lower needs, thereby underestimating the health risks for Black patients and perpetuating a cycle of inadequate care allocation. This reflects how the algorithm manages to reproduce structural racism through its uncritical use of data that embodies these systemic inequalities.</p><p>It remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source <ref type="bibr" target="#b52">[53]</ref>.</p><p>Unfortunately, many failures of algorithmic decision systems in the healthcare industry disproportionately impact people or communities who have been put already in a structurally vulnerable social positions <ref type="bibr" target="#b35">[36]</ref>. This can be due to many factors. However, a consistent theme in the study of these failures, that is often only revealed after the fact, is that there is a lack of socio-structural understanding among the designers and users of these systems <ref type="bibr" target="#b39">[40]</ref>. The study presented in this section exemplifies this challenge. Employing model-centric explanations would likely highlight the importance of the cost feature to algorithmic output, but would not expose the underlying racial bias originating from historical and systemic inequalities in healthcare access and delivery. In this context, socio-structural explanations consider the relevant societal context in which the model operates, in relation to relevant historical biases, societal norms, and institutional practices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLICATIONS FOR ML TRANSPARENCY RESEARCH AND CONCLUSION</head><p>ML research and practice are fundamentally shaped by the approaches adopted by practitioners. These approaches influence the entire process: from the questions asked and data collected, to the choice of objective functions and the selection of proxy or target variables for optimization. Throughout this paper, we have argued that model-centric explanations, while valuable, can be inadequate for comprehensively understanding whether a model truly benefits or potentially harms people. This inadequacy is particularly pronounced in high-stakes domains where ML models are often developed and deployed into complex social and structural contexts without sufficient domain-specific theoretical understanding. We have argued that to meaningfully interpret the social predictions (or decisions) of models in highstake domains, a deep socio-structural understanding is required.</p><p>One challenge lies in that many ML practitioners and researchers may not feel adequately equipped to analyze and respond to social structures. Alternatively, they may be hindered from leveraging social structural knowledge due to constraints in time, training, incentives, or resources <ref type="bibr" target="#b60">[61]</ref>. This gap between technical expertise and socio-structural understanding presents a significant hurdle in developing truly beneficial ML systems. Algorithmic transparency and accountability research in ML is often motivated by the need to foster trust in these systems. Much of this research rightly argues for the critical importance of model-centric interpretations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39]</ref>. However, the demands for transparency of ML models must extend beyond model-centric details to encompass socio-structural factors in socially-salient prediction or decision domains. Producing new, more representative labels and objectives for ML models requires a deep understanding of the domain, the ability to identify and extract relevant data elements, and the capacity to iterate and experiment <ref type="bibr" target="#b48">[49]</ref>.</p><p>The importance of socio-structures has been increasingly recognized in recent literature on algorithmic justice <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. These works argue for a more holistic approach to ML development and deployment, one that considers not just model-centric measures but also societal impacts. In light of these considerations, we call for further research into the integration of socio-structural understanding into different stages of the ML lifecycle, from problem formulation and data collection to model development, deployment, and ongoing monitoring. We think that sometimes sociostructural interpretations can reveal causally-relevant reasons for why an algorithm behave in a certain way for a certain population. By doing so, we can work towards ML systems that are not only technically proficient but also socially aware and beneficially aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>We would like to thank Donald Martin, Been Kim, Darlene Neal, Mayo Clinic Accelerate Program and The Impact Lab team at Google Research for helpful discussions and feedback on earlier drafts of this paper.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For the purposes of this paper, we use "interpretable ML," "explainable ML," "interpretable AI," and "explainable AI" interchangeably.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Researchers are actively developing unified frameworks that integrate multiple interpretability methods, with the aim of providing a comprehensive conceptual toolkit for understanding the outputs of complex ML models[30,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b59">60]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>The interpretable ML literature has grown extensively, making a comprehensive survey beyond the scope of this paper. For recent overviews, see<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b58">59</ref>].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Post-hoc methods can also be applied to intrinsically interpretable models, such as computing permutation feature importance for decision trees, which can provide additional insights into their decision-making process.Manuscript submitted to ACM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>Manuscript submitted to ACM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>For a discussion of the levels of interpretation see Creel<ref type="bibr" target="#b11">[12]</ref> and Kasirzadeh and Klein<ref type="bibr" target="#b32">[33]</ref>.Manuscript submitted to ACM</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sanity checks for saliency maps</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Chirag</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshika</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyapriya</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Pawelczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nari</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isha</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11104</idno>
		<title level="m">OpenXAI: Towards a Transparent Evaluation of Model Explanations</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08049</idno>
		<title level="m">On the robustness of interpretability methods</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visualizing the effects of predictor variables in black box supervised learning models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Apley</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1059" to="1086" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<ptr target="http://www.fairmlbook.org" />
		<title level="m">Fairness and Machine Learning. fairmlbook.org</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Bereska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14082</idno>
		<title level="m">Mechanistic Interpretability for AI Safety-A Review</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explainable machine learning in deployment</title>
		<author>
			<persName><forename type="first">Umang</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruchir</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><surname>Eckersley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on fairness, accountability, and transparency</title>
		<meeting>the 2020 conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="648" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structural vulnerability: operationalizing the concept to address health disparities in clinical care</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Bourgois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">M</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Quesada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic medicine: journal of the Association of American Medical Colleges</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page">299</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Trenton</forename><surname>Bricken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adly</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carson</forename><surname>Denison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karina</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brayden</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josiah</forename><forename type="middle">E</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2023/monosemantic-features" />
		<title level="m">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</title>
		<imprint>
			<publisher>Transformer Circuits</publisher>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Activation atlas</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zan</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon Sik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Plumb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interpretable Machine Learning: Moving from mythos to diagnostics</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="28" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transparency in complex computational systems</title>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">A</forename><surname>Creel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="568" to="589" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<title level="m">Towards a rigorous science of interpretable machine learning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">Rudresh</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devam</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Het</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smiti</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Omer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pankesh</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejal</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable AI (XAI): Core ideas, techniques, and solutions</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10652</idno>
		<title level="m">Toy models of superposition</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2021/framework/index.html" />
		<title level="m">A Mathematical Framework for Transformer Circuits</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The ant trap: Rebuilding the foundations of the social sciences</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Epstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><surname>Jerome H Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Forms of Explanation: Rethinking the Questions in Social Theory</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Garfinkel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Yale University Press</publisher>
			<pubPlace>New Haven</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpretation of neural networks is fragile</title>
		<author>
			<persName><forename type="first">Amirata</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3681" to="3688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Counterfactual explanations and how to find them: literature review and benchmarking</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="page" from="1" to="55" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Tessa</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01254</idno>
		<title level="m">Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post hoc Explanations</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards a critical race methodology in algorithmic fairness</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamila</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Loud</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on fairness, accountability, and transparency</title>
		<meeting>the 2020 conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="501" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Resisting reality: Social construction and social critique</title>
		<author>
			<persName><forename type="first">Sally</forename><surname>Haslanger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What is a (social) structural explanation?</title>
		<author>
			<persName><forename type="first">Sally</forename><surname>Haslanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Studies</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="113" to="130" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Failures of methodological individualism: the materiality of social systems</title>
		<author>
			<persName><forename type="first">Sally</forename><surname>Haslanger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Where fairness fails: data, algorithms, and the limits of antidiscrimination discourse</title>
		<author>
			<persName><forename type="first">Anna</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information, Communication &amp; Society</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="900" to="915" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Explainable AI methods-a brief overview</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Saranti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemyslaw</forename><surname>Biecek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Extending Explainable AI Beyond Deep Models and Classifiers</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A benchmark for interpretability methods in deep neural networks</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Elisabeth Andr, and Ofra Amir. 2021. Local and global explanations of agent behavior: Integrating strategy summaries with saliency maps</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Weitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">301</biblScope>
			<biblScope unit="page">103571</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00752</idno>
		<title level="m">Reasons, values, stakeholders: A philosophical framework for explainable artificial intelligence</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Algorithmic fairness and structural injustice: Insights from feminist political philosophy</title>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2022 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The ethical gravity thesis: Marrian levels and the persistence of bias in automated decision-making systems</title>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The use and misuse of counterfactuals in ethical machine learning</title>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Smart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="228" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Millions of black people affected by racial bias in health-care algorithms</title>
		<author>
			<persName><forename type="first">Heidi</forename><surname>Ledford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">574</biblScope>
			<biblScope unit="page" from="608" to="610" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Peppin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">K</forename><surname>Wolters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexa</forename><surname>Hagerty</surname></persName>
		</author>
		<title level="m">Does &quot;AI&quot; stand for augmenting inequality in the era of covid-19 healthcare? bmj</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page">372</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Artificial intelligence in a structurally unjust society</title>
		<author>
			<persName><forename type="first">Ting-An</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Hsuan</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Feminist Philosophy Quarterly</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery</title>
		<author>
			<persName><surname>Zachary C Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="31" to="57" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Participatory problem formulation for fairer machine learning through community based system dynamics</title>
		<author>
			<persName><forename type="first">Donald</forename><surname>Martin</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jill</forename><surname>Kuhlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Isaac</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07572</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence for process mining: A general overview and application of a novel local explanation approach for predictive process monitoring</title>
		<author>
			<persName><forename type="first">Nijat</forename><surname>Mehdiyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Fettke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interpretable Artificial Intelligence: A Perspective of Granular Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piers</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liz</forename><surname>Sonenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00547</idno>
		<title level="m">Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Explaining explanations in AI</title>
		<author>
			<persName><forename type="first">Brent</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Wachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Interpretable machine learning</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Leanpub</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">General pitfalls of model-agnostic interpretation methods for machine learning models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Knig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Herbinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Freiesleben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Dandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">A</forename><surname>Scholbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Grosse-Wentrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Extending Explainable AI Beyond Deep Models and Classifiers</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">EXPLAIN-IT: Towards explainable AI for unsupervised network traffic analysis</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Morichetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Mellia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks</title>
		<meeting>the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Explaining machine learning classifiers through diverse counterfactual explanations</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Ramaravind K Mothilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on fairness, accountability, and transparency</title>
		<meeting>the 2020 conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="607" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Lieberum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.05217</idno>
		<title level="m">Progress measures for grokking via mechanistic interpretability</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dissecting racial bias in an algorithm used to manage the health of populations</title>
		<author>
			<persName><forename type="first">Ziad</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Vogeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11895</idno>
		<title level="m">-context learning and induction heads</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Structural racism-a 60-year-old black woman with breast cancer</title>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Pallok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>De Maio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Ansell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">380</biblScope>
			<biblScope unit="page" from="1489" to="1493" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Failing loudly: An empirical study of methods for detecting dataset shift</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Rabanser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gnnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing</title>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Inioluwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">N</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamila</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smith-Loud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Theron</surname></persName>
		</author>
		<author>
			<persName><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on fairness, accountability, and transparency</title>
		<meeting>the 2020 conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Toward transparent ai: A survey on interpreting the inner structures of deep neural networks</title>
		<author>
			<persName><forename type="first">Tilman</forename><surname>Ruker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anson</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Hadfield-Menell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 ieee conference on secure and trustworthy machine learning (satml)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="464" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">What Kind of Learning Is Machine Learning?</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Reigeluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Castelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Cultural Life of Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="79" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>Why should i trust you?</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Teaching yourself about structural racism will improve your machine learning</title>
		<author>
			<persName><forename type="first">Audrey</forename><surname>Whitney R Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashley</forename><forename type="middle">I</forename><surname>Renson</surname></persName>
		</author>
		<author>
			<persName><surname>Naimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="339" to="344" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature machine intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities</title>
		<author>
			<persName><forename type="first">Waddah</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Omlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">263</biblScope>
			<biblScope unit="page">110273</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Schrouff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mincu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Loreaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Blanes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08641</idno>
		<title level="m">Best of both worlds: local and global explanations with human-understandable concepts</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolando</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">G</forename><surname>Parameswaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.09125</idno>
		<title level="m">Operationalizing Machine Learning: An Interview Study</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Explanation Hacking: The perils of algorithmic recourse</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11843</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Adly</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trenton</forename><surname>Bricken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Ameisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoagy</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Callum</forename><surname>Mcdougall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monte</forename><surname>Macdiarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><forename type="middle">R</forename><surname>Sumers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" />
		<title level="m">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</title>
		<imprint>
			<publisher>Transformer Circuits Thread</publisher>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Variengien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Conmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buck</forename><surname>Shlegeris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.00593</idno>
		<title level="m">Interpretability in the wild: a circuit for indirect object identification in gpt-2 small</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Conceptual challenges for interpretable machine learning</title>
		<author>
			<persName><surname>David S Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Local explanations via necessity and sufficiency: unifying theory and practice</title>
		<author>
			<persName><forename type="first">Limor</forename><surname>David S Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Gultchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><surname>Floridi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1382" to="1392" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Responsibility and global justice: A social connection model</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social philosophy and policy</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="130" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Responsibility for justice</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
