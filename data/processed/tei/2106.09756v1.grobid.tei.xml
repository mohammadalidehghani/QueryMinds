<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python</title>
				<funder ref="#_kASZVKD">
					<orgName type="full">Wellcome Trust</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-06-17">17 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haiping</forename><surname>Lu</surname></persName>
							<email>h.lu@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sheffield Sheffield</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianyuan</forename><surname>Liu</surname></persName>
							<email>xianyuan.liu@sheffield.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sheffield Sheffield</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Turner</surname></persName>
							<email>r.d.turner@sheffield.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sheffield Sheffield</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peizhen</forename><surname>Bai</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The University of Sheffield Sheffield</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raivo</forename><forename type="middle">E</forename><surname>Koot</surname></persName>
							<email>rekoot1@sheffield.ac.uk</email>
							<affiliation key="aff4">
								<orgName type="institution">The University of Sheffield Sheffield</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">The University of Sheffield Sheffield</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mustafa</forename><surname>Chasmai</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Delhi New</orgName>
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<region>Delhi</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lawrence</forename><surname>Schobs</surname></persName>
							<email>laschobs1@sheffield.ac.uk</email>
							<affiliation key="aff7">
								<orgName type="institution">The University of Sheffield Sheffield</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-17">17 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">0FCD4647456D5B73E904BB78B960470F</idno>
					<idno type="arXiv">arXiv:2106.09756v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>machine learning</term>
					<term>deep learning</term>
					<term>domain adaptation</term>
					<term>multimodal learning</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning is a general-purpose technology holding promises for many interdisciplinary research problems. However, significant barriers exist in crossing disciplinary boundaries when most machine learning tools are developed in different areas separately. We present Pykale -a Python library for knowledge-aware machine learning on graphs, images, texts, and videos to enable and accelerate interdisciplinary research. We formulate new green machine learning guidelines based on standard software engineering practices and propose a novel pipeline-based application programming interface (API). PyKale focuses on leveraging knowledge from multiple sources for accurate and interpretable prediction, thus supporting multimodal learning and transfer learning (particularly domain adaptation) with latest deep learning and dimensionality reduction models. We build PyKale on PyTorch and leverage the rich PyTorch ecosystem. Our pipeline-based API design enforces standardization and minimalism, embracing green machine learning concepts via reducing repetitions and redundancy, reusing existing resources, and recycling learning models across areas. We demonstrate its interdisciplinary nature via examples in bioinformatics, knowledge graph, image/video recognition, and medical imaging.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine learning is the cornerstone technology for artificial intelligence (AI), driving many advances in our everyday lives and industrial sectors. AI research becomes more and more interdisciplinary as many problems rely on expertise from various domains. We have also witnessed many machine learning models transverse different research areas and disciplines. For example, the success of convolutional neural networks (CNNs) <ref type="bibr" target="#b25">[26]</ref> has spread from computer vision to graph analysis via graph convolutional networks (GCN) <ref type="bibr" target="#b22">[23]</ref> and medical imaging via U-net <ref type="bibr" target="#b48">[49]</ref>, and transformers <ref type="bibr" target="#b58">[59]</ref> developed in natural language processing (NLP) have become a hot topic in solving vision tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>With rapid development and growing interests in machine learning, many researchers hope to solve real-world interdisciplinary problems using machine learning. However, even with the popularity of open-source software and high-level scripting language such as Python, navigating the abundant choices and variety of machine learning software is not trivial. Researchers often run into barriers when adapting a machine learning tool for a new task of their interest. Solving complex real-world problems in practice often involve analyzing multiple sources of data, e.g., multiple modalities, multiple domains, and multiple knowledge bases. Most machine learning software packages are developed with a specific domain of application in mind. While popular generic packages such as PyTorch and TensorFlow support multiple domains and are not tailored for a specific domain, their focus on generic frameworks makes them inadequate to directly support interdisciplinary research where both flexible configurations and high-level integration are important.</p><p>In this paper, we propose PyKale, an open-source Python library to enable and accelerate interdisciplinary research via knowledgeaware multimodal learning and transfer learning on graphs, images, texts, and videos. It aims to fill the gaps between rich data sources, abundant machine learning libraries, and eager interdisciplinary researchers, with a focus on leveraging knowledge from multiple sources for accurate and interpretable prediction. To the best of our knowledge, this is the first publicly available Python library that considers both multimodal learning and transfer learning under a common framework of learning from multiple sources. It will make latest machine learning tools more accessible and accelerate the development of such tools. The name of the library consists of Py for Python, and Kale for Knowledge-aware learning.</p><p>PyKale proposes a novel pipeline-based application programming interface (API) to enforce standardization and minimalism, as shown in Figure <ref type="figure" target="#fig_1">1</ref>. It advocates our newly formulated green machine learning concepts of reducing repetitions and redundancy (Fig. <ref type="figure" target="#fig_5">2(a)</ref>), reusing existing resources (Fig. <ref type="figure" target="#fig_5">2(b</ref>)), and recycling learning models across areas (Fig. <ref type="figure" target="#fig_3">2</ref>  (a) Reduce (b) Reuse (c) Recycle (d) The PyKale logo engineering practices, extending them, and tailoring the philosophies to machine learning. We include examples in bioinformatics, knowledge graph, image/video recognition, and medical imaging in PyKale. This library was motivated by needs in healthcare applications and thus considers healthcare as a primary domain of usage.</p><p>PyKale is largely built on PyTorch and leverages many packages in the PyTorch ecosystem,<ref type="foot" target="#foot_0">foot_0</ref> with the aim to become part of it. Our logo in Fig. <ref type="figure" target="#fig_3">2</ref>(d) reflects the above characteristics, using an icon of simplified kale leaves. Specifically, the main contributions are fourfold:</p><p>‚Ä¢ We introduce the PyKale library for knowledge-aware machine learning.</p><p>It focuses on leveraging knowledge from multiple sources for accurate and interpretable prediction to enable and accelerate interdisciplinary research. ‚Ä¢ We propose a pipeline-based API for a standardized and minimal design to help break interdisciplinary barriers. We advocate green machine learning concepts of reduce, reuse, and recycle in such a design. ‚Ä¢ We demonstrate the usage of PyKale on real-world examples from multiple disciplines including bioinformatics, knowledge graph, image/video recognition, and medical imaging. ‚Ä¢ We provide many community-engaging features including a detailed documentation, a project board to show the progress and road map, and GitHub discussions open to all users.</p><p>The rest of this paper is organized as follows. We first review the state of the art open source software packages that are related to PyKale in Section 2. Then we discuss the design principles and API structure of PyKale in Section 3. Next, we describe the usage of PyKale in Section 4 and show example use cases from different applications in Section 5. Finally, we show the openness of our package and discuss its limitations and future developments in Section 6, with conclusions drawn in Section 7.</p><p>The PyKale library is publicly available at <ref type="url" target="https://github.com/pykale/pykale">https://github.com/  pykale/pykale</ref> with accompanying data (mainly for testing at the moment) at <ref type="url" target="https://github.com/pykale/data">https://github.com/pykale/data</ref> under an MIT license. PyKale can be installed from the Python Package Index (PyPI) via pip install pykale. PyKale documentation is hosted at <ref type="url" target="https://pykale.readthedocs.io">https://pykale.readthedocs.io</ref>. The primary targeted users are researchers and practitioners who have experience in Python and PyTorch programming and need to apply or develop machine learning systems taking data from multiple sources for prediction tasks, particularly in interdisciplinary areas such as healthcare. This paper refers to release version 0.1.0rc2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>As an open source project, we have learned from numerous libraries in the public domain to build ours. Here, we can only briefly mention several that have been particularly influential or relevant. In particular, we focus on those PyTorch-based libraries that we have frequently studied in our development, regretfully omitting many libraries, such as those based on TensorFlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PyTorch ecosystem</head><p>PyTorch is a popular open source machine learning library, particularly for computer vision and NLP applications. The PyTorch ecosystem has a rich collection of tools and libraries for the development of advanced machine learning and AI systems. PyKale aims to fill the gap within the PyTorch ecosystem to support more interdisciplinary research based on multiple data sources. Therefore, we make extensive usage of existing libraries from the PyTorch ecosystem to reduce duplicated implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">PyTorch</head><p>Lightning. PyTorch Lightning is a popular deep learning framework providing a high-level interface for PyTorch. By removing boilerplate code, it simplifies the development of research code and improves the reproducibility, flexibility, and readability of the resulting models <ref type="bibr" target="#b10">[11]</ref>. The goal of PyKale shares some similarity with PyTorch Lightning, but with a different focus on supporting interdisciplinary research. We have lots of inspirations from the design of PyTorch Lightning.</p><p>2.1.2 Other PyTorch libraries. PyKale depends on some other libraries from the PyTorch ecosystem including TensorLy for tensor analysis <ref type="bibr" target="#b23">[24]</ref>, TorchVision for computer vision <ref type="bibr" target="#b36">[37]</ref>, and PyTorch Geometric for graph analysis <ref type="bibr" target="#b12">[13]</ref>. We also learned from MONAI for medical image analysis <ref type="bibr" target="#b35">[36]</ref>, GPyTorch for Gaussian processes <ref type="bibr" target="#b15">[16]</ref>, Kornia for computer vision <ref type="bibr" target="#b47">[48]</ref>, and TorchIO for medical imaging preprocessing <ref type="bibr" target="#b44">[45]</ref>. These libraries have focuses different from ours on interdisciplinary research and multiple data sources.</p><p>The above libraries are listed as references at the end of our contributing guideline page. <ref type="foot" target="#foot_1">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal/transfer learning libraries</head><p>To the best of our knowledge, no other libraries in the PyTorch ecosystem have the same pipeline-based API design as PyKale. Several PyTorch-based libraries on multimodal learning or transfer learning are summarized below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">MultiModal Framework (MMF)</head><p>. MMF <ref type="bibr" target="#b51">[52]</ref> is the only library in the current PyTorch ecosystem focusing on multimodal learning of vision and language data in applications such as visual question answering, image captioning, visual dialog, hate detection and other vision and language tasks. PyKale differs from MMF not only in the API design, but also in the scientific fields covered and interdisciplinarity. PyKale aims to support interdisciplinary research such as medical imaging and drug discovery, and includes examples in these areas in the current release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.2</head><p>Transfer-Learning-Library. The Transfer-Learning-Library <ref type="bibr" target="#b20">[21]</ref> is a library on transfer learning, providing domain adaptation and fine tuning algorithms for computer vision applications. PyKale differs not only in the API design but also in the multiple modalities of data supported, including also graphs and texts, as well as in interdisciplinarity. <ref type="bibr" target="#b49">[50]</ref> is a library for multimodal recommender systems, leveraging auxiliary data (e.g., item descriptive text and image, social network, etc). PyKale differs from Cornac not only in the API design but also in the more diverse machine learning models and applications supported, not limited to recommender systems. <ref type="bibr" target="#b55">[56]</ref>, with a full name "(Yet) Another Domain Adaptation library", is an excellent package built on PyTorch Lightning for unsupervised and semi-supervised domain adaptation. We refactored ADA and made many changes for adaption to our pipeline-based API. We include a docstring at the top of each module adapted from ADA to indicate the source at ADA. Beyond a new API design, PyKale extends ADA substantially to support video domain adaptation and also supports non-vision data for interdisciplinary research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Cornac. Cornac</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">ADA. ADA</head><p>There are some other smaller libraries on multimodal or transfer learning that are more narrowly focused than the above, such as Multimodal-Toolkit <ref type="bibr" target="#b16">[17]</ref>. PyKale frames multimodal learning and transfer learning under one roof of knowledge-aware machine learning from multiple sources with a unified pipeline-based API, aiming to support interdisciplinary research rather than just popular vision or language tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PYKALE DESIGN 3.1 Green machine learning</head><p>Green machine learning (and green AI) is a scarcely used term referring to energy-efficient computing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>. Here, we propose a new green machine learning perspective for machine learning software development by formulating the 3R guiding principles below. We build these principles on standard software engineering practices by extending them and tailoring the philosophies to machine learning:</p><p>‚Ä¢ Reduce repetition and redundancy -Refactor code to standardize workflow and enforce styles, e.g., we refactored the deep drug-target binding affinity (DeepDTA) <ref type="bibr" target="#b63">[64]</ref> into our PyKale pipeline (Fig. <ref type="figure" target="#fig_1">1(c)</ref>). -Identify and remove duplicated functionalities, e.g., construct data loading API for popular dataset to share among different projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢ Reuse existing resources</head><p>-Reuse the same machine learning pipeline for different data and applications, such as using the same multilinear principal component analysis (MPCA) pipeline for gait <ref type="bibr" target="#b34">[35]</ref>, brain <ref type="bibr" target="#b52">[53]</ref>, and heart <ref type="bibr" target="#b54">[55]</ref>. -Reuse existing libraries (e.g., those in the PyTorch ecosystem, such as PyTorch Geometric) for available functionalities rather than implementing them again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢ Recycle learning models across areas</head><p>-Identify commonalities between applications, e.g., the similarity between commercial recommender systems (predicting user-item interactions) and drug discovery (predicting drug-target interactions).</p><p>-Recycle models for one application to another, e.g. from recommender systems <ref type="bibr" target="#b1">[2]</ref> to drug discovery <ref type="bibr" target="#b60">[61]</ref>.</p><p>Although the above is largely based on standard software engineering practices, this new formulation offers a new perspective to focus on core principles of standardization and minimalism. It has guided us to design a unique pipeline-based API to unify workflow, break barriers between areas and applications, and cross boundaries to fuse existing ideas and nurture new ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pipeline-based API</head><p>Inspired by the convenience of machine learning pipelines in machine learning library like Spark MLlib <ref type="bibr" target="#b37">[38]</ref> and scikit-learn <ref type="bibr" target="#b43">[44]</ref>, we design PyKale with a pipeline-based API as shown in Fig. <ref type="figure" target="#fig_1">1(a)</ref>. This design has six key steps and embodies our green machine learning principles above by organizing code along a standardized machine learning pipeline to identify commonalities, reduce redundancy, and minimize cognitive overhead.</p><p>1 # Load digits from multiple sources [digits_dann_lightn/main.py] 2 from kale.loaddata.digits_access import DigitDataset 3 from kale.loaddata.multi_domain import MultiDomainDatasets 4 5 source, target, _ = DigitDataset.get_source_target( 6 DigitDataset("MNIST"), DigitDataset("USPS"), data_path) 7 dataset = MultiDomainDatasets(source, target)) 8 9 # Preprocess digits [kale/loaddata/digits_access.py] import kale.prepdata.image_transform as image_transform self._transform = image_transform.get_transform(transform_kind) def get_train(self): return MNIST(data_path, train=True, transform=self._transform) # Embed digit representations [digits_dann_lightn/model.py] from kale.embed.image_cnn import SmallCNNFeature # Predict digit class and domain [digits_dann_lightn/model.py] from kale.predict.class_domain_nets import ClassNetSmallImage, DomainNetSmallImage # Build domain adaption pipeline [digits_dann_lightn/model.py] import kale.pipeline.domain_adapter as domain_adapter model = domain_adapter.create_dann_based(method="DANN", dataset=dataset, feature_extractor= SmallCNNFeature(), task_classifier=ClassNetSmallImage(), critic=DomainNetSmallImage(), **train_params) # Utility functions [digits_dann_lightn/main.py] from kale.utils.csv_logger import setup_logger from kale.utils.seed import set_seed Code 1: Code snippets from the source code for the digits domain adaptation example at pykale/examples/digits_dann_lightn/main.py to demonstrate the unified pipeline-based API, simplified for inclusion here.</p><p>In the following, we explain our unified API by starting with what the input and output are. We provide Python code snippets to help this explanation in Code 1, mainly using the domain adaptation for digit classification example with a pipeline in Fig. <ref type="figure" target="#fig_1">1(b</ref>). <ref type="foot" target="#foot_2">3</ref> Figure <ref type="figure" target="#fig_1">1(c)</ref> shows another pipeline for drug discovery. More code snippets are in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Load. The kale.loaddata module mainly takes source paths (local or online) as the input and constructs dataloaders for datasets as the output. Its primary function is to load data for input to the machine learning system/pipeline. See line 2-7 of Code 1 for an example of loading digit images from multiple sources and line 8-13 of Code 4 for an example of loading drug and targets data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>Preprocess. The kale.prepdata module takes the loaded raw input data as input and preprocesses (transforms) them into a suitable representation for the following machine learning modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.3</head><p>Embed. The kale.embed module takes preprocessed, normalized data representations to learn new representations in a new space as the output. It includes dimensionality reduction algorithms (feature extraction and feature selection), such as MPCA <ref type="bibr" target="#b34">[35]</ref> and CNNs. They can be viewed as encoders or embedding functions that learn suitable representations from data. This is a machine learning module. See line 17 of Code 1 for an example of (importing) a CNN feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.4</head><p>Predict. The kale.predict module takes the learned (or preprocessed, if skipping kale.embed ) representations to predict a desired target value as the output. Thus, this module provides prediction functions or decoders that learn a mapping from the input representation to a target prediction. This is also a machine learning module. See line 20-21 of Code 1 for an example of (importing) digit and domain classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.5</head><p>Evaluate. The kale.evaluate module evaluates the prediction performance using some metrics. We reuse metrics from other libraries (e.g., sklearn.metrics in line 4 of Code 5) and only implement metrics not commonly available, such as the Concordance Index (CI) <ref type="bibr" target="#b0">[1]</ref> for measuring the proportion of concordant pairs. See line 19 of Code 4 for its example usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.6</head><p>Interpret. The kale.interpret module aims to provide functions for interpretation of the learned features, the model, or the prediction results/outputs, e.g., via further analysis or visualization, and we only implement functions not commonly available. This module has implemented functions for selecting and visualizing weights from a trained model. See line 16-20 of Code 5 for an example of visualizing weights of a linear model for interpretation.</p><p>3.2.7 Pipeline. The kale.pipeline module provides mature, offthe-shelf machine learning pipelines for "plug-in usage". Its submodules typically specify a machine learning workflow by combining several other modules. See line 24-31 of Code 1 for an example of calling a domain adaptation pipeline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Machine learning models</head><p>Machine learning models in PyKale can be categorized into four main (possibly overlapping) groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Multimodal learning.</head><p>To support learning from data of multiple modalities, we need to first support learning from each individual modality. Thanks to the rich PyTorch ecosystem, we can build upon other libraries to have machine learning models supporting graphs, images, texts, and videos, primarily using PyTorch Dataloaders. The only missing major modality is audio but we have ongoing effort to include it in the near future, building upon torchaudio.</p><p>Learning from heterogeneous data sources and data integration can be viewed as multimodal learning as well. To this end, PyKale has built a DeepDTA <ref type="bibr" target="#b41">[42]</ref> pipeline kale.pipeline.deep_dti that learns from drug and target data, the chemical representation of which can be transformed into sequence or vector representations. PyKale also implemented our recent Graph information propagation Network (GripNet) <ref type="bibr" target="#b60">[61]</ref> kale.embed.gripnet for link prediction and data integration on heterogeneous knowledge graphs. PyKale has an example drug_gripnet to show the model usage on public bioinformatics knowledge graph with drug and protein's information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Transfer learning.</head><p>In transfer learning, PyKale currently focuses on domain adaptation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref>. We largely inherited the excellent, modular architecture from ADA <ref type="bibr" target="#b55">[56]</ref>, covering many important semi-supervised and unsupervised domain adaptation algorithms, such as domain-adversarial neural networks (DANN) <ref type="bibr" target="#b13">[14]</ref>, conditional adversarial domain adaptation networks (CDAN) <ref type="bibr" target="#b32">[33]</ref>, deep adaptation networks (DAN) <ref type="bibr" target="#b31">[32]</ref> and joint adaptation networks (JAN) <ref type="bibr" target="#b33">[34]</ref>, and Wasserstein distance guided representation learning (WDGRL) <ref type="bibr" target="#b50">[51]</ref>. These algorithms are applicable to all modalities with appropriate representations. PyKale currently has two pipelines for domain adaptation: domain_adapter and video_domain_adapter in kale.pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Deep learning. PyKale builds deep neural networks (DNNs)</head><p>upon the PyTorch API. Current implementations include CNNs <ref type="bibr" target="#b25">[26]</ref> / 3D CNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b56">57]</ref>, GCNs <ref type="bibr" target="#b22">[23]</ref>, and attention-based networks such as transformers <ref type="bibr" target="#b58">[59]</ref> and squeeze and excitation networks <ref type="bibr" target="#b17">[18]</ref> (see more in Section 5). We use TorchVision <ref type="bibr" target="#b36">[37]</ref>, PyTorch Geometric <ref type="bibr" target="#b12">[13]</ref>, and PyTorch Lightning <ref type="bibr" target="#b10">[11]</ref> in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Dimensionality reduction.</head><p>PyKale has built a Python version of the MPCA algorithm <ref type="bibr" target="#b34">[35]</ref> at kale.embed.mpca, as well as an MPCA-based pipeline at kale.pipeline.mpca_trainer, using both the scikit-Learn library <ref type="bibr" target="#b43">[44]</ref> and the TensorLy library <ref type="bibr" target="#b23">[24]</ref>. This pipeline has been successfully used for interpretable prediction in gait recognition from video sequences <ref type="bibr" target="#b34">[35]</ref>, cardiovascular disease diagnosis <ref type="bibr" target="#b54">[55]</ref> and prognosis <ref type="bibr" target="#b57">[58]</ref> using cardiac magnetic resonance imaging (MRI), and brain state classification using functional MRI (fMRI). We are further building into PyKale other advanced tensorbased algorithms such as regularized Multilinear Regression and Selection (Remurs) <ref type="bibr" target="#b52">[53]</ref> and sparse tubal-regularized multilinear regression (Sturm) <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Software engineering</head><p>The PyKale team includes machine learning researchers and Research Software Engineers (RSEs). We have adopted good software engineering practices in a research context, often based on other libraries, particularly those in the PyTorch ecosystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Version control and collaboration.</head><p>We use git for version control and GitHub for collaborative working. The PyKale repository stipulates a license (MIT) and contributing guidelines. 4 These enable reuse of the software and sustainability of the project through 4 <ref type="url" target="https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.md">https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.md</ref> community contributions. This is a platform for long term availability of the resource and lays the foundation for community maintenance over an indefinite period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Documentation.</head><p>We use "docstrings" to embed documentation within the source code to maximize synchronicity between code and documentation. Sphinx<ref type="foot" target="#foot_3">foot_3</ref> is used to automatically build these (along with additional information in "reStructuredText" format) into html docs. Documentation is published via readthedocs. com and is kept up to date by Continuous Integration (CI). This is useful for keeping users and developers up-to-date with new features and bug fixes in a sustainable way. Detailed installation instructions are included. <ref type="foot" target="#foot_4">6</ref>3.4.3 Tests and continuous integration. We use the PyTest framework and currently have 88% test coverage. The test suite can be run locally, and also runs automatically on GitHub and must pass for code to be merged into the main branch. This ensures that new features do not create unintended side-effects. CI is implemented using GitHub workflows/actions. <ref type="foot" target="#foot_5">7</ref> Our CI checks include static analysis, pre-commit checks (e.g., maximum file size), documentation building (via Read the Docs), changelog update, project assignment for issues and pull requests, PyPI release of packages, PyTest tests on multiple platforms and multiple python versions, and Codecov code coverage report. This ensures that the version in the main branch is always the most up to date working version, and meets our standards of functionality and coding style.</p><p>To maintain a small repository size (currently less than 1MB), we store test data in a separate repository at <ref type="url" target="https://github.com/pykale/data">https://github.com/  pykale/data</ref>, which can be downloaded via download_file_by_url in kale.utils.download automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PYKALE USAGE</head><p>Interdisciplinary research is a complex subject to support and care has to be taken to lower the barriers to entry. PyKale includes examples and tutorials to help users' exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Usage of pipeline-based API in examples</head><p>PyKale examples are highly standardized. Each example typically has three essential modules (main.py, config.py, model.py), one optional directory (configs), and possibly other modules (trainer.py):</p><p>‚Ä¢ main.py is the main module to be run, showing the main workflow. ‚Ä¢ config.py is the configuration module that sets up the data, prediction problem, hyper-parameters, etc. The settings in this module are the default configuration. ‚Ä¢ configs is the directory to place customized configurations for individual runs. We use .yaml files (see Section 4.3) for this purpose. ‚Ä¢ model.py is the model module to define the machine learning model and configure its training parameters. ‚Ä¢ trainer.py is the trainer module to define the training and testing workflow. This module is only needed when NOT using PyTorch Lightning.</p><p>Lu, et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Building new modules or projects</head><p>Users can build new modules or projects following the steps below.</p><p>‚Ä¢ Step 1 -Examples: Choose one of the examples of the users' interest (e.g., most relevant to the users' project) to browse through the configuration, main, and model modules, -download the data if needed, and run the example following instructions in the example's README.</p><p>‚Ä¢ Step 2a -New model: To develop new machine learning models under PyKale, -define the blocks in the users' pipeline to figure out what the methods are for data loading, preprocessing data, embedding (encoder/representation), prediction (decoder), evaluation, and interpretation, and modify existing pipelines with the users' customized blocks or build a new pipeline with pykale blocks and blocks from other libraries. ‚Ä¢ Step 2b -New applications: To develop new applications using PyKale, -clarify the input data and the prediction target to find matching functionalities in pykale (request if not found), and tailor data loading, preprocessing, and evaluation (and interpretation if needed) to the users' application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">YAML configuration</head><p>PyKale examples configure a machine learning system using YAML <ref type="bibr" target="#b3">[4]</ref>. This is inspired by the usage of YAML in the GitHub package for the Isometric Network (ISONet) <ref type="bibr" target="#b46">[47]</ref>, 8 with our adapted version illustrated in Code 2. As modern machine learning systems typically have many settings to configure, specifying many/all settings in command line or Python modules becomes difficult to manage and read. Using YAML greatly improves the readability and reproducibility, and makes configuration changes much easier, via a default configuration specified in config.py (top of Code 2) and customized configurations specified in a respective .yaml file (bottom of Code 2), which will be merged to overwrite the default setting at run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Notebook tutorials with Binder and Colab</head><p>We have eight real-world examples of PyKale usage. 9 However, tutorials without the need of any installation are important for new users to get familiar with the PyKale workflow and API. For these we must scale-back on real-world datasets due to the computational resources needed, as these lead to long runtimes, unsuitable for interactive learning. Therefore, we are simplifying our examples into Jupyter notebook tutorials so that each tutorial takes minutes instead of hours to run. This will strike a balance between computational requirements and runtime, without resorting to toy examples.</p><p>8 <ref type="url" target="https://github.com/HaozhiQi/ISONet">https://github.com/HaozhiQi/ISONet</ref> 9 <ref type="url" target="https://github.com/pykale/pykale/tree/main/examples">https://github.com/pykale/pykale/tree/main/examples</ref> 1 # The file config.py that defines the default configuration 2 from yacs.config import CfgNode as CN 3 4 # Config definition 5 _C = CN() 6 7 # Dataset 8 _C.DATASET = CN() 9 _C.DATASET.ROOT = "../data" 10 _C.DATASET.NAME = "CIFAR10" 11 12 # Solver 13 _C.SOLVER = CN() 14 _C.SOLVER.SEED = 2020 15 _C.SOLVER.BASE_LR = 0.05 16 _C.SOLVER.TRAIN_BATCH_SIZE = 128 17 _C.SOLVER.MAX_EPOCHS = 100 18 19 # ISONet configs 20 _C.ISON = CN() 21 _C.ISON.DEPTH = 34 22 23 # Misc options 24 _C.OUTPUT_DIR = "./outputs" 1 # Customization in a .yaml file 2 SOLVER: 3 BASE_LR: 0.01 4 MAX_EPOCHS: 10 # For quick testing 5 ISON: 6 DEPTH: 38 To bring further convenience, we set up cloud-based services with both Binder<ref type="foot" target="#foot_6">foot_6</ref> and Google Colaboratory (Colab) <ref type="foot" target="#foot_7">11</ref> for our notebook tutorials so that any users can run PyKale tutorials without the need of any installation. The first such tutorial has been released, <ref type="foot" target="#foot_8">12</ref> with screenshots in Figure <ref type="figure" target="#fig_7">3</ref>. More such tutorials are in development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USE CASES: PYKALE EXAMPLES</head><p>PyKale currently has example applications from three areas below:</p><p>‚Ä¢ Image/video recognition: classification of images (objects, digits) or videos (actions in first-person videos); ‚Ä¢ Bioinformatics/graph analysis: prediction of links between entities in knowledge graphs (BindingDB, BioSNAP-Decagon); ‚Ä¢ Medical imaging: disease diagnosis from cardiac MRIs.</p><p>The above examples deal with graphs, images, and videos. Our current APIs support text processing (e.g., for NLP tasks) but an example in this area is still in development. We are also conducting research in integrating audio features in action recognition so an</p><p>(a) Binder (b) Google Colab example involving audio data is a future task as well. Examples in computer vision applications such as image and video recognition are a good start for most users due to the popularity of vision applications and a low barrier to entry (e.g., no need for specific domain knowledge as in drug discovery). Models first developed in computer vision can be reused or recycled for other applications.</p><p>The data used in PyKale examples are real-world data frequently used in research papers. Subsequently, it may take quite some time to finish running these examples. For quick running and demonstration of the workflow, tutorials (Section 4.4) are simplified examples that serve as a better starting point. The following subsections give an overview of the data and algorithms used in the example machine learning systems in PyKale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CIFAR and digits classification</head><p>Small-image datasets are good for building examples of real-world relevance. PyKale has three such examples: two on the CIFAR datasets <ref type="bibr" target="#b24">[25]</ref> and one on digits datasets including MNIST <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>, modified MNIST <ref type="bibr" target="#b13">[14]</ref>, USPS <ref type="bibr" target="#b19">[20]</ref>, and SVHN <ref type="bibr" target="#b39">[40]</ref>. cifar_isonet is the first example in PyKale refactoring the ISONet code <ref type="url" target="https://github.com/HaozhiQi/ISONet">https:  //github.com/HaozhiQi/ISONet</ref> on CIFAR10 and CIFAR100 into the PyKale API. cifar_cnntransformer is another example on CIFAR showing the simple application of a mixed CNN and transformer model for vision tasks. Code 1 has shown code snippets of digits classification via domain adaptation using MNIST as the source domain and USPS as the target domain, in the digits_dann_lightn example.</p><p>Examples on these popular datasets could help users familiar with them make easy connections between the PyKale API and what they are already familiar with. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Action recognition via domain adaptation</head><p>5.2.1 Data. For this action recognition example, we constructed three first-person vision datasets, ADL ùë†ùëöùëéùëôùëô , GTEA-KITCHEN and EPIC ùëêùë£ùëùùëü 20 , with 222/454/10094 action videos respectively. We selected and reorganized three videos from ADL dataset <ref type="bibr" target="#b45">[46]</ref> as three domains of ADL ùë†ùëöùëéùëôùëô , re-annotated GTEA <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> and KITCHEN <ref type="bibr" target="#b7">[8]</ref> datasets to build GTEA-KITCHEN, and adopted the public dataset EPIC ùëêùë£ùëùùëü 20 <ref type="bibr" target="#b38">[39]</ref>. We will provide instructions on how to construct these datasets from the source at <ref type="url" target="https://github.com/pykale/data/tree/main/video_data/video_test_data">https://github.com/  pykale/data/tree/main/video_data/video_test_data</ref> (in progress). Each dataset has two modalities: RGB and optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Algorithms.</head><p>For video feature extraction, we built two stateof-the-art action recognition algorithms, I3D <ref type="bibr" target="#b6">[7]</ref> and 3D ResNet <ref type="bibr" target="#b56">[57]</ref>, into PyKale. For domain adaptation, we extended the domain adaptation framework for images (digits, adapted from <ref type="bibr" target="#b55">[56]</ref>) to videos. We followed the same pipeline as digits classification while providing additional specific functions for action videos and multimodal data. As shown in Code 3, the image modality parameter can be set to choose the proper data transform and loader for different modalities: RGB, optical flow, and joint, and all video feature extractors are accessed via a unified interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Drug-target interaction prediction</head><p>5.3.1 Data. Predicting the binding affinity between drug compounds and target proteins is fundamental for drug discovery and drug repurposing. This example uses three public datasets (for three metrics Ki, Kd and IC50) from BindingDB <ref type="bibr" target="#b30">[31]</ref> containing 52,284, 375,032, and 991,486 interaction pairs respectively, accessed via the Therapeutics Data Commons (TDC) platform <ref type="bibr" target="#b18">[19]</ref>. Given the amino acid sequences of targets and SMILES (Simplified Molecular Input Line Entry System) strings of drug compounds, the task is to predict drug-target binding affinity. Following <ref type="bibr" target="#b21">[22]</ref>, the affinity metrics are transformed into the logarithm form for more stable training and validation. behind PyKale. This paper is another effort to reach out to the wider research community to share this resource and get feedback for further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Limitations and future development</head><p>PyKale is an open-source project started in June 2020, with the first PyPI release in January 2021. It was motivated by the growing needs for machine learning systems that can deal with multiple sources of data, particularly in interdisciplinary areas such as healthcare. For example, clinicians often need to make use of a combination of medical images (e.g., X-rays, CTs, MRIs), biological data (gene, DNA, RNA), and electronic health record for decision making. Our focus on multimodal learning and transfer learning has defined a challenging scope, while holding the promises to break barriers in interdisciplinary research.</p><p>To date, PyKale has built APIs supporting machine learning from graphs, images, texts, and videos, with four mature pipelines implemented. Nevertheless, we do not have an example on text data yet, and we have not built APIs for audio yet. Developing projects involving multiple data sources takes considerably longer time than developing those involving a single data source. The current version of PyKale has two examples on multimodal learning involving heterogeneous drug and target data and two examples on domain adaptation (transfer learning) for images and videos. These examples laid solid foundations for us to grow our research in these areas and build more advanced examples in future development. In addition, our tests currently have a coverage of 88%. We need further improvements for a higher coverage and more rigorous tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we have introduced PyKale, a Python library for knowledge-aware machine learning from multiple sources, particularly from multiple modalities for multimodal learning and from multiple domains for transfer learning. This library was motivated by needs in healthcare applications (hence the acronym kale, a healthy vegetable) and aims to enable and accelerate interdisciplinary research. Building on standard software engineering practices, we proposed a new green machine learning perspective to advocate reducing repetitions and redundancy, reusing existing resources, and recycling learning models across areas. Following such principles, we designed our API to be pipeline-based to unify the workflow and increase the flexibility. This design can help to break barriers between different areas or applications and facilitate the fusion and nurture of ideas across discipline boundaries.</p><p>The goal of PyKale is to facilitate interdisciplinary, knowledgeaware machine learning research for graphs, images, texts, and videos. It will make it easier to bring machine learning models developed in one area to the other, and integrate data from multiple sources for prediction tasks in interdisciplinary areas. Its focus on leveraging knowledge from multiple sources also helps accurate and interpretable prediction. To demonstrate such potential, we have shown example applications including bioinformatics, knowledge graph, image/video recognition, and medical imaging on real-world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>(c)) by building on standard software Drug target interaction prediction pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proposed pipeline-based API in PyKale and two real-world examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Green machine learning concepts in PyKale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 2 . 8</head><label>28</label><figDesc>Utilities. The kale.utils module provides common utility functions, such as setting random seeds, logging results, or downloading data. See line 34-35 of Code 1 for examples of importing the seed-setting and csv-logging submodules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Code 2 :</head><label>2</label><figDesc>Code snippets to demonstrate the usage of YAML to configure machine learning systems in PyKale, from pykale/examples/cifar_isonet/config.py, which is adapted from https://github.com/HaozhiQi/ISONet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PyKale digits domain adaptation example on cloudbased services.</figDesc><graphic coords="7,77.82,226.88,192.20,79.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 # 4 5 13 #</head><label>1413</label><figDesc>Transform and dataset for multi-modal video data 2 from kale.prepdata.video_transform import get_transform 3 from kale.loaddata.video_datasets import BasicVideoDataset transform = get_transform(transform_kind, image_modality) 6 dataset = BasicVideoDataset(data_path, train_list, 7 imagefile_template="frame_{:010d}.jpg" 8 if image_modality in ["rgb"] 9 else "flow_{}_{:010d}.jpg", Action video feature extractor 14 from kale.embed.video_feature_extractor import 15 get_video_feat_extractor 16 feature_extractor = get_video_feat_extractor("I3D", image_modality, 17 attention, num_classes) Code 3: Code snippets to demonstrate the example on action recognition via domain adaptation.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://pytorch.org/ecosystem/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.md</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/pykale/pykale/tree/main/examples/digits_dann_lightn Preprocessing steps include data normalization, augmentation, and other transformations of data representation not involving machine learning. Its submodules are typically imported in kale.loaddata. See line 10-14 of Code 1 for an example of standardizing digit images with predefined transforms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://www.sphinx-doc.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://pykale.readthedocs.io/en/latest/installation.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/pykale/pykale/tree/main/.github/workflows</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>https://mybinder.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p>https://colab.research.google.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8"><p>https://github.com/pykale/pykale/blob/main/examples/digits_dann_lightn/tutorial. ipynb</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The development of PyKale is partially supported by the Innovator Awards: Digital Technologies from the <rs type="funder">Wellcome Trust</rs> (grant <rs type="grantNumber">215799/Z/19/Z</rs>). We thank the support and contributions from <rs type="person">David Jones</rs>, <rs type="person">Will Furnass</rs>, and other members of the <rs type="institution">Research Software Engineering (RSE)</rs> team headed by <rs type="person">Paul Richmond</rs>. We also thank early users of our library and their helpful feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kASZVKD">
					<idno type="grant-number">215799/Z/19/Z</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code 4: Demonstration code for drug-target interaction prediction with DeepDTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Algorithms.</head><p>For this drug-target interaction prediction problem, we built DeepDTA <ref type="bibr" target="#b41">[42]</ref> into PyKale, a typical CNN-based model with encoder-decoder architecture. The drug SMILES string and target amino acid sequence are encoded by their independent CNNs, and then a multilayer perceptron is used to decode the affinity from the drug-target combined encoding. We refactored DeepDTA into the PyKale API structure with separate modules for load data, preprocess data, encode (embed) and decode (predict). These refactored modules are flexible and can be reused across different applications. Code 4 illustrates the usage of the DeepDTA model as implemented in PyKale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Polypharmacy side effect prediction via</head><p>knowledge graph link prediction 5.4.1 Data. Polypharmacy uses drug combination to treat complex diseases, which may cause side effects. Predicting such side effects can be formulated as a link prediction problem on knowledge graphs of drugs and proteins <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>. This example uses the public BioSNAP-Decagon dataset <ref type="bibr" target="#b62">[63]</ref> with 6,075,428 edges, 1,100 different edge labels, and three types of edges: drug-drug interaction, protein-protein interaction and drug-target interaction.</p><p>The drug-drug interactions model polypharmacy side effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Algorithms.</head><p>We built our recent GripNet <ref type="bibr" target="#b60">[61]</ref> into PyKale for this example. GripNet is a subgraph network framework for multirelational link prediction on heterogeneous graphs via segregated node representation learning on "supervertices" and "superedges". PyKale implements APIs to support these advanced concepts for node embedding and link prediction on heterogeneous graphs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Cardiac MRI classification</head><p>5.5.1 Data. The data used to build this example are a subset of the dataset used in <ref type="bibr" target="#b54">[55]</ref>, which consists of cardiac MRI (CMRI) sequences acquired from patients with pulmonary arterial hypertension and health controls. They are not yet in the public domain but we are exploring release options. The CMRI sequences are standardized using methods in kale.prepdata.image_transform. 5.5.2 Algorithms. This example uses the MPCATrainer pipeline in kale.pipeline.mpca_trainer, which implemented the machine learning pipeline used in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> in the scikit-learn style. The three key steps are MPCA dimensionality reduction, feature selection, and classification (SVM, Linear SVM, or logisitic regression), where the feature selection and classification algorithms reuse respective APIs in scikit-learn <ref type="bibr" target="#b43">[44]</ref>. Code 5 shows the steps for MPCAbased prediction and interpretation on cardiac MRI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">PYKALE OPENNESS AND PLAN 6.1 License and community engagement</head><p>PyKale is publicly available at <ref type="url" target="https://github.com/pykale/pykale">https://github.com/pykale/pykale</ref> under an MIT license, which is a simple permissive license with minimal restrictions. The PyKale GitHub repository has active discussion board at <ref type="url" target="https://github.com/pykale/pykale/discussions">https://github.com/pykale/pykale/discussions</ref> and project board at <ref type="url" target="https://github.com/pykale/pykale/projects">https://github.com/pykale/pykale/projects</ref> to interact with users and make the development process and plan transparent to all users. Complete documentation is hosted at <ref type="url" target="https://pykale.readthedocs.io/">https://pykale.readthedocs.io/</ref> with multiple versions available, generated automatically from <ref type="url" target="https://github.com/pykale/pykale/tree/main/docs">https://github.com/pykale/pykale/tree/  main/docs</ref>. We have provided tutorials and examples as well as detailed contributing guidelines at <ref type="url" target="https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.md">https://github.com/pykale/pykale/  blob/main/.github/CONTRIBUTING.md</ref> and change logs at <ref type="url" target="https://github.com/pykale/pykale/blob/main/.github/CHANGELOG.md">https://  github.com/pykale/pykale/blob/main/.github/CHANGELOG.md</ref>.</p><p>We also released a 12-minute YouTube video at <ref type="url" target="https://youtu.be/i5BYdMfbpMQ">https://youtu.  be/i5BYdMfbpMQ</ref> to briefly explain the motivation and principles</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint Concordance Index</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 53rd Asilomar Conference on Signals, Systems, and Computers</title>
		<meeting>the 2019 53rd Asilomar Conference on Signals, Systems, and Computers</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2206" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint interaction with context operation for collaborative filtering</title>
		<author>
			<persName><forename type="first">Peizhen</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="729" to="738" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Yaml ain&apos;t markup language (yaml‚Ñ¢) version 1.1. Working Draft</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Ben-Kiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ingerson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2009. 2008. 2009</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Green machine learning via augmented Gaussian processes and multi-information source optimization</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Candelieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Archetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Guide to the carnegie mellon university multimodal activity (cmu-mmac) database</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Bargteil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Macey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Collado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pep</forename><surname>Beltran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The mnist database of handwritten digit images for machine learning research</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><surname>Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning" />
	</analytic>
	<monogr>
		<title level="j">PyTorch Lightning. GitHub</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran√ßois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Energy efficiency in machine learning: A position paper</title>
		<author>
			<persName><forename type="first">Eva</forename><surname>Garc√≠a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mart√≠n</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual Workshop of the Swedish Artificial Intelligence Society</title>
		<meeting>the 30th Annual Workshop of the Swedish Artificial Intelligence Society</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="68" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7587" to="7597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><surname>Georgian</surname></persName>
		</author>
		<ptr target="https://github.com/georgian-io/Multimodal-Toolkit" />
		<title level="m">Multimodal-Toolkit. GitHub</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09548</idno>
		<title level="m">Therapeutics Data Commons: Machine Learning Datasets and Tasks for Therapeutics</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transfer-Learning-library</title>
		<author>
			<persName><forename type="first">Junguang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<ptr target="https://github.com/thuml/Transfer-Learning-Library" />
	</analytic>
	<monogr>
		<title level="j">GitHub</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepAffinity: interpretable deep learning of compound-protein affinity through unified recurrent and convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3329" to="3338" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TensorLy: Tensor Learning in Python</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Kernel methods and machine learning</title>
		<author>
			<persName><forename type="first">Sun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kung</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">The MNIST database of handwritten digits</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sturm: Sparse tubalregularized multilinear regression for fmri</title>
		<author>
			<persName><forename type="first">Wenwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Machine Learning in Medical Imaging</title>
		<meeting>the International Workshop on Machine Learning in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="256" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhefan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BindingDB: a web-accessible database of experimentally determined protein-ligand binding affinities</title>
		<author>
			<persName><forename type="first">Tiqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhmei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jorissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="198" to="D201" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional Adversarial Domain Adaptation</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MPCA: Multilinear principal component analysis of tensor objects</title>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><forename type="middle">N</forename><surname>Venetsanopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="18" to="39" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nic</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4891800</idno>
		<idno>Project-MONAI/MONAI: 0.5.3</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4891800" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Torchvision the Machine-Vision Package of Torch</title>
		<author>
			<persName><forename type="first">S√©bastien</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimedia</title>
		<meeting>the 18th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1485" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mllib: Machine learning in apache spark</title>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burak</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davies</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Amde</surname></persName>
		</author>
		<author>
			<persName><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1235" to="1241" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-modal domain adaptation for fine-grained action recognition</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reading Digits in Natural Images with Unsupervised Feature Learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Predicting Polypharmacy Side-effects Using Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">V√≠t</forename><surname>Nov√°ƒçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sameh</surname></persName>
		</author>
		<author>
			<persName><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMIA Joint Summits on Translational Science</title>
		<meeting>the AMIA Joint Summits on Translational Science</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DeepDTA: deep drug-target binding affinity prediction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hakime √ñzt√ºrk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzucan</forename><surname>Olmez</surname></persName>
		</author>
		<author>
			<persName><surname>√ñzg√ºr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="821" to="829" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">TorchIO: a Python library for efficient loading, preprocessing, augmentation and patchbased sampling of medical images in deep learning</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>P√©rez-Garc√≠a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2003.04696" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep isometric learning for visual recognition</title>
		<author>
			<persName><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7824" to="7835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Kornia: an open source differentiable computer vision library for pytorch</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cornac: A Comparative Framework for Multimodal Recommender Systems</title>
		<author>
			<persName><forename type="first">Aghiles</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc-Tuan</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hady</forename><forename type="middle">W</forename><surname>Lauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">MMF: A multimodal framework for vision and language research</title>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/mmf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multilinear regression for embedded feature selection with application to fmri analysis</title>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning tensor-based features for whole-brain fMRI classification</title>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingnan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiquan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="613" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A machine learning cardiac magnetic resonance approach to extract disease features and automate pulmonary arterial hypertension diagnosis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johanna</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Uthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcella</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cogliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Metherall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samer</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><surname>Alabed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Heart Journal-Cardiovascular Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="236" to="245" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Anne-Marie</forename><surname>Tousch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Renaudin</surname></persName>
		</author>
		<ptr target="https://github.com/criteo-research/pytorch-ada" />
		<title level="m">Another Domain Adaptation library</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Geodesically Smoothed Tensor Features for Pulmonary Hypertension Prognosis Using the Heart and Surrounding Tissues</title>
		<author>
			<persName><forename type="first">Johanna</forename><surname>Uthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samer</forename><surname>Alabed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
	<note>Non-local neural networks</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Grip-Net: Graph Information Propagation on Supergraph for Heterogeneous Graphs</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengqi</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhen</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15914[cs.LG]</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rok</forename><surname>Sosiƒç</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">BioSNAP Datasets: Stanford Biomedical Network Dataset Collection</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">DeepDTA: deep drug-target binding affinity prediction</title>
		<author>
			<persName><forename type="first">Hakime</forename><surname>√ñzt√ºrk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzucan</forename><surname>√ñzg√ºr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elif</forename><surname>Ozkirimli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="821" to="829" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
