<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Top 10 Topics in Machine Learning Revisited: A Quantitative Meta-Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-03-29">29 Mar 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Glauner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interdisciplinary Centre for Security, Reliability and Trust</orgName>
								<orgName type="institution">University of Luxembourg</orgName>
								<address>
									<addrLine>4 rue Alphonse Weicker</addrLine>
									<postCode>2721</postCode>
									<settlement>Luxembourg</settlement>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manxing</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interdisciplinary Centre for Security, Reliability and Trust</orgName>
								<orgName type="institution">University of Luxembourg</orgName>
								<address>
									<addrLine>4 rue Alphonse Weicker</addrLine>
									<postCode>2721</postCode>
									<settlement>Luxembourg</settlement>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Victor</forename><surname>Paraschiv</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Numbers of others London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrey</forename><surname>Boytsov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interdisciplinary Centre for Security, Reliability and Trust</orgName>
								<orgName type="institution">University of Luxembourg</orgName>
								<address>
									<addrLine>4 rue Alphonse Weicker</addrLine>
									<postCode>2721</postCode>
									<settlement>Luxembourg</settlement>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Isabel</forename><forename type="middle">LÃ³pez</forename><surname>Andrade</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">American Express Sussex House</orgName>
								<address>
									<addrLine>Civic Way Burgess Hill</addrLine>
									<postCode>RH15 9AQ</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jorge</forename><forename type="middle">Augusto</forename><surname>Meira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interdisciplinary Centre for Security, Reliability and Trust</orgName>
								<orgName type="institution">University of Luxembourg</orgName>
								<address>
									<addrLine>4 rue Alphonse Weicker</addrLine>
									<postCode>2721</postCode>
									<settlement>Luxembourg</settlement>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Petko</forename><surname>Valtchev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Radu</forename><surname>State</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interdisciplinary Centre for Security, Reliability and Trust</orgName>
								<orgName type="institution">University of Luxembourg</orgName>
								<address>
									<addrLine>4 rue Alphonse Weicker</addrLine>
									<postCode>2721</postCode>
									<settlement>Luxembourg</settlement>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Quebec</orgName>
								<address>
									<addrLine>in Montreal 201, av. President Kennedy</addrLine>
									<postCode>H2X 3Y7</postCode>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Top 10 Topics in Machine Learning Revisited: A Quantitative Meta-Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-03-29">29 Mar 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">AFFE4E89633A2EC83DCC3099182E382E</idno>
					<idno type="arXiv">arXiv:1703.10121v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Which topics of machine learning are most commonly addressed in research? This question was initially answered in 2007 by doing a qualitative survey among distinguished researchers. In our study, we revisit this question from a quantitative perspective. Concretely, we collect 54K abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences. We then use machine learning in order to determine the top 10 topics in machine learning. We not only include models, but provide a holistic view across optimization, data, features, etc. This quantitative approach allows reducing the bias of surveys. It reveals new and up-to-date insights into what the 10 most prolific topics in machine learning research are. This allows researchers to identify popular topics as well as new and rising topics for their research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In 2007, a paper named "Top 10 algorithms in data mining" identified and presented the top 10 most influential data mining algorithms within the research community <ref type="bibr" target="#b0">[1]</ref>. The selection criteria were created by consolidating direct nominations from award winning researchers, the research community opinions and the number of citations in Google Scholar. The top 10 algorithms in that prior work are: C4.5, k-means, support vector machine, Apriori, EM, PageRank, Ad-aBoost, kNN, naive Bayes and CART.</p><p>In the decade that passed since then, machine learning has expanded, responding to incremental development of computational capabilities and substantial increase of problems in the commercial applications. This study reflects on the top 10 most popular fields of active research in machine learning, as they emerged from the quantitative analysis of leading journals and conferences. This work sees some topics in the broader sense including not only models but also concepts like data sets, features, optimization techniques and evaluation metrics. This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models <ref type="bibr" target="#b1">[2]</ref>.</p><p>Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts. This attempt aims at reducing bias and looking where the research community puts its focus on. The results of this study allow researchers to put their research into the global context of machine learning. This provides researchers with the opportunity to both conduct research in popular topics and identify topics that have not received sufficient attention in recent research. The rest of this paper is organized as follows. Section 2 describes the data sources and quantitative methodology. Section 3 presents and discusses the top 10 topics identified. Section 4 summarizes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we discuss how we determine quantitatively the top 10 topics in machine learning from articles of leading journals and conferences published between January 2007 and June 2016. We selected referenced journals that cover extensively the field of machine learning, neural networks, pattern recognition and data mining both from the theoretical perspective and also with applications on image, video and text processing, inference on networks and graphs, knowledge basis and applications on real data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data collection</head><p>In the data collection, we focus on the abstracts of publications, as they provide the main results and conclusions of a paper. In contrast, the full text includes details on the research, which also comes with more noise that is not relevant to an overall summary of published work. We have chosen 31 leading journals related to machine learning as summarized in Table <ref type="table" target="#tab_0">1</ref>, ranked by their impact factor. For each journal, we have collected as many abstracts as possible of articles published in the timeframe of interest. In total, we have collected 39,067 abstracts of those 31 journals, which also include special issues. a Computing the average citation count of this mixture of various conferences and workshops has proven to not be feasible. Instead, we use the impact factor of the Journal of Machine Learning Research as the average citation count. We expect the impact of the approximation error to be low since it only concerns 1,507 of the total 53,526 abstracts used in this research.</p><p>Table 2: Source conferences. and Conference Proceedings series, which includes further conferences, such as International Conference on Artificial Intelligence and Statistics and Asian Conference on Machine Learning among others. We have collected 14,459 abstracts from the proceedings of those conferences in the time frame of interest. Combining the journals and conference proceedings, we have collected 53,526 abstracts in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Key phrase extraction</head><p>We focus on extracting the most relevant key phrases of each abstract, which we call topics in the remainder of this study. First, we apply Porter stemming to an abstract <ref type="bibr" target="#b2">[3]</ref>. In stemming, only the stem of a word is retained. For example, "paper" and "papers" have the same stem, which is "paper". For the extraction of key phrases from each abstract, we compare two different methods:</p><p>1. We remove the stop words 1 from each abstract and then use all bigrams and trigrams as key phrases. 2. The Rapid Automatic Keyword Extraction Algorithm (RAKE) is an unsupervised, domain-independent and language-independent learning algorithm that generates key phrases from text <ref type="bibr" target="#b3">[4]</ref>. First, RAKE splits each abstract into parts that are separated by signs -such as commas and full stops -and stop words. These parts are then split into n-gram key phrases.</p><p>In our work, we use 1 â¤ n â¤ 4. Next, a word co-occurrence graph is learned from the generated n-grams. Last, each key phrase is ranked by the sum of the ratio of degree to frequency per word.</p><p>When merging the key phrases of different journals or conferences, we weight each key phrase by the impact factor or average citation count, respectively. The list of key phrases is then sorted in descending order by their total weighted count. We then manually clean the top 500 key phrases by removing key phrases unrelated to machine learning, such as "propos[ed] 2 method" or "experiment[al] result <ref type="bibr">[s]</ref> show", but also other irrelevant computer science terms, such as "comput[er] vision". Last, starting with the most popular key phrase, we iteratively skip related key phrases. We continue this merger until we find 10 distinct key phrases of different topics, which are the top 10 topics in machine learning. For example, key phrases related to "data set" are "train[ing] data" and "real data". Our implementation is available as open source: <ref type="url" target="http://github.com/pglauner/MLtop10">http://github.com/pglauner/MLtop10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Using method 1, which utilizes bigrams and trigrams for extraction, we only get very general topics. Concretely, the top 5 topics are "network pretraining", "supervised classification part", "learn binary representation", "unsupervised [and] supervised learning" and "predict label [from the] input". In contrast, performing method 2, which is machine learning based key word extraction using RAKE, we get the top 10 topics depicted in Figure <ref type="figure">1</ref>. We notice that after the first three topics, i.e. "support vector machine", "neural network" and "data set", there is a significant drop in terms of popularity. We notice another drop after "objective function". The next 7 topics are vey close in terms of their popularity. "Hidden Markov model" has a popularity only slightly lower than "principal component analysis".</p><p>1 Stop words are the words most frequently used in a language that usually provide very little information. For example, "and" or "of" are typical stop words in the English language.</p><p>2 Stemmed words are completed to their original form for clarity in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support vector machine</head><p>Neural network Data set Objective function Markov random field Feature space Generative model Linear matrix inequality Gaussian mixture model Principal component analysis Hidden Markov model Conditional random field Graphical model Maximum likelihood estimation Clustering algorithm Nearest neighbors Genetic algorithm Latent Dirichlet allocation Gaussian process Markov chain Monte Carlo 0 500 1000 1500 2000 2500 3000 Score</p><p>Fig. <ref type="figure">1</ref>: Top 10 topics highlighted in black, the top 11-20 topics in grey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discussion</head><p>Comparing the two key phrase extraction methods, we see that using RAKE we obtain more robust results that reflect for example frequent keywords and unbalanced terms much better.</p><p>Comparing our list of top 10 topics to the list of top 10 algorithms in data mining from 2007 <ref type="bibr" target="#b0">[1]</ref>, we make the following important observations: Due to their popularity in research, we have expected that support vector machines would appear in the top 10. Also, neural networks have been celebrating a comeback under the umbrella term "deep learning" since 2006 <ref type="bibr" target="#b4">[5]</ref> and we therefore expected them to appear in the top 10 as well under either term. We can also confirm that Hidden Markov models have received significantly less attention in research than neural networks over the last 10 years. We have not expected that the linear matrix inequality would appear in the top 10. However, given its importance to the theoretical foundations of the field of machine learning it is absolutely justified to appear in the top 10. Its appearance does not indicate a fallacy in our methodology. Naive Bayes has often been described as a wide-spread baseline model in the literature. Furthermore, tree classifiers such as random forests have become popular in the literature and do not appear in the top 10 either. Both, C4.5 and CART are tree learning algorithms that were found to be among the top 10 data mining algorithms in 2007. In terms of models, we did not expect that Markov random fields and Gaussian mixture models receive more attention than naive Bayes or tree based learning methods in current research publications.</p><p>A quantitative approach comes with a potential new bias depending on which data sources are used. Possible factors include the quality of publications and focus of each source (journal/conference). The vast majority of source abstracts are from journals and conferences that have a high impact factor or average citation count. We have made sure to include as many sources as possible that have a wide scope. In return, we have attempted to keep the number of sources with a very narrow scope to a minimum. Also, if the inclusion or omission of a specific source is questioned, this has only very little impact due to the distribution of abstracts: There are in total 39 sources (31 journals + 8 conferences). In average, a source has 1,372 abstracts or 2.56% of all abstracts. The largest source is the Neurocomputing journal, which has 6,165 abstracts or 11.52% of all abstracts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In our study, we use machine learning in order to find the top 10 topics in machine learning from about 54K abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences. Concretely, we found support vector machine, neural network, data set, objective function, Markov random field, feature space, generative model, linear matrix inequality, Gaussian mixture model and principal component analysis to be the top 10 topics. Compared to previous work in this field from 2007, support vector machine is the only intersection of both top 10 lists. This intersection is small for the following reasons: First, we do not only consider models, but span a wider view across the entire field of machine learning also including features, data and optimization. Second, we perform a quantitative study rather than opinion-based surveying of domain experts in order to reduce the bias. Third, the models of interest have significantly changed of the last 10 years, most prominently symbolized by the comeback of neural networks under the term deep learning. Overall, we are confident that our quantitative study provides a comprehensive view on the ground truth of current machine learning topics of interest in order to strengthen and streamline future research activities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Source journals.Furthermore, we have chosen 7 major international conferences related to machine learning as summarized in Table2, ranked by their average citation count. We have collected as many proceedings as possible of those conferences. In addition, we consider the Journal of Machine Learning Research Workshop</figDesc><table><row><cell>Name</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Top 10 algorithms in data mining</title>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2007">2008. 2007</date>
		</imprint>
	</monogr>
	<note>Published online: 4 December</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A few useful things to know about machine learning</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="87" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Readings in information retrieval. chapter An Algorithm for Suffix Stripping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="313" to="316" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Automatic Keyword Extraction from Individual Documents</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><surname>Cowley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Ltd</publisher>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
