<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-03-04">4 Mar 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Garcia</forename><surname>Sanchez</surname></persName>
						</author>
						<author role="corresp">
							<persName><surname>Joaquin</surname></persName>
							<email>joaqsan@math.utoronto.com</email>
						</author>
						<title level="a" type="main">On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-03-04">4 Mar 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">6A9DFE523A43E4CB0A337CB302159B3B</idno>
					<idno type="arXiv">arXiv:2403.02432v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>measure pre-conditioning</term>
					<term>recovery systems</term>
					<term>stability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study a new technique for understanding convergence of learning agents under small modifications of data. We show that such convergence can be understood via an analogue of Fatou's lemma which yields Γ-convergence. We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction 1.1 Organization of this document . . . . . . . . . . . . . . . . . . 1.2 Relation to literature . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Necessity of non-parametric measure pre-conditioning techniques 2 Measure pre-conditionings 3 A mathematical framework admitting pre-conditioning 3.1 Formulation of the problem . . . . . . . . . . . . . . . . . . . . 3.2 Convergence of the learning problem . . . . . . . . . . . . . . . 3.3 The main question . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Main Theorem . . . . . . . . . . . . . . . . . . . . . . . 3.4 A version of the envelope Theorem . . . . . . . . . . . . . . . . 3.4.1 No Empirical Probability Measure can Converge in the Total Variation Sense for all Distributions . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent progress in the use of optimal transportation techniques for machine learning in domain adaptation <ref type="bibr" target="#b2">[3]</ref> and development of Wasserstein Generative adversarial networks <ref type="bibr" target="#b3">[4]</ref> have helped our understanding of potential learning derived from theoretic properties of the underlying data. The topic of optimal transportation has grown significantly in recent years (see <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref> and references therein). Machine learning models aim to solve a task (to prescribed accuracy) using only the information of known data (training set). In this context it is preferred to have non-parametric models over parametric statistical families.</p><p>In this document we explore an idea that we call measure pre-conditioning the training data which consists in modifying the statistical model in order to improve performance of algorithms while preserving the limiting model. One can argue that measure pre-conditioning implicitly imposes unjustified structure to a problem but the idea is that measure pre-conditioning will simplify computations and ensure convergence to the original model. For example measure pre-conditioning one of the measures may allow using optimal transportation techniques to adapt a domain which would otherwise be very costly, this would yield a desired training in a task with little information. We use the terminology "measure pre-conditioning" as the technique reminds us of pre-conditioning matrices from linear algebra and optimization.</p><p>1.1 Organization of this document</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Relation to literature</head><p>The authots of <ref type="bibr" target="#b2">[3]</ref> develop the idea of optimal transport domain adaptation on which a linear approximation of the transport map is used to infer labels on target domain and <ref type="bibr" target="#b4">[5]</ref> developed CO-OT, a technique on which optimal transport is not only done between source and target domains of data but in the space of data and labels.</p><p>Recently <ref type="bibr" target="#b11">[12]</ref> developed the META-optimal transport technique which by presolving an optimization problem improves on the optimal transport efficiency. In this work, the idea is similar: can we modify training sets to ensure properties of learning? The modifications considered in this document, differ from the ones on <ref type="bibr" target="#b11">[12]</ref> as we only consider measure pre-conditioning data without establishing a minimization purpose beforehand. These techniques should remind the reader of the concept of preconditioning in optimization, on which one modifies a matrix via a correct scaling to benefit the algorithm computations.</p><p>In the same fashion, here one modifies the measure associated to a training set to benefit statistical properties of the learning agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Necessity of non-parametric measure pre-conditioning techniques</head><p>The need for non-parametric measure pre-conditioning techniques arises from the modeller's attempt to not intervene in the learning while improving it's computational performance. Measure pre-conditioning is posed in this document as a general technique and it is the modeller's task to determine which pre-conditioning is useful for their own goal. In section 4.1 we give several examples with different goals in mind.</p><p>2 Measure pre-conditionings</p><p>In this section we introduce the main concept and discuss several possible "measure pre-conditionings". In this context a measure pre-conditioning will be a technique to manipulate data in order to obtain a "nicer" measure. For example, we can regularize our problem to obtain a measure that is absolutely continuous with respect to Lebesgue or a measure that has a different type of support.</p><p>Measure pre-conditioning is also similar to parameter fitting for curves. In the case of real variable one attempts to infer information from isolated data points by first creating a continuous (typically smooth) curve joining the points. Pre-conditioning between points in R has drawbacks (overfitting, highvariation, etc) and so will measure pre-conditioning (see section 8). Measure pre-conditionining will have the advantage of enabling stronger techniques to infer learning as we will see throughout the paper. We start by defining several possible measure pre-conditioning techniques and analyzing their properties.</p><p>Problem 1 (General measure pre-conditioning problem for independent identically distributed data) Let (X 1 , X 2 , . . . , Xn) be a sample, that is {X i } n i=1 is a set of independent identically distributed data such that X 1 ∼ µ. Suppose that the sample will be used to train a machine learning model, the measure pre-conditioning problem is to find a good way to obtain a measure μn from the sample such that μn improves performance of the model or the computational cost of the algorithms while keeping the most relevant features of the problem intact.</p><p>As such, this measure pre-conditioning problem is not mathematically well posed, as we haven't defined what "improves performance of the model" or " keeping the most relevant features" mean. Performance improvement can be done in several ways: simplification of algorithms, computational cost, control on domain adaptation or even yielding mathematical properties for the learning agent. All of these type of improvements are valid and impactful in machine learning research. The aim of this paper is to analyze how different measure pre-conditionings impact model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A mathematical framework admitting pre-conditioning</head><p>Let us start with a basic framework from Machine Learning models in order to be able to define measure-preconditioning and show it's relevance. The simplest case is the minimization over all fitting functions f within a class of fitters C minimizing the expected value of the loss function L measuring the loss of fitting the random variable Y with the variable X via f (X). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation of the problem</head><p>Now assume we don't know the full model π but we have a training sample, i.e. we have (X 1 , Y 1 ), . . . , (Xn, Yn) ∼ π, statistically we know the values on the sample but not the full distribution. Assume we approximate π using the sample via a probability measure πn, the associated C-model for L under πn reads arg min</p><formula xml:id="formula_1">f ∈C Eπ n [L(f (x), y)] .<label>(2)</label></formula><p>This formulation immediately give rise to the following questions i If L and C are fixed, what conditions on π n ensure that the minimizer in (2) approaches <ref type="bibr" target="#b0">(1)</ref>? In what topology? ii What properties could <ref type="bibr" target="#b1">(2)</ref> have that (1) may lack? iii Given a choosing of π n 's, could we find sequences L n 's and C n so that the computations on the C n -problem with loss function L n associated to π n converge to (1)? Could these problems improve the algorithmic performance?</p><p>Idea 1 (Measure pre-conditioning) A measure pre-condition is a way to define πn from the sample (X 1 , Y 1 ), . . . , (Xn, Yn) such that the associated C-problem with loss function L has improved performance in any way while preserving the convergence of minimizers of (2) to that of (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convergence of the learning problem</head><p>Our main focus will be answering: when do minimizers of (2) converge to minimizers of 1 and in which way?.</p><p>We first notice that in many situations it is possible to obtain the same total loss under convergence of the measures (without necessarilly having convergence of minimizers), this situation is rather general and known and is not the main question in the ML community but it gives a good starting point for the techniques used in this document. For many applications it is enough to know convergence of the total loss and so we exemplify conditions that yield such convergence.</p><p>Proposition 1 (Standard convergence results on total loss (not minimizers))</p><formula xml:id="formula_2">1. If ||L||∞ &lt; ∞ or if spt(µ) is compact, |Eπ n [L(f (X), Y )] -Eπ[L(f (X), Y )]| ≤ ||πn -π|| T V . 2. Given f ∈ C if (x, y) → L(f (x), y) is Lipschitz, then |Eπ n [L(f (X), Y )] -Eπ[L(f (X), Y )]| ≤ d 1 (πn, π). 3. If L is C 2 and || ∂L ∂1 || &lt; ∞ then |Eπ n [L(f (X), Y )] -Eπ[L(f (X), Y )]| ≲ ||πn -π|| T V + sup x∈Ω d(f * (x), f * n (x))</formula><p>4. If C is a compact class on C(Y), and πn → π in d 1 then along a subsequence n k</p><formula xml:id="formula_3">Eπ n k [L(f * n k (X), Y )] → Eπ[L(f * (X), Y )] where f *</formula><p>n k is the C-optimizing argument for πn and f * is the C-optimizing argument for π.</p><p>The proof of proposition 1 is direct and hence omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The main question</head><p>Measure preconditioning modifies the minimization problem at level n, i.e. it changes the structure of the measure used to evaluate loss with a sample of size n. If the model was unchanged we would expect convergence of the learning agent trained with the sample of size n, i.e. f * n to the best fit with respect to the loss for the parametric distribution f * . If measure pre-conditioning modifies the measure at level n, the true question is when and in which ways does f * n → f * ?.</p><p>To answer the convergence of minimizers, as it is usual in functional analysis and economics, we introduce Γ-convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Main Theorem</head><p>We present an informal version of the main theorem of the work. This informal version corresponds to the rigorous statements answered in Theorem 5, Proposition 6 and section 5</p><p>Theorem 2 Full learner recovery system is a concept that allows us to show convergence of learning agents to the ideal parametric agent in cases not covered previously in the literature. This concept allows us to generalize stability arguments for less regular losses and a bigger class of classification/regression problems. Full learner recovery systems are general enough to be applied to several settings in Machine-Learning, including Domain Adaptation transfer learning. These systems explain many phenomena in ML-research where convergence is improved. Full learner recovery systems give a guideline on how and when to modify training data without disturbing the original problem.</p><p>The formulation of Theorem 2 is not mathematically precise, we dedicate this work to make the Theorem rigorous and prove it in the subsequent sections. We start with the introduction of the main mathematical tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A version of the envelope Theorem</head><p>Definition 1 (Γ-convergence on a metric space) Let (X, d) be a metric space and let F j , F : X → R ∪ {±∞},we say Fn Γ-converges to F , denoted Fn Γ -→ F if and only if the following two conditions hold I For all sequences {x j } such that x j d -→ x we have lim inf j→∞ F j (x j ) ≥ F (x). II For every x ∈ X there exists a sequence x j d -→ x such that</p><formula xml:id="formula_4">F (x) ≥ lim sup j→∞ F j (x j ).</formula><p>Remark 1 The most general definition for Γ-convergence is one where X is assumed to be a topological space and not necessarily metric. The definition presented above (Definition 1) is the sequential-definition. We have chosen the sequential definition as it simplifies the theory significantly, knowing that some important examples that we have in mind are only topological spaces on which the Γ-limit is defined via</p><formula xml:id="formula_5">Γ -lim n→∞ Fn(x) = sup U ∈N (x) lim inf n→∞ inf y∈U fn(y).<label>(3)</label></formula><p>In some of the examples below the underlying convergence will not correspond to a metric space, on which one must think of (3) instead of (I) and (II).</p><p>The motivation behind the definition of Γ-convergence is that minimizers converge to minimizers, the content of the following theorem from <ref type="bibr" target="#b12">[13]</ref>:</p><p>Theorem 3 (Γ-convergence and minima) Let (X, d) and F j , F be as in Definition 1, then 1. If I from definition 1 is satisfied for all x ∈ X and K is a compact subset of X then inf</p><formula xml:id="formula_6">K F ≤ lim inf j→∞ inf K F j (4)</formula><p>2. Similarly, if II from definition 1 is satisfied and U is an open subset of X then</p><formula xml:id="formula_7">lim sup j→∞ inf U F j ≤ inf U F<label>(5)</label></formula><p>This Theorem can be found as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">Proposition 1.18]</ref>. Finally we recall one more Theorem from <ref type="bibr" target="#b12">[13]</ref>. We say that a sequence {F j } of functions on a metric space (X, d) is equi-mildly coercive if there exists a non-empty compact set K such that inf</p><formula xml:id="formula_8">X F j = inf K F k for all j.</formula><p>Theorem 4 (Minimizers and Γ-limits)</p><p>In a metric space (X, d) if {F j } is equi-mildly coercive and Fn</p><formula xml:id="formula_9">Γ -→ F then min X F = lim j→∞ inf K F j<label>(6)</label></formula><p>Furthermore, every limit point of a sequence of minimizers of (6) is a minimizer of F .</p><p>For a proof see <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">Theorem 1.21]</ref>. With the theory in hand we take a general approach to answer the questions (i) and (ii). Instead of a constructive proof to find the optimal topologies (on C and P(X × Y )) we reformulate the convergence problem for it to satisfy the hypothesis of Theorem 1. This way we can relate to classical problems by looking at the given topologies of each framework and verifying the hypothesis. Going back to the framework of Problems (1) and 2, we want to be able to recover minimizers from our measure conditioning. We note the interaction of the class of fitters C, the loss function L and the mode of convergence of the conditioners that we choose to evaluate, henceforth it is logical to check conditions for them as a collective, rather than separately. This motivates the following definition.  </p><formula xml:id="formula_10">-→ f , we have lim inf n→∞ Eπ n [L(fn(X), Y )] ≥ Eπ[L(f (X), Y )].<label>(7)</label></formula><p>2. If π j m -→ π and for every f ∈ C there exists a sequence</p><formula xml:id="formula_11">f j ∈ C, such that f j d -→ f and Eπ[L(f (X), Y )] ≥ lim sup j→∞ Eπ j [L(f j (X), Y )]<label>(8)</label></formula><p>Remark 2 In analytical terms, these conditions ensure 2-sided Fatou-Lemmas for integration with respect to L on the first coordinate.</p><p>Γ-convergence can be also used to address the existence of minimizers of the parametric model but that is not the approach of this work, we assume existence of minimizers of the limiting problem and study recovery sequences, from now on we assume the existence of a unique minimizers for (2).  <ref type="figure">(C</ref>, <ref type="figure">d</ref>) is a compact metric space, assume the limiting problem from 1 has a solution f ∈ C, then there exists a sub-sequence {fn k } of {fn} ∈ C such that</p><formula xml:id="formula_12">fn k ∈ arg min f ∈C Eπ n k [L(f (X), Y )] such that as k → ∞, fn k d -→ f and Eπ n k [L(fn k (X), Y )] → Eπ[L(f (X), Y ))].<label>(9)</label></formula><p>Proof The definition of (C, d, L, m -→) forming a full learner recovery system is such that E L πn Γ -→ E L π , i.e. by taking the functional Fn(f ) : C → R, defined via Fn(f</p><formula xml:id="formula_13">) := Eπ n [L(f (X), Y )],</formula><p>the definition 2 is equivalent to Fn Γ -→ F . By compactness of C we get the hypothesis for Theorem 4 so we get the thesis. □</p><p>In many cases C is not necessarily compact. The assumption of compactness simplifies the arguments but the argument above can be obtained without compactness of C if instead one assumes equi-mild-coercivity of {F n }, that is there exists a compact set K for which all F n 's satisfy inf C F n = inf K F n . See [13, Theorem 1.21], we instead assume compactness of C to avoid this subtlety.</p><p>Remark 3 Evidently the statement of Theorem 5 is useless unless we explore examples and explain the ideas and how to use it. So far, we have just re-written the problem so that we can conclude (subsequential) convergence of learned agents by checking a modified version of Fatou's Lemma. This rewriting allows us to cover different cases at the same time, as we do in the following examples.</p><p>The goal of this list is not to be exhaustive but to show the many different formulations that can be included in Definition 2. Notice that checking Definition 2 involves only studying a two sided version of Fatou's Lemma that can be corroborated in every particular case. Once one establishes that the given ML problem of the form (2) and (1) are indeed a full recovery system with {π n }, π, C, L one has ensured convergence of minimizers (which amounts to perfect approximation of the model).</p><p>In the following proposition we show the wide range of options one has for full recovery systems, although the d-convergence in some items of the following proposition are not necessarily with respect to a metric, we have in mind Remark 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 6</head><p>The following are full learner recovery systems</p><formula xml:id="formula_14">1. Let K ⊂ R p be compact, C a compact subset of {f : R p → R s.t. (x, y) → L(f (x), y) ∈ L 1 (π)</formula><p>} with respect to d, where d denotes point-wise convergence, L : R p × R → R be any positive, bounded, continuous function and let m -→ denote set-wise convergence i.e µn(A) → µ(A) for every Borel set A, where µn, µ ∈ P(K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>m -→:=⇀ (weak convergence of measures), C be compact such that {L(f (x), y)} f ∈C uniformly integrable with respect to {πn} and there exists g such that L(g(x), y) ∈ L 1 π such that fn(x) ≤ g(x) holds π-a.e.  Proof In all of the cases above we only need to ensure a Fatou-like lemma (Definition 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>1. Here Γ-convergence must be thought as in Remark 1. This is a direct consequence of Fatou's lemma for varying measures (found in <ref type="bibr" target="#b13">[14]</ref> or [29, Theorem 1.1]).</p><p>2. See [29, Theorem 2.2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The uniform Lipschitz condition gives</head><formula xml:id="formula_15">L(f j (x), y)dπn -dπ(x) + L(f j (x), y) -L(f (x), y)dπ(x, y) ≤ d 1 (πn, π) + L(f j (x), y) -L(f (x), y)dπ(x, y)</formula><p>where the first term comes from Kantorovich-Rubinstein [9, Particular Case 5.16] and the second one vanishes by dominated convergece.</p><p>4. In this case we don't only have the inequalities of definition 2 but the limits coincide:</p><formula xml:id="formula_16">L(fn(x), y)dπn(x, y) -L(f (x), y)dπ(x, y) ≤ L(fn(x), y)d(πn -π) + L(fn(x), y) -L(f (x), y)dπ ≤ M ||πn -π|| T V + L(fn(x), y) -L(f (x), y)dπ</formula><p>where the first one goes to zero by the assumption πn T V --→ π and the second one by the assumed d-continuity and dominated convergence. □</p><p>The goal of this list is not to be exhaustive but to show the many different formulations that can be included in Definition 2. Notice that checking Definition 2 involves only studying a two sided version of Fatou's Lemma that can be corroborated in every particular case. Once one establishes that the given ML problem of the form ( <ref type="formula" target="#formula_1">2</ref>) and ( <ref type="formula" target="#formula_0">1</ref>) are indeed a full recovery system with {π n }, π, C, L one has ensured convergence of minimizers (which amounts to perfect approximation of the model).</p><p>Remark 4 Observe that the conditions imposed for C and L on Proposition 6 case 4 are less restrictive than the ones on 6 case 2. This is intuitively obvious as the total variation convergence is stronger than weak convergence. This means that ensuring a stronger convergence in measure is a degree of improvement for the ML-problem associated to fixed C and L. It is also evident that regularity conditions usually assumed in ML-theory (like Lipschitz properties of L) yield strong approximations in most types of convergence m -→, making this framework not only inclusive but rather general.</p><p>One of the main advantages of measure pre-conditioning is the ability to change the training sample. It is common to use the empirical measure in nonparametric statistics, nevertheless the next section shows that the empirical measure is in general, not the best formulation for (2) as it may happen that the conditions for convergence hold for a different sequence of measures and not the sequence of empirical measures. We will see this is the case of Proposition 6 case 4, where the sequence of empirical measures would not ensure subsequential convergence but a different sequence does, justifying completely the use of measure pre-conditioning as it improves the likelihood that the algorithm gives a reasonable final learnt agent.</p><p>Remark 5 (Compactness) Stronger conditions like compactness of the underlying sets yield a more elegant theory. Many of the modes of convergence are equivalent under the assumption on compactness (see <ref type="bibr" target="#b25">[26]</ref> or <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">Chapter 7]</ref>). The assumption of compactness simplifyies most theorems as it will automatically bound sequences and so Definition 2 is much easier to satisfy and verify which automatically yields:</p><formula xml:id="formula_17">Proposition 7 If C ⊆ C(Y ), (x, y) → L(x, y) is continuous and sup f ∈C sup (x,y) |L(f (x), y)| &lt; ∞ then (2) → (1) in the C uniform topology, i.e. arg min f ∈C Eπ [L(f (x), y)] C -→ arg min f ∈C Eπ [L(f (x), y)] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">No Empirical Probability Measure can Converge in the Total Variation Sense for all Distributions</head><p>Towards studying when to measure pre-condition we realize that it is important to know what types of empirical measures converge and in which cases. In the seminal work <ref type="bibr" target="#b14">[15]</ref>, the authors proved the following theorem:</p><p>Theorem 8 (No Empirical Probability Measure can Converge in the Total Variation Sense for all Distributions)</p><p>Let {πn} be a sequence of empirical distributions and δ &gt; 0, then there exists a proability measure π such that</p><formula xml:id="formula_18">inf n sup A |πn(A) -π(A)| &gt; 1 2 -δ a.s.</formula><p>For a proof see <ref type="bibr" target="#b14">[15]</ref>. Theorem 8 tells us that the class of measures approximated in total variation norm by the empirical measure is not all measures. For different measures, other probability measures formed from data can converge in total variation but the empirical measure does not converge to all measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 6</head><p>In <ref type="bibr" target="#b14">[15]</ref> it is shown that the standard empirical measure does not converge in total variation sense to absolutely continuous limits. Hence, Theorem 5 does not apply with Proposition 6 case 4 if we use the standard empirical measure. Nevertheless, as shown in <ref type="bibr" target="#b15">[16]</ref>, the kernel-empirical measure given by</p><formula xml:id="formula_19">πn = 1 hn K(f /h)</formula><p>does converge in total variation (see Definition 5 below). Hence, Theorem 5 via Proposition 6 case 4 applies to the sequence {πn} but not the sequence of standard empirical measures. This shows that the model solution for ML-program 2 will converge to the best parametric C-model. This argumentation explains why standard techniques in Machine-Learning, such as shifting and adding noise give better results in practice, as convergence is ensured by this system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Example: Linear regression</head><p>Let us consider π ∈ P ac (R 2 ), we consider the linear regression problem with square-loss function with respect to target measure π:</p><formula xml:id="formula_20">min (a,b)∈R 2 E π [(Y -aX + b) 2 ]. (TargetLR)</formula><p>By differentiating with respect to a, b from first order conditions we know that the solutions to (TargetLR) are</p><formula xml:id="formula_21">a = y • xdπ(x, y) -ydπ(x, y) xdπ(x, y) x 2 dπ(x, y) - xdπ(x, y) 2 (10) b = ydπ(x, y) -      y • xdπ(x, y) -ydπ(x, y) xdπ(x, y) x 2 dπ(x, y) - xdπ(x, y) 2      xdπ(x, y)<label>(11)</label></formula><p>If we consider a sequence of measures π n , obtained using the sample (X 1 , Y 1 ), . . . , (X n , Y n ) then the linear regression problem with square-loss function with respect to approximating measure</p><formula xml:id="formula_22">π n is min (a,b)∈R 2 E πn [(Y -aX + b) 2 ]. (AppxLR)</formula><p>The solution (a πn , b πn ) to (AppxLR) is given by</p><formula xml:id="formula_23">a πn = y • xdπ n (x, y) -ydπ n (x, y) xdπ n (x, y) x 2 dπ n (x, y) - xdπ n (x, y) 2<label>(12)</label></formula><formula xml:id="formula_24">b πn = ydπ n (x, y) -      y • xdπ n (x, y) -ydπ n (x, y) xdπ n (x, y) x 2 dπ n (x, y) - xdπ n (x, y) 2      xdπ n (x, y)<label>(13)</label></formula><p>If π n corresponds to the empirical measure, then rate of convergence of a πn and b πn have been widely studied. See <ref type="bibr" target="#b26">[27,</ref><ref type="bibr">Chapter 3]</ref> for example. We also know by Theorem 8 that <ref type="bibr">Section 2]</ref> we can find a sequence of measures (Parzen windows) {π n } such that πn</p><formula xml:id="formula_25">π n ̸ T V --→ π. By [16,</formula><formula xml:id="formula_26">T V --→ π.</formula><p>For simplicity, assume that</p><formula xml:id="formula_27">xdπ n (x, y) = 0, x 2 dπ n (x, y) = 1, xdπ(x, y) = 0 and x 2 dπ(x, y) = 1.</formula><p>With this assumption we immediately obtain the following bound:</p><formula xml:id="formula_28">|a πn -a π | ≤ sup (x,y)∈spt(πn)∪spt(π) |x • y| ||π n -π|| T V .<label>(14)</label></formula><p>Which in the case where {π n }, π are uniformly compactly supported yields</p><formula xml:id="formula_29">|a πn -b πn | ≲ ||π n -π|| T V .<label>(15)</label></formula><p>Equation ( <ref type="formula" target="#formula_29">15</ref>) is a bound on the order of convergence on the coefficient of linear regression of (AppxLR) to that of (TargetLR) which is not available in the case of the empirical measure, as indicated by Theorem 8. The bound <ref type="bibr" target="#b14">(15)</ref> different to the usual order of convergence bounds for linear regression exemplifies the impact of measure pre-conditioning. Equation ( <ref type="formula" target="#formula_29">15</ref>) shows (uniform) stability of learning agents corresponding to the measure pre-conditioned problem, allowing us to use more tools than the standard ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Measure pre-conditioning approaches</head><p>Measure pre-conditioning approaches impose certain structures to the original data. The idea is to analyze how does this structure impacts final outcomes of the modelling. In some way, this process resembles plain statistical inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Background and Notation</head><p>Let Ω ⊂ R n be fixed. We denote by P p (Ω) to be the set of probability measures with p-th finite moment. That is P p (Ω) = {µ ∈ P(Ω) : Ω |x -x 0 | p dµ &lt; ∞, for some x 0 ∈ Ω}. We define the Wasserstein p distance between µ, ν ∈ P p (Ω)</p><formula xml:id="formula_30">d p (µ, ν) = inf π∈Γ(µ,ν) Ω×Ω |x -y| p dπ(x, y) 1/p</formula><p>where Γ(µ, ν) denotes the set of probability measures on Ω × Ω having first marginal µ and second marginal ν. We say a map T : Ω 1 → Ω 2 is a Monge map with respect to the cost function c : Ω 1 × Ω 2 → R, between Borel measures µ and ν whenever</p><formula xml:id="formula_31">T ∈ arg min T #µ=ν Ω1 c(x, T (x))dµ(x)<label>(16)</label></formula><p>where T #µ means that for every Borel set A, ν(A) = µ(T -1 (A)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical measures and non-parametric estimation</head><p>In this section we discuss common non-parametric estimates and their relations to the structure of the ML-problems ( <ref type="formula" target="#formula_1">2</ref>) and <ref type="bibr" target="#b0">(1)</ref>. We aim to explain how each measure can be used to pre-condition and the pros and cons coming with their use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Non-exhausting list of non-parametric estimation techniques</head><p>Definition 3 (Empirical measure) Given X 1 , . . . , Xn we define the standard empirical measure as the number of successes on the n occurrences:</p><formula xml:id="formula_32">µn(A) = 1 n n k=1 δ X k (A).</formula><p>Definition 4 (Histogram) Given X 1 , . . . , Xn we define the histogram measure associated to the sets B 1 , . . . , Bm</p><formula xml:id="formula_33">µn(A) = 1 n n k=1 m l=1 1 ρ(B l ) δ X k (A ∩ B l ).</formula><p>where ρ is a probability measure (usually taken to be normalized Lebesgue).</p><p>Definition 5 (Kernel estimation via Parzen windows) Given X 1 , . . . , Xn, we define the n-th density estimation with kernel K via</p><formula xml:id="formula_34">fπ n (x) = 1 nHn n i=1 K x -X i Hn</formula><p>where K is fixed and {Hn} is any sequence of random variables, that (may) depend on the sample X 1 , . . . , Xn that satisfy that Hn → 0 almost surely and nHn → ∞ almost surely.</p><p>The idea of this formulation of the kernel estimation comes from <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b16">[17]</ref> and it is fully justified by Theorem 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Wasserstein 2-Barycenter</head><p>Definition 6 (Wasserstein Barycenter) Given a sample X 1 , X 2 , . . . , Xn random variables in R p we define the 2-Wasserstein Barycenter of the sample (also called Frechet mean) as any probability measure satisfying</p><formula xml:id="formula_35">µ * ∈ arg min ρ∈P 2 (R p ) n k=1 d 2 (ρ, δ X k ) 2<label>(17)</label></formula><p>where δ X k denotes the unit mass at X k .</p><p>Remark 7 Note that ρ → d 2 (•, ν) 2 is lower-semicontinuous for all ν and so Wasserstein Barycenters exist. In general, Wassertein barycenters with respect to random Dirac measures are not unique. If instead, one of the deltas is replaced by an absolutely continuous measure, uniqueness can be shown. We don't do this replacement in this document, instead we study the entropic regularization of the minimization problem in Definition 8.</p><p>The theory of Wasserstein Barycenters has recently received attention from several fields of applied mathematics, see for example <ref type="bibr" target="#b17">[18]</ref> for a more complete theory.</p><p>Remark 8 The barycenter can be defined given any distance function d : P(R p ) × P(R p ) → R and a sample (X 1 , . . . , Xn) the d-barycenter is any probability measure µ satisfying</p><formula xml:id="formula_36">µ * ∈ arg min ρ 1 n n k=1 d(ρ, δ X k ) (<label>18</label></formula><formula xml:id="formula_37">)</formula><p>where the infimum is taken over all probability measures on R p .. We have only chosen the Wasserstein 2-distance as we aim to focus on Domain Adaptation.</p><p>Remark 9 It is important to notice that efficient algorithms to compute Wasserstein Barycenters have recently been developed (see <ref type="bibr" target="#b1">[2]</ref>) in the case of empirical measures. This efficient computability is essential for the applications we have in mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Uniform convex hull</head><p>Definition 7 (Convex Hull)</p><p>The convex hull of a set B ⊆ R p is defined to be the smallest convex set on which B is contained, equivalently</p><formula xml:id="formula_38">Conv(B) = C convex B⊆C C.</formula><p>We define the uniform convex hull of the sample (X 1 , X 2 , . . . Xn) to be the uniform measure on the convex hull of {X 1 , X 2 , . . . , Xn}, i.e.</p><formula xml:id="formula_39">µconv = L p |c L p (Conv({X 1 , X 2 , . . . , Xn}))<label>(19)</label></formula><p>where L p denotes the Lebesgue measure in R p .</p><p>Remark 10 Note that µconv is the restriction of the Lebesgue measure to the convex hull of the sample so it's support is automatically convex. This particular property could be significant for future applications as the theory of convex optimization unlocks several numerical techniques. Evidently, it's support also includes all points of the sample. Note that Definition 7 always gives a well defined measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Entropically regularized barycenter</head><p>Definition 8 Given a sample X 1 , X 2 , . . . , Xn random variables in R p and a reference probability measure ν we define the ν-entropically regularized 2-Wasserstein Barycenter of the sample as any probability measure satisfying</p><formula xml:id="formula_40">µ * ∈ arg min ρ∈P 2 (R p ) 1 n n k=1 d 2 (ρ, δ X k ) 2 + Ent(ρ | ν)<label>(20)</label></formula><p>where δ X k denotes the unit mass at X k and Ent(µ | ν) denotes the relative entropy of ρ with respect to ν given by</p><formula xml:id="formula_41">Ent(ρ | ν) = log dρ dν dν<label>(21)</label></formula><p>whenever ρ ≪ ν and Ent(ρ | ν) = ∞ otherwise.</p><p>Remark 11 If ν ≪ L p , the functional to minimize is lower semi-continuous and with the addition of entropy a unique absolutely continuous minimizer of (20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Class-regularized barycenter</head><p>Motivated from the work of <ref type="bibr" target="#b2">[3]</ref> we can also think of measure pre-conditioning in terms of pre-established class based groups. The idea behind the next definition is that elements in the same class may be very similar while elements from different classes could be very different from each other.</p><p>Definition 9 (Class barycenter) Given a sample X 1 , X 2 , . . . , Xn random variables in R p suppose that each X i belongs to one and only one of a finite collection of classes {C l } m l=1 , then we can define the class-based barycenter to be any measure µ satisfying</p><formula xml:id="formula_42">µ * ∈ arg min µ∈P 2 (R p ) 1 m m k=1 d 2 (ρ, ν k ) 2 + Ent(ρ | ν)<label>(22)</label></formula><p>where ν k is a measure determined only from class C k . For example, one would obtain a barycenter of barycenters if one were to choose ν k to be the 2-Wasserstein barycenter of {X i :</p><formula xml:id="formula_43">X i ∈ C k }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">MMD-regularized Conditional measures</head><p>Definition 10 Given a characteristic kernel function k (see <ref type="bibr" target="#b0">[1]</ref> for details), define the maximum mean discrepancy between µ, ν with respect to k via</p><formula xml:id="formula_44">mmd k (µ, ν) = E µ×µ [K(X, X)] + E ν×ν [k(Y, Y )] -2E µ×ν [k(X, Y )]</formula><p>The empirical optimal transference plan between conditional distributions for a given lower-semicontinuous cost function c, denoted π * ,c n is defined in <ref type="bibr" target="#b19">[20]</ref> via the minimization over Γ(µ.ν) of the following functional:</p><formula xml:id="formula_45">c(x, y)dπ + λ 1 1 n n i=1 mmd 2 k (P roj 1 #π, δ Yi ) + n i=1 mmd 2 k (P roj 1 #π ′ , δ Y ′ i ). (<label>23</label></formula><formula xml:id="formula_46">)</formula><p>Existence and uniqueness depends on the cost function and usual conditions (smoothness and twist) are required, see <ref type="bibr" target="#b0">[1]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Some properties of the measure pre-conditioners</head><p>Proposition 9 When they exist, the measures from definitions 8 and 9 are absolutely continuous with respect to ν.</p><p>Proof By definition, Ent(ρ|ν) = ∞ if ρ ̸ ≪ v, because ν is always feasible, the functional is not infinity and hence the minimizer is a.c. with respect to ν. □ Corollary 1 If ν = L p in Definitions 8 or 9, the minimizer has a density (w.r.t. Lebesgue).</p><p>Although the proof is simple, the importance of Proposition 9 and Corollary 1 is fundamental for practice. If we can estimate the density, we can use it to improve the convergence of algorithms by numerical methods. See for example <ref type="bibr" target="#b22">[23]</ref> where the entropic regularization allows a closed (and very simple) form of the density which then yields a dual-descent algorithm. Knowing explicitly the density allows us to find minimizers of Problem 2 via formulae and so we can focus our attention on estimating numerically these minimizers without carrying a second numerical error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimality (Euler-Lagrange)</head><p>Most of the measure pre-conditioners defined on section 3.5 require the minimization of a functional. Let Ω ⊆ R p , in this section we study the first order conditions for minimization in (P 2 (Ω), d 2 ) which can be found in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">Theorem 7.20]</ref>.</p><p>Definition 11 (First variation of a functional in P(Ω)) Let F be a functional F : P 2 (Ω) → R, let ρ ∈ P 2 (Ω) be fixed and ϵ &gt; 0, for any ρ ∈ P 2 ac ∩ L ∞ (Ω), define ν = ρ -ρ, we say that δF δρ (ρ) is the first variation of F evaluated at ρ if d dϵ ϵ=0 F (ρ + ϵν) = δF δρ (ρ)dν.</p><p>Theorem 10 (Optimality criteria) For a functional F : P 2 (Ω) → R suppose that µ ∈ arg min ν∈P2(Ω) F (ν). Assume that for every ϵ &gt; 0 and for every ρ absolutely continuous with L ∞ (M ) density</p><formula xml:id="formula_47">F ((1 -ϵ)µ + ϵρ) &lt; ∞ let c := essinf δF δρ (µ) . If δF δρ (µ) is continuous, δF δρ (µ)(x) ≥ c ∀x ∈ M,<label>(24)</label></formula><formula xml:id="formula_48">δF δρ (µ)(x) = c ∀x ∈ supp(µ). (<label>25</label></formula><formula xml:id="formula_49">)</formula><p>The proof can be found as Theorem 7.20 in <ref type="bibr" target="#b10">[11]</ref>. Just as in the remark after Corollary 1, the main use of this tool is to focus the algorithmic implementation towards the computation of the first variation of the functional it minimizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Convergence</head><p>The objective of the reformulation of the general ML-problem in terms of Problem 2 and 1 is that we can adapt every stage of the learning process by using a measure estimation that fits the problem better. In order for us to know that we can recover the ML-problem in this process we need to know the types of convergence on which the sequences of measures formulated with the data converge to the underlying distribution. Many theorems and specific cases on density estimation have been studied, we recollect some of them here in terms of the definitions of section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Convergence of density estimations</head><p>Observe that Theorem 2 and Proposition 6 allow different systems of convergence, i.e. depending on the 'strength' of the type of convergence m -→ of the probability measures, different requirements on C, d, L are needed. In this section we give a non-exhaustive list of modes of convergence for density estimation and the sequences in Section 4.1 that can be used as measure preconditioners. In this section one should notice that every type of convergence should be coupled with hypothesis that ensure the system is a full learner recovery system (Definition 2).</p><p>Theorem 11 (Glivenko Cantelli in R) Let µ be any probability measure on R and µn be the standard empirical measure (Definition 3), if F (t) = µ((-∞, t]) and Fn(t) = µn((-∞, t]) then Fn → F uniformly on R as n → ∞ This theorem is well-known see for example <ref type="bibr" target="#b20">[21,</ref><ref type="bibr">Theorem 7.4]</ref> or <ref type="bibr">[32,</ref> Theormem 11.4.2.]. By account's of Donsker's theorem one can get the following improvement:</p><p>Proposition 12 (Rate of convergence for continuous F ) If µ is a law on R for which F is continuous, the order of convergence of Theorem 11 satisfies</p><formula xml:id="formula_50">n 1/2 sup t |Fn(t) -F (t)| ⇀ max 0≤s≤1 |Bs -sB 1 | (<label>26</label></formula><formula xml:id="formula_51">)</formula><p>where {Bs} is a Brownian motion, i.e. the rate of convergence approaches the law of the absolute value of a Brownian bridge on [0, 1] and so it's law can be computed explicitly:</p><formula xml:id="formula_52">P 0 sup 0≤s≤1 |Bs -sB 1 | &lt; b = ∞ m=-∞ (-1) m e -2m 2 b 2<label>(27)</label></formula><p>See <ref type="bibr" target="#b20">[21,</ref><ref type="bibr">Theorem 8</ref>.10] and the following proposition for the explicit formula of it's law.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 12</head><p>The theorem presented here as Theorem 11 is just a specific version. In general, one refers to any theorem of this type as "a Glivenko-Cantelli type theorem" see for example <ref type="bibr" target="#b31">[32]</ref>.</p><p>Theorem 13 (Varadarajan) If π is any probability measure on X × Y and X × Y is a separable metric space then the standard empirical measures (Definition 3) for (X, Y ) converge weakly in probability to π.</p><p>For a proof see <ref type="bibr">[32, 11.4.1]</ref>. It is important to notice that the convergence is almost surely. In some cases, like the case of real numbers, the convergence can be upgraded. n n → ∞ and µ ≪ Leb, the empirical density estimate of Definition 5 converges uniformly in measure to µ, i.e. for every ϵ &gt; 0,</p><formula xml:id="formula_53">P ω : sup x∈R |fn(x, ω) -f (x)| &lt; ϵ n→∞ ----→ 1. (<label>28</label></formula><formula xml:id="formula_54">)</formula><p>For a proof see <ref type="bibr" target="#b16">[17]</ref>. The following theorem is a specific case of the much more general convergence of Barycenters proved in <ref type="bibr" target="#b29">[30]</ref>, in the paper the authors prove the d p -convergence in metric measure spaces satisfying a positive curvature condition. For a proof see <ref type="bibr" target="#b29">[30]</ref> and apply it to the simple case where (R p , |•|, µ) is given as the initial measure space. In <ref type="bibr" target="#b19">[20]</ref> the following proposition was shown: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Convergence and full learner recovery systems</head><p>In the previous section 4.4.1 we have listed several convergence results for different types of empirical measures. Empirical measures encompass our understanding of the sample. Theorems 11, 13 and 14, Propositions 12, 15 and 16 need to be coupled with regularity properties of L and the underlying class of functions C as in Proposition 6. This list shows that given an underlying model, it's intrinsic features will determine the type of measure preconditioners needed to ensure convergence on the specific convergence mode that the limiting measure admits. For example, Proposition 16 involves convergence in Total Variation norm from which one can infer that the measure pre-conditioning of Definition of 10 applies for a d-continuous (in the first coordinate) loss function L as in Proposition 6. 4. In contrast, Theorem 8 shows that the empirical (uniform) measure is not well-suited for every limiting distribution and so in the case of a continuous density, preconditioning by 10 is proved to have better results (theoretically) than the empirical measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Estimating the marginal instead</head><p>In the discussion of density estimation (Section 4.1) we haven't done any specific distinction on the particular form the data for Problems 2 and 1. Definitions 3-10 work for all kinds of data. In the particular case of the ML Problems 1 and 2, our objective is to model in the class C the dependence of Y on X penalized by the loss function L. We aim to study how good (with respect to L) a C-model f (X) approximates Y . In this context the distribution π refers to that of (X, Y ). Measure pre-conditioning amounts to approximating π using the sample in a way that benefits computations. We note that this gives rise to two different approaches: (a) We can estimate π directly via π n according to definitions 3 -10. (b) We can make assumptions on the conditional distribution of Y |X and then use definitions 3-10 for approximations on the X-marginal of π. Most of the study of this document has focused on approach (a). Let us give an example of the approach (b) to show it's interaction with measure preconditioning.</p><p>Theorem 17 Assume that Y |X = x ∼ νx and that we have estimated νx via ν x n such that ν x n dp --→ νx uniformly on x, i.e. given ϵ there exists N &gt; 0 such that for every n ≥ N dp(νx, ν x n ) &lt; ϵ for every x assume also that µn dp --→ µ, and L : R p × R → R is continuous. Let f ∈ C and assume that there exists g ∈ L 1 (µ) such that L(f (x), y)dν x n (y) ≤ g(y).</p><p>and that y → L(f (x), y)dν x n (y) is continuous and bounded, then</p><formula xml:id="formula_55">L(f (x), y)dν x n (y)dµn(x) n→∞ ----→ Eπ[L(f (X), Y )].</formula><p>Proof The proof is a direct consequence of dominated convergence applied twice, observe that</p><formula xml:id="formula_56">L(f (x), y)dν x n dµn(x) -L(f (x), y)dπ = L(f (x), y)dν x n dµn(x) - L(f (x), y)dν x n dµ(x)+ L(f (x), y)dν x n dµ(x) -L(f (x), y)dπ.</formula><p>the first term goes to zero if we ensure y → L(f (x), y)dν x n (y) is continuous and bounded, the second term goes to zero by dominated convergence (using µ as reference measure). □ Remark 14 (On the general approach and the restrictiveness of the hypothesis on Theorem 17).</p><p>Theorem 17 is only one example of the multiple approaches one can use to estimate π from µn and assumptions on Y |X, even though the hypothesis of Theorem 17 are very difficult to meet in practice, it is presented here to illustrate the general idea. Measure pre-conditioning on the marginals ν x allows the modeller to include the specific features of each data class. It is clear the many lines of investigations one can explore to get similar results (with less restrictive hypothesis), we choose not to develop any further and leave it for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The recipe: How to choose a measure and how to implement the algorithm</head><p>The general approach for this document is to put in a single, standard, theoretical background many ideas that have come to light in ML-research. Namely, ML-reaserchers have realized that their algorithms improve in performance or convergence properties after a small "tweak" to either data or the loss function occurs. Stability of ML-algorithms has been widely known and is one of the main focus of ML-research. The idea of measure pre-conditioning is that the standard empirical distribution, though it may contain all the possible information in terms of inference (except for order) may not be well adapted to the specific problem one aims to minimize. It is well-known for example that if the functional to be minimized is convex, algorithms used for minimization can take advantage of convexity. This encourages the solver to find an empirical estimation from definitions 3-10 that makes their functional convex.</p><p>Finding such a measure is what we call pre-conditioning, if the preconditioning satisfies any of the assumptions of Proposition 6 then one is ensured to have a full learner recovery system and hence have not lost anything on the process while achieving improved performance. One could instead use a pre-conditioner based on many reasons (such as having a specific algorithm to compute already at hand for example), this work explains how as soon as a condition like Proposition 6 is satisfied, one will end up with the same classifier/regressor.</p><p>5 The problem of Domain Adaptation and the impact of measure pre-conditioning Domain Adaptation (DA) is a sub-problem of transfer learning on which one aims to infer the parameters for a new learning agent in terms of an agent that learn in similar data. Many of the DA adaptation formulations are well-suited for Optimal Transport (OT), our framework of Problems 2 and 1 was motivated at first by the recent research in optimal transportation in Machine Learning (see <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b2">[3]</ref>) and so in this section we explore the implications of measure-preconditioning in the specific case of domain adaptation problems related to optimal transportation and the recent research in the area (see <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b2">[3]</ref> and references therein for a more complete exposition of the use of optimal transportation in machine learning). </p><formula xml:id="formula_57">min f ∈C n k=1 E[L(f (X s i ), Y s i )] (<label>29</label></formula><formula xml:id="formula_58">)</formula><p>If f * s realizes the minimum in <ref type="bibr" target="#b28">(29)</ref>, we say that it is the learnt agent or that f * s correspond to the learnt parameters. Now suppose we have another sample (X T 1 , X T 2 , . . . , X T n2 ) which we believe is similar in some features to the original sample. The domain adaptation problem is: How much can one learn from the previous learning? That is, how can we transfer the learning from the source domain to target domain?.</p><p>The research field which attempts to answer Problem 3 is known as Domain adaptation for transfer learning. For a general introduction and approach see <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b26">[27]</ref> and references therein. The problem of domain adaptation 3 is different to Problems 1, 2 as it aims to transfer the statistical knowledge obtained by a minimization on sourcedomain to a minimization on the target-domain. The formulation of Problem on <ref type="bibr" target="#b28">(29)</ref> has the implicit assumption of the empirical distribution being imposed at level n. In this section we aim to explain how measure pre-conditioners as defined in section 4.1 can be used in the field of DA for transfer learning.</p><p>Problem 4 (Domain Adaptation and transfer learning with varying losses and classes) Suppose that we have a sample (X s 1 , X s 2 , . . . , X s n ) of features together with the a sample of the dependent variable (Y s 1 , Y s 2 , . . . , Y s n ) and we use the learning agent to minimize a loss function Ls : R p × R → R among a class of functions C ∫ . The learning problem is to obtain the best possible parametric function f , among the class C explaining the data, i.e.</p><formula xml:id="formula_59">min f ∈C ∫ Eπ n [Ls(f (X s ), Y s )] (<label>30</label></formula><formula xml:id="formula_60">)</formula><p>and compare it with the perfect learner on target domain with class C t and loss function L t : R p × R → R:</p><formula xml:id="formula_61">min f ∈Ct E π t [L t (f (X t ), Y t )] (<label>31</label></formula><formula xml:id="formula_62">)</formula><p>If f * denotes the minimizing argument for (30), the Domain Adaptation problem is: How can we use f * to obtain good estimates for (31)? What is the structure of such agent? How does it compare to the actual minimizer of (31)?</p><p>Suppose that every X s i ∼ µ s and X t i ∼ µ t , under "similarity assumptions" on µ s and µ t , one expects to be able to transfer learning to some accuracy. Of course "similarity assumptions" depends on the context of the ML-task in hand. For example, two measures might be considered similar in a classification problem that may not be considered similar in a generative model. In the same fashion, suppose that µ s and µ t satisfy that there exists a solution, T , for Problem <ref type="bibr" target="#b15">(16)</ref> with a given cost function c : R p × R p → R. A good candidate for a new learnt agent can be immediately obtained via f * • T -1 . As seen in <ref type="bibr" target="#b2">[3]</ref>, the error made by this agent relative to the total error obtained from training an agent from scratch can be controlled as soon as µ s and µ t are d 2 -close and C is rich enough. In the field of Domain Adpatation (DA) usually at least one of the following assumptions is made:</p><formula xml:id="formula_63">Assumption 1 (Conditional structure of learning task) In the context of Problem 4, if (X s , Y s ) is the source variable and (X t , Y t ) the target variable, it is common to ask that (Y s i | X s i ) ∼ (Y t i | X t i ), (<label>32</label></formula><formula xml:id="formula_64">)</formula><p>where Y | X denotes the random variable whose law is the regular conditional probability of Y given X.</p><p>This assumption means that the probabilistic structure of the dependence of Y on X is the same in both domains. We understand this assumption as a strong hypothesis of similarity in the modellings.</p><p>Assumption 2 (Identical dependence) In the context of Problem 4, if (X s , Y s ) is the source variable and (X t , Y t ) the target variable, it is common to ask that (Xs, Ys) ∼ (X t , Y t )</p><p>The identical dependence assumption has been used extensively but is in general not a good idea to pre-impose. The identical assumption implies that any sample of the source domain can be considered a sample of the target domain so if L s = L t and C s = C t then the learning transfer is perfect as we can identify the source data as target data in the empirical destimation of π s = π t The following assumption can be found in recent papers in DA-ML, see <ref type="bibr" target="#b4">[5]</ref> for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 3 (c-optimal map)</head><p>There exists an optimal transport map (with respect to a cost function c : R p ×R p → R) Tc as in ( <ref type="formula" target="#formula_31">16</ref>) that satisfies</p><formula xml:id="formula_65">(X s i , Y s i ) ∼ (Tc(X s i ), Y t i ).</formula><p>Remark 15 Though it is straightforward to use Assumption 3 (postulated in <ref type="bibr" target="#b4">[5]</ref>) in the context of optimal transportation, it is of significant importance to understand the necessary conditions that yield this assumption.</p><p>Remark 16 Note that these assumptions and the framework of DA is closely related to the line of investigation proposed in Remark 14 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">General Idea in the non-linear case</head><p>Domain Adaptation should be used when the target and source measures are believed to be similar. If the source measure satisfies the assumptions of Brenier's Theorem (see <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">Theorem 2.32]</ref>) and the loss function is quadratic (or strictly convex function of quadratic distance) the optimal transport map T transporting µ s onto µ t can be used as an learning agent on the target domain.</p><p>We do this by first mapping onto the source domain using the optimal transport map and only then evaluating the agent that has learnt paramters, i.e. define f ad a candidate for the minimization of loss for learning agents in the target domain by</p><formula xml:id="formula_66">f ad = f * (T -1</formula><p>). The work in <ref type="bibr" target="#b4">[5]</ref> shows a convergence for this agent under Assumption 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main question: What cost should we impose?</head><p>Note that Assumption 3 is an existence condition. If there exists a cost function for Assumption 3 one would need to check that it satisfies the conditions for existence and uniqueness of optimal transport maps like regularity and the twist condition (see <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>). In the general approach for DA using transfer learning via optimal transport in the framework of Problem 4, two problems seem to arise more often in practice: P.i) When the conditions of the trainings are fixed and not to be chosen: study a learnt agent when L 1 , L 2 , C 1 , C 2 are given and fixed. P.ii) When we are able to choose L 1 , C 1 with the goal of maximizing (in any way) the transfer learning for a given loss function L 2 and class C 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">A measure of transferrability</head><p>In Problems 30 and 2, we start under a similarity assumption on the source measures. This follows an intuitive statement: in order to be able to transfer any learning, the original measures should share some features. We can't expect to transfer any learning if the problems have nothing in common.</p><p>We may expect to transfer the learning (classifier) differentiating between dogs and cats to a new agent aiming to differentiate wolves and lions. In this case the distribution of dogs and cats is believed to be similar to that of wolves and lions.</p><p>How much could we transfer? Could we guess beforehand how much learning we can transfer? As a thought experiment, let us study a way to measure the transfer of learning. There are many ways to measure transferability, see <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref> or references therein. We propose another one, assume that π s and π t are as in Problem 3, let h : R → R be any strictly convex function with h(0) = 0, set</p><formula xml:id="formula_67">d h (π s , π t ) = inf Π∈Π(π s ,π t ) h(L 1 (f 1 (x 1 ), y 1 )-L 2 (f 2 (x 2 ), y 2 ))dΠ((x 1 , y 1 ), (x 2 , y<label>2 )).</label></formula><p>(33) where f 1 is the solution for the C 1 , L 1 -source problem and f 2 the corresponding solution for the C 2 , L 2 -target problem. Evidently, a-priori the value of d h (π s , π t ) can not be computed as f 1 , f 2 are unknown and the value of (33) depends on the choice of models (C 1 , L 1 ) and (C 2 , L 2 ). We claim (33) is a reasonable way to measure transfer depending on C 1 , L 1 , C 2 , L 2 , in the sense that the closest d h is to 0 the more likely it is that a learnt agent for the L 1 problem with source data (X s , Y s ) would perform decently in the L 2 problem with data (X t , Y t ). This is to be expected as it may be reasonable to transfer the learnt agent for certain loss functions but not with all of them. Even though f 1 and f 2 are unknown, in some cases some estimates can be obtained. To the knowledge of the author no measure of transferrability of the form (33) has been studied which points to a promising line of investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Problem 1</head><p>Let us first address problem P.i) where all the conditions (C 1 , C 2 , L 1 , L 2 ) are fixed and we aim to measure the efficiency of a solution to (1) and (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Measure pre-conditioning in the conditional average guess</head><p>Let us consider here a different approach to the general Problem 4, suppose that we have solved the source problem i.e.</p><formula xml:id="formula_68">f * ∈ arg min f ∈C1 E π s [L 1 (f (X), Y )].<label>(34)</label></formula><p>Similar to the ideas in <ref type="bibr" target="#b4">[5]</ref> one can make assumptions like Assumption 3 in order to benefit from the source sample by using conditional distributions. Given y ∈ spt(Proj 2 #π s ) and f ∈ C 1 , assume we can find T f,y optimal transport map for the cost function c y (x, x) = |L 1 (f * (x), y) -L 2 (f * (x), y)| between the conditional distributions π s (x|Y = y) and π t (x|Y = y). The question is now how to generate an element in C 2 from the learnt information on the conditional distributions. The first immediate guess is to average with respect to the target distribution, that is if</p><formula xml:id="formula_69">dπ t (x, y) = dπ t (x|Y = y)dν t (y)</formula><p>a guess for a learnt agent would be</p><formula xml:id="formula_70">f ad = f * • (T f * ) -1 , where T f * (x) = Y T f,y (x)dν t (y).<label>(35)</label></formula><p>In the general case, no estimates on the control of learning for agent (35) are known.</p><p>It is expected that if the measures satisfy that d h from (33) is small then the agent obtained using (35) is good although so far no precise statements have been shown. Formula (35) is a reasonable guess because it takes into account the best agent at each y before averaging over all y ∈ Y .</p><p>Open Question 1 In the context of Problem 4, is it true that if d h (π s , π t ) is small, then f * ad performs well in (31) when constructed using (35) and pre-conditioning? Is this performance quantifiable? Is it true that as n → ∞,</p><formula xml:id="formula_71">Eπ n [L 2 (f * n (X), Y )] -min f ∈C2 [L 2 (f (X), Y )] → 0?.</formula><p>Can such performance be studied by d h of (33) when h(r) = |r| ?. How does (35) compare to</p><formula xml:id="formula_72">f * • T 2 , where T 2 = Y (T f,y ) -1 (x)dν t (y)?<label>(36)</label></formula><p>This questions are relevant both in the field of transfer learning and to measure pre-conditioning. The computation of T f,y may be difficult in practice and we expect measurepreconditioning for every y to benefit the performance of the intermediate algorithms without disruption on convergence. Numerical simulations are being performed to corrobate this idea and study the performance of (35) and will appear in subsequent works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Data-driven conditional OT</head><p>On <ref type="bibr" target="#b30">[31]</ref> the authors studied the following problem given a cost function c in the product space X × Z and a probability measure on X × Z:</p><formula xml:id="formula_73">min T (•,z) ∀zT (•,z)#ρ(•|z)=µ(•|z) c(x, T (x, z))dρ(x, z)<label>(37)</label></formula><p>which they denoted the data-driven optimal transport problem. In the same work, the authors showed that the minimization of (37) is equivalent to min</p><formula xml:id="formula_74">T (•,z) max λ≥0 c(x, T (x, z))dρ(x, z) + λ Ent µ(•|z) 1 2 (T #ρ(, z) + µ(•, z)) (38)</formula><p>The dual formulation of (37) via (38) already hints a connection with our work. As the algorithm implemented in <ref type="bibr" target="#b30">[31]</ref> is a sequential algorithm using gradient descent, it can be interpreted in the sense of measure pre-conditioners that entropically regularize at every discrete step n, just as Definition 8 in the framework of Wasserstein distance and problem 2. This means that an algorithm to compute data-driven conditional optimal transport can benefit directly from measure-preconditioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Control on optimal transport domain adapted learning</head><p>In this section we present different hypothesis and assumptions that yield stability results on transferred learning. The results are not as strong as those conjectured in section 5.4.1 but directly related to measure pre-conditioning. It is evident that there are many options on C 1 , C 2 , L 2 , L 2 that will ensure the transfer learning is efficient. In this section we reduce to present the most straight-forward formulations.</p><p>Proposition 18 Let T be any map with T #µ = ν and dπ 1 (x, y) = dπ 2 (T (x), y) if</p><formula xml:id="formula_75">C 1 • T = C 2 then arg min f ∈C1 Eπ s [L(f (X s ), Y s )] = arg min f ∈C2 Eπ t [L(f (X t ), Y t )]</formula><p>The proof is a direct consequence of the composition of classes</p><formula xml:id="formula_76">C 1 • T = C 2 . Proposition 19 If C 1 = C 2 = C and L 1 = L 2 and if C is so that (x, y) → L 1 (f (x), y)</formula><p>is Lipschitz and bounded then for every f</p><formula xml:id="formula_77">|Eπ 1 [L 1 (f (x), y)] -Eπ 2 [L 1 (f (x), y)]| ≤ d 1 (π 1 , π 2 )</formula><p>and so the total loss of transfer learning when the learned agent is adapted is controlled by the d 1 -distance between joint measures.</p><p>Proof The proposition follows directly from the Kantorovich-Rubinstein representation of the d 1 norm as d 1 is the suprema over Lipschitz functions. □</p><p>In <ref type="bibr" target="#b2">[3]</ref> the authors proved the following theorem: See in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">Theorem 3.1]</ref>. We now generalize this idea before we continue.</p><p>Theorem 21 Let πs, π t be the joint measures for the the source (X s , Y s ) and (X t , Y t ) target domains respectively. Denote µs and µ t the projections into the Xcoordinates of πs, π t and by µ s</p><p>x and µ t x the conditional distributions of Y s |X s and Y t |X t . Assume there exists a map T : R p → R p such that</p><formula xml:id="formula_78">1. T #µs = µ t 2. µ t T (x) = µ s x 3. C 2 = T • C 1</formula><p>if f is the solution for (30) then f • T -1 is a perfect learner in the sense that it minimizers <ref type="bibr" target="#b30">(31)</ref>.</p><p>Proof The proof relies only on the disintegration of measures, as</p><formula xml:id="formula_79">E π t [L(f (X t ), Y t )] = L(f (x), y)dµ t x (y) dµ t (x) = L(f (T (x), y)dµ s x dµs(x)</formula><p>where we have used the condition dµ t T (x) = dµ s (x) in the last equality. Minimization over C 2 and the condition C 2 = T • C 1 yields the result. □</p><p>Open Question 2 (Can learning error be totally controlled?) Assume f * minimizes the target problem, under what conditions on µ, ν, L, C 1 , C 2 does there exist C &gt; 0 such that</p><formula xml:id="formula_80">1 n n2 i=1 E[L(f * (X t i ), Y t i ) -L(f ad (X t i ), Y t i )] ≤ Cd 2 (µs, µ t )?</formula><p>The previous theorems and the ideas of <ref type="bibr" target="#b4">[5]</ref> respond this question in very restricted situations. Having a general context to answer this question similar to the one of 2 would be essential for the theory of domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Numerical examples</head><p>In this section we present 2 simple numeric examples using the mnist data set:</p><p>1. First we exemplify the convergence of Theorem 1 and Proposition 6 by considering convolutional neural networks applied to a gaussian-filter blurred version of the data set mnist. 2. In the second part of this section we apply the conditional average guess of section 5.4.1 to try to predict whether an image corresponds to a 6 or a 7 using the model trained only on differentiating 1s from 9s. The underlying hypothesis is "that sixes are a lot like nines and ones are a lot like sevens". We explain what this means and how to use the conditional average guess. Both experiments can be found publicly in the github repository joaxchon\slash measure_precon with the goal of reproducibility. These examples should be understood as "Toy Examples" as numerical tests with much more detail and precision will be saved for a work in preparation with several co-authors. The idea of this section is to illustrate the main features of measure pre-conditioning and explain the results of the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Convergence of agents under gaussian filter blurring</head><p>We consider the mnist data set and use the keras and tensorflow packages to train a convolutional neural net under the modified images. We modify each image by first applying a gaussian filter with variance σ to blurr it. At each fixed level of σ we obtain a learnt agent f n,σ as in Problem 2 and show that both the losses and the accuracy converge to the agent f n,0 as in Proposition 6 considering the blurred image as corresponding to the measure µ X * N σ , where N σ denotes the unbiased normal distribution with variance σ. By proposition 6 and Theorem 1 we expect the learnt agents f n,σ to get close to f n,0 , although the precise formulation of the Theorem ensures the convergence as n → ∞ of lim σ→0 f n,σ to f 0 . Note that our theorem also ensures the converges of the weights (in appropriate sense). Proposition 6 ensures that if we apply a technique for unblurring (Weiner filters, Tychonov's regularization, etc) convergence of the learning agents is guaranteed. We see this result in the following figures: Figures 4 and 5 represent the total accuracy and total loss of the model during training. Different colors correspond to changes on the variance parameter of the gaussian filter. Given the convergence (in weak sense) of the convolutions with gaussians, proposition 6 indicates the convergence seen in the plots. Figures 6 and 7 show the behaviour of the agent as we pre-condition (by deblurring). The change in the training set improves the performance. Note that the conditions of Theorem 1 ensure that we will see similar behaviour as long as we ensure the method satisfies Definition 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The conditional average guess and optimal domain adaptation for 6s and 7s</head><p>In this section we use the mnist dataset to exemplify the technique of measure pre-conditioning for the conditional average guess of section 5.4.1. The idea is the following: 1. We train a convolutional neural network only on a data base formed by 1s and 9s. The objective is to classify whether a new imput is a 1 or a 9. We use sparse cross-entropy as loss function for this step of the learning.</p><p>2. We make the following assumption: The distribution of 1s is similar to the distribution of 7s and the distribution of 6s is similar to that of 9s. 3. We use the Python Library POT to approximate optimal transport between the distribution of 1s and 7s and that of 6s and 9s. We use c(x, y) = |x -y| as cost function. Independently of the loss function for the model, we note that the cost function c penalizes absolute distance without taking into account the shape, as pre-conditioning we flip every 6 to make it closer to a 9. 4. Finally we use formula (35) as new model to obtain a new agent. 5. We test this agent. Observe that (35) requires the knowledge of T (x) for every x in the support of the measure, nevertheless the computational package can only provide a matching between samples. In order to approximate (35) we approximate via</p><formula xml:id="formula_81">f * =T -1 (Proj {X 1 i } (x)) • ||x -Proj {X 1 i } (x)|| ||Proj {X 1 i } (x) + Proj {X 9 i } (x)|| + T -1 (Proj {X 9 i } (x)) • ||x -Proj {X 9 i } (x)|| ||Proj {X 1 i } (x) + Proj {X 9 i } (x)||</formula><p>where {X k i } n i=1 corresponds to the sample associated to y = k in the training set (the conditional sample).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Results</head><p>After the training and the computation of the learnt agent via (5.4.1), using the testing data and we obtain that 51.94% of 7s were correctly classified by the model and 99.584% of 6s were correctly labelled. This is due to the fact that the distribution of 6's and the distribution of flipped 9's is indeed very similar but the distribution of 1s and 7s have more differences. This is exactly what we expected as the Wasserstein distance between the distribution of 6's and flipped 9's is indeed very small, making the conditional guess of (5.4.1) efficient on identifying 60s with the knowledge of 9's and 1's.   We can see that the agent (5.4.1) mistakes 7s for 6's when the middle line on the 7 is big and hence increasing the Wasserstein distance to the distribution of 1's.</p><p>In a following work in preparation, additional to the flipping method to relate 6s and 9s we will pre-condition the distribution of 1's by adding noise in a way to make it look more like 7's. Theorem 1 shows that the learnt agent converges to the original one as we reduce the noise to 0. The script can be found in joaxchon\slash measure_precon\conditional_average_adaptation.py</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Outside of the framework</head><p>In this section we explain how the framework developed in this article can be extended to encompass more general situations (whose formulation is not exactly represented by ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>)) but benefit from the same ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Using pre-conditioners on WGANs</head><p>The Wasserstein Generative Adversarial Networks (WGAN) introduced in <ref type="bibr" target="#b3">[4]</ref> is a generalization of the generative adversarial networks (GAN) introduced in the seminal work <ref type="bibr" target="#b5">[6]</ref>. The reason to consider the Wasserstein framework is due to the convergence properties of the Wasserstein metric together with the representation of Kantorovich-Rubinstein. The WGAN problem consists in computing arg min</p><formula xml:id="formula_82">θ arg max w∈W E[f w (X)] -E[f w (g θ (Z))]<label>(39)</label></formula><p>where X ∼ P 1 is prescribed, Z ∼ P 2 and {g θ } θ∈Θ is a parametric function space. Further work would study the same principles applied in this document to the more general version of the problem admitting (39) using maybe 2 parametric families C, C. The only difference between our problem and (39) is the presence of an extra outer minimization problem. It is clear that algorithms like TTC presented in <ref type="bibr" target="#b6">[7]</ref> that take a dual approach can benefit from sequential measure-pre-conditionining. In the original formulation, as in <ref type="bibr" target="#b3">[4]</ref> sup</p><formula xml:id="formula_83">f ∈C f dµ -f dν + -λ (|∇f | -1) 2 dσ</formula><p>where Z ∼ σ iff Z = tX +(1-t)Y where t ∼ U [0, 1], note that we can replace µ and ν at level n via the empirical measures or measure pre-conditioners. This means that measure-preconditioning can be applied in more general circumstances than Problem 1 as the estimation of σ can be done via tX n + (1 -t)Y n where t ∼ U [0, 1] and the triangle inequality yields convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Covariate shift domain adaptation problem</head><p>In general, the label-shift domain adaptation problem is usually written as min h,g</p><formula xml:id="formula_84">1 n n i=1 L(h(g(x s i )), y s i ) + λ Ent(µ g s |µ g t ) + Ω(h, g)<label>(40)</label></formula><p>where h is the hypothesis, g is a representation mapping and Ω is a regularization term. The first term corresponds to losses in approximation while the second and the third correspond to regularizations. Compared to the framework used in Problems 30 and 31, ( <ref type="formula" target="#formula_84">40</ref>) is a more general version. Nevertheless, the idea of measure pre-conditiniong can substitute the entropy term by using a sequence of entropic regularizations and Ω + L can be used as a modified loss function. The difference in algorithmic performance of both approaches is an interesting project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">COOT and measure pre-conditioning</head><p>In <ref type="bibr" target="#b9">[10]</ref>, the following problem was introduced to handle at the same time the disparity between correlated distributions and the data marginals. In the case where X i ∈ R p , authors in <ref type="bibr" target="#b9">[10]</ref> consider the matrix X = (X 1 , . . . , X n ) not only as a sample where the randomness comes form a single distribution but as a doubly-random matrix in the sense that each row is considered a sample and the columns are consider features, in this context let µ S denote the probability measure associated to samples and µ F the associated feature distribution one should perform optimal transport simultaneously in sampling and feature spaces. We expect the techniques of the two previous sections to also work in this context mutandis mutatis.</p><p>8 Researcher's criteria on measure pre-conditioning</p><p>In section 4.5 we explained what a ML-developer should consider as recipe for applying measure pre-conditing. It explained that each modification of the n-level measure had different implications which should be pointed towards some (algorithmic) benefit. In general, it may be difficult to know a-priori exactly what to use and so this (and subsequent) work should be considered as a guideline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Trade-offs</head><p>In low-dimensional regimes, absolutely continuous (w.r.t. Lebesgue) tend to behave better, while in higher dimensions highly concentrated measures tend to have better properties, see <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">Chapter 4]</ref>. This is already a hint on what to do, if the problem involved has few features, absolutely continuous measures may improve the performance of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions and further work</head><p>Recent work <ref type="bibr" target="#b9">[10]</ref> has introduced new techniques for domain adaptation, the idea is to optimally match features and samples, it is still open lines of investigation how different measure pre-conditioning techniques would impact the co-optimal transport problem. The features and samples are in general of very different nature for which combining more than one of the techniques of section 3.5 could improve the performance of the algorithms. For example, it may be the case that features share a structure that can be exploited by a specific technique while the relation between samples may algorithmically benefit from another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Order of convergence</head><p>Establishing that the ML problem gives a full learner recovery system is good in order to know convergence is ensured, in algorithmic practice we need more.</p><p>We need to study the order of convergence and the imrpovements on this order by Measure pre-conditioners, this work is left for future work and other researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.1">Data-driven model changes and convergence</head><p>In the start of section 3.1 we asked question (iii): Given a choosing of π n 's, could we find sequences L n 's and C n so that the computations on the C nproblem with loss function L n associated to π n converge to 1? Could these problems improve the algorithmic performance?</p><p>In section 3 we studied conditions on C and L to ensure Definition 2 and consequently Theorem 5. The question of how and when to change C n and L n at every step is still open and interesting. A good answer would yield heuristics to change the model given the data in terms of the parametric space, this means to not only change the way we measure the information from the data but also how we learn from it. This line of investigation is left for future work.</p><p>9.2 k-nearest neighbohrs and relation to meta-transport</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.1">k-nearest neighbors and point-process notation</head><p>The list of empirical estimating probabilities (section 4) is obviously nonexhausting. Algorithmic treatment of data such as k-nearest neighbors represent a potentially significant pre-conditioning method. The theory of this algorithms is usually developed through point-processes. The extension of this work to point-processes together with section 9.1.1 is a promising area for mathematical theory of learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.2">Meta-transport</head><p>Another recent development in Optimal Transport based machine learning is the development of meta-optimal transport in <ref type="bibr" target="#b11">[12]</ref>. The basic idea, similar to the basic idea of this document is to present a way to improve the performance of ML-algorithms through pre-working on them. The seminal work <ref type="bibr" target="#b11">[12]</ref> develops completely algorithmic-focused techniques, as explained in section 8.1. This work is focus on the underlying structured of pre-condiitoning the samples, the statistics in Wasserstein space and how they impact the outputs of the algorithms. In some way, <ref type="bibr" target="#b11">[12]</ref> tackles the pre-conditioning/pre-measure pre-conditioning in a different manner, with a clever approach based on numerical algorithms. We expect that a theory similar to the one developed in section 4 can also encapsulate the algorithmic pre-conditioning. This can be modelled via point-processes (as it's done for k-nearest neighbors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">General disintegration estimates</head><p>One can study different conditions on L, C, µ, π, Y |X such that a convergence similar to Theorem 17 occurs. This area is particularly technical as disintegration is not a continuous operation with respect to some metrics on spaces of probability measures. Generally, one does not necessarily need to estimate the disintegration but can explore different methods of convergence. An approach to full learner recovery systems (2) in the special case of assumptions on Y |X would be interesting and related with sections 5.4, 5.4.1 and literature as to <ref type="bibr" target="#b30">[31]</ref> and references therein.</p><p>9.4 Problem 2 of section 5.2</p><p>If L can be chosen thinking ahead of the Target problem, choose the cost function by chosing an L 1 depending on L 2 or viceversa. The idea of the problem is to ensure learning can be transferred by picking the problems with the goal of transferring. A full theory with the approach of training with the goal of transferring would be interesting on it's own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.1">Choosing the first Loss function to improve the second</head><p>With the same approach as in Section 9.4, if we know that we aim to solve the target problem for L 2 , π t , C 2 , what loss function L 1 should we chose given π s and C 1 ? Similarly, allow C 1 to be chosen. We should chose L 1 in a way that data under π s behave similar to L 2 under π t . How one takes the target problem into consideration is an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">Choosing the target loss model according to the source</head><p>Assume we have solved Problem 1 with set of features L 1 , C 1 , π s and we know there is a distribution (unknown to us) on which we aim to transfer the knowledge, what loss function L 2 would ensure good properties of the learnt agent on target space? One can think of an L 2 loss function that penalizes the error of the learnt agent and simultaneously penalizes the difference between probabilities. This function would take into account that a mistake in the model is not relevant when one knows the error on difference of distributions is big. The L 2 loss function could be used to simultaneously control model error with (probability) transfer error.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Problem 2</head><label>2</label><figDesc>Let Ω ⊆ R n be convex and compact. Assume we have data X ∼ µ ∈ P(Ω) and we aim to do a Machine-learning model towards a dependent variable Y ∈ Y where (Y, d Y ) is a separable complete metric space, we denote by π ∈ P(Ω×Y) the joint distribution of (X, Y ). Given L : Y × Y → R (called a loss function), let C ⊆ Y Ω and assume d is a distance function on C, the C-optimal model for L under π is the following non-linear program arg min f ∈C Eπ [L(f (x), y)] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 2 (</head><label>2</label><figDesc>Full learner recovery system) In the context of Problem 2, we say that (C, d, L, m -→) forms a full learner recovery system if it holds that 1. If πn m -→ π for all d-converging sequences sequences fn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 5</head><label>5</label><figDesc>If (C, d, L, m -→) forms a full learner recovery system (Definition 2) where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>, d point-wise convergence and (x, y) → L(f (x), y) uniformly Lipschitz and uniformly bounded for f ∈ C (compact metric space).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>, L(x, y) is d-continuous on the first coordinate and uniformly bounded by some constant M &gt; 0 on a compact metric space (C, d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Remark 13</head><label>13</label><figDesc>Notice that from Theorem 11 one can infer the convergence of the Histogram (Definition 4) weakly in R p . Theorem 14 (Devroye) If H 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Proposition 15 (</head><label>15</label><figDesc>Barycenters dp converge) If µ has compact support and µn is the a p-Wasserstein Barycenter of Definition 6, then µn dp --→ µ as n → ∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Proposition 16 (</head><label>16</label><figDesc>Total variation)The mmd k minimizer of Definition 10, π mmd k n converges in total variation norm to the solution π * of unrestricted transport with respect to c (Definition 16), i.e.π mmd k n ||•|| T V ----→ π * .See<ref type="bibr" target="#b19">[20,</ref> Theorem 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Problem 3 (</head><label>3</label><figDesc>General domain adaptation problem)Suppose that we have a sample (X s 1 , X s 2 , . . . , X s n ) of features together with the a sample of the dependent variable (Y s 1 , Y s 2 , . . . , Y s n ) and we use the learning agent to minimize a loss function L : R p × R → R among a class of functions C. The learning problem is to obtain the best possible parametric function f , among the class C explaining the data, i.e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Theorem 20 ( 1 n n k=1 δ x s i where x 1</head><label>2011</label><figDesc>Courty-Flamary) If L(x, y) = |x -y| 2 and µ s = , x 2 , . . . , xn ∈ R n and there exist A positive definite matrix and a vector b such thatµ t = 1 n n k=1 δ Ax s i +b , set T (x) = Ax + b then f * • T -1is a perfect learning agent in the sense that it minimizes<ref type="bibr" target="#b30">(31)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 1 5 Fig. 3</head><label>153</label><figDesc>Fig. 1 Unblurred, σ = 0 Fig. 2 Blurred, σ = 0.5 Fig. 3 Blurred, σ = 1</figDesc><graphic coords="30,51.02,450.56,107.94,80.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Loss function during training Fig. 5 Accuracy of the model during training</figDesc><graphic coords="31,51.02,55.28,168.67,126.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Change in loss during de-blurring Fig. 7 Change in accuracy durin de-blurring</figDesc><graphic coords="31,51.02,275.81,168.67,126.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Example of a 6.Fig. 9 Flipped 6 looks like a 9.</figDesc><graphic coords="32,219.69,464.12,168.67,126.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 9</head><label>9</label><figDesc>Fig. 8 Example of a 6.Fig. 9 Flipped 6 looks like a 9.</figDesc><graphic coords="32,51.02,464.12,168.67,126.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 Missclassified 7. Fig. 11 Correctly classified 7.</figDesc><graphic coords="33,51.02,55.28,168.67,126.50" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the empirical estimation of integral probability metrics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fast Computation of Wasserstein Barycenters, Proceedings of the 31st International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="685" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal Transport for Domain Adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2615921</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1853" to="1865" />
			<date type="published" when="2016-10-07">2017 Sep. 2016 Oct 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">El</forename><surname>Alaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<title level="m">Optimal Transport for Conditional Domain Matching and Label Shift, Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="1651" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Optimal Transport, Congested Transport, and Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Milne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Topics in Optimal Transportation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graduate Studies in Mathematics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<date type="published" when="2003">2003</date>
			<publisher>American Mathematical Society</publisher>
			<pubPlace>Book</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<title level="m">A series of Comprehensive Studies in Mathematics</title>
		<meeting><address><addrLine>Book; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Optimal Transport: Old and New</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Redko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Co-Optimal</forename><surname>Transport</surname></persName>
		</author>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Santambrogio</surname></persName>
		</author>
		<title level="m">Optimal Transport for Applied Mathematicians Calculus of Variations, PDEs, and Modeling, Progress in Nonlinear Differential Equations and Their Applications</title>
		<meeting><address><addrLine>Book</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Redko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meta Optimal Transport</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gamma-convergence for beginners, Book, Oxford lecture series in mathematics and its applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Braides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Oxford University Press</publisher>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Royden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Fitzpatrich</surname></persName>
		</author>
		<title level="m">Real Analysis, Book, Fourth Edition</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gyorfi No empirical probability measure can converge in the total variation sense for all distributions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1496" to="1499" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On arbitrarily slow rates of global convergence in density estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<idno type="DOI">10.1007/BF00534199</idno>
		<ptr target="https://doi.org/10.1007/BF00534199" />
	</analytic>
	<monogr>
		<title level="j">Z. Wahrscheinlichkeitstheorie verw Gebiete</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="475" to="483" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Remarks on some nonparametric estimates of a density function</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="832" to="837" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">An Invitation to Statistics in Wasserstein Space, Book, SpringerBriefs in Probability and Mathematical Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
		<title level="m">On Estimation of a Probability Density Function and Mode</title>
		<imprint>
			<date type="published" when="1962">1962</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
		</imprint>
	</monogr>
	<note>The Annals of Mathematical Statistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Manupriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keerti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandhok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Nath</forename><surname>Jagarlapudi</surname></persName>
		</author>
		<idno type="arXiv">arxiv:arXiv:2305.15901</idno>
		<title level="m">Empirical Optimal Transport between Conditional Distributions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Durrett</surname></persName>
		</author>
		<title level="m">Probability: Theory and Examples</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Dudley</surname></persName>
		</author>
		<title level="m">Real Analysis and Probability</title>
		<meeting><address><addrLine>Book; Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000073</idno>
		<ptr target="http://dx.doi.org/10.1561/2200000073" />
		<title level="m">Computational Optimal Transport, Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="355" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Five lectures on optimal transportation: geometry, regularity and applications, Analysis and Geometry of Metric Measure Spaces: Lecture Notes of the Seminaire de Mathematiques Superieure (SMS), Montreal 2011</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mccann</surname></persName>
		</author>
		<editor>G. Dafni et al</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Amer. Math. Soc</publisher>
			<biblScope unit="page" from="145" to="180" />
			<pubPlace>Providence</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><surname>Lieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cordero-Erausquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><surname>Schmuckenschläger</surname></persName>
		</author>
		<idno type="DOI">10.1007/s002220100160</idno>
	</analytic>
	<monogr>
		<title level="m">A Riemannian interpolation inequality à la Borell, Brascamp</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="219" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Billingsley</surname></persName>
		</author>
		<title level="m">Convergence of probability measures, Book, Wiley Series in Probability and Statistics</title>
		<meeting><address><addrLine>Chicago, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><surname>Book</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Polar factorization of maps on Riemannian manifolds</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geom. Funct. Anal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="589" to="608" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fatou&apos;s Lemma for Weakly Converging Probabilities</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kasyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Theory of Probability and Its Applications</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convergence rates for empirical barycenters in metric spaces: curvature, convexity and extendable geodesics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahidar-Coutrix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le Gouic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability Theory and Related Fields</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="323" to="368" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data-Driven Optimal Transport</title>
		<author>
			<persName><forename type="first">G</forename><surname>Trigila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Tabak</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpa.21588</idno>
		<ptr target="https://doi.org/10.1002/cpa.21588" />
	</analytic>
	<monogr>
		<title level="j">Commun. Pur. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="613" to="648" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Dudley</surname></persName>
		</author>
		<title level="m">Real Analysis and Probability</title>
		<meeting><address><addrLine>Book; Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
