<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MODELING GENERALIZATION IN MACHINE LEARNING: A METHODOLOGICAL AND COMPUTATIONAL STUDY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-09-26">26 September 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pietro</forename><surname>Barbiero</surname></persName>
							<email>barbiero@tutanota.com</email>
						</author>
						<author>
							<persName><forename type="first">Giovanni</forename><surname>Squillero</surname></persName>
							<email>squillero@polito.it</email>
						</author>
						<author>
							<persName><forename type="first">Alberto</forename><surname>Tonda</surname></persName>
							<email>alberto.tonda@inrae.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Politecnico di Torino Italy</orgName>
								<orgName type="institution" key="instit2">Universit√© Paris-Saclay</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MODELING GENERALIZATION IN MACHINE LEARNING: A METHODOLOGICAL AND COMPUTATIONAL STUDY</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-26">26 September 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">C77B9BCBD91EBC497BCB95D905A6D83A</idno>
					<idno type="arXiv">arXiv:2006.15680v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convex hull</term>
					<term>Curse of dimensionality</term>
					<term>Data set characteristics</term>
					<term>Extrapolation</term>
					<term>Generalization</term>
					<term>Interpolation</term>
					<term>Machine Learning</term>
					<term>Symbolic regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As machine learning becomes more and more available to the general public, theoretical questions are turning into pressing practical issues. Possibly, one of the most relevant concerns is the assessment of our confidence in trusting machine learning predictions. In many real-world cases, it is of utmost importance to estimate the capabilities of a machine learning algorithm to generalize, i.e., to provide accurate predictions on unseen data, depending on the characteristics of the target problem. In this work we perform a meta-analysis of 109 publicly-available classification data sets, modeling machine learning generalization as a function of a variety of data set characteristics, ranging from number of samples to intrinsic dimensionality, from class-wise feature skewness to F 1 evaluated on test samples falling outside the convex hull of the training set. Experimental results demonstrate the relevance of using the concept of the convex hull of the training data in assessing machine learning generalization, by emphasizing the difference between interpolated and extrapolated predictions. Besides several predictable correlations, we observe unexpectedly weak associations between the generalization ability of machine learning models and all metrics related to dimensionality, thus challenging the common assumption that the curse of dimensionality might impair generalization in machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The term machine learning (ML) traditionally includes algorithms that are able to improve their performance on a specific task over time, given an increasing amount of relevant data <ref type="bibr" target="#b0">[1]</ref>. In recent years, this field of research is enjoying a growing popularity, driven by the breakthrough of Deep Learning <ref type="bibr" target="#b1">[2]</ref> and an impressive track record of success stories in different fields, ranging from natural language processing <ref type="bibr" target="#b2">[3]</ref> to autonomous vehicles <ref type="bibr" target="#b3">[4]</ref>, image classification <ref type="bibr" target="#b4">[5]</ref> human-competitive performance in boardgames <ref type="bibr" target="#b5">[6]</ref>. An interesting online collection about various uses of ML has been compiled by the journal Nature is late 2018 1 , although the fast pace the field is progressing made it to appear outdated after few quarters.</p><p>As out-of-the-box ML solutions are becoming increasingly available to both researchers and the general public <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, theoretical questions are suddenly turning into practical issues. Among all common inquiries, perhaps the most basic is: can ML work on a specific problem? Or, in other words: given the characteristics of a target data set, can the effectiveness of a ML approach be predicted? Interestingly, this latter question can be further rephrased as: what are the characteristics of a data set that are well correlated with the possibility, or the impossibility, of obtaining ML models able to effectively extrapolate to unknown instances of the problem? It is well known that ML algorithms are affected by the curse of dimensionality <ref type="bibr" target="#b10">[11]</ref>, but ML practitioners also know that it could be possible to obtain reliable models even for high-dimensional data sets, and with a relatively small number of samples <ref type="bibr" target="#b11">[12]</ref>. The common approach among practitioners in the field, when dealing with a new data set, seems to be: try as many different ML algorithms as possible in a cross-validation, and evaluate the outcomes; then focus on the techniques that provided the best results, possibly applying them in an ensemble <ref type="bibr" target="#b12">[13]</ref>.</p><p>Taking inspiration from <ref type="bibr" target="#b13">[14]</ref>, where the authors find links between data set characteristics and efficiency of feature selection techniques, we propose to empirically explore the relation between data-set characteristics and effectiveness of standard ML models, in order to obtain a general meta-model able to extrapolate. In order to answer the question, we analyzed 109 publicly available classification data sets from open-access, curated sources. We decided to focus on classification, as supervised ML represents a quite significant portion of real-world problems; and, differently from regression, several sophisticated quality metrics have already been developed for this task <ref type="bibr" target="#b14">[15]</ref>.</p><p>During the analysis, we take into account characteristics such as number of features, number of classes, number of samples, and we look for correlations with quality metrics, such as accuracy of a ML model on training and test points. Extrapolation is assessed not just by alternatively dividing the data into training and test sets, but by analyzing whether data points fall inside or outside of the convex hull of the training data. After collecting the meta-data on the performance of a state-of-the-art classification algorithm on the data sets, the statistical analysis presents both predictable and surprising results, hinting at the fact that dimensionality might not be so cursed after all. Main contributions 1. We ran a quantitative evaluations of ML models over 109 publicly available data sets. 2. In Section 2 and Section 3 we provide a general overview on generalization in ML, and of the so-called curse of dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>In Section 4 we propose to take into consideration the convex hull of the training set to better understand generalization in ML, as it makes it possible to separate cases when an algorithm interpolates or extrapolates on test points. <ref type="bibr" target="#b3">4</ref>. In Section 5 we empirically model associations between the ability to generalize and the data set characteristics, challenging the relationship between generalization ability and the dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The curses of dimensionality</head><p>The curse of dimensionality denotes a variety of different phenomena that impair data analysis if a large number of variables need to be considered at the same time <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. While in most cases it has no closed form nor a unique solution, it has distinctive deleterious effects. Problems like data sparsity, collinearity, and overfitting, seem to confirm the platitude that in ML dimensionality is cursed <ref type="bibr" target="#b10">[11]</ref>.</p><p>As an example, let consider a collection of n data points generated by sampling two random variables X 1 and X 2 originated from two standard normal distributions (¬µ = 0, œÉ = 1.0). The amount of samples falling within the interval x i ¬± œÉ with x i = 1.5 is around the 30% both for X 1 and X 2 , individually (grey histograms in Figure <ref type="figure" target="#fig_0">1</ref>). However, when the joint distribution of X 1 and X 2 is considered, the amount of samples falling within the joint interval (x 1 ¬± œÉ, x 2 ¬± œÉ) with x 1,2 = 1.5 drops. The worst-case scenario arises when the two random variables are uncorrelated (Figure <ref type="figure" target="#fig_0">1</ref>, left).</p><p>As the number of random variables (p) increases, the fraction of points within the p-dimensional interval of radius œÉ decreases rapidly: ‚àº 5% for p = 2, ‚àº 1% for p = 3, and ‚àº 0% for p = 4 <ref type="bibr" target="#b10">[11]</ref>. In practice, the sparser the samples the harder will be collecting data that are representative of the population. The best-case scenario occurs when the random variables are perfectly correlated (Figure <ref type="figure" target="#fig_0">1</ref>, right). In this case, the decrease is still significant, but much slower (from 30% to 20% rather than 5% for p = 2).</p><p>In most real-world scenarios, a considerable number of variables are correlated. As supervised ML algorithms are usually devised to select and use variables strongly correlated with the target variable, the data-sparsity problem may be usually mitigated by considering together highly correlated sets of variables. However, exploiting correlated variables may also have a catastrophic impact when variables are used for prediction: as there could be more than one subset of variables yielding approximately the same result, considering them together can make to make impossible to understand the individual impact of each variable. For example, suppose that two variables X 1 and X 2 can be used to predict a target variable Y by means of a predictive model f :</p><formula xml:id="formula_0">Y = f (X 1 , X 2 )<label>(1)</label></formula><p>Now suppose that there exists another variable X 3 that can be expressed as a function of both X 1 and X 2 , e.g., X 3 = X 1 + 2X 2 . Such a system of equations can perfectly be solved by using an arbitrary pair of variables {(X 1 , X 2 ), (X 1 , X 3 ), (X 2 , X 3 )}, as they are all perfectly correlated. Even though the predictive problem appears to be solved, the causative source of variability of Y is now uncertain, and the relative importance of the variables cannot be estimated from data. The situation gets critical when the number of variables exceeds the number of samples (p &gt; n): at least one of the variables can always be expressed as a linear combination of the others, thus yielding multiple perfect correlations <ref type="bibr" target="#b17">[18]</ref>. In case of multicollinearity, one of the possible solutions exploited by supervised ML algorithms (e.g., logistic regression <ref type="bibr" target="#b18">[19]</ref>) is to associate a weight to each variable, corresponding to its relevance. Instead of discarding variables which may be the true causative source of variability, these approaches make it possible to take into account all the observed variables at the same time, by increasing the number of model's parameters. Such increase in model complexity may in turn be a possible cause of overfitting, another phenomenon related to the curse of dimensionality. In fact, the increased flexibility makes the model not only able to fit the underlying relationship between variables, but also the random idiosyncrasies of the observations <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extrapolation and interpolation in machine learning</head><p>The objective of supervised ML can be roughly summarized as automatically obtaining a predictive model for a specific phenomenon starting from a set of known cases. Usually such known cases consist in different instances of measurements, or samples, each one specifying the values of all different variables, or features of the problem. One of these features is the target variable, that is, what should be predicted by the final model. ML algorithms try to find a mathematical relationship between the target feature and the others in the set of known data, or training set of data. Eventually, such relation can be encoded with a linear model <ref type="bibr" target="#b20">[21]</ref>, more complex structures <ref type="bibr" target="#b1">[2]</ref>, or even ensembles of simpler models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. Indeed, it is quite important to verify to what extent the model obtained is able to generalize, that is, to provide meaningful prediction for new instances of the problem. A model that is only able to deal with the data it was trained on is said to be overfitted <ref type="bibr" target="#b19">[20]</ref>, and it is generally considered useless regardless its performance on the train set. In ML the term capacity may describe the ability of a model to represent complex relationships -the term complexity, when referred to a model, can also be used with a similar meaning. While it sounds obvious that a second-degree polynomial has a higher capacity than a linear regression, and can better fit more instances of data, there are relatively few contributions in literature that attempt to provide a more formal framing <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, and these terms are often used in rather intuitive and qualitative statements. As a fast but gross approximation, ML practitioners often assess model capacity by evaluating the number of parameters that can be tuned inside a model. In theory, the best ML model for a task is the one with just enough capacity to properly represent the training data: Models with lower capacity would underfit, i.e., they deliver unsatisfying results as they are unable to cope with the complexity of the phenomenon; models with higher capacity would risk to overfit and consequently generalize poorly.</p><p>A simple depiction of overfitting and underfitting is provided in Figure <ref type="figure" target="#fig_3">4</ref>. In practice, unfortunately, it's extremely hard to estimate the capacity necessary to represent a data set; and the solution that many ML practitioners use isyet again -to apply several techniques with increasing capacity to the data set, until either the gain in fitting stops, or the improvements are not considered important enough to justify an increase in capacity. More interestingly, even estimating effective model capacity is not trivial, as there is evidence from works on deep learning that models with enough parameters to theoretically overfit the training data are actually able to generalize well in real-world case studies <ref type="bibr" target="#b25">[26]</ref>. The trade-off between fitting and capacity has been independently explored by different ML communities, with the definition of model-dependent metrics that attempt to take into account both fitting and capacity to assess overall quality, to facilitate model selection: A few examples include the Akaike Information Criterion <ref type="bibr" target="#b26">[27]</ref>, the Bayesian Information Criterion <ref type="bibr" target="#b27">[28]</ref>, and Pareto-based approaches used mainly in symbolic regression <ref type="bibr" target="#b28">[29]</ref>.</p><p>While evaluating model capacity can help reducing the chance of overfitting, measuring overfitting remains far from trivial. Ideal ML models should be able to obtain good predictions even for unknown samples of the same problems, but -by definition -the models cannot be tested on unknown data sets. Given this practical need, ML researchers found ways to at least assess overfitting, through different techniques. A basic, but extremely efficient technique, is cross-validation (with all its variations, such as leave-one-out cross-validation, stratified cross-validation and the like): The training data is split into k folds of equal size, a ML algorithm is iteratively trained on all folds minus one, and tested on the remaining fold <ref type="bibr" target="#b29">[30]</ref>. Analyzing the results of a k-fold cross-validation, for example the average performance, or single instances where the performance on a test fold differs greatly from the others, can provide further insight on the problem characteristics.</p><p>As the real objective of evaluating overfitting is to assess a model's capability of extrapolating to unknown instances of the same problem, it is worth it to spend a few words on the meaning of extrapolation in supervised ML. As for model capacity, there is an intuitive and imprecise concept of extrapolation, defined as the ability of the model to correctly predict data points that are considerably different from the information provided in the training data, but still belong to the same problem. An alternative outlook on extrapolation comes from computational geometry: Interpreting the data points in the training set as points in R f , where f is the number of features, it is possible to compute its convex hull, the smallest polygon that contains all training points. Given the convex hull of the training set, it is then possible The original data points can be properly represented by a polynomial of degree 4 (middle), so a polynomial regression with lower capacity will underfit (left), while a polynomial regression with higher capacity will overfit (right).</p><p>to assess whether an unseen test data point will fall inside or outside the convex hull. It is reasonable to assume that, for points inside the convex hull, a ML model will interpolate known data to obtain a prediction; while the same model will extrapolate for test points placed outside of the convex hull. An example is presented in Figure <ref type="figure" target="#fig_5">5</ref>. It is important to notice that, depending on the characteristics and the distribution of the training points, this interpretation of interpolation/extrapolation might not correspond to the actual difficulty of predictions for the model. For example, it is possible to imagine a situation where the model will provide better predictions for of a test point outside of the convex hull of the training data, but still relatively close to known points, than for a test point located inside the convex hull of the training data, but in a part of the space where training points are relatively sparse. Still, in most practical scenarios, it is generally harder for models to reliably predict values for test points outside of the convex hull of the training data.</p><p>A more in-depth discussion on the convex hull is provided in the following Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Assessing generalization tasks using the convex hull</head><p>In an Euclidean space, the convex hull of a set of points X = {x i ‚àà R d } is the smallest convex set containing all the points in X. If the number of points n in X is finite (i.e. if X is a matrix R n√ód ), then the convex hull forms a convex polytope in R d . Finding faces or the set of extreme points of such convex polytope is a NP-hard problem <ref type="bibr" target="#b30">[31]</ref>. However, checking if a point z lies inside or outside the polytope is much easier, and can be performed in polynomial time <ref type="bibr" target="#b31">[32]</ref>.</p><p>When dealing with ML models, evaluating the convex hull of a training set can provide extra information on unseen test points: If a test point falls inside the convex hull, it is expected that the ML model will probably interpolate known points to find the predicted value; vice versa, if a test point falls outside the convex hull of the training set, the ML model will likely be required to extrapolate to obtain a prediction.</p><p>The problem of checking whether a point z lies inside the polytope of X has a simple linear programming formulation <ref type="bibr" target="#b32">[33]</ref>:</p><formula xml:id="formula_1">min y c T y s.t. Ay = b c, y ‚àà R n .<label>(2)</label></formula><p>where:</p><formula xml:id="formula_2">c = 0 A = X T 1 T b = z 1<label>(3)</label></formula><p>0.0 0.2 0.4 0.6 0.8 1.0 Dimension #1 0.2 0.4 0.6 0.8 1.0 Dimension #2 Training samples Convex hull 0.0 0.2 0.4 0.6 0.8 1.0 Dimension #1 0.2 0.4 0.6 0.8 1.0 Dimension #2 Test sample inside convex hull Test sample outside convex hull Such formulation is known as a Phase I method <ref type="bibr" target="#b33">[34]</ref>, as the final goal is not the actual optimization of variable z, but rather checking whether a feasible solution does exist. In such contexts, the cost function can be a constant, as the only objective is satisfying the constraints. The first n constraints impose that the position of z in the feature space must be a combination of the points X:</p><formula xml:id="formula_3">z = n i=1 y i x i<label>(4)</label></formula><p>The last constraint imposes that such combination must be convex, which implies, by definition, that the coefficients y i must sum to 1:</p><formula xml:id="formula_4">n i=1 y i = 1<label>(5)</label></formula><p>If the Phase I problem is feasible, point x can be expressed as a convex combination of the set of points X. By definition, this means that point z lies inside the convex polytope of X. On the contrary, if the Phase I problem does not have any feasible solution, then point z lies outside the convex hull of X.</p><p>Even the proposed approach exploiting the convex hull can be affected by the curse of dimensionality. The critical point occurs when the dimension of the Euclidean space exceeds the number of observations (d &gt; n). In this case the upper bound of the rank of the matrix X corresponds to the number of samples n <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>:</p><formula xml:id="formula_5">rank(X) ‚â§ n<label>(6)</label></formula><p>Therefore, the maximal number of linearly independent columns of X cannot be higher than n. Whatever the number of dimensions d, the points x i will lie in a subspace R s where s ‚â§ rank(X) ‚â§ n &lt; d. As a consequence, the convex hull generated by the set of points x i will belong to the same subspace. By definition, for s &lt; d, the subspace R s has measure zero in R d and can be considered as negligible <ref type="bibr" target="#b36">[37]</ref>. Hence, when a new point z is added to the space, it is almost sure that it will fall outside negligible subspaces as R s <ref type="bibr" target="#b37">[38]</ref>. In summary, when d &gt; n, the new point z will almost never belong to the convex hull of X, making the computation of the convex hull ultimately useless.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Modeling generalization as a function of data set characteristics</head><p>The objective of this work consists in assessing generalization abilities of machine learning models with empirical experiments. To this aim, first we selected (i) a large set of publicly available data sets for classification, (ii) a representative set of machine learning classifiers, and (iii) a set of data set characteristics. Then, on each data set we perform a cross-validation using ML models and we compute relevant metrics for each fold. Finally, we analyze the relationships between data set characteristics and generalization ability of ML models using both classical correlation metrics, and association models derived through symbolic regression.</p><p>In the following subsections, the methodology adopted for selecting the data sets and the corresponding metrics to characterize them are introduced and discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data sets</head><p>Following the analyses presented in <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b14">[15]</ref>, this study focuses on classification, as it is easier to characterize classification rather than regression data sets, as several of the characteristics analyzed are based on comparing batches of samples belonging to different classes (see Statistical measures paragraph in subsection 5.2). Additionally, as the following analysis is partly based on evaluating convex hulls, only data sets with real-valued features are considered.</p><p>The data sets examined are acquired from the OpenML repository <ref type="bibr" target="#b38">[39]</ref>, on online, curated collection that, as the time of writing, includes over 3,100 data sets of different kinds. After selecting only data sets related to classification problems, with real-valued features exclusively, and discarding those containing errors, a total of 109 data sets was ultimately retained for the analysis. The number of samples in the selected collection ranges from 47 to 44, 819, while the number of features spans from 2 to 3, 758. All selected data sets have real-valued features and a discrete target (suited for classification). The mean feature correlation of the data sets is 0.619, with a standard error of the mean of 0.007, the average intrinsic dimensionality ratio is 0.629, with a standard error of the mean of 0.047.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data set characterization</head><p>As in previous meta-analyses <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, each data set is characterized using metrics, grouped into four categories: simple, statistical, Euclidean, and generalization metrics. For each data set, such measures are computed over a stratified 10-fold cross validation <ref type="bibr" target="#b29">[30]</ref>.</p><p>Simple metrics Simple metrics describe general characteristics of data sets, namely number of samples (n), number of features (d, a.k.a. dimensionality), and number of classes (c) <ref type="bibr" target="#b39">[40]</ref>.</p><p>Statistical metrics Statistical metrics assess (i) class differences in feature distributions and shapes, and (ii) relationships between features and classification target.</p><p>Levene's test <ref type="bibr" target="#b40">[41]</ref> is an inferential statistical test used to assess if a vector of random variables is homoscedastic, i.e. if the variance of the random variables is almost equal. In the following, Levene's test is used to compare class covariances for each data set feature. The lower the p-value, the higher will be the probability that the class covariances of the feature under study are different <ref type="bibr" target="#b41">[42]</ref>. In the following experiments, the score Œª collected for each data set is the average of the Levene's p-values of its features.</p><p>Pearson's correlation coefficient <ref type="bibr" target="#b42">[43]</ref> measures the linear relationship between two variables, providing an indication of the interdependence between pairs of features. The correlations between all pairs of attributes are calculated for each class separately. Since the objective is to evaluate the strength of the relationship and not its sign (positive or negative), the absolute value of the coefficient is used. For each data set, the collected score œÅ is the average of the coefficient over all pairs of features and over all classes.</p><p>Skewness <ref type="bibr" target="#b43">[44]</ref> corresponds to the third standardized moment of a random variable. It indicates the magnitude of the asymmetry of a feature around its mean, yielding an estimate of the feature's departure from normality. The skewness for a class is computed as a weighted average of the skewness of the feature values of its samples. The final skewness score Œ≥ represents the average skewness over all classes.</p><p>Kurtosis <ref type="bibr" target="#b44">[45]</ref>  Mutual information <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> measures the mutual dependence between two variables. In the following experiments, it is used to estimate the amount of information obtained about the classification target by observing a data set feature. The overall mutual information score Œ∑ is computed as the average over all features.</p><p>Euclidean metrics Euclidean metrics assess the shape of the data manifold.</p><p>The intrinsic dimensionality ratio provides a normalized estimate of the dimensionality of the data, considering a linear manifold. It is computed counting the number of principal components needed to explain 90% of the variance in the target <ref type="bibr" target="#b47">[48]</ref> (I). The final score I r is normalized over the number of original features .</p><p>Feature noise is an estimate of the amount of useless information. Following <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b48">[49]</ref>, the score is computed through the difference between dimensionality (the original number of features) and intrinsic dimensionality. The final score N is normalized over the original dimensionality.</p><p>The average sample distance ¬µ D and the standard deviation of sample distances œÉ D <ref type="bibr" target="#b49">[50]</ref> measure the average pairwise distance between two data set points, and the standard deviation of the resulting distribution, respectively.</p><p>Generalization metrics Generalization metrics estimate the nature and hardness of the classification task. Given the convex hull of a training data set, the ratio of test points inside it T in and outside of it T out assesses the type of generalization task ML models are asked to perform. Indeed, if test points often fall inside the convex hull of the training set, it is expected that the ML model will probably interpolate known points to find predicted values, most of the time; vice versa, if test points frequently fall outside the convex hull, the ML model will likely be required to recurrently extrapolate to obtain predictions. Class-imbalance may also play a role in impairing classifier performance. It has been computed for training and test samples, both inside and outiside the convex hull.</p><p>Classification performance estimates how difficult it is for a given classifier to learn from the training set and generalize to the test set. Since the analyzed data sets usually have more than two classes, the F 1 score <ref type="bibr" target="#b50">[51]</ref> is used to measure the classification performance. More specifically, the weighted F 1 score is adopted, to account for label imbalance. Two scores related to the test set are computed, assessing effectiveness in both interpolation (F 1 in ) and extrapolation (F 1 out ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental results and discussion</head><p>This section describes the experimental results obtained through the analysis of the selected data sets. First, classical linear correlations between the chosen metrics are considered. Then, more complex non-linear models are explored and discussed, outlining the importance of the convex hull. While different algorithms might perform differently on the same data set, testing all possible classifying alternatives is impractical. For this purpose, we selected a Logistic Regression (LR) <ref type="bibr" target="#b51">[52]</ref> classifier, a Support Vector Machines classifier with radial basis function kernel (SVC) <ref type="bibr" target="#b52">[53]</ref>, and a Random Forest classifier with 100 decision tree estimators (RF) <ref type="bibr" target="#b53">[54]</ref> as representative classifiers for the following experiments, taking into account their considerable efficiency and their heterogeneous capacity.</p><p>All the code and data necessary to reproduce the experiments is available in a public GitHub repository<ref type="foot" target="#foot_0">foot_0</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Correlations between data set characteristics: sometimes dimensionality not so cursed</head><p>Once all the considered metrics described in Sec. 5 have been computed for the 109 selected data sets, classical statistical correlations can be evaluated. In particular, computing Pearson's correlation coefficients results in the matrix presented in Fig. <ref type="figure" target="#fig_6">6</ref>. Analyzing the matrix, several predictable correlations can be found: In the following, we will analyze a few of the least immediately obvious.</p><p>Œ≥ Œ∫: the mean skewness and mean kurtosis of a dataset are highly correlated (0.84), as both metrics assess the difference in data distribution with respect to a reference Gaussian distribution.</p><p>I œÅ: very often, the higher the correlation between features, the lower the intrinsic dimensionality of a dataset, so as expected the two metrics are anti-correlated (-0.71).</p><p>d: dimensionality is positively correlated with I (0.85) and negatively correlated with œÅ (-0.70). In other words, as the number of features increase, intrinsic dimensionality tends to rise; and, at the same time, features are less likely to be strongly correlated with each other.</p><p>¬µ D œÅ: as the number of features/dimensions increases, the average distance between samples also increases, unless the additional features are strongly correlated with existing ones. For this reason, as expected, the average distance between samples is negatively correlated with the average correlation between features (-0.76). For the same reason, we find a 0.85 positive correlation between average sample distance and intrinsic dimensionality (I), and an equally strong positive correlation between ¬µ D and number of features d.</p><p>T in I r : the correlation between the ratio of test samples falling inside the convex hull of the training set and the intrinsic dimensionality ratio (0.81) is maybe the less intuitive of the relationships analyzed so far. When the intrinsic dimensionality I is lower than the number of dimensions d, training points used to compute the convex hull might actually all lie in a polytope of dimension d ‚âà I &lt; d; the likelihood of test points laying exactly inside this polytope becomes then low, especially when compared to a situation where I ‚âà d, and the convex hull of the training set occupies a much larger portion of the feature space. The very same concept is also expressed by the negative correlation between T in and N (-0.81), as a higher feature noise also represents a lower effective dimensionality. The same reasoning holds for the correlations T out -I r and T out -N.</p><p>Regarding classifier-specific metrics, such as F 1 for training and validation, we can observe how LR presents the highest correlation between the two, suggesting a similar behavior for training and unseen samples. SVC and RF, on the other hand, have poorer correlations, as they tend to overfit the training set more, as expected by classifiers with higher capacity. It is important to notice that the strength of this correlation does not imply a poor performance, as RF, with the lowest correlation, shows the highest F 1 on the test set (see Table <ref type="table" target="#tab_2">2</ref>).</p><p>Correlations that are unexpectedly weak in the analysis, those between F 1 test and all metrics related to dimensionality (d, I, I r ), hint at a surprising conclusion: The performance of the ML algorithms on an unseen test set is almost independent from the dimensionality of the data set. This is particularly true for LR (F 1 test I r = 0.1) and RF (F 1 test I r = 0.9), while corresponding correlations for SVC are higher, but still not very strong (F 1 test d = -0.</p><p>43, F 1 test I = -0.37, but F 1 test I r = 0.28). feature_avg_kurtosis feature_avg_skew feature_avg_correlation intrinsic_dimensionality intrinsic_dimensionality_ratio n_features sample_avg_distance in_hull_ratio val_f1 feature_avg_kurtosis feature_avg_skew feature_avg_correlation intrinsic_dimensionality intrinsic_dimensionality_ratio n_features sample_avg_distance in_hull_ratio val_f1 0.84 -0.10 -0.35 0.10 0.41 -0.71 -0.18 -0.33 0.45 -0.32 0.03 0.25 -0.70 0.85 -0.44 0.11 0.36 -0.76 0.85 -0.57 0.97 -0.17 -0.34 0.64 -0.33 0.81 -0.34 -0.48 0.14 0.04 0.33 -0.14 0.16 -0.23 -0.21 0.23 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 correlation For the complete correlation matrices between all considered characteristics, see Figs. 9, 10, 11, in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The key role of the convex hull and association models</head><p>In Table <ref type="table" target="#tab_2">2</ref> we reported F 1-scores of ML models on training and validation sets. A high difference between F 1 train and F 1 test corresponds to overfitting. We record RF exhibiting higher overfitting, while still providing the best generalization performance during validation. On the other hand, the difference between |F 1 train -F 1 in | and |F 1 train -F 1 out | reveals the discrepancy between interpolation and extrapolation performances, pinpointing the importance of the convex hull in assessing machine learning generalization.</p><p>While classical correlation metrics try to optimize the coefficients of models of known structure (e.g., often linear), it might be useful to extend such analysis to models of different structure. In statistical terms, this implies assessing association, a relationship between variables more general than correlation: Two or more variables are associated if the values of some provide information on the value of the others <ref type="bibr" target="#b54">[55]</ref>. To this purpose, we propose the use of symbolic , a technique that searches the space of mathematical expressions to find the model that best fits a given data set. The irony of using machine learning techniques to analyze the results of a meta-analysis on machine learning is not lost on the authors, but symbolic regression has the advantage of returning completely human-readable models, that can later be interpreted and explained, all the while considering relationships between data set characteristic more complex than just linear correlations. Furthermore, symbolic regression can deliver multiple candidate solutions, models of increasing complexity and fitting, whose meta-analysis can deliver additional information to the user. For this task, the commercial symbolic regression software Eureqa Formulize<ref type="foot" target="#foot_1">foot_1</ref> is employed. All available building blocks for equations were selected, with the exception of those specifically designed for time series analysis. Each run is stopped when the convergence metric of Eureqa crosses the threshold of 90%.</p><p>In order to assess generalization abilities of machine learning models, we analyze and compare the results provided by Eureqa in different scenarios. For each classifier, symbolic regression generates a set of Pareto-optimal models, predicting the performance of the classifier in terms of F 1-score. Pareto optimality is considered as a function of both R 2 (i.e. the accuracy of the formula) and complexity (the number of terms and complexity of formula's building blocks). Among all Pareto-optimal equations proposed by symbolic regression, we manually analyze a selection representing reasonable compromises between fitting and complexity. Eqs. 7-13 represent candidate equations of similar complexity (C = 9), describing nonlinear associations between data set characteristics and generalization metrics for each machine learning classifier.  0 20 40 60 80 100 complexity 0.2 0.4 0.6 0.8 1.0 error Inside convex hull model Logistic Regression Random forest SVC 0 5 10 15 20 25 30 complexity 0.5 0.6 0.7 0.8 0.9 1.0 error Outside convex hull model Logistic Regression Random forest SVC T in = 0.46 + 0.42</p><formula xml:id="formula_6">F 1 LR in = 0.857 + (0.959 ‚Ä¢ F 1 LR train -0.831) step(2.38 ‚Ä¢ I r -0.0019) [R 2 = 0.68, C = 9]<label>(7)</label></formula><formula xml:id="formula_7">F 1 LR out = 0.843 ‚Ä¢ F 1 LR train + 0.176 ‚Ä¢ Œª -0.040 -0.097 ‚Ä¢ N [R 2 = 0.47, C = 9]<label>(8)</label></formula><formula xml:id="formula_8">F 1 SV C in = 1.072 ‚Ä¢ F 1 SV C train + 0.021 ‚Ä¢ œÉ D -0.08 -0.003 ‚Ä¢ c [R 2 = 0.67, C = 9]<label>(9)</label></formula><formula xml:id="formula_9">F 1 SV C out = 0.820 + (1.605 ‚Ä¢ F 1 SV C train -1.389) cos(5.424 + 2.377 ‚Ä¢ I r ) [R 2 = 0.45, C = 9]<label>(10)</label></formula><formula xml:id="formula_10">F 1 RF in = 7.66 ‚Ä¢ œÅ -1.822 -5.266 ‚Ä¢ œÅ 2 [R 2 = 0.36, C = 9]<label>(11)</label></formula><formula xml:id="formula_11">‚Ä¢ erf(4.65 ‚Ä¢ œÅ -2.20 -2.98 ‚Ä¢ N) [R 2 = 0.88, C = 9]<label>(13)</label></formula><p>Overall, the interpolation ability (F 1 in ) of a ML algorithm on a data set can be predicted in a satisfying way using only the data set characteristics that we analyzed in this study. On the contrary, predicting extrapolation ability (F 1 out ) from data set characteristics seems much harder, as pointed out by the lower R 2 scores of models for corresponding complexity. Besides, the difference between predictors found for LR or SVC, with respect to RF, is noteworthy: In fact, the generalization ability of the models for the first two algorithms seems strongly associated with their training performance (F 1 train ) and either the intrinsic dimensionality ratio (I r ) or the feature noise (N = 1 -I r ). On the other hand, the test accuracy of RF seems much harder to predict, and associated with the average class-wise correlation among features (œÅ) only. Moreover, we observe how the ratio of test samples falling inside the convex hull could be easily estimated by considering the class-wise feature correlation (œÅ) and either the feature noise (N) or the intrinsic dimensionality ratio (I r = 1-N), confirming the reasoning derived from the previous analysis of the linear correlations: Higher feature noise leads to a lower effective dimensionality, thus reducing the likelihood of test samples falling inside the convex hull.</p><p>It is interesting to remark how F 1 train is the variable that explains most of the variance for LR and SVC models; while the same variable does not appear in models for RF, showing how the generalization ability of this classifier is poorly correlated with its performance on the training set, it is generally harder to predict (lower R 2 ), and seems to solely depend on the class-wise feature correlation œÅ. The model for T in also presents interesting insights, displaying rather high R 2 = 0.88 and depending on just two variables, œÅ and N. The negative influence of N can be explained intuitively: As the feature noise increases, the intrinsic dimensionality ratio reduces, and thus the points belonging to the convex hull of the training set lie more and more on a polytope of lower dimensionality than the entire feature space, making it more difficult for test points to fall inside its hypervolume. The positive influence of œÅ on T in is harder to explain: We speculate that a higher class-wise feature correlation would bring points belonging to the same class closer together in the feature space, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. Having the data points gathered in a smaller part of the feature space might imply that more test points will fall inside the convex hull of the training set, without necessarily reducing the true dimensionality of the feature space. This is somehow confirmed by the results reported in Fig. <ref type="figure" target="#fig_6">6</ref>, where intrinsic dimensionality ratio and average class-wise feature correlation are positively correlated, albeit weakly (I r œÅ = 0.45).</p><p>We further extend the analysis of symbolic regression results by comparing Pareto fronts of ML models both inside and outside the convex hull to further inspect whether data set characteristics have a significant impact on models' performances. If Pareto front A dominates Pareto front B it is likely that data set characteristics have a higher impact on ML performances in the first scenario rather than in the second one. The Pareto front analysis is presented in Figs. <ref type="figure" target="#fig_8">7</ref> and <ref type="figure" target="#fig_10">8</ref>. In Fig. <ref type="figure" target="#fig_8">7</ref>, we analyze the link between data set characteristics and generalization ability by comparing interpolation and extrapolation results of ML models. In all scenarios the relationship looks stronger when predictions are made inside the convex hull of training samples. However, while this difference is emphasized for LR and SVC, it is less pronounced for RF. In Fig. <ref type="figure" target="#fig_10">8</ref> we further inspect symbolic regression results by comparing Pareto fronts inside and outside the convex hull. Once more, we observe stronger associations between data set characteristics and LR or SVC compared to RF. This means that the impact of data set-specific properties on RF performances is lower, as if the higher capacity of the model makes it more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights of experimental findings</head><p>‚Ä¢ The convex hull has a key role in assessing ML algorithm generalization, in terms of interpolation and extrapolation abilities. ‚Ä¢ The structure and fitting of the association models found for F 1 in suggests that predicting the interpolation ability of a ML algorithm on a data set might be feasible, using only the data set characteristics that we analyzed in this study. On the contrary, after analyzing the association models obtained for F 1 out , predicting extrapolation abilities from data set characteristics seems much harder. ‚Ä¢ ML models with high capacity seem to generalize better (both inside and outside the convex hull), despite their higher potential for overfitting. The phenomenon is similar to what has already been observed in neural networks by Zhang et al. <ref type="bibr" target="#b25">[26]</ref>. ‚Ä¢ The performance of the ML algorithms on an unseen test set seems almost independent from the dimensionality of the data set, thus challenging the common assumption that the curse of dimensionality might impair generalization in machine learning.</p><p>After presenting our analysis of the correlations found on the 109 data sets we analyzed, there is a (rather ironic) question we have to face: how general are the results we found? Or, in other words, how well do the predictions we perform extrapolate to unknown data sets? Frankly speaking, we cannot claim that the correlations described in this work hold for all possible data sets, but the sheer number of different data sets analyzed gives us some hope of generality.</p><p>There is, however, a possible bias in the selection of data sets for this study: we focused on openly accessible, curated data sets, that already had to pass several quality checks in order to be hosted on repositories such as OpenML. This pre-selection process might make the data sets considered in this work not representative of all real-world data sets. In other words, usually a data set is uploaded on OpenML because the authors already know that at least one ML technique is going to work well for that specific data set; thus, what we analyzed might be representative only of data sets for which ML techniques work well.</p><p>Another possible explanation for the most counter-intuitive correlations we uncovered is that real-world data sets are a subset of all possible data sets. While some general mathematical conclusions, such as the curse of dimensionality, might hold for the set of all hypothetical data sets, they might not necessarily be true for the subset of data that is measured from real phenomena. This observation mirrors the remarks by Lin et al. <ref type="bibr" target="#b57">[58]</ref>: In an attempt to explain the effectiveness of neural networks and ML at representing physical phenomena, the authors notice that laws of physics can typically be approximated by a tiny subset of all possible polynomials, of order ranging from 2 to 4; this is a consequence of such phenomena usually being symmetrical when it comes to rotation and translation. As the data sets we analyzed come from either simulations or real-world experiments, their characteristics might lead ML algorithms to represent them more easily than expected. -0.10 1.00 -0.05 0.84 0.18 -0.06 0.05 -0.13 -0.13 -0.17 0.10 -0.18 0.10 0.64 0.01 -0.05 0.03 0.13 0.17 0.11 0.16 0.12 0.13 0.14 0.05 0.17 0.15 0.06 0.17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Full correlation matrices</head><p>0.40 -0.05 1.00 -0.11 0.30 -0.41 -0.03 -0.33 -0.32 -0.13 -0.29 -0.30 -0.07 -0.01 0.34 0.33 -0.28 -0.19 0.13 -0.26 -0.21 0.07 0.10 0.12 -0.04 0.19 0.14 0.00 0.18 -0.35 0.84 -0.11 1.00 0.33 -0.17 0.05 -0.22 -0.21 -0.34 0.41 -0.33 0.09 0.52 -0.00 -0.02 0.25 0.07 0.34 0.36 0.37 0.13 0.14 0.07 -0.09 0.13 0.09 -0.08 0.13 -0.45 0.18 0.30 0.33 1.00 -0.44 0.11 -0.47 -0.46 -0.81 0.32 -1.00 0.13 0.07 0.09 0.23 0.44 -0.12 0.81 0.57 0.57 0.15 0.19 -0.12 -0.14 0.02 -0.10 -0.10 -0.00 0.26 -0.06 -0.41 -0.17 -0.44 1.00 0.13 0.82 0.94 0.43 -0.20 0.44 -0.19 0.23 -0.07 -0.09 -0.23 0.33 -0.43 -0.28 -0.22 0.32 0.31 0.34 0.27 0.15 0.33 0.26 0.14 -0.01 0.05 -0.03 0.05 0.11 0.13 1.00 0.25 0.23 -0.03 0.06 -0.11 0.14 0.08 0.01 -0.03 0.08 0.07 0.03 0.10 0.19 -0.02 -0.02 0.01 -0.08 0.01 0.01 -0.09 0.01 0.32 -0.13 -0.33 -0.22 -0.47 0.82 0.25 1.00 0.93 0.54 -0.15 0.47 -0.15 0.07 -0.00 -0.12 -0.15 0.30 -0.54 -0.22 -0.19 0.08 0.06 0.21 0.20 0.08 0.20 0.19 0.08 0.33 -0.13 -0.32 -0.21 -0.46 0.94 0.23 0.93 1.00 0.53 -0.13 0.46 -0.16 0.10 -0.01 -0.11 -0.13 0.33 -0.53 -0.20 -0.17 0.14 0.12 0.26 0.26 0.11 0.25 0.25 0.11 0.64 -0.17 -0.13 -0.34 -0.81 0.43 -0.03 0.54 0.53 1.00 -0.33 0.81 -0.18 -0.02 0.03 -0.23 -0.34 0.18 -1.00 -0.48 -0.45 -0.07 -0.11 0.20 0.18 0.03 0.18 0.15 0.06 -0.71 0.10 -0.29 0.41 0.32 -0.20 0.06 -0.15 -0.13 -0.33 1.00 -0.32 0.23 -0.11 -0.26 0.08 0.85 -0.08 0.33 0.85 0.69 0.23 0.24 -0.02 -0.22 0.02 -0.00 -0.21 0.03 0.45 -0.18 -0.30 -0.33 -1.00 0.44 -0.11 0.47 0.46 0.81 -0.32 1.00 -0.13 -0.07 -0.09 -0.23 -0.44 0.12 -0.81 -0.57 -0.57 -0.15 -0.19 0.12 0.14 -0.02 0.10 0.10 0.00 -0.26 0.10 -0.07 0.09 0.13 -0.19 0.14 -0.15 -0.16 -0.18 0.23 -0.13 1.00 -0.13 -0.65 -0.16 0.21 -0.30 0.18 0.19 0.20 -0.03 -0.00 -0.17 -0.08 -0.12 -0.16 -0.05 -0.13 0.04 0.64 -0.01 0.52 0.07 0.23 0.08 0.07 0.10 -0.02 -0.11 -0.07 -0.13 1.00 0.23 -0.11 -0.10 0.57 0.02 -0.03 0.04 0.11 0.11 0.24 0.11 0.22 0.25 0.12 0.23 0.29 0.01 0.34 -0.00 0.09 -0.07 0.01 -0.00 -0.01 0.03 -0.26 -0.09 -0.65 0.23 1.00 0.18 -0.21 0.28 -0.03 -0.16 -0.10 -0.01 -0.02 0.19 -0.05 0.21 0.19 -0.06 0.22 0.01 -0.05 0.33 -0.02 0.23 -0.09 -0.03 -0.12 -0.11 -0.23 0.08 -0.23 -0.16 -0.11 0.18 1.00 0.07 -0.05 0.23 0.11 0.12 0.06 0.07 -0.05 -0.38 -0.01 -0.04 -0.35 -0.02 -0.70 0.03 -0.28 0.25 0.44 -0.23 0.08 -0.15 -0.13 -0.34 0.85 -0.44 0.21 -0.10 -0.21 0.07 1.00 0.02 0.34 0.97 0.82 0.05 0.06 -0.17 -0.34 -0.11 -0.14 -0.32 -0.10 0.01 0.13 -0.19 0.07 -0.12 0.33 0.07 0.30 0.33 0.18 -0.08 0.12 -0.30 0.57 0.28 -0.05 0.02 1.00 -0.18 0.02 0.02 -0.16 -0.17 0.01 -0.02 -0.05 0.01 -0.03 -0.04 -0.64 0.17 0.13 0.34 0.81 -0.43 0.03 -0.54 -0.53 -1.00 0.33 -0.81 0.18 0.02 -0.03 0.23 0.34 -0.18 1.00 0.48 0.45 0.07 0.11 -0.20 -0.18 -0.03 -0.18 -0.15 -0.06 -0.76 0.11 -0.26 0.36 0.57 -0.28 0.10 -0.22 -0.20 -0.48 0.85 -0.57 0.19 -0.03 -0.16 0.11 0.97 0.02 0.48 1.00 0.90 0.10 0.12 -0.15 -0.33 -0.07 -0.12 -0.31 -0.06 -0.61 0.16 -0.21 0.37 0.57 -0.22 0.19 -0.19 -0.17 -0.45 0.69 -0.57 0.20 0.04 -0.10 0.12 0.82 0.02 0.45 0.90 1.00 0.07 0.08 -0.15 -0.28 -0.08 -0.13 -0.27 -0.08 0.03 0.12 0.07 0.13 0.15 0.32 -0.02 0.08 0.14 -0.07 0.23 -0.15 -0.03 0.11 -0.01 0.06 0.05 -0.16 0.07 0.10 0.07 1.00 1.00 0.80 0.80 0.68 0.80 0.81 0.66 0.01 0.13 0.10 0.14 0.19 0.31 -0.02 0.06 0.12 -0.11 0.24 -0.19 -0.00 0.11 -0.02 0.07 0.06 -0.17 0.11 0.12 0.08 1.00 1.00 0.79 0.79 0.67 0.79 0.81 0.66 0.30 0.14 0.12 0.07 -0.12 0.34 0.01 0.21 0.26 0.20 -0.02 0.12 -0.17 0.24 0.19 -0.05 -0.17 0.01 -0.20 -0.15 -0.15 0.80 0.79 1.00 0.84 0.83 0.99 0.84 0.83 0.37 0.05 -0.04 -0.09 -0.14 0.27 -0.08 0.20 0.26 0.18 -0.22 0.14 -0.08 0.11 -0.05 -0.38 -0.34 -0.02 -0.18 -0.33 -0.28 0.80 0.79 0.84 1.00 0.52 0.84 0.99 0.51 0.19 0.17 0.19 0.13 0.02 0.15 0.01 0.08 0.11 0.03 0.02 -0.02 -0.12 0.22 0.21 -0.01 -0.11 -0.05 -0.03 -0.07 -0.08 0.68 0.67 0.83 0.52 1.00 0.83 0.53 0.99 0.27 0.15 0.14 0.09 -0.10 0.33 0.01 0.20 0.25 0.18 -0.00 0.10 -0.16 0.25 0.19 -0.04 -0.14 0.01 -0.18 -0.12 -0.13 0.80 0.79 0.99 0.84 0.83 1.00 0.85 0.84 0.36 0.06 0.00 -0.08 -0.10 0.26 -0.09 0.19 0.25 0.15 -0.21 0.10 -0.05 0.12 -0.06 -0.35 -0.32 -0.03 -0.15 -0.31 -0.27 0.81 0.81 0.84 0.99 0.53 0.85 1.00 0.52 0.19 0.17 0.18 0.13 -0.00 0.14 0.01 0.08 0.11 0.06 0.03 0.00 -0.13 0.23 0.22 -0.02 -0.10 -0.04 -0.06 -0.06 -0.08 0.66 0.66 0.83 0.51 0.99 0.84 0.52 1.00 LR Figure 9: Correlations between dataset characteristics using logistic regression. feature_avg_correlation feature_avg_kurtosis feature_avg_mutual_information feature_avg_skew feature_noise imbalance_ratio_in_hull imbalance_ratio_out_hull imbalance_ratio_train imbalance_ratio_val in_hull_ratio intrinsic_dimensionality intrinsic_dimensionality_ratio levene_pvalue levene_stat levene_success n_classes n_features n_samples out_hull_ratio sample_avg_distance sample_std_distance train_accuracy train_f1 val_accuracy val_accuracy_in_hull val_accuracy_out_hull val_f1 val_f1_in_hull val_f1_out_hull feature_avg_correlation feature_avg_kurtosis feature_avg_mutual_information feature_avg_skew feature_noise imbalance_ratio_in_hull imbalance_ratio_out_hull imbalance_ratio_train imbalance_ratio_val in_hull_ratio intrinsic_dimensionality intrinsic_dimensionality_ratio levene_pvalue levene_stat levene_success n_classes n_features n_samples out_hull_ratio sample_avg_distance sample_std_distance train_accuracy train_f1 val_accuracy val_accuracy_in_hull val_accuracy_out_hull val_f1 val_f1_in_hull val_f1_out_hull 1.00 -0.10 0.40 -0.35 -0.45 0.26 -0.01 0.32 0.33 0.64 -0.71 0.45 -0.26 0.04 0.29 0.01 -0.70 0.01 -0.64 -0.76 -0.61 0.05 0.01 0.50 0.31 0.41 0.47 0.29 0.40 -0.10 1.00 -0.05 0.84 0.18 -0.06 0.05 -0.13 -0.13 -0.17 0.10 -0.18 0.10 0.64 0.01 -0.05 0.03 0.13 0.17 0.11 0.16 0.13 0.14 0.12 0.03 0.14 0.13 0.03 0.15 0.40 -0.05 1.00 -0.11 0.30 -0.41 -0.03 -0.33 -0.32 -0.13 -0.29 -0.30 -0.07 -0.01 0.34 0.33 -0.28 -0.19 0.13 -0.26 -0.21 0.04 0.07 0.17 -0.03 0.22 0.19 0.02 0.23 -0.35 0.84 -0.11 1.00 0.33 -0.17 0.05 -0.22 -0.21 -0.34 0.41 -0.33 0.09 0.52 -0.00 -0.02 0.25 0.07 0.34 0.36 0.37 0.08 0.10 -0.06 -0.12 -0.00 -0.04 -0.12 0.01 -0.45 0.18 0.30 0.33 1.00 -0.44 0.11 -0.47 -0.46 -0.81 0.32 -1.00 0.13 0.07 0.09 0.23 0.44 -0.12 0.81 0.57 0.57 -0.09 -0.06 -0.29 -0.17 -0.16 -0.28 -0.13 -0.17 0.26 -0.06 -0.41 -0.17 -0.44 1.00 0.13 0.82 0.94 0.43 -0.20 0.44 -0.19 0.23 -0.07 -0.09 -0.23 0.33 -0.43 -0.28 -0.22 0.29 0.27 0.31 0.23 0.11 0.30 0.22 0.09 -0.01 0.05 -0.03 0.05 0.11 0.13 1.00 0.25 0.23 -0.03 0.06 -0.11 0.14 0.08 0.01 -0.03 0.08 0.07 0.03 0.10 0.19 -0.08 -0.07 -0.05 -0.09 -0.04 -0.04 -0.09 -0.05 0.32 -0.13 -0.33 -0.22 -0.47 0.82 0.25 1.00 0.93 0.54 -0.15 0.47 -0.15 0.07 -0.00 -0.12 -0.15 0.30 -0.54 -0.22 -0.19 0.11 0.09 0.22 0.15 0.10 0.21 0.14 0.09 0.33 -0.13 -0.32 -0.21 -0.46 0.94 0.23 0.93 1.00 0.53 -0.13 0.46 -0.16 0.10 -0.01 -0.11 -0.13 0.33 -0.53 -0.20 -0.17 0.18 0.15 0.26 0.22 0.13 0.25 0.21 0.12 0.64 -0.17 -0.13 -0.34 -0.81 0.43 -0.03 0.54 0.53 1.00 -0.33 0.81 -0.18 -0.02 0.03 -0.23 -0.34 0.18 -1.00 -0.48 -0.45 0.09 0.06 0.33 0.19 0.20 0.32 0.16 0.21 -0.71 0.10 -0.29 0.41 0.32 -0.20 0.06 -0.15 -0.13 -0.33 1.00 -0.32 0.23 -0.11 -0.26 0.08 0.85 -0.08 0.33 0.85 0.69 0.13 0.15 -0.39 -0.20 -0.33 -0.37 -0.20 -0.32 0.45 -0.18 -0.30 -0.33 -1.00 0.44 -0.11 0.47 0.46 0.81 -0.32 1.00 -0.13 -0.07 -0.09 -0.23 -0.44 0.12 -0.81 -0.57 -0.57 0.09 0.06 0.29 0.17 0.16 0.28 0.13 0.17 -0.26 0.10 -0.07 0.09 0.13 -0.19 0.14 -0.15 -0.16 -0.18 0.23 -0.13 1.00 -0.13 -0.65 -0.16 0.21 -0.30 0.18 0.19 0.20 -0.10 -0.08 -0.27 -0.06 -0.21 -0.28 -0.03 -0.24 0.04 0.64 -0.01 0.52 0.07 0.23 0.08 0.07 0.10 -0.02 -0.11 -0.07 -0.13 1.00 0.23 -0.11 -0.10 0.57 0.02 -0.03 0.04 0.22 0.23 0.31 0.15 0.28 0.32 0.16 0.29 0.29 0.01 0.34 -0.00 0.09 -0.07 0.01 -0.00 -0.01 0.03 -0.26 -0.09 -0.65 0.23 1.00 0.18 -0.21 0.28 -0.03 -0.16 -0.10 0.06 0.07 0.33 0.02 0.33 0.35 0.02 0.36 0.01 -0.05 0.33 -0.02 0.23 -0.09 -0.03 -0.12 -0.11 -0.23 0.08 -0.23 -0.16 -0.11 0.18 1.00 0.07 -0.05 0.23 0.11 0.12 -0.09 -0.07 -0.14 -0.41 -0.10 -0.13 -0.38 -0.09 -0.70 0.03 -0.28 0.25 0.44 -0.23 0.08 -0.15 -0.13 -0.34 0.85 -0.44 0.21 -0.10 -0.21 0.07 1.00 0.02 0.34 0.97 0.82 0.01 0.05 -0.46 -0.37 -0.39 -0.43 -0.35 -0.37 0.01 0.13 -0.19 0.07 -0.12 0.33 0.07 0.30 0.33 0.18 -0.08 0.12 -0.30 0.57 0.28 -0.05 0.02 1.00 -0.18 0.02 0.02 0.05 0.06 0.17 0.06 0.11 0.20 0.05 0.13 -0.64 0.17 0.13 0.34 0.81 -0.43 0.03 -0.54 -0.53 -1.00 0.33 -0.81 0.18 0.02 -0.03 0.23 0.34 -0.18 1.00 0.48 0.45 -0.09 -0.06 -0.33 -0.19 -0.20 -0.32 -0.16 -0.21 -0.76 0.11 -0.26 0.36 0.57 -0.28 0.10 -0.22 -0.20 -0.48 0.85 -0.57 0.19 -0.03 -0.16 0.11 0.97 0.02 0.48 1.00 0.90 0.01 0.05 -0.43 -0.35 -0.36 -0.41 -0.33 -0.34 -0.61 0.16 -0.21 0.37 0.57 -0.22 0.19 -0.19 -0.17 -0.45 0.69 -0.57 0.20 0.04 -0.10 0.12 0.82 0.02 0.45 0.90 1.00 -0.10 -0.07 -0.38 -0.32 -0.31 -0.37 -0.30 -0.31 0.05 0.13 0.04 0.08 -0.09 0.29 -0.08 0.11 0.18 0.09 0.13 0.09 -0.10 0.22 0.06 -0.09 0.01 0.05 -0.09 0.01 -0.10 1.00 0.99 0.72 0.78 0.60 0.73 0.79 0.61 0.01 0.14 0.07 0.10 -0.06 0.27 -0.07 0.09 0.15 0.06 0.15 0.06 -0.08 0.23 0.07 -0.07 0.05 0.06 -0.06 0.05 -0.07 0.99 1.00 0.69 0.77 0.58 0.71 0.79 0.60 0.50 0.12 0.17 -0.06 -0.29 0.31 -0.05 0.22 0.26 0.33 -0.39 0.29 -0.27 0.31 0.33 -0.14 -0.46 0.17 -0.33 -0.43 -0.38 0.72 0.69 1.00 0.84 0.86 0.99 0.84 0.86 0.31 0.03 -0.03 -0.12 -0.17 0.23 -0.09 0.15 0.22 0.19 -0.20 0.17 -0.06 0.15 0.02 -0.41 -0.37 0.06 -0.19 -0.35 -0.32 0.78 0.77 0.84 1.00 0.52 0.83 0.99 0.51 0.41 0.14 0.22 -0.00 -0.16 0.11 -0.04 0.10 0.13 0.20 -0.33 0.16 -0.21 0.28 0.33 -0.10 -0.39 0.11 -0.20 -0.36 -0.31 0.60 0.58 0.86 0.52 1.00 0.86 0.52 0.99 0.47 0.13 0.19 -0.04 -0.28 0.30 -0.04 0.21 0.25 0.32 -0.37 0.28 -0.28 0.32 0.35 -0.13 -0.43 0.20 -0.32 -0.41 -0.37 0.73 0.71 0.99 0.83 0.86 1.00 0.84 0.87 0.29 0.03 0.02 -0.12 -0.13 0.22 -0.09 0.14 0.21 0.16 -0.20 0.13 -0.03 0.16 0.02 -0.38 -0.35 0.05 -0.16 -0.33 -0.30 0.79 0.79 0.84 0.99 0.52 0.84 1.00 0.51 0.40 0.15 0.23 0.01 -0.17 0.09 -0.05 0.09 0.12 0.21 -0.32 0.17 -0.24 0.29 0.36 -0.09 -0.37 0.13 -0.21 -0.34 -0.31 0.61 0.60 0.86 0.51 0.99 0.87 0.51 1.00 SVC Figure 10: Correlations between dataset characteristics using SVC. feature_avg_correlation feature_avg_kurtosis feature_avg_mutual_information feature_avg_skew feature_noise imbalance_ratio_in_hull imbalance_ratio_out_hull imbalance_ratio_train imbalance_ratio_val in_hull_ratio intrinsic_dimensionality intrinsic_dimensionality_ratio levene_pvalue levene_stat levene_success n_classes n_features n_samples out_hull_ratio sample_avg_distance sample_std_distance train_accuracy train_f1 val_accuracy val_accuracy_in_hull val_accuracy_out_hull val_f1 val_f1_in_hull val_f1_out_hull feature_avg_correlation feature_avg_kurtosis feature_avg_mutual_information feature_avg_skew feature_noise imbalance_ratio_in_hull imbalance_ratio_out_hull imbalance_ratio_train imbalance_ratio_val in_hull_ratio intrinsic_dimensionality intrinsic_dimensionality_ratio levene_pvalue levene_stat levene_success n_classes n_features n_samples out_hull_ratio sample_avg_distance sample_std_distance train_accuracy train_f1 val_accuracy val_accuracy_in_hull val_accuracy_out_hull val_f1 val_f1_in_hull val_f1_out_hull 1.00 -0.10 0.40 -0.35 -0.45 0.26 -0.01 0.32 0.33 0.64 -0.71 0.45 -0.26 0.04 0.29 0.01 -0.70 0.01 -0.64 -0.76 -0.61 -0.18 -0.18 0.25 0.20 0.16 0.24 0.19 0.15 -0.10 1.00 -0.05 0.84 0.18 -0.06 0.05 -0.13 -0.13 -0.17 0.10 -0.18 0.10 0.64 0.01 -0.05 0.03 0.13 0.17 0.11 0.16 0.05 0.05 0.14 0.05 0.16 0.15 0.06 0.17 0.40 -0.05 1.00 -0.11 0.30 -0.41 -0.03 -0.33 -0.32 -0.13 -0.29 -0.30 -0.07 -0.01 0.34 0.33 -0.28 -0.19 0.13 -0.26 -0.21 0.05 0.05 0.16 0.09 0.19 0.17 0.12 0.19 -0.35 0.84 -0.11 1.00 0.33 -0.17 0.05 -0.22 -0.21 -0.34 0.41 -0.33 0.09 0.52 -0.00 -0.02 0.25 0.07 0.34 0.36 0.37 0.04 0.04 0.06 -0.11 0.11 0.07 -0.10 0.13 -0.45 0.18 0.30 0.33 1.00 -0.44 0.11 -0.47 -0.46 -0.81 0.32 -1.00 0.13 0.07 0.09 0.23 0.44 -0.12 0.81 0.57 0.57 0.17 0.17 -0.09 -0.02 0.04 -0.09 0.01 0.04 0.26 -0.06 -0.41 -0.17 -0.44 1.00 0.13 0.82 0.94 0.43 -0.20 0.44 -0.19 0.23 -0.07 -0.09 -0.23 0.33 -0.43 -0.28 -0.22 0.13 0.13 0.28 0.20 0.06 0.27 0.19 0.04 -0.01 0.05 -0.03 0.05 0.11 0.13 1.00 0.25 0.23 -0.03 0.06 -0.11 0.14 0.08 0.01 -0.03 0.08 0.07 0.03 0.10 0.19 0.04 0.04 0.02 -0.00 -0.00 0.02 -0.00 -0.00 0.32 -0.13 -0.33 -0.22 -0.47 0.82 0.25 1.00 0.93 0.54 -0.15 0.47 -0.15 0.07 -0.00 -0.12 -0.15 0.30 -0.54 -0.22 -0.19 0.08 0.08 0.20 0.16 0.01 0.18 0.15 -0.01 0.33 -0.13 -0.32 -0.21 -0.46 0.94 0.23 0.93 1.00 0.53 -0.13 0.46 -0.16 0.10 -0.01 -0.11 -0.13 0.33 -0.53 -0.20 -0.17 0.07 0.07 0.23 0.20 0.04 0.22 0.19 0.03 0.64 -0.17 -0.13 -0.34 -0.81 0.43 -0.03 0.54 0.53 1.00 -0.33 0.81 -0.18 -0.02 0.03 -0.23 -0.34 0.18 -1.00 -0.48 -0.45 -0.22 -0.22 0.21 0.06 0.03 0.20 0.04 0.03 -0.71 0.10 -0.29 0.41 0.32 -0.20 0.06 -0.15 -0.13 -0.33 1.00 -0.32 0.23 -0.11 -0.26 0.08 0.85 -0.08 0.33 0.85 0.69 0.06 0.06 -0.02 -0.15 0.03 -0.01 -0.14 0.04 0.45 -0.18 -0.30 -0.33 -1.00 0.44 -0.11 0.47 0.46 0.81 -0.32 1</p><p>.00 -0.13 -0.07 -0.09 -0.23 -0.44 0.12 -0.81 -0.57 -0.57 -0.17 -0.17 0.09 0.02 -0.04 0.09 -0.01 -0.04 -0.26 0.10 -0.07 0.09 0.13 -0.19 0.14 -0.15 -0.16 -0.18 0.23 -0.13 1.00 -0.13 -0.65 -0.16 0.21 -0.30 0.18 0.19 0.20 -0.03 -0.03 -0.20 -0.06 -0.15 -0.22 -0.03 -0.17 -0.61 0.16 -0.21 0.37 0.57 -0.22 0.19 -0.19 -0.17 -0.45 0.69 -0.57 0.20 0.04 -0.10 0.12 0.82 0.02 0.45 0.90 1.00 0.08 0.08 -0.10 -0.13 -0.02 -0.10 -0.13 -0.02 -0.18 0.05 0.05 0.04 0.17 0.13 0.04 0.08 0.07 -0.22 0.06 -0.17 -0.03 0.07 0.05 0.03 0.06 0.09 0.22 0.09 0.08 1.00 1.00 0.26 0.39 0.19 0.24 0.37 0.17 -0.18 0.05 0.05 0.04 0.17 0.13 0.04 0.08 0.07 -0.22 0.06 -0.17 -0.03 0.07 0.05 0.03 0.06 0.09 0.22 0.09 0.08 1.00 1.00 0.26 0.39 0.19 0.24 0.37 0.17 0.25 0.14 0.16 0.06 -0.09 0.28 0.02 0.20 0.23 0.21 -0.02 0.09 -0.20 0.27 0.28 -0.08 -0.12 0.08 -0.21 -0.10 -0.10 0.26 0.26 1.00 0.79 0.80 0.99 0.78 0.79 0.20 0.05 0.09 -0.11 -0.02 0.20 -0.00 0.16 0.20 0.06 -0.15 0.02 -0.06 0.13 0.04 -0.15 -0.16 0.02 -0.06 -0.15 -0.13 0.39 0.39 0.79 1.00 0.44 0.79 0.99 0.42 0.16 0.16 0.19 0.11 0.04 0.06 -0.00 0.01 0.04 0.03 0.03 -0.04 -0.15 0.20 0.29 -0.04 -0.05 -0.05 -0.03 -0.02 -0.02 0.19 0.19 0.80 0.44 1.00 0.79 0.43 0.99 0.24 0.15 0.17 0.07 -0.09 0.27 0.02 0.18 0.22 0.20 -0.01 0.09 -0.22 0.29 0.30 -0.08 -0.10 0.10 -0.20 -0.09 -0.10 0.24 0.24 0.99 0.79 0.79 1.00 0.79 0.79 0.19 0.06 0.12 -0.10 0.01 0.19 -0.00 0.15 0.19 0.04 -0.14 -0.01 -0.03 0.14 0.03 -0.14 -0.15 0.02 -0.04 -0.14 -0.13 0.37 0.37 0.78 0.99 0.43 0.79 1.00 0.41 0.15 0.17 0.19 0.13 0.04 0.04 -0.00 -0.01 0.03 0.03 0.04 -0.04 -0.17 0.22 0.32 -0.04 -0.04 -0.02 -0.03 -0.01 -0.02 0.17 0.17 0.79 0.42 0.99 0.79 0.41 1.00 RF <ref type="bibr">Figure 11</ref>: Correlations between dataset characteristics using random forest.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visual representation of the data sparsity phenomenon in case of correlated random variables (left) and collinear random variables (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Two-class samples drawn from two correlated Gaussian random variables. Notably the classification problem is trivial as the two classes are linearly separable (top-left). By increasing the number of correlated random variables the highest value (in green), the lowest value (in orange), and the variance of samples' correlation shrink towards zero (top-right). The number of highly correlated variables (œÅ &gt; 0.8) increases polynomially with the overall number of features (bottom-left). On the other hand, the generalization accuracy of logistic regression in predicting class labels decreases, down to the point of providing almost random estimates (bottom-right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Mutual relationships among generalization ability (cross-validation accuracy), sample correlation, and number of features when all variables are correlated and gaussian. When the number of features is higher than 20-25, samples' correlation drops leading to a dramatic decrease in generalization ability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Visual representation of the effect of model capacity on fitting. The original data points can be properly represented by a polynomial of degree 4 (middle), so a polynomial regression with lower capacity will underfit (left), while a polynomial regression with higher capacity will overfit (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example of convex hull. The convex hull of a set of training samples is the minimal hyper-polygon (in this case, a 2D polygon) that contains all the data inside its edges (left). For a machine learning model trained on the initial training set, predicting a value for an unseen test point inside the convex hull (in green) probably requires interpolation; but predicting for a test point outside the convex hull requires extrapolation (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Heatmap showing a selection of the most relevant correlations between data set characteristics. The F 1 score computed on test samples (val_f1) corresponds to the average score over the three ML models used for the experiments, i.e. LR, SVC, and RF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>F 1</head><label>1</label><figDesc>RF out = 0.818 + (1.237 ‚Ä¢ œÅ -0.767) erfc(4.65 ‚Ä¢ œÅ -2.883) [R 2 = 0.28, C = 9]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Pareto-optimal models predicting F 1 in and F 1 out based on data set characteristics taking into account the results on all ML models (top-left). Pareto fronts for each ML model: logistic regression (top-right), SVC (bottom-left), and random forest (bottom-right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparison of Pareto fronts predicting F 1 in and F 1 out based on data set characteristics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>-0.10 0.40 -0.35 -0.45 0.26 -0.01 0.32 0.33 0.64 -0.71 0.45 -0.26 0.04 0.29 0.01 -0.70 0.01 -0.64 -0.76 -0.61 0.03 0.01 0.30 0.37 0.19 0.27 0.36 0.19</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>0</head><figDesc>.04 0.64 -0.01 0.52 0.07 0.23 0.08 0.07 0.10 -0.02 -0.11 -0.07 -0.13 1.00 0.23 -0.11 -0.10 0.57 0.02 -0.03 0.04 0.07 0.07 0.27 0.13 0.20 0.29 0.14 0.22 0.29 0.01 0.34 -0.00 0.09 -0.07 0.01 -0.00 -0.01 0.03 -0.26 -0.09 -0.65 0.23 1.00 0.18 -0.21 0.28 -0.03 -0.16 -0.10 0.05 0.05 0.28 0.04 0.29 0.30 0.03 0.32 0.01 -0.05 0.33 -0.02 0.23 -0.09 -0.03 -0.12 -0.11 -0.23 0.08 -0.23 -0.16 -0.11 0.18 1.00 0.07 -0.05 0.23 0.11 0.12 0.03 0.03 -0.08 -0.15 -0.04 -0.08 -0.14 -0.04 -0.70 0.03 -0.28 0.25 0.44 -0.23 0.08 -0.15 -0.13 -0.34 0.85 -0.44 0.21 -0.10 -0.21 0.07 1.00 0.02 0.34 0.97 0.82 0.06 0.06 -0.12 -0.16 -0.05 -0.10 -0.15 -0.04 0.01 0.13 -0.19 0.07 -0.12 0.33 0.07 0.30 0.33 0.18 -0.08 0.12 -0.30 0.57 0.28 -0.05 0.02 1.00 -0.18 0.02 0.02 0.09 0.09 0.08 0.02 -0.05 0.10 0.02 -0.02 -0.64 0.17 0.13 0.34 0.81 -0.43 0.03 -0.54 -0.53 -1.00 0.33 -0.81 0.18 0.02 -0.03 0.23 0.34 -0.18 1.00 0.48 0.45 0.22 0.22 -0.21 -0.06 -0.03 -0.20 -0.04 -0.03 -0.76 0.11 -0.26 0.36 0.57 -0.28 0.10 -0.22 -0.20 -0.48 0.85 -0.57 0.19 -0.03 -0.16 0.11 0.97 0.02 0.48 1.00 0.90 0.09 0.09 -0.10 -0.15 -0.02 -0.09 -0.14 -0.01</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the metrics used to characterize the data sets analyzed in the study.</figDesc><table><row><cell>Metric type</cell><cell cols="2">Symbol Description</cell></row><row><cell></cell><cell>n</cell><cell>Number of samples</cell></row><row><cell>Standard metrics</cell><cell>d</cell><cell>Number of features</cell></row><row><cell></cell><cell>c</cell><cell>Number of classes</cell></row><row><cell></cell><cell>I</cell><cell>Intrinsic dimensionality</cell></row><row><cell></cell><cell>I r</cell><cell>Intrinsic dimensionality ratio</cell></row><row><cell>Euclidean metrics</cell><cell>N</cell><cell>Feature noise (1 -I r )</cell></row><row><cell></cell><cell>¬µ D</cell><cell>Average sample distance</cell></row><row><cell></cell><cell>œÉ D</cell><cell>Standard deviation of sample distance</cell></row><row><cell></cell><cell>Œª</cell><cell>Average of Levene's test p-values</cell></row><row><cell></cell><cell>œÅ</cell><cell>Average of class-wise feature correlation</cell></row><row><cell>Statistical metrics</cell><cell>Œ≥</cell><cell>Average of class-wise feature skewness</cell></row><row><cell></cell><cell>Œ∫</cell><cell>Average of class-wise feature kurtosis</cell></row><row><cell></cell><cell>Œ∑</cell><cell>Average of feature-target mutual information</cell></row><row><cell></cell><cell cols="2">CI train Class-imbalance of training samples</cell></row><row><cell></cell><cell>CI test</cell><cell>Class-imbalance of test samples</cell></row><row><cell></cell><cell>T</cell><cell></cell></row><row><cell>Generalization metrics</cell><cell></cell><cell></cell></row></table><note><p>corresponds to the fourth standardized moment of a random variable. It indicates the "thickness" of the tails of a density function. Distributions with kurtosis less than 3 are called platykurtic, i.e. they produce fewer and less extreme outliers than the normal distribution. Inversely, distributions with kurtosis higher than 3 are called leptokurtic and produce more outliers with respect to the normal distribution. The kurtosis for a class is computed as a weighted average of the kurtosis of the feature values of its samples. The final kurtosis score Œ∫ represents the average kurtosis over all classes. in Ratio of test samples inside the convex hull T out Ratio of test samples outside the convex hull (1 -T in ) CI in Class-imbalance of test samples inside the convex hull CI out Class-imbalance of test samples outside the convex hull F 1 train F 1 for the training set F 1 test F 1 for the whole test set F 1 in F 1 for the part of the test set inside the convex hull (interpolation ability) F 1 out F 1 for the part of test set outside the convex hull (extrapolation ability)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average F 1-score and standard error of the mean of ML models.</figDesc><table><row><cell cols="3">ML model Sample set F 1-score</cell><cell>‚àÜ</cell></row><row><cell></cell><cell>F 1 train</cell><cell>0.87 ¬± 0.01</cell></row><row><cell>LR</cell><cell>F 1 test</cell><cell cols="2">0.79 ¬± 0.01 0.08 ¬± 0.03</cell></row><row><cell></cell><cell>F 1 in</cell><cell cols="2">0.82 ¬± 0.01 0.05 ¬± 0.03</cell></row><row><cell></cell><cell>F 1 out</cell><cell cols="2">0.78 ¬± 0.01 0.09 ¬± 0.03</cell></row><row><cell></cell><cell>F 1 train</cell><cell>0.86 ¬± 0.00</cell></row><row><cell>SVC</cell><cell>F 1 test</cell><cell cols="2">0.78 ¬± 0.01 0.08 ¬± 0.01</cell></row><row><cell></cell><cell>F 1 in</cell><cell cols="2">0.85 ¬± 0.01 0.01 ¬± 0.01</cell></row><row><cell></cell><cell>F 1 out</cell><cell cols="2">0.76 ¬± 0.01 0.10 ¬± 0.01</cell></row><row><cell></cell><cell>F 1 train</cell><cell>0.99 ¬± 0.00</cell></row><row><cell>RF</cell><cell>F 1 test</cell><cell cols="2">0.84 ¬± 0.00 0.15 ¬± 0.01</cell></row><row><cell></cell><cell>F 1 in</cell><cell cols="2">0.88 ¬± 0.00 0.11 ¬± 0.01</cell></row><row><cell></cell><cell>F 1 out</cell><cell cols="2">0.82 ¬± 0.00 0.17 ¬± 0.01</cell></row><row><cell>regression [56, 57]</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/pietrobarbiero/dataset-characteristics</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Eureqa Formulize is developed by Nutonian, Inc. https://www.nutonian.com/products/eureqa/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Code</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">McGraw-Hill series in computer science</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName><forename type="first">Chenyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga√´l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The WEKA workbench. online appendix for data mining: Practical machine learning tools and techniques</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Eibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><surname>Witten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automated machine learning for predictive modeling</title>
		<author>
			<persName><surname>Datarobot</surname></persName>
		</author>
		<ptr target="https://www.datarobot.com/" />
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Fran√ßois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The curse (s) of dimensionality</title>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krzywinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="399" to="400" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding cancer phenomenon at gene-expression level by using a shallow neural network chain</title>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Barbiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bertotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Ciravegna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Italian Workshop on Neural Networks (WIRN 2018)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Giansalvo Cirrincione, Elio Piccolo, and Alberto Tonda</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krzywinski</surname></persName>
		</author>
		<title level="m">Points of significance: Ensemble methods: bagging and random forests</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effects of dataset characteristics on the performance of feature selection techniques</title>
		<author>
			<persName><forename type="first">Dijana</forename><surname>Oreski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stjepan</forename><surname>Oreski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bozidar</forename><surname>Klicek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="109" to="119" />
			<date type="published" when="2017-03">mar 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Donald Michie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><surname>Campbell</surname></persName>
		</author>
		<title level="m">Machine Learning, Neural and Statistical Classification</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Ellis Horwood</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic programming</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">3731</biblScope>
			<biblScope unit="page" from="34" to="37" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adaptive control processes: a guided tour</title>
		<author>
			<persName><surname>Richard E Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Princeton university press</publisher>
			<biblScope unit="volume">2045</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on unsupervised outlier detection in highdimensional numerical data. Statistical Analysis and Data Mining: The ASA</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Science Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="363" to="387" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A statistical model for the analysis of ordinal level dependent variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Mckelvey</surname></persName>
		</author>
		<author>
			<persName><surname>Zavoina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical sociology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="120" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Points of significance: model selection and overfitting</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krzywinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Altman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Points of significance: Logistic regression</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krzywinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Altman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Points of significance: Classification and regression trees</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krzywinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Altman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequencies of events to their probabilities</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chervonenkis</forename><surname>Ya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Measures of complexity</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="11" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">General conditions for predictivity in learning theory</title>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">428</biblScope>
			<biblScope unit="issue">6981</biblScope>
			<biblScope unit="page">419</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new look at the statistical model identification</title>
		<author>
			<persName><forename type="first">Hirotugu</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selected Papers of Hirotugu Akaike</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1974">1974</date>
			<biblScope unit="page" from="215" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pareto-front exploitation in symbolic regression</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Smits</surname></persName>
		</author>
		<author>
			<persName><surname>Kotanchek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic programming theory and practice II</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="283" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-validatory choice and assessment of statistical predictions</title>
		<author>
			<persName><forename type="first">Mervyn</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="133" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the hardness of computing intersection, union and minkowski sum of polytopes</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Hans</surname></persName>
		</author>
		<author>
			<persName><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="469" to="479" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="463" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linear programming approaches to the convex hull problem in rm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Panos M Pardalos</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Mathematics with Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="23" to="29" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An introduction to linear algebra</title>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Mirsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Courier Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A note on the equality of the column, and row rank of a matrix</title>
		<author>
			<persName><forename type="first">George</forename><surname>Mackiw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics Magazine</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">285</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><surname>Gerald B Folland</surname></persName>
		</author>
		<title level="m">Real analysis: modern techniques and their applications</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Probability essentials</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Jacod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Protter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Openml: Networked science in machine learning</title>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Torgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Evaluation of classifiers for an uneven class distribution problem</title>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Daskalaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Kopanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Avouris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="381" to="417" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust tests for equality of variances</title>
		<author>
			<persName><forename type="first">Howard</forename><surname>Levene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ingram olkin, harold hotelling, et alia</title>
		<imprint>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The earth is flat (p&gt; 0.05): significance thresholds and the crisis of unreplicable research</title>
		<author>
			<persName><forename type="first">Fr√§nzi</forename><surname>Valentin Amrhein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Korner-Nievergelt</surname></persName>
		</author>
		<author>
			<persName><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3544</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Notes on the history of correlation</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="45" />
			<date type="published" when="1920">1920</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Skew variation, a rejoinder</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="169" to="212" />
			<date type="published" when="1905">1905</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Das Fehlergesetz und Seine Verallgemeiner-Ungen Durch Fechner und Pearson. A Rejoinder</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="169" to="212" />
			<date type="published" when="1905">1905</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sample estimate of the entropy of a random vector</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Kozachenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><forename type="middle">N</forename><surname>Leonenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problemy Peredachi Informatsii</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mutual information between discrete and continuous data sets</title>
		<author>
			<persName><forename type="first">Brian C</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">87357</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>L√≥pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Fern√°ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvador</forename><surname>Garc√≠a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Vasile Palade</surname></persName>
		</author>
		<author>
			<persName><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information sciences</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page" from="113" to="141" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Elementary linear algebra: applications version</title>
		<author>
			<persName><forename type="first">Howard</forename><surname>Anton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rorres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Cornelis Joost</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information retrieval</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dual coordinate descent methods for logistic regression and maximum entropy models</title>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang-Lan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in large margin classifiers</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Association, correlation and causation</title>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krzywinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="899" to="900" />
			<date type="published" when="2015-09">September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Genetic programming: on the programming of computers by means of natural selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Koza</surname></persName>
		</author>
		<author>
			<persName><surname>Koza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Distilling free-form natural laws from experimental data</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="issue">5923</biblScope>
			<biblScope unit="page" from="81" to="85" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Why does deep and cheap learning work so well</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Henry W Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><surname>Rolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1223" to="1247" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
