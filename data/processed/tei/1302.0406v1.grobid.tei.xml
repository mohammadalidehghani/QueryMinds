<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalization Guarantees for a Binary Classification Framework for Two-Stage Multiple Kernel Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-09-08">September 8, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
							<email>purushot@cse.iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering IIT Kanpur</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalization Guarantees for a Binary Classification Framework for Two-Stage Multiple Kernel Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-08">September 8, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">D38EEABA8D72A74FFC1AFCB48FE54E4F</idno>
					<idno type="arXiv">arXiv:1302.0406v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present generalization bounds for the TS-MKL framework for two stage multiple kernel learning. We also present bounds for sparse kernel learning formulations within the TS-MKL framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently Kumar et al <ref type="bibr" target="#b5">[6]</ref> proposed a framework for two-stage multiple kernel learning that combines the idea of target kernel alignment and the notion of a good kernel proposed in <ref type="bibr" target="#b0">[1]</ref> to learn a good Mercer kernel. More specifically, given a finite set of base kernels K 1 , . . . , K p over some common domain X , we wish to find some combination of these base kernels that is well suited to the learning task at hand. The paper considers learning a positive linear combination of the kernels K µ = p i=1 µ i K i for some µ ∈ R p , µ ≥ 0. It is assumed that the kernels are uniformly bounded i.e. for all x 1 , x 2 ∈ X and i = 1 . . . p, we have K i (x 1 , x 2 ) ≤ κ 2 i for some κ i &gt; 0. Let κ = κ 2 1 , . . . , κ 2 p ∈ R p . Note that κ ≥ 0. Also note that for any µ and any x 1 , x 2 ∈ X , we have K µ (x 1 , x 2 ) ≤ µ, κ .</p><p>The notion of suitability used in <ref type="bibr" target="#b5">[6]</ref> is that of kernel-goodness first proposed in <ref type="bibr" target="#b0">[1]</ref> for classification tasks. For sake of simplicity, we shall henceforth consider only binary classification tasks, the extension to multi-class classification tasks being straightforward. We present below the notion of goodness used in <ref type="bibr" target="#b5">[6]</ref>. For any binary classification task over a domain X characterized by a distribution D over X × {±1}, a Mercer kernel K : X × X → R with associated Reproducing Kernel Hilbert Space H K and feature map Φ K : X → H K is said to be (ǫ, γ)-kernel good if there exists a unit norm vector w ∈ H K such that w H K = 1 and the following holds</p><formula xml:id="formula_0">E (x,y)∼D 1 - y w, Φ(x) γ + ≤ ǫ</formula><p>2 Learning a Good Kernel</p><p>The key idea behind <ref type="bibr" target="#b5">[6]</ref> is to try and learn a positive linear combination of kernels that is good according to the notion presented above. We define the risk functional R(•) : R p → R + as follows:</p><formula xml:id="formula_1">R(µ) := E (x,y),(x ′ ,y ′ )∼D×D 1 -yy ′ K µ (x, x ′ ) +</formula><p>A combination µ will be said to be ǫ-combination good if R(µ) ≤ ǫ. The quantity R(µ) is of interest since an application of Jensen's inequality (see <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">Lemma 3.2]</ref>) shows us that for any µ ≥ 0 that is ǫ-combination good, the kernel K µ is ǫ, 1 µ,κ -kernel good. Furthermore, one can show, using standard results on capacity of linear function classes (see for example <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">Theorem 21]</ref>), that an (ǫ, γ)-good kernel can be used to learn, with confidence 1 -δ, a classifier with expected misclassification rate at most ǫ + ǫ 1 by using at most O κ 4 ǫ 2 1 γ 2 log 1 δ labeled samples. In order to cast this learning problem more cleanly, <ref type="bibr" target="#b5">[6]</ref> proposes the construction of a K-space using the following feature map</p><formula xml:id="formula_2">z : (x, x ′ ) → K 1 (x, x ′ ), . . . , K p (x, x ′ ) ∈ R p</formula><p>This allows us to write, for any µ ∈ R p , K µ (x, x ′ ) = µ, z(x, x ′ ) . Given n labeled training points (x 1 , y 1 ), . . . , (x n , y n ), define the empirical risk functional R(•) : R p → R + as follows 1 : <ref type="bibr" target="#b5">6]</ref> poses the learning problem as the following optimization problem:</p><formula xml:id="formula_3">R(µ) := 2 n(n -1) 1≤i&lt;j≤n [1 -y i y j µ, z(x i , x j ) ] +<label>[</label></formula><formula xml:id="formula_4">min µ≥0 λ 2 µ 2 2 + R(µ)</formula><p>3 Generalization Guarantees for a Learned Kernel Combination</p><p>Our generalization guarantee shall proceed in two steps. We shall assume that we have with us a training set (x 1 , y 1 ), . . . , (x n , y n ) using which we are able to determine a combination vector μ such that R(μ) ≤ ǫ.</p><p>1. We shall first prove that, with high probability over the choice of the training points, the learned combination vector μ will give us a kernel K μ that is ǫ + ǫ 1 , 1 μ,κ -kernel good where ǫ 1 &gt; 0 is a quantity that can be made arbitrarily small. 1 We note that <ref type="bibr" target="#b5">[6]</ref> includes the terms [1µ, z(xi, xi) ] + into the empirical risk as well. This does not change the asymptotics of our analysis except for causing a bit of notational annoyance. In order to account for this term, the true risk functional will have to include an additional term R add (µ) := E (x,y)∼D</p><p>[1 -Kµ(x, x)] + . This will add a negligible term to the uniform convergence bound because we will have to consider the convergence of the term Radd (µ) :=</p><formula xml:id="formula_5">2 n(n+1)</formula><p>1≤i≤n [1µ, z(xi, xi) ] + to R add . However, from thereon, the analysis will remain unaffected since R add (µ) ≥ 0 so a combination µ having true risk R(µ) + R add (µ) ≤ ǫ will still give a kernel Kµ that is ǫ, 1 µ,κ -kernel good.</p><p>2. We shall then prove that given that there exists a good combination of kernels in the K-space, with very high probability ǫ will be very small. This we will prove by showing a converse of the inequality proved in the first step. This will allow us to give oracle inequalities for the kernel goodness of the learned combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Step 1</head><p>In this step, we prove a uniform convergence guarantee for the learning problem at hand. Using standard proof techniques, we shall reduce the problem of uniform convergence to that of estimating the capacity of a certain function class. The notion of capacity we shall use is the Rademacher complexity which we shall bound using the heavy hammer of strong convexity based bounds from <ref type="bibr" target="#b4">[5]</ref>. We note that the proof progression used in this step is fairly routine within the empirical process community and has been used to give generalization proofs for other problems as well (see for example <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>).</p><p>First of all we note that due to the optimization process we have<ref type="foot" target="#foot_0">foot_0</ref> </p><formula xml:id="formula_6">λ 2 μ 2 2 ≤ λ 2 μ 2 2 + R(μ) ≤ λ 2 0 2 2 + R(0) = 1</formula><p>which implies that we need only concern ourselves with combination vectors inside the L 2 ball of</p><formula xml:id="formula_7">radius r λ = 2 λ . B 2 (r λ ) := {µ ∈ R p : µ 2 ≤ r λ }</formula><p>For notational simplicity, we denote z = (x, y) as a training sample. For any training set z 1 , . . . , z n where z i = (x i , y i ) and for any µ ∈ R p , we write ℓ(µ, z i , z j ) := [1 -y i y j µ, z(x i , x j ) ] + . We assume, yet again for the sake of notational simplicity, that we obtain at all times, an even number of training samples i.e. n is even. For a ghost sample z1 , . . . , zn then, we can write</p><formula xml:id="formula_8">E 2 n(n -1) 1≤i&lt;j≤n ℓ(µ, zi , zj ) = 2 n(n -1) 1≤i&lt;j≤n E ℓ(µ, zi , zj ) = 2 n(n -1) 1≤i&lt;j≤n R(μ) = R(μ)</formula><p>Thus we can write</p><formula xml:id="formula_9">R(μ) -R(μ) = E 2 n(n -1) 1≤i&lt;j≤n ℓ(µ, zi , zj ) -R(μ) ≤ sup µ∈B 2 (r λ )    E 2 n(n -1) 1≤i&lt;j≤n ℓ(µ, zi , zj ) -R(µ)    Let g (z 1 , . . . , z n ) = 2 n(n -1) sup µ∈B 2 (r λ )    E 1≤i&lt;j≤n ℓ(µ, zi , zj ) - 1≤i&lt;j≤n ℓ(µ, z i , z j )    For any µ ∈ B 2 (r λ ) and any x 1 , x 2 ∈ X , we have K µ (x 1 , x 2 ) ≤ µ, κ ≤ r λ κ 2 .</formula><p>Using this, it is not difficult to see that the expression g (z 1 , . . . , z n ) can be perturbed by at most 2 n (1 + r λ κ 2 ) by the change of a single true training sample z i = (x i , y i ) (see <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">Theorem 3.4]</ref> for the calculations). Applying McDiarmid's inequality to this expression, we get with probability at least 1 -δ,</p><formula xml:id="formula_10">R(μ) -R(μ) ≤ E g (z 1 , . . . , z n ) + (1 + r λ κ 2 ) 2 log 1 δ n</formula><p>We now estimate the the expectation term on the right hand side.</p><formula xml:id="formula_11">E g (z 1 , . . . , z n ) = 2 n(n -1) E sup µ∈B 2 (r λ )    E 1≤i&lt;j≤n ℓ(µ, zi , zj ) - 1≤i&lt;j≤n ℓ(µ, z i , z j )    ≤ 2 n(n -1) E sup µ∈B 2 (r λ )    1≤i&lt;j≤n ℓ(µ, zi , zj ) - 1≤i&lt;j≤n ℓ(µ, z i , z j )   </formula><p>We now invoke a powerful alternate representation for U-statistics to simplify the above expression. This method can be found in <ref type="bibr" target="#b3">[4]</ref> that itself attributes this method to <ref type="bibr" target="#b7">[8]</ref>. This, along with the Hoeffding decomposition, are two of the most powerful techniques to deal with "coupled" random variables as we have in this situation.</p><p>Theorem 1 ([4], Lemma A.1). For any set of real valued functions q τ : X × X → R indexed by τ ∈ T , if X 1 , . . . , X n are i.i.d. random variables then we have</p><formula xml:id="formula_12">E sup τ ∈T 2 n(n -1) 1≤i&lt;j≤n q τ (X i , X j ) ≤ E sup τ ∈T 2 n n/2 i=1 q τ (X i , X n/2+i )</formula><p>Applying this decoupling result to the random variables X i = (z i , z i ), the index set B 2 (r λ ) and functions q τ (X i , X j ) = ℓ(µ, zi , zj ) -ℓ(µ, z i , z j ) we get</p><formula xml:id="formula_13">E g (z 1 , . . . , z n ) ≤ 2 n E sup µ∈B 2 (r λ )    n/2 i=1 ℓ(µ, zi , zn/2+i ) -ℓ(µ, z i , z n/2+i )    = 2 n E sup µ∈B 2 (r λ )    n/2 i=1 ǫ i ℓ(µ, zi , zn/2+i ) -ℓ(µ, z i , z n/2+i )    ≤ 4 n E sup µ∈B 2 (r λ )    n/2 i=1 ǫ i ℓ(µ, z i , z n/2+i )    = 4 n E sup µ∈B 2 (r λ )    n/2 i=1 ǫ i 1 -y i y n/2+i µ, z(x i , x n/2+i ) +    ≤ 4 n E sup µ∈B 2 (r λ )    n/2 i=1 ǫ i µ, z(x i , x n/2+i )    = 2R n/2 (B 2 (r λ ))</formula><p>where in the second step, we performed symmetrization on the decoupled expression by introducing Rademacher random variables ǫ i , i = 1, . . . , n/2. In the fifth step we have applied the contraction inequality stated in Theorem 2 below on the 1-Lipschitz function φ i : x →= [1 -a i x] + where a i = y i y n/2+i . We have exploited the fact that Theorem 2 actually proves the contraction inequality for the empirical Rademacher averages which allows us to treat a i as constants dependent only on i.</p><p>Theorem 2. Let H be a set of bounded real valued functions from some domain X and let x 1 , . . . , x n be arbitrary elements from X . Furthermore, let φ i : R → R, i = 1, . . . , n be L-Lipschitz functions.</p><p>Then we have</p><formula xml:id="formula_14">E sup h∈H 1 n n i=1 ǫ i φ i (h(x i )) ≤ LE sup h∈H 1 n n i=1 ǫ i h(x i )</formula><p>Proof. <ref type="bibr">Ledoux</ref> and Talagrand (see <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">Theorem 4</ref>.12]) prove the same result but for wrapper functions that satisfy φ i (0) = 0 for all i. To get the result, simply apply the result to the functions φi :</p><formula xml:id="formula_15">x → φ i (x) -φ i (0) to get E sup h∈H 1 n n i=1 ǫ i φ i (h(x i )) ≤ E sup h∈H 1 n n i=1 ǫ i φi (h(x i )) +E 1 n n i=1 ǫ i φ i (0) ≤ LE sup h∈H 1 n n i=1 ǫ i h(x i )</formula><p>where we apply <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">Theorem 4</ref>.12] to the first term and the second term vanishes by linearity of expectation.</p><p>The concluding term in the last chain of inequalities gives us the Rademacher complexity of the hypothesis class B 2 (r λ ). At this point we introduce the following result on Rademacher complexities of regularized linear predictor classes Theorem 3 ([5], Theorem 1). Let W be a closed convex set and let F : W → R be λ-strongly convex w.r.t. • * . Assume W ⊆ w : F (w) ≤ W 2 * . Furthermore, let X = {x : x ≤ X} and F W := {w → w, x : w ∈ W, x ∈ X }. Then, we have</p><formula xml:id="formula_16">Rn (F W ) ≤ XW * 2 λn</formula><p>Although <ref type="bibr" target="#b4">[5]</ref> make their claim for the normal Rademacher average but their proof actually gives bounds for the empirical Rademacher averages. Since our hypothesis class is L 2 regularized, we can apply Theorem 3 to the L 2 /L 2 case with F (µ) = µ 2 2 as the regularizer. Since we have sup</p><formula xml:id="formula_17">x 1 ,x 2 ∈X z(x 1 , x 2 ) 2 ≤ κ 2 , we get R n/2 (B 2 (r λ )) ≤ r λ κ 2 2 n = 2 κ 2 1 λn</formula><p>We have thus proved the following result </p><formula xml:id="formula_18">R(µ) ≤ R(µ) + 4 κ 2 1 λn + 1 + κ 2 2 λ 2 log 1 δ n ≤ R(µ) + 6 κ 2 log 1 δ λn Since μ ∈ B 2 (r λ ), we have μ, κ ≤ 2 λ κ 2 .</formula><p>This implies that the kernel K μ is at least</p><formula xml:id="formula_19">ǫ + ǫ 1 , 1 κ 2 λ</formula><p>2 -kernel good where ǫ = R(µ) and ǫ 1 ≤ 6 κ 2 log 1 δ λn . In particular, if all the p kernels share a common bound i.e. κ i ≤ κ for all i, then κ 2 ≤ κ 2 √ p and we can show the kernel</p><formula xml:id="formula_20">K μ to be ǫ + 6κ 2 p log 1 δ λn , 1 κ 2 λ 2p -kernel good.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Step 2</head><p>Just as we analyzed the excess risk expression R(µ) -R(µ) uniformly over vectors the ball B 2 (r λ ), we can similarly analyze the expression R(µ) -R(µ) uniformly over any (fixed) ball B 2 (r) to get the following result.</p><p>Theorem 5. Let r &gt; 0 be some fixed radius, then with probability at least 1 -δ over the choice of training samples, all combination vectors µ ∈ B 2 (r) satisfy</p><formula xml:id="formula_21">R(µ) ≤ R(µ) + 2r κ 2 2 n + (1 + r κ 2 ) 2 log 1 δ n</formula><p>This allows us to give the following oracle inequality: Theorem 6. Suppose as an oracle assumption we assume that there exists a good combination vector µ o that is ǫ o -combination good, then we can output with probability at least 1 -δ, for any</p><formula xml:id="formula_22">ǫ 1 &gt; 0 using n = Ω µ o 2 2 ǫ 3 1</formula><p>training samples, a combination vector such that the corresponding kernel that is</p><formula xml:id="formula_23">ǫ o + ǫ 1 , 1 κ 2 µ o 2 ǫ 1 3 -kernel good.</formula><p>Proof. Using Theorem 5 we have with probability at least 1 -δ,</p><formula xml:id="formula_24">R(µ o ) ≤ ǫ o + 2 µ o 2 κ 2 2 n + (1 + µ o 2 κ 2 ) 2 log 1 δ n ≤ ǫ o + 6 µ o 2 κ 2 2 log 1 δ n</formula><p>Since μ is the minimizer of the regularized empirical risk, we have</p><formula xml:id="formula_25">λ 2 μ 2 2 + R(μ) ≤ λ 2 µ o 2 2 + R(µ o ) ≤ λ 2 µ o 2 2 + ǫ o + 6 µ o 2 κ 2 2 log 1 δ n which gives us, since μ 2 ≥ 0, R(μ) ≤ ǫ o + λ 2 µ o 2 2 + 6 µ o 2 κ 2 2 log 1 δ n</formula><p>Applying Theorem 4 we get with probability at least 1 -2δ,</p><formula xml:id="formula_26">R(μ) ≤ ǫ o + λ 2 µ o 2 2 + 6 µ o 2 κ 2 2 log 1 δ n + 6 κ 2 log 1 δ λn For any 0 &lt; ǫ 1 &lt; 3/4, setting λ = 2ǫ 1 3 µ o 2 2</formula><p>and requiring n ≥ 500</p><formula xml:id="formula_27">ǫ 3 1 µ o 2 2 κ 2 2 log 1</formula><p>δ so that all three terms in the above expression are less than ǫ 1 /3 gives us the result (for values of ǫ 1 larger than 3/4, n ≥ 650</p><formula xml:id="formula_28">ǫ 2 1 µ o 2 2 κ 2 2 log 1 δ suffices).</formula><p>Such oracle inequalities are very desirable since they tell us that we would be able to give a performance that is competitive against any fixed kernel in foresight. If we set λ to an oracle oblivious value such as λ = 3 1 n then although we get an inferior claim with respect to the kernelgoodness, we are able to make that claim in hindsight as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Sparse Kernel Combinations</head><p>Since the complexity of the evaluating the kernel K µ goes up roughly as µ 0 , it is desirable to learn sparse combinations. This can be done by changing the learning formulation slightly to the following:</p><formula xml:id="formula_29">min µ≥0 λ 2 µ 1 + 2 n(n -1) 1≤i&lt;j≤n [1 -y i y j µ, z(x i , x j ) ] +</formula><p>The above learning algorithm can also shown to admit generalization guarantees. For sake of brevity we only give below the main points where the analysis differs from the L 2 regularized case. First of all, we would be able to show that the regularized empirical risk minimizer μ would lie in the L 1 ball B 1 (s λ ) := {µ ∈ R p : µ 1 ≤ s λ } where s λ = 2 λ . Due to this the perturbations to the expression g (z 1 , . . . , z n ) would be limited by 2  n (1 + s λ κ ∞ ). While applying Theorem 3, we would instead consider the regularizer F (µ) = µ 2 q for q = log p log p-1</p><p>which is 1 log p -strongly convex with respect to the norm • 1 . This would allow us to bound the Rademacher complexity of the hypothesis class B 1 (s λ ) as</p><formula xml:id="formula_30">R n/2 (B 1 (s λ )) ≤ s λ κ ∞ 2 log p n = 2 κ ∞ λ 2 log p n</formula><p>This allows us to make the following claim:</p><p>Theorem 7. With probability at least 1 -δ the choice of training samples, the minimizer μ of the expression</p><formula xml:id="formula_31">min µ≥0 λ 2 µ 1 + R(µ)</formula><p>satisfies the following</p><formula xml:id="formula_32">R(µ) ≤ R(µ)+ 4 κ ∞ λ 2 log p n + 1 + 2 κ ∞ λ 2 log 1 δ n ≤ R(µ)+ 6 κ ∞ λ √ n log p + log 1/δ Since μ ∈ B 1 (s λ ), we have μ, κ ≤ 2 κ ∞ λ . This implies that the kernel K μ is ǫ + ǫ 1 , λ 2 κ ∞ - kernel good where ǫ = R(µ) and ǫ 1 ≤ 6 κ ∞ λ √ n</formula><p>√ log p + log 1/δ . In particular, if all the p kernels share a common bound i.e. κ i ≤ κ for all i, then κ ∞ ≤ κ 2 and we can show the kernel K μ to be</p><formula xml:id="formula_33">ǫ + 6κ 2 λ √ n</formula><p>√ log p + log 1/δ , λ 2κ 2 -kernel good. Note that this result has a much better dependence on p that the result for L 2 regularized learning where we were only able to show that the kernel K μ was ǫ + 6κ</p><p>2 p log 1 δ λn , 1 κ 2 λ 2p -kernel good.</p><p>We can also show the following version of Theorem 5 to be true Theorem 8. Let s &gt; 0 be some fixed radius, then with probability at least 1 -δ over the choice of training samples, all combination vectors µ ∈ B 1 (s) satisfy</p><formula xml:id="formula_34">R(µ) ≤ R(µ) + 2s κ ∞ 2 log p n + (1 + s κ ∞ ) 2 log 1 δ n</formula><p>Using this, and going as before, we are also able to guarantee the following oracle inequality similar to Theorem 6 Theorem 9. Suppose as an oracle assumption we assume that there exists a good combination vector µ o that is ǫ o -combination good, then we can output with probability at least 1 -δ, for any</p><formula xml:id="formula_35">ǫ 1 &gt; 0 using n = Ω µ o 1 ǫ 2 1</formula><p>training samples, a combination vector such that the corresponding kernel that is</p><formula xml:id="formula_36">ǫ o + ǫ 1 , ǫ 1 3 κ ∞ µ o 1 -kernel good.</formula><p>Proof. Following the chain of inequalities given by Theorems 7 and 8 and using optimality of the regularized empirical risk minimizer μ, we get, with probability at least 1 -2δ,</p><formula xml:id="formula_37">R(μ) ≤ ǫ o + λ 2 µ o 1 + 6 κ ∞ √ log p + log 1/δ √ n µ o 1 + 1 λ Setting λ = 2ǫ 1 3 µ o 1</formula><p>and requiring n ≥</p><formula xml:id="formula_38">135 µ o 1 κ ∞ √ log p+ √ log 1/δ ǫ 2 1</formula><p>finishes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion on the Nature of Guarantees</head><p>The guarantees given above, both for the sparse as well as the non-sparse kernel learning cases are slightly unsatisfactory in the sense they assume combination goodness to ensure kernel goodness. In other words they assume the existence of a combination that is ǫ-combination before guaranteeing that the output would be a kernel that is (ǫ ′ , γ ′ )-kernel good. Ideally, we should have used the promise of existence of a kernel that (ǫ, γ)-kernel good to ensure that a good kernel is output. One way to prove such a result would be to show that if there exists a kernel combination that is (ǫ, γ)-kernel good, then there also exists some combination µ ∈ R p that is ǫ ′ -combination good for some ǫ ′ &gt; 0. However, this is an unlikely result and the the aim of this section is to discuss this point. It turns out that the biggest hurdle that one faces in proving such a result is the form of combination goodness chosen by <ref type="bibr" target="#b5">[6]</ref>. The definition of combination goodness used in <ref type="bibr" target="#b5">[6]</ref> is related to the notion of similarity goodness proposed in <ref type="bibr" target="#b0">[1]</ref> except for the absence of a weight function.</p><p>More specifically, <ref type="bibr" target="#b0">[1]</ref> consider a kernel K µ to be ǫ-similarity good if for some weight function w : X → R the following holds:</p><formula xml:id="formula_39">E (x,y)∼D 1 -y E (x ′ ,y ′ )∼D y ′ w(x ′ )K µ (x, x ′ ) + ≤ ǫ</formula><p>For ease of comparison, we have absorbed the margin parameter γ in the definition given in <ref type="bibr" target="#b0">[1]</ref> into the weight function w(•). Note that if the notion of combination goodness had been defined using</p><formula xml:id="formula_40">R(µ) := E (x,y),(x ′ ,y ′ )∼D×D 1 -yy ′ w(x ′ )K µ (x, x ′ ) +</formula><p>instead, then one could have used some form of inverse Jensen's inequality to convert similarity goodness into combination goodness. Since the presence of the weight function makes it possible for crisp conversions of kernel goodness into similarity goodness as was done in <ref type="bibr" target="#b8">[9]</ref>, this could have been one way to convert kernel goodness into combination goodness (i.e. via similarity goodness). However, due to the absence of such weight functions, it seems difficult to convert kernel goodness into combination goodness using the methods of <ref type="bibr" target="#b8">[9]</ref>. Another reason to believe in the non-existence of such conversions from kernel to combination goodness is the form of the predictor in the RKHS. If one looks at the proof of Lemma 3.2 in <ref type="bibr" target="#b5">[6]</ref> then one notices that the kernel goodness is proven with respect to the predictor w = E (x,y)∼D y ′ Φ H Kµ (x) where Φ H Kµ : X → H Kµ is the feature map corresponding to the kernel K µ . This turns out to be very a restrictive form for the predictor. A kernel can be good due to the existence of any unit norm predictor in its RKHS. However the notion of combination goodness seems to prefer predictors that point from the mean of the images of the negative points to the mean of the images of the positive points in the RHKS. It was noted in <ref type="bibr" target="#b0">[1]</ref> that such a notion of goodness is too strong ( <ref type="bibr" target="#b0">[1]</ref> actually call this the strongly-good notion of similarity goodness) and that there exist kernels that are very good with respect to the learning task at hand but the uniform vectors w = E (x,y)∼D y ′ Φ H Kµ (x) in their RKHSes perform poorly (see <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">Definition 2]</ref> and the discussion thereafter). Thus it seems unlikely that the current proof technique can be extended to accept promises of kernel goodness. The technique seems inherently suited to accept combination goodness and output good kernels. It would be interesting to see whether the existing proofs can be modified or whether the algorithms can be modified to accommodate kernel goodness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 4 .</head><label>4</label><figDesc>With probability at least 1 -δ over the choice of training samples, the minimizer μ</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Any tolerance ǫopt offered by the optimizer can easily be incorporated into the bounds. However, we do not do so for sake of clarity.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On a Theory of Learning with Similarity Functions</title>
		<author>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian Complexities: Risk Bounds and Structural Results</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generalization Bounds for Metric and Similarity Learning</title>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Chu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Ying</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.5437</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ranking and empirical minimization of U-statistics</title>
		<author>
			<persName><forename type="first">Stéphan</forename><surname>Clémençon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vayatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="844" to="874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Binary Classification Framework for Two-Stage Multiple Kernel Learning</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Michel</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Talagrand</surname></persName>
		</author>
		<title level="m">Probability in Banach Spaces: Isoperimetry and Processes</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Serfling</surname></persName>
		</author>
		<title level="m">Approximation Theorems of Mathematical Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How Good Is a Kernel When Used as a Similarity Measure?</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th Annual Conference on Computational Learning Theory</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="323" to="335" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
