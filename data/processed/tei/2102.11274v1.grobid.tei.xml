<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUSTAINABLE FEDERATED LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Başak</forename><surname>Güler</surname></persName>
							<email>bguler@ece.ucr.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Riverside Department of Electrical and Computer Engineering Riverside</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92521</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aylin</forename><surname>Yener</surname></persName>
							<email>yener@ece.osu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Riverside Department of Electrical and Computer Engineering Riverside</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92521</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering Columbus</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SUSTAINABLE FEDERATED LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18F95DF878F22D0F02AC993EAD5B3A9C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sustainable machine learning</term>
					<term>federated learning</term>
					<term>green AI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Potential environmental impact of machine learning by large-scale wireless networks is a major challenge for the sustainability of future smart ecosystems. In this paper, we introduce sustainable machine learning in federated learning settings, using rechargeable devices that can collect energy from the ambient environment. We propose a practical federated learning framework that leverages intermittent energy arrivals for training, with provable convergence guarantees. Our framework can be applied to a wide range of machine learning settings in networked environments, including distributed and federated learning in wireless and edge networks. Our experiments demonstrate that the proposed framework can provide significant performance improvement over the benchmark energy-agnostic federated learning settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>The environmental impact of machine learning matters. Modern machine learning systems consume massive amounts of energy. In fact, the computational resources needed to train a state-of-the-art deep learning model has increased by 300000x between 2012-2018 <ref type="bibr" target="#b0">[1]</ref>. Today, it is estimated that training a single deep learning model can generate as much CO 2 as the total lifetime of five cars <ref type="bibr" target="#b1">[2]</ref>. This impact will worsen with the emergence of machine learning in distributed and federated learning settings, where billions of devices are expected to train machine learning models on a regular basis. In this paper, we provide a first study for sustainable machine learning in the federated learning setting, through the use of compute devices that can generate energy from renewable sources in the ambient environment, such as solar, kinetic, ambient light, or ambient RF energy <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>.</p><p>Federated learning is a communication-efficient and privacy-preserving distributed learning framework for training machine learning models over large volumes of data created and stored locally at millions of remote clients, such as the data generated at mobile or edge devices <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref>. This is an iterative training process that is coordinated by a central server. The server maintains a global model and sends its current state to the clients at the beginning of each training iteration. During training, the data collected by the individual devices never leaves the device, instead, devices locally update the global model using their local dataset, creating a local model. The local models are then sent to the central server, who then aggregates the local models to update the global model. It has received significant attention in the recent years and has found a variety of applications from keyboard query recommendations to healthcare, from electrical load forecasting to traffic flow prediction <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2102.11274v1 [cs.LG] 22 Feb 2021</head><p>Recent works have considered energy efficient training strategies for federated learning <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. In these works, the primary focus is on either minimizing the total energy cost of training <ref type="bibr" target="#b23">[24]</ref>, or minimizing the training loss within a given total energy budget <ref type="bibr" target="#b24">[25]</ref>, where all of the energy is available at the beginning of training. In contrast, our focus is on federated learning when devices generate energy through an intermittent and non-homogeneous renewal process. Our goal is to build a scalable and practical federated learning framework with provable convergence guarantees, for networks with intermittent energy arrivals.</p><p>Prior work has investigated client selection in the context of federated learning primarily for improving the convergence rate or reducing the communication overhead of training <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b29">[30]</ref>. Such algorithms are designed with the underlying assumption that all clients are available to participate in training if selected. Then, the goal is to either sample a small number of clients uniformly at random to minimize the communication overhead per iteration, or to select the clients that maximize the convergence rate of training. In contrast, in our setting, whether or not a client can participate in training is determined by an underlying energy arrival process, which is intermittent and non-homogeneous across the clients. Several works have considered federated learning when clients may dropout from the system during training, however, in these setups, the main assumption is that the client dropouts occur uniformly at random, which does not bias the training process <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p><p>In this work, we consider a federated learning scenario with N clients and a server. Client i holds a local dataset D i . Clients wish to jointly train a machine learning model over the datasets D 1 , . . . , D N . Training is coordinated by the central server, who maintains a global model that is updated locally by the clients through an iterative process. More specially, at each training iteration, the server sends the current state of the global model to the clients. Then, clients locally update the global model, through multiple stochastic gradient descent (SGD) iterations over their local dataset, and send their local updates to the server. Finally, the server updates the global model by aggregating the local updates received from the clients, and sends the updated global model back to the clients, to be used in the next iteration.</p><p>Unlike the conventional federated learning setting, we assume that clients only have intermittent energy availability, and can participate in the training process only when they have energy available. The energy generation process is not uniform across the devices, that is, some clients may have more frequent energy arrivals than others. One potential approach in this setting is to let each client to participate in training as soon as they generate enough energy to do so. However, as we demonstrate in our experiments, in this setting, conventional federated learning strategies may bias the global model towards clients with more frequent energy arrivals, causing a performance loss in the resulting accuracy. Another approach is to wait until all clients generate enough energy to participate in training before each iteration of the conventional federated learning scheme. Doing so, however, would require waiting for the clients with the slowest energy generation, therefore, even though the training is unbiased, the convergence rate can be very slow to reach a desired performance level.</p><p>We propose a simple federated learning framework with provable convergence guarantees, for networks in which devices generate energy through an intermittent renewable energy source. The proposed framework consists of three main components, client scheduling, local training at the clients, and model update at the server. Client scheduling is performed at the client level, in other words, each client decides whether or not to participate at any given training iteration based solely on the local estimation of the energy arrival process. The client scheduling process requires no coordination between the clients, and is scalable to large networks. During the local training phase, clients who choose to participate at the current training iteration update the global model using their local datasets, and then send their local updates to the server. Upon receiving the local updates, the server updates the global model for the next iteration.</p><p>In our experiments, we compare the performance of the proposed framework with benchmark federated learning settings that are agnostic to the energy-arrival process of the clients. The first benchmark is the federated learning setting in which clients participate in training as soon as they generate enough energy, and then wait for the next energy arrival. The second benchmark is the setting in which the server waits for all clients to have enough energy to participate in training before initiating a single training iteration. We show that the proposed framework can significantly outperform both benchmarks in terms of the test accuracy.</p><p>This paper is a first study of sustainable federated learning, and we hope our work to open up new research directions in building sustainable federated and distributed learning schemes for large-scale networks, where millions of devices jointly train machine learning models over large volumes of data. Some of these research directions include, formalizing the fundamental performance limits of distributed training under stochastic and unknown energy arrival processes, model quantization and compression techniques that can adapt to the resource and energy arrival patterns, and characterizing the relationship between the energy renewal processes and training performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Problem Formulation A. Federated Learning Setup</head><p>We consider a distributed training setting with N clients. Client i has a local dataset D i with D i data points. The total number of data points across all clients is D = i∈[N ] D i . The clients are connected through a central server that coordinates the training.</p><p>The goal is to train a model w to minimize a global loss function</p><formula xml:id="formula_0">F (w) = 1 D N i=1 Di j=1 l(w, x ij )<label>(1)</label></formula><p>where l(w, x ij ) represents the loss of a single data point x ij in the local dataset D i of client i.</p><p>By defining a local loss function</p><formula xml:id="formula_1">F i (w) = 1 D i Di j=1 l(w, x ij )<label>(2)</label></formula><p>for client i, the global loss function in (1) can be written as</p><formula xml:id="formula_2">F (w) = N i=1 p i F i (w)<label>(3)</label></formula><p>where p i := Di D and thus,</p><formula xml:id="formula_3">i∈[N ] p i = 1<label>(4)</label></formula><p>We next provide the details of training in the conventional federated learning setting <ref type="bibr" target="#b6">[7]</ref>. In this setting, the server maintains a global model that is updated locally by the clients. The local updates are then aggregated at the server to update the global model. As such, the training process consists of local and global update iterations.</p><p>Each iteration (local or global) is represented by a discrete time instant t ∈ {0, 1, 2, . . .}. It is assumed that a global update occurs at every T time instants, where T is the number of local training iterations that take place between two global updates. Without loss of generality, we assume that a global update occurs when t mod T = 0, and let</p><formula xml:id="formula_4">T = {t : t mod T = 0}<label>(5)</label></formula><p>denote the set of time instances at which a global update occurs, which we also refer to as synchronization steps.</p><p>At the beginning of each global round t ∈ T , the server sends the current state of the global model to the clients, which is denoted by a vector w (t) ∈ R d of dimension d, where d is the model size. Then, all or a fraction of the clients update the global model w (t) through T local training iterations, using their local datasets. The set of clients that participate in training at a given iteration depends on the specifics of the client scheduling algorithm, which could range from all clients to a small fraction of clients. We let S t denote the set of participating clients at iteration t.</p><p>Local training at the clients is performed through stochastic gradient descent (SGD), in which the model parameters are updated iteratively in the negative direction of the gradient evaluated over a random sample (or a minibatch) from the local dataset.</p><p>To present the details of the local training process, we consider a synchronization step t ∈ T , at which the server sends the current estimation of the global model w (t) to the clients. We also let w</p><formula xml:id="formula_5">(t) i</formula><p>denote the local estimation of the model parameters at client i at time t. Accordingly, we will call w</p><formula xml:id="formula_6">(t) i the local model of client i at time t. When t ∈ T , client i ∈ [N ] sets its local model as, w (t) i ← w (t) .<label>(6)</label></formula><p>In other words, at each synchronization step, clients synchronize their local models with the current state of the global model. Then, client i ∈ S t updates their local model through T SGD iterations,</p><formula xml:id="formula_7">w (t+j+1) i = w (t+j) i -η t+j ∇F i (w (t+j) i , ξ (t+j) i )<label>(7)</label></formula><p>for j ∈ {0, . . . , T -1}, where w</p><formula xml:id="formula_8">(t+0) i = w (t)</formula><p>, η t+j is the learning rate (step size), and ∇F i (w</p><formula xml:id="formula_9">(t+j) i , ξ<label>(t+j) i</label></formula><p>) denotes the stochastic gradient of client i, with ξ (t) i representing a uniformly random sample (or a minibatch) from D i . The stochastic gradient is an unbiased estimator of the true gradient of client i,</p><formula xml:id="formula_10">E[∇F i (w (t+j) i , ξ (t+j) i )] = ∇F i (w (t+j) i )<label>(8)</label></formula><p>where ∇F i (w (t+j) ) is the true gradient of client i, i.e., the gradient of the local loss function (2) evaluated at w (t+j) i</p><p>.</p><p>At the end of T local SGD operations, clients i ∈ S t+T -1 send their local updates w (t+T ) i from ( <ref type="formula" target="#formula_7">7</ref>) to the server. For all clients not participating in the current global round, i.e., for clients i / ∈ S t+T -1 , it is assumed that w</p><formula xml:id="formula_11">(t+T ) i = w (t) .</formula><p>Finally, the server updates the global model,</p><formula xml:id="formula_12">w (t+T ) = i∈[N ] p i w (t+T ) i . (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>by aggregating the local models of the clients. After updating the global model, the server sends the updated global model w (t+T ) to the clients for the next iteration.</p><p>We use the term global round to refer to the block of T time instances between two consecutive syncronization steps (global updates). In other words, global round t corresponds to the block of time instances t ∈ {t, . . . , t + T -1}.</p><p>We also note that our focus is on the conventional synchronous federated learning setup, in which all clients participating at a given global round perform the same number of local training iterations and the global model is updated only at specified time instances t ∈ T . Asynchronous learning scenarios in which clients can perform varying number of local iterations and communicate their local models with the server at arbitrary time instances are interesting future directions, but are beyond our scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Energy Profile of the Clients</head><p>In this work, we consider devices that are powered by the small quantities of energy generated from the ambient environment, such as solar, kinetic, ambient light or RF energy <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b34">[35]</ref>. Clients can participate in training only if they have available energy to do so.</p><p>It is assumed that it takes E i global rounds for device i to generate enough energy to participate in one global round of training, which includes the energy cost of computing the T local updates from <ref type="bibr" target="#b6">(7)</ref> and communicating it with the server. We call E i the energy renewal cycle of client i.</p><p>As we demonstrate in our experiments, in this setting, i.e., when clients have intermittent energy arrivals, the conventional federated learning setup from Section II-A might bias the model towards clients with more frequent energy availability. This calls for an energy-aware client scheduling and training strategy which we study in this paper.</p><p>Main Problem. Given the above energy arrival and training setup, the main problem we study in this work is, "How to design a scalable federated learning framework for devices with intermittent energy availability?". In the sequel, we provide a practical federated learning framework that takes into account the energy limitations of the clients during training, while ensuring theoretical convergence guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Federated Learning with Intermittent Resource Arrivals</head><p>We now introduce a practical federated learning framework for networks in which devices have intermittent energy availability. The overall procedure of our framework is provided in Algorithm 1. Our framework consists of three main components, client scheduling, local training at the clients, and global model update at the server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Client scheduling</head><p>The first component of our framework is client scheduling for training. Client selection in conventional federated learning algorithms are primarily based on the assumption that all clients are inherently available to participate in training if chosen, or that client dropouts occur uniformly at random (which does not bias the training), and focus on selecting the clients to maximize the convergence rate or to reduce the communication overhead of training <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. In contrast, in our setup, not all clients can participate in the training process at all rounds. In</p><p>Algorithm 1 Federated Learning with Intermittent Resource Arrivals input Number of devices N , local dataset D i and energy renewal cycle E i of device i ∈ [N ], number of local training iterations T at each global round, total number of training iterations K where K T ×Ei ∈ Z + for i ∈ [N ]. output Global model w (K) . Initialization: 1: for client i = 1, . . . , N do 2: Initialize I t i ← 0 for t ∈ [K] and i ∈ [N ]. // Indicates whether client i participates at iteration t. Training: 3: for iteration t = 1, . . . , K do Clients: 4: for client i = 1, . . . , N do 5: if t mod T E i = 0 then // Client i has enough energy to participate in training. 6: Sample an integer J uniformly random from {0, . . . , E i -1}. 7: Update I t+JT +l i ← 1 for l ∈ {0, . . . , T -1}. // Client i is scheduled at global round t + JT . 8: if t mod T = 0 then 9: if I t i = 1 then // Client i locally updates the model. 10: Initialize the local model w (t) i ← w (t) 11:</p><p>for iteration j = 0, . . . , T -1 do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Update the local model according to <ref type="bibr" target="#b6">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Send the local update g (t) i from ( <ref type="formula" target="#formula_17">12</ref>) to the server. Server:</p><formula xml:id="formula_14">14: if (t + 1) mod T = 0 then 15:</formula><p>Receive the local updates g (t+1) i from the clients in S t = {i : I t i = 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Update the global model according to <ref type="bibr" target="#b12">(13)</ref>,</p><formula xml:id="formula_15">w (t+1) = w (t-T +1) + i∈St p i g (t+1) i<label>(10) 17:</label></formula><p>Send the updated global model w (t+1) to the clients. particular, if a client has participated in one round, they may not have enough energy to participate in the next round. Moreover, the energy availability of the clients is non-uniform, i.e., some clients have less frequent energy arrivals than others.</p><p>A naive approach for client scheduling is to schedule clients as soon as they have collected enough energy to participate in training. However, as we will demonstrate in our experiments, doing so can bias the trained model towards clients with better (more frequent) energy availability. Another approach is to wait until all clients become available for training, and then use a conventional client sampling algorithm. However, waiting for all clients to have enough energy can significantly increase the total training time needed to achieve a target performance level.</p><p>Instead, we propose a simple client scheduling protocol that can be performed locally by the clients. In our protocol, clients participate in training through a stochastic process based on their energy profile. The details of this process is as follows. First, we note that it takes E i global rounds for client i to harvest enough energy to participate in one global round of training<ref type="foot" target="#foot_0">foot_0</ref> . When t mod (E i T ) = 0, client i samples an integer J uniformly at random from the set {0, . . . , E i -1}. Then, within the E i global rounds starting at the time instances {t, t + T, . . . , t + (E i -1)T }, client i only participates during the global round that starts at t + JT , and does not participate in the remaining global rounds.</p><p>We note that the client selection algorithm decides whether or not a client will participate at a given global round. If a client chooses to participate at a global round starting at some t ∈ T , then the client participates for the whole duration of that global round, i.e., for {t, . . . , t + T -1}, by computing the local model as in <ref type="bibr" target="#b6">(7)</ref>. As such, for any global round starting at t ∈ T , the set of participating clients S t at time t, . . . , t + T -1 satisfy,</p><formula xml:id="formula_16">S t = S t+1 = . . . = S t+T -1 ,<label>(11)</label></formula><p>i.e., the set of clients participating in a given global round stays the same throughout the duration of that global round.</p><p>As we demonstrate in our theoretical analysis, the proposed client scheduling strategy provides provable convergence guarantees for the global model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Local training</head><p>At the beginning of each global round, the server sends the current state of the global model to the clients. The clients then locally update the global model on their local datasets.</p><p>The details of this local update process is as follows. Consider a global round starting at some t ∈ T . Then, the server sends the current state of the global model w (t) to the clients. Then, clients choose whether or not to participate in the current global round, based on the client scheduling process from Section III-A. Clients who choose to participate in the current global round then compute a local model, by updating the global model w (t) through T local SGD iterations as in <ref type="bibr" target="#b6">(7)</ref>.</p><p>After T local SGD iterations, client i ∈ S t+T -1 sends a local update to the server. The local update is defined as,</p><formula xml:id="formula_17">g (t+T ) i E i (w (t+T ) i -w (t) )<label>(12)</label></formula><p>which is obtained by shifting w (t+E) i by w (t) and then scaling it with respect to the energy renewal cycle E i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Global model update</head><p>After receiving the local updates in <ref type="bibr" target="#b11">(12)</ref> from the participating clients, the server updates the global model as,</p><formula xml:id="formula_18">w (t+T ) = w (t) + i∈S t+T -1 p i g (t+T ) i (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>and sends the updated model w (t+T ) back to the clients, for the next iteration.</p><p>We note that the complexity of Algorithm 1 is the same as that of conventional federated learning, i.e., the federated averaging algorithm (FedAvg) from <ref type="bibr" target="#b6">[7]</ref>. In the following, we demonstrate the theoretical convergence guarantees of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Convergence Analysis</head><p>In this section, we provide the convergence guarantees of our framework. First, we review a few common technical assumptions <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b35">[36]</ref> that will be useful in our further analysis. Assumption 1. (Strong-convexity) The local loss functions F i (w) for i ∈ [N ] are µ-strongly convex:</p><formula xml:id="formula_20">F i (v) ≥ F i (w) + F i (w) T (v -w) + µ 2 ||w -v|| 2<label>(14)</label></formula><p>Assumption 2. (Smoothness) The local loss functions F i (w) for i ∈ [N ] are L-smooth:</p><formula xml:id="formula_21">F i (v) ≤ F i (w) + F i (w) T (v -w) + L 2 ||w -v|| 2<label>(15)</label></formula><p>Assumption 3. (Variance bound) The stochastic gradient ∇F i (w</p><formula xml:id="formula_22">(t+j) i , ξ (t+j) i ) has bounded variance for all i ∈ [N ] E[||∇F i (w (t+j) i , ξ (t+j) i ) -∇F i (w (t) )|| 2 ] ≤ σ 2 . (<label>16</label></formula><formula xml:id="formula_23">)</formula><p>Assumption 4. (Bounded second moment) The stochastic gradient ∇F i (w</p><formula xml:id="formula_24">(t+j) i , ξ<label>(t+j) i</label></formula><p>) has bounded expected squared norm for all i ∈ [N ],</p><p>E[||∇F i (w</p><formula xml:id="formula_25">(t+j) i , ξ (t+j) i )|| 2 ] ≤ G 2 . (<label>17</label></formula><formula xml:id="formula_26">)</formula><p>Our convergence analysis is simple and follows along the lines of standard convergence analysis techniques for distributed SGD with local averaging <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p><p>We first represent the model update process in an equivalent but more tractable form. Note that in our original problem formulation, at any global round, only the clients that choose to participate in that global round perform the local update. To make the mathematical analysis simpler, one can instead assume that all clients perform local training at each global round, but the global model is updated by using only the local updates from the clients that were originally scheduled at that global round. Note that, mathematically, the two processes lead to the same global model. Hence, we will use the latter approach in the following, and represent the model update process from Algorithm 1 as,</p><formula xml:id="formula_27">v (t+1) k = w (t) k -η t ∇F k (w (t) k , ξ (t) k )<label>(18)</label></formula><p>w</p><formula xml:id="formula_28">(t+1) k = v (t+1) k t+1 / ∈ T w (t+1-T ) k + i∈St p i E i (v (t+1) i -w (t+1-T ) i ) t+1 ∈ T (<label>19</label></formula><formula xml:id="formula_29">)</formula><formula xml:id="formula_30">for t ∈ [K] and k ∈ [N ]. Note that for t + 1 ∈ T , w (t+1-T ) k = w (t+1-T )<label>(20)</label></formula><p>for all k ∈ [N ], since t + 1 -T ∈ T whenever t + 1 ∈ T .</p><p>We next define two virtual sequences that will be useful in our analysis:</p><formula xml:id="formula_31">w (t+1) = k∈[N ] p k w (t+1) k<label>(21)</label></formula><p>and</p><formula xml:id="formula_32">v (t+1) = k∈[N ] p k v<label>(t+1) k . (22)</label></formula><p>We next provide a key lemma.</p><p>Lemma 1. (Unbiased client scheduling) For all t ∈ T ,</p><formula xml:id="formula_33">E w (t+1) = v (t+1) ,<label>(23)</label></formula><p>hence, the client scheduling process from Section III-A is unbiased.</p><p>Proof. Define a binary random variable α t i such that:</p><formula xml:id="formula_34">α t i = 1 if client i participates at iteration t 0 otherwise<label>(24)</label></formula><p>According to the client scheduling algorithm from Section III-A, at t ∈ T , client i participates in one of the E i consecutive global rounds uniformly at random. Hence, among the global rounds starting at {t, t+T, . . . , t+(E i -1)T }, the probability of participating at a specific round is 1 Ei , from which we have,</p><formula xml:id="formula_35">P [α t i = 1] = 1 E i .<label>(25)</label></formula><p>Then, by defining α t (α t 1 , . . . , α t N ), we find that,</p><formula xml:id="formula_36">E w (t+1) = k∈[N ] p k E w (t+1-T ) k + i∈S (t+1) E i p i (v (t+1) i -w (t+1-T ) i )<label>(26)</label></formula><p>=</p><formula xml:id="formula_37">k∈[N ] p k w (t+1-T ) k + E i∈S (t+1) E i p i (v (t+1) i -w (t+1-T ) i ) (27) = k∈[N ] p k w (t+1-T ) k + E i∈[N ] α t i E i p i (v (t+1) i -w (t+1-T ) i ) (28) = k∈[N ] p k w (t+1-T ) k + i∈[N ] E[α t i E i p i (v (t+1) i -w (t+1-T ) i )]<label>(29)</label></formula><formula xml:id="formula_38">= k∈[N ] p k w (t+1-T ) k + i∈[N ] 1 E i E i p i (v (t+1) i -w (t+1-T ) i ) (30) = w (t+1-T ) + k∈[N ] p k i∈[N ] p i v (t+1) i -w (t+1-T ) (31) = w (t+1-T ) + i∈[N ] p i v (t+1) i -w (t+1-T ) (32) = i∈[N ] p i v (t+1) i = v (t+1)<label>(33)</label></formula><p>where ( <ref type="formula">30</ref>) is from <ref type="bibr" target="#b24">(25)</ref>, and ( <ref type="formula">31</ref>) is from ( <ref type="formula" target="#formula_30">20</ref>) and ( <ref type="formula" target="#formula_3">4</ref>).</p><p>Next, we provide another key lemma.</p><p>Lemma 2. (Bounded variance for w (t+1) ) For all t ∈ T , by assuming a decreasing learning rate η t with η t ≤ 2η t+T for all t ≥ 0, we have</p><formula xml:id="formula_39">E[ v (t+1) -w (t+1) 2 ] ≤ 4E 2 max G 2 η 2 t T 2 , (<label>34</label></formula><formula xml:id="formula_40">) where E max = max i∈[N ] E i .</formula><p>Hence, the aggregate of the local models have bounded variance.</p><p>Proof. From ( <ref type="formula" target="#formula_31">21</ref>) and ( <ref type="formula">22</ref>), we have that,</p><formula xml:id="formula_41">E[ v (t+1) -w (t+1) 2 ] = E[ k∈[N ] p k v (t+1) k A - k∈[N ] p k w (t+1) k B 2 ]<label>(35)</label></formula><p>The first term in <ref type="bibr" target="#b34">(35)</ref> can be written as:</p><formula xml:id="formula_42">A = k∈[N ] p k w (t+1-T ) k - t j=t+1-T η j ∇F k (w (j) k , ξ (j) k )<label>(36)</label></formula><p>= w (t+1-T ) -</p><formula xml:id="formula_43">k∈[N ] t j=t+1-T p k η j ∇F k (w (j) k , ξ (j) k )<label>(37)</label></formula><p>where <ref type="bibr" target="#b35">(36)</ref> follows from <ref type="bibr" target="#b17">(18)</ref>, and (37) is from ( <ref type="formula" target="#formula_3">4</ref>) and <ref type="bibr" target="#b19">(20)</ref>. Similarly, the second term in <ref type="bibr" target="#b34">(35)</ref> can be written as:</p><formula xml:id="formula_44">B = k∈[N ] p k w (t+1-T ) k + i∈St E i p i (v (t+1) i -w (t+1-T ) i ) (38) = w (t+1-T ) + i∈St E i p i (w (t+1-T ) - t j=t+1-T η j ∇F i (w (j) i , ξ (j) i ) -w (t+1-T ) ) (39) = w (t+1-T ) - i∈St E i p i t j=t+1-T η j ∇F i (w (j) i , ξ (j) i )<label>(40)</label></formula><p>where <ref type="bibr" target="#b37">(38)</ref> follows from <ref type="bibr" target="#b18">(19)</ref>, <ref type="bibr" target="#b19">(20)</ref>, and (4), whereas (39) is from <ref type="bibr" target="#b17">(18)</ref>. By combining (40), <ref type="bibr" target="#b36">(37)</ref>, and (35), we have,</p><formula xml:id="formula_45">E[ k∈[N ] p k v (t+1) k - k∈[N ] p k w (t+1) k 2 ] = E[ - k∈[N ] t j=t+1-T p k η j ∇F k (w (j) k , ξ (j) k ) + i∈St E i p i t j=t+1-T η j ∇F i (w (j) i , ξ (j) i ) 2 ] (41) ≤ E[ i∈St E i p i t j=t+1-T η j ∇F i (w (j) i , ξ (j) i ) 2 ]<label>(42)</label></formula><p>where (42</p><formula xml:id="formula_46">) holds from E[(X -E[X]) 2 ] ≤ E[X 2 ] and that, E[ i∈St E i p i t j=t+1-T η j ∇F i (w (j) i , ξ (j) i )] = E[ i∈[N ] α t i E i p i t j=t+1-T η j ∇F i (w (j) i , ξ (j) i )] (43) = i∈[N ] 1 E i E i p i t j=t+1-T η j ∇F i (w (j) i , ξ (j) i ) (44) = k∈[N ] t j=t+1-T p k η j ∇F k (w (j) k , ξ (j) k )<label>(45)</label></formula><p>by defining α t (α t 1 , . . . , α t N ) as in <ref type="bibr" target="#b23">(24)</ref>. Finally, by letting r j i ∇F i (w</p><formula xml:id="formula_47">(j) i , ξ (j) i ), E[ i∈St E i p i t j=t+1-T η j ∇F i (w (j) i , ξ (j) i ) 2 ] = E[ i∈St i ∈St E i E i p i p i t j=t+1-T η j r j i , t j =t+1-T η j r j i ]<label>(46)</label></formula><p>≤ E[</p><formula xml:id="formula_48">i∈St i ∈St E i E i p i p i t j=t+1-T t j =t+1-T η j η j G 2 ] (47) ≤ E[ i∈[N ] i ∈[N ] α i α i E i E i p i p i t j=t+1-T t j =t+1-T η j η j G 2 ] ≤ i∈[N ] i ∈[N ] E[α i α i ]E i E i p i p i t j=t+1-T t j =t+1-T η j η j G 2 ≤ E 2 max T 2 η 2 t+1-T G 2 ≤ 4E 2 max T 2 η 2 t G 2<label>(48)</label></formula><p>where (47) follows from <ref type="bibr" target="#b16">(17)</ref> and that,</p><formula xml:id="formula_49">t j=t+1-T η j r j i , t j =t+1-T η j r j i = t j=t+1-T t j =t+1-T η j η j r j i , r j i (49) ≤ t j=t+1-T t j =t+1-T η j η j r j i r j i (50) ≤ t j=t+1-T t j =t+1-T η j η j 1 2 ( r j i 2 + r j i 2 )<label>(51)</label></formula><p>where ( <ref type="formula">50</ref>) is from the Cauchy-Schwarz inequality, and (51) is from the AM-GM (arithmetic mean-geometric mean) inequality. Equation (48) follows from using a decreasing learning rate η t with t and η t ≤ 2η t+T . In equation (48), we define</p><formula xml:id="formula_50">E max max i∈[N ] E i .</formula><p>We next define the degree of heterogeneity between the clients as in <ref type="bibr" target="#b25">[26]</ref>,</p><formula xml:id="formula_51">Γ = F * - i∈[N ] p i F * i (<label>52</label></formula><formula xml:id="formula_52">)</formula><p>where F * and F * i denote the minimum of the global and local loss functions from ( <ref type="formula" target="#formula_0">1</ref>) and (2), respectively. We are now ready to state our convergence guarantees. Theorem 1. For the federated learning problem from (1) over N clients and an energy renewal cycle</p><formula xml:id="formula_53">E i for client i ∈ [N ], Algorithm 1 converges, E[F (w (T ) )] -F (w * ) ≤ 2κ γ + K B + C µ + 2L w (0) -w * 2<label>(53)</label></formula><p>in K iterations, where w * denotes the optimal model parameters that minimize the global loss function in (1), and</p><formula xml:id="formula_54">C 4E 2 max T 2 η 2 t G 2 ,<label>(54)</label></formula><p>where</p><formula xml:id="formula_55">E max max i∈[N ] E i , κ = L µ , γ = max{8κ, T }, learning rate η t = 2 µ(γ+t) , and B = σ 2 6LΓ + 8(T -1) 2 G 2 .</formula><p>Proof. The proof follows directly from Lemmas 1 and 2 along with standard steps in the convergence analysis of distributed SGD with local averaging <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b35">[36]</ref>, e.g., from Section B.3 of <ref type="bibr" target="#b25">[26]</ref> by replacing Lemmas 4 and 5 from <ref type="bibr" target="#b25">[26]</ref> with Lemmas 1 and 2 from our work, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Experiments</head><p>We now demonstrate the convergence properties of Algorithm 1 compared to federated learning benchmarks that are agnostic to the energy availability of the clients.</p><p>Network architecture. We consider an image classification task with 10 classes on the CIFAR-10 dataset <ref type="bibr" target="#b36">[37]</ref>.</p><p>Training is done using a convolutional neural network with the same architecture from <ref type="bibr" target="#b6">[7]</ref> (about 10 6 parameters).</p><p>Experiment Setup. We consider a network of N = 40 clients. The dataset is distributed in an i.i.d. fashion across the network, by shuffling the dataset and distributing it evenly across the clients. Clients use the ADAM optimizer <ref type="bibr" target="#b37">[38]</ref> during training, and the number of local training iterations is set to T = 5.</p><p>Energy Profile. In order to show the impact of non-homogeneous energy arrivals, clients are partitioned into 4 groups U 0 , . . . , U 3 of equal size, such that U k = {i : i mod 4 = k}. Then, the energy arrivals of clients in group U k are assigned as E i = τ k for all i ∈ U, where (τ 0 , τ 1 , τ 2 , τ 3 ) = <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20)</ref>. In other words, clients in group U 0 receive energy at every global round, whereas clients in groups U 1 , U 2 , and U 3 receive energy at every 5, 10, and 20 global rounds, respectively.</p><p>Benchmarks. To evaluate the performance of Algorithm 1, we consider the conventional federated learning algorithm from Section II-A (known as FedAvg <ref type="bibr" target="#b6">[7]</ref>), but under the constraint that users receive energy according to the energy arrival process in Section II-B, and implement two benchmarks with respect to the specific client scheduling policy.</p><p>Benchmark 1: In the first benchmark, each client participates in training as soon as they have enough energy, and then waits until the next energy arrival. More specifically, as soon as t mod T E i = 0, the client participates in the current global round, by updating the current state of the global model through T local training iterations as in <ref type="bibr" target="#b6">(7)</ref>, and then sending the local update to the server. The server then updates the global model according to <ref type="bibr" target="#b8">(9)</ref>. The client does not participate in training in the next E i -1 global rounds, until the next energy arrival.</p><p>Benchmark 2: In the second benchmark, the global model is updated only when all clients have received energy, i.e., the server waits until all clients have energy available before initiating a global update. After all clients have received energy, the server sends the current state of the global model to the clients, the clients compute a local model as in <ref type="bibr" target="#b6">(7)</ref>, and then the server aggregates the local models to update the global model as in <ref type="bibr" target="#b8">(9)</ref>. Note that in this case, the server needs to wait for the slowest client, hence the global model is updated once in the duration of 20 global rounds.</p><p>We evaluate the training performance in terms of the test accuracy with respect to the number of global rounds.</p><p>Our results are given in Figure <ref type="figure" target="#fig_0">1</ref>. In our experiments, we also implement the original federated learning algorithm (FedAvg) from Section II-A without any resource limitations, which acts as an upper bound on the accuracy. We observe that Algorithm 1 achieves an accuracy of 77%, which is comparable to the accuracy of FedAvg, whereas the accuracy of the two benchmarks are 60% and 62%, respectively, within 1000 global rounds. This is caused by the fact that, in the first benchmark, the training algorithm favors clients with more frequent energy availability, which causes the global model to be biased. In the second benchmark, the server waits until all clients have energy available before each global update, which causes the convergence to be very slow even though the algorithm is unbiased. On the other hand, Algorithm 1 converges fast and significantly outperforms the benchmarks in terms of test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Conclusion</head><p>This paper proposes sustainable federated learning with the utilization of intermittently powered devices, where a large number of remote devices are expected to perform training on a daily basis. We demonstrate a simple and scalable federated learning strategy with provable convergence guarantees, for devices with intermittent energy availability, and show that the proposed framework can significantly improve the training performance compared to the energy-agnostic benchmarks. We hope our work to open up further research on sustainable learning in large-scale federated and decentralized settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Test accuracy of Algorithm 1 compared to federated learning benchmarks for N = 40 clients on the CIFAR-10 dataset.</figDesc><graphic coords="10,163.86,75.59,280.82,192.60" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For simplicity, we assume that when t = 0, all clients have enough energy to participate in one global round. Our results hold even if clients start at different time instances.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Green ai</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-07">Jul 2019</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Energy harvesting: an integrated view of materials, devices and applications</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Radousky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page">502001</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On enabling sustainable edge computing with renewable energy resources</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Delicato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="101" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From insight to impact: Building a sustainable edge computing platform for smart homes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 24th International Conference on Parallel and Distributed Systems (ICPADS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Communication-Efficient Learning of Deep Networks from Decentralized Data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Artificial Intelligence and Statistics</title>
		<meeting><address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">Apr 2017</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics. PMLR</title>
		<imprint>
			<biblScope unit="page" from="1273" to="1282" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards federated learning at scale: System design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grieskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ingerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konecny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>nd SysML Conf.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Advances and open problems in federated learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cummings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04977</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Federated learning: Challenges, methods, and future directions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Applied federated learning: Improving Google keyboard query suggestions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02903</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtarik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems: Workshop on Private Multi-Party Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Federated learning with autotuned communicationefficient secure aggregation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gruteser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00131</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fair resource allocation in federated learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10497</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Analyzing federated learning through an adversarial lens</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12470</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Differentially private federated learning: A client level perspective</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07557</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Electrical load forecasting using edge computing and federated learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taïk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cherkaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICC 2020-2020 IEEE International Conference on Communications (ICC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Energy demand prediction with federated learning for electric vehicle networks</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Saputra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dutkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srikanteswara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Global Communications Conference (GLOBECOM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Privacy-preserving traffic flow prediction: A federated learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="7751" to="7763" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Federated learning for healthcare informatics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Glicksberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Healthcare Informatics Research</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Federated machine learning: Concept and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fedhealth: A federated transfer learning framework for wearable healthcare</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="83" to="93" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning context-aware policies from multiple smart homes via federated multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="115" />
		</imprint>
	</monogr>
	<note>IoTDI</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Energy-efficient radio resource allocation for federated edge learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Communications Workshops (ICC Workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive federated learning in resource constrained edge computing systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salonidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Makaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1205" to="1221" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the convergence of FedAvg on Non-IID data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">cpSGD: Communication-efficient and differentially-private distributed SGD</title>
		<author>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (Neurips 2018)</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7575" to="7586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Optimal client sampling for federated learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtarik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13723</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Active federated learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12641</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Client selection in federated learning: Convergence analysis and power-of-choice selection strategies</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01243</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Practical secure aggregation for privacy-preserving machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kreuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcedone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1175" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Energy harvesting from hybrid indoor ambient light and thermal energy sources for enhanced performance of wireless sensor nodes</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4424" to="4435" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A wireless sensing platform utilizing ambient RF energy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Parks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Sample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Topical Conference on Biomedical Wireless Technologies, Networks, and Sensing Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="154" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ambient rf energy-harvesting technologies for self-sustainable standalone wireless sensor platforms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Niotaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Georgiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Tentzeris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1649" to="1666" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Movers and shakers: Kinetic energy harvesting for the internet of things</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gorlatova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sarik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grebla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kymissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zussman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1624" to="1639" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Local sgd converges fast and communicates little</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09767</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
