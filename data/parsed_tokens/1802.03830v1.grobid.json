{
  "title": [
    {
      "sentence": "Distributed Stochastic Multi-Task Learning with Graph Regularization",
      "tokens": [
        "Distributed",
        "Stochastic",
        "Multi-Task",
        "Learning",
        "with",
        "Graph",
        "Regularization"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "We propose methods for distributed graph-based multi-task learning that are based on weighted averaging of messages from other machines.",
      "tokens": [
        "We",
        "propose",
        "methods",
        "for",
        "distributed",
        "graph-based",
        "multi-task",
        "learning",
        "that",
        "are",
        "based",
        "on",
        "weighted",
        "averaging",
        "of",
        "messages",
        "from",
        "other",
        "machines",
        "."
      ]
    },
    {
      "sentence": "Uniform averaging or diminishing stepsize in these methods would yield consensus (single task) learning.",
      "tokens": [
        "Uniform",
        "averaging",
        "or",
        "diminishing",
        "stepsize",
        "in",
        "these",
        "methods",
        "would",
        "yield",
        "consensus",
        "(",
        "single",
        "task",
        ")",
        "learning",
        "."
      ]
    },
    {
      "sentence": "We show how simply skewing the averaging weights or controlling the stepsize allows learning different, but related, tasks on the different machines.",
      "tokens": [
        "We",
        "show",
        "how",
        "simply",
        "skewing",
        "the",
        "averaging",
        "weights",
        "or",
        "controlling",
        "the",
        "stepsize",
        "allows",
        "learning",
        "different",
        ",",
        "but",
        "related",
        ",",
        "tasks",
        "on",
        "the",
        "different",
        "machines",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Introduction We consider a distributed learning problem in a multi-task setting: each machine i has access to samples from a different data distribution D i , with potentially a different optimal predictor, and thus a different learning task, but where we still assume some similarity between different tasks.",
      "tokens": [
        "Introduction",
        "We",
        "consider",
        "a",
        "distributed",
        "learning",
        "problem",
        "in",
        "a",
        "multi-task",
        "setting",
        ":",
        "each",
        "machine",
        "i",
        "has",
        "access",
        "to",
        "samples",
        "from",
        "a",
        "different",
        "data",
        "distribution",
        "D",
        "i",
        ",",
        "with",
        "potentially",
        "a",
        "different",
        "optimal",
        "predictor",
        ",",
        "and",
        "thus",
        "a",
        "different",
        "learning",
        "task",
        ",",
        "but",
        "where",
        "we",
        "still",
        "assume",
        "some",
        "similarity",
        "between",
        "different",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "The goal of each machine is to find a good predictor for its own task, based on its own local data, as well as communicating with the other machines so as to leverage the similarity to other related tasks.",
      "tokens": [
        "The",
        "goal",
        "of",
        "each",
        "machine",
        "is",
        "to",
        "find",
        "a",
        "good",
        "predictor",
        "for",
        "its",
        "own",
        "task",
        ",",
        "based",
        "on",
        "its",
        "own",
        "local",
        "data",
        ",",
        "as",
        "well",
        "as",
        "communicating",
        "with",
        "the",
        "other",
        "machines",
        "so",
        "as",
        "to",
        "leverage",
        "the",
        "similarity",
        "to",
        "other",
        "related",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "Distributed multi-task learning lies between a homogeneous distributed learning setting (e.g.",
      "tokens": [
        "Distributed",
        "multi-task",
        "learning",
        "lies",
        "between",
        "a",
        "homogeneous",
        "distributed",
        "learning",
        "setting",
        "(",
        "e.g",
        "."
      ]
    },
    {
      "sentence": "Shamir and Srebro, 2014) , where all machines have data from the same source distribution, and inhomogeneous consensus problems (e.g.",
      "tokens": [
        "Shamir",
        "and",
        "Srebro",
        ",",
        "2014",
        ")",
        ",",
        "where",
        "all",
        "machines",
        "have",
        "data",
        "from",
        "the",
        "same",
        "source",
        "distribution",
        ",",
        "and",
        "inhomogeneous",
        "consensus",
        "problems",
        "(",
        "e.g",
        "."
      ]
    },
    {
      "sentence": "Ram et al., 2010; Boyd et al., 2011; Balcan et al., 2012) , where each machine sees data from a different source, but the goal is to reach a single consensus predictor.",
      "tokens": [
        "Ram",
        "et",
        "al.",
        ",",
        "2010",
        ";",
        "Boyd",
        "et",
        "al.",
        ",",
        "2011",
        ";",
        "Balcan",
        "et",
        "al.",
        ",",
        "2012",
        ")",
        ",",
        "where",
        "each",
        "machine",
        "sees",
        "data",
        "from",
        "a",
        "different",
        "source",
        ",",
        "but",
        "the",
        "goal",
        "is",
        "to",
        "reach",
        "a",
        "single",
        "consensus",
        "predictor",
        "."
      ]
    },
    {
      "sentence": "In many distributed learning problems, different machines do indeed see different distributions.",
      "tokens": [
        "In",
        "many",
        "distributed",
        "learning",
        "problems",
        ",",
        "different",
        "machines",
        "do",
        "indeed",
        "see",
        "different",
        "distributions",
        "."
      ]
    },
    {
      "sentence": "For example, machines might serve different geographical regions.",
      "tokens": [
        "For",
        "example",
        ",",
        "machines",
        "might",
        "serve",
        "different",
        "geographical",
        "regions",
        "."
      ]
    },
    {
      "sentence": "In a more extreme \"federated learning\" (Konecny et al., 2015) scenario, each machine is a single user device, and its data distribution might reflect e.g.",
      "tokens": [
        "In",
        "a",
        "more",
        "extreme",
        "``",
        "federated",
        "learning",
        "''",
        "(",
        "Konecny",
        "et",
        "al.",
        ",",
        "2015",
        ")",
        "scenario",
        ",",
        "each",
        "machine",
        "is",
        "a",
        "single",
        "user",
        "device",
        ",",
        "and",
        "its",
        "data",
        "distribution",
        "might",
        "reflect",
        "e.g",
        "."
      ]
    },
    {
      "sentence": "the user's speech, language biases, usage patterns, etc.",
      "tokens": [
        "the",
        "user",
        "'s",
        "speech",
        ",",
        "language",
        "biases",
        ",",
        "usage",
        "patterns",
        ",",
        "etc",
        "."
      ]
    },
    {
      "sentence": "Such heterogeneity requires departing from a homogeneous model.",
      "tokens": [
        "Such",
        "heterogeneity",
        "requires",
        "departing",
        "from",
        "a",
        "homogeneous",
        "model",
        "."
      ]
    },
    {
      "sentence": "But if the data distribution on each machine is different, we might as well learn a personalized predictor for each machine, while still leveraging commonalities as in multi-task learning, instead of insisting on consensus.",
      "tokens": [
        "But",
        "if",
        "the",
        "data",
        "distribution",
        "on",
        "each",
        "machine",
        "is",
        "different",
        ",",
        "we",
        "might",
        "as",
        "well",
        "learn",
        "a",
        "personalized",
        "predictor",
        "for",
        "each",
        "machine",
        ",",
        "while",
        "still",
        "leveraging",
        "commonalities",
        "as",
        "in",
        "multi-task",
        "learning",
        ",",
        "instead",
        "of",
        "insisting",
        "on",
        "consensus",
        "."
      ]
    },
    {
      "sentence": "Unlike when seeking consensus, we could learn a predictor entirely locally, ignoring data on other machines.",
      "tokens": [
        "Unlike",
        "when",
        "seeking",
        "consensus",
        ",",
        "we",
        "could",
        "learn",
        "a",
        "predictor",
        "entirely",
        "locally",
        ",",
        "ignoring",
        "data",
        "on",
        "other",
        "machines",
        "."
      ]
    },
    {
      "sentence": "But the premise of multi-task learning is that by communicating with other machines we can improve our predictions, reduce the sample complexity, and hopefully also reduce the computational cost on each machine by distributing the computation.",
      "tokens": [
        "But",
        "the",
        "premise",
        "of",
        "multi-task",
        "learning",
        "is",
        "that",
        "by",
        "communicating",
        "with",
        "other",
        "machines",
        "we",
        "can",
        "improve",
        "our",
        "predictions",
        ",",
        "reduce",
        "the",
        "sample",
        "complexity",
        ",",
        "and",
        "hopefully",
        "also",
        "reduce",
        "the",
        "computational",
        "cost",
        "on",
        "each",
        "machine",
        "by",
        "distributing",
        "the",
        "computation",
        "."
      ]
    },
    {
      "sentence": "Central to multi-task learning is the notion of relatedness between tasks.",
      "tokens": [
        "Central",
        "to",
        "multi-task",
        "learning",
        "is",
        "the",
        "notion",
        "of",
        "relatedness",
        "between",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "In a high-dimensional setting, with large number of variables, we might expect a small common set of predictive variables, where the form of the dependence on variables in this common set varies between tasks (Turlach et al., 2005; Obozinski et al., 2011; Lounici et al., 2011; Wang et al., 2015) .",
      "tokens": [
        "In",
        "a",
        "high-dimensional",
        "setting",
        ",",
        "with",
        "large",
        "number",
        "of",
        "variables",
        ",",
        "we",
        "might",
        "expect",
        "a",
        "small",
        "common",
        "set",
        "of",
        "predictive",
        "variables",
        ",",
        "where",
        "the",
        "form",
        "of",
        "the",
        "dependence",
        "on",
        "variables",
        "in",
        "this",
        "common",
        "set",
        "varies",
        "between",
        "tasks",
        "(",
        "Turlach",
        "et",
        "al.",
        ",",
        "2005",
        ";",
        "Obozinski",
        "et",
        "al.",
        ",",
        "2011",
        ";",
        "Lounici",
        "et",
        "al.",
        ",",
        "2011",
        ";",
        "Wang",
        "et",
        "al.",
        ",",
        "2015",
        ")",
        "."
      ]
    },
    {
      "sentence": "Another approach is to assume that the predictors lie in a shared lower dimensional subspace (Ando and Zhang, 2005; Yuan et al., 2007; Wang et al., 2016) or all have low-norm under some shared linear representation (Amit et al., 2007; Argyriou et al., 2008) .",
      "tokens": [
        "Another",
        "approach",
        "is",
        "to",
        "assume",
        "that",
        "the",
        "predictors",
        "lie",
        "in",
        "a",
        "shared",
        "lower",
        "dimensional",
        "subspace",
        "(",
        "Ando",
        "and",
        "Zhang",
        ",",
        "2005",
        ";",
        "Yuan",
        "et",
        "al.",
        ",",
        "2007",
        ";",
        "Wang",
        "et",
        "al.",
        ",",
        "2016",
        ")",
        "or",
        "all",
        "have",
        "low-norm",
        "under",
        "some",
        "shared",
        "linear",
        "representation",
        "(",
        "Amit",
        "et",
        "al.",
        ",",
        "2007",
        ";",
        "Argyriou",
        "et",
        "al.",
        ",",
        "2008",
        ")",
        "."
      ]
    },
    {
      "sentence": "Both the shared sparsity and shared subspaces models have recently been considered in a distributed learning setting (Wang et al., 2015 (Wang et al., , 2016)) , and nuclear-norm regularized multi-task learning has been studied from a distributed optimization perspective (Baytas et al., 2016) .",
      "tokens": [
        "Both",
        "the",
        "shared",
        "sparsity",
        "and",
        "shared",
        "subspaces",
        "models",
        "have",
        "recently",
        "been",
        "considered",
        "in",
        "a",
        "distributed",
        "learning",
        "setting",
        "(",
        "Wang",
        "et",
        "al.",
        ",",
        "2015",
        "(",
        "Wang",
        "et",
        "al.",
        ",",
        ",",
        "2016",
        ")",
        ")",
        ",",
        "and",
        "nuclear-norm",
        "regularized",
        "multi-task",
        "learning",
        "has",
        "been",
        "studied",
        "from",
        "a",
        "distributed",
        "optimization",
        "perspective",
        "(",
        "Baytas",
        "et",
        "al.",
        ",",
        "2016",
        ")",
        "."
      ]
    },
    {
      "sentence": "In this paper, we consider graph-based multi-task learning, where relatedness between tasks is specified through a weighted graph over the tasks.",
      "tokens": [
        "In",
        "this",
        "paper",
        ",",
        "we",
        "consider",
        "graph-based",
        "multi-task",
        "learning",
        ",",
        "where",
        "relatedness",
        "between",
        "tasks",
        "is",
        "specified",
        "through",
        "a",
        "weighted",
        "graph",
        "over",
        "the",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "Neighboring tasks in the graph are expected to be similar, with a penalty for dis-similarity specified by the weight between them (see precise formulation in Section 2) (Maurer, 2006; Evgeniou et al., 2005) .",
      "tokens": [
        "Neighboring",
        "tasks",
        "in",
        "the",
        "graph",
        "are",
        "expected",
        "to",
        "be",
        "similar",
        ",",
        "with",
        "a",
        "penalty",
        "for",
        "dis-similarity",
        "specified",
        "by",
        "the",
        "weight",
        "between",
        "them",
        "(",
        "see",
        "precise",
        "formulation",
        "in",
        "Section",
        "2",
        ")",
        "(",
        "Maurer",
        ",",
        "2006",
        ";",
        "Evgeniou",
        "et",
        "al.",
        ",",
        "2005",
        ")",
        "."
      ]
    },
    {
      "sentence": "This also generalized a simpler \"fully connected\" multi-task model where all predictors are close to each other (Evgeniou and Pontil, 2004) .",
      "tokens": [
        "This",
        "also",
        "generalized",
        "a",
        "simpler",
        "``",
        "fully",
        "connected",
        "''",
        "multi-task",
        "model",
        "where",
        "all",
        "predictors",
        "are",
        "close",
        "to",
        "each",
        "other",
        "(",
        "Evgeniou",
        "and",
        "Pontil",
        ",",
        "2004",
        ")",
        "."
      ]
    },
    {
      "sentence": "A predictor-homogeneous assumption can also be viewed as an extreme case where all weights go to infinity, forcing all predictors to be identical.",
      "tokens": [
        "A",
        "predictor-homogeneous",
        "assumption",
        "can",
        "also",
        "be",
        "viewed",
        "as",
        "an",
        "extreme",
        "case",
        "where",
        "all",
        "weights",
        "go",
        "to",
        "infinity",
        ",",
        "forcing",
        "all",
        "predictors",
        "to",
        "be",
        "identical",
        "."
      ]
    },
    {
      "sentence": "In distributed multi-task learning, graph-based relatedness is especially appealing if the relatedness graph also matches the graph of network links between machines, as might be the case, e.g.",
      "tokens": [
        "In",
        "distributed",
        "multi-task",
        "learning",
        ",",
        "graph-based",
        "relatedness",
        "is",
        "especially",
        "appealing",
        "if",
        "the",
        "relatedness",
        "graph",
        "also",
        "matches",
        "the",
        "graph",
        "of",
        "network",
        "links",
        "between",
        "machines",
        ",",
        "as",
        "might",
        "be",
        "the",
        "case",
        ",",
        "e.g",
        "."
      ]
    },
    {
      "sentence": "in a geographical setting or with physical sensors.",
      "tokens": [
        "in",
        "a",
        "geographical",
        "setting",
        "or",
        "with",
        "physical",
        "sensors",
        "."
      ]
    },
    {
      "sentence": "We therefor emphasize and prefer methods with communication only between neighboring tasks on the graph.",
      "tokens": [
        "We",
        "therefor",
        "emphasize",
        "and",
        "prefer",
        "methods",
        "with",
        "communication",
        "only",
        "between",
        "neighboring",
        "tasks",
        "on",
        "the",
        "graph",
        "."
      ]
    },
    {
      "sentence": "In designing methods for graph-based multi-task learning, we are interested in methods that (1) are natural and simple-all our algorithms have a similar and natural structure, involving weighted averaging of messages from neighboring machines and a local gradient or prox calculation; (2) have low communication costs, are sample efficient, and preferably also have low computational cost; and (3) are backed by rigorous guarantees on the amount of communication, samples and computation required.",
      "tokens": [
        "In",
        "designing",
        "methods",
        "for",
        "graph-based",
        "multi-task",
        "learning",
        ",",
        "we",
        "are",
        "interested",
        "in",
        "methods",
        "that",
        "(",
        "1",
        ")",
        "are",
        "natural",
        "and",
        "simple-all",
        "our",
        "algorithms",
        "have",
        "a",
        "similar",
        "and",
        "natural",
        "structure",
        ",",
        "involving",
        "weighted",
        "averaging",
        "of",
        "messages",
        "from",
        "neighboring",
        "machines",
        "and",
        "a",
        "local",
        "gradient",
        "or",
        "prox",
        "calculation",
        ";",
        "(",
        "2",
        ")",
        "have",
        "low",
        "communication",
        "costs",
        ",",
        "are",
        "sample",
        "efficient",
        ",",
        "and",
        "preferably",
        "also",
        "have",
        "low",
        "computational",
        "cost",
        ";",
        "and",
        "(",
        "3",
        ")",
        "are",
        "backed",
        "by",
        "rigorous",
        "guarantees",
        "on",
        "the",
        "amount",
        "of",
        "communication",
        ",",
        "samples",
        "and",
        "computation",
        "required",
        "."
      ]
    },
    {
      "sentence": "Graph-based multi-task learning has been recently studied by Vanhaesebrouck et al.",
      "tokens": [
        "Graph-based",
        "multi-task",
        "learning",
        "has",
        "been",
        "recently",
        "studied",
        "by",
        "Vanhaesebrouck",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "(2017) and Liu et al.",
      "tokens": [
        "(",
        "2017",
        ")",
        "and",
        "Liu",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "(2017) , both considering the problem as distributed optimization of the multitask regularized empirical objective, similar to our approach in Section 3.2).",
      "tokens": [
        "(",
        "2017",
        ")",
        ",",
        "both",
        "considering",
        "the",
        "problem",
        "as",
        "distributed",
        "optimization",
        "of",
        "the",
        "multitask",
        "regularized",
        "empirical",
        "objective",
        ",",
        "similar",
        "to",
        "our",
        "approach",
        "in",
        "Section",
        "3.2",
        ")",
        "."
      ]
    },
    {
      "sentence": "Vanhaesebrouck et al.",
      "tokens": [
        "Vanhaesebrouck",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "suggested an asynchronous gossip-type algorithms and an ADMM procedure, while Liu et al.",
      "tokens": [
        "suggested",
        "an",
        "asynchronous",
        "gossip-type",
        "algorithms",
        "and",
        "an",
        "ADMM",
        "procedure",
        ",",
        "while",
        "Liu",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "proposed using SDCA, and also considered learning the relatedness graph itself.",
      "tokens": [
        "proposed",
        "using",
        "SDCA",
        ",",
        "and",
        "also",
        "considered",
        "learning",
        "the",
        "relatedness",
        "graph",
        "itself",
        "."
      ]
    },
    {
      "sentence": "Neither provides any statistical analysis, nor analysis of the iteration complexity and communication cost based on the methods.",
      "tokens": [
        "Neither",
        "provides",
        "any",
        "statistical",
        "analysis",
        ",",
        "nor",
        "analysis",
        "of",
        "the",
        "iteration",
        "complexity",
        "and",
        "communication",
        "cost",
        "based",
        "on",
        "the",
        "methods",
        "."
      ]
    },
    {
      "sentence": "We conduct detailed comparison of convergence properties with these methods in Appendix H, providing upper bounds of their iteration complexities when possible; our methods have faster convergence than the guarantees we could obtain for them.",
      "tokens": [
        "We",
        "conduct",
        "detailed",
        "comparison",
        "of",
        "convergence",
        "properties",
        "with",
        "these",
        "methods",
        "in",
        "Appendix",
        "H",
        ",",
        "providing",
        "upper",
        "bounds",
        "of",
        "their",
        "iteration",
        "complexities",
        "when",
        "possible",
        ";",
        "our",
        "methods",
        "have",
        "faster",
        "convergence",
        "than",
        "the",
        "guarantees",
        "we",
        "could",
        "obtain",
        "for",
        "them",
        "."
      ]
    },
    {
      "sentence": "Also, neither directly considers the underlying learning problem (minimizing the actual expected errors), and so neither studies stochastic methods (in the flavor of our Section 4).",
      "tokens": [
        "Also",
        ",",
        "neither",
        "directly",
        "considers",
        "the",
        "underlying",
        "learning",
        "problem",
        "(",
        "minimizing",
        "the",
        "actual",
        "expected",
        "errors",
        ")",
        ",",
        "and",
        "so",
        "neither",
        "studies",
        "stochastic",
        "methods",
        "(",
        "in",
        "the",
        "flavor",
        "of",
        "our",
        "Section",
        "4",
        ")",
        "."
      ]
    },
    {
      "sentence": "Here, we show how methods that arise naturally by skewing averaging weights or controlling stepsize of consensus learning methods do yield good guarantees.",
      "tokens": [
        "Here",
        ",",
        "we",
        "show",
        "how",
        "methods",
        "that",
        "arise",
        "naturally",
        "by",
        "skewing",
        "averaging",
        "weights",
        "or",
        "controlling",
        "stepsize",
        "of",
        "consensus",
        "learning",
        "methods",
        "do",
        "yield",
        "good",
        "guarantees",
        "."
      ]
    },
    {
      "sentence": "We also propose stochastic methods which allow reducing the computational cost, and we compare the empirical performance of both our batch and stochastic methods to those of Vanhaesebrouck et al.",
      "tokens": [
        "We",
        "also",
        "propose",
        "stochastic",
        "methods",
        "which",
        "allow",
        "reducing",
        "the",
        "computational",
        "cost",
        ",",
        "and",
        "we",
        "compare",
        "the",
        "empirical",
        "performance",
        "of",
        "both",
        "our",
        "batch",
        "and",
        "stochastic",
        "methods",
        "to",
        "those",
        "of",
        "Vanhaesebrouck",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "(2017) and Ma et al.",
      "tokens": [
        "(",
        "2017",
        ")",
        "and",
        "Ma",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "(2015) .",
      "tokens": [
        "(",
        "2015",
        ")",
        "."
      ]
    },
    {
      "sentence": "Notations In this paper, boldface lower-case letters denote column vectors, boldface capital letters denote matrices, vec(U) is the vectorial form of a matrix U which concatenates columns of U, and U⊗V is the Kronecker product between two matrices U and V. Furthermore, u, v = u v denotes the inner product of two vectors u and v, while U, V = tr U V denotes inner product of two matrices U and V of the same dimensions.",
      "tokens": [
        "Notations",
        "In",
        "this",
        "paper",
        ",",
        "boldface",
        "lower-case",
        "letters",
        "denote",
        "column",
        "vectors",
        ",",
        "boldface",
        "capital",
        "letters",
        "denote",
        "matrices",
        ",",
        "vec",
        "(",
        "U",
        ")",
        "is",
        "the",
        "vectorial",
        "form",
        "of",
        "a",
        "matrix",
        "U",
        "which",
        "concatenates",
        "columns",
        "of",
        "U",
        ",",
        "and",
        "U⊗V",
        "is",
        "the",
        "Kronecker",
        "product",
        "between",
        "two",
        "matrices",
        "U",
        "and",
        "V.",
        "Furthermore",
        ",",
        "u",
        ",",
        "v",
        "=",
        "u",
        "v",
        "denotes",
        "the",
        "inner",
        "product",
        "of",
        "two",
        "vectors",
        "u",
        "and",
        "v",
        ",",
        "while",
        "U",
        ",",
        "V",
        "=",
        "tr",
        "U",
        "V",
        "denotes",
        "inner",
        "product",
        "of",
        "two",
        "matrices",
        "U",
        "and",
        "V",
        "of",
        "the",
        "same",
        "dimensions",
        "."
      ]
    },
    {
      "sentence": "We use u = u, u to denote the length of a vector u, U F = vec(U) the Frobenius norm of a matrix U, and U M = tr (UMU ) = UM, U the norm of U with respect to some positive definite matrix M. A function f (x) is Lipschitz if |f (x) -f (y)| ≤ L x -y , ∀x, y.",
      "tokens": [
        "We",
        "use",
        "u",
        "=",
        "u",
        ",",
        "u",
        "to",
        "denote",
        "the",
        "length",
        "of",
        "a",
        "vector",
        "u",
        ",",
        "U",
        "F",
        "=",
        "vec",
        "(",
        "U",
        ")",
        "the",
        "Frobenius",
        "norm",
        "of",
        "a",
        "matrix",
        "U",
        ",",
        "and",
        "U",
        "M",
        "=",
        "tr",
        "(",
        "UMU",
        ")",
        "=",
        "UM",
        ",",
        "U",
        "the",
        "norm",
        "of",
        "U",
        "with",
        "respect",
        "to",
        "some",
        "positive",
        "definite",
        "matrix",
        "M.",
        "A",
        "function",
        "f",
        "(",
        "x",
        ")",
        "is",
        "Lipschitz",
        "if",
        "|f",
        "(",
        "x",
        ")",
        "-f",
        "(",
        "y",
        ")",
        "|",
        "≤",
        "L",
        "x",
        "-y",
        ",",
        "∀x",
        ",",
        "y",
        "."
      ]
    },
    {
      "sentence": "A convex function f (x) is β-smooth and µ-strongly convex if µ 2 x -y 2 ≤ f (x) -f (y) -∇f (y), x -y ≤ β 2 x -y 2 , ∀x, y.",
      "tokens": [
        "A",
        "convex",
        "function",
        "f",
        "(",
        "x",
        ")",
        "is",
        "β-smooth",
        "and",
        "µ-strongly",
        "convex",
        "if",
        "µ",
        "2",
        "x",
        "-y",
        "2",
        "≤",
        "f",
        "(",
        "x",
        ")",
        "-f",
        "(",
        "y",
        ")",
        "-∇f",
        "(",
        "y",
        ")",
        ",",
        "x",
        "-y",
        "≤",
        "β",
        "2",
        "x",
        "-y",
        "2",
        ",",
        "∀x",
        ",",
        "y",
        "."
      ]
    },
    {
      "sentence": "This definition extends to functions of matrices, by replacing the vector norm with the Frobenius norm in the above inequality.",
      "tokens": [
        "This",
        "definition",
        "extends",
        "to",
        "functions",
        "of",
        "matrices",
        ",",
        "by",
        "replacing",
        "the",
        "vector",
        "norm",
        "with",
        "the",
        "Frobenius",
        "norm",
        "in",
        "the",
        "above",
        "inequality",
        "."
      ]
    }
  ]
}