{
  "title": [
    {
      "sentence": "Meta-Learning: A Survey",
      "tokens": [
        "Meta-Learning",
        ":",
        "A",
        "Survey"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible.",
      "tokens": [
        "Meta-learning",
        ",",
        "or",
        "learning",
        "to",
        "learn",
        ",",
        "is",
        "the",
        "science",
        "of",
        "systematically",
        "observing",
        "how",
        "different",
        "machine",
        "learning",
        "approaches",
        "perform",
        "on",
        "a",
        "wide",
        "range",
        "of",
        "learning",
        "tasks",
        ",",
        "and",
        "then",
        "learning",
        "from",
        "this",
        "experience",
        ",",
        "or",
        "meta-data",
        ",",
        "to",
        "learn",
        "new",
        "tasks",
        "much",
        "faster",
        "than",
        "otherwise",
        "possible",
        "."
      ]
    },
    {
      "sentence": "Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way.",
      "tokens": [
        "Not",
        "only",
        "does",
        "this",
        "dramatically",
        "speed",
        "up",
        "and",
        "improve",
        "the",
        "design",
        "of",
        "machine",
        "learning",
        "pipelines",
        "or",
        "neural",
        "architectures",
        ",",
        "it",
        "also",
        "allows",
        "us",
        "to",
        "replace",
        "hand-engineered",
        "algorithms",
        "with",
        "novel",
        "approaches",
        "learned",
        "in",
        "a",
        "data-driven",
        "way",
        "."
      ]
    },
    {
      "sentence": "In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.",
      "tokens": [
        "In",
        "this",
        "chapter",
        ",",
        "we",
        "provide",
        "an",
        "overview",
        "of",
        "the",
        "state",
        "of",
        "the",
        "art",
        "in",
        "this",
        "fascinating",
        "and",
        "continuously",
        "evolving",
        "field",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Introduction When we learn new skills, we rarely -if ever -start from scratch.",
      "tokens": [
        "Introduction",
        "When",
        "we",
        "learn",
        "new",
        "skills",
        ",",
        "we",
        "rarely",
        "-if",
        "ever",
        "-start",
        "from",
        "scratch",
        "."
      ]
    },
    {
      "sentence": "We start from skills learned earlier in related tasks, reuse approaches that worked well before, and focus on what is likely worth trying based on experience (Lake et al., 2017) .",
      "tokens": [
        "We",
        "start",
        "from",
        "skills",
        "learned",
        "earlier",
        "in",
        "related",
        "tasks",
        ",",
        "reuse",
        "approaches",
        "that",
        "worked",
        "well",
        "before",
        ",",
        "and",
        "focus",
        "on",
        "what",
        "is",
        "likely",
        "worth",
        "trying",
        "based",
        "on",
        "experience",
        "(",
        "Lake",
        "et",
        "al.",
        ",",
        "2017",
        ")",
        "."
      ]
    },
    {
      "sentence": "With every skill learned, learning new skills becomes easier, requiring fewer examples and less trial-and-error.",
      "tokens": [
        "With",
        "every",
        "skill",
        "learned",
        ",",
        "learning",
        "new",
        "skills",
        "becomes",
        "easier",
        ",",
        "requiring",
        "fewer",
        "examples",
        "and",
        "less",
        "trial-and-error",
        "."
      ]
    },
    {
      "sentence": "In short, we learn how to learn across tasks.",
      "tokens": [
        "In",
        "short",
        ",",
        "we",
        "learn",
        "how",
        "to",
        "learn",
        "across",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "Likewise, when building machine learning models for a specific task, we often build on experience with related tasks, or use our (often implicit) understanding of the behavior of machine learning techniques to help make the right choices.",
      "tokens": [
        "Likewise",
        ",",
        "when",
        "building",
        "machine",
        "learning",
        "models",
        "for",
        "a",
        "specific",
        "task",
        ",",
        "we",
        "often",
        "build",
        "on",
        "experience",
        "with",
        "related",
        "tasks",
        ",",
        "or",
        "use",
        "our",
        "(",
        "often",
        "implicit",
        ")",
        "understanding",
        "of",
        "the",
        "behavior",
        "of",
        "machine",
        "learning",
        "techniques",
        "to",
        "help",
        "make",
        "the",
        "right",
        "choices",
        "."
      ]
    },
    {
      "sentence": "The challenge in meta-learning is to learn from prior experience in a systematic, datadriven way.",
      "tokens": [
        "The",
        "challenge",
        "in",
        "meta-learning",
        "is",
        "to",
        "learn",
        "from",
        "prior",
        "experience",
        "in",
        "a",
        "systematic",
        ",",
        "datadriven",
        "way",
        "."
      ]
    },
    {
      "sentence": "First, we need to collect meta-data that describe prior learning tasks and previously learned models.",
      "tokens": [
        "First",
        ",",
        "we",
        "need",
        "to",
        "collect",
        "meta-data",
        "that",
        "describe",
        "prior",
        "learning",
        "tasks",
        "and",
        "previously",
        "learned",
        "models",
        "."
      ]
    },
    {
      "sentence": "They comprise the exact algorithm configurations used to train the models, including hyperparameter settings, pipeline compositions and/or network architectures, the resulting model evaluations, such as accuracy and training time, the learned model parameters, such as the trained weights of a neural net, as well as measurable properties of the task itself, also known as meta-features.",
      "tokens": [
        "They",
        "comprise",
        "the",
        "exact",
        "algorithm",
        "configurations",
        "used",
        "to",
        "train",
        "the",
        "models",
        ",",
        "including",
        "hyperparameter",
        "settings",
        ",",
        "pipeline",
        "compositions",
        "and/or",
        "network",
        "architectures",
        ",",
        "the",
        "resulting",
        "model",
        "evaluations",
        ",",
        "such",
        "as",
        "accuracy",
        "and",
        "training",
        "time",
        ",",
        "the",
        "learned",
        "model",
        "parameters",
        ",",
        "such",
        "as",
        "the",
        "trained",
        "weights",
        "of",
        "a",
        "neural",
        "net",
        ",",
        "as",
        "well",
        "as",
        "measurable",
        "properties",
        "of",
        "the",
        "task",
        "itself",
        ",",
        "also",
        "known",
        "as",
        "meta-features",
        "."
      ]
    },
    {
      "sentence": "Second, we need to learn from this prior meta-data, to extract and transfer knowledge that guides the search for optimal models for new tasks.",
      "tokens": [
        "Second",
        ",",
        "we",
        "need",
        "to",
        "learn",
        "from",
        "this",
        "prior",
        "meta-data",
        ",",
        "to",
        "extract",
        "and",
        "transfer",
        "knowledge",
        "that",
        "guides",
        "the",
        "search",
        "for",
        "optimal",
        "models",
        "for",
        "new",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "This chapter presents a concise overview of different meta-learning approaches to do this effectively.",
      "tokens": [
        "This",
        "chapter",
        "presents",
        "a",
        "concise",
        "overview",
        "of",
        "different",
        "meta-learning",
        "approaches",
        "to",
        "do",
        "this",
        "effectively",
        "."
      ]
    },
    {
      "sentence": "The term meta-learning covers any type of learning based on prior experience with other tasks.",
      "tokens": [
        "The",
        "term",
        "meta-learning",
        "covers",
        "any",
        "type",
        "of",
        "learning",
        "based",
        "on",
        "prior",
        "experience",
        "with",
        "other",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "The more similar those previous tasks are, the more types of meta-data we can leverage, and defining task similarity will be a key overarching challenge.",
      "tokens": [
        "The",
        "more",
        "similar",
        "those",
        "previous",
        "tasks",
        "are",
        ",",
        "the",
        "more",
        "types",
        "of",
        "meta-data",
        "we",
        "can",
        "leverage",
        ",",
        "and",
        "defining",
        "task",
        "similarity",
        "will",
        "be",
        "a",
        "key",
        "overarching",
        "challenge",
        "."
      ]
    },
    {
      "sentence": "Perhaps needless to say, there is no free lunch (Wolpert and Macready, 1996; Giraud-Carrier and Provost, 2005) .",
      "tokens": [
        "Perhaps",
        "needless",
        "to",
        "say",
        ",",
        "there",
        "is",
        "no",
        "free",
        "lunch",
        "(",
        "Wolpert",
        "and",
        "Macready",
        ",",
        "1996",
        ";",
        "Giraud-Carrier",
        "and",
        "Provost",
        ",",
        "2005",
        ")",
        "."
      ]
    },
    {
      "sentence": "When a new task represents completely unrelated phenomena, or random noise, leveraging prior experience will not be effective.",
      "tokens": [
        "When",
        "a",
        "new",
        "task",
        "represents",
        "completely",
        "unrelated",
        "phenomena",
        ",",
        "or",
        "random",
        "noise",
        ",",
        "leveraging",
        "prior",
        "experience",
        "will",
        "not",
        "be",
        "effective",
        "."
      ]
    },
    {
      "sentence": "Luckily, in real-world tasks, there are plenty of opportunities to learn from prior experience.",
      "tokens": [
        "Luckily",
        ",",
        "in",
        "real-world",
        "tasks",
        ",",
        "there",
        "are",
        "plenty",
        "of",
        "opportunities",
        "to",
        "learn",
        "from",
        "prior",
        "experience",
        "."
      ]
    },
    {
      "sentence": "In the remainder of this chapter, we categorize meta-learning techniques based on the type of meta-data they leverage, from the most general to the most task-specific.",
      "tokens": [
        "In",
        "the",
        "remainder",
        "of",
        "this",
        "chapter",
        ",",
        "we",
        "categorize",
        "meta-learning",
        "techniques",
        "based",
        "on",
        "the",
        "type",
        "of",
        "meta-data",
        "they",
        "leverage",
        ",",
        "from",
        "the",
        "most",
        "general",
        "to",
        "the",
        "most",
        "task-specific",
        "."
      ]
    },
    {
      "sentence": "First, in Section 2, we discuss how to learn purely from model evaluations.",
      "tokens": [
        "First",
        ",",
        "in",
        "Section",
        "2",
        ",",
        "we",
        "discuss",
        "how",
        "to",
        "learn",
        "purely",
        "from",
        "model",
        "evaluations",
        "."
      ]
    },
    {
      "sentence": "These techniques can be used to recommend generally useful configurations and configuration search spaces, as well as transfer knowledge from empirically similar tasks.",
      "tokens": [
        "These",
        "techniques",
        "can",
        "be",
        "used",
        "to",
        "recommend",
        "generally",
        "useful",
        "configurations",
        "and",
        "configuration",
        "search",
        "spaces",
        ",",
        "as",
        "well",
        "as",
        "transfer",
        "knowledge",
        "from",
        "empirically",
        "similar",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "In Section 3, we discuss how we can characterize tasks to more explicitly express task similarity and build meta-models that learn the relationships between data characteristics and learning performance.",
      "tokens": [
        "In",
        "Section",
        "3",
        ",",
        "we",
        "discuss",
        "how",
        "we",
        "can",
        "characterize",
        "tasks",
        "to",
        "more",
        "explicitly",
        "express",
        "task",
        "similarity",
        "and",
        "build",
        "meta-models",
        "that",
        "learn",
        "the",
        "relationships",
        "between",
        "data",
        "characteristics",
        "and",
        "learning",
        "performance",
        "."
      ]
    },
    {
      "sentence": "Finally, Section 4 covers how we can transfer trained model parameters between tasks that are inherently similar, e.g.",
      "tokens": [
        "Finally",
        ",",
        "Section",
        "4",
        "covers",
        "how",
        "we",
        "can",
        "transfer",
        "trained",
        "model",
        "parameters",
        "between",
        "tasks",
        "that",
        "are",
        "inherently",
        "similar",
        ",",
        "e.g",
        "."
      ]
    },
    {
      "sentence": "sharing the same input features, which enables transfer learning (Pan and Yang, 2010) and few-shot learning (Ravi and Larochelle, 2017) .",
      "tokens": [
        "sharing",
        "the",
        "same",
        "input",
        "features",
        ",",
        "which",
        "enables",
        "transfer",
        "learning",
        "(",
        "Pan",
        "and",
        "Yang",
        ",",
        "2010",
        ")",
        "and",
        "few-shot",
        "learning",
        "(",
        "Ravi",
        "and",
        "Larochelle",
        ",",
        "2017",
        ")",
        "."
      ]
    },
    {
      "sentence": "Note that while multi-task learning (Caruana, 1997) (learning multiple related tasks simultaneously) and ensemble learning (Dietterich, 2000) (building multiple models on the same task), can often be meaningfully combined with meta-learning systems, they do not in themselves involve learning from prior experience on other tasks.",
      "tokens": [
        "Note",
        "that",
        "while",
        "multi-task",
        "learning",
        "(",
        "Caruana",
        ",",
        "1997",
        ")",
        "(",
        "learning",
        "multiple",
        "related",
        "tasks",
        "simultaneously",
        ")",
        "and",
        "ensemble",
        "learning",
        "(",
        "Dietterich",
        ",",
        "2000",
        ")",
        "(",
        "building",
        "multiple",
        "models",
        "on",
        "the",
        "same",
        "task",
        ")",
        ",",
        "can",
        "often",
        "be",
        "meaningfully",
        "combined",
        "with",
        "meta-learning",
        "systems",
        ",",
        "they",
        "do",
        "not",
        "in",
        "themselves",
        "involve",
        "learning",
        "from",
        "prior",
        "experience",
        "on",
        "other",
        "tasks",
        "."
      ]
    }
  ]
}