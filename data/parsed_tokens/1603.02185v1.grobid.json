{
  "title": [
    {
      "sentence": "Distributed Multi-Task Learning with Shared Representation",
      "tokens": [
        "Distributed",
        "Multi-Task",
        "Learning",
        "with",
        "Shared",
        "Representation"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "We study the problem of distributed multitask learning with shared representation, where each machine aims to learn a separate, but related, task in an unknown shared low-dimensional subspaces, i.e.",
      "tokens": [
        "We",
        "study",
        "the",
        "problem",
        "of",
        "distributed",
        "multitask",
        "learning",
        "with",
        "shared",
        "representation",
        ",",
        "where",
        "each",
        "machine",
        "aims",
        "to",
        "learn",
        "a",
        "separate",
        ",",
        "but",
        "related",
        ",",
        "task",
        "in",
        "an",
        "unknown",
        "shared",
        "low-dimensional",
        "subspaces",
        ",",
        "i.e",
        "."
      ]
    },
    {
      "sentence": "when the predictor matrix has low rank.",
      "tokens": [
        "when",
        "the",
        "predictor",
        "matrix",
        "has",
        "low",
        "rank",
        "."
      ]
    },
    {
      "sentence": "We consider a setting where each task is handled by a different machine, with samples for the task available locally on the machine, and study communication-efficient methods for exploiting the shared structure.",
      "tokens": [
        "We",
        "consider",
        "a",
        "setting",
        "where",
        "each",
        "task",
        "is",
        "handled",
        "by",
        "a",
        "different",
        "machine",
        ",",
        "with",
        "samples",
        "for",
        "the",
        "task",
        "available",
        "locally",
        "on",
        "the",
        "machine",
        ",",
        "and",
        "study",
        "communication-efficient",
        "methods",
        "for",
        "exploiting",
        "the",
        "shared",
        "structure",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Introduction Multi-task learning is widely used learning framework in which similar tasks are considered jointly for the purpose of improving performance compared to learning the tasks separately [13] .",
      "tokens": [
        "Introduction",
        "Multi-task",
        "learning",
        "is",
        "widely",
        "used",
        "learning",
        "framework",
        "in",
        "which",
        "similar",
        "tasks",
        "are",
        "considered",
        "jointly",
        "for",
        "the",
        "purpose",
        "of",
        "improving",
        "performance",
        "compared",
        "to",
        "learning",
        "the",
        "tasks",
        "separately",
        "[",
        "13",
        "]",
        "."
      ]
    },
    {
      "sentence": "By transferring information between related tasks it is hoped that samples will be better utilized, leading to improved generalization performance.",
      "tokens": [
        "By",
        "transferring",
        "information",
        "between",
        "related",
        "tasks",
        "it",
        "is",
        "hoped",
        "that",
        "samples",
        "will",
        "be",
        "better",
        "utilized",
        ",",
        "leading",
        "to",
        "improved",
        "generalization",
        "performance",
        "."
      ]
    },
    {
      "sentence": "Multi-task learning has been successfully applied, for example, in natural language understanding [15] , speech recognition [32] , remote sensing [44] , image classification [25] , spam filtering [43] , web search [14] , disease prediction [49] , and eQTL mapping [23] among other applications.",
      "tokens": [
        "Multi-task",
        "learning",
        "has",
        "been",
        "successfully",
        "applied",
        ",",
        "for",
        "example",
        ",",
        "in",
        "natural",
        "language",
        "understanding",
        "[",
        "15",
        "]",
        ",",
        "speech",
        "recognition",
        "[",
        "32",
        "]",
        ",",
        "remote",
        "sensing",
        "[",
        "44",
        "]",
        ",",
        "image",
        "classification",
        "[",
        "25",
        "]",
        ",",
        "spam",
        "filtering",
        "[",
        "43",
        "]",
        ",",
        "web",
        "search",
        "[",
        "14",
        "]",
        ",",
        "disease",
        "prediction",
        "[",
        "49",
        "]",
        ",",
        "and",
        "eQTL",
        "mapping",
        "[",
        "23",
        "]",
        "among",
        "other",
        "applications",
        "."
      ]
    },
    {
      "sentence": "Here, we study multi-task learning in a distributed setting, where each task is handled by a different machine and communication between machines is expensive.",
      "tokens": [
        "Here",
        ",",
        "we",
        "study",
        "multi-task",
        "learning",
        "in",
        "a",
        "distributed",
        "setting",
        ",",
        "where",
        "each",
        "task",
        "is",
        "handled",
        "by",
        "a",
        "different",
        "machine",
        "and",
        "communication",
        "between",
        "machines",
        "is",
        "expensive",
        "."
      ]
    },
    {
      "sentence": "That is, each machine has access to data for a different task and needs to learn a predictor for that task, where machines communicate with each other in order to leverage the relationship between the tasks.",
      "tokens": [
        "That",
        "is",
        ",",
        "each",
        "machine",
        "has",
        "access",
        "to",
        "data",
        "for",
        "a",
        "different",
        "task",
        "and",
        "needs",
        "to",
        "learn",
        "a",
        "predictor",
        "for",
        "that",
        "task",
        ",",
        "where",
        "machines",
        "communicate",
        "with",
        "each",
        "other",
        "in",
        "order",
        "to",
        "leverage",
        "the",
        "relationship",
        "between",
        "the",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "This situation lies between a homogeneous distributed learning setting [e.g.",
      "tokens": [
        "This",
        "situation",
        "lies",
        "between",
        "a",
        "homogeneous",
        "distributed",
        "learning",
        "setting",
        "[",
        "e.g",
        "."
      ]
    },
    {
      "sentence": "36] , where all machines have data from the same source distribution, and inhomogeneous consensus problems [e.g.",
      "tokens": [
        "36",
        "]",
        ",",
        "where",
        "all",
        "machines",
        "have",
        "data",
        "from",
        "the",
        "same",
        "source",
        "distribution",
        ",",
        "and",
        "inhomogeneous",
        "consensus",
        "problems",
        "[",
        "e.g",
        "."
      ]
    },
    {
      "sentence": "30, 11, 6] where the goal is to reach a single consensus predictor or iterate which is the same on all machines.",
      "tokens": [
        "30",
        ",",
        "11",
        ",",
        "6",
        "]",
        "where",
        "the",
        "goal",
        "is",
        "to",
        "reach",
        "a",
        "single",
        "consensus",
        "predictor",
        "or",
        "iterate",
        "which",
        "is",
        "the",
        "same",
        "on",
        "all",
        "machines",
        "."
      ]
    },
    {
      "sentence": "The main argument for this setting is that if each machine indeed has access to different data (e.g.",
      "tokens": [
        "The",
        "main",
        "argument",
        "for",
        "this",
        "setting",
        "is",
        "that",
        "if",
        "each",
        "machine",
        "indeed",
        "has",
        "access",
        "to",
        "different",
        "data",
        "(",
        "e.g",
        "."
      ]
    },
    {
      "sentence": "from a different geographical region or different types of users), as in the consensus problems studied by Balcan et al.",
      "tokens": [
        "from",
        "a",
        "different",
        "geographical",
        "region",
        "or",
        "different",
        "types",
        "of",
        "users",
        ")",
        ",",
        "as",
        "in",
        "the",
        "consensus",
        "problems",
        "studied",
        "by",
        "Balcan",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "[6] , then we should allow a different predictor for each distribution, instead of insisting on a single consensus predictor, while still trying to leverage the relationship and similarity between data distributions, as in classical multi-task learning.",
      "tokens": [
        "[",
        "6",
        "]",
        ",",
        "then",
        "we",
        "should",
        "allow",
        "a",
        "different",
        "predictor",
        "for",
        "each",
        "distribution",
        ",",
        "instead",
        "of",
        "insisting",
        "on",
        "a",
        "single",
        "consensus",
        "predictor",
        ",",
        "while",
        "still",
        "trying",
        "to",
        "leverage",
        "the",
        "relationship",
        "and",
        "similarity",
        "between",
        "data",
        "distributions",
        ",",
        "as",
        "in",
        "classical",
        "multi-task",
        "learning",
        "."
      ]
    },
    {
      "sentence": "As was recently pointed out by Wang et al.",
      "tokens": [
        "As",
        "was",
        "recently",
        "pointed",
        "out",
        "by",
        "Wang",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "[41] , allowing separate predictors for each task instead of insisting on a consensus predictor changes the fundamental nature of the distributed learning problem, allows for different optimization methods, and necessitates a different analysis approach, more similar to homogeneous distributed learning as studied by Shamir and Srebro [36] .",
      "tokens": [
        "[",
        "41",
        "]",
        ",",
        "allowing",
        "separate",
        "predictors",
        "for",
        "each",
        "task",
        "instead",
        "of",
        "insisting",
        "on",
        "a",
        "consensus",
        "predictor",
        "changes",
        "the",
        "fundamental",
        "nature",
        "of",
        "the",
        "distributed",
        "learning",
        "problem",
        ",",
        "allows",
        "for",
        "different",
        "optimization",
        "methods",
        ",",
        "and",
        "necessitates",
        "a",
        "different",
        "analysis",
        "approach",
        ",",
        "more",
        "similar",
        "to",
        "homogeneous",
        "distributed",
        "learning",
        "as",
        "studied",
        "by",
        "Shamir",
        "and",
        "Srebro",
        "[",
        "36",
        "]",
        "."
      ]
    },
    {
      "sentence": "The success of multi-task learning relies on the relatedness between tasks.",
      "tokens": [
        "The",
        "success",
        "of",
        "multi-task",
        "learning",
        "relies",
        "on",
        "the",
        "relatedness",
        "between",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "While Wang et al.",
      "tokens": [
        "While",
        "Wang",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "[41] studied tasks related through shared sparsity, here we turn to a more general, powerful and empirically more successful model of relatedness, where the predictors for different tasks lie in some (a-priori unknown) shared lowdimensional subspace and so the matrix of predictors is of low rank [3, 2, 45, 4] .",
      "tokens": [
        "[",
        "41",
        "]",
        "studied",
        "tasks",
        "related",
        "through",
        "shared",
        "sparsity",
        ",",
        "here",
        "we",
        "turn",
        "to",
        "a",
        "more",
        "general",
        ",",
        "powerful",
        "and",
        "empirically",
        "more",
        "successful",
        "model",
        "of",
        "relatedness",
        ",",
        "where",
        "the",
        "predictors",
        "for",
        "different",
        "tasks",
        "lie",
        "in",
        "some",
        "(",
        "a-priori",
        "unknown",
        ")",
        "shared",
        "lowdimensional",
        "subspace",
        "and",
        "so",
        "the",
        "matrix",
        "of",
        "predictors",
        "is",
        "of",
        "low",
        "rank",
        "[",
        "3",
        ",",
        "2",
        ",",
        "45",
        ",",
        "4",
        "]",
        "."
      ]
    },
    {
      "sentence": "In a shared sparsity model, information from all tasks is used to learn a subset of the input features which are then used by all tasks.",
      "tokens": [
        "In",
        "a",
        "shared",
        "sparsity",
        "model",
        ",",
        "information",
        "from",
        "all",
        "tasks",
        "is",
        "used",
        "to",
        "learn",
        "a",
        "subset",
        "of",
        "the",
        "input",
        "features",
        "which",
        "are",
        "then",
        "used",
        "by",
        "all",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "In contrast, in a shared subspace model, novel features, which are linear functions of the input features, are learned.",
      "tokens": [
        "In",
        "contrast",
        ",",
        "in",
        "a",
        "shared",
        "subspace",
        "model",
        ",",
        "novel",
        "features",
        ",",
        "which",
        "are",
        "linear",
        "functions",
        "of",
        "the",
        "input",
        "features",
        ",",
        "are",
        "learned",
        "."
      ]
    },
    {
      "sentence": "The model can thus be viewed as a two-layer neural network, with the bottom layer learned jointly across tasks and the top layer task-specific.",
      "tokens": [
        "The",
        "model",
        "can",
        "thus",
        "be",
        "viewed",
        "as",
        "a",
        "two-layer",
        "neural",
        "network",
        ",",
        "with",
        "the",
        "bottom",
        "layer",
        "learned",
        "jointly",
        "across",
        "tasks",
        "and",
        "the",
        "top",
        "layer",
        "task-specific",
        "."
      ]
    },
    {
      "sentence": "Being arguably the most complex multi-layer network that we can fully analyze, studying such models can also serve as a gateway to using deeper networks for learning shared representations.",
      "tokens": [
        "Being",
        "arguably",
        "the",
        "most",
        "complex",
        "multi-layer",
        "network",
        "that",
        "we",
        "can",
        "fully",
        "analyze",
        ",",
        "studying",
        "such",
        "models",
        "can",
        "also",
        "serve",
        "as",
        "a",
        "gateway",
        "to",
        "using",
        "deeper",
        "networks",
        "for",
        "learning",
        "shared",
        "representations",
        "."
      ]
    },
    {
      "sentence": "Multi-task learning with a shared subspace is wellstudied in a centralized setting, where data for all tasks are on the same machine, and some global centralized procedure is used to find a good predictor for each task.",
      "tokens": [
        "Multi-task",
        "learning",
        "with",
        "a",
        "shared",
        "subspace",
        "is",
        "wellstudied",
        "in",
        "a",
        "centralized",
        "setting",
        ",",
        "where",
        "data",
        "for",
        "all",
        "tasks",
        "are",
        "on",
        "the",
        "same",
        "machine",
        ",",
        "and",
        "some",
        "global",
        "centralized",
        "procedure",
        "is",
        "used",
        "to",
        "find",
        "a",
        "good",
        "predictor",
        "for",
        "each",
        "task",
        "."
      ]
    },
    {
      "sentence": "In such a situation, nuclear norm regularization is often used to leverage the low rank structure [e.g.",
      "tokens": [
        "In",
        "such",
        "a",
        "situation",
        ",",
        "nuclear",
        "norm",
        "regularization",
        "is",
        "often",
        "used",
        "to",
        "leverage",
        "the",
        "low",
        "rank",
        "structure",
        "[",
        "e.g",
        "."
      ]
    },
    {
      "sentence": "4, 2] and learning guarantees are known ([28] and see also Section 2).",
      "tokens": [
        "4",
        ",",
        "2",
        "]",
        "and",
        "learning",
        "guarantees",
        "are",
        "known",
        "(",
        "[",
        "28",
        "]",
        "and",
        "see",
        "also",
        "Section",
        "2",
        ")",
        "."
      ]
    },
    {
      "sentence": "With the growth of modern massive data sets, where tasks and data often too big to handle on a single machine, it is important to develop methods also for the distributed setting.",
      "tokens": [
        "With",
        "the",
        "growth",
        "of",
        "modern",
        "massive",
        "data",
        "sets",
        ",",
        "where",
        "tasks",
        "and",
        "data",
        "often",
        "too",
        "big",
        "to",
        "handle",
        "on",
        "a",
        "single",
        "machine",
        ",",
        "it",
        "is",
        "important",
        "to",
        "develop",
        "methods",
        "also",
        "for",
        "the",
        "distributed",
        "setting",
        "."
      ]
    },
    {
      "sentence": "Unfortunately, the distributed multi-task setting is largely unexplored and we are not aware of any prior for on distributed multi-task learning with shared subspaces.",
      "tokens": [
        "Unfortunately",
        ",",
        "the",
        "distributed",
        "multi-task",
        "setting",
        "is",
        "largely",
        "unexplored",
        "and",
        "we",
        "are",
        "not",
        "aware",
        "of",
        "any",
        "prior",
        "for",
        "on",
        "distributed",
        "multi-task",
        "learning",
        "with",
        "shared",
        "subspaces",
        "."
      ]
    },
    {
      "sentence": "In this paper we focus on methods with efficient communication complexity (i.e.",
      "tokens": [
        "In",
        "this",
        "paper",
        "we",
        "focus",
        "on",
        "methods",
        "with",
        "efficient",
        "communication",
        "complexity",
        "(",
        "i.e",
        "."
      ]
    },
    {
      "sentence": "with as small as possible communication between machines), that can still leverage most of the statistical benefit of sharedsubspace multi-task learning.",
      "tokens": [
        "with",
        "as",
        "small",
        "as",
        "possible",
        "communication",
        "between",
        "machines",
        ")",
        ",",
        "that",
        "can",
        "still",
        "leverage",
        "most",
        "of",
        "the",
        "statistical",
        "benefit",
        "of",
        "sharedsubspace",
        "multi-task",
        "learning",
        "."
      ]
    },
    {
      "sentence": "Although all our methods are also computationally tractable and can be implemented efficiently, we are less concerned here with minimizing the runtime on each machine separately, considering communication, instead, as the main bottleneck and the main resource to be minimized [8] .",
      "tokens": [
        "Although",
        "all",
        "our",
        "methods",
        "are",
        "also",
        "computationally",
        "tractable",
        "and",
        "can",
        "be",
        "implemented",
        "efficiently",
        ",",
        "we",
        "are",
        "less",
        "concerned",
        "here",
        "with",
        "minimizing",
        "the",
        "runtime",
        "on",
        "each",
        "machine",
        "separately",
        ",",
        "considering",
        "communication",
        ",",
        "instead",
        ",",
        "as",
        "the",
        "main",
        "bottleneck",
        "and",
        "the",
        "main",
        "resource",
        "to",
        "be",
        "minimized",
        "[",
        "8",
        "]",
        "."
      ]
    },
    {
      "sentence": "This is similar to the focus in distributed optimization approaches such as ADMM [11] and DANE [37] where optimization within each machine is taken as an atomic step.",
      "tokens": [
        "This",
        "is",
        "similar",
        "to",
        "the",
        "focus",
        "in",
        "distributed",
        "optimization",
        "approaches",
        "such",
        "as",
        "ADMM",
        "[",
        "11",
        "]",
        "and",
        "DANE",
        "[",
        "37",
        "]",
        "where",
        "optimization",
        "within",
        "each",
        "machine",
        "is",
        "taken",
        "as",
        "an",
        "atomic",
        "step",
        "."
      ]
    }
  ]
}