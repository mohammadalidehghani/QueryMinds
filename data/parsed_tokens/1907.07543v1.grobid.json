{
  "title": [
    {
      "sentence": "Low-Shot Classification: A Comparison of Classical and Deep Transfer Machine Learning Approaches",
      "tokens": [
        "Low-Shot",
        "Classification",
        ":",
        "A",
        "Comparison",
        "of",
        "Classical",
        "and",
        "Deep",
        "Transfer",
        "Machine",
        "Learning",
        "Approaches"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "Despite the recent success of deep transfer learning approaches in NLP, there is a lack of quantitative studies demonstrating the gains these models offer in low-shot text classification tasks over existing paradigms.",
      "tokens": [
        "Despite",
        "the",
        "recent",
        "success",
        "of",
        "deep",
        "transfer",
        "learning",
        "approaches",
        "in",
        "NLP",
        ",",
        "there",
        "is",
        "a",
        "lack",
        "of",
        "quantitative",
        "studies",
        "demonstrating",
        "the",
        "gains",
        "these",
        "models",
        "offer",
        "in",
        "low-shot",
        "text",
        "classification",
        "tasks",
        "over",
        "existing",
        "paradigms",
        "."
      ]
    },
    {
      "sentence": "Deep transfer learning approaches such as BERT and ULMFiT demonstrate that they can beat state-of-the-art results on larger datasets, however when one has only 100-1000 labelled examples per class, the choice of approach is less clear, with classical machine learning and deep transfer learning representing valid options.",
      "tokens": [
        "Deep",
        "transfer",
        "learning",
        "approaches",
        "such",
        "as",
        "BERT",
        "and",
        "ULMFiT",
        "demonstrate",
        "that",
        "they",
        "can",
        "beat",
        "state-of-the-art",
        "results",
        "on",
        "larger",
        "datasets",
        ",",
        "however",
        "when",
        "one",
        "has",
        "only",
        "100-1000",
        "labelled",
        "examples",
        "per",
        "class",
        ",",
        "the",
        "choice",
        "of",
        "approach",
        "is",
        "less",
        "clear",
        ",",
        "with",
        "classical",
        "machine",
        "learning",
        "and",
        "deep",
        "transfer",
        "learning",
        "representing",
        "valid",
        "options",
        "."
      ]
    },
    {
      "sentence": "This paper compares the current best transfer learning approach with top classical machine learning approaches on a trinary sentiment classification task to assess the best paradigm.",
      "tokens": [
        "This",
        "paper",
        "compares",
        "the",
        "current",
        "best",
        "transfer",
        "learning",
        "approach",
        "with",
        "top",
        "classical",
        "machine",
        "learning",
        "approaches",
        "on",
        "a",
        "trinary",
        "sentiment",
        "classification",
        "task",
        "to",
        "assess",
        "the",
        "best",
        "paradigm",
        "."
      ]
    },
    {
      "sentence": "We find that BERT, representing the best of deep transfer learning, is the best performing approach, outperforming top classical machine learning algorithms by 9.7% on average when trained with 100 examples per class, narrowing to 1.8% at 1000 labels per class.",
      "tokens": [
        "We",
        "find",
        "that",
        "BERT",
        ",",
        "representing",
        "the",
        "best",
        "of",
        "deep",
        "transfer",
        "learning",
        ",",
        "is",
        "the",
        "best",
        "performing",
        "approach",
        ",",
        "outperforming",
        "top",
        "classical",
        "machine",
        "learning",
        "algorithms",
        "by",
        "9.7",
        "%",
        "on",
        "average",
        "when",
        "trained",
        "with",
        "100",
        "examples",
        "per",
        "class",
        ",",
        "narrowing",
        "to",
        "1.8",
        "%",
        "at",
        "1000",
        "labels",
        "per",
        "class",
        "."
      ]
    },
    {
      "sentence": "We also show the robustness of deep transfer learning in moving across domains, where the maximum loss in accuracy is only 0.7% in similar domain tasks and 3.2% cross domain, compared to classical machine learning which loses up to 20.6%.",
      "tokens": [
        "We",
        "also",
        "show",
        "the",
        "robustness",
        "of",
        "deep",
        "transfer",
        "learning",
        "in",
        "moving",
        "across",
        "domains",
        ",",
        "where",
        "the",
        "maximum",
        "loss",
        "in",
        "accuracy",
        "is",
        "only",
        "0.7",
        "%",
        "in",
        "similar",
        "domain",
        "tasks",
        "and",
        "3.2",
        "%",
        "cross",
        "domain",
        ",",
        "compared",
        "to",
        "classical",
        "machine",
        "learning",
        "which",
        "loses",
        "up",
        "to",
        "20.6",
        "%",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Introduction Transfer learning in the Natural Language Processing (NLP) field has advanced significantly in the last two years, introducing fine-tuning approaches akin to those seen in computer vision some years earlier (Donahue et al., 2013) .",
      "tokens": [
        "Introduction",
        "Transfer",
        "learning",
        "in",
        "the",
        "Natural",
        "Language",
        "Processing",
        "(",
        "NLP",
        ")",
        "field",
        "has",
        "advanced",
        "significantly",
        "in",
        "the",
        "last",
        "two",
        "years",
        ",",
        "introducing",
        "fine-tuning",
        "approaches",
        "akin",
        "to",
        "those",
        "seen",
        "in",
        "computer",
        "vision",
        "some",
        "years",
        "earlier",
        "(",
        "Donahue",
        "et",
        "al.",
        ",",
        "2013",
        ")",
        "."
      ]
    },
    {
      "sentence": "This growth originated from feature-based transfer learning, which in the form of word embeddings has been in use for some years, particularly driven by (Mikolov, Chen, Corrado, & Dean, 2013) .",
      "tokens": [
        "This",
        "growth",
        "originated",
        "from",
        "feature-based",
        "transfer",
        "learning",
        ",",
        "which",
        "in",
        "the",
        "form",
        "of",
        "word",
        "embeddings",
        "has",
        "been",
        "in",
        "use",
        "for",
        "some",
        "years",
        ",",
        "particularly",
        "driven",
        "by",
        "(",
        "Mikolov",
        ",",
        "Chen",
        ",",
        "Corrado",
        ",",
        "&",
        "Dean",
        ",",
        "2013",
        ")",
        "."
      ]
    },
    {
      "sentence": "As part of this new wave, we have seen advancements in feature-based transfer learning in the form of ELMo (Peters et al., 2018) .",
      "tokens": [
        "As",
        "part",
        "of",
        "this",
        "new",
        "wave",
        ",",
        "we",
        "have",
        "seen",
        "advancements",
        "in",
        "feature-based",
        "transfer",
        "learning",
        "in",
        "the",
        "form",
        "of",
        "ELMo",
        "(",
        "Peters",
        "et",
        "al.",
        ",",
        "2018",
        ")",
        "."
      ]
    },
    {
      "sentence": "In addition a characteristic trend in this wave of transfer learning models is a class of algorithms that primarily focus on a finetuning approach, where a base language model (Bengio, Ducharme, Vincent, & Jauvin, 2003) is trained and then fine-tuned on a target task.",
      "tokens": [
        "In",
        "addition",
        "a",
        "characteristic",
        "trend",
        "in",
        "this",
        "wave",
        "of",
        "transfer",
        "learning",
        "models",
        "is",
        "a",
        "class",
        "of",
        "algorithms",
        "that",
        "primarily",
        "focus",
        "on",
        "a",
        "finetuning",
        "approach",
        ",",
        "where",
        "a",
        "base",
        "language",
        "model",
        "(",
        "Bengio",
        ",",
        "Ducharme",
        ",",
        "Vincent",
        ",",
        "&",
        "Jauvin",
        ",",
        "2003",
        ")",
        "is",
        "trained",
        "and",
        "then",
        "fine-tuned",
        "on",
        "a",
        "target",
        "task",
        "."
      ]
    },
    {
      "sentence": "This base language model is typically very large (100M + parameters) and takes a relatively long time to train.",
      "tokens": [
        "This",
        "base",
        "language",
        "model",
        "is",
        "typically",
        "very",
        "large",
        "(",
        "100M",
        "+",
        "parameters",
        ")",
        "and",
        "takes",
        "a",
        "relatively",
        "long",
        "time",
        "to",
        "train",
        "."
      ]
    },
    {
      "sentence": "However, the fine-tuning task is usually much quicker to train as only a few parameters are added to the model, typically a single dense layer to the end of a multilayer LSTM or Transformer (Vaswani et al., 2017) .",
      "tokens": [
        "However",
        ",",
        "the",
        "fine-tuning",
        "task",
        "is",
        "usually",
        "much",
        "quicker",
        "to",
        "train",
        "as",
        "only",
        "a",
        "few",
        "parameters",
        "are",
        "added",
        "to",
        "the",
        "model",
        ",",
        "typically",
        "a",
        "single",
        "dense",
        "layer",
        "to",
        "the",
        "end",
        "of",
        "a",
        "multilayer",
        "LSTM",
        "or",
        "Transformer",
        "(",
        "Vaswani",
        "et",
        "al.",
        ",",
        "2017",
        ")",
        "."
      ]
    },
    {
      "sentence": "The model continues training either all, or part, of the network, but this is typically on much less data and for much less time, as only the task specific information is being learned and the general \"understanding\" of the language is transferred.",
      "tokens": [
        "The",
        "model",
        "continues",
        "training",
        "either",
        "all",
        ",",
        "or",
        "part",
        ",",
        "of",
        "the",
        "network",
        ",",
        "but",
        "this",
        "is",
        "typically",
        "on",
        "much",
        "less",
        "data",
        "and",
        "for",
        "much",
        "less",
        "time",
        ",",
        "as",
        "only",
        "the",
        "task",
        "specific",
        "information",
        "is",
        "being",
        "learned",
        "and",
        "the",
        "general",
        "``",
        "understanding",
        "''",
        "of",
        "the",
        "language",
        "is",
        "transferred",
        "."
      ]
    },
    {
      "sentence": "These approaches have, on multiple occasions, broken the state-of-the-art records (SO-TAs) across the board on a range of NLP tasks and datasets (Devlin, Chang, Lee, & Toutanova, 2018) (Howard & Ruder, 2018) .",
      "tokens": [
        "These",
        "approaches",
        "have",
        ",",
        "on",
        "multiple",
        "occasions",
        ",",
        "broken",
        "the",
        "state-of-the-art",
        "records",
        "(",
        "SO-TAs",
        ")",
        "across",
        "the",
        "board",
        "on",
        "a",
        "range",
        "of",
        "NLP",
        "tasks",
        "and",
        "datasets",
        "(",
        "Devlin",
        ",",
        "Chang",
        ",",
        "Lee",
        ",",
        "&",
        "Toutanova",
        ",",
        "2018",
        ")",
        "(",
        "Howard",
        "&",
        "Ruder",
        ",",
        "2018",
        ")",
        "."
      ]
    },
    {
      "sentence": "However, all of these datasets are designed for deep learning: they are typically large enough that they warrant the use of deep learning (5000+ examples per class), without the necessity of transfer learning.",
      "tokens": [
        "However",
        ",",
        "all",
        "of",
        "these",
        "datasets",
        "are",
        "designed",
        "for",
        "deep",
        "learning",
        ":",
        "they",
        "are",
        "typically",
        "large",
        "enough",
        "that",
        "they",
        "warrant",
        "the",
        "use",
        "of",
        "deep",
        "learning",
        "(",
        "5000+",
        "examples",
        "per",
        "class",
        ")",
        ",",
        "without",
        "the",
        "necessity",
        "of",
        "transfer",
        "learning",
        "."
      ]
    },
    {
      "sentence": "It is our view that what transfer learning does, in these cases, is push the boundaries of performance.",
      "tokens": [
        "It",
        "is",
        "our",
        "view",
        "that",
        "what",
        "transfer",
        "learning",
        "does",
        ",",
        "in",
        "these",
        "cases",
        ",",
        "is",
        "push",
        "the",
        "boundaries",
        "of",
        "performance",
        "."
      ]
    },
    {
      "sentence": "The prevalence of deep learning algorithms in surpassing SOTA records suggests quite clearly that, for the datasets assessed, deep learning surpasses the limits of classical machine learning algorithms in NLP tasks.",
      "tokens": [
        "The",
        "prevalence",
        "of",
        "deep",
        "learning",
        "algorithms",
        "in",
        "surpassing",
        "SOTA",
        "records",
        "suggests",
        "quite",
        "clearly",
        "that",
        ",",
        "for",
        "the",
        "datasets",
        "assessed",
        ",",
        "deep",
        "learning",
        "surpasses",
        "the",
        "limits",
        "of",
        "classical",
        "machine",
        "learning",
        "algorithms",
        "in",
        "NLP",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "Low-shot transfer learning is another usecase for transfer learning in NLP, one of particular interest to companies working with real-world data.",
      "tokens": [
        "Low-shot",
        "transfer",
        "learning",
        "is",
        "another",
        "usecase",
        "for",
        "transfer",
        "learning",
        "in",
        "NLP",
        ",",
        "one",
        "of",
        "particular",
        "interest",
        "to",
        "companies",
        "working",
        "with",
        "real-world",
        "data",
        "."
      ]
    },
    {
      "sentence": "Low-shot transfer learning (also referred to as \"few-shot\") is the use of transfer learning in training models where we have little training data available.",
      "tokens": [
        "Low-shot",
        "transfer",
        "learning",
        "(",
        "also",
        "referred",
        "to",
        "as",
        "``",
        "few-shot",
        "''",
        ")",
        "is",
        "the",
        "use",
        "of",
        "transfer",
        "learning",
        "in",
        "training",
        "models",
        "where",
        "we",
        "have",
        "little",
        "training",
        "data",
        "available",
        "."
      ]
    },
    {
      "sentence": "This is important as many potential real-world applications of machine learning NLP do not have access to sufficiently large datasets to train deep learning algorithms, and obtaining such a dataset can often be too expensive or time consuming.",
      "tokens": [
        "This",
        "is",
        "important",
        "as",
        "many",
        "potential",
        "real-world",
        "applications",
        "of",
        "machine",
        "learning",
        "NLP",
        "do",
        "not",
        "have",
        "access",
        "to",
        "sufficiently",
        "large",
        "datasets",
        "to",
        "train",
        "deep",
        "learning",
        "algorithms",
        ",",
        "and",
        "obtaining",
        "such",
        "a",
        "dataset",
        "can",
        "often",
        "be",
        "too",
        "expensive",
        "or",
        "time",
        "consuming",
        "."
      ]
    },
    {
      "sentence": "Howard & Ruder (2018) note, and Devlin et.",
      "tokens": [
        "Howard",
        "&",
        "Ruder",
        "(",
        "2018",
        ")",
        "note",
        ",",
        "and",
        "Devlin",
        "et",
        "."
      ]
    },
    {
      "sentence": "al.",
      "tokens": [
        "al",
        "."
      ]
    },
    {
      "sentence": "(2018) hypothesize that their respective approaches can be used with low quantities of data to give good results.",
      "tokens": [
        "(",
        "2018",
        ")",
        "hypothesize",
        "that",
        "their",
        "respective",
        "approaches",
        "can",
        "be",
        "used",
        "with",
        "low",
        "quantities",
        "of",
        "data",
        "to",
        "give",
        "good",
        "results",
        "."
      ]
    },
    {
      "sentence": "However, in sources such as (Howard & Ruder, 2018) , results on low-shot learning are presented relative to training deep models from scratch, but as mentioned in (Goodfellow, Bengio, & Courville, 2016) , deep learning generally only achieves reasonable performance at about 5000 examples per class and is therefore not necessarily the best paradigm at these scales.",
      "tokens": [
        "However",
        ",",
        "in",
        "sources",
        "such",
        "as",
        "(",
        "Howard",
        "&",
        "Ruder",
        ",",
        "2018",
        ")",
        ",",
        "results",
        "on",
        "low-shot",
        "learning",
        "are",
        "presented",
        "relative",
        "to",
        "training",
        "deep",
        "models",
        "from",
        "scratch",
        ",",
        "but",
        "as",
        "mentioned",
        "in",
        "(",
        "Goodfellow",
        ",",
        "Bengio",
        ",",
        "&",
        "Courville",
        ",",
        "2016",
        ")",
        ",",
        "deep",
        "learning",
        "generally",
        "only",
        "achieves",
        "reasonable",
        "performance",
        "at",
        "about",
        "5000",
        "examples",
        "per",
        "class",
        "and",
        "is",
        "therefore",
        "not",
        "necessarily",
        "the",
        "best",
        "paradigm",
        "at",
        "these",
        "scales",
        "."
      ]
    },
    {
      "sentence": "This is shown quantitatively in (Chen, Mckeever, & Delany, 2018) where, at scales of 2000+ labels per class, an SVM outperforms several deep learning approaches on text classification tasks.",
      "tokens": [
        "This",
        "is",
        "shown",
        "quantitatively",
        "in",
        "(",
        "Chen",
        ",",
        "Mckeever",
        ",",
        "&",
        "Delany",
        ",",
        "2018",
        ")",
        "where",
        ",",
        "at",
        "scales",
        "of",
        "2000+",
        "labels",
        "per",
        "class",
        ",",
        "an",
        "SVM",
        "outperforms",
        "several",
        "deep",
        "learning",
        "approaches",
        "on",
        "text",
        "classification",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "As such, we propose that to evaluate the low-shot learning benefits of deep transfer learning models, we should in fact look at performance against the strongest classical machine learning methods.",
      "tokens": [
        "As",
        "such",
        ",",
        "we",
        "propose",
        "that",
        "to",
        "evaluate",
        "the",
        "low-shot",
        "learning",
        "benefits",
        "of",
        "deep",
        "transfer",
        "learning",
        "models",
        ",",
        "we",
        "should",
        "in",
        "fact",
        "look",
        "at",
        "performance",
        "against",
        "the",
        "strongest",
        "classical",
        "machine",
        "learning",
        "methods",
        "."
      ]
    },
    {
      "sentence": "However, we have yet to find a comprehensive quantitative study performing this analysis and show that low-shot transfer learning in NLP is actually the optimal approach when dealing with small quantities of data.",
      "tokens": [
        "However",
        ",",
        "we",
        "have",
        "yet",
        "to",
        "find",
        "a",
        "comprehensive",
        "quantitative",
        "study",
        "performing",
        "this",
        "analysis",
        "and",
        "show",
        "that",
        "low-shot",
        "transfer",
        "learning",
        "in",
        "NLP",
        "is",
        "actually",
        "the",
        "optimal",
        "approach",
        "when",
        "dealing",
        "with",
        "small",
        "quantities",
        "of",
        "data",
        "."
      ]
    },
    {
      "sentence": "In this paper we attempt to answer this question in the context of classification tasks.",
      "tokens": [
        "In",
        "this",
        "paper",
        "we",
        "attempt",
        "to",
        "answer",
        "this",
        "question",
        "in",
        "the",
        "context",
        "of",
        "classification",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "What is the best paradigm to use in the case where we have 100 -1000 labelled training examples per class -classical machine learning or deep transfer learning?",
      "tokens": [
        "What",
        "is",
        "the",
        "best",
        "paradigm",
        "to",
        "use",
        "in",
        "the",
        "case",
        "where",
        "we",
        "have",
        "100",
        "-1000",
        "labelled",
        "training",
        "examples",
        "per",
        "class",
        "-classical",
        "machine",
        "learning",
        "or",
        "deep",
        "transfer",
        "learning",
        "?"
      ]
    },
    {
      "sentence": "We seek to compare the best-in-class approaches from both deep transfer learning and classical machine learning by training a variety of models and evaluating by analysing intra-domain and inter-domain performance (details in section 2).",
      "tokens": [
        "We",
        "seek",
        "to",
        "compare",
        "the",
        "best-in-class",
        "approaches",
        "from",
        "both",
        "deep",
        "transfer",
        "learning",
        "and",
        "classical",
        "machine",
        "learning",
        "by",
        "training",
        "a",
        "variety",
        "of",
        "models",
        "and",
        "evaluating",
        "by",
        "analysing",
        "intra-domain",
        "and",
        "inter-domain",
        "performance",
        "(",
        "details",
        "in",
        "section",
        "2",
        ")",
        "."
      ]
    },
    {
      "sentence": "The choice of 100 -1000 is motivated by the amount of data feasible for companies and researchers to tag in-house, as well as the scale of data occurring organically through other means.",
      "tokens": [
        "The",
        "choice",
        "of",
        "100",
        "-1000",
        "is",
        "motivated",
        "by",
        "the",
        "amount",
        "of",
        "data",
        "feasible",
        "for",
        "companies",
        "and",
        "researchers",
        "to",
        "tag",
        "in-house",
        ",",
        "as",
        "well",
        "as",
        "the",
        "scale",
        "of",
        "data",
        "occurring",
        "organically",
        "through",
        "other",
        "means",
        "."
      ]
    },
    {
      "sentence": "For example, in marketing these figures typically represent the base sizes of surveys that can be used as training data.",
      "tokens": [
        "For",
        "example",
        ",",
        "in",
        "marketing",
        "these",
        "figures",
        "typically",
        "represent",
        "the",
        "base",
        "sizes",
        "of",
        "surveys",
        "that",
        "can",
        "be",
        "used",
        "as",
        "training",
        "data",
        "."
      ]
    },
    {
      "sentence": "The rest of this paper is laid out as follows.",
      "tokens": [
        "The",
        "rest",
        "of",
        "this",
        "paper",
        "is",
        "laid",
        "out",
        "as",
        "follows",
        "."
      ]
    },
    {
      "sentence": "Section 2 details the datasets we use.",
      "tokens": [
        "Section",
        "2",
        "details",
        "the",
        "datasets",
        "we",
        "use",
        "."
      ]
    },
    {
      "sentence": "Section 3 looks at the methodology used to evaluate the optimal paradigm.",
      "tokens": [
        "Section",
        "3",
        "looks",
        "at",
        "the",
        "methodology",
        "used",
        "to",
        "evaluate",
        "the",
        "optimal",
        "paradigm",
        "."
      ]
    },
    {
      "sentence": "In section 4 we present the algorithms we use to test, along with related work influencing our choices in selecting those models.",
      "tokens": [
        "In",
        "section",
        "4",
        "we",
        "present",
        "the",
        "algorithms",
        "we",
        "use",
        "to",
        "test",
        ",",
        "along",
        "with",
        "related",
        "work",
        "influencing",
        "our",
        "choices",
        "in",
        "selecting",
        "those",
        "models",
        "."
      ]
    },
    {
      "sentence": "Section 5 details our experiments including choosing the optimal configuration of hyperparameters and preprocessing for each algorithm.",
      "tokens": [
        "Section",
        "5",
        "details",
        "our",
        "experiments",
        "including",
        "choosing",
        "the",
        "optimal",
        "configuration",
        "of",
        "hyperparameters",
        "and",
        "preprocessing",
        "for",
        "each",
        "algorithm",
        "."
      ]
    },
    {
      "sentence": "In section 6 we present the results followed by our comments and conclusions.",
      "tokens": [
        "In",
        "section",
        "6",
        "we",
        "present",
        "the",
        "results",
        "followed",
        "by",
        "our",
        "comments",
        "and",
        "conclusions",
        "."
      ]
    },
    {
      "sentence": "Finally, we highlight a few key points and considerations worthy of mention for the two paradigms in 7.",
      "tokens": [
        "Finally",
        ",",
        "we",
        "highlight",
        "a",
        "few",
        "key",
        "points",
        "and",
        "considerations",
        "worthy",
        "of",
        "mention",
        "for",
        "the",
        "two",
        "paradigms",
        "in",
        "7",
        "."
      ]
    }
  ]
}