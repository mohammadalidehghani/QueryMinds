{
  "title": [
    {
      "sentence": "Logistic Regression as Soft Perceptron Learning",
      "tokens": [
        "Logistic",
        "Regression",
        "as",
        "Soft",
        "Perceptron",
        "Learning"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "We show that gradient ascent for logistic regression has a connection with the perceptron learning algorithm.",
      "tokens": [
        "We",
        "show",
        "that",
        "gradient",
        "ascent",
        "for",
        "logistic",
        "regression",
        "has",
        "a",
        "connection",
        "with",
        "the",
        "perceptron",
        "learning",
        "algorithm",
        "."
      ]
    },
    {
      "sentence": "Logistic learning is the \"soft\" variant of perceptron learning.",
      "tokens": [
        "Logistic",
        "learning",
        "is",
        "the",
        "``",
        "soft",
        "''",
        "variant",
        "of",
        "perceptron",
        "learning",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Logistic Regression is used to build classifiers with a function which can be given a probabilistic interpretation.",
      "tokens": [
        "Logistic",
        "Regression",
        "is",
        "used",
        "to",
        "build",
        "classifiers",
        "with",
        "a",
        "function",
        "which",
        "can",
        "be",
        "given",
        "a",
        "probabilistic",
        "interpretation",
        "."
      ]
    },
    {
      "sentence": "We are given the data set (x i , y i ), for i = 1, .",
      "tokens": [
        "We",
        "are",
        "given",
        "the",
        "data",
        "set",
        "(",
        "x",
        "i",
        ",",
        "y",
        "i",
        ")",
        ",",
        "for",
        "i",
        "=",
        "1",
        ",",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ", N, where the x i are (n + 1)-dimensional extended vectors and the y i are zero or one (representing the positive or negative class, respectively).",
      "tokens": [
        ",",
        "N",
        ",",
        "where",
        "the",
        "x",
        "i",
        "are",
        "(",
        "n",
        "+",
        "1",
        ")",
        "-dimensional",
        "extended",
        "vectors",
        "and",
        "the",
        "y",
        "i",
        "are",
        "zero",
        "or",
        "one",
        "(",
        "representing",
        "the",
        "positive",
        "or",
        "negative",
        "class",
        ",",
        "respectively",
        ")",
        "."
      ]
    },
    {
      "sentence": "We would like to build a function p(x, β), which depends on a single (n + 1)dimensional parameter vector β (like in linear regression) but where p(x, β) approaches one when x belongs to the positive class, and zero if not.",
      "tokens": [
        "We",
        "would",
        "like",
        "to",
        "build",
        "a",
        "function",
        "p",
        "(",
        "x",
        ",",
        "β",
        ")",
        ",",
        "which",
        "depends",
        "on",
        "a",
        "single",
        "(",
        "n",
        "+",
        "1",
        ")",
        "dimensional",
        "parameter",
        "vector",
        "β",
        "(",
        "like",
        "in",
        "linear",
        "regression",
        ")",
        "but",
        "where",
        "p",
        "(",
        "x",
        ",",
        "β",
        ")",
        "approaches",
        "one",
        "when",
        "x",
        "belongs",
        "to",
        "the",
        "positive",
        "class",
        ",",
        "and",
        "zero",
        "if",
        "not",
        "."
      ]
    },
    {
      "sentence": "An extended vector x is one in which the collection of features has been augmented by attaching the additional component 1 to n features, so that the scalar product of the β and x vectors can be written as β T x = β 0 + β 1 x 1 + • • • + β n x n The extra component allows us to handle a constant term β 0 in the scalar product in an elegant way.",
      "tokens": [
        "An",
        "extended",
        "vector",
        "x",
        "is",
        "one",
        "in",
        "which",
        "the",
        "collection",
        "of",
        "features",
        "has",
        "been",
        "augmented",
        "by",
        "attaching",
        "the",
        "additional",
        "component",
        "1",
        "to",
        "n",
        "features",
        ",",
        "so",
        "that",
        "the",
        "scalar",
        "product",
        "of",
        "the",
        "β",
        "and",
        "x",
        "vectors",
        "can",
        "be",
        "written",
        "as",
        "β",
        "T",
        "x",
        "=",
        "β",
        "0",
        "+",
        "β",
        "1",
        "x",
        "1",
        "+",
        "•",
        "•",
        "•",
        "+",
        "β",
        "n",
        "x",
        "n",
        "The",
        "extra",
        "component",
        "allows",
        "us",
        "to",
        "handle",
        "a",
        "constant",
        "term",
        "β",
        "0",
        "in",
        "the",
        "scalar",
        "product",
        "in",
        "an",
        "elegant",
        "way",
        "."
      ]
    },
    {
      "sentence": "A proposal for a function such as the one described above is p(x, β) = exp(β T x)/(1 + exp(β T x)) where p(x, β) denotes the probability that x belongs to the positive class.",
      "tokens": [
        "A",
        "proposal",
        "for",
        "a",
        "function",
        "such",
        "as",
        "the",
        "one",
        "described",
        "above",
        "is",
        "p",
        "(",
        "x",
        ",",
        "β",
        ")",
        "=",
        "exp",
        "(",
        "β",
        "T",
        "x",
        ")",
        "/",
        "(",
        "1",
        "+",
        "exp",
        "(",
        "β",
        "T",
        "x",
        ")",
        ")",
        "where",
        "p",
        "(",
        "x",
        ",",
        "β",
        ")",
        "denotes",
        "the",
        "probability",
        "that",
        "x",
        "belongs",
        "to",
        "the",
        "positive",
        "class",
        "."
      ]
    },
    {
      "sentence": "The function is always positive and never greater than one.",
      "tokens": [
        "The",
        "function",
        "is",
        "always",
        "positive",
        "and",
        "never",
        "greater",
        "than",
        "one",
        "."
      ]
    },
    {
      "sentence": "It saturates asymptotically to 1 in the direction of β.",
      "tokens": [
        "It",
        "saturates",
        "asymptotically",
        "to",
        "1",
        "in",
        "the",
        "direction",
        "of",
        "β",
        "."
      ]
    },
    {
      "sentence": "Note that the probability of x belonging to the negative class is given by: 1 -p(x, β) = 1/(1 + exp(β T x)) With this interpretation we can adjust β so that the data has maximum likelihood.",
      "tokens": [
        "Note",
        "that",
        "the",
        "probability",
        "of",
        "x",
        "belonging",
        "to",
        "the",
        "negative",
        "class",
        "is",
        "given",
        "by",
        ":",
        "1",
        "-p",
        "(",
        "x",
        ",",
        "β",
        ")",
        "=",
        "1/",
        "(",
        "1",
        "+",
        "exp",
        "(",
        "β",
        "T",
        "x",
        ")",
        ")",
        "With",
        "this",
        "interpretation",
        "we",
        "can",
        "adjust",
        "β",
        "so",
        "that",
        "the",
        "data",
        "has",
        "maximum",
        "likelihood",
        "."
      ]
    },
    {
      "sentence": "If N 1 is the number of data points in the positive class and N 2 the number o data points in the negative class, the likelihood is given by the product of all points probabilities L(β) = N 1 p(x i , β) N 2 (1 -p(x i , β)) We want to maximize the likelihood of the data, but we usually maximize the log-likelihood, since the logarithm is a monotonic function.",
      "tokens": [
        "If",
        "N",
        "1",
        "is",
        "the",
        "number",
        "of",
        "data",
        "points",
        "in",
        "the",
        "positive",
        "class",
        "and",
        "N",
        "2",
        "the",
        "number",
        "o",
        "data",
        "points",
        "in",
        "the",
        "negative",
        "class",
        ",",
        "the",
        "likelihood",
        "is",
        "given",
        "by",
        "the",
        "product",
        "of",
        "all",
        "points",
        "probabilities",
        "L",
        "(",
        "β",
        ")",
        "=",
        "N",
        "1",
        "p",
        "(",
        "x",
        "i",
        ",",
        "β",
        ")",
        "N",
        "2",
        "(",
        "1",
        "-p",
        "(",
        "x",
        "i",
        ",",
        "β",
        ")",
        ")",
        "We",
        "want",
        "to",
        "maximize",
        "the",
        "likelihood",
        "of",
        "the",
        "data",
        ",",
        "but",
        "we",
        "usually",
        "maximize",
        "the",
        "log-likelihood",
        ",",
        "since",
        "the",
        "logarithm",
        "is",
        "a",
        "monotonic",
        "function",
        "."
      ]
    },
    {
      "sentence": "The log-likelihood of the data is obtained taking the logarithm of L(β).",
      "tokens": [
        "The",
        "log-likelihood",
        "of",
        "the",
        "data",
        "is",
        "obtained",
        "taking",
        "the",
        "logarithm",
        "of",
        "L",
        "(",
        "β",
        ")",
        "."
      ]
    },
    {
      "sentence": "The products transform into sums of logarithms of the probabilities: ℓ(β) = N 1 β T x i - N 1 log(1 + exp(β T x i )) - N 2 log(1 + exp(β T x i )) which can be simplified to ℓ(β) = N y i β T x i - N log(1 + exp(β T x i )) where N = N 1 + N 2 and y i = 0 for all points in the negative class.",
      "tokens": [
        "The",
        "products",
        "transform",
        "into",
        "sums",
        "of",
        "logarithms",
        "of",
        "the",
        "probabilities",
        ":",
        "ℓ",
        "(",
        "β",
        ")",
        "=",
        "N",
        "1",
        "β",
        "T",
        "x",
        "i",
        "-",
        "N",
        "1",
        "log",
        "(",
        "1",
        "+",
        "exp",
        "(",
        "β",
        "T",
        "x",
        "i",
        ")",
        ")",
        "-",
        "N",
        "2",
        "log",
        "(",
        "1",
        "+",
        "exp",
        "(",
        "β",
        "T",
        "x",
        "i",
        ")",
        ")",
        "which",
        "can",
        "be",
        "simplified",
        "to",
        "ℓ",
        "(",
        "β",
        ")",
        "=",
        "N",
        "y",
        "i",
        "β",
        "T",
        "x",
        "i",
        "-",
        "N",
        "log",
        "(",
        "1",
        "+",
        "exp",
        "(",
        "β",
        "T",
        "x",
        "i",
        ")",
        ")",
        "where",
        "N",
        "=",
        "N",
        "1",
        "+",
        "N",
        "2",
        "and",
        "y",
        "i",
        "=",
        "0",
        "for",
        "all",
        "points",
        "in",
        "the",
        "negative",
        "class",
        "."
      ]
    },
    {
      "sentence": "If we want to find the best parameter β we could set the gradient of ℓ(β) to zero and solve for β.",
      "tokens": [
        "If",
        "we",
        "want",
        "to",
        "find",
        "the",
        "best",
        "parameter",
        "β",
        "we",
        "could",
        "set",
        "the",
        "gradient",
        "of",
        "ℓ",
        "(",
        "β",
        ")",
        "to",
        "zero",
        "and",
        "solve",
        "for",
        "β",
        "."
      ]
    },
    {
      "sentence": "However the nonlinear function makes an analytic solution very difficult.",
      "tokens": [
        "However",
        "the",
        "nonlinear",
        "function",
        "makes",
        "an",
        "analytic",
        "solution",
        "very",
        "difficult",
        "."
      ]
    },
    {
      "sentence": "Therefore we can try to maximize ℓ(β) numerically, using gradient ascent.",
      "tokens": [
        "Therefore",
        "we",
        "can",
        "try",
        "to",
        "maximize",
        "ℓ",
        "(",
        "β",
        ")",
        "numerically",
        ",",
        "using",
        "gradient",
        "ascent",
        "."
      ]
    },
    {
      "sentence": "Therefore we compute the derivative of ℓ(β) relative to the vector β: ∇ℓ(β) = N (y i x i -(exp(β T x i )x i )/(1 + exp(β T x i ) which reduces to ∇ℓ(β) = N (y i x i -p(x i , β)x i ) = N x i (y i -p(x i , β)) This is a very interesting expression.",
      "tokens": [
        "Therefore",
        "we",
        "compute",
        "the",
        "derivative",
        "of",
        "ℓ",
        "(",
        "β",
        ")",
        "relative",
        "to",
        "the",
        "vector",
        "β",
        ":",
        "∇ℓ",
        "(",
        "β",
        ")",
        "=",
        "N",
        "(",
        "y",
        "i",
        "x",
        "i",
        "-",
        "(",
        "exp",
        "(",
        "β",
        "T",
        "x",
        "i",
        ")",
        "x",
        "i",
        ")",
        "/",
        "(",
        "1",
        "+",
        "exp",
        "(",
        "β",
        "T",
        "x",
        "i",
        ")",
        "which",
        "reduces",
        "to",
        "∇ℓ",
        "(",
        "β",
        ")",
        "=",
        "N",
        "(",
        "y",
        "i",
        "x",
        "i",
        "-p",
        "(",
        "x",
        "i",
        ",",
        "β",
        ")",
        "x",
        "i",
        ")",
        "=",
        "N",
        "x",
        "i",
        "(",
        "y",
        "i",
        "-p",
        "(",
        "x",
        "i",
        ",",
        "β",
        ")",
        ")",
        "This",
        "is",
        "a",
        "very",
        "interesting",
        "expression",
        "."
      ]
    },
    {
      "sentence": "It essentially says that, when doing gradient ascent, the corrections to the vector β are computed in the following way: if x belongs to the positive class (y i = 1) but the probability p(x i , β) is low, we add the vector x i to β, weighted by y i -p(x i , β).",
      "tokens": [
        "It",
        "essentially",
        "says",
        "that",
        ",",
        "when",
        "doing",
        "gradient",
        "ascent",
        ",",
        "the",
        "corrections",
        "to",
        "the",
        "vector",
        "β",
        "are",
        "computed",
        "in",
        "the",
        "following",
        "way",
        ":",
        "if",
        "x",
        "belongs",
        "to",
        "the",
        "positive",
        "class",
        "(",
        "y",
        "i",
        "=",
        "1",
        ")",
        "but",
        "the",
        "probability",
        "p",
        "(",
        "x",
        "i",
        ",",
        "β",
        ")",
        "is",
        "low",
        ",",
        "we",
        "add",
        "the",
        "vector",
        "x",
        "i",
        "to",
        "β",
        ",",
        "weighted",
        "by",
        "y",
        "i",
        "-p",
        "(",
        "x",
        "i",
        ",",
        "β",
        ")",
        "."
      ]
    },
    {
      "sentence": "And conversely: if x belongs to the negative class (y i = 0) but the probability p(x i , β) is high, we subtract the vector x i from β, weighted by |y i -p(x i , β)|.",
      "tokens": [
        "And",
        "conversely",
        ":",
        "if",
        "x",
        "belongs",
        "to",
        "the",
        "negative",
        "class",
        "(",
        "y",
        "i",
        "=",
        "0",
        ")",
        "but",
        "the",
        "probability",
        "p",
        "(",
        "x",
        "i",
        ",",
        "β",
        ")",
        "is",
        "high",
        ",",
        "we",
        "subtract",
        "the",
        "vector",
        "x",
        "i",
        "from",
        "β",
        ",",
        "weighted",
        "by",
        "|y",
        "i",
        "-p",
        "(",
        "x",
        "i",
        ",",
        "β",
        ")",
        "|",
        "."
      ]
    },
    {
      "sentence": "This is the way the perceptron learning algorithm works, without the weights.",
      "tokens": [
        "This",
        "is",
        "the",
        "way",
        "the",
        "perceptron",
        "learning",
        "algorithm",
        "works",
        ",",
        "without",
        "the",
        "weights",
        "."
      ]
    },
    {
      "sentence": "If instead of a logistic function we had a step function for assigning \"hard\" probabilities of zero and one to the data vectors, we would obtain the perceptron learning algorithm.",
      "tokens": [
        "If",
        "instead",
        "of",
        "a",
        "logistic",
        "function",
        "we",
        "had",
        "a",
        "step",
        "function",
        "for",
        "assigning",
        "``",
        "hard",
        "''",
        "probabilities",
        "of",
        "zero",
        "and",
        "one",
        "to",
        "the",
        "data",
        "vectors",
        ",",
        "we",
        "would",
        "obtain",
        "the",
        "perceptron",
        "learning",
        "algorithm",
        "."
      ]
    }
  ]
}