{
  "abstract": [
    {
      "sentence": "Recent works on recurrent neural network and deep learning architecture have shown the power of deep learning in modeling time dependent input sequences.",
      "tokens": [
        "Recent",
        "works",
        "on",
        "recurrent",
        "neural",
        "network",
        "and",
        "deep",
        "learning",
        "architecture",
        "have",
        "shown",
        "the",
        "power",
        "of",
        "deep",
        "learning",
        "in",
        "modeling",
        "time",
        "dependent",
        "input",
        "sequences",
        "."
      ]
    },
    {
      "sentence": "Specific learning structure such as Higher-order boltzmann machine, gradient-based learning manifold, and Recurrent \"grammar cell\" reveal their ability to learn feature transformation between related input maps, and perform well in time-related learning & prediction tasks in higher order cases.",
      "tokens": [
        "Specific",
        "learning",
        "structure",
        "such",
        "as",
        "Higher-order",
        "boltzmann",
        "machine",
        ",",
        "gradient-based",
        "learning",
        "manifold",
        ",",
        "and",
        "Recurrent",
        "``",
        "grammar",
        "cell",
        "''",
        "reveal",
        "their",
        "ability",
        "to",
        "learn",
        "feature",
        "transformation",
        "between",
        "related",
        "input",
        "maps",
        ",",
        "and",
        "perform",
        "well",
        "in",
        "time-related",
        "learning",
        "&",
        "prediction",
        "tasks",
        "in",
        "higher",
        "order",
        "cases",
        "."
      ]
    },
    {
      "sentence": "In this article, we extend the conventional convolutional-Restricted-Boltzmann-Machine to learn highly abstract features among abitrary number of time related input maps by constructing a layer of multiplicative units, which capture the relations among inputs.",
      "tokens": [
        "In",
        "this",
        "article",
        ",",
        "we",
        "extend",
        "the",
        "conventional",
        "convolutional-Restricted-Boltzmann-Machine",
        "to",
        "learn",
        "highly",
        "abstract",
        "features",
        "among",
        "abitrary",
        "number",
        "of",
        "time",
        "related",
        "input",
        "maps",
        "by",
        "constructing",
        "a",
        "layer",
        "of",
        "multiplicative",
        "units",
        ",",
        "which",
        "capture",
        "the",
        "relations",
        "among",
        "inputs",
        "."
      ]
    },
    {
      "sentence": "In some cases, we only care about how one map transforms into another, so the multiplicative unit takes features from only this two maps.",
      "tokens": [
        "In",
        "some",
        "cases",
        ",",
        "we",
        "only",
        "care",
        "about",
        "how",
        "one",
        "map",
        "transforms",
        "into",
        "another",
        ",",
        "so",
        "the",
        "multiplicative",
        "unit",
        "takes",
        "features",
        "from",
        "only",
        "this",
        "two",
        "maps",
        "."
      ]
    },
    {
      "sentence": "In other cases, however, more than two maps are strongly related, so it is reasonable to make multiplicative unit learn relations among more input maps, in other words, to find the optimal relational-order(number of related input maps that the multiplicative unit extracts features from) of each unit.",
      "tokens": [
        "In",
        "other",
        "cases",
        ",",
        "however",
        ",",
        "more",
        "than",
        "two",
        "maps",
        "are",
        "strongly",
        "related",
        ",",
        "so",
        "it",
        "is",
        "reasonable",
        "to",
        "make",
        "multiplicative",
        "unit",
        "learn",
        "relations",
        "among",
        "more",
        "input",
        "maps",
        ",",
        "in",
        "other",
        "words",
        ",",
        "to",
        "find",
        "the",
        "optimal",
        "relational-order",
        "(",
        "number",
        "of",
        "related",
        "input",
        "maps",
        "that",
        "the",
        "multiplicative",
        "unit",
        "extracts",
        "features",
        "from",
        ")",
        "of",
        "each",
        "unit",
        "."
      ]
    },
    {
      "sentence": "In order to enable our machine to learn relational order, we developed a reinforcement-learning method whose optimality is proven to train the network.",
      "tokens": [
        "In",
        "order",
        "to",
        "enable",
        "our",
        "machine",
        "to",
        "learn",
        "relational",
        "order",
        ",",
        "we",
        "developed",
        "a",
        "reinforcement-learning",
        "method",
        "whose",
        "optimality",
        "is",
        "proven",
        "to",
        "train",
        "the",
        "network",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Ⅰ.",
      "tokens": [
        "Ⅰ",
        "."
      ]
    },
    {
      "sentence": "INTRODUCTION Unsupervised learning combined with deep architecture has unveiled part of the mystery of artificial intelligence.",
      "tokens": [
        "INTRODUCTION",
        "Unsupervised",
        "learning",
        "combined",
        "with",
        "deep",
        "architecture",
        "has",
        "unveiled",
        "part",
        "of",
        "the",
        "mystery",
        "of",
        "artificial",
        "intelligence",
        "."
      ]
    },
    {
      "sentence": "Such learning techniques have boarder applications in areas like visual recognition, natural language processing, audio detection, and cognitive analysis.",
      "tokens": [
        "Such",
        "learning",
        "techniques",
        "have",
        "boarder",
        "applications",
        "in",
        "areas",
        "like",
        "visual",
        "recognition",
        ",",
        "natural",
        "language",
        "processing",
        ",",
        "audio",
        "detection",
        ",",
        "and",
        "cognitive",
        "analysis",
        "."
      ]
    },
    {
      "sentence": "With growing computational capabilities, deep learning framework like convolutional deep belief network [1] can bring more contribution to cognitive science.",
      "tokens": [
        "With",
        "growing",
        "computational",
        "capabilities",
        ",",
        "deep",
        "learning",
        "framework",
        "like",
        "convolutional",
        "deep",
        "belief",
        "network",
        "[",
        "1",
        "]",
        "can",
        "bring",
        "more",
        "contribution",
        "to",
        "cognitive",
        "science",
        "."
      ]
    },
    {
      "sentence": "Recently, researchers begin to take the next step, trying to develop model that can handle Manuscript received October 16, 2016 Zizhuang Wang is with Xiaogan Senior High School, China (email: 1012125144@qq.com ; web: http://kingofspace0wzz.github.io/ ) time dependent learning tasks.",
      "tokens": [
        "Recently",
        ",",
        "researchers",
        "begin",
        "to",
        "take",
        "the",
        "next",
        "step",
        ",",
        "trying",
        "to",
        "develop",
        "model",
        "that",
        "can",
        "handle",
        "Manuscript",
        "received",
        "October",
        "16",
        ",",
        "2016",
        "Zizhuang",
        "Wang",
        "is",
        "with",
        "Xiaogan",
        "Senior",
        "High",
        "School",
        ",",
        "China",
        "(",
        "email",
        ":",
        "1012125144",
        "@",
        "qq.com",
        ";",
        "web",
        ":",
        "http",
        ":",
        "//kingofspace0wzz.github.io/",
        ")",
        "time",
        "dependent",
        "learning",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "Traditionally, recurrent neural network(RNN) has shown its efficiency in time-dependent recognition problems.",
      "tokens": [
        "Traditionally",
        ",",
        "recurrent",
        "neural",
        "network",
        "(",
        "RNN",
        ")",
        "has",
        "shown",
        "its",
        "efficiency",
        "in",
        "time-dependent",
        "recognition",
        "problems",
        "."
      ]
    },
    {
      "sentence": "For instance, RNN is widely used as an elegant framework to manipulate audio model [2] , which is based on time-related inputs.",
      "tokens": [
        "For",
        "instance",
        ",",
        "RNN",
        "is",
        "widely",
        "used",
        "as",
        "an",
        "elegant",
        "framework",
        "to",
        "manipulate",
        "audio",
        "model",
        "[",
        "2",
        "]",
        ",",
        "which",
        "is",
        "based",
        "on",
        "time-related",
        "inputs",
        "."
      ]
    },
    {
      "sentence": "Recent researches also combined RNN with the power of convolutional restricted boltzmann machine(CRBM), such as gated autocoder and factorized CRBM [3] [4] .",
      "tokens": [
        "Recent",
        "researches",
        "also",
        "combined",
        "RNN",
        "with",
        "the",
        "power",
        "of",
        "convolutional",
        "restricted",
        "boltzmann",
        "machine",
        "(",
        "CRBM",
        ")",
        ",",
        "such",
        "as",
        "gated",
        "autocoder",
        "and",
        "factorized",
        "CRBM",
        "[",
        "3",
        "]",
        "[",
        "4",
        "]",
        "."
      ]
    },
    {
      "sentence": "Later, more attention has been driven to a special form of time-based model, in which a restricted boltzmann machine is built to learn feature transformations that describe relations between time-related input maps.",
      "tokens": [
        "Later",
        ",",
        "more",
        "attention",
        "has",
        "been",
        "driven",
        "to",
        "a",
        "special",
        "form",
        "of",
        "time-based",
        "model",
        ",",
        "in",
        "which",
        "a",
        "restricted",
        "boltzmann",
        "machine",
        "is",
        "built",
        "to",
        "learn",
        "feature",
        "transformations",
        "that",
        "describe",
        "relations",
        "between",
        "time-related",
        "input",
        "maps",
        "."
      ]
    },
    {
      "sentence": "In the model, which is similar to conventional RBM [5] [6], a hidden stack of layers is constructed to describe conditional probabilistic distributions over inputs.",
      "tokens": [
        "In",
        "the",
        "model",
        ",",
        "which",
        "is",
        "similar",
        "to",
        "conventional",
        "RBM",
        "[",
        "5",
        "]",
        "[",
        "6",
        "]",
        ",",
        "a",
        "hidden",
        "stack",
        "of",
        "layers",
        "is",
        "constructed",
        "to",
        "describe",
        "conditional",
        "probabilistic",
        "distributions",
        "over",
        "inputs",
        "."
      ]
    },
    {
      "sentence": "The difference from traditional model is that the hidden layers take into account of several input maps at different time moment.",
      "tokens": [
        "The",
        "difference",
        "from",
        "traditional",
        "model",
        "is",
        "that",
        "the",
        "hidden",
        "layers",
        "take",
        "into",
        "account",
        "of",
        "several",
        "input",
        "maps",
        "at",
        "different",
        "time",
        "moment",
        "."
      ]
    },
    {
      "sentence": "That is , the hidden layers are able to extract features from a combination of several time-related maps.",
      "tokens": [
        "That",
        "is",
        ",",
        "the",
        "hidden",
        "layers",
        "are",
        "able",
        "to",
        "extract",
        "features",
        "from",
        "a",
        "combination",
        "of",
        "several",
        "time-related",
        "maps",
        "."
      ]
    },
    {
      "sentence": "By doing so, we can extract features that are explained by hidden layers to represent correlation between inputs maps, in other words, to describe a matrix transformation from one map to another.",
      "tokens": [
        "By",
        "doing",
        "so",
        ",",
        "we",
        "can",
        "extract",
        "features",
        "that",
        "are",
        "explained",
        "by",
        "hidden",
        "layers",
        "to",
        "represent",
        "correlation",
        "between",
        "inputs",
        "maps",
        ",",
        "in",
        "other",
        "words",
        ",",
        "to",
        "describe",
        "a",
        "matrix",
        "transformation",
        "from",
        "one",
        "map",
        "to",
        "another",
        "."
      ]
    },
    {
      "sentence": "Previous works [7] [8][9] [10] noted that multiplicative interaction is an effective way to correlate input maps.",
      "tokens": [
        "Previous",
        "works",
        "[",
        "7",
        "]",
        "[",
        "8",
        "]",
        "[",
        "9",
        "]",
        "[",
        "10",
        "]",
        "noted",
        "that",
        "multiplicative",
        "interaction",
        "is",
        "an",
        "effective",
        "way",
        "to",
        "correlate",
        "input",
        "maps",
        "."
      ]
    },
    {
      "sentence": "We use this method in our model to combine related inputs.",
      "tokens": [
        "We",
        "use",
        "this",
        "method",
        "in",
        "our",
        "model",
        "to",
        "combine",
        "related",
        "inputs",
        "."
      ]
    },
    {
      "sentence": "With the learned hidden features, or transformation, our machine can predict later inputs based on conditional distributions that are learned and carried by the hidden layers of RBM.",
      "tokens": [
        "With",
        "the",
        "learned",
        "hidden",
        "features",
        ",",
        "or",
        "transformation",
        ",",
        "our",
        "machine",
        "can",
        "predict",
        "later",
        "inputs",
        "based",
        "on",
        "conditional",
        "distributions",
        "that",
        "are",
        "learned",
        "and",
        "carried",
        "by",
        "the",
        "hidden",
        "layers",
        "of",
        "RBM",
        "."
      ]
    },
    {
      "sentence": "Also, with the power of high-order temporal dependencies that is describe by [10] , we can learn features that are even more abstract.",
      "tokens": [
        "Also",
        ",",
        "with",
        "the",
        "power",
        "of",
        "high-order",
        "temporal",
        "dependencies",
        "that",
        "is",
        "describe",
        "by",
        "[",
        "10",
        "]",
        ",",
        "we",
        "can",
        "learn",
        "features",
        "that",
        "are",
        "even",
        "more",
        "abstract",
        "."
      ]
    },
    {
      "sentence": "In orther words, we can learn the features of transformations of input maps.",
      "tokens": [
        "In",
        "orther",
        "words",
        ",",
        "we",
        "can",
        "learn",
        "the",
        "features",
        "of",
        "transformations",
        "of",
        "input",
        "maps",
        "."
      ]
    },
    {
      "sentence": "This can be achieved with more hidden layers to be constructed and through learning efficiency brought by deep learning architecture.",
      "tokens": [
        "This",
        "can",
        "be",
        "achieved",
        "with",
        "more",
        "hidden",
        "layers",
        "to",
        "be",
        "constructed",
        "and",
        "through",
        "learning",
        "efficiency",
        "brought",
        "by",
        "deep",
        "learning",
        "architecture",
        "."
      ]
    },
    {
      "sentence": "There are limitations in higher order temporal model.",
      "tokens": [
        "There",
        "are",
        "limitations",
        "in",
        "higher",
        "order",
        "temporal",
        "model",
        "."
      ]
    },
    {
      "sentence": "Since conventional multiplicative interaction only takes into account of two related maps, it lacks the ability to correlate more input maps (input maps sequence for instance) and therefore can only learn features from two related inputs.",
      "tokens": [
        "Since",
        "conventional",
        "multiplicative",
        "interaction",
        "only",
        "takes",
        "into",
        "account",
        "of",
        "two",
        "related",
        "maps",
        ",",
        "it",
        "lacks",
        "the",
        "ability",
        "to",
        "correlate",
        "more",
        "input",
        "maps",
        "(",
        "input",
        "maps",
        "sequence",
        "for",
        "instance",
        ")",
        "and",
        "therefore",
        "can",
        "only",
        "learn",
        "features",
        "from",
        "two",
        "related",
        "inputs",
        "."
      ]
    },
    {
      "sentence": "Theoretically, we can learn temporal dependence among inputs no matter how far they are through high-order training process.",
      "tokens": [
        "Theoretically",
        ",",
        "we",
        "can",
        "learn",
        "temporal",
        "dependence",
        "among",
        "inputs",
        "no",
        "matter",
        "how",
        "far",
        "they",
        "are",
        "through",
        "high-order",
        "training",
        "process",
        "."
      ]
    },
    {
      "sentence": "Thus, combining only two input maps through multiplicative interaction seems to be achievable and efficient.",
      "tokens": [
        "Thus",
        ",",
        "combining",
        "only",
        "two",
        "input",
        "maps",
        "through",
        "multiplicative",
        "interaction",
        "seems",
        "to",
        "be",
        "achievable",
        "and",
        "efficient",
        "."
      ]
    },
    {
      "sentence": "In practice, however, this will cause the number of layers and parameters that are needed to be learned to explode.",
      "tokens": [
        "In",
        "practice",
        ",",
        "however",
        ",",
        "this",
        "will",
        "cause",
        "the",
        "number",
        "of",
        "layers",
        "and",
        "parameters",
        "that",
        "are",
        "needed",
        "to",
        "be",
        "learned",
        "to",
        "explode",
        "."
      ]
    },
    {
      "sentence": "Moreover, in some cases in which we may have a large amount of similar or strongly correlated Temporal-related convolutional-Restricted-Boltzmann-Machine capable of learning relational order via reinforcement learning procedure Zizhuang Wang input maps sequence, it is wise to combine them all together and to use only one stack of hidden layers to describe their correlation, saving a lot of space for parameters and time for learning.",
      "tokens": [
        "Moreover",
        ",",
        "in",
        "some",
        "cases",
        "in",
        "which",
        "we",
        "may",
        "have",
        "a",
        "large",
        "amount",
        "of",
        "similar",
        "or",
        "strongly",
        "correlated",
        "Temporal-related",
        "convolutional-Restricted-Boltzmann-Machine",
        "capable",
        "of",
        "learning",
        "relational",
        "order",
        "via",
        "reinforcement",
        "learning",
        "procedure",
        "Zizhuang",
        "Wang",
        "input",
        "maps",
        "sequence",
        ",",
        "it",
        "is",
        "wise",
        "to",
        "combine",
        "them",
        "all",
        "together",
        "and",
        "to",
        "use",
        "only",
        "one",
        "stack",
        "of",
        "hidden",
        "layers",
        "to",
        "describe",
        "their",
        "correlation",
        ",",
        "saving",
        "a",
        "lot",
        "of",
        "space",
        "for",
        "parameters",
        "and",
        "time",
        "for",
        "learning",
        "."
      ]
    },
    {
      "sentence": "To do this, we define the term \"relational order\" as the number of maps that one stack of hidden layers learn features from, in other words, the number of maps that we correlate with each other using multiplicative interactions.",
      "tokens": [
        "To",
        "do",
        "this",
        ",",
        "we",
        "define",
        "the",
        "term",
        "``",
        "relational",
        "order",
        "''",
        "as",
        "the",
        "number",
        "of",
        "maps",
        "that",
        "one",
        "stack",
        "of",
        "hidden",
        "layers",
        "learn",
        "features",
        "from",
        ",",
        "in",
        "other",
        "words",
        ",",
        "the",
        "number",
        "of",
        "maps",
        "that",
        "we",
        "correlate",
        "with",
        "each",
        "other",
        "using",
        "multiplicative",
        "interactions",
        "."
      ]
    },
    {
      "sentence": "Finally, we developed a reinforcement-learning [11] based method to learn the relational order at each time by minimizing reconstruction error.",
      "tokens": [
        "Finally",
        ",",
        "we",
        "developed",
        "a",
        "reinforcement-learning",
        "[",
        "11",
        "]",
        "based",
        "method",
        "to",
        "learn",
        "the",
        "relational",
        "order",
        "at",
        "each",
        "time",
        "by",
        "minimizing",
        "reconstruction",
        "error",
        "."
      ]
    },
    {
      "sentence": "We then proved that it satisfies the sub-problem structure of dynamic programming [12][13] [14] .",
      "tokens": [
        "We",
        "then",
        "proved",
        "that",
        "it",
        "satisfies",
        "the",
        "sub-problem",
        "structure",
        "of",
        "dynamic",
        "programming",
        "[",
        "12",
        "]",
        "[",
        "13",
        "]",
        "[",
        "14",
        "]",
        "."
      ]
    },
    {
      "sentence": "Therefore, by finding the optimized reconstruction error at each group of related input maps, we get the globally optimal solution of the entire input sequence.",
      "tokens": [
        "Therefore",
        ",",
        "by",
        "finding",
        "the",
        "optimized",
        "reconstruction",
        "error",
        "at",
        "each",
        "group",
        "of",
        "related",
        "input",
        "maps",
        ",",
        "we",
        "get",
        "the",
        "globally",
        "optimal",
        "solution",
        "of",
        "the",
        "entire",
        "input",
        "sequence",
        "."
      ]
    }
  ]
}