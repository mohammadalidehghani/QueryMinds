{
  "title": [
    {
      "sentence": "Tuning Learning Rates with the Cumulative-Learning Constant",
      "tokens": [
        "Tuning",
        "Learning",
        "Rates",
        "with",
        "the",
        "Cumulative-Learning",
        "Constant"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "This paper introduces a novel method for optimizing learning rates in machine learning.",
      "tokens": [
        "This",
        "paper",
        "introduces",
        "a",
        "novel",
        "method",
        "for",
        "optimizing",
        "learning",
        "rates",
        "in",
        "machine",
        "learning",
        "."
      ]
    },
    {
      "sentence": "A previously unrecognized proportionality between learning rates and dataset sizes is discovered, providing valuable insights into how dataset scale influences training dynamics.",
      "tokens": [
        "A",
        "previously",
        "unrecognized",
        "proportionality",
        "between",
        "learning",
        "rates",
        "and",
        "dataset",
        "sizes",
        "is",
        "discovered",
        ",",
        "providing",
        "valuable",
        "insights",
        "into",
        "how",
        "dataset",
        "scale",
        "influences",
        "training",
        "dynamics",
        "."
      ]
    },
    {
      "sentence": "Additionally, a cumulative learning constant is identified, offering a framework for designing and optimizing advanced learning rate schedules.",
      "tokens": [
        "Additionally",
        ",",
        "a",
        "cumulative",
        "learning",
        "constant",
        "is",
        "identified",
        ",",
        "offering",
        "a",
        "framework",
        "for",
        "designing",
        "and",
        "optimizing",
        "advanced",
        "learning",
        "rate",
        "schedules",
        "."
      ]
    },
    {
      "sentence": "These findings have the potential to enhance training efficiency and performance across a wide range of machine learning applications.",
      "tokens": [
        "These",
        "findings",
        "have",
        "the",
        "potential",
        "to",
        "enhance",
        "training",
        "efficiency",
        "and",
        "performance",
        "across",
        "a",
        "wide",
        "range",
        "of",
        "machine",
        "learning",
        "applications",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Introduction The learning rate is a deep learning hyperparameter that is very important for training models efficiently.",
      "tokens": [
        "Introduction",
        "The",
        "learning",
        "rate",
        "is",
        "a",
        "deep",
        "learning",
        "hyperparameter",
        "that",
        "is",
        "very",
        "important",
        "for",
        "training",
        "models",
        "efficiently",
        "."
      ]
    },
    {
      "sentence": "There has been a lot of research done on optimizing learning rates including sophisticated methods for adaptive learning rates as well as alternating learning rates (Wu et al., 2019) .",
      "tokens": [
        "There",
        "has",
        "been",
        "a",
        "lot",
        "of",
        "research",
        "done",
        "on",
        "optimizing",
        "learning",
        "rates",
        "including",
        "sophisticated",
        "methods",
        "for",
        "adaptive",
        "learning",
        "rates",
        "as",
        "well",
        "as",
        "alternating",
        "learning",
        "rates",
        "(",
        "Wu",
        "et",
        "al.",
        ",",
        "2019",
        ")",
        "."
      ]
    },
    {
      "sentence": "However, the learning rate selection is arbitrary -An optimal learning rate is found without understanding due to the belief that the learning rate is too sensitive to changes in other hyperparameters within the model.",
      "tokens": [
        "However",
        ",",
        "the",
        "learning",
        "rate",
        "selection",
        "is",
        "arbitrary",
        "-An",
        "optimal",
        "learning",
        "rate",
        "is",
        "found",
        "without",
        "understanding",
        "due",
        "to",
        "the",
        "belief",
        "that",
        "the",
        "learning",
        "rate",
        "is",
        "too",
        "sensitive",
        "to",
        "changes",
        "in",
        "other",
        "hyperparameters",
        "within",
        "the",
        "model",
        "."
      ]
    },
    {
      "sentence": "However, this paper will present that the learning rate follows some strict proportionality laws.",
      "tokens": [
        "However",
        ",",
        "this",
        "paper",
        "will",
        "present",
        "that",
        "the",
        "learning",
        "rate",
        "follows",
        "some",
        "strict",
        "proportionality",
        "laws",
        "."
      ]
    },
    {
      "sentence": "Previous work has been done to compare proportionality of learning rates in reference to batch size (Diego et al., 2020) .",
      "tokens": [
        "Previous",
        "work",
        "has",
        "been",
        "done",
        "to",
        "compare",
        "proportionality",
        "of",
        "learning",
        "rates",
        "in",
        "reference",
        "to",
        "batch",
        "size",
        "(",
        "Diego",
        "et",
        "al.",
        ",",
        "2020",
        ")",
        "."
      ]
    },
    {
      "sentence": "Therefore, this paper will not be exploring how batch size affects the optimal learning rate.",
      "tokens": [
        "Therefore",
        ",",
        "this",
        "paper",
        "will",
        "not",
        "be",
        "exploring",
        "how",
        "batch",
        "size",
        "affects",
        "the",
        "optimal",
        "learning",
        "rate",
        "."
      ]
    }
  ]
}