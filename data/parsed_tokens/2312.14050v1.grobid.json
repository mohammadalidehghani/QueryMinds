{
  "title": [
    {
      "sentence": "MACHINE LEARNING AND DOMAIN DECOMPOSITION METHODS -A SURVEY",
      "tokens": [
        "MACHINE",
        "LEARNING",
        "AND",
        "DOMAIN",
        "DECOMPOSITION",
        "METHODS",
        "-A",
        "SURVEY"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "Hybrid algorithms, which combine black-box machine learning methods with experience from traditional numerical methods and domain expertise from diverse application areas, are progressively gaining importance in scientific machine learning and various industrial domains, especially in computational science and engineering.",
      "tokens": [
        "Hybrid",
        "algorithms",
        ",",
        "which",
        "combine",
        "black-box",
        "machine",
        "learning",
        "methods",
        "with",
        "experience",
        "from",
        "traditional",
        "numerical",
        "methods",
        "and",
        "domain",
        "expertise",
        "from",
        "diverse",
        "application",
        "areas",
        ",",
        "are",
        "progressively",
        "gaining",
        "importance",
        "in",
        "scientific",
        "machine",
        "learning",
        "and",
        "various",
        "industrial",
        "domains",
        ",",
        "especially",
        "in",
        "computational",
        "science",
        "and",
        "engineering",
        "."
      ]
    },
    {
      "sentence": "In the present survey, several promising avenues of research will be examined which focus on the combination of machine learning (ML) and domain decomposition methods (DDMs).",
      "tokens": [
        "In",
        "the",
        "present",
        "survey",
        ",",
        "several",
        "promising",
        "avenues",
        "of",
        "research",
        "will",
        "be",
        "examined",
        "which",
        "focus",
        "on",
        "the",
        "combination",
        "of",
        "machine",
        "learning",
        "(",
        "ML",
        ")",
        "and",
        "domain",
        "decomposition",
        "methods",
        "(",
        "DDMs",
        ")",
        "."
      ]
    },
    {
      "sentence": "The aim of this survey is to provide an overview of existing work within this field and to structure it into domain decomposition for machine learning and machine learning-enhanced domain decomposition, including: domain decomposition for classical machine learning, domain decomposition to accelerate the training of physics-aware neural networks, machine learning to enhance the convergence properties or computational efficiency of DDMs, and machine learning as a discretization method in a DDM for the solution of PDEs.",
      "tokens": [
        "The",
        "aim",
        "of",
        "this",
        "survey",
        "is",
        "to",
        "provide",
        "an",
        "overview",
        "of",
        "existing",
        "work",
        "within",
        "this",
        "field",
        "and",
        "to",
        "structure",
        "it",
        "into",
        "domain",
        "decomposition",
        "for",
        "machine",
        "learning",
        "and",
        "machine",
        "learning-enhanced",
        "domain",
        "decomposition",
        ",",
        "including",
        ":",
        "domain",
        "decomposition",
        "for",
        "classical",
        "machine",
        "learning",
        ",",
        "domain",
        "decomposition",
        "to",
        "accelerate",
        "the",
        "training",
        "of",
        "physics-aware",
        "neural",
        "networks",
        ",",
        "machine",
        "learning",
        "to",
        "enhance",
        "the",
        "convergence",
        "properties",
        "or",
        "computational",
        "efficiency",
        "of",
        "DDMs",
        ",",
        "and",
        "machine",
        "learning",
        "as",
        "a",
        "discretization",
        "method",
        "in",
        "a",
        "DDM",
        "for",
        "the",
        "solution",
        "of",
        "PDEs",
        "."
      ]
    },
    {
      "sentence": "In each of these fields, we summarize existing work and key advances within a common framework and, finally, disuss ongoing challenges and opportunities for future research.",
      "tokens": [
        "In",
        "each",
        "of",
        "these",
        "fields",
        ",",
        "we",
        "summarize",
        "existing",
        "work",
        "and",
        "key",
        "advances",
        "within",
        "a",
        "common",
        "framework",
        "and",
        ",",
        "finally",
        ",",
        "disuss",
        "ongoing",
        "challenges",
        "and",
        "opportunities",
        "for",
        "future",
        "research",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "1.",
      "tokens": [
        "1",
        "."
      ]
    },
    {
      "sentence": "Introduction.",
      "tokens": [
        "Introduction",
        "."
      ]
    },
    {
      "sentence": "Domain decomposition methods (DDMs) are divide-and-conquer strategies which decompose a given problem into a number of smaller subproblems.",
      "tokens": [
        "Domain",
        "decomposition",
        "methods",
        "(",
        "DDMs",
        ")",
        "are",
        "divide-and-conquer",
        "strategies",
        "which",
        "decompose",
        "a",
        "given",
        "problem",
        "into",
        "a",
        "number",
        "of",
        "smaller",
        "subproblems",
        "."
      ]
    },
    {
      "sentence": "This strategy often leads to easier parallelizable algorithms where the subproblems can be solved on different processors (cpu or gpu).",
      "tokens": [
        "This",
        "strategy",
        "often",
        "leads",
        "to",
        "easier",
        "parallelizable",
        "algorithms",
        "where",
        "the",
        "subproblems",
        "can",
        "be",
        "solved",
        "on",
        "different",
        "processors",
        "(",
        "cpu",
        "or",
        "gpu",
        ")",
        "."
      ]
    },
    {
      "sentence": "Sometimes, an additional problem is needed for parallel scalability, the so-called global problem, or to gather and connect local information obtained from the solution of the local subproblems.",
      "tokens": [
        "Sometimes",
        ",",
        "an",
        "additional",
        "problem",
        "is",
        "needed",
        "for",
        "parallel",
        "scalability",
        ",",
        "the",
        "so-called",
        "global",
        "problem",
        ",",
        "or",
        "to",
        "gather",
        "and",
        "connect",
        "local",
        "information",
        "obtained",
        "from",
        "the",
        "solution",
        "of",
        "the",
        "local",
        "subproblems",
        "."
      ]
    },
    {
      "sentence": "DDMs can be useful in different cases, for example, if the original problem is too large to be solved in the available memory, the computing time is too long on a single processor, or the decomposed problem has other preferable properties, for example, being better conditioned, yielding more accurate results, or boosting generalization properties and mitigating the spectral bias, respectively, of machine learning models.",
      "tokens": [
        "DDMs",
        "can",
        "be",
        "useful",
        "in",
        "different",
        "cases",
        ",",
        "for",
        "example",
        ",",
        "if",
        "the",
        "original",
        "problem",
        "is",
        "too",
        "large",
        "to",
        "be",
        "solved",
        "in",
        "the",
        "available",
        "memory",
        ",",
        "the",
        "computing",
        "time",
        "is",
        "too",
        "long",
        "on",
        "a",
        "single",
        "processor",
        ",",
        "or",
        "the",
        "decomposed",
        "problem",
        "has",
        "other",
        "preferable",
        "properties",
        ",",
        "for",
        "example",
        ",",
        "being",
        "better",
        "conditioned",
        ",",
        "yielding",
        "more",
        "accurate",
        "results",
        ",",
        "or",
        "boosting",
        "generalization",
        "properties",
        "and",
        "mitigating",
        "the",
        "spectral",
        "bias",
        ",",
        "respectively",
        ",",
        "of",
        "machine",
        "learning",
        "models",
        "."
      ]
    },
    {
      "sentence": "DDMs have a long and successful history in constructing parallel scalable preconditioners for linear and nonlinear systems obtained from discretized partial differential equations.",
      "tokens": [
        "DDMs",
        "have",
        "a",
        "long",
        "and",
        "successful",
        "history",
        "in",
        "constructing",
        "parallel",
        "scalable",
        "preconditioners",
        "for",
        "linear",
        "and",
        "nonlinear",
        "systems",
        "obtained",
        "from",
        "discretized",
        "partial",
        "differential",
        "equations",
        "."
      ]
    },
    {
      "sentence": "Recently, there has been an increased interest in using DDMs in combination with machine learning algorithms, especially in the field of scientific machine learning (SciML).",
      "tokens": [
        "Recently",
        ",",
        "there",
        "has",
        "been",
        "an",
        "increased",
        "interest",
        "in",
        "using",
        "DDMs",
        "in",
        "combination",
        "with",
        "machine",
        "learning",
        "algorithms",
        ",",
        "especially",
        "in",
        "the",
        "field",
        "of",
        "scientific",
        "machine",
        "learning",
        "(",
        "SciML",
        ")",
        "."
      ]
    },
    {
      "sentence": "In recent years, the rapidly growing research area of SciML has drawn increasing interest and attention in various fields of applications, as, for example, in computational fluid mechanics [20, 26] , optics and electromagnetic applications [8, 55] , and earthquake detections [83] .",
      "tokens": [
        "In",
        "recent",
        "years",
        ",",
        "the",
        "rapidly",
        "growing",
        "research",
        "area",
        "of",
        "SciML",
        "has",
        "drawn",
        "increasing",
        "interest",
        "and",
        "attention",
        "in",
        "various",
        "fields",
        "of",
        "applications",
        ",",
        "as",
        ",",
        "for",
        "example",
        ",",
        "in",
        "computational",
        "fluid",
        "mechanics",
        "[",
        "20",
        ",",
        "26",
        "]",
        ",",
        "optics",
        "and",
        "electromagnetic",
        "applications",
        "[",
        "8",
        ",",
        "55",
        "]",
        ",",
        "and",
        "earthquake",
        "detections",
        "[",
        "83",
        "]",
        "."
      ]
    },
    {
      "sentence": "Originally, the term SciML was introduced and made publicly known by the report [1] prepared for the U.S. Department of Energy.",
      "tokens": [
        "Originally",
        ",",
        "the",
        "term",
        "SciML",
        "was",
        "introduced",
        "and",
        "made",
        "publicly",
        "known",
        "by",
        "the",
        "report",
        "[",
        "1",
        "]",
        "prepared",
        "for",
        "the",
        "U.S.",
        "Department",
        "of",
        "Energy",
        "."
      ]
    },
    {
      "sentence": "The core idea of SciML is to combine algorithms from supervised or unsupervised machine learning with other domain-specific methods or knowledge to develop new, hybrid methods.",
      "tokens": [
        "The",
        "core",
        "idea",
        "of",
        "SciML",
        "is",
        "to",
        "combine",
        "algorithms",
        "from",
        "supervised",
        "or",
        "unsupervised",
        "machine",
        "learning",
        "with",
        "other",
        "domain-specific",
        "methods",
        "or",
        "knowledge",
        "to",
        "develop",
        "new",
        ",",
        "hybrid",
        "methods",
        "."
      ]
    },
    {
      "sentence": "In particular, the authors of [1] have identified six priority research di- rections in SciML which are, among others, domain-awareness, interpretability, and robustness of the considered method.",
      "tokens": [
        "In",
        "particular",
        ",",
        "the",
        "authors",
        "of",
        "[",
        "1",
        "]",
        "have",
        "identified",
        "six",
        "priority",
        "research",
        "di-",
        "rections",
        "in",
        "SciML",
        "which",
        "are",
        ",",
        "among",
        "others",
        ",",
        "domain-awareness",
        ",",
        "interpretability",
        ",",
        "and",
        "robustness",
        "of",
        "the",
        "considered",
        "method",
        "."
      ]
    },
    {
      "sentence": "With respect to practical applications, this means that a key factor of SciML methods is to connect \"black-box\" approaches such as deep learning with well-understood and theoretical approaches from science and expert knowledge.",
      "tokens": [
        "With",
        "respect",
        "to",
        "practical",
        "applications",
        ",",
        "this",
        "means",
        "that",
        "a",
        "key",
        "factor",
        "of",
        "SciML",
        "methods",
        "is",
        "to",
        "connect",
        "``",
        "black-box",
        "''",
        "approaches",
        "such",
        "as",
        "deep",
        "learning",
        "with",
        "well-understood",
        "and",
        "theoretical",
        "approaches",
        "from",
        "science",
        "and",
        "expert",
        "knowledge",
        "."
      ]
    },
    {
      "sentence": "One framework that fits very well to the priority research directions of SciML are physics-informed neural networks (PINNs) [75] .",
      "tokens": [
        "One",
        "framework",
        "that",
        "fits",
        "very",
        "well",
        "to",
        "the",
        "priority",
        "research",
        "directions",
        "of",
        "SciML",
        "are",
        "physics-informed",
        "neural",
        "networks",
        "(",
        "PINNs",
        ")",
        "[",
        "75",
        "]",
        "."
      ]
    },
    {
      "sentence": "PINNs are a specific machine learning technique usually used to solve forward or inverse problems involving Partial Differential Equations (PDEs).",
      "tokens": [
        "PINNs",
        "are",
        "a",
        "specific",
        "machine",
        "learning",
        "technique",
        "usually",
        "used",
        "to",
        "solve",
        "forward",
        "or",
        "inverse",
        "problems",
        "involving",
        "Partial",
        "Differential",
        "Equations",
        "(",
        "PDEs",
        ")",
        "."
      ]
    },
    {
      "sentence": "Similar to other deep learning models, PINNs approximate PDE solutions by training a neural network such that a given loss function is minimized.",
      "tokens": [
        "Similar",
        "to",
        "other",
        "deep",
        "learning",
        "models",
        ",",
        "PINNs",
        "approximate",
        "PDE",
        "solutions",
        "by",
        "training",
        "a",
        "neural",
        "network",
        "such",
        "that",
        "a",
        "given",
        "loss",
        "function",
        "is",
        "minimized",
        "."
      ]
    },
    {
      "sentence": "However, the characteristic feature of PINNs is that knowledge with respect to the underlying physics behind the data is integrated into the loss function as well as, possibly, initial or boundary conditions of the computational domain's boundary.",
      "tokens": [
        "However",
        ",",
        "the",
        "characteristic",
        "feature",
        "of",
        "PINNs",
        "is",
        "that",
        "knowledge",
        "with",
        "respect",
        "to",
        "the",
        "underlying",
        "physics",
        "behind",
        "the",
        "data",
        "is",
        "integrated",
        "into",
        "the",
        "loss",
        "function",
        "as",
        "well",
        "as",
        ",",
        "possibly",
        ",",
        "initial",
        "or",
        "boundary",
        "conditions",
        "of",
        "the",
        "computational",
        "domain",
        "'s",
        "boundary",
        "."
      ]
    },
    {
      "sentence": "For a detailed overview of existing research and applications around PINNs as well as remaining challenges and open research questions, we refer to [11] .",
      "tokens": [
        "For",
        "a",
        "detailed",
        "overview",
        "of",
        "existing",
        "research",
        "and",
        "applications",
        "around",
        "PINNs",
        "as",
        "well",
        "as",
        "remaining",
        "challenges",
        "and",
        "open",
        "research",
        "questions",
        ",",
        "we",
        "refer",
        "to",
        "[",
        "11",
        "]",
        "."
      ]
    },
    {
      "sentence": "One drawback of PINNs and also other neural network architectures, in general, is the large computational complexity of the related optimization problem for large problem domains and multi-scale problems as well as the resulting long training times.",
      "tokens": [
        "One",
        "drawback",
        "of",
        "PINNs",
        "and",
        "also",
        "other",
        "neural",
        "network",
        "architectures",
        ",",
        "in",
        "general",
        ",",
        "is",
        "the",
        "large",
        "computational",
        "complexity",
        "of",
        "the",
        "related",
        "optimization",
        "problem",
        "for",
        "large",
        "problem",
        "domains",
        "and",
        "multi-scale",
        "problems",
        "as",
        "well",
        "as",
        "the",
        "resulting",
        "long",
        "training",
        "times",
        "."
      ]
    },
    {
      "sentence": "Hence, a wide range of attempts have been made to distribute and parallelize the training of different network architectures; see [5, 72, 91] for a comprehensive overview.",
      "tokens": [
        "Hence",
        ",",
        "a",
        "wide",
        "range",
        "of",
        "attempts",
        "have",
        "been",
        "made",
        "to",
        "distribute",
        "and",
        "parallelize",
        "the",
        "training",
        "of",
        "different",
        "network",
        "architectures",
        ";",
        "see",
        "[",
        "5",
        ",",
        "72",
        ",",
        "91",
        "]",
        "for",
        "a",
        "comprehensive",
        "overview",
        "."
      ]
    },
    {
      "sentence": "In general, the wide range of techniques used in parallel and distributed deep learning can roughly be categorized into data parallelism, that is, partitioning the input samples, model parallelism, that is, partitioning the network structure, and pipelining, that is, partitioning by layer; see also [5, Sect.",
      "tokens": [
        "In",
        "general",
        ",",
        "the",
        "wide",
        "range",
        "of",
        "techniques",
        "used",
        "in",
        "parallel",
        "and",
        "distributed",
        "deep",
        "learning",
        "can",
        "roughly",
        "be",
        "categorized",
        "into",
        "data",
        "parallelism",
        ",",
        "that",
        "is",
        ",",
        "partitioning",
        "the",
        "input",
        "samples",
        ",",
        "model",
        "parallelism",
        ",",
        "that",
        "is",
        ",",
        "partitioning",
        "the",
        "network",
        "structure",
        ",",
        "and",
        "pipelining",
        ",",
        "that",
        "is",
        ",",
        "partitioning",
        "by",
        "layer",
        ";",
        "see",
        "also",
        "[",
        "5",
        ",",
        "Sect",
        "."
      ]
    },
    {
      "sentence": "6] .",
      "tokens": [
        "6",
        "]",
        "."
      ]
    },
    {
      "sentence": "From an abstract perspective, model parallelism can be and often is interpreted as a form of DDMs [73, 90] .",
      "tokens": [
        "From",
        "an",
        "abstract",
        "perspective",
        ",",
        "model",
        "parallelism",
        "can",
        "be",
        "and",
        "often",
        "is",
        "interpreted",
        "as",
        "a",
        "form",
        "of",
        "DDMs",
        "[",
        "73",
        ",",
        "90",
        "]",
        "."
      ]
    },
    {
      "sentence": "In this paper, our aim is to review existing work on combining DDMs with different machine learning models.",
      "tokens": [
        "In",
        "this",
        "paper",
        ",",
        "our",
        "aim",
        "is",
        "to",
        "review",
        "existing",
        "work",
        "on",
        "combining",
        "DDMs",
        "with",
        "different",
        "machine",
        "learning",
        "models",
        "."
      ]
    },
    {
      "sentence": "Although data parallelism can be also interpreted as a form of domain decomposition such that the global data space is decomposed into smaller subspaces operating only on parts of the data, in the following, we focus Ω 1 Ω 2 Ω 3 Ω 4 Ω 5 Ω 6 Ω 7 Ω 8 Ω 9 δ δ δ δ Ω ′ 5 Ω 1 Ω 2 Ω 3 Ω 4 Ω 5 Ω 6 Ω 7 Ω 8 Ω 9 Γ Figure 2 .",
      "tokens": [
        "Although",
        "data",
        "parallelism",
        "can",
        "be",
        "also",
        "interpreted",
        "as",
        "a",
        "form",
        "of",
        "domain",
        "decomposition",
        "such",
        "that",
        "the",
        "global",
        "data",
        "space",
        "is",
        "decomposed",
        "into",
        "smaller",
        "subspaces",
        "operating",
        "only",
        "on",
        "parts",
        "of",
        "the",
        "data",
        ",",
        "in",
        "the",
        "following",
        ",",
        "we",
        "focus",
        "Ω",
        "1",
        "Ω",
        "2",
        "Ω",
        "3",
        "Ω",
        "4",
        "Ω",
        "5",
        "Ω",
        "6",
        "Ω",
        "7",
        "Ω",
        "8",
        "Ω",
        "9",
        "δ",
        "δ",
        "δ",
        "δ",
        "Ω",
        "′",
        "5",
        "Ω",
        "1",
        "Ω",
        "2",
        "Ω",
        "3",
        "Ω",
        "4",
        "Ω",
        "5",
        "Ω",
        "6",
        "Ω",
        "7",
        "Ω",
        "8",
        "Ω",
        "9",
        "Γ",
        "Figure",
        "2",
        "."
      ]
    },
    {
      "sentence": "Decomposition of a domain Ω ⊂ R 2 into nine nonoverlapping subdomains Ω i , i = 1, ..., 9.",
      "tokens": [
        "Decomposition",
        "of",
        "a",
        "domain",
        "Ω",
        "⊂",
        "R",
        "2",
        "into",
        "nine",
        "nonoverlapping",
        "subdomains",
        "Ω",
        "i",
        ",",
        "i",
        "=",
        "1",
        ",",
        "...",
        ",",
        "9",
        "."
      ]
    },
    {
      "sentence": "Left: The overlapping domain decomposition is obtained by extending the nonoverlapping subdomains by the width δ.",
      "tokens": [
        "Left",
        ":",
        "The",
        "overlapping",
        "domain",
        "decomposition",
        "is",
        "obtained",
        "by",
        "extending",
        "the",
        "nonoverlapping",
        "subdomains",
        "by",
        "the",
        "width",
        "δ",
        "."
      ]
    },
    {
      "sentence": "The resulting overlapping subdomain Ω ′ 5 is marked in blue.",
      "tokens": [
        "The",
        "resulting",
        "overlapping",
        "subdomain",
        "Ω",
        "′",
        "5",
        "is",
        "marked",
        "in",
        "blue",
        "."
      ]
    },
    {
      "sentence": "Right: The interface Γ is marked in red.",
      "tokens": [
        "Right",
        ":",
        "The",
        "interface",
        "Γ",
        "is",
        "marked",
        "in",
        "red",
        "."
      ]
    },
    {
      "sentence": "on model parallelism and pipelining when using domain decomposition in machine learning algorithms.",
      "tokens": [
        "on",
        "model",
        "parallelism",
        "and",
        "pipelining",
        "when",
        "using",
        "domain",
        "decomposition",
        "in",
        "machine",
        "learning",
        "algorithms",
        "."
      ]
    },
    {
      "sentence": "The remainder of the paper is organized as follows.",
      "tokens": [
        "The",
        "remainder",
        "of",
        "the",
        "paper",
        "is",
        "organized",
        "as",
        "follows",
        "."
      ]
    },
    {
      "sentence": "In section 2, we provide a general description of DDMs for the solution of PDEs and introduce some necessary notation.",
      "tokens": [
        "In",
        "section",
        "2",
        ",",
        "we",
        "provide",
        "a",
        "general",
        "description",
        "of",
        "DDMs",
        "for",
        "the",
        "solution",
        "of",
        "PDEs",
        "and",
        "introduce",
        "some",
        "necessary",
        "notation",
        "."
      ]
    },
    {
      "sentence": "Subsequently, in section 3, we briefly summarize the main idea and central algorithmic formulae of multilayer perceptrons, PINNs, and the Deep Ritz method.",
      "tokens": [
        "Subsequently",
        ",",
        "in",
        "section",
        "3",
        ",",
        "we",
        "briefly",
        "summarize",
        "the",
        "main",
        "idea",
        "and",
        "central",
        "algorithmic",
        "formulae",
        "of",
        "multilayer",
        "perceptrons",
        ",",
        "PINNs",
        ",",
        "and",
        "the",
        "Deep",
        "Ritz",
        "method",
        "."
      ]
    },
    {
      "sentence": "Note that both, section 2 and section 3, can be skipped by the experienced reader who is already familiar with DDMs and PINNs and who can directly proceed with section 4.",
      "tokens": [
        "Note",
        "that",
        "both",
        ",",
        "section",
        "2",
        "and",
        "section",
        "3",
        ",",
        "can",
        "be",
        "skipped",
        "by",
        "the",
        "experienced",
        "reader",
        "who",
        "is",
        "already",
        "familiar",
        "with",
        "DDMs",
        "and",
        "PINNs",
        "and",
        "who",
        "can",
        "directly",
        "proceed",
        "with",
        "section",
        "4",
        "."
      ]
    },
    {
      "sentence": "In the present paper, we categorize all cited work into two main classes: i) Methods using domain decomposition within machine learning models (section 4) ii) Machine learning-enhanced DDMs (section 5).",
      "tokens": [
        "In",
        "the",
        "present",
        "paper",
        ",",
        "we",
        "categorize",
        "all",
        "cited",
        "work",
        "into",
        "two",
        "main",
        "classes",
        ":",
        "i",
        ")",
        "Methods",
        "using",
        "domain",
        "decomposition",
        "within",
        "machine",
        "learning",
        "models",
        "(",
        "section",
        "4",
        ")",
        "ii",
        ")",
        "Machine",
        "learning-enhanced",
        "DDMs",
        "(",
        "section",
        "5",
        ")",
        "."
      ]
    },
    {
      "sentence": "Additionally, we classify all methods in group i) into two subgroups.",
      "tokens": [
        "Additionally",
        ",",
        "we",
        "classify",
        "all",
        "methods",
        "in",
        "group",
        "i",
        ")",
        "into",
        "two",
        "subgroups",
        "."
      ]
    },
    {
      "sentence": "First, we consider all approaches using domain decomposition for the acceleration and parallel training of PINNs; see subsection 4.1.",
      "tokens": [
        "First",
        ",",
        "we",
        "consider",
        "all",
        "approaches",
        "using",
        "domain",
        "decomposition",
        "for",
        "the",
        "acceleration",
        "and",
        "parallel",
        "training",
        "of",
        "PINNs",
        ";",
        "see",
        "subsection",
        "4.1",
        "."
      ]
    },
    {
      "sentence": "Second, in subsection 4.2, we consider approaches using domain decomposition in machine learning algorithms other than PINNs, in particular, classical supervised and unsupervised algorithms.",
      "tokens": [
        "Second",
        ",",
        "in",
        "subsection",
        "4.2",
        ",",
        "we",
        "consider",
        "approaches",
        "using",
        "domain",
        "decomposition",
        "in",
        "machine",
        "learning",
        "algorithms",
        "other",
        "than",
        "PINNs",
        ",",
        "in",
        "particular",
        ",",
        "classical",
        "supervised",
        "and",
        "unsupervised",
        "algorithms",
        "."
      ]
    },
    {
      "sentence": "With respect to work within class ii), one can also subdivide the approaches into two subclasses; see also [38] .",
      "tokens": [
        "With",
        "respect",
        "to",
        "work",
        "within",
        "class",
        "ii",
        ")",
        ",",
        "one",
        "can",
        "also",
        "subdivide",
        "the",
        "approaches",
        "into",
        "two",
        "subclasses",
        ";",
        "see",
        "also",
        "[",
        "38",
        "]",
        "."
      ]
    },
    {
      "sentence": "In this case, the first class consists of methods where machine learning is used to improve the convergence properties or the computational efficiency within classical DDMs.",
      "tokens": [
        "In",
        "this",
        "case",
        ",",
        "the",
        "first",
        "class",
        "consists",
        "of",
        "methods",
        "where",
        "machine",
        "learning",
        "is",
        "used",
        "to",
        "improve",
        "the",
        "convergence",
        "properties",
        "or",
        "the",
        "computational",
        "efficiency",
        "within",
        "classical",
        "DDMs",
        "."
      ]
    },
    {
      "sentence": "This is typically done by learning or approximating optimal interface conditions or by learning other optimal parameters; see subsection 5.1.",
      "tokens": [
        "This",
        "is",
        "typically",
        "done",
        "by",
        "learning",
        "or",
        "approximating",
        "optimal",
        "interface",
        "conditions",
        "or",
        "by",
        "learning",
        "other",
        "optimal",
        "parameters",
        ";",
        "see",
        "subsection",
        "5.1",
        "."
      ]
    },
    {
      "sentence": "In the second class, different types of neural networks are used as discretization methods and replace classical local subdomain or coarse solvers based on finite elements or finite differences; see subsection 5.2.",
      "tokens": [
        "In",
        "the",
        "second",
        "class",
        ",",
        "different",
        "types",
        "of",
        "neural",
        "networks",
        "are",
        "used",
        "as",
        "discretization",
        "methods",
        "and",
        "replace",
        "classical",
        "local",
        "subdomain",
        "or",
        "coarse",
        "solvers",
        "based",
        "on",
        "finite",
        "elements",
        "or",
        "finite",
        "differences",
        ";",
        "see",
        "subsection",
        "5.2",
        "."
      ]
    },
    {
      "sentence": "For a schematic overview of the described categorization and the structure of this paper, see Figure 1 .",
      "tokens": [
        "For",
        "a",
        "schematic",
        "overview",
        "of",
        "the",
        "described",
        "categorization",
        "and",
        "the",
        "structure",
        "of",
        "this",
        "paper",
        ",",
        "see",
        "Figure",
        "1",
        "."
      ]
    },
    {
      "sentence": "Note that for some approaches which combine domain decomposition with machine learning the categorization is not uniquely defined.",
      "tokens": [
        "Note",
        "that",
        "for",
        "some",
        "approaches",
        "which",
        "combine",
        "domain",
        "decomposition",
        "with",
        "machine",
        "learning",
        "the",
        "categorization",
        "is",
        "not",
        "uniquely",
        "defined",
        "."
      ]
    },
    {
      "sentence": "For example, D3M [58] and DeepDDM (Deep Domain Decomposition Method) [60] can be interpreted as replacing subdomain solvers within a DDM by neural networks but, at the same time, also as a DDM within neural network training.",
      "tokens": [
        "For",
        "example",
        ",",
        "D3M",
        "[",
        "58",
        "]",
        "and",
        "DeepDDM",
        "(",
        "Deep",
        "Domain",
        "Decomposition",
        "Method",
        ")",
        "[",
        "60",
        "]",
        "can",
        "be",
        "interpreted",
        "as",
        "replacing",
        "subdomain",
        "solvers",
        "within",
        "a",
        "DDM",
        "by",
        "neural",
        "networks",
        "but",
        ",",
        "at",
        "the",
        "same",
        "time",
        ",",
        "also",
        "as",
        "a",
        "DDM",
        "within",
        "neural",
        "network",
        "training",
        "."
      ]
    },
    {
      "sentence": "Hence, we have made the following classifications to the best of our knowledge and with regard to the subject focus of the concrete considered work.",
      "tokens": [
        "Hence",
        ",",
        "we",
        "have",
        "made",
        "the",
        "following",
        "classifications",
        "to",
        "the",
        "best",
        "of",
        "our",
        "knowledge",
        "and",
        "with",
        "regard",
        "to",
        "the",
        "subject",
        "focus",
        "of",
        "the",
        "concrete",
        "considered",
        "work",
        "."
      ]
    },
    {
      "sentence": "2.",
      "tokens": [
        "2",
        "."
      ]
    },
    {
      "sentence": "Domain decomposition methods for the solution of PDEs.",
      "tokens": [
        "Domain",
        "decomposition",
        "methods",
        "for",
        "the",
        "solution",
        "of",
        "PDEs",
        "."
      ]
    },
    {
      "sentence": "In this section, we briefly introduce the main principles of overlapping and nonoverlapping DDMs as well as some necessary notation.",
      "tokens": [
        "In",
        "this",
        "section",
        ",",
        "we",
        "briefly",
        "introduce",
        "the",
        "main",
        "principles",
        "of",
        "overlapping",
        "and",
        "nonoverlapping",
        "DDMs",
        "as",
        "well",
        "as",
        "some",
        "necessary",
        "notation",
        "."
      ]
    },
    {
      "sentence": "DDMs [73, 90] are highly scalable, iterative methods for the solution of sparse, linear systems of equations as, for example, arising from the discretization of PDEs.",
      "tokens": [
        "DDMs",
        "[",
        "73",
        ",",
        "90",
        "]",
        "are",
        "highly",
        "scalable",
        ",",
        "iterative",
        "methods",
        "for",
        "the",
        "solution",
        "of",
        "sparse",
        ",",
        "linear",
        "systems",
        "of",
        "equations",
        "as",
        ",",
        "for",
        "example",
        ",",
        "arising",
        "from",
        "the",
        "discretization",
        "of",
        "PDEs",
        "."
      ]
    },
    {
      "sentence": "DDMs have been shown to be numerically stable for many practically relevant problems and are designed for the application on parallel computers.",
      "tokens": [
        "DDMs",
        "have",
        "been",
        "shown",
        "to",
        "be",
        "numerically",
        "stable",
        "for",
        "many",
        "practically",
        "relevant",
        "problems",
        "and",
        "are",
        "designed",
        "for",
        "the",
        "application",
        "on",
        "parallel",
        "computers",
        "."
      ]
    },
    {
      "sentence": "Generally speaking, they rely on a divide-and-conquer strategy and decompose the original global problem into a number of smaller subproblems.",
      "tokens": [
        "Generally",
        "speaking",
        ",",
        "they",
        "rely",
        "on",
        "a",
        "divide-and-conquer",
        "strategy",
        "and",
        "decompose",
        "the",
        "original",
        "global",
        "problem",
        "into",
        "a",
        "number",
        "of",
        "smaller",
        "subproblems",
        "."
      ]
    },
    {
      "sentence": "Mathematically, this corresponds to a subdivision of the domain into a number of smaller subdomains, where the local problems can be solved in parallel on different processors of a parallel computer.",
      "tokens": [
        "Mathematically",
        ",",
        "this",
        "corresponds",
        "to",
        "a",
        "subdivision",
        "of",
        "the",
        "domain",
        "into",
        "a",
        "number",
        "of",
        "smaller",
        "subdomains",
        ",",
        "where",
        "the",
        "local",
        "problems",
        "can",
        "be",
        "solved",
        "in",
        "parallel",
        "on",
        "different",
        "processors",
        "of",
        "a",
        "parallel",
        "computer",
        "."
      ]
    },
    {
      "sentence": "Additionally, a small global problem is often needed for numerical and parallel scalability.",
      "tokens": [
        "Additionally",
        ",",
        "a",
        "small",
        "global",
        "problem",
        "is",
        "often",
        "needed",
        "for",
        "numerical",
        "and",
        "parallel",
        "scalability",
        "."
      ]
    },
    {
      "sentence": "Certain mathematical conditions at the interface of neighboring subdomains have to be satisfied such that the solution of the original problem is recovered.",
      "tokens": [
        "Certain",
        "mathematical",
        "conditions",
        "at",
        "the",
        "interface",
        "of",
        "neighboring",
        "subdomains",
        "have",
        "to",
        "be",
        "satisfied",
        "such",
        "that",
        "the",
        "solution",
        "of",
        "the",
        "original",
        "problem",
        "is",
        "recovered",
        "."
      ]
    },
    {
      "sentence": "For a generic description of DDMs, we consider a boundary value problem of the general form (2.1) L(u) = f in Ω B(u) = g on ∂Ω on the domain Ω ⊂ R d , d = 2, 3 , where L is a linear, second-order, elliptic differential operator and B represents the boundary conditions.",
      "tokens": [
        "For",
        "a",
        "generic",
        "description",
        "of",
        "DDMs",
        ",",
        "we",
        "consider",
        "a",
        "boundary",
        "value",
        "problem",
        "of",
        "the",
        "general",
        "form",
        "(",
        "2.1",
        ")",
        "L",
        "(",
        "u",
        ")",
        "=",
        "f",
        "in",
        "Ω",
        "B",
        "(",
        "u",
        ")",
        "=",
        "g",
        "on",
        "∂Ω",
        "on",
        "the",
        "domain",
        "Ω",
        "⊂",
        "R",
        "d",
        ",",
        "d",
        "=",
        "2",
        ",",
        "3",
        ",",
        "where",
        "L",
        "is",
        "a",
        "linear",
        ",",
        "second-order",
        ",",
        "elliptic",
        "differential",
        "operator",
        "and",
        "B",
        "represents",
        "the",
        "boundary",
        "conditions",
        "."
      ]
    },
    {
      "sentence": "In order to compute a numerical solution of boundary value problem (2.1), we discretize the variational formulation of the given linear PDE with an appropriate numerical method, for example, conforming finite elements.",
      "tokens": [
        "In",
        "order",
        "to",
        "compute",
        "a",
        "numerical",
        "solution",
        "of",
        "boundary",
        "value",
        "problem",
        "(",
        "2.1",
        ")",
        ",",
        "we",
        "discretize",
        "the",
        "variational",
        "formulation",
        "of",
        "the",
        "given",
        "linear",
        "PDE",
        "with",
        "an",
        "appropriate",
        "numerical",
        "method",
        ",",
        "for",
        "example",
        ",",
        "conforming",
        "finite",
        "elements",
        "."
      ]
    },
    {
      "sentence": "This results in a, usually sparse, linear system of equations (2.2) K g u g = f g .",
      "tokens": [
        "This",
        "results",
        "in",
        "a",
        ",",
        "usually",
        "sparse",
        ",",
        "linear",
        "system",
        "of",
        "equations",
        "(",
        "2.2",
        ")",
        "K",
        "g",
        "u",
        "g",
        "=",
        "f",
        "g",
        "."
      ]
    },
    {
      "sentence": "The basic idea of DDMs is then, instead of directly solving the completely assembled system of equations (2.2), to decompose the problem into smaller subproblems and thus, solving smaller systems of equations which are assembled on local parts of the domain exclusively.",
      "tokens": [
        "The",
        "basic",
        "idea",
        "of",
        "DDMs",
        "is",
        "then",
        ",",
        "instead",
        "of",
        "directly",
        "solving",
        "the",
        "completely",
        "assembled",
        "system",
        "of",
        "equations",
        "(",
        "2.2",
        ")",
        ",",
        "to",
        "decompose",
        "the",
        "problem",
        "into",
        "smaller",
        "subproblems",
        "and",
        "thus",
        ",",
        "solving",
        "smaller",
        "systems",
        "of",
        "equations",
        "which",
        "are",
        "assembled",
        "on",
        "local",
        "parts",
        "of",
        "the",
        "domain",
        "exclusively",
        "."
      ]
    },
    {
      "sentence": "In the following, we denote these local parts of the domain as subdomains and we assume to have a decomposition of Ω into N ∈ N nonoverlapping subdomains Ω i , i = 1, ..., N , which fulfills Ω = N i=1 Ω i ; see also Figure 2 .",
      "tokens": [
        "In",
        "the",
        "following",
        ",",
        "we",
        "denote",
        "these",
        "local",
        "parts",
        "of",
        "the",
        "domain",
        "as",
        "subdomains",
        "and",
        "we",
        "assume",
        "to",
        "have",
        "a",
        "decomposition",
        "of",
        "Ω",
        "into",
        "N",
        "∈",
        "N",
        "nonoverlapping",
        "subdomains",
        "Ω",
        "i",
        ",",
        "i",
        "=",
        "1",
        ",",
        "...",
        ",",
        "N",
        ",",
        "which",
        "fulfills",
        "Ω",
        "=",
        "N",
        "i=1",
        "Ω",
        "i",
        ";",
        "see",
        "also",
        "Figure",
        "2",
        "."
      ]
    },
    {
      "sentence": "Moreover, we define Γ := N i=1 ∂Ω i \\ ∂Ω as the interface between neighboring subdomains, that is, degrees of freedom shared by at least two subdomains.",
      "tokens": [
        "Moreover",
        ",",
        "we",
        "define",
        "Γ",
        ":",
        "=",
        "N",
        "i=1",
        "∂Ω",
        "i",
        "\\",
        "∂Ω",
        "as",
        "the",
        "interface",
        "between",
        "neighboring",
        "subdomains",
        ",",
        "that",
        "is",
        ",",
        "degrees",
        "of",
        "freedom",
        "shared",
        "by",
        "at",
        "least",
        "two",
        "subdomains",
        "."
      ]
    },
    {
      "sentence": "In order to define overlapping DDMs, we additionally introduce overlapping subdomains Ω ′ i , i = 1, ..., N , where the overlap δ is a measure for the amount of local information shared between neighboring subdomains; see also Figure 2 (left) for a visualization.",
      "tokens": [
        "In",
        "order",
        "to",
        "define",
        "overlapping",
        "DDMs",
        ",",
        "we",
        "additionally",
        "introduce",
        "overlapping",
        "subdomains",
        "Ω",
        "′",
        "i",
        ",",
        "i",
        "=",
        "1",
        ",",
        "...",
        ",",
        "N",
        ",",
        "where",
        "the",
        "overlap",
        "δ",
        "is",
        "a",
        "measure",
        "for",
        "the",
        "amount",
        "of",
        "local",
        "information",
        "shared",
        "between",
        "neighboring",
        "subdomains",
        ";",
        "see",
        "also",
        "Figure",
        "2",
        "(",
        "left",
        ")",
        "for",
        "a",
        "visualization",
        "."
      ]
    },
    {
      "sentence": "However, simply solving decoupled local subdomain systems independently from each other might result in discontinuities along the interface Γ and will not result in the correct solution of the original and global system.",
      "tokens": [
        "However",
        ",",
        "simply",
        "solving",
        "decoupled",
        "local",
        "subdomain",
        "systems",
        "independently",
        "from",
        "each",
        "other",
        "might",
        "result",
        "in",
        "discontinuities",
        "along",
        "the",
        "interface",
        "Γ",
        "and",
        "will",
        "not",
        "result",
        "in",
        "the",
        "correct",
        "solution",
        "of",
        "the",
        "original",
        "and",
        "global",
        "system",
        "."
      ]
    },
    {
      "sentence": "Hence, in order to obtain the correct global solution which is continuous across the different subdomains, also communication between the different subproblems is necessary, that is, a global coupling between the different subdomains has to be ensured.",
      "tokens": [
        "Hence",
        ",",
        "in",
        "order",
        "to",
        "obtain",
        "the",
        "correct",
        "global",
        "solution",
        "which",
        "is",
        "continuous",
        "across",
        "the",
        "different",
        "subdomains",
        ",",
        "also",
        "communication",
        "between",
        "the",
        "different",
        "subproblems",
        "is",
        "necessary",
        ",",
        "that",
        "is",
        ",",
        "a",
        "global",
        "coupling",
        "between",
        "the",
        "different",
        "subdomains",
        "has",
        "to",
        "be",
        "ensured",
        "."
      ]
    },
    {
      "sentence": "With respect on how the global exchange of information across the original domain Ω is ensured, we distinguish between one-and two-level DDMs.",
      "tokens": [
        "With",
        "respect",
        "on",
        "how",
        "the",
        "global",
        "exchange",
        "of",
        "information",
        "across",
        "the",
        "original",
        "domain",
        "Ω",
        "is",
        "ensured",
        ",",
        "we",
        "distinguish",
        "between",
        "one-and",
        "two-level",
        "DDMs",
        "."
      ]
    },
    {
      "sentence": "One-level DDMs, where information is only exchanged between neighboring subdomains, result in a rate of convergence which deteriorates with a growing number of subdomains when used for the approximate .",
      "tokens": [
        "One-level",
        "DDMs",
        ",",
        "where",
        "information",
        "is",
        "only",
        "exchanged",
        "between",
        "neighboring",
        "subdomains",
        ",",
        "result",
        "in",
        "a",
        "rate",
        "of",
        "convergence",
        "which",
        "deteriorates",
        "with",
        "a",
        "growing",
        "number",
        "of",
        "subdomains",
        "when",
        "used",
        "for",
        "the",
        "approximate",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": "h 1 1 h 1 2 .",
      "tokens": [
        "h",
        "1",
        "1",
        "h",
        "1",
        "2",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": "h 1 K .",
      "tokens": [
        "h",
        "1",
        "K",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": "h N 1 h N 2 .",
      "tokens": [
        "h",
        "N",
        "1",
        "h",
        "N",
        "2",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": "h N K .",
      "tokens": [
        "h",
        "N",
        "K",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": "i p,1 i p,2 i p,j i p,n o p,1 o p,m",
      "tokens": [
        "i",
        "p,1",
        "i",
        "p,2",
        "i",
        "p",
        ",",
        "j",
        "i",
        "p",
        ",",
        "n",
        "o",
        "p,1",
        "o",
        "p",
        ",",
        "m"
      ]
    }
  ]
}