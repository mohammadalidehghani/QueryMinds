{"chunk_id": "1501.04309v1_title_0000", "paper_id": "1501.04309v1", "section": "title", "text": "Information Theory and its Relation to Machine Learning"}
{"chunk_id": "1501.04309v1_abstract_0001", "paper_id": "1501.04309v1", "section": "abstract", "text": "In this position paper , I first describe a new perspective on machine learning ( ML ) by four basic problems ( or levels ) , namely , `` What to learn ? `` , `` How to learn ? `` , `` What to evaluate ? `` , and `` What to adjust ? `` . The paper stresses more on the first level of `` What to learn ? `` , or `` Learning Target Selection '' . Towards this primary problem within the four levels , I briefly review the existing studies about the connection between information theoretical learning ( ITL [ 1 ] ) and machine learning . A theorem is given on the relation between the empirically-defined similarity measure and information measures . Finally , a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection ."}
{"chunk_id": "1501.04309v1_introduction_0002", "paper_id": "1501.04309v1", "section": "introduction", "text": "Introduction Machine learning is the study and construction of systems that can learn from data . The systems are called learning machines . When Big Data emerges increasingly , more learning machines are developed and applied in different domains . However , the ultimate goal of machine learning study is insight , not machine itself . By the term insight I mean learning mechanisms in descriptions of mathematical principles . In a loose sense , learning mechanisms can be regarded as the natural entity . As the `` Tao ( 道 ) '' reflects the most fundamental of the universe by Lao Tzu ( 老子 ) , Einstein suggests that we should pursue the simplest mathematical interpretations to the nature . Although learning mechanisms are related to the subjects of psychology , cognitive and brain science , this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms . Up to now , we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles ."}
{"chunk_id": "1501.04309v1_introduction_0003", "paper_id": "1501.04309v1", "section": "introduction", "text": "to the subjects of psychology , cognitive and brain science , this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms . Up to now , we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles . It is the author 's belief that `` mathematical-principle-based machine '' might be more important and critical than `` brain-inspired machine '' in the study of machine learning . The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning . In what follows I will present four basic problems ( or levels ) in machine learning . The study on information theoretical learning is briefly reviewed . A theorem between the empirically-defined similarity measures and information measures are given . Based on the existing investigations , a conjecture is proposed in this paper ."}
{"chunk_id": "1504.03874v1_title_0000", "paper_id": "1504.03874v1", "section": "title", "text": "Bridging belief function theory to modern machine learning"}
{"chunk_id": "1504.03874v1_abstract_0001", "paper_id": "1504.03874v1", "section": "abstract", "text": "Machine learning is a quickly evolving field which now looks really different from what it was 15 years ago , when classification and clustering were major issues . This document proposes several trends to explore the new questions of modern machine learning , with the strong afterthought that the belief function framework has a major role to play ."}
{"chunk_id": "1504.03874v1_introduction_0002", "paper_id": "1504.03874v1", "section": "introduction", "text": "Introduction In an age of user generated web-contents and of portable devices with embedded computer vision capabilities , machine learning ( ML ) and big data mining questions are fundamental . As a result , these questions naturally penetrate neighboring research fields , including belief function theory ( BFT ) , so that it is now usual to attend a `` Classification '' session [ 26 ] or a `` Machine Learning '' session [ 16 ] in a conference devoted to belief functions . However , it is hard to accept that among the various proposed approaches based on BF , very few have become state-of-the-art ML methods , the knowledge of which has spread beyond the BF community . Without any doubt , this can be partly explained by the relative size of the scientific communities under consideration : although quickly growing , the BF one is relatively small with respect to that of statistics , Bayesian networks , neural networks , etc ."}
{"chunk_id": "1504.03874v1_introduction_0003", "paper_id": "1504.03874v1", "section": "introduction", "text": "beyond the BF community . Without any doubt , this can be partly explained by the relative size of the scientific communities under consideration : although quickly growing , the BF one is relatively small with respect to that of statistics , Bayesian networks , neural networks , etc . However , this reason alone is not sufficient : There are indeed other topics , such as for instance , information fusion , where BF-based methods are now as well recognized as are methods based on more classical formalisms , such as probabilities , or ontologies . In this report , I assume an additional reason : that some researchers focused on BFT ( especially the youngest ) , who have progressively turned their interests towards ML problems , may not capture the newest trends of this field . In fact , I used to be an example of such researchers , and I acknowledge that my first perceptions of ML were clearly outdated . This is why , I propose a short review of the respective evolution of BFT and of ML , as well as an attempt to put them in perspective ."}
{"chunk_id": "1504.03874v1_introduction_0004", "paper_id": "1504.03874v1", "section": "introduction", "text": "be an example of such researchers , and I acknowledge that my first perceptions of ML were clearly outdated . This is why , I propose a short review of the respective evolution of BFT and of ML , as well as an attempt to put them in perspective . Of course , many senior researchers may find this exercise futile , as they have their own broad view on the question . However , to my knowledge , no recent referenced article is available for any reader seeking for a starting point to question the links between ML and BFT . This document is structured as follow : In Section 2 , a brief recall of the evolution of the mainstream in the BF community is provided . Then , in Section 3 , a short summary of the earlier ages of ML up to the mid-90s , is sketched , as well as a coarse description of the successful interactions between ML and BFT in those times ."}
{"chunk_id": "1504.03874v1_introduction_0005", "paper_id": "1504.03874v1", "section": "introduction", "text": "the mainstream in the BF community is provided . Then , in Section 3 , a short summary of the earlier ages of ML up to the mid-90s , is sketched , as well as a coarse description of the successful interactions between ML and BFT in those times . Afterward , I provide in Section 4 a synthetic overview of the revolution that blew over ML around the early 2000s , and which modified its goals and the organization of its supporting community . As BFT does not seem to fit in this new picture of the ML world , I list in Section 5 a few problems that may still be of interest for the current mainstream of BFT , as well as some potential interesting evolutions for the community to adapt to the newly raised questions ."}
{"chunk_id": "1505.06614v1_title_0000", "paper_id": "1505.06614v1", "section": "title", "text": "Electre Tri-Machine Learning Approach to the Record Linkage Problem"}
{"chunk_id": "1505.06614v1_abstract_0001", "paper_id": "1505.06614v1", "section": "abstract", "text": "In this short paper , the Electre Tri-Machine Learning Method , generally used to solve ordinal classification problems , is proposed for solving the Record Linkage problem . Preliminary experimental results show that , using the Electre Tri method , high accuracy can be achieved and more than 99 % of the matches and nonmatches were correctly identified by the procedure ."}
{"chunk_id": "1505.06614v1_introduction_0002", "paper_id": "1505.06614v1", "section": "introduction", "text": "Introduction Machine Learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to `` learn data '' . More precisely , `` learn '' is here intended as the possibility to automatically recognize complex patterns and make `` intelligent '' decisions , based on information data . Hence , machine learning is closely related to fields such as statistics , probability theory , data mining , pattern recognition , artificial intelligence , adaptive control and theoretical computer science . Machine learning algorithms can be classified in the following types : • supervised learning algorithms : a function/classifier is generated , that maps outputs on the training inputs , based on labeled examples inputoutput ; • unsupervised learning algorithms : patterns in the input are recognized , the examples have no labels ; • semi-supervised learning algorithms : supervised and unsupervised learning information is combined ; • reinforcement learning : actions from observation of the world are generated . Every action has some impact in the environment and the environment provides feedbacks that are translated into a score that guide the learning process ."}
{"chunk_id": "1505.06614v1_introduction_0003", "paper_id": "1505.06614v1", "section": "introduction", "text": "semi-supervised learning algorithms : supervised and unsupervised learning information is combined ; • reinforcement learning : actions from observation of the world are generated . Every action has some impact in the environment and the environment provides feedbacks that are translated into a score that guide the learning process . The principal supervised learning techniques currently applied or under consideration at statistical agencies worldwide to solve the record linkage matching problem are : classification tree [ 4 , 7 ] , support vector machine [ 1 , 2 , 3 ] and neural network [ 15 ] . In this short paper , another machine learning technique is proposed to solve the record linkage problem : the multi-criteria classification method Electre Tri . It is the first time that multi-criteria machine learning technique is used to solve the record linkage problem . This application answers to one of `` many challenges in applying supervised machine learning to record linkage matching '' [ 10 ] , showing that the use of multi-criteria classification method Electre Tri to solve the record linkage problem provides good results in term of classification model performances ."}
{"chunk_id": "1505.06614v1_introduction_0004", "paper_id": "1505.06614v1", "section": "introduction", "text": ". This application answers to one of `` many challenges in applying supervised machine learning to record linkage matching '' [ 10 ] , showing that the use of multi-criteria classification method Electre Tri to solve the record linkage problem provides good results in term of classification model performances . The importance of this application is in light of the increasing development of the use of administrative sources data . In this context , an important problem is that of finding matching pairs of records from heterogeneous databases , while maintaining privacy of the databases parties . To this purpose secure computation of distance metrics is important for secure record linkage [ 5 ] . The paper is organized as follows . Section 2 describes an introduction to the Record Linkage problem ; then the next Section 3 describes the method Electre Tri , used to solved the Record Linkage and in the last Section 4 a preliminary experiment is conducted on simulated data . The paper closes with some final remarks and conclusions ."}
{"chunk_id": "1505.06614v1_introduction_0005", "paper_id": "1505.06614v1", "section": "introduction", "text": "an introduction to the Record Linkage problem ; then the next Section 3 describes the method Electre Tri , used to solved the Record Linkage and in the last Section 4 a preliminary experiment is conducted on simulated data . The paper closes with some final remarks and conclusions . 2 Linked Data : the Record Linkage Generally speaking , in integration of two data sets the objective is the detection of those records , in the different data sets , that belong to the same statistical unit . This action allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] ."}
{"chunk_id": "1505.06614v1_introduction_0006", "paper_id": "1505.06614v1", "section": "introduction", "text": "allows the reconstruction of a unique record of data that contains all the unit information collected from different data sources on that unit . Therefore , record linkage is the methodology of bringing together corresponding records from two or more files or finding duplicates within files [ 16 ] . In the first situation , the definition of record linkage in [ 9 ] is more precise `` Record linkage is a solution to the problem of recognizing those records in two files which represent identical persons , objects , or events ( said to be matched ) '' The term record linkage originated in the public health area when files of individual patients were brought together using name , date-of-birth and other information [ 16 ] . One of the main motivations for the utilize of the record linkage method is the construction of the big data bases for answer to the new informative needs [ 8 ] . In order to better understand the problem , small practical example is now presented . Suppose the user wants to link two datasets of persons A and B , whose the variables Name , Address and Age are known ."}
{"chunk_id": "1505.06614v1_introduction_0007", "paper_id": "1505.06614v1", "section": "introduction", "text": "answer to the new informative needs [ 8 ] . In order to better understand the problem , small practical example is now presented . Suppose the user wants to link two datasets of persons A and B , whose the variables Name , Address and Age are known . Suppose that Table A contains the following values : Table A : Data in the first dataset Unit Name Address Age a1 John A Smith 16 Main Street 16 a2 Javier Martinez 49 E Applecross Road 33 a3 Gillian Jones 645 Reading Aev 22 Furthermore , suppose that Table B contains the following values : Table B : Data in the second dataset Unit Name Address Age b1 J H Smith 16 Main St 17 b2 Haveir Marteenez 49 Aplecross Raod 36 b3 Jilliam Brown 123 Norcross Blvd 43 The matching table A × B contains two units referring probably to the same persons , that the method should individuate as matches : 'John A Smith ' with ' J H Smith ' and 'Javier Martinez ' with 'Haveir Marteenez ' . Modern record linkage begins with the pioneering work of Newcombe et al ."}
{"chunk_id": "1505.06614v1_introduction_0008", "paper_id": "1505.06614v1", "section": "introduction", "text": "contains two units referring probably to the same persons , that the method should individuate as matches : 'John A Smith ' with ' J H Smith ' and 'Javier Martinez ' with 'Haveir Marteenez ' . Modern record linkage begins with the pioneering work of Newcombe et al . [ 14 ] , who introduced odds ratio of frequencies and the decision rules for delineating matches and nonmatches . In recent years , advances have yielded computer system that incorporate sophisticated ideas from computer sciences , statistics and operational research [ 16 ] . Then , Fellegi and Sunter [ 9 ] introduced a mathematical foundation for record linkage . Their theory demonstrated the optimality of the decision rules used by Newcombe and introduced a variety of ways of estimating crucial matching probabilities ( parameters ) directly from the files being matches . Formally , given two files A and B to be matched , each pair ( a , b ) ∈ Γ = A×B has to be classified into true match or true nonmatch ."}
{"chunk_id": "1505.06614v1_introduction_0009", "paper_id": "1505.06614v1", "section": "introduction", "text": "ways of estimating crucial matching probabilities ( parameters ) directly from the files being matches . Formally , given two files A and B to be matched , each pair ( a , b ) ∈ Γ = A×B has to be classified into true match or true nonmatch . The odds ratios of probabilities is : R = P r ( γ ∈ Γ | M ) P r ( γ ∈ Γ | U ) where γ is an arbitrary agreement pattern in the comparison space Γ , M is the set of of true matches and U is the set of true nonmatches . Between these two sets , the intermediate set of the possible matches exists . The decision rule reported below helps to classify the pairs : • if R > U pper , then the pair ( a , b ) is a designated match , • if Lower ≤ R ≤ U pper , then the pair ( a , b ) is a designated potential match , • if R < Lower , then the pair ( a , b ) is a designated nonmatch ."}
{"chunk_id": "1505.06614v1_introduction_0010", "paper_id": "1505.06614v1", "section": "introduction", "text": ", b ) is a designated match , • if Lower ≤ R ≤ U pper , then the pair ( a , b ) is a designated potential match , • if R < Lower , then the pair ( a , b ) is a designated nonmatch . The estimation of the thresholds Upper and Lower is not easy in an objective way ; the choice is competence of the analyst . In the decision rule , three different sets were created : the designated matches , designated potential matches , designated nonmatches . They constitute the partition of the set of all the records in the space Γ in three subsets C 3 ( matches ) , C 2 ( potential matches ) and C 1 ( nonmatches ) , whose intersections are empty sets . The idea is to solve the record linkage problem as a multi-criteria based classification problem , whose a priori defined classes are the subsets of the partition . Without going into too much details , in the next section a brief introduction to the method Electre Tri is presented ."}
{"chunk_id": "1612.04858v1_title_0000", "paper_id": "1612.04858v1", "section": "title", "text": "Bayesian Optimization for Machine Learning A Practical Guidebook"}
{"chunk_id": "1612.04858v1_abstract_0001", "paper_id": "1612.04858v1", "section": "abstract", "text": "The engineering of machine learning systems is still a nascent field ; relying on a seemingly daunting collection of quickly evolving tools and best practices . It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques . We outline four example machine learning problems that can be solved using open source machine learning libraries , and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications ."}
{"chunk_id": "1612.04858v1_introduction_0002", "paper_id": "1612.04858v1", "section": "introduction", "text": "Introduction Recently , there has been interest in applying Bayesian black-box optimization strategies to better conduct optimization over hyperparameter configurations of machine learning models and systems [ 19 ] [ 21 ] [ 11 ] . Most of these techniques require that the objective be a scalar value depending on the hyperparamter configuration x. x opt = arg max x∈X f ( x ) A more detailed introduction to Bayesian optimization and related techniques is provided in [ 8 ] . The focus of this guidebook is on demonstrating several example problems where Bayesian optimization provides a noted benefit . Our hope is to clearly show how Bayesian optimization can assist in better designing and optimizing real-world machine learning systems . All of the examples in this guidebook have corresponding code available on SigOpt 's example github repo ."}
{"chunk_id": "1612.07640v1_title_0000", "paper_id": "1612.07640v1", "section": "title", "text": "Deep Learning and Its Applications to Machine Health Monitoring : A Survey"}
{"chunk_id": "1612.07640v1_abstract_0001", "paper_id": "1612.07640v1", "section": "abstract", "text": "Since 2006 , deep learning ( DL ) has become a rapidly growing research direction , redefining state-of-the-art performances in a wide range of areas such as object recognition , image segmentation , speech recognition and machine translation . In modern manufacturing systems , data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet . Meanwhile , deep learning provides useful tools for processing and analyzing these big machinery data . The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring . After the brief introduction of deep learning techniques , the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects : Autoencoder ( AE ) and its variants , Restricted Boltzmann Machines and its variants including Deep Belief Network ( DBN ) and Deep Boltzmann Machines ( DBM ) , Convolutional Neural Networks ( CNN ) and Recurrent Neural Networks ( RNN ) . Finally , some new trends of DL-based machine health monitoring methods are discussed ."}
{"chunk_id": "1612.07640v1_introduction_0002", "paper_id": "1612.07640v1", "section": "introduction", "text": "I . INTRODUCTION I NDUSTRIAL Internet of Things ( IoT ) and data-driven techniques have been revolutionizing manufacturing by enabling computer networks to gather the huge amount of data from connected machines and turn the big machinery data into actionable information [ 1 ] , [ 2 ] , [ 3 ] . As a key component in modern manufacturing system , machine health monitoring has fully embraced the big data revolution . Compared to top-down modeling provided by the traditional physics-based models [ 4 ] , [ 5 ] , [ 6 ] , data-driven machine health monitoring systems offer a new paradigm of bottom-up solution for detection of faults after the occurrence of certain failures ( diagnosis ) and predictions of the future working conditions and the remaining useful life ( prognosis ) [ 1 ] , [ 7 ] . As we know , the complexity and noisy working condition hinder the construction of physical models . And most of these physicsbased models are unable to be updated with on-line measured data , which limits their effectiveness and flexibility ."}
{"chunk_id": "1612.07640v1_introduction_0003", "paper_id": "1612.07640v1", "section": "introduction", "text": "prognosis ) [ 1 ] , [ 7 ] . As we know , the complexity and noisy working condition hinder the construction of physical models . And most of these physicsbased models are unable to be updated with on-line measured data , which limits their effectiveness and flexibility . On the other hand , with significant development of sensors , sensor networks and computing systems , data-driven machine health monitoring models have become more and more attractive . To extract useful knowledge and make appropriate decisions from big data , machine learning techniques have been regarded as a powerful solution . As the hottest subfield of machine R. Yan is the corresponding author . E-mail : ruqiang @ seu.edu.cn This manuscript has been submitted to IEEE Transactions on Neural Networks and Learning Systems learning , deep learning is able to act as a bridge connecting big machinery data and intelligent machine health monitoring . As a branch of machine learning , deep learning attempts to model high level representations behind data and classify ( predict ) patterns via stacking multiple layers of information processing modules in hierarchical architectures ."}
{"chunk_id": "1612.07640v1_introduction_0004", "paper_id": "1612.07640v1", "section": "introduction", "text": "to act as a bridge connecting big machinery data and intelligent machine health monitoring . As a branch of machine learning , deep learning attempts to model high level representations behind data and classify ( predict ) patterns via stacking multiple layers of information processing modules in hierarchical architectures . Recently , deep learning has been successfully adopted in various areas such as computer vision , automatic speech recognition , natural language processing , audio recognition and bioinformatics [ 8 ] , [ 9 ] , [ 10 ] , [ 11 ] . In fact , deep learning is not a new idea , which even dates back to the 1940s [ 12 ] , [ 13 ] . The popularity of deep learning today can be contributed to the following points : * Increasing Computing Power : the advent of graphics processor unit ( GPU ) , the lowered cost of hardware , the better software infrastructure and the faster network connectivity all reduce the required running time of deep learning algorithms significantly ."}
{"chunk_id": "1612.07640v1_introduction_0005", "paper_id": "1612.07640v1", "section": "introduction", "text": "can be contributed to the following points : * Increasing Computing Power : the advent of graphics processor unit ( GPU ) , the lowered cost of hardware , the better software infrastructure and the faster network connectivity all reduce the required running time of deep learning algorithms significantly . For example , as reported in [ 14 ] , the time required to learn a four-layer DBN with 100 million free parameters can be reduced from several weeks to around a single day . * Increasing Data Size : there is no doubt that the era of Big Data is coming . Our activities are almost all digitized , recorded by computers and sensors , connected to Internet , and stored in cloud . As pointed out in [ 1 ] that in industry-related applications such as industrial informatics and electronics , almost 1000 exabytes are generated per year and a 20-fold increase can be expected in the next ten years . The study in [ 3 ] predicts that 30 billion devices will be connected by 2020 ."}
{"chunk_id": "1612.07640v1_introduction_0006", "paper_id": "1612.07640v1", "section": "introduction", "text": "1 ] that in industry-related applications such as industrial informatics and electronics , almost 1000 exabytes are generated per year and a 20-fold increase can be expected in the next ten years . The study in [ 3 ] predicts that 30 billion devices will be connected by 2020 . Therefore , the huge amount of data is able to offset the complexity increase behind deep learning and improve its generalization capability . * Advanced Deep Learning Research : the first breakthrough of deep learning is the pre-training method in an unsupervised way [ 15 ] , where Hinton proposed to pre-train one layer at a time via restricted Boltzmann machine ( RBM ) and then fine-tune using backpropagation . This has been proven to be effective to train multilayer neural networks . Considering the capability of deep learning to address largescale data and learn high-level representation , deep learning can be a powerful and effective solution for machine health monitoring systems ( MHMS ) . Conventional data-driven MHMS usually consists of the following key parts : handcrafted feature design , feature extraction/selection and model training ."}
{"chunk_id": "1612.07640v1_introduction_0007", "paper_id": "1612.07640v1", "section": "introduction", "text": "deep learning to address largescale data and learn high-level representation , deep learning can be a powerful and effective solution for machine health monitoring systems ( MHMS ) . Conventional data-driven MHMS usually consists of the following key parts : handcrafted feature design , feature extraction/selection and model training . The right set of features are designed , and then provided to some shallow machine learning algorithms including arXiv:1612.07640v1 [ cs.LG ] 16 Dec 2016 Support Vector Machines ( SVM ) , Naive Bayes ( NB ) , logistic regression [ 16 ] , [ 17 ] , [ 18 ] . It is shown that the representation defines the upper-bound performances of machine learning algorithms [ 19 ] . However , it is difficult to know and determine what kind of good features should be designed . To alleviate this issue , feature extraction/selection methods , which can be regarded as a kind of information fusion , are performed between hand-crafted feature design and classification/regression models [ 20 ] , [ 21 ] , [ 22 ] ."}
{"chunk_id": "1612.07640v1_introduction_0008", "paper_id": "1612.07640v1", "section": "introduction", "text": "what kind of good features should be designed . To alleviate this issue , feature extraction/selection methods , which can be regarded as a kind of information fusion , are performed between hand-crafted feature design and classification/regression models [ 20 ] , [ 21 ] , [ 22 ] . However , manually designing features for a complex domain requires a great deal of human labor and can not be updated on-line . At the same time , feature extraction/selection is another tricky problem , which involves prior selection of hyperparameters such as latent dimension . At last , the above three modules including feature design , feature extraction/selection and model training can not be jointly optimized which may hinder the final performance of the whole system . Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values ."}
{"chunk_id": "1612.07640v1_introduction_0009", "paper_id": "1612.07640v1", "section": "introduction", "text": "the whole system . Deep learning based MHMS ( DL-based MHMS ) aim to extract hierarchical representations from input data by building deep neural networks with multiple layers of non-linear transformations . Intuitively , one layer operation can be regarded as a transformation from input values to output values . Therefore , the application of one layer can learn a new representation of the input data and then , the stacking structure of multiple layers can enable MHMS to learn complex concepts out of simpler concepts that can be constructed from raw input . In addition , DL-based MHMS achieve an end-to-end system , which can automatically learn internal representations from raw input and predict targets . Compared to conventional data driven MHMS , DL-based MHMS do not require extensive human labor and knowledge for handcrafted feature design . All model parameters including feature module and pattern classification/regression module can be trained jointly . Therefore , DL-based models can be applied to addressing machine health monitoring in a very general way ."}
{"chunk_id": "1612.07640v1_introduction_0010", "paper_id": "1612.07640v1", "section": "introduction", "text": "MHMS , DL-based MHMS do not require extensive human labor and knowledge for handcrafted feature design . All model parameters including feature module and pattern classification/regression module can be trained jointly . Therefore , DL-based models can be applied to addressing machine health monitoring in a very general way . For example , it is possible that the model trained for fault diagnosis problem can be used for prognosis by only replacing the top softmax layer with a linear regression layer . The comparison between conventional data-driven MHMS and DL-based MHMS is given in Table I . A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown in Figure 1 . Deep learning models have several variants such as Auto-Dncoders [ 23 ] , Deep Belief Network [ 24 ] , Deep Boltzmann Machines [ 25 ] , Convolutional Neural Networks [ 26 ] and Recurrent Neural Networks [ 27 ] . During recent years , various researchers have demonstrated success of these deep learning models in the application of machine health monitoring ."}
{"chunk_id": "1612.07640v1_introduction_0011", "paper_id": "1612.07640v1", "section": "introduction", "text": "Deep Belief Network [ 24 ] , Deep Boltzmann Machines [ 25 ] , Convolutional Neural Networks [ 26 ] and Recurrent Neural Networks [ 27 ] . During recent years , various researchers have demonstrated success of these deep learning models in the application of machine health monitoring . This paper attempts to provide a wide overview on these latest DL-based MHMS works that impact the state-of-the art technologies . Compared to these frontiers of deep learning including Computer Vision and Natural Language Processing , machine health monitoring community is catching up and has witnessed an emerging research . Therefore , the purpose of this survey article is to present researchers and engineers in the area of machine health monitoring system , a global view of this hot and active topic , and help them to acquire basic knowledge , quickly apply deep learning models and develop novel DL-based MHMS . The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring ."}
{"chunk_id": "1612.07640v1_introduction_0012", "paper_id": "1612.07640v1", "section": "introduction", "text": "apply deep learning models and develop novel DL-based MHMS . The remainder of this paper is organized as follows . The basic information on these above deep learning models are given in section II . Then , section III reviews applications of deep learning models on machine health monitoring . Finally , section IV gives a brief summary of the recent achievements of DL-based MHMS and discusses some potential trends of deep learning in machine health monitoring . II . DEEP LEARNING Originated from artificial neural network , deep learning is a branch of machine learning which is featured by multiple nonlinear processing layers . Deep learning aims to learn hierarchy representations of data . Up to date , there are various deep learning architectures and this research topic is fast-growing , in which new models are being developed even per week . And the community is quite open and there are a number of deep learning tutorials and books of good-quality [ 28 ] , [ 29 ] . Therefore , only a brief introduction to some major deep learning techniques that have been applied in machine health monitoring is given ."}
{"chunk_id": "1612.07640v1_introduction_0013", "paper_id": "1612.07640v1", "section": "introduction", "text": "And the community is quite open and there are a number of deep learning tutorials and books of good-quality [ 28 ] , [ 29 ] . Therefore , only a brief introduction to some major deep learning techniques that have been applied in machine health monitoring is given . In the following , four deep architectures including Auto-encoders , RBM , CNN and RNN and their corresponding variants are reviewed , respectively ."}
{"chunk_id": "1703.10121v1_title_0000", "paper_id": "1703.10121v1", "section": "title", "text": "The Top 10 Topics in Machine Learning Revisited : A Quantitative Meta-Study"}
{"chunk_id": "1703.10121v1_abstract_0001", "paper_id": "1703.10121v1", "section": "abstract", "text": "Which topics of machine learning are most commonly addressed in research ? This question was initially answered in 2007 by doing a qualitative survey among distinguished researchers . In our study , we revisit this question from a quantitative perspective . Concretely , we collect 54K abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences . We then use machine learning in order to determine the top 10 topics in machine learning . We not only include models , but provide a holistic view across optimization , data , features , etc . This quantitative approach allows reducing the bias of surveys . It reveals new and up-to-date insights into what the 10 most prolific topics in machine learning research are . This allows researchers to identify popular topics as well as new and rising topics for their research ."}
{"chunk_id": "1703.10121v1_introduction_0002", "paper_id": "1703.10121v1", "section": "introduction", "text": "Introduction In 2007 , a paper named `` Top 10 algorithms in data mining '' identified and presented the top 10 most influential data mining algorithms within the research community [ 1 ] . The selection criteria were created by consolidating direct nominations from award winning researchers , the research community opinions and the number of citations in Google Scholar . The top 10 algorithms in that prior work are : C4.5 , k-means , support vector machine , Apriori , EM , PageRank , Ad-aBoost , kNN , naive Bayes and CART . In the decade that passed since then , machine learning has expanded , responding to incremental development of computational capabilities and substantial increase of problems in the commercial applications . This study reflects on the top 10 most popular fields of active research in machine learning , as they emerged from the quantitative analysis of leading journals and conferences . This work sees some topics in the broader sense including not only models but also concepts like data sets , features , optimization techniques and evaluation metrics ."}
{"chunk_id": "1703.10121v1_introduction_0003", "paper_id": "1703.10121v1", "section": "introduction", "text": "popular fields of active research in machine learning , as they emerged from the quantitative analysis of leading journals and conferences . This work sees some topics in the broader sense including not only models but also concepts like data sets , features , optimization techniques and evaluation metrics . This wider view on the entire machine learning field is largely ignored in the literature by keeping a strong focus entirely on models [ 2 ] . Our core contribution in this study is that we provide a clear view of the active research in machine learning by relying solely on a quantitative methodology without interviewing experts . This attempt aims at reducing bias and looking where the research community puts its focus on . The results of this study allow researchers to put their research into the global context of machine learning . This provides researchers with the opportunity to both conduct research in popular topics and identify topics that have not received sufficient attention in recent research . The rest of this paper is organized as follows . Section 2 describes the data sources and quantitative methodology ."}
{"chunk_id": "1703.10121v1_introduction_0004", "paper_id": "1703.10121v1", "section": "introduction", "text": "of machine learning . This provides researchers with the opportunity to both conduct research in popular topics and identify topics that have not received sufficient attention in recent research . The rest of this paper is organized as follows . Section 2 describes the data sources and quantitative methodology . Section 3 presents and discusses the top 10 topics identified . Section 4 summarizes this work ."}
{"chunk_id": "1706.05749v1_title_0000", "paper_id": "1706.05749v1", "section": "title", "text": "Dex : Incremental Learning for Complex Environments in Deep Reinforcement Learning"}
{"chunk_id": "1706.05749v1_abstract_0001", "paper_id": "1706.05749v1", "section": "abstract", "text": "This paper introduces Dex , a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems . We also present the novel continual learning method of incremental learning , where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment . We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments . We finally develop a saliency method for qualitative analysis of reinforcement learning , which shows the impact incremental learning has on network attention ."}
{"chunk_id": "1706.05749v1_introduction_0002", "paper_id": "1706.05749v1", "section": "introduction", "text": "Introduction Complex environments such as Go , Starcraft , and many modern video-games present profound challenges in deep reinforcement learning that have yet to be solved . They often require long , precise sequences of actions and domain knowledge in order to obtain reward , and have yet to be learned from random weight initialization . Solutions to these problems would mark a significant breakthrough on the path to artificial general intelligence . Recent works in reinforcement learning have shown that environments such as Atari games [ 2 ] can be learned from pixel input to superhuman expertise [ 9 ] . The agents start with randomly initialized weights , and learn largely from trial and error , relying on a reward signal to indicate performance . Despite these successes , complex games , including those where rewards are sparse such as Montezuma 's Revenge , have been notoriously difficult to learn . While methods such as intrinsic motivation [ 3 ] have been used to partially overcome these challenges , we suspect this becomes intractable as complexity increases . Additionally , as environments become more complex , they will become more expensive to simulate ."}
{"chunk_id": "1706.05749v1_introduction_0003", "paper_id": "1706.05749v1", "section": "introduction", "text": "have been notoriously difficult to learn . While methods such as intrinsic motivation [ 3 ] have been used to partially overcome these challenges , we suspect this becomes intractable as complexity increases . Additionally , as environments become more complex , they will become more expensive to simulate . This poses a significant problem , since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms , representing days of training on a single machine . Thus , it appears likely that complex environments will become too costly to learn from randomly initialized weights , due both to the increased simulation cost as well as the inherent difficulty of the task . Therefore , some form of prior information must be given to the agent . This can be seen with AlphaGo [ 18 ] , where the agent never learned to play the game without first using supervised learning on human games ."}
{"chunk_id": "1706.05749v1_introduction_0004", "paper_id": "1706.05749v1", "section": "introduction", "text": "as the inherent difficulty of the task . Therefore , some form of prior information must be given to the agent . This can be seen with AlphaGo [ 18 ] , where the agent never learned to play the game without first using supervised learning on human games . While supervised learning certainly has been shown to aid reinforcement learning , it is very costly to obtain sufficient samples and requires the environment to be a task humans can play with reasonable skill , and is therefore impractical for a wide variety of important reinforcement learning problems . In this paper we introduce Dex , the first continual reinforcement learning toolkit for training and evaluating continual learning methods . We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent . This task can be split into a series of subtasks that are solved simultaneously ."}
{"chunk_id": "1706.05749v1_introduction_0005", "paper_id": "1706.05749v1", "section": "introduction", "text": "We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments . In incremental learning , environments are framed as a task to be learned by an agent . This task can be split into a series of subtasks that are solved simultaneously . Similar to how natural language processing and object detection are subtasks of neural image caption generation [ 23 ] , reinforcement learning environments also have subtasks relevant to a given environment . These subtasks often include player detection , player control , obstacle detection , enemy detection , and player-object interaction , to name a few . These subtasks are common to many environments , but they are often sufficiently different in function and representation that reinforcement learning algorithms fail to generalize them across environments , such as in Atari . These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments , and are a reason for humans superior data efficiency in learning complex tasks ."}
{"chunk_id": "1706.05749v1_introduction_0006", "paper_id": "1706.05749v1", "section": "introduction", "text": "learning algorithms fail to generalize them across environments , such as in Atari . These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments , and are a reason for humans superior data efficiency in learning complex tasks . In the case of deliberately similar environments , we can construct the subtasks such that they are similar in function and representation that an agent trained on the first environment can accelerate learning on the second environment due to its preconstructed subtask representations , thus partially avoiding the more complex environment 's increased simulation cost and inherent learning difficulty ."}
{"chunk_id": "1711.01431v1_title_0000", "paper_id": "1711.01431v1", "section": "title", "text": "The Case for Meta-Cognitive Machine Learning : On Model Entropy and Concept Formation in Deep Learning"}
{"chunk_id": "1711.01431v1_abstract_0001", "paper_id": "1711.01431v1", "section": "abstract", "text": "Machine learning is usually defined in behaviourist terms , where external validation is the primary mechanism of learning . In this paper , I argue for a more holistic interpretation in which finding more probable , efficient and abstract representations is as central to learning as performance . In other words , machine learning should be extended with strategies to reason over its own learning process , leading to so-called meta-cognitive machine learning . As such , the de facto definition of machine learning should be reformulated in these intrinsically multiobjective terms , taking into account not only the task performance but also internal learning objectives . To this end , we suggest a `` model entropy function '' to be defined that quantifies the efficiency of the internal learning processes . It is conjured that the minimization of this model entropy leads to concept formation . Besides philosophical aspects , some initial illustrations are included to support the claims ."}
{"chunk_id": "1711.01431v1_introduction_0002", "paper_id": "1711.01431v1", "section": "introduction", "text": "Introduction Machine learning is often approached from a behaviourist perspective , in which external feedback in the form of a reinforcement signal is the major driving force of improvement . Though this method has lead to many successes , it is confronted with interesting and unsolved challenges like tackling overfitting , providing comprehensibility , building reusable abstractions and concept formation , among many other [ Kotsiantis et al. , 2007 ; Bengio , 2009 ] . The problem with these behaviourist approaches is that they ignore the central importance of internal processes when considering learning . Model internals are often regarded just as a means to achieve higher performance . Analogous to studying human behaviour , however , appreciating the mechanisms of learning boils down to the question : `` when have we really learnt ? '' In this paper , we argue that a computer has learnt when : • the programme becomes better at the task at hand ; • the programme can perform the task more efficiently ; • the code becomes `` more structured '' and simpler . One possible analogy to better understand the above statements can be found in software engineering ."}
{"chunk_id": "1711.01431v1_introduction_0003", "paper_id": "1711.01431v1", "section": "introduction", "text": "when : • the programme becomes better at the task at hand ; • the programme can perform the task more efficiently ; • the code becomes `` more structured '' and simpler . One possible analogy to better understand the above statements can be found in software engineering . When considering code that performs a specific task , we do not care only about its functionality , but also about its execution speed/efficiency and other so-called `` non-functional requirements '' . Furthermore , a carefully modularized design probably reflects more understanding than an endless enumeration of IF-ELSE clauses . In other words , finding a more efficient and structured way to represent/reproduce information and to perform a learning task , is as central to machine learning as the reproduction of results . Different to humans , of course , machines are measurable . This provides us with a unique opportunity to study the nature of learning in principle , at the same time improving Machine Intelligence . We are not claiming that model complexity/efficiency has not been subject to past research efforts ."}
{"chunk_id": "1711.01431v1_introduction_0004", "paper_id": "1711.01431v1", "section": "introduction", "text": "to humans , of course , machines are measurable . This provides us with a unique opportunity to study the nature of learning in principle , at the same time improving Machine Intelligence . We are not claiming that model complexity/efficiency has not been subject to past research efforts . On the contrary , many techniques and design principles have attempted to improve exactly these properties -like Occam 's razor , Bayesian structure learning , pruning , the use of prototypes to compact information , regularization as a strategy to reduce energy , weight sharing in RNNs or CNNs to decrease model complexity , etc . Indeed , the whole evolution of Deep Learning can be seen as one specific approach in the quest to find models that are more structured ( i.e . have a lower entropy ) , by organizing and training them in a layer-wise fashion [ Bengio , 2009 ] . The focus has been mainly on training algorithms and designing model architectures that are adapted to these kinds of `` deep '' structures [ Deng and Yu , 2014 ] ."}
{"chunk_id": "1711.01431v1_introduction_0005", "paper_id": "1711.01431v1", "section": "introduction", "text": "lower entropy ) , by organizing and training them in a layer-wise fashion [ Bengio , 2009 ] . The focus has been mainly on training algorithms and designing model architectures that are adapted to these kinds of `` deep '' structures [ Deng and Yu , 2014 ] . Similar to efforts in multiobjective machine learning , these techniques are considered as a means to improve ( externally measured ) performance rather than a goal in itself [ Jin and Sendhoff , 2008 ] . We , however , do believe that minimizing the model 's structural complexity and optimizing its efficiency of representation , is not only a means to improve ( externally validated ) performance , but a central pillar to machine intelligence that leads to concept formulation and should be made explicit . In this sense , our vision aligns to that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows ."}
{"chunk_id": "1711.01431v1_introduction_0006", "paper_id": "1711.01431v1", "section": "introduction", "text": "that of Ray Kurzweil , who claimed that `` the theory behind deep learning . . . is that you have a model that reflects the hierarchy in the natural phenomenon you 're trying to learn [ Hof , 2013 ] . '' This paper is structured as follows . The theoretical ideas are laid out and the case for a new operational definition of machine learning is made . We put forward the conjecture that the optimization of model entropy , leads to concept formation . Last , conclusions and further steps to operationalize these concepts are formulated . 2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a ( non- ) parametric model with respect to some performance measure ."}
{"chunk_id": "1711.01431v1_introduction_0007", "paper_id": "1711.01431v1", "section": "introduction", "text": "formation . Last , conclusions and further steps to operationalize these concepts are formulated . 2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a ( non- ) parametric model with respect to some performance measure . This view is clearly reflected in the de facto definition of machine learning by Mitchell [ Mitchell , 1997 ] : `` A computer program is said to learn from an experience X with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experimental data D '' . Traditional machine learning techniques typically exploit shallow-structured , and often fixed , architectures . Nevertheless , there is a general consensus that the learning of `` higherorder '' concepts is problematic , and that the solution to this issue is somehow connected to deep architectures that create ever higher forms of abstraction . Experimental research as well as neurological evidence on the organization of the brain , supports this finding [ Bianchini and Scarselli , 2014 ] ."}
{"chunk_id": "1711.01431v1_introduction_0008", "paper_id": "1711.01431v1", "section": "introduction", "text": "concepts is problematic , and that the solution to this issue is somehow connected to deep architectures that create ever higher forms of abstraction . Experimental research as well as neurological evidence on the organization of the brain , supports this finding [ Bianchini and Scarselli , 2014 ] . The limitation of architecture complexity is preferred , primarily because their behaviour could be understood and the training of more complex or adaptive architectures leads to a explosion of complexity . That was until recently . The recent advanced in so-called `` Deep Learning '' , have focused on training algorithms that are adapted to new kinds of deep architectures [ Deng and Yu , 2014 ] , and heuristic strategies to attain specific structural properties like sparse coding that lead to higher forms of abstractions . With the exception of studies on `` interpretability '' [ Jin and Sendhoff , 2008 ] , structural properties are mainly considered a by-product , a ( desirable ) side effect of the applied training mechanisms ."}
{"chunk_id": "1711.01431v1_introduction_0009", "paper_id": "1711.01431v1", "section": "introduction", "text": "structural properties like sparse coding that lead to higher forms of abstractions . With the exception of studies on `` interpretability '' [ Jin and Sendhoff , 2008 ] , structural properties are mainly considered a by-product , a ( desirable ) side effect of the applied training mechanisms . Though the organization and complexity of model topologies is acknowledged to be crucial , current approaches are mainly limited to analysing the data space , i.e . the implemented regression functions or decision boundaries [ Bianchini and Scarselli , 2014 ] . There is a problem with this approach . Consider an neural network algorithm that needs to learn a simple concept like an `` XOR '' function depicted in Fig . 1 . An infinite number of neural networks with very similar or identical decision boundaries can be constructed -of which two are shown in Fig . 2 . From an external point of view , there is no way to discriminate between these two models : describing the difference between these two models can only occur in terms of the model internals ."}
{"chunk_id": "1711.01431v1_introduction_0010", "paper_id": "1711.01431v1", "section": "introduction", "text": "decision boundaries can be constructed -of which two are shown in Fig . 2 . From an external point of view , there is no way to discriminate between these two models : describing the difference between these two models can only occur in terms of the model internals . Of course the weight space , which represents the model of a neural network , is related to the data space , as it performs calculations on the data . In other words : Data representation and model computation should be considered as two sides of the same coin . As a result the structural properties of both the model and data space are key to the modelling of higher abstractions . Sparse coding is a perfect example of this . Without sparse coding , although the information is intrinsically `` present '' in the data , neural networks become intractable to train due to the extremely volatile and complex decision surface . From this perspective we follow the observations that have been made by Bengio in [ Bengio et al. , 2013 ] on representation learning ."}
{"chunk_id": "1711.01431v1_introduction_0011", "paper_id": "1711.01431v1", "section": "introduction", "text": "is intrinsically `` present '' in the data , neural networks become intractable to train due to the extremely volatile and complex decision surface . From this perspective we follow the observations that have been made by Bengio in [ Bengio et al. , 2013 ] on representation learning . One of the interesting phenomena is `` information entanglement '' [ Glorot et al. , 2011 ] . In this case , the model space is of a lower dimensionality or complexity than the data space . The projection of the data onto a high-dimensional space using sparse coding , then , has the advantage that the representations are more likely to be linearly separable , or at least less nonlinear . On the other hand , when the model complexity is increased considerably ( e.g . by adding layers ) , the neural network becomes untrainable using traditional techniques , because the dimensionality of the search space explodes . Deep learning techniques tackle this issue by -among other techniques -pre-initializing the model-space of particular layer in a maximum-likelihood/minimal-energy state ."}
{"chunk_id": "1807.10681v1_title_0000", "paper_id": "1807.10681v1", "section": "title", "text": "Learnable : Theory vs Applications"}
{"chunk_id": "1807.10681v1_abstract_0001", "paper_id": "1807.10681v1", "section": "abstract", "text": "Two different views on machine learning problem : Applied learning ( machine learning with business applications ) and Agnostic PAC learning are formalized and compared here . I show that , under some conditions , the theory of PAC Learnable provides a way to solve the Applied learning problem . However , the theory requires to have the training sets so large , that it would make the learning practically useless . I suggest to shed some theoretical misconceptions about learning to make the theory more aligned with the needs and experience of practitioners ."}
{"chunk_id": "1807.10681v1_introduction_0002", "paper_id": "1807.10681v1", "section": "introduction", "text": "Introduction Machine learning includes a practical side as well as a theoretical side . Practitioners solve real life problems , theoreticians study theory of learning . Practitioners need help answering the questions the life poses . Theoreticians give the answers . Unfortunately , they are , apparently , answering different questions , about some-what different subjects . For example , practitioners deal with limited data and deadlines , while the theory talks about what happens when training data increase indefinitely . There appears to be some disconnect here . More the over , the practitioners often can not formulate their questions exactly , because the language of the existing theory was developed by theoreticians to study different issues and different situations . Here I formulate the learning problem as it is encountered in practical applications , pose the related real life questions and show the answers to these questions which follow from PAC learnable theory . To do it , I express both questions of practitioners and the results of the PAC learnable theory in terms of problem solving ."}
{"chunk_id": "1811.04422v1_title_0000", "paper_id": "1811.04422v1", "section": "title", "text": "An Optimal Control View of Adversarial Machine Learning"}
{"chunk_id": "1811.04422v1_abstract_0001", "paper_id": "1811.04422v1", "section": "abstract", "text": "I describe an optimal control view of adversarial machine learning , where the dynamical system is the machine learner , the input are adversarial actions , and the control costs are defined by the adversary 's goals to do harm and be hard to detect . This view encompasses many types of adversarial machine learning , including test-item attacks , training-data poisoning , and adversarial reward shaping . The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning ."}
{"chunk_id": "1811.04422v1_introduction_0002", "paper_id": "1811.04422v1", "section": "introduction", "text": "1 Adversarial Machine Learning is not Machine Learning Machine learning has its mathematical foundation in concentration inequalities . This is a consequence of the independent and identically-distributed ( i.i.d . ) data assumption . In contrast , I suggest that adversarial machine learning may adopt optimal control as its mathematical foundation [ 3 , 25 ] . There are telltale signs : adversarial attacks tend to be subtle and have peculiar non-i.i.d . structures -as control input might be ."}
{"chunk_id": "1811.04871v1_title_0000", "paper_id": "1811.04871v1", "section": "title", "text": "Characterizing machine learning process : A maturity framework"}
{"chunk_id": "1811.04871v1_abstract_0001", "paper_id": "1811.04871v1", "section": "abstract", "text": "Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises . For example , existing machine learning processes can not address how to define business use cases for an AI application , how to convert business requirements from offering managers into data requirements for data scientists , and how to continuously improve AI applications in term of accuracy and fairness , how to customize general purpose machine learning models with industry , domain , and use case specific data to make them more accurate for specific situations etc . Making AI work for enterprises requires special considerations , tools , methods and processes . In this paper we present a maturity framework for machine learning model lifecycle management for enterprises . Our framework is a re-interpretation of the software Capability Maturity Model ( CMM ) for machine learning model development process . We present a set of best practices from authors ' personal experience of building large scale real-world machine learning models to help organizations achieve higher levels of maturity independent of their starting point ."}
{"chunk_id": "1811.04871v1_introduction_0002", "paper_id": "1811.04871v1", "section": "introduction", "text": "I . INTRODUCTION Software and Services development has gone through various phases of maturity in the past few decades . The community has evolved lifecycle management theories and practices to disseminate best practices to developers , companies and consultants alike . For example , in software field , Software Development Life Cycle ( SDLC ) Management , capability maturity models ( CMM ) Application Life Cycle Management ( ALM ) , Product Life Cycle Management ( PLM ) models prescribe systematic theories and practical guidance for developing products in general , and software products in particular . Information Technology Infrastructure Library ( ITIL ) organization presents a set of detailed practices for IT Services management ( ITSM ) by aligning IT services with business objectives . All these practices provide useful guidance for developers in systematically building software and services assets . However , these methods fall short in managing a new breed of software services being developed rapidly in the industry . These are software services built with machine learnt models ."}
{"chunk_id": "1811.04871v1_introduction_0003", "paper_id": "1811.04871v1", "section": "introduction", "text": "business objectives . All these practices provide useful guidance for developers in systematically building software and services assets . However , these methods fall short in managing a new breed of software services being developed rapidly in the industry . These are software services built with machine learnt models . We are well into the era of Artificial Intelligence ( AI ) , spurred by algorithmic , and computational advances , the availability of the latest algorithms in various software libraries , Cloud technologies , and the desire of companies to unleash insights from the vast amounts of untapped unstructured data lying in their enterprises . Companies are actively exploring and deploying trial versions of AI-enabled applications such as chat bots , personal digital assistants , doctors ' assistants , radiology assistants , legal assistants , health and wellness coaches in their enterprises . Powering these applications are the AI building block services such as conversation enabling service , speech-to-text and text to speech , image recognition service , language translation and natural language understanding services that detect entities , relations , keywords , concepts , sentiments and emotions in text ."}
{"chunk_id": "1811.04871v1_introduction_0004", "paper_id": "1811.04871v1", "section": "introduction", "text": "enterprises . Powering these applications are the AI building block services such as conversation enabling service , speech-to-text and text to speech , image recognition service , language translation and natural language understanding services that detect entities , relations , keywords , concepts , sentiments and emotions in text . Several of these services are machine learnt , if not all . As more and more machine learnt services make their way into software applications , which themselves are part of business processes , robust life cycle management of these machine learnt models becomes critical for ensuring the integrity of business processes that rely on them . We argue that two reasons necessitate a new maturity framework for machine learning models . First , the lifecycle of machine learning models is significantly different from that of the traditional software and therefore a reinterpretation of the software capability maturity model ( CMM ) maturity framework for building and managing the lifecycle of machine learning models is called for . Second , building machine learning models that work for enterprises requires solutions to a very different set of problems than the academic literature on machine learning typically focuses on ."}
{"chunk_id": "1811.04871v1_introduction_0005", "paper_id": "1811.04871v1", "section": "introduction", "text": "model ( CMM ) maturity framework for building and managing the lifecycle of machine learning models is called for . Second , building machine learning models that work for enterprises requires solutions to a very different set of problems than the academic literature on machine learning typically focuses on . We explain these two reasons below a bit more in detail ."}
{"chunk_id": "1811.11669v1_title_0000", "paper_id": "1811.11669v1", "section": "title", "text": "TOWARDS IDENTIFYING AND MANAGING SOURCES OF UNCERTAINTY IN AI AND MACHINE LEARNING MODELS -AN OVERVIEW"}
{"chunk_id": "1811.11669v1_abstract_0001", "paper_id": "1811.11669v1", "section": "abstract", "text": "Quantifying and managing uncertainties that occur when data-driven models such as those provided by AI and machine learning methods are applied is crucial . This whitepaper provides a brief motivation and first overview of the state of the art in identifying and quantifying sources of uncertainty for data-driven components as well as means for analyzing their impact ."}
{"chunk_id": "1811.11669v1_introduction_0002", "paper_id": "1811.11669v1", "section": "introduction", "text": "As a consequence , the functional behavior expected from data-driven components can only be specified in part on their intended domain , and we can not assure that they will behave as expected in all cases . Moreover , their processing structure is usually difficult to trace and validate by humans because this structure rarely follows human intuition but is generated to provide the algorithmically generalized input-output relationship in an effective manner . Prominent representatives of models used by data-driven components are artificial neural networks and support vector machines ( Russell & Norvig , 2016 ) . Since data-driven models are an important source of uncertainty in embedded systems that collaborate in an open context , the uncertainty they introduce has to be appropriately understood and managed during design time and runtime . Previous work ( Kläs & Vollmer , 2018 ) proposes separating the sources of uncertainty in data-driven components into three major classes , distinguishing between uncertainty caused by limitations in terms of model fit , data quality , and scope compliance ."}
{"chunk_id": "1811.11669v1_introduction_0003", "paper_id": "1811.11669v1", "section": "introduction", "text": "and managed during design time and runtime . Previous work ( Kläs & Vollmer , 2018 ) proposes separating the sources of uncertainty in data-driven components into three major classes , distinguishing between uncertainty caused by limitations in terms of model fit , data quality , and scope compliance . Whereas model fit focuses on the inherent uncertainty in data-driven models , data quality covers the additional uncertainty caused by their application to input data obtained in suboptimal conditions and scope compliance covers situations where the model is likely applied outside the scope for which it was trained and validated ."}
{"chunk_id": "1812.10422v1_title_0000", "paper_id": "1812.10422v1", "section": "title", "text": "Machine Learning in Official Statistics"}
{"chunk_id": "1812.10422v1_abstract_0001", "paper_id": "1812.10422v1", "section": "abstract", "text": "In the first half of 2018 , the Federal Statistical Office of Germany ( Destatis ) carried out a `` Proof of Concept Machine Learning '' as part of its Digital Agenda . A major component of this was surveys on the use of machine learning methods in official statistics , which were conducted at selected national and international statistical institutions and among the divisions of Destatis . It was of particular interest to find out in which statistical areas and for which tasks machine learning is used and which methods are applied . This paper is intended to make the results of the surveys publicly accessible ."}
{"chunk_id": "1812.10422v1_introduction_0002", "paper_id": "1812.10422v1", "section": "introduction", "text": "Introduction 1.Digital Agenda of the Federal Statistical Office of Germany On 10 October 2017 , the development of a Digital Agenda of the Federal Statistical Office of Germany ( Destatis ) has started ( Statistisches Bundesamt 2018 ) . One of many topics that were intensively discussed was Machine Learning . In a meeting at 13-15 November 2017 , the office and department heads of Destatis evaluated and prioritised 59 measures of the Digital Agenda according to their benefits and costs . A `` Proof of Concept Machine Learning '' was given high priority and classified as one of four lighthouse projects of the Digital Agenda . The content specification was `` Proof of Concept Machine Learning -Set up Proof of Concept for Machine Learning , e.g . in business statistics , to perform automatic categorization and improve analysis potential '' . The deadline for completion of the project was set for mid-2018 . foot_0"}
{"chunk_id": "1903.00092v2_title_0000", "paper_id": "1903.00092v2", "section": "title", "text": "Optimal Algorithms for Ski Rental with Soft Machine-Learned Predictions"}
{"chunk_id": "1903.00092v2_abstract_0001", "paper_id": "1903.00092v2", "section": "abstract", "text": "We consider a variant of the classic Ski Rental online algorithm with applications to machine learning . In our variant , we allow the skier access to a black-box machine learning algorithm that provides an estimate of the probability that there will be less than a threshold number of ski-days . We derive a class of optimal randomized algorithms to determine the strategy that minimizes the worst-case expected competitive ratio for the skier given a prediction from the machine learning algorithm , and analyze the performance and robustness of these algorithms ."}
{"chunk_id": "1903.00092v2_introduction_0002", "paper_id": "1903.00092v2", "section": "introduction", "text": "Introduction Online decision-making problems fundamentally address the issue of dealing with the uncertainty inherently present in the future . In broad terms , these problems can be addressed in two ways . First , a predictive approach like a machine learning algorithm can be used to guess at future events and to act accordingly . This method , while clearly powerful , has the drawback that it is very difficult to make any guarantees about the performance of the algorithm . Another paradigm for solving these problems is to use competitive analysis to guarantee a bound on the performance of a given algorithm . In this approach , we consider some cost function and note the cost that would be incurred by an omniscient algorithm that could access future data . Then , we compare this optimal omniscient cost to the worst-case cost of an online algorithm which can not use future data . By bounding the ratio of these two costs , a guarantee can be made as to how well the online algorithm will perform in terms of the performance of an omniscient algorithm . A classic problem in competitive analysis is the Ski Rental Problem ."}
{"chunk_id": "1903.00092v2_introduction_0003", "paper_id": "1903.00092v2", "section": "introduction", "text": "not use future data . By bounding the ratio of these two costs , a guarantee can be made as to how well the online algorithm will perform in terms of the performance of an omniscient algorithm . A classic problem in competitive analysis is the Ski Rental Problem . In this problem , a skier must decide whether to rent skis at a rate of $ 1 per day , or to buy skis at a price of $ B . The uncertainty lies in the number of ski days left in the season -if there are very few days , it is optimal to rent the skis . On the other hand , if there are many ski days it may be more cost-effective to buy the skis outright . This simple paradigm extends to various practical problems , such as a firm deciding between purchasing servers or using an on-demand cloud computing solution to provide the computational power needed to satisfy an unknown future demand ."}
{"chunk_id": "1907.03010v1_title_0000", "paper_id": "1907.03010v1", "section": "title", "text": "Financial Time Series Data Processing for Machine Learning"}
{"chunk_id": "1907.03010v1_abstract_0001", "paper_id": "1907.03010v1", "section": "abstract", "text": "This article studies the financial time series data processing for machine learning . It introduces the most frequent scaling methods , then compares the resulting stationarity and preservation of useful information for trend forecasting . It proposes an empirical test based on the capability to learn simple data relationship with simple models . It also speaks about the data split method specific to time series , avoiding unwanted overfitting and proposes various labelling for classification and regression ."}
{"chunk_id": "1907.03010v1_introduction_0002", "paper_id": "1907.03010v1", "section": "introduction", "text": "INTRODUCTION In the field of machine learning Time Series are very special data needed their specific processing and methods [ 1 ] [ 2 ] . On top of that , Financial data adds a big challenge due to their proportion of randomness and their non-stationary nature [ 4 ] [ 3 ] . There is a lot of research relative to the Financial Market forecast with Machine Learning [ 5 ] . However , many studies only cover one type of data scaling or labelling while the decisions made on this step can have a huge impact on the results . Not only in term of pure model performances metrics but in term of capabilities to really implement a profitable trading strategy based on the model . This study covers the following points : • Pre-processing and Stationarity • Pre-processing and preservation of useful prices relationships • Labelling for classifiers and regressors"}
{"chunk_id": "1907.07543v1_title_0000", "paper_id": "1907.07543v1", "section": "title", "text": "Low-Shot Classification : A Comparison of Classical and Deep Transfer Machine Learning Approaches"}
{"chunk_id": "1907.07543v1_abstract_0001", "paper_id": "1907.07543v1", "section": "abstract", "text": "Despite the recent success of deep transfer learning approaches in NLP , there is a lack of quantitative studies demonstrating the gains these models offer in low-shot text classification tasks over existing paradigms . Deep transfer learning approaches such as BERT and ULMFiT demonstrate that they can beat state-of-the-art results on larger datasets , however when one has only 100-1000 labelled examples per class , the choice of approach is less clear , with classical machine learning and deep transfer learning representing valid options . This paper compares the current best transfer learning approach with top classical machine learning approaches on a trinary sentiment classification task to assess the best paradigm . We find that BERT , representing the best of deep transfer learning , is the best performing approach , outperforming top classical machine learning algorithms by 9.7 % on average when trained with 100 examples per class , narrowing to 1.8 % at 1000 labels per class ."}
{"chunk_id": "1907.07543v1_abstract_0002", "paper_id": "1907.07543v1", "section": "abstract", "text": "paradigm . We find that BERT , representing the best of deep transfer learning , is the best performing approach , outperforming top classical machine learning algorithms by 9.7 % on average when trained with 100 examples per class , narrowing to 1.8 % at 1000 labels per class . We also show the robustness of deep transfer learning in moving across domains , where the maximum loss in accuracy is only 0.7 % in similar domain tasks and 3.2 % cross domain , compared to classical machine learning which loses up to 20.6 % ."}
{"chunk_id": "1907.07543v1_introduction_0003", "paper_id": "1907.07543v1", "section": "introduction", "text": "Introduction Transfer learning in the Natural Language Processing ( NLP ) field has advanced significantly in the last two years , introducing fine-tuning approaches akin to those seen in computer vision some years earlier ( Donahue et al. , 2013 ) . This growth originated from feature-based transfer learning , which in the form of word embeddings has been in use for some years , particularly driven by ( Mikolov , Chen , Corrado , & Dean , 2013 ) . As part of this new wave , we have seen advancements in feature-based transfer learning in the form of ELMo ( Peters et al. , 2018 ) . In addition a characteristic trend in this wave of transfer learning models is a class of algorithms that primarily focus on a finetuning approach , where a base language model ( Bengio , Ducharme , Vincent , & Jauvin , 2003 ) is trained and then fine-tuned on a target task . This base language model is typically very large ( 100M + parameters ) and takes a relatively long time to train ."}
{"chunk_id": "1907.07543v1_introduction_0004", "paper_id": "1907.07543v1", "section": "introduction", "text": ", where a base language model ( Bengio , Ducharme , Vincent , & Jauvin , 2003 ) is trained and then fine-tuned on a target task . This base language model is typically very large ( 100M + parameters ) and takes a relatively long time to train . However , the fine-tuning task is usually much quicker to train as only a few parameters are added to the model , typically a single dense layer to the end of a multilayer LSTM or Transformer ( Vaswani et al. , 2017 ) . The model continues training either all , or part , of the network , but this is typically on much less data and for much less time , as only the task specific information is being learned and the general `` understanding '' of the language is transferred . These approaches have , on multiple occasions , broken the state-of-the-art records ( SO-TAs ) across the board on a range of NLP tasks and datasets ( Devlin , Chang , Lee , & Toutanova , 2018 ) ( Howard & Ruder , 2018 ) ."}
{"chunk_id": "1907.07543v1_introduction_0005", "paper_id": "1907.07543v1", "section": "introduction", "text": "language is transferred . These approaches have , on multiple occasions , broken the state-of-the-art records ( SO-TAs ) across the board on a range of NLP tasks and datasets ( Devlin , Chang , Lee , & Toutanova , 2018 ) ( Howard & Ruder , 2018 ) . However , all of these datasets are designed for deep learning : they are typically large enough that they warrant the use of deep learning ( 5000+ examples per class ) , without the necessity of transfer learning . It is our view that what transfer learning does , in these cases , is push the boundaries of performance . The prevalence of deep learning algorithms in surpassing SOTA records suggests quite clearly that , for the datasets assessed , deep learning surpasses the limits of classical machine learning algorithms in NLP tasks . Low-shot transfer learning is another usecase for transfer learning in NLP , one of particular interest to companies working with real-world data . Low-shot transfer learning ( also referred to as `` few-shot '' ) is the use of transfer learning in training models where we have little training data available ."}
{"chunk_id": "1907.07543v1_introduction_0006", "paper_id": "1907.07543v1", "section": "introduction", "text": "learning is another usecase for transfer learning in NLP , one of particular interest to companies working with real-world data . Low-shot transfer learning ( also referred to as `` few-shot '' ) is the use of transfer learning in training models where we have little training data available . This is important as many potential real-world applications of machine learning NLP do not have access to sufficiently large datasets to train deep learning algorithms , and obtaining such a dataset can often be too expensive or time consuming . Howard & Ruder ( 2018 ) note , and Devlin et . al . ( 2018 ) hypothesize that their respective approaches can be used with low quantities of data to give good results . However , in sources such as ( Howard & Ruder , 2018 ) , results on low-shot learning are presented relative to training deep models from scratch , but as mentioned in ( Goodfellow , Bengio , & Courville , 2016 ) , deep learning generally only achieves reasonable performance at about 5000 examples per class and is therefore not necessarily the best paradigm at these scales ."}
{"chunk_id": "1907.07543v1_introduction_0007", "paper_id": "1907.07543v1", "section": "introduction", "text": "are presented relative to training deep models from scratch , but as mentioned in ( Goodfellow , Bengio , & Courville , 2016 ) , deep learning generally only achieves reasonable performance at about 5000 examples per class and is therefore not necessarily the best paradigm at these scales . This is shown quantitatively in ( Chen , Mckeever , & Delany , 2018 ) where , at scales of 2000+ labels per class , an SVM outperforms several deep learning approaches on text classification tasks . As such , we propose that to evaluate the low-shot learning benefits of deep transfer learning models , we should in fact look at performance against the strongest classical machine learning methods . However , we have yet to find a comprehensive quantitative study performing this analysis and show that low-shot transfer learning in NLP is actually the optimal approach when dealing with small quantities of data . In this paper we attempt to answer this question in the context of classification tasks . What is the best paradigm to use in the case where we have 100 -1000 labelled training examples per class -classical machine learning or deep transfer learning ?"}
{"chunk_id": "1907.07543v1_introduction_0008", "paper_id": "1907.07543v1", "section": "introduction", "text": "with small quantities of data . In this paper we attempt to answer this question in the context of classification tasks . What is the best paradigm to use in the case where we have 100 -1000 labelled training examples per class -classical machine learning or deep transfer learning ? We seek to compare the best-in-class approaches from both deep transfer learning and classical machine learning by training a variety of models and evaluating by analysing intra-domain and inter-domain performance ( details in section 2 ) . The choice of 100 -1000 is motivated by the amount of data feasible for companies and researchers to tag in-house , as well as the scale of data occurring organically through other means . For example , in marketing these figures typically represent the base sizes of surveys that can be used as training data . The rest of this paper is laid out as follows . Section 2 details the datasets we use . Section 3 looks at the methodology used to evaluate the optimal paradigm . In section 4 we present the algorithms we use to test , along with related work influencing our choices in selecting those models ."}
{"chunk_id": "1907.07543v1_introduction_0009", "paper_id": "1907.07543v1", "section": "introduction", "text": "laid out as follows . Section 2 details the datasets we use . Section 3 looks at the methodology used to evaluate the optimal paradigm . In section 4 we present the algorithms we use to test , along with related work influencing our choices in selecting those models . Section 5 details our experiments including choosing the optimal configuration of hyperparameters and preprocessing for each algorithm . In section 6 we present the results followed by our comments and conclusions . Finally , we highlight a few key points and considerations worthy of mention for the two paradigms in 7 ."}
{"chunk_id": "2006.15680v1_title_0000", "paper_id": "2006.15680v1", "section": "title", "text": "MODELING GENERALIZATION IN MACHINE LEARNING : A METHODOLOGICAL AND COMPUTATIONAL STUDY"}
{"chunk_id": "2006.15680v1_abstract_0001", "paper_id": "2006.15680v1", "section": "abstract", "text": "As machine learning becomes more and more available to the general public , theoretical questions are turning into pressing practical issues . Possibly , one of the most relevant concerns is the assessment of our confidence in trusting machine learning predictions . In many real-world cases , it is of utmost importance to estimate the capabilities of a machine learning algorithm to generalize , i.e. , to provide accurate predictions on unseen data , depending on the characteristics of the target problem . In this work we perform a meta-analysis of 109 publicly-available classification data sets , modeling machine learning generalization as a function of a variety of data set characteristics , ranging from number of samples to intrinsic dimensionality , from class-wise feature skewness to F 1 evaluated on test samples falling outside the convex hull of the training set . Experimental results demonstrate the relevance of using the concept of the convex hull of the training data in assessing machine learning generalization , by emphasizing the difference between interpolated and extrapolated predictions ."}
{"chunk_id": "2006.15680v1_abstract_0002", "paper_id": "2006.15680v1", "section": "abstract", "text": "to F 1 evaluated on test samples falling outside the convex hull of the training set . Experimental results demonstrate the relevance of using the concept of the convex hull of the training data in assessing machine learning generalization , by emphasizing the difference between interpolated and extrapolated predictions . Besides several predictable correlations , we observe unexpectedly weak associations between the generalization ability of machine learning models and all metrics related to dimensionality , thus challenging the common assumption that the curse of dimensionality might impair generalization in machine learning ."}
{"chunk_id": "2006.15680v1_introduction_0003", "paper_id": "2006.15680v1", "section": "introduction", "text": "Introduction The term machine learning ( ML ) traditionally includes algorithms that are able to improve their performance on a specific task over time , given an increasing amount of relevant data [ 1 ] . In recent years , this field of research is enjoying a growing popularity , driven by the breakthrough of Deep Learning [ 2 ] and an impressive track record of success stories in different fields , ranging from natural language processing [ 3 ] to autonomous vehicles [ 4 ] , image classification [ 5 ] human-competitive performance in boardgames [ 6 ] . An interesting online collection about various uses of ML has been compiled by the journal Nature is late 2018 1 , although the fast pace the field is progressing made it to appear outdated after few quarters . As out-of-the-box ML solutions are becoming increasingly available to both researchers and the general public [ 7 , 8 , 9 , 10 ] , theoretical questions are suddenly turning into practical issues . Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ?"}
{"chunk_id": "2006.15680v1_introduction_0004", "paper_id": "2006.15680v1", "section": "introduction", "text": "solutions are becoming increasingly available to both researchers and the general public [ 7 , 8 , 9 , 10 ] , theoretical questions are suddenly turning into practical issues . Among all common inquiries , perhaps the most basic is : can ML work on a specific problem ? Or , in other words : given the characteristics of a target data set , can the effectiveness of a ML approach be predicted ? Interestingly , this latter question can be further rephrased as : what are the characteristics of a data set that are well correlated with the possibility , or the impossibility , of obtaining ML models able to effectively extrapolate to unknown instances of the problem ? It is well known that ML algorithms are affected by the curse of dimensionality [ 11 ] , but ML practitioners also know that it could be possible to obtain reliable models even for high-dimensional data sets , and with a relatively small number of samples [ 12 ] ."}
{"chunk_id": "2006.15680v1_introduction_0005", "paper_id": "2006.15680v1", "section": "introduction", "text": "It is well known that ML algorithms are affected by the curse of dimensionality [ 11 ] , but ML practitioners also know that it could be possible to obtain reliable models even for high-dimensional data sets , and with a relatively small number of samples [ 12 ] . The common approach among practitioners in the field , when dealing with a new data set , seems to be : try as many different ML algorithms as possible in a cross-validation , and evaluate the outcomes ; then focus on the techniques that provided the best results , possibly applying them in an ensemble [ 13 ] . Taking inspiration from [ 14 ] , where the authors find links between data set characteristics and efficiency of feature selection techniques , we propose to empirically explore the relation between data-set characteristics and effectiveness of standard ML models , in order to obtain a general meta-model able to extrapolate . In order to answer the question , we analyzed 109 publicly available classification data sets from open-access , curated sources ."}
{"chunk_id": "2006.15680v1_introduction_0006", "paper_id": "2006.15680v1", "section": "introduction", "text": ", we propose to empirically explore the relation between data-set characteristics and effectiveness of standard ML models , in order to obtain a general meta-model able to extrapolate . In order to answer the question , we analyzed 109 publicly available classification data sets from open-access , curated sources . We decided to focus on classification , as supervised ML represents a quite significant portion of real-world problems ; and , differently from regression , several sophisticated quality metrics have already been developed for this task [ 15 ] . During the analysis , we take into account characteristics such as number of features , number of classes , number of samples , and we look for correlations with quality metrics , such as accuracy of a ML model on training and test points . Extrapolation is assessed not just by alternatively dividing the data into training and test sets , but by analyzing whether data points fall inside or outside of the convex hull of the training data ."}
{"chunk_id": "2006.15680v1_introduction_0007", "paper_id": "2006.15680v1", "section": "introduction", "text": "metrics , such as accuracy of a ML model on training and test points . Extrapolation is assessed not just by alternatively dividing the data into training and test sets , but by analyzing whether data points fall inside or outside of the convex hull of the training data . After collecting the meta-data on the performance of a state-of-the-art classification algorithm on the data sets , the statistical analysis presents both predictable and surprising results , hinting at the fact that dimensionality might not be so cursed after all . Main contributions 1 . We ran a quantitative evaluations of ML models over 109 publicly available data sets . 2 . In Section 2 and Section 3 we provide a general overview on generalization in ML , and of the so-called curse of dimensionality ."}
{"chunk_id": "2007.01503v1_title_0000", "paper_id": "2007.01503v1", "section": "title", "text": "Mathematical Perspective of Machine Learning"}
{"chunk_id": "2007.01503v1_abstract_0001", "paper_id": "2007.01503v1", "section": "abstract", "text": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation , gradient descent as the default optimization algorithm , limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective ."}
{"chunk_id": "2007.01503v1_introduction_0002", "paper_id": "2007.01503v1", "section": "introduction", "text": "Introduction In the nutshell the idea of training a neural network ( NN ) is equivalent to the problem approximation of a given function , f , with the domain , D , and codomain , C , f : D → C ( 1 ) which depends on some data of size N ∈ N of k-dimensional input vectors , x j ∈ D ⊆ R k and l-dimensional output ( label ) vectors , y j ∈ C ⊆ R l , by a composition of functions of the form P i ( z i-1 • w i ) = ( P i ( z i-1 • w i ) , ... , P i ( z i-1 • w i ) ) , ( 2 ) where P i is called an activation function of layer i , z i-1 is an output vector of the layer i -1 , and w i is called the weight vector of layer i ."}
{"chunk_id": "2007.01503v1_introduction_0003", "paper_id": "2007.01503v1", "section": "introduction", "text": "P i ( z i-1 • w i ) ) , ( 2 ) where P i is called an activation function of layer i , z i-1 is an output vector of the layer i -1 , and w i is called the weight vector of layer i . Once the size of each layer and the choice of each activation function is made , one usually uses , so called , back propagation algorithm , adjusting the values of each weight vector according to some type of gradient descent rule . In other words , one is trying to solve an optimization problem , minimizing the `` difference norm '' C : = f -f d , where f = P r P r-1 ( . . . P 1 ) ( 3 ) and r ∈ N is the number of layers of the neural network . So apriori , we are making a choice of the function f of weights w 1 , . . . , w r . Note that the dimension of each vector w i is the size of the layer i ."}
{"chunk_id": "2007.01503v1_introduction_0004", "paper_id": "2007.01503v1", "section": "introduction", "text": "the number of layers of the neural network . So apriori , we are making a choice of the function f of weights w 1 , . . . , w r . Note that the dimension of each vector w i is the size of the layer i . To simplify the problem we may always find the maximum , m , of the size layers , and assume that each w i ∈ R m . We are implicitly assuming that for each i = 1 , ... , r , P i ( 0 ) = 0. foot_0 2 Existence of function f and the toll of cost function C First thing to consider , given a labeled data set { ( x j , y j ) : j = 1 , . . . , N } , if there is a representation function f such that f ( x j ) = y j for all j = 1 , . . . , N . This important step is often overlooked in practice ."}
{"chunk_id": "2007.01503v1_introduction_0005", "paper_id": "2007.01503v1", "section": "introduction", "text": ": j = 1 , . . . , N } , if there is a representation function f such that f ( x j ) = y j for all j = 1 , . . . , N . This important step is often overlooked in practice . In theory , if there is x j = x k such that y j = y k ( 4 ) then no such function exist . In other words f is a function of more variables than provided in the data set and the idea approximation by f is meaningless ."}
{"chunk_id": "2007.01503v1_introduction_0006", "paper_id": "2007.01503v1", "section": "introduction", "text": ", if there is x j = x k such that y j = y k ( 4 ) then no such function exist . In other words f is a function of more variables than provided in the data set and the idea approximation by f is meaningless . One may always assume some measurement error ε > 0 ( noise ) of the data set and consider instead a weaker condition x j = x k =⇒ y j -y k l 2 < ε ( 5 ) necessary for existence of a function f. In such case one may look at the average values of duplicate points f ave = x j = x k y k x j = x k 1 ( 6 ) and choose to approximate f ave instead of f. There is no guarantee that a good approximation f of function f ave is a good approximation of f itself . One should also consider the norm • d of the approximation function f. It is well known that various classes of `` nice '' functions are dense in L p spaces ."}
{"chunk_id": "2007.01503v1_introduction_0007", "paper_id": "2007.01503v1", "section": "introduction", "text": "guarantee that a good approximation f of function f ave is a good approximation of f itself . One should also consider the norm • d of the approximation function f. It is well known that various classes of `` nice '' functions are dense in L p spaces . In particular , the class of functions f defined in ( 3 ) are dense with respect to convergence in measure and L p norm ( see [ 1 ] ) . This important fact implies that given any functions f and any ε > 0 , there is a function f s.t . f -f L p < ε . ( 7 ) The approximation function f is a function of weights vectors w i . The pursuit of such function f is a two part problem . The first part , defining the structure of the neural network , is done by a human . The methodology behind the choice of NN structures is at the stage of experimental science . The second part , weights optimization , is done by a computer , usually capable of trillions of operations per second ."}
{"chunk_id": "2007.01503v1_introduction_0008", "paper_id": "2007.01503v1", "section": "introduction", "text": "structure of the neural network , is done by a human . The methodology behind the choice of NN structures is at the stage of experimental science . The second part , weights optimization , is done by a computer , usually capable of trillions of operations per second . Needless to say that the effectiveness of latter part depends heavily on the former . In practice one usually does not use an approximation with respect to L p norm . Computationally one may only evaluate the function f at finitely many points and approximate it by f at such points , often with respect to the • l p norm . Since for all 0 < p < q < ∞ , f ( y j ) -f ( y j ) l q f ( y j ) -f ( y j ) l p , ( 8 ) one can choose any p > 0 to obtain the approximation for all q p. foot_1 Here is an interesting question ."}
{"chunk_id": "2007.01503v1_introduction_0009", "paper_id": "2007.01503v1", "section": "introduction", "text": "f ( y j ) -f ( y j ) l q f ( y j ) -f ( y j ) l p , ( 8 ) one can choose any p > 0 to obtain the approximation for all q p. foot_1 Here is an interesting question . Given f ∈ L p ( R k ) , does it follow that the sequence { f ( y j ) } ∞ j=1 ∈ l p for any choice y j ∈ R k ? One can easily show it is not the case . What about a sequence of randomly chosen points y j ∈ R k ? In this case the answer is affirmative . What about the converse statement ? Given a sequence { f ( y j ) } ∞ j=1 ∈ l p , does it follow that f ∈ L p ( R k ) ? What if for any sequence of points { y j } ∞ j=1 in the domain of f , the sequence { f ( y j ) } ∞ j=1 belongs to l p ?"}
{"chunk_id": "2007.01503v1_introduction_0010", "paper_id": "2007.01503v1", "section": "introduction", "text": "p , does it follow that f ∈ L p ( R k ) ? What if for any sequence of points { y j } ∞ j=1 in the domain of f , the sequence { f ( y j ) } ∞ j=1 belongs to l p ? What if the measure µ of the domain D of f is not a Lebesgue measure ? Things get even more bizarre if the set function µ is only finitely additive . In some cases L p spaces may not be complete for any p > 0 . In Measure Theory , `` functions '' that agree almost everywhere are indistinguishable . In case of finitely additive measures the equivalence classes of a `` function '' are often more complex . Why would anyone care about finitely ( and not countably ) additive measure on D ? From the point of view of Constructive Mathematics , it is impossible to verify countable additivity of µ in the first place ."}
{"chunk_id": "2008.08080v2_title_0000", "paper_id": "2008.08080v2", "section": "title", "text": "mlr3proba : An R Package for Machine Learning in Survival Analysis"}
{"chunk_id": "2008.08080v2_abstract_0001", "paper_id": "2008.08080v2", "section": "abstract", "text": "picture ( 0,0 ) ( -35,0 ) ( 1,0 ) 30 ( 0,35 ) ( 0 , -1 ) 30 picture picture ( 0,0 ) ( 35,0 ) ( -1,0 ) 30 ( 0,35 ) ( 0 , -1 ) 30 picture `` mlr3proba '' -2020/12/15 -page 1 - # 1 picture ( 0,0 ) ( -35,0 ) ( 1,0 ) 30 ( 0 , -35 ) ( 0,1 ) 30 picture picture ( 0,0 ) ( 35,0 ) ( -1,0 ) 30 ( 0 , -35 ) ( 0,1 ) 30 picture Doc-Start"}
{"chunk_id": "2008.08080v2_introduction_0002", "paper_id": "2008.08080v2", "section": "introduction", "text": "Introduction Survival analysis is the field of statistics concerned with the estimation of time-to-event distributions while accounting for censoring and truncation . mlr3proba introduces survival modelling to the mlr3 ( Lang et al. , 2019a ) ecosystem of machine learning packages . By utilising a probabilistic supervised learning ( Gressmann et al. , 2018 ) framework mlr3proba allows for multiple survival analysis predictions : predicting the time to an event , the probability of an event over time , and the relative risk of an event . mlr3proba includes an extensive collection of classical and machine learning models and many specialised survival measures . The R programming language ( R Core Team , 2020 ) provides extensive support for both survival analysis and machine learning via its core functionality and through open-source add-on packages available from CRAN and Bioconductor . mlr3proba leverages these packages by connecting a multitude of machine learning models and measures for survival analysis . mlr3proba currently supports simulation of survival data , classical survival models , prediction of survival distributions by machine learning , and support for high-dimensional data ."}
{"chunk_id": "2008.08080v2_introduction_0003", "paper_id": "2008.08080v2", "section": "introduction", "text": "available from CRAN and Bioconductor . mlr3proba leverages these packages by connecting a multitude of machine learning models and measures for survival analysis . mlr3proba currently supports simulation of survival data , classical survival models , prediction of survival distributions by machine learning , and support for high-dimensional data . Interfacing other packages in the mlr3 family provides functionality for optimisation , tuning , benchmarking , and more ."}
{"chunk_id": "2103.11249v1_title_0000", "paper_id": "2103.11249v1", "section": "title", "text": "SELM : Software Engineering of Machine Learning Models"}
{"chunk_id": "2103.11249v1_abstract_0001", "paper_id": "2103.11249v1", "section": "abstract", "text": "One of the pillars of any machine learning model is its concepts . Using software engineering , we can engineer these concepts and then develop and expand them . In this article , we present a SELM framework for Software Engineering of machine Learning Models . We then evaluate this framework through a case study . Using the SELM framework , we can improve a machine learning process efficiency and provide more accuracy in learning with less processing hardware resources and a smaller training dataset . This issue highlights the importance of an interdisciplinary approach to machine learning . Therefore , in this article , we have provided interdisciplinary teams ' proposals for machine learning ."}
{"chunk_id": "2103.11249v1_introduction_0002", "paper_id": "2103.11249v1", "section": "introduction", "text": "Introduction Machine learning usually aims to find and develop a computational model for an intelligent task on a practical problem . Development based on calculation can be called engineering ( 1 ) . In software engineering , software systems are calculated and engineered by models . In fact , these software models are platforms for analysis , design , development , and system engineering . For modeling in software engineering , we need several elements : 1 . The modeling perspective , 2 . The system under modeling , and 3 . Modeling language and tools ( 5 ) . So we look at a system under modeling from one or several perspectives , and we discover a set of meanings about that system . Using the modeling language and tool , we express and record those perceived meanings of the system . This allows us to engineer the system under modeling ( as-is system or to-be system ) by changing and transforming it . We could perhaps consider the modeling perspective as the most crucial part of this conceptual architecture ."}
{"chunk_id": "2103.11249v1_introduction_0003", "paper_id": "2103.11249v1", "section": "introduction", "text": "we express and record those perceived meanings of the system . This allows us to engineer the system under modeling ( as-is system or to-be system ) by changing and transforming it . We could perhaps consider the modeling perspective as the most crucial part of this conceptual architecture . Because if we do not use a proper perspective or perspectives to look at the meanings of a system , it is practically impossible for us to achieve a model , start modeling , and ultimately develop through engineering calculations . To obtain the proper perspective or perspectives of a system , we need human intuition of that system , meaning there is a deep connection between the modeler 's intuition and system engineering ( including software , hardware , and intelligence ) . Therefore , we can consider human intuition as the infrastructure for system engineering activities ."}
{"chunk_id": "2110.12773v1_title_0000", "paper_id": "2110.12773v1", "section": "title", "text": "Scientific Machine Learning Benchmarks"}
{"chunk_id": "2110.12773v1_abstract_0001", "paper_id": "2110.12773v1", "section": "abstract", "text": "The breakthrough in Deep Learning neural networks has transformed the use of AI and machine learning technologies for the analysis of very large experimental datasets . These datasets are typically generated by large-scale experimental facilities at national laboratories . In the context of science , scientific machine learning focuses on training machines to identify patterns , trends , and anomalies to extract meaningful scientific insights from such datasets . With a new generation of experimental facilities , the rate of data generation and the scale of data volumes will increasingly require the use of more automated data analysis . At present , identifying the most appropriate machine learning algorithm for the analysis of any given scientific dataset is still a challenge for scientists . This is due to many different machine learning frameworks , computer architectures , and machine learning models . Historically , for modelling and simulation on HPC systems such problems have been addressed through benchmarking computer applications , algorithms , and architectures . Extending such a benchmarking approach and identifying metrics for the application of machine learning methods to scientific datasets is a new challenge for both scientists and computer scientists ."}
{"chunk_id": "2110.12773v1_abstract_0002", "paper_id": "2110.12773v1", "section": "abstract", "text": "modelling and simulation on HPC systems such problems have been addressed through benchmarking computer applications , algorithms , and architectures . Extending such a benchmarking approach and identifying metrics for the application of machine learning methods to scientific datasets is a new challenge for both scientists and computer scientists . In this paper , we describe our approach to the development of scientific machine learning benchmarks and review other approaches to benchmarking scientific machine learning ."}
{"chunk_id": "2110.12773v1_introduction_0003", "paper_id": "2110.12773v1", "section": "introduction", "text": "Introduction In the past decade , a sub-field of artificial intelligence ( AI ) , namely Deep Learning ( DL ) neural networks ( or deep neural networks , DNNs ) , has made significant breakthroughs in many scientifically and commercially 2 important applications 1 . Such neural networks are themselves a subset of a wide range of machine learning ( ML ) methods ( Figure 1 . ) ML methods have been widely used for many years in several domains of science , but DNNs have been transformational and are gaining a lot of traction in many scientific communities 3 . Most of the national laboratories that host large-scale experimental facilities are now relying on DNN-based data analytic methods to extract scientific insights from their increasingly large datasets . A recent spectacular success is DeepMind 's use of Deep Learning in their Alpha Fold-1 and Alpha Fold-2 4 solutions to the protein folding 'Grand Challenge ' . This promises to transform much of biological science and open up exciting new research avenues . Other domains of science are exploring physical representations of the system with the data-driven learning ability of neural networks ."}
{"chunk_id": "2110.12773v1_introduction_0004", "paper_id": "2110.12773v1", "section": "introduction", "text": "Fold-1 and Alpha Fold-2 4 solutions to the protein folding 'Grand Challenge ' . This promises to transform much of biological science and open up exciting new research avenues . Other domains of science are exploring physical representations of the system with the data-driven learning ability of neural networks . Current developments are towards specialising these ML approaches to be more domain-specific and domain-aware [ 5 ] [ 6 ] [ 7 ] , and aiming to connect the apparent 'black box ' successes of DL networks with well-understood approaches from science . The overarching scope of ML in science is very broad , including identifying patterns , anomalies , and trends from relevant scientific datasets , and using ML for classification and predicting of those patterns , clustering of data , and generating near-realistic synthetic data . There are three approaches for developing ML-based solutions , namely , supervised , unsupervised , and reinforcement learning . In supervised learning , the ML model is trained for a given task with examples . In order to have examples , the data used for training the ML model must contain the ground truth or labels ."}
{"chunk_id": "2110.12773v1_introduction_0005", "paper_id": "2110.12773v1", "section": "introduction", "text": "solutions , namely , supervised , unsupervised , and reinforcement learning . In supervised learning , the ML model is trained for a given task with examples . In order to have examples , the data used for training the ML model must contain the ground truth or labels . Supervised learning is therefore only possible when there is a labelled subset of the data . Once trained , the learned model can be deployed for real-time usage , such as pattern classification or estimation -- -which is often referred to as inference . Because of the difficulty in generating labelled data for supervised learning , particularly for experimental datasets , it is often difficult to apply supervised learning directly . To circumvent this limitation , training is often performed on simulated data , which provides an opportunity to have relevant labels . However , the simulated data may not be representative of the real data and the model may therefore not perform satisfactorily when used for inferencing . The unsupervised learning technique , in contrast , does not rely on labels ."}
{"chunk_id": "2110.12773v1_introduction_0006", "paper_id": "2110.12773v1", "section": "introduction", "text": ", which provides an opportunity to have relevant labels . However , the simulated data may not be representative of the real data and the model may therefore not perform satisfactorily when used for inferencing . The unsupervised learning technique , in contrast , does not rely on labels . A simple example of this technique is clustering , where the aim is to identify several groups of data points that have common features . Another example is identification of anomalies in data . Example algorithms include k-Means Clustering 8 , Support Vector Machines ( SVM ) 9 , or neural network-based autoencoders 10 . Finally , reinforcement learning relies on a trial-and-error approach to learn a given task with the learning system being positively rewarded whenever the system behaves correctly , and penalised whenever it behaved incorrectly 11 . Each of these learning paradigms have a large number of algorithms , and modern developmental approaches are often hybrid and use one of more of these techniques together . This leaves a very large choice of ML algorithms for any given problem ."}
{"chunk_id": "2110.12773v1_introduction_0007", "paper_id": "2110.12773v1", "section": "introduction", "text": "penalised whenever it behaved incorrectly 11 . Each of these learning paradigms have a large number of algorithms , and modern developmental approaches are often hybrid and use one of more of these techniques together . This leaves a very large choice of ML algorithms for any given problem . In practice , the selection of an ML algorithm for a given scientific problem is more complex than just selecting one of the machine learning technologies and any particular algorithm . The selection of the most effective ML algorithm is based on many factors , including the type , quantity , and quality of the training data , the availability of labelled data , the type of problem being addressed ( prediction , classification , and so on ) , the overall accuracy and performance required , and the hardware systems available for training and inferencing . With such a multi-dimensional problem consisting of a choice of ML algorithms , hardware architectures , and a range of scientific problems , selecting an optimal ML algorithm for a given task is not trivial ."}
{"chunk_id": "2110.12773v1_introduction_0008", "paper_id": "2110.12773v1", "section": "introduction", "text": "and performance required , and the hardware systems available for training and inferencing . With such a multi-dimensional problem consisting of a choice of ML algorithms , hardware architectures , and a range of scientific problems , selecting an optimal ML algorithm for a given task is not trivial . This constitutes a significant barrier for many scientists wishing to use modern ML methods in their scientific research . In this paper we use suitable scientific ML benchmarks to develop guidelines and best practices to assist the scientific community in successfully exploiting these methods . Moreover , developing such guidelines and best practices at the community level will not only benefit the science community but also highlight where further research into ML algorithms , computer architectures , and software solutions for using ML in scientific applications is needed . Such guidelines and best practices need to be based on real-world application examples and relevant data . For instance , demonstrating the success of a specific ML technique on a specific scientific problem will assist researchers in applying the technique to similar problems . We refer to the development of guidelines and best practices as Benchmarking ."}
{"chunk_id": "2110.12773v1_introduction_0009", "paper_id": "2110.12773v1", "section": "introduction", "text": "be based on real-world application examples and relevant data . For instance , demonstrating the success of a specific ML technique on a specific scientific problem will assist researchers in applying the technique to similar problems . We refer to the development of guidelines and best practices as Benchmarking . In our case , this is very specific to ML techniques applied to scientific datasets . The applications used to demonstrate the guideline and best practices are referred to as Benchmarks . The notion of benchmarking computer systems and applications has been a fundamental cornerstone of computer science , particularly for compiler , architectural and system development , with a key focus on using benchmarks for ranking systems , such as the Top500 or Green500 [ 12 ] [ 13 ] [ 14 ] [ 15 ] [ 16 ] . However , our notion of scientific ML benchmarking has a different focus . Firstly , these machine learning benchmarks can be considered as blueprints for use on a range of scientific problems , and hence are aimed at fostering the use of ML in science more generally ."}
{"chunk_id": "2110.12773v1_introduction_0010", "paper_id": "2110.12773v1", "section": "introduction", "text": "] . However , our notion of scientific ML benchmarking has a different focus . Firstly , these machine learning benchmarks can be considered as blueprints for use on a range of scientific problems , and hence are aimed at fostering the use of ML in science more generally . Secondly , by using these ML benchmarks , a number of aspects in an ML ecosystem can be compared and contrasted . For example , it is possible to rank different computer architectures for their performance , or to rank different ML algorithms for their effectiveness . Thirdly , these ML benchmarks are accompanied by relevant dataset ( s ) on which the training and/or inference will be based . This is different to conventional benchmarks for high-performance computing ( HPC ) where there is little dependency on datasets . The establishment of a set of open curated datasets with associated ML benchmarks is therefore an important step for scientists to be able to effectively utilise ML methods in their research and also to identify further directions for ML research ."}
{"chunk_id": "2110.12773v1_introduction_0011", "paper_id": "2110.12773v1", "section": "introduction", "text": ") where there is little dependency on datasets . The establishment of a set of open curated datasets with associated ML benchmarks is therefore an important step for scientists to be able to effectively utilise ML methods in their research and also to identify further directions for ML research . In this paper , we first discuss what we mean by scientific machine learning benchmarks , the scope of such benchmarks , and the challenges in creating such benchmarks . We then review a number of benchmarking initiatives in light of this discussion . The paper is organised as follows . In Section 2 , we discuss the primary considerations in designing benchmarks to advance the application of ML methods for scientific research along with relevant examples . We then define the scope and challenges around establishing such scientific machine learning benchmarks in Section 3 . In Section 4 , we review a number of ML benchmarking initiatives in light of our discussions in Sections 2 and 3 . We then discuss SciMLBench , one of the most recent and versatile scientific ML benchmarking initiatives , in Section 5 . We summarise our findings and conclusions in Section 6 ."}
{"chunk_id": "2201.06921v1_title_0000", "paper_id": "2201.06921v1", "section": "title", "text": "Can Machine Learning be Moral ?"}
{"chunk_id": "2201.06921v1_introduction_0001", "paper_id": "2201.06921v1", "section": "introduction", "text": "Introduction The ethics of Machine Learning has become an unavoidable topic in the AI Community . The deployment of machine learning systems in multiple social contexts has resulted in a closer ethical scrutiny of the design , development , and application of these systems . The AI/ML community has come to terms with the imperative to think about the ethical implications of machine learning , not only as a product but also as a practice ( Birhane , 2021 ; Shen et al . 2021 ) . The critical question that is troubling many debates is what can constitute an ethically accountable machine learning system . In this paper we explore possibilities for ethical evaluation of machine learning methodologies . We scrutinize techniques , methods and technical practices in machine learning from a relational ethics perspective , taking into consideration how machine learning systems are part of the world and how they relate to different forms of agency . Taking a page from Phil Agre ( 1997 ) we use the notion of a critical technical practice as a means of analysis of machine learning approaches ."}
{"chunk_id": "2201.06921v1_introduction_0002", "paper_id": "2201.06921v1", "section": "introduction", "text": "taking into consideration how machine learning systems are part of the world and how they relate to different forms of agency . Taking a page from Phil Agre ( 1997 ) we use the notion of a critical technical practice as a means of analysis of machine learning approaches . Our radical proposal is that supervised learning appears to be the only machine learning method that is ethically defensible ."}
{"chunk_id": "2205.00210v1_title_0000", "paper_id": "2205.00210v1", "section": "title", "text": "Software Testing for Machine Learning"}
{"chunk_id": "2205.00210v1_abstract_0001", "paper_id": "2205.00210v1", "section": "abstract", "text": "Machine learning has become prevalent across a wide variety of applications . Unfortunately , machine learning has also shown to be susceptible to deception , leading to errors , and even fatal failures . This circumstance calls into question the widespread use of machine learning , especially in safety-critical applications , unless we are able to assure its correctness and trustworthiness properties . Software verification and testing are established technique for assuring such properties , for example by detecting errors . However , software testing challenges for machine learning are vast and profuse -yet critical to address . This summary talk discusses the current state-of-the-art of software testing for machine learning . More specifically , it discusses six key challenge areas for software testing of machine learning systems , examines current approaches to these challenges and highlights their limitations . The paper provides a research agenda with elaborated directions for making progress toward advancing the state-of-the-art on testing of machine learning . Index termstesting challenges , machine learning , machine learning testing , testing ML , testing AI"}
{"chunk_id": "2205.00210v1_introduction_0002", "paper_id": "2205.00210v1", "section": "introduction", "text": "Introduction Applications of machine learning ( ML ) technology have become vital in many innovative domains . At the same time , the vulnerability of ML has become evident , sometimes leading to catastrophic failures foot_0 . This entails that comprehensive testing of ML needs to be performed , to ensure the correctness and trustworthiness of ML-enabled systems . Software testing of ML systems is susceptible to a number of challenges compared to testing of traditional software systems . In this paper , by traditional systems we mean software systems not integrating ML , and by ML systems we mean software systems containing ML-trained components ( e.g self-driving cars , autonomous ships , or space exploration robots ) . As an example , one such challenge of testing ML systems stems from non-determinism intrinsic to ML . Traditional systems are typically pre-programmed and execute a set of rules , while ML systems reason in a probabilistic manner and exhibit non-deterministic behavior . This means that for constant test inputs and preconditions , an ML-trained software component can produce different outputs in consecutive runs . Researchers have tried using testing techniques from traditional software development ( Hutchison et al ."}
{"chunk_id": "2205.00210v1_introduction_0003", "paper_id": "2205.00210v1", "section": "introduction", "text": "while ML systems reason in a probabilistic manner and exhibit non-deterministic behavior . This means that for constant test inputs and preconditions , an ML-trained software component can produce different outputs in consecutive runs . Researchers have tried using testing techniques from traditional software development ( Hutchison et al . 2018 ) , to deal with some of these challenges . However , it has been observed that traditional testing approaches in general fail to adequately address fundamental challenges of testing ML ( Helle and Schamai 2016 ) , and that these traditional approaches require adaptation to the new context of ML . The better we understand current research challenges of testing ML , the more successful we can be in developing novel techniques that effectively address these challenges and advance this scientific field . In this paper , we : i ) identify and discuss the most challenging areas in software testing for ML , ii ) synthesize the most promising approaches to these challenges , iii ) spotlight their limitations , and iv ) make recommendations of further research efforts on software testing of ML ."}
{"chunk_id": "2205.00210v1_introduction_0004", "paper_id": "2205.00210v1", "section": "introduction", "text": ", we : i ) identify and discuss the most challenging areas in software testing for ML , ii ) synthesize the most promising approaches to these challenges , iii ) spotlight their limitations , and iv ) make recommendations of further research efforts on software testing of ML . We note that the aim of the paper is not to exhaustively list all published work , but distill the most representative work ."}
{"chunk_id": "2205.14136v1_title_0000", "paper_id": "2205.14136v1", "section": "title", "text": "PSL is Dead . Long Live PSL"}
{"chunk_id": "2205.14136v1_abstract_0001", "paper_id": "2205.14136v1", "section": "abstract", "text": "Property Specification Language ( PSL ) is a form of temporal logic that has been mainly used in discrete domains ( e.g . formal hardware verification ) . In this paper , we show that by merging machine learning techniques with PSL monitors , we can extend PSL to work on continuous domains . We apply this technique in machine learning-based anomaly detection to analyze scenarios of real-time streaming events from continuous variables in order to detect abnormal behaviors of a system . By using machine learning with formal models , we leverage the strengths of both machine learning methods and formal semantics of time . On one hand , machine learning techniques can produce distributions on continuous variables , where abnormalities can be captured as deviations from the distributions . On the other hand , formal methods can characterize discrete temporal behaviors and relations that can not be easily learned by machine learning techniques . Interestingly , the anomalies detected by machine learning and the underlying time representation used are discrete events ."}
{"chunk_id": "2205.14136v1_abstract_0002", "paper_id": "2205.14136v1", "section": "abstract", "text": "captured as deviations from the distributions . On the other hand , formal methods can characterize discrete temporal behaviors and relations that can not be easily learned by machine learning techniques . Interestingly , the anomalies detected by machine learning and the underlying time representation used are discrete events . We implemented a temporal monitoring package ( TEF ) that operates in conjunction with normal data science packages for anomaly detection machine learning systems , and we show that TEF can be used to perform accurate interpretation of temporal correlation between events ."}
{"chunk_id": "2205.14136v1_introduction_0003", "paper_id": "2205.14136v1", "section": "introduction", "text": "I . INTRODUCTION Property Specification Language ( PSL ) is a form of temporal logic that is designed to capture temporal relations between discrete variables over discrete time . Due to this nature , PSL has been mainly used in hardware design and verification since it was standardized by IEEE in 2004 [ 1 ] , [ 2 ] , [ 8 ] . There have been attempts to extend PSL to deal with continuous variables over continuous time [ 6 ] . Due to its inherent limitation of expressibility , there have not been many successful applications . In recent years , anomaly detection has been widely used in practice [ 3 ] , [ 13 ] . There are many applications where realtime streaming events are monitored and analyzed in order to detect abnormal behaviors . For example , if the amount of free memory of a computer is below a certain threshold , it can be considered as an anomaly . As another example , if there is an anomalous drop in purchase of a product in an online store , it is possible that the product is out of stock , which needs attention ."}
{"chunk_id": "2205.14136v1_introduction_0004", "paper_id": "2205.14136v1", "section": "introduction", "text": "computer is below a certain threshold , it can be considered as an anomaly . As another example , if there is an anomalous drop in purchase of a product in an online store , it is possible that the product is out of stock , which needs attention . The state-of-the-art technique Fig . 1 . A New Framework for Anomaly Detection for anomaly detection is machine learning [ 4 ] , [ 7 ] , [ 9 ] , [ 11 ] , [ 12 ] , [ 14 ] . Machine learning techniques learn distributions on continuous variables . Anomaly events can be captured as deviations from established patterns ( distributions ) . However , there are certain temporal behaviors and relations that can not be easily learned by machine learning techniques , but can be easily characterized by formal languages such as PSL . In this paper , we propose a new framework called TEmporal Filtering ( TEF ) for anomaly detection ( Fig . 1 ) . The idea is to merge machine learning with PSL monitors ."}
{"chunk_id": "2205.14136v1_introduction_0005", "paper_id": "2205.14136v1", "section": "introduction", "text": "learning techniques , but can be easily characterized by formal languages such as PSL . In this paper , we propose a new framework called TEmporal Filtering ( TEF ) for anomaly detection ( Fig . 1 ) . The idea is to merge machine learning with PSL monitors . The machine learning module takes as input a number of continuous variables x 1 , x 2 , . . . . . . , x m , and outputs some discrete events y 1 , y 2 , . . . . . . , y n , which become the input of the PSL monitor . The PSL monitor encodes a user-defined temporal relation , which filters the output from the machine learning module . In this new framework , machine learning techniques extend the capability of PSL by discretizing continuous time and events ; the PSL monitor refines the results produced by the machine learning module . This combination of machine learning and formal methods yields a whole that is greater than the sum of its parts . The rest of this paper is organized as follows ."}
{"chunk_id": "2205.14136v1_introduction_0006", "paper_id": "2205.14136v1", "section": "introduction", "text": "discretizing continuous time and events ; the PSL monitor refines the results produced by the machine learning module . This combination of machine learning and formal methods yields a whole that is greater than the sum of its parts . The rest of this paper is organized as follows . In Section II , we give a brief introduction to anomaly detection . Section III discusses the overall architecture of TEF . In Section IV , we describe how TEF is implemented . Section V illustrates how TEF can be used to capture temporal relations . Section VI summarizes the conclusions of the paper and future work ."}
{"chunk_id": "2209.02057v3_title_0000", "paper_id": "2209.02057v3", "section": "title", "text": "APPLYING MACHINE LEARNING TO LIFE INSURANCE : SOME KNOWLEDGE SHARING TO MASTER IT PREPRINT"}
{"chunk_id": "2209.02057v3_abstract_0001", "paper_id": "2209.02057v3", "section": "abstract", "text": "Machine Learning permeates many industries , which brings new sources of benefits for companies . However , within the life insurance industry , Machine Learning is not widely used in practice as over the past years statistical models have shown their efficiency for risk assessment . Insurers may thus face difficulties assessing the value of the artificial intelligence . Focusing on the modification of the life insurance industry over time highlights the stake of using Machine Learning for insurers and benefits that it can bring by unleashing data value . This paper reviews traditional actuarial methodologies for survival modeling and extends them with Machine Learning techniques . It points out differences with regular machine learning models and emphasizes the importance of specific implementations to face censored data with the Machine Learning models family . In complement to this article , a Python library has been developed . Different open-source Machine Learning algorithms have been adjusted to adapt the specificities of life insurance data , namely censoring and truncation . Such models can be easily applied from this SCOR library to accurately model life insurance risks ."}
{"chunk_id": "2209.02057v3_abstract_0002", "paper_id": "2209.02057v3", "section": "abstract", "text": "to this article , a Python library has been developed . Different open-source Machine Learning algorithms have been adjusted to adapt the specificities of life insurance data , namely censoring and truncation . Such models can be easily applied from this SCOR library to accurately model life insurance risks . This library is briefly presented in section 5 Why consider Machine Learning for risk modeling ? The life insurance industry became more complex as the modern life insurance industry now offers all kinds of products covering death events or other hazards related to the health condition of the insured . The main products are covering Mortality , Critical Illness , Disability , Medical Expenses , and Longevity . Besides , thanks to technological advances and data storage capacity improvements , information considered for risk assessment has increased a lot and is still increasing . For instance , nowadays , life insurance applicants are asked to share , besides their age , part of their medical history , financial situation , and their profession ."}
{"chunk_id": "2209.02057v3_abstract_0003", "paper_id": "2209.02057v3", "section": "abstract", "text": "and data storage capacity improvements , information considered for risk assessment has increased a lot and is still increasing . For instance , nowadays , life insurance applicants are asked to share , besides their age , part of their medical history , financial situation , and their profession . In addition to this new information , thanks to the seniority of the industry , insurers may also have access to centuries of data accumulation to deepen their knowledge ."}
{"chunk_id": "2209.02057v3_introduction_0004", "paper_id": "2209.02057v3", "section": "introduction", "text": "As the information available is increasing over time , new challenges for insurance companies and actuaries are arising , namely efficiently extracting and analyzing information from a large amount of data to assess risk better . • Machine Learning as a solution to deal with huge databases For many years , only a limited amount of information about the applicants was collected . For instance , at a time , only age , gender , and smoking status of an insured were used to assess some biometric risks . Therefore , simple models such as linear regressions or classifications were considered sufficient to grasp the risk . However , by using such simple models the potential of the current increasing amount of data in the insurance industry may not be optimally tapped into . Indeed , insurers are handling larger and larger databases , both vertically ( very large amount of policies sold ) and horizontally ( numerous features collected for an insured ) . This may lead to difficulties in the calibration of simple models such as linear regressions ."}
{"chunk_id": "2209.02057v3_introduction_0005", "paper_id": "2209.02057v3", "section": "introduction", "text": "tapped into . Indeed , insurers are handling larger and larger databases , both vertically ( very large amount of policies sold ) and horizontally ( numerous features collected for an insured ) . This may lead to difficulties in the calibration of simple models such as linear regressions . • And to tackle new types of data In addition to collecting more data , insurers are also collecting new types of data that can not be handled by traditional statistical models . This is the case for textual data , or badly structured data , such as large databases that integrate correlated and non-linear relationships between variables . One can also mention real-time data collected on distributed storage systems in the cloud via connected objects . The number of steps in a day is an example of such real-time data . Thanks to the increased computing power , the insurance industry can use Machine Learning models to capture the increasingly complex information contained in these new and larger datasets ."}
{"chunk_id": "2305.00520v1_title_0000", "paper_id": "2305.00520v1", "section": "title", "text": "The ART of Transfer Learning : An Adaptive and Robust Pipeline"}
{"chunk_id": "2305.00520v1_abstract_0001", "paper_id": "2305.00520v1", "section": "abstract", "text": "Transfer learning is an essential tool for improving the performance of primary tasks by leveraging information from auxiliary data resources . In this work , we propose Adaptive Robust Transfer Learning ( ART ) , a flexible pipeline of performing transfer learning with generic machine learning algorithms . We establish the non-asymptotic learning theory of ART , providing a provable theoretical guarantee for achieving adaptive transfer while preventing negative transfer . Additionally , we introduce an ART-integrated-aggregating machine that produces a single final model when multiple candidate algorithms are considered . We demonstrate the promising performance of ART through extensive empirical studies on regression , classification , and sparse learning . We further present a real-data analysis for a mortality study ."}
{"chunk_id": "2305.00520v1_introduction_0002", "paper_id": "2305.00520v1", "section": "introduction", "text": "INTRODUCTION The age of rapid technological change is unfolding in real time , empowering the collection of massive amounts of data in a variety of fields . Despite this , many fields still struggle with data acquisition with limited sample sizes , particularly in experiments that involve human or animal subjects and can be prohibitively expensive . To improve the performance of the primary task on those occasions , transfer learning has been widely advocated as a means of leveraging knowledge from available auxiliary data that are different while related to the primary data . Successful applications of transfer learning in data-scarce fields include drug development ( Turki , Wei , & Wang 2017 ) , clinical trials ( Bellot & van der Schaar 2019 ) , and material sciences ( Hutchinson et al . 2017 ) , among others . For an overview of transfer learning methodologies and applications , interested readers may refer to survey papers by Pan and Yang ( 2009 ) , Weiss , Khoshgoftaar , and Wang ( 2016 ) , Niu , Liu , Wang , and Song ( 2020 ) , and Zhuang et al . ( 2020 ) ."}
{"chunk_id": "2305.00520v1_introduction_0003", "paper_id": "2305.00520v1", "section": "introduction", "text": "methodologies and applications , interested readers may refer to survey papers by Pan and Yang ( 2009 ) , Weiss , Khoshgoftaar , and Wang ( 2016 ) , Niu , Liu , Wang , and Song ( 2020 ) , and Zhuang et al . ( 2020 ) . Although transfer learning has achieved pervasive success and shows great promise , there is no guarantee that it will always improve performance -there is no free lunch for transfer learning . When a large discrepancy exists between the primary and auxiliary data , the performance of the primary estimator is likely to be negatively affected by auxiliary data . This phenomenon is referred to as `` negative transfer '' ( Rosenstein , Marx , Kaelbling , & Dietterich 2005 ; Z. Wang , Dai , Póczos , & Carbonell 2019 ) . Therefore , the success of a transfer learning method largely depends on its ability to be robust against negative transfer . As quoted from the survey paper Zhuang et al . ( 2020 ) , `` The negative transfer still needs further systematic analyses . ''"}
{"chunk_id": "2305.00520v1_introduction_0004", "paper_id": "2305.00520v1", "section": "introduction", "text": "Carbonell 2019 ) . Therefore , the success of a transfer learning method largely depends on its ability to be robust against negative transfer . As quoted from the survey paper Zhuang et al . ( 2020 ) , `` The negative transfer still needs further systematic analyses . '' In recent literature , the study of negative transfer has been embraced from the perspective of statistical guarantees for transfer learning . Bastani ( 2021 ) studied estimation and prediction in high-dimensional linear models with one informative auxiliary data , where the sample size of the auxiliary data is required to be larger than its dimension . Li , Cai , and Li ( 2021 ) proposed trans-lasso under a more general setting allowing for multiple auxiliary data , which can be even high-dimensional , i.e. , the size can be smaller than the dimension . Trans-lasso has been shown to improve the learning efficiency with known informative auxiliary data and can be robust to non-informative auxiliary data . The idea of trans-lasso was further extended to generalized linear models in Tian and Feng ( 2022 ) ."}
{"chunk_id": "2305.00520v1_introduction_0005", "paper_id": "2305.00520v1", "section": "introduction", "text": "size can be smaller than the dimension . Trans-lasso has been shown to improve the learning efficiency with known informative auxiliary data and can be robust to non-informative auxiliary data . The idea of trans-lasso was further extended to generalized linear models in Tian and Feng ( 2022 ) . Cai and Wei ( 2021 ) proposed an estimation algorithm with a faster convergence arXiv:2305.00520v1 [ stat.ML ] 30 Apr 2023 Algorithm 1 ART : Adaptive and Robust Transfer Learning Pipeline Input : Primary data T ( 0 ) = { ( x ( 0 ) i , y ( 0 ) i ) } n 0 i=1 and M auxiliary data T ( m ) = { ( x ( m ) i , y ( m ) i ) } nm i=1 , for m = 1 , 2 , . . . , M ."}
{"chunk_id": "2305.00520v1_introduction_0006", "paper_id": "2305.00520v1", "section": "introduction", "text": "y ( 0 ) i ) } n 0 i=1 and M auxiliary data T ( m ) = { ( x ( m ) i , y ( m ) i ) } nm i=1 , for m = 1 , 2 , . . . , M . 1 : Split the primary data into two parts : T ( 0 ) train = { ( x ( 0 ) i , y ( 0 ) i ) } n 0 , train i=1 and T ( 0 ) test = { ( x ( 0 ) i , y ( 0 ) i ) } n 0 i=n 0 , train +1 . 2 : Fit the model ĝ ( 0 ) by A ( T ( 0 ) train ) . 3 : for m = 1 to M do 4 : Stack T ( m ) with T ( 0 ) train to have T ( m ) , and obtain the model ĝ ( m ) by A ( T ( m ) ) ."}
{"chunk_id": "2306.14624v2_title_0000", "paper_id": "2306.14624v2", "section": "title", "text": "Insights From Insurance for Fair Machine Learning"}
{"chunk_id": "2306.14624v2_abstract_0001", "paper_id": "2306.14624v2", "section": "abstract", "text": "We argue that insurance can act as an analogon for the social situatedness of machine learning systems , hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature . Tracing the interaction of uncertainty , fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning . We link insurance fairness conceptions to their machine learning relatives , and use this bridge to problematize fairness as calibration . In this process , we bring to the forefront two themes that have been largely overlooked in the machine learning literature : responsibility and aggregate-individual tensions . See Baker ( , p ."}
{"chunk_id": "2306.14624v2_introduction_0002", "paper_id": "2306.14624v2", "section": "introduction", "text": "Introduction Insurance is `` interestingly uninteresting '' . In this work , we argue that in fact insurance is far from uninteresting and indeed a rich source of inspiration and insight to scholarship interested in social issues surrounding machine learning , specifically the field now known as fair machine learning . Our proposal is that insurance can be viewed as an analogon to machine learning with respect to these issues arising from the social situatedness . While machine learning is a relatively recent technology , debates regarding social issues in the context of insurance have been ongoing for a long time . Thus , we argue that taking inspiration from studies of insurance can contribute to a more integrative view of machine learning systems as socio-technical systems ( Selbst et al. , ) . Both machine learning and insurance are firmly based on a statistical , probabilistic mode of reasoningan actuarial mode . Indeed , insurance can be viewed as the first commercial test of probability theory ( Gigerenzer et al. , ; McFall , ) . Insurance , a technology for doing risk , transforms uncertainty into calculable risk ( Lehtonen & Van Hoyweghen , ) ."}
{"chunk_id": "2306.14624v2_introduction_0003", "paper_id": "2306.14624v2", "section": "introduction", "text": "of reasoningan actuarial mode . Indeed , insurance can be viewed as the first commercial test of probability theory ( Gigerenzer et al. , ; McFall , ) . Insurance , a technology for doing risk , transforms uncertainty into calculable risk ( Lehtonen & Van Hoyweghen , ) . The key idea is to share the risk of a loss in a collective , organized through an abstract mutuality ; due to the 'law ' of large numbers , uncertainty thus becomes manageable and the effect of chance can be offset ( Ewald , ) . In this way , insurance creates a `` community of fate '' in the face of uncertainty ( Heimer , ) . To enter into this community ( the insurance pool ) , the insurer demands a certain fee , called premium , from the policyholder . In insurance , questions of fairness inevitably arise , and have been the subject of much debate . The central point of debate is the tension between risk assessment and distribution ( Abraham , ) . In other words , who is to be mutualized in the pool ."}
{"chunk_id": "2306.14624v2_introduction_0004", "paper_id": "2306.14624v2", "section": "introduction", "text": ". In insurance , questions of fairness inevitably arise , and have been the subject of much debate . The central point of debate is the tension between risk assessment and distribution ( Abraham , ) . In other words , who is to be mutualized in the pool . Some form of segmentation is found in many insurantial arrangements : the pool of policyholders can be stratified by separating high and low risk individuals . But the specific nature that such segmentation McFall et al . ( ) call insurance `` interestingly uninteresting '' , referring to how insurance is `` hugely underresearched '' given its societal importance , which is typically not recognized ( Ewald , ) . takes typically depends not only on risk assessment , but on further considerations such as assignment of responsibility , modulated by social context ; in this way , insurance is not a neutral technology ( Baker & Simon , ; Glenn , a ) . Our non-comprehensive outline of the history of insurance illustrates how uncertainty , fairness and responsibility interact , and can be entangled and disentangled ."}
{"chunk_id": "2306.14624v2_introduction_0005", "paper_id": "2306.14624v2", "section": "introduction", "text": "modulated by social context ; in this way , insurance is not a neutral technology ( Baker & Simon , ; Glenn , a ) . Our non-comprehensive outline of the history of insurance illustrates how uncertainty , fairness and responsibility interact , and can be entangled and disentangled . From this background , we can extract conceptual insights which also apply to machine learning . The tension between risk assessment and distribution is mirrored in formal fairness principles : solidarity , which can be linked to independence in fair machine learning , contrasts with actuarial fairness , linked to calibration . Briefly , actuarial fairness demands that each policyholder should pay only for their own risk , that is , mutualization should occur only between individuals with the same 'true ' risk . In contrast , solidarity calls for equal contribution to the pool . On one level of this text , we problematize actuarial fairness ( by extension , calibration ) as a notion of fairness in the normative sense by taking inspiration from insurance ."}
{"chunk_id": "2306.14624v2_introduction_0006", "paper_id": "2306.14624v2", "section": "introduction", "text": "the same 'true ' risk . In contrast , solidarity calls for equal contribution to the pool . On one level of this text , we problematize actuarial fairness ( by extension , calibration ) as a notion of fairness in the normative sense by taking inspiration from insurance . This perspective is aligned with recent proposals that stress the discrepancy of formal algorithmic fairness and `` substantive '' fairness ( Green , ) , which some prefer to call justice ( Vredenburgh , ) . Parallel to this runs a distinct textual level , where we emphasize two intricately interacting themes : responsibility and tensions between aggregate and individual . Both entail criticism of actuarial fairness , but we suggest that they additionally provide much broader , fruitful lessons for machine learning from insurance . At the highest level of abstraction , our goal is to establish a general conceptual bridge between insurance and machine learning . Traversing this bridge , machine learning scholars can obtain new perspectives on the social situatedness of a probabilistic , statistical technology -we attempt to offer a new 'cognitive toolkit ' for thinking about the social situatedness of machine learning ."}
{"chunk_id": "2306.14624v2_introduction_0007", "paper_id": "2306.14624v2", "section": "introduction", "text": "a general conceptual bridge between insurance and machine learning . Traversing this bridge , machine learning scholars can obtain new perspectives on the social situatedness of a probabilistic , statistical technology -we attempt to offer a new 'cognitive toolkit ' for thinking about the social situatedness of machine learning . Our point of view is that fairness can not be reduced to a formal , mathematical issue , but that it requires taking broader social context into account , reasoning for instance about responsibility . And for this , we suggest , insurance is an insightful analogon . Therefore , our objective is to furnish the reader with a guide that charts the landscape of insurance with respect to social issues and to establish links to machine learning . On a formal level , we use the following analogy . In a machine learning task , we are given some features X and associated outcomes Y , which we attempt to approximate by predictions Ŷ . The structural relation to insurance is established by conceiving of X as the features of policyholders ( e.g . age , gender ) with outcomes Y ( e.g ."}
{"chunk_id": "2306.14624v2_introduction_0008", "paper_id": "2306.14624v2", "section": "introduction", "text": ", we are given some features X and associated outcomes Y , which we attempt to approximate by predictions Ŷ . The structural relation to insurance is established by conceiving of X as the features of policyholders ( e.g . age , gender ) with outcomes Y ( e.g . having an accident or not ) , and the task is to set a corresponding premium Ŷ ."}
{"chunk_id": "2401.11351v2_title_0000", "paper_id": "2401.11351v2", "section": "title", "text": "A comprehensive review of Quantum Machine Learning : from NISQ to Fault Tolerance"}
{"chunk_id": "2401.11351v2_abstract_0001", "paper_id": "2401.11351v2", "section": "abstract", "text": "Quantum machine learning , which involves running machine learning algorithms on quantum devices , has garnered significant attention in both academic and business circles . In this paper , we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning . This includes techniques used in Noisy Intermediate-Scale Quantum ( NISQ ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware . Our review covers fundamental concepts , algorithms , and the statistical learning theory pertinent to quantum machine learning ."}
{"chunk_id": "2401.11351v2_introduction_0002", "paper_id": "2401.11351v2", "section": "introduction", "text": "Introduction Quantum machine learning represents a highly promising realm in contemporary physics and computer science research , with far-reaching implications spanning quantum chemistry [ 108 ] , artificial intelligence [ 89 ] , and even high-energy physics [ 7 ] . Nevertheless , it remains in its nascent stages of development . This is evident from the absence of a precise definition for quantum machine learning . Some describe it as the convergence of quantum computing and machine learning , wherein machine learning algorithms are executed on quantum devices . In simpler terms , it can be thought of as the quantum counterpart to classical machine learning . In recent times , artificial intelligence , exemplified by technologies like ChatGPT , has become an integral part of everyday life . It 's entirely plausible that , in the future , we will harness artificial intellegence in an even wider array of applications , including medical diagnostics , education , and aiding scientific research . Much of artificial intellegence 's success hinges on machine learning , a departure from traditional computer programs that entail crafting explicit instructions to directly solve problems [ 112 ] ."}
{"chunk_id": "2401.11351v2_introduction_0003", "paper_id": "2401.11351v2", "section": "introduction", "text": "intellegence in an even wider array of applications , including medical diagnostics , education , and aiding scientific research . Much of artificial intellegence 's success hinges on machine learning , a departure from traditional computer programs that entail crafting explicit instructions to directly solve problems [ 112 ] . Machine learning models , on the other hand , are trained on real-world data stored in a dataset ( often denoted as D ) , acquiring the ability to tackle problems autonomously . This represents a departure , in a sense , from the conventional von Neumann model of digital computing . Researchers might initially wonder why there 's a need for quantum machine learning when its classical counterpart has demonstrated impressive performance . One of the primary rationales stems from the fact that classical machine learning relies heavily on linear algebra procedures [ 18 ] . For instance , classical machine learning can address problems like identifying the optimal hyperplane to separate two data clusters by employing matrix inversion techniques ."}
{"chunk_id": "2401.11351v2_introduction_0004", "paper_id": "2401.11351v2", "section": "introduction", "text": "performance . One of the primary rationales stems from the fact that classical machine learning relies heavily on linear algebra procedures [ 18 ] . For instance , classical machine learning can address problems like identifying the optimal hyperplane to separate two data clusters by employing matrix inversion techniques . Quantum mechanics , at its core , is inherently grounded in linear algebra , where we construct a Hilbert space ( H ) of a system described by an Hermitian operator known as the Hamiltonian ( Ĥ ) [ 114 ] . Over time , it has become evident that quantum computing has the potential to dramatically enhance a computer 's problem-solving capabilities in certain specific scenarios . To illustrate , consider the task of matrix inversion again ."}
{"chunk_id": "2401.11351v2_introduction_0005", "paper_id": "2401.11351v2", "section": "introduction", "text": "Hermitian operator known as the Hamiltonian ( Ĥ ) [ 114 ] . Over time , it has become evident that quantum computing has the potential to dramatically enhance a computer 's problem-solving capabilities in certain specific scenarios . To illustrate , consider the task of matrix inversion again . Classical computers typically require computational efforts with a complexity of O ( N log N ) to accomplish this , while quantum computers can leverage algorithms like the Harrow-Hassidim-Lloyd algorithm ( HHL ) algorithm [ 58 ] , which has a complexity of O ( ( log N ) 2 ) , leading to significant speedup , in certain conditions like sparsity , fault-tolerance , small condition numbers , and fast interfaces between classical and quantum processors . Although there is no known explicit realizations of HHL algorithm at the large scale at the moment , it is still an exciting direction deserves further studies , especially when quantum devices are fast developing with more and more capabilities . Due to its vague definition , quantum machine learning encompasses a broader spectrum of topics ."}
{"chunk_id": "2401.11351v2_introduction_0006", "paper_id": "2401.11351v2", "section": "introduction", "text": "of HHL algorithm at the large scale at the moment , it is still an exciting direction deserves further studies , especially when quantum devices are fast developing with more and more capabilities . Due to its vague definition , quantum machine learning encompasses a broader spectrum of topics . For instance , quantum ( shadow ) tomography [ 2 ] has gained prominence , focusing on the characterization of a given quantum state by accumulating data from various measurements . This involves determining the minimum number of identical quantum state copies needed to extract sufficient information about the state 's properties . Another facet involves machine learning for quantum physics , which entails employing ( perhaps classical ) machine learning tools to explore various aspects of quantum physics . Additionally , some developments in both quantum algorithms and quantum hardware are often encompassed within the broader umbrella of quantum machine learning . Although these topics may not directly align with the narrow definition of quantum machine learning outlined earlier , they hold substantial promise for its future development ."}
{"chunk_id": "2401.11351v2_introduction_0007", "paper_id": "2401.11351v2", "section": "introduction", "text": "Additionally , some developments in both quantum algorithms and quantum hardware are often encompassed within the broader umbrella of quantum machine learning . Although these topics may not directly align with the narrow definition of quantum machine learning outlined earlier , they hold substantial promise for its future development . To illustrate , within the narrow sensed field of quantum machine learning , a significant challenge is known as the input/output problem . In particular , the output problem pertains to the task of comprehending the solution generated by a quantum algorithm , and this aligns closely with the domain of shadow tomography . And by harnessing the capabilities of classical machine learning to gain deeper insights into quantum physics , it might lead to advancements in quantum computing too . In this review , we primarily focus on quantum machine learning in its narrower sense , which pertains to execute quantum algorithms designed for machine learning purposes . Presently , we find ourselves in what 's referred to as ( perhaps the end of ) the noisy intermediate scale quantum or NISQ era of quantum computing ."}
{"chunk_id": "2401.11351v2_introduction_0008", "paper_id": "2401.11351v2", "section": "introduction", "text": "on quantum machine learning in its narrower sense , which pertains to execute quantum algorithms designed for machine learning purposes . Presently , we find ourselves in what 's referred to as ( perhaps the end of ) the noisy intermediate scale quantum or NISQ era of quantum computing . Quantum computers are susceptible to background noise , which imposes limitations on our ability to construct quantum computers with sufficient depth for executing tasks demanding fast and precise computations . The quantum computers available today can only handle on the order of around 100 qubits , and they all exhibit noise , making it challenging to derive tangible benefits for our daily lives . The solution to this predicament is known as quantum error correction ( QEC ) code [ 113 ] . Think of QEC as a safeguard for quantum information . Typically , quantum information is lost once it 's measured , which becomes especially likely in noisy environments . However , information protected by QEC can persist if it remains undamaged within certain limits ."}
{"chunk_id": "2401.11351v2_introduction_0009", "paper_id": "2401.11351v2", "section": "introduction", "text": "code [ 113 ] . Think of QEC as a safeguard for quantum information . Typically , quantum information is lost once it 's measured , which becomes especially likely in noisy environments . However , information protected by QEC can persist if it remains undamaged within certain limits . It 's worth noting that all error correction codes have their constraints , implying that information will inevitably be lost if it 's severely damaged . Nevertheless , error correction provides a protective buffer zone against such losses . Our objective is to implement QEC codes across all quantum devices , ushering in an era of fault-tolerant quantum computing ( FTQC ) in the future . This trajectory parallels the history of classical computing , where , before the invention of classical error correction , scaling up and running useful algorithms on classical computers was also a formidable challenge [ 29 ] . Today , we can reliably operate classical computers everyday . Given the promising advancements in quantum error correction in recent times , our optimism about the future of quantum computing remains steadfast ."}
{"chunk_id": "2401.11351v2_introduction_0010", "paper_id": "2401.11351v2", "section": "introduction", "text": "up and running useful algorithms on classical computers was also a formidable challenge [ 29 ] . Today , we can reliably operate classical computers everyday . Given the promising advancements in quantum error correction in recent times , our optimism about the future of quantum computing remains steadfast . However , when it comes to machine learning , classical machine learning does n't inherently reject noise [ 88 ] . The widely recognized learning algorithm , known as stochastic gradient descent , explicitly incorporates noise , and surprisingly , this noise addition actually enhances its performance . To grasp this concept , consider that noise can effectively steer us away from saddle points , offering an automatic mechanism for avoiding them . In a way , one can interpret this as machine learning 's ability to withstand and even benefit from noise . Consequently , this insight suggests that running certain machine learning algorithms on current ( NISQ ) quantum devices could have some significance . This prospect promises to enrich our present-day experiences significantly , especially considering the challenges associated with constructing QEC , which could take a few years to become fully integrated ."}
{"chunk_id": "2401.11351v2_introduction_0011", "paper_id": "2401.11351v2", "section": "introduction", "text": "insight suggests that running certain machine learning algorithms on current ( NISQ ) quantum devices could have some significance . This prospect promises to enrich our present-day experiences significantly , especially considering the challenges associated with constructing QEC , which could take a few years to become fully integrated . Prior to entering the era of fault-tolerant quantum computing , we 'll have the opportunity to experiment with quantum devices and apply machine learning techniques , adding a vibrant dimension to our current scientific research . In addition to discussing the current applications of quantum capabilities for machine learning , we should also let our imaginations soar . The era of fault-tolerant quantum computing ( FTQC ) is a foreseeable future , and it 's crucial to look ahead at what challenges we can tackle with quantum machine learning on FTQC devices . One of the most renowned and valuable algorithms in this context is the Harrow-Hassidim-Lloyd ( HHL ) algorithm [ 58 ] . Additionally , there are other algorithms that can be deployed to address a range of problems , such as principal component analysis [ 18 ] ."}
{"chunk_id": "2401.11351v2_introduction_0012", "paper_id": "2401.11351v2", "section": "introduction", "text": "devices . One of the most renowned and valuable algorithms in this context is the Harrow-Hassidim-Lloyd ( HHL ) algorithm [ 58 ] . Additionally , there are other algorithms that can be deployed to address a range of problems , such as principal component analysis [ 18 ] . Beside the optimistic future that quantum machine learning has , there are also a number of controversial issues with the subject . For example , some might argue that the variational quantum algorithm will not work in some circumstance . People working on quantum landscapes theory observed the famous barren plateau phenomena which leads to a kind of no-go theorem [ 27 , 86 ] . It amounts to situations that are hard to find minimum when we optimizes our objective function by gradient descent with randomized variational circuits . Debates indeed exist regarding whether quantum algorithms can consistently deliver exponential speedups . Some argue that quantum speedup is only guaranteed when dealing with quantum information . When it comes to classical information , it 's conceivable to design classical machine learning models capable of achieving comparable average prediction accuracy [ 121 , 66 ] ."}
{"chunk_id": "2401.11351v2_introduction_0013", "paper_id": "2401.11351v2", "section": "introduction", "text": "algorithms can consistently deliver exponential speedups . Some argue that quantum speedup is only guaranteed when dealing with quantum information . When it comes to classical information , it 's conceivable to design classical machine learning models capable of achieving comparable average prediction accuracy [ 121 , 66 ] . In such cases , the computational complexity difference between classical and quantum approaches may , at worst , be a modest polynomial factor . This debate underscores the importance of carefully assessing the specific problem and context when considering the potential advantages of quantum algorithms . However , the landscape shifts when the objective is to attain a low worst-case prediction error . In this scenario , it becomes feasible to achieve an exponential divergence in computational complexities between classical and quantum approaches . This underscores the potential superiority of quantum algorithms when stringent requirements for worst-case accuracy are in play . This phenomenon can be viewed as a manifestation of classical effects casting their `` shadow '' in the quantum realm ."}
{"chunk_id": "2401.11351v2_introduction_0014", "paper_id": "2401.11351v2", "section": "introduction", "text": "exponential divergence in computational complexities between classical and quantum approaches . This underscores the potential superiority of quantum algorithms when stringent requirements for worst-case accuracy are in play . This phenomenon can be viewed as a manifestation of classical effects casting their `` shadow '' in the quantum realm . It should come as no surprise to individuals well-versed in quantum theory that quantum predictions tend to align with classical theory when dealing with systems possessing a high degree of freedom . This approach can also be applied to tackle a critical issue in variational quantum algorithms known as the `` output problem , '' often referred to as the classical shadow . Indeed , it is truly heartening and captivating to witness the evolution of quantum computing beyond celebrated algorithms like Shor 's factoring algorithm [ 117 ] and Grover search [ 50 ] . Algorithms like HHL [ 58 ] offer a versatile blueprint , shedding light on how quantum computers hold the potential to deliver significant acceleration for fundamental tasks like clustering , pattern-matching , and principal component analysis ."}
{"chunk_id": "2401.11351v2_introduction_0015", "paper_id": "2401.11351v2", "section": "introduction", "text": "'s factoring algorithm [ 117 ] and Grover search [ 50 ] . Algorithms like HHL [ 58 ] offer a versatile blueprint , shedding light on how quantum computers hold the potential to deliver significant acceleration for fundamental tasks like clustering , pattern-matching , and principal component analysis . However , as Scott Aaronson aptly noted in his paper [ 1 ] , the pursuit of these speedups will still demand significant effort and ingenuity , as nature continues to present challenges that compel us to work diligently for these advancements . We have structured our review as follows . In section 2 , we focus on the historical and ongoing developments in Quantum Machine Learning ( QML ) during the Noisy Intermediate-Scale Quantum ( NISQ ) era . Within this era , one of the central frameworks is the Variational Quantum Algorithm ( VQA ) [ 17 , 44 , 108 ] , which we explore in detail in section 2.1 ."}
{"chunk_id": "2401.11351v2_introduction_0016", "paper_id": "2401.11351v2", "section": "introduction", "text": "Quantum Machine Learning ( QML ) during the Noisy Intermediate-Scale Quantum ( NISQ ) era . Within this era , one of the central frameworks is the Variational Quantum Algorithm ( VQA ) [ 17 , 44 , 108 ] , which we explore in detail in section 2.1 . VQA comprises four key elements : the choice of an objective function ( discussed in section 2.1.1 ) , the employment of Parameterized Quantum Circuits ( PQC ) with adjustable parameters optimized by classical algorithms ( covered in section 2.1.2 ) , measurement strategies ( explored in section 2.1.3 ) , and the classical optimizer responsible for minimizing the objective function ( explained in section 2.1.4 ) . Subsequently , we provide insights into the construction of the Quantum Neural Tangent Kernel ( QNTK ) [ 87 ] in section 2.2 , offering a theoretical foundation for quantum neural networks and an understanding of stochastic gradient descent dynamics from first principles . Finally , we address the issue of barren plateaus in section 2.3 , approaching this topic through the lens of quantum landscape theory [ 27 ] ."}
{"chunk_id": "2401.11351v2_introduction_0017", "paper_id": "2401.11351v2", "section": "introduction", "text": "section 2.2 , offering a theoretical foundation for quantum neural networks and an understanding of stochastic gradient descent dynamics from first principles . Finally , we address the issue of barren plateaus in section 2.3 , approaching this topic through the lens of quantum landscape theory [ 27 ] . Additionally , we present an alternative formulation of laziness [ 86 ] , providing another perspective on the same problem . In section 3 , our focus shifts to quantum algorithms that have the potential for exponential speedup in the Fault-Tolerant Quantum Computing ( FTQC ) era . We begin by introducing Quantum Phase Estimation ( QPE ) [ 29 ] in section 3.1 , where we also delve into the Quantum Principal Component Analysis ( QPCA ) program [ 91 ] . Subsequently , we present a counterpoint regarding the attainability of exponential speedup for programs such as the recommendation system in section 3.2 . Moving on to section 3.3 , we introduce the pivotal Harrow-Hassidim-Lloyd ( HHL ) algorithm [ 58 ] , which opens up possibilities for various new quantum machine learning algorithms ."}
{"chunk_id": "2401.11351v2_introduction_0018", "paper_id": "2401.11351v2", "section": "introduction", "text": "a counterpoint regarding the attainability of exponential speedup for programs such as the recommendation system in section 3.2 . Moving on to section 3.3 , we introduce the pivotal Harrow-Hassidim-Lloyd ( HHL ) algorithm [ 58 ] , which opens up possibilities for various new quantum machine learning algorithms . In section 3.3.1 , we illustrate a visionary perspective on the future of machine learning algorithms with the aid of Carleman linearization [ 90 ] and the HHL algorithm . Finally , in section 3.4 , we introduce Quantum Random Access Memory ( QRAM ) , focusing on theoretical designs [ 56 ] , practical implementation efforts [ 54 ] , and its necessity in certain algorithms [ 18 ] . In section 4 , we delve into various topics that amalgamate quantum principles with statistical learning theory . Our primary focus is on shadow tomography [ 2 ] in section 4.1 , where we explore the motivation behind shadow tomography and delve into the construction of the theorem . A pivotal subject inspired by shadow tomography , the classical shadow formalism [ 41 ] , is thoroughly discussed in section 4.2 ."}
{"chunk_id": "2401.11351v2_introduction_0019", "paper_id": "2401.11351v2", "section": "introduction", "text": "on shadow tomography [ 2 ] in section 4.1 , where we explore the motivation behind shadow tomography and delve into the construction of the theorem . A pivotal subject inspired by shadow tomography , the classical shadow formalism [ 41 ] , is thoroughly discussed in section 4.2 . Moving on to section 4.2.1 , we examine the application of classical shadow as an efficient quantum-to-classical information converter [ 66 ] . In section 4.3 , we then shift our attention to the applications of Quantum Machine Learning ( QML ) in the study of quantum data and quantum simulators [ 18 ] . It 's essential to note that our review does n't encompass various topics , including quantum machine learning algorithms whose advantages stem from sampling-related statements , the studies of quantum error corrections , quantum memory , and designs of quantum data centers ."}
{"chunk_id": "2409.03632v1_title_0000", "paper_id": "2409.03632v1", "section": "title", "text": "Beyond Model Interpretability : Socio-Structural Explanations in Machine Learning"}
{"chunk_id": "2409.03632v1_abstract_0001", "paper_id": "2409.03632v1", "section": "abstract", "text": "What is it to interpret the outputs of an opaque machine learning model ? One approach is to develop interpretable machine learning techniques . These techniques aim to show how machine learning models function by providing either model-centric local or global explanations , which can be based on mechanistic interpretations ( revealing the inner working mechanisms of models ) or non-mechanistic approximations ( showing input feature-output data relationships ) . In this paper , we draw on social philosophy to argue that interpreting machine learning outputs in certain normatively-salient domains could require appealing to a third type of explanation that we call `` socio-structural '' explanation . The relevance of this explanation type is motivated by the fact that machine learning models are not isolated entities but are embedded within and shaped by social structures . Socio-structural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models . We demonstrate the importance of socio-structural explanations by examining a racially biased healthcare allocation algorithm ."}
{"chunk_id": "2409.03632v1_abstract_0002", "paper_id": "2409.03632v1", "section": "abstract", "text": "are not isolated entities but are embedded within and shaped by social structures . Socio-structural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models . We demonstrate the importance of socio-structural explanations by examining a racially biased healthcare allocation algorithm . Our proposal highlights the need for transparency beyond model interpretability : understanding the outputs of machine learning systems could require a broader analysis that extends beyond the understanding of the machine learning model itself ."}
{"chunk_id": "2409.03632v1_introduction_0003", "paper_id": "2409.03632v1", "section": "introduction", "text": "INTRODUCTION In order to formulate a learning theory of machine learning , it may be necessary to move from seeing an inert model as the machine learner to seeing the human developer-along with , and not separate from , his or her model and surrounding social relations-as the machine learner . -Reigeluth & Castelle [ 55 ] The past decade has seen massive research on interpretable machine learning ( ML ) . 1 Here is a rough restatement of the goal of interpretable ML research program : many ML models are opaque in that even the expert humans can not robustly understand , in non-mathematical terms , the reasons for why particular outputs are generated by these models [ 31 , 42 , 66 ] . To overcome this opacity , various model-centric techniques have been developed to interpret their outputs . These techniques are diverse . They range from producing counterfactual explanations or heatmaps that offer insights into how changing inputs affect outputs [ 28 , 41 , 46 ] , to interpreting the inner workings of the model by probing patterns of neuron activations or attention mechanisms [ 10 , 15 , 48 ] ."}
{"chunk_id": "2409.03632v1_introduction_0004", "paper_id": "2409.03632v1", "section": "introduction", "text": "They range from producing counterfactual explanations or heatmaps that offer insights into how changing inputs affect outputs [ 28 , 41 , 46 ] , to interpreting the inner workings of the model by probing patterns of neuron activations or attention mechanisms [ 10 , 15 , 48 ] . 2 Despite these advancements , ML interpretability remains a contentious and ambiguous topic in the scientific community , lacking a universally accepted scope and definition [ 11 , 13 , 38 , 45 ] . This ambiguity complicates the evaluation and regulation of opaque ML systems , raising questions about what constitutes sufficient interpretation and how it should be assessed . A pragmatic and pluralistic approach to interpretability has gained traction , viewing explanations as context-dependent responses to why-questions [ 12 , 31 , 42 , 43 ] . On this pluralistic approach , the adequacy of an explanation depends on the specific inquiry . For simple classification tasks , techniques like saliency maps or feature importance may suffice . For instance , if a model is differentiating between images of cats and dogs , saliency maps could highlight the pixels most influential in the decision-making process ."}
{"chunk_id": "2409.03632v1_introduction_0005", "paper_id": "2409.03632v1", "section": "introduction", "text": "explanation depends on the specific inquiry . For simple classification tasks , techniques like saliency maps or feature importance may suffice . For instance , if a model is differentiating between images of cats and dogs , saliency maps could highlight the pixels most influential in the decision-making process . However , for complex and socially-embedded topics -such as biased healthcare algorithms -these model-centric explanations can fall short . Consider an algorithm that predicts hospital readmission risk but systematically underestimates it for certain racial groups . A model-centric explanation might highlight `` total healthcare costs incurred in the past year '' as an important feature . However , this alone might not fully reveal why the algorithm underestimates risk for a specific racial group . The algorithmic choice could come from the fact that this racial group , due to systemic inequities , have historically been unable to afford adequate healthcare and thus incurred lower costs . As a result , the low value for the `` total healthcare costs incurred in the past year '' feature does not necessarily indicate better health ."}
{"chunk_id": "2409.03632v1_introduction_0006", "paper_id": "2409.03632v1", "section": "introduction", "text": "this racial group , due to systemic inequities , have historically been unable to afford adequate healthcare and thus incurred lower costs . As a result , the low value for the `` total healthcare costs incurred in the past year '' feature does not necessarily indicate better health . Instead , it may suggest unmet healthcare needs , leading to higher readmission rates that the algorithm does not effectively account for . In such cases , interpretations that consider both model-specific details like feature importance and relevant social and structural factors like healthcare affordability disparities among racial groups are crucial for understanding ML predictions or decisions . In this paper , we draw on social philosophy [ 17 , 25 , 26 , 67 , 68 ] to advocate for a more comprehensive approach to ML interpretability research , expanding beyond model-centric explanations . We propose incorporating relevant socio-structural explanations to achieve a deeper understanding of ML outputs in domains with substantial societal impact . In the rest of the paper , we introduce the concept of socio-structural explanations and discuss their relevance to understanding ML outputs ."}
{"chunk_id": "2409.03632v1_introduction_0007", "paper_id": "2409.03632v1", "section": "introduction", "text": ", expanding beyond model-centric explanations . We propose incorporating relevant socio-structural explanations to achieve a deeper understanding of ML outputs in domains with substantial societal impact . In the rest of the paper , we introduce the concept of socio-structural explanations and discuss their relevance to understanding ML outputs . We then examine how these explanations can enhance the interpretation of automated decision-making by ML systems in healthcare [ 49 ] . Our paper expands the discourse on transparency in machine learning , arguing that it extends beyond model interpretability . We propose that in high-stake decision domains , a sociostructural analysis could be necessary to understand system outputs , uncover societal biases , ensure accountability , and guide policy decisions ."}
{"chunk_id": "2505.13457v1_title_0000", "paper_id": "2505.13457v1", "section": "title", "text": "Tuning Learning Rates with the Cumulative-Learning Constant"}
{"chunk_id": "2505.13457v1_abstract_0001", "paper_id": "2505.13457v1", "section": "abstract", "text": "This paper introduces a novel method for optimizing learning rates in machine learning . A previously unrecognized proportionality between learning rates and dataset sizes is discovered , providing valuable insights into how dataset scale influences training dynamics . Additionally , a cumulative learning constant is identified , offering a framework for designing and optimizing advanced learning rate schedules . These findings have the potential to enhance training efficiency and performance across a wide range of machine learning applications ."}
{"chunk_id": "2505.13457v1_introduction_0002", "paper_id": "2505.13457v1", "section": "introduction", "text": "Introduction The learning rate is a deep learning hyperparameter that is very important for training models efficiently . There has been a lot of research done on optimizing learning rates including sophisticated methods for adaptive learning rates as well as alternating learning rates ( Wu et al. , 2019 ) . However , the learning rate selection is arbitrary -An optimal learning rate is found without understanding due to the belief that the learning rate is too sensitive to changes in other hyperparameters within the model . However , this paper will present that the learning rate follows some strict proportionality laws . Previous work has been done to compare proportionality of learning rates in reference to batch size ( Diego et al. , 2020 ) . Therefore , this paper will not be exploring how batch size affects the optimal learning rate ."}
