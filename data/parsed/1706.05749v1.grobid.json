{
  "title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement Learning",
  "abstract": "This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.",
  "introduction": "Introduction Complex environments such as Go, Starcraft, and many modern video-games present profound challenges in deep reinforcement learning that have yet to be solved. They often require long, precise sequences of actions and domain knowledge in order to obtain reward, and have yet to be learned from random weight initialization. Solutions to these problems would mark a significant breakthrough on the path to artificial general intelligence. Recent works in reinforcement learning have shown that environments such as Atari games [2] can be learned from pixel input to superhuman expertise [9] . The agents start with randomly initialized weights, and learn largely from trial and error, relying on a reward signal to indicate performance. Despite these successes, complex games, including those where rewards are sparse such as Montezuma's Revenge, have been notoriously difficult to learn. While methods such as intrinsic motivation [3] have been used to partially overcome these challenges, we suspect this becomes intractable as complexity increases. Additionally, as environments become more complex, they will become more expensive to simulate. This poses a significant problem, since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms, representing days of training on a single machine. Thus, it appears likely that complex environments will become too costly to learn from randomly initialized weights, due both to the increased simulation cost as well as the inherent difficulty of the task. Therefore, some form of prior information must be given to the agent. This can be seen with AlphaGo [18] , where the agent never learned to play the game without first using supervised learning on human games. While supervised learning certainly has been shown to aid reinforcement learning, it is very costly to obtain sufficient samples and requires the environment to be a task humans can play with reasonable skill, and is therefore impractical for a wide variety of important reinforcement learning problems. In this paper we introduce Dex, the first continual reinforcement learning toolkit for training and evaluating continual learning methods. We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments. In incremental learning, environments are framed as a task to be learned by an agent. This task can be split into a series of subtasks that are solved simultaneously. Similar to how natural language processing and object detection are subtasks of neural image caption generation [23] , reinforcement learning environments also have subtasks relevant to a given environment. These subtasks often include player detection, player control, obstacle detection, enemy detection, and player-object interaction, to name a few. These subtasks are common to many environments, but they are often sufficiently different in function and representation that reinforcement learning algorithms fail to generalize them across environments, such as in Atari. These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments, and are a reason for humans superior data efficiency in learning complex tasks. In the case of deliberately similar environments, we can construct the subtasks such that they are similar in function and representation that an agent trained on the first environment can accelerate learning on the second environment due to its preconstructed subtask representations, thus partially avoiding the more complex environment's increased simulation cost and inherent learning difficulty.",
  "body": "Introduction Complex environments such as Go, Starcraft, and many modern video-games present profound challenges in deep reinforcement learning that have yet to be solved. They often require long, precise sequences of actions and domain knowledge in order to obtain reward, and have yet to be learned from random weight initialization. Solutions to these problems would mark a significant breakthrough on the path to artificial general intelligence. Recent works in reinforcement learning have shown that environments such as Atari games [2] can be learned from pixel input to superhuman expertise [9] . The agents start with randomly initialized weights, and learn largely from trial and error, relying on a reward signal to indicate performance. Despite these successes, complex games, including those where rewards are sparse such as Montezuma's Revenge, have been notoriously difficult to learn. While methods such as intrinsic motivation [3] have been used to partially overcome these challenges, we suspect this becomes intractable as complexity increases. Additionally, as environments become more complex, they will become more expensive to simulate. This poses a significant problem, since many Atari games already require upwards of 100 million steps using state-of-the-art algorithms, representing days of training on a single machine. Thus, it appears likely that complex environments will become too costly to learn from randomly initialized weights, due both to the increased simulation cost as well as the inherent difficulty of the task. Therefore, some form of prior information must be given to the agent. This can be seen with AlphaGo [18] , where the agent never learned to play the game without first using supervised learning on human games. While supervised learning certainly has been shown to aid reinforcement learning, it is very costly to obtain sufficient samples and requires the environment to be a task humans can play with reasonable skill, and is therefore impractical for a wide variety of important reinforcement learning problems. In this paper we introduce Dex, the first continual reinforcement learning toolkit for training and evaluating continual learning methods. We present and demonstrate a novel continual learning method we call incremental learning to solve complex environments. In incremental learning, environments are framed as a task to be learned by an agent. This task can be split into a series of subtasks that are solved simultaneously. Similar to how natural language processing and object detection are subtasks of neural image caption generation [23] , reinforcement learning environments also have subtasks relevant to a given environment. These subtasks often include player detection, player control, obstacle detection, enemy detection, and player-object interaction, to name a few. These subtasks are common to many environments, but they are often sufficiently different in function and representation that reinforcement learning algorithms fail to generalize them across environments, such as in Atari. These critical subtasks are what expert humans utilize to quickly learn in new environments that share subtasks with previously learned environments, and are a reason for humans superior data efficiency in learning complex tasks. In the case of deliberately similar environments, we can construct the subtasks such that they are similar in function and representation that an agent trained on the first environment can accelerate learning on the second environment due to its preconstructed subtask representations, thus partially avoiding the more complex environment's increased simulation cost and inherent learning difficulty. Related work Transfer learning [13] is the method of utilizing data from one domain to enhance learning of another domain. While sharing significant similarities to continual learning, transfer learning is applicable across all machine learning domains, rather than being confined to reinforcement learning. For example, it has had significant use with using networks trained on ImageNet [5] to accelerate or enhance learning and classification accuracy by finetuning in a variety of vision tasks [12] . While the concept of continual learning, where an agent learns from a variety of experiences to enhance future learning, has been defined for some time [15] , it has remained largely untapped by recent powerful algorithms that are best fit to benefit from its effects. Recent work has been done with Progressive Neural Networks [17] , where transfer learning was used to apply positive transfer in a variety of reinforcement learning domains. However, our method differs in that it does not add additional parameters for each environment or use lateral connections to features that result in increased memory space and training time. Recent work related to subtask utilization comes from Kirkpatrick et al. [7] , which shows that expertise can be maintained on multiple environments that have not been experienced for a long time through elastic weight consolidation (EWC). Viable weights were found that simultaneously achieve expertise in a variety of Atari games. Incremental learning similarly trains on multiple environments, but with the goal of achieving enhanced expertise in a single environment, rather than expertise in all environments. We leave it to future work to overcome this limitation. Dex Dex is a novel deep reinforcement learning toolkit for training and evaluating agents on continual learning methods. Dex acts as a wrapper to the game Open Hexagon [16] , sending screen pixel information, reward information and performing actions via an OpenAI Gym like API [4] . Dex contains hundreds of levels, each acting as their own environment. These environments are collected into groups of similar environments for the task of continual learning. Dex environments vary greatly in difficulty, ranging from very simple levels where agents achieve superhuman performance in less that 4 minutes, to levels we consider far more complex than any previously learned environments. Refer to videos available at github.com/innixma/dex of the Dex environments shown in Figure 1 , as screenshots do not capture the environment complexity. Open Hexagon is a game involving navigating a triangle around a center figure to avoid incoming randomly generated walls. Screenshots of various environments from the game are shown in Figure 1 . The game progresses regardless of player action, and thus the player must react to the environment in real-time. If the player contacts a wall the game is over. At each point in the game, a player has only three choices for actions: move left, right, or stay put. It is a game of survival, with the score and thus total reward being the survival time in seconds. Open Hexagon contains hundreds of levels drastically ranging in difficulty, yet they each contain many similar core subtasks. This makes Open Hexagon an ideal platform for testing continual learning methods. The small triangle is the player that must be rotated around the center to avoid incoming walls. Many of these environments incorporate various distortion effects that are not evident in screenshots. Reversal periodically flips game controls, and some environments even add additional actions, such as in Arithmetic, which requires the agent to correctly solve various math equations during the level through the numpad. The Dex toolkit along with its source code is available at github.com/innixma/dex. Incremental learning The novel continual learning method of incremental learning is defined as follows. In the formal case, an agent must learn from a series of n environments E, each with identical legal game actions A = {1, ..., K}. Note that any series of environments can be made to have the same action space by considering A to be the superset of all possible actions in each game, with new actions performing identically to no action, assuming no action is within the action space. Each environment E i has a corresponding cost per step c i > 0 and a step count s i ≥ 0, indicating the number of steps taken in that environment. Typically, more complex environments will have a higher cost per step. A resource maximum M is given, which indicates the total amount of data that can be gathered, shown in the following inequality: M ≥ n i=1 c i s i The problem is to maximize the mean total reward R of the agent in the goal environment, E n , while maintaining the above inequality. Steps can be taken in any order from the n environments, and there is no assumed correlation between the environments beyond their data and action dimensions. While it may appear an optimal solution to only examine and gather data from E n , as has been done for virtually all reinforcement learning algorithms in the past, this is not always the case. For example, if E n-1 and E n are highly correlated, and c n-1 << c n , then training with E n-1 may be superior due to its lesser cost. Additionally, an environment E n-1 may contain important aspects of E n , while avoiding state spaces and rewards that are not useful for training, potentially allowing training on E n-1 to be optimal, even if c n-1 > c n . By taking environments E to represent all real environments with their respective costs, the solution to this problem corresponds to the globally optimal sequence of training steps to achieve maximum performance with a finite amount of computational resources for a given algorithm. It therefore necessarily contains the solution of achieving artificial general intelligence with minimal resources. Unfortunately, this has several drawbacks. Most importantly, the selection of the optimal environments is not obvious, and their order even less so. While this formal definition is useful to define incremental learning, the following simplified version will be what this paper focuses on. In the simplified case, all variables are the same, except that steps must be taken in environments sequentially, without going back to previous environments. Thus, once a step has been taken in E i , no steps may be taken in future environments E j where j < i. Furthermore, it is assumed that the environments are correlated, where environment E i-1 contains a subset of subtasks in environment E i , with environment E i being typically harder than environment E i-1 . The intuition behind this simplified case is that simple environments are both cheap to simulate and easy to learn, and that the features and strategies learned from the simple environment could transfer to a more difficult correlated environment. This process can be done repeatedly, producing a compounding acceleration of learning as additional useful incremental environments are added. Thus, the general process of incremental environment selection is to use easier subsets of the goal environment. Furthermore, this means that every environment E i in a simplified incremental learning problem can be seen as the goal environment in an easier problem containing E 1:i . Baseline learning algorithm and model architecture To analyse the effectiveness of incremental learning, our agents learned environments from Dex. For training agents, we use Asynchronous Advantage Actor-Critic (A3C) framework introduced in [10] , coupled with the network architecture described below. ConvNet implementation details. All Dex experiments were learned by a network with the following architecture. The network takes as input a series of 2 consecutive 42 × 42 grayscale images of a game state. It is then processed through a series of 3 convolutional layers, of stride 1 × 1 and size 5 × 5, 4 × 4, and 3 × 3 with filter counts of 32, 64, and 64 respectively. Between each pair of convolutional layers is a 2 dimensional maxpooling layer, of size 2 × 2. The intermediate output is then flattened and processed by a dense layer of 512 nodes. The output layers are identical to those specified for A3C. All convolutional layers and maxpooling layers are zero-padded. This results in an network with approximately 4,000,000 parameters. All intermediate layers are followed by rectified linear units (ReLU) [11] , and Adam is used for the optimizer, with a learning rate of 10 -3 . Learning rate is not changed over the course of training as opposed to Mnih et al. [10] , but instead stays constant. A gamma of 0.99 is used, along with n-step reward [14] with n = 4 , which is used to speed up value propagation in the network, at a cost of minor instability in the learning. Training is done with batches of 128 samples. The architecture was created using TensorFlow 1.1.0 [1] , and Keras 2.0.3. Since Dex runs in real time, a slightly altered method of learning was utilized to avoid inconsistent time between steps. Our implemented A3C was modified to work in an offline manner, with experience replay as done in Deep-Q Networks [8] . This is a naive approximation to the more complex ACER algorithm [22] . While this does destabilize the algorithm, which bases its computations on data being representative of the network in its current state, the agents are still able to learn the environments and significantly outperform Double Deep-Q Networks [20] , thus serving as a reasonable baseline. Experiments So far, we have performed experiments on ten different environments in Dex for incremental learning. These ten environments are split into two incremental learning sets of three and seven environments. The first set will be referred to as a, and deals with increasingly complex patterns. The second set will be referred to as b, and deals with increasingly complex task representation. The results show that incremental learning can have a significant positive impact on learning speed and task performance, but can also lead to bad initializations when trained on overly simplistic environments. Code to reproduce the experiments in this paper will be released at a future date. Setup In the following experiments, we naively assume that achieving near optimal performance in E i with minimal resources requires as a prerequisite learning E i-1 with minimal resources. This proceeds downward to the base case of E 1 , which can be considered a trivially solvable environment from random weight initialization. This assumption is used to simplify training. Furthermore, due to the exploratory nature of the baseline experiments, the costs per step are ignored, as we seek only to show that positive feature transfer is occurring, rather than optimizing the feature transfer itself, which we leave for future work. All environments are learned with identical architecture, algorithm and hyperparameters. We act and gather data on the environments 50 times per second. We use an -greedy policy with = 0.05, and a replay memory size of 40,000. Replay memory is initialized by a random agent. For all experiments, agents are trained with 75 batches after each episode. Total reward is equal to the number of seconds survived in the environment each episode. Mean reward is calculated from an hour of testing weights without training. Each episode scales in difficulty indefinitely the longer the agent survives. Models We evaluate four different types of models in this experiment. The first is a random agent for comparison. The second is the baseline, which is the standard reinforcement learning training method. To establish the baseline, each environment E i is trained on from random initialization for one hour, equivalent to roughly 150,000 training steps. The weights which achieve the maximum total reward in a single episode are selected as the output weights from the training, called w i . A third model, which we shall call initial, is the initial weights to the incremental learning method before continued training. Thus, for environment E i this model uses weights w i-1 . This is used to measure the correlation between the two environments. We would expect uncorrelated environments to result in near random agent reward for initial. To establish the incremental learning agents, for each environment E i we take the weights w i-1 outputted by the baseline, using them as the initial weights to training on E i for an additional hour. The weights outputted by this method we call w i . Results and Observations The results can be found in Table 1 and Table 2 . As can be seen in Table 1 , incremental learning provided superior or roughly equivalent results on every environment in set b, with some environments such as b 3 and b 4 experiencing substantial increases in maximum reward, with the incrementally learned b 4 achieving nearly triple the baseline in both maximum and mean reward. However, on the harder environments, there was not significant improvement seen. This is likely because the earlier environments were not sufficiently learned along with the fact that the tasks were generally too difficult to learn in one hour of training, indicated by the near random performance of b 6 and b 7 on both the baseline and incremental methods. The other set, results shown in Table 2 for set a, show that incremental learning was harmful in the case of a 2 . This is likely due to the difference in the wall patterns of a 1 and a 2 . In a 1 , a single wall is on the screen at a time, requiring a simple avoidance. In a 2 , up to four separate walls occupy the screen at once, requiring a more complex method involving future planning and understanding of which walls are more important in a given state. We suspect that learning a 1 leads the agent to This table shows the max and mean reward for an agent on the given environment E with a given training method, as described in the experimental setup. Here we observe that incremental learning provided superior results to the baseline in nearly all environments, particularly b 3 and b 4 . This table shows the max and mean reward for an agent on the given environment E with a given training method, as described in the experimental setup. Here we observe that incremental learning provided inferior results to the baseline in the a 2 environment, likely due to overfitting. overtrain on a variety of weights, hindering future learning. This indicates that certain environments may be too simple to include in incremental learning, as a 1 can be learned to a reward of over 700 in less than four minutes. Additionally, the initial model shows that the environments are correlated, with generally far superior performance than random, despite never training on the environment explicitly. In the case of b 4 , it is nearly equal to the baseline in the maximum metric, indicating significant correlation. This is likely a reason for the greatly enhanced performance of incremental learning over the baseline in b 4 . Note that these experimental results are strictly a simple baseline for both Dex and incremental learning, and suffer from instability due to the short timeline of training and lack of repeated experiments. This means that agents do not necessarily consistently improve throughout training, but rather may quickly improve to maximum performance followed by decreased performance for the remainder of training. We leave more comprehensive experiments to future work. Visualization To qualitatively analyze the effects of incremental learning on a networks weights, we develop a saliency visualization method based on Simonyan et al. [19] for reinforcement learning. Heatmaps are generated with a given networks weights and an input image. The method for gathering the heatmaps is identical to Simonyan et al. [19] , and thus equations and derivations shall not be repeated in this paper. The most likely action the network will take at a given frame is used for the action to minimize through the gradient. This is a difference from the supervised learning case, where the ground truth is used. In reinforcement learning, the ground truth is not known, and thus must be inferred, as done in Wang et al. [21] . Results of the visualization on the trained weights of environment sets a and b can be seen in Figure 2 and Figure 3 .  As can be expected, saliency of a well performing trained agent will focus on the player location, being very sensitive to changes in a small region surrounding the player. This intuition is confirmed in the first row of Figure 3 . The agents trained on the easier environments, such as b 1 , b 2 , and b 3 , focus attention on the player and nearby threats. Interestingly, as the trained environment becomes more complex, the agent appears less developed, and increasingly focuses on irrelevant locations in the state, most prominent in the agent trained on b 7 , which explicitly avoids attention on the player and nearby threats, such as the walls. This indicates that the agent did not learn its environment sufficiently in the time it was given, due to the complexity of its training environment. This is further confirmed through the experimental results of b 6 and b 7 , which were near random. Additionally, the saliency mappings show the correlation between the environments, as indicated by the similarity in saliency in w 1 and w 2 to w 3 in both Figure 2 and Figure 3 , despite never explicitly being trained on the environment that the state in the visualization is from. Interestingly, the saliency mappings of the incrementally learned agents more closely resemble the mappings of the weights it was initialized to than the weights its environment learned without incremental learning. This suggests that a significant amount of the initialized weights are retained after incrementally learning. Further visualizations are included as a video of realtime saliency of an agent episode at github.com/innixma/dex. Future work We hope to expand the analysis done in this paper by investigating incremental learning chains involving more than two environments. This will more effectively explore the effects of incremental learning in complex problems. We also wish to extend the training time of experiments to allow for more complex environments such as those shown in Figure 1 to be learned, as well as to compare our method to Progressive Neural Networks [17] . Incremental learning could be expanded to function as a feature extractor in reinforcement learning. This would allow a more contained action space for incremental learning tasks of varying domains, and is similar to the work done in Rusu et al. [17] . Additionally, training could be done separately on multiple environments and then combined to learn an environment that shares subproblems with all the previous environments. This would be a natural merging of incremental learning and elastic weight consolidation [7] . It may also provide a synergistic effect to algorithms such as UNREAL [6] which rely on auxiliary tasks. These tasks could act to maximize network utilization across multiple environments, potentially leading the network to better generalizations through incremental learning. Finally, environments such as Montezuma's Revenge and Labyrinth [10] which require exploration with sparse rewards are a natural expansion to the application of incremental learning. Developing incremental exploration environments could result in far superior performance on exploration tasks. Conclusion The ability to learn and transfer knowledge across domains is essential to the advancement of agents that can solve complex tasks. This paper introduced the continual learning toolkit Dex for training and evaluation of continual learning methods. We proposed the method of incremental learning for deep reinforcement learning, and demonstrated its ability to accelerate learning and produce drastically superior results to standard training methods in multiple Dex environments, supporting the notion of avoiding randomly initialized weights and instead using continual learning techniques to solve complex tasks. Source code for both the training methods used in this paper as well as the Dex toolkit source code can be found at github.com/innixma/dex. Figure 1 : 1 Figure1: Dex environments. The small triangle is the player that must be rotated around the center to avoid incoming walls. Many of these environments incorporate various distortion effects that are not evident in screenshots. Reversal periodically flips game controls, and some environments even add additional actions, such as in Arithmetic, which requires the agent to correctly solve various math equations during the level through the numpad. Figure 2 : 2 Figure 2: Saliency mappings of a state from environment a 3 , with weights from set a. The first row consists of the trained baseline weights w 1 to w 3 . The second row consists of the incrementally learned weights w 2 to w 3 . Figure 3 : 3 Figure 3: Saliency mappings of a state from environment b 3 , with weights from set b. The first row consists of the trained baseline weights w 1 to w 7 . The second row consists of the incrementally learned weights w 2 to w 7 . Table 1 : 1 Set b rewards Max E Random Initial Baseline Incremental b 1 31.43 - 425.92 - b 2 8.23 120.77 259.80 280.59 b 3 7.73 58.30 132.31 221.96 b 4 8.27 35.10 35.66 105.48 b 5 8.79 19.76 52.81 41.57 b 6 9.33 11.05 10.79 13.11 b 7 9.97 7.13 9.59 14.54 Mean E Random Initial Baseline Incremental b 1 9.10 - 169.54 - b 2 2.23 36.05 92.24 85.52 b 3 2.43 8.22 41.17 66.32 b 4 2.17 5.30 8.11 26.75 b 5 1.88 4.17 12.23 11.61 b 6 2.17 2.89 2.15 2.25 b 7 2.30 2.34 2.08 2.58 Table 2 : 2 Set a rewards Max E Random Initial Baseline Incremental a 1 40.73 - 771.67 - a 2 19.65 53.37 445.85 86.57 a 3 10.85 15.69 49.50 59.10 Mean E Random Initial Baseline Incremental a 1 8.93 - 717.93 - a 2 7.46 18.34 87.06 18.31 a 3 6.01 7.32 9.81 13.52"
}