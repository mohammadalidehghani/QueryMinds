{
  "title": "Machine Learning as Ecology",
  "abstract": "Machine learning methods have had spectacular success on numerous problems. Here we show that a prominent class of learning algorithms -including Support Vector Machines (SVMs) -have a natural interpretation in terms of ecological dynamics. We use these ideas to design new online SVM algorithms that exploit ecological invasions, and benchmark performance using the MNIST dataset. Our work provides a new ecological lens through which we can view statistical learning and opens the possibility of designing ecosystems for machine learning.",
  "introduction": "INTRODUCTION Machine learning (ML) is one of the most exciting and useful areas of modern computer science [1, 2] . One common machine learning task is classification: given labeled data from one or more categories, predict the category of a new, unlabeled data point. Another common task is to perform outlier detection (i.e. find data points that appear to be irregular). Both of these difficult problems can be solved efficiently using kernel-based methods such as Support Vector Machines (SVMs) [1, 3, 4] . The basic idea behind SVMs is to use a non-linear map to embed the input data in a high-dimensional feature space where it can be classified using a simple linear classifier (see Figure 1 ). To ensure good generalization and avoid overfitting, SVMs focus on the \"hardest to classify\" points that lie closest to the linear decision surface in the high-dimensional feature space. These points are called \"support vectors\" and play a prominent role in SVM algorithms. The real power and utility of SVMs comes from the fact that these ideas can be implemented quickly and efficiently using kernel methods and quadratic optimization [1, 4] . The idea of a kernel function is to replace the explicit mapping to a high-dimensional feature space with an implicit kernel function that specifies the similarity (dot product) between data points in the highdimensional feature space. Once the kernel function is specified, the support vectors and decision surface can be easily computed as an instance of a Quadratic Programming (QP) problem. There exist efficient exact and approximate optimization algorithms for QP that scale weakly polynomially in input size. The original motivation for SVMs and other kernel methods were deep results in statistical learning theory concerning generalization errors [3] [4] [5] . Here, we show that these statistical problems can also be understood using ideas from niche theory in community ecology (see Table I ) [6, 7] . Our construction exploits the recently discovered duality between ecological dynamics and constrained optimization problems, specifically quadratic programming [8] [9] [10] . In particular, we show FIG. 1 : Overview of Support Vector Machines (SVMs). Data points are mapped into a high-dimensional feature space via φ(X) where they can be separated using a linear decision surface. The SVM tries to maximize the distance (margin) from the decision boundary to the nearest data point. Points that lie on the maximum-margin planes (circled) are called support vectors and used to classify new, unlabeled data. that data points can be viewed as \"species\" that compete for resources, with each feature identified with a distinct resource, and the kernel function specifying the niche overlap between species/datapoints [11, 12] . This mapping allows us to reinterpret SVMs as complex ecosystems that self-organize into ecologically stable steady states defined by their support vectors. This new ecological perspective naturally leads to a new online algorithms based on ecological invasion for SVMs as well as for outlier detection kernel methods such as Support Vector Data Description (SVDD) [13, 14] . We also show that our ecological SVDD method is equivalent to the online algorithm derived in [15] .",
  "body": "INTRODUCTION Machine learning (ML) is one of the most exciting and useful areas of modern computer science [1, 2] . One common machine learning task is classification: given labeled data from one or more categories, predict the category of a new, unlabeled data point. Another common task is to perform outlier detection (i.e. find data points that appear to be irregular). Both of these difficult problems can be solved efficiently using kernel-based methods such as Support Vector Machines (SVMs) [1, 3, 4] . The basic idea behind SVMs is to use a non-linear map to embed the input data in a high-dimensional feature space where it can be classified using a simple linear classifier (see Figure 1 ). To ensure good generalization and avoid overfitting, SVMs focus on the \"hardest to classify\" points that lie closest to the linear decision surface in the high-dimensional feature space. These points are called \"support vectors\" and play a prominent role in SVM algorithms. The real power and utility of SVMs comes from the fact that these ideas can be implemented quickly and efficiently using kernel methods and quadratic optimization [1, 4] . The idea of a kernel function is to replace the explicit mapping to a high-dimensional feature space with an implicit kernel function that specifies the similarity (dot product) between data points in the highdimensional feature space. Once the kernel function is specified, the support vectors and decision surface can be easily computed as an instance of a Quadratic Programming (QP) problem. There exist efficient exact and approximate optimization algorithms for QP that scale weakly polynomially in input size. The original motivation for SVMs and other kernel methods were deep results in statistical learning theory concerning generalization errors [3] [4] [5] . Here, we show that these statistical problems can also be understood using ideas from niche theory in community ecology (see Table I ) [6, 7] . Our construction exploits the recently discovered duality between ecological dynamics and constrained optimization problems, specifically quadratic programming [8] [9] [10] . In particular, we show FIG. 1 : Overview of Support Vector Machines (SVMs). Data points are mapped into a high-dimensional feature space via φ(X) where they can be separated using a linear decision surface. The SVM tries to maximize the distance (margin) from the decision boundary to the nearest data point. Points that lie on the maximum-margin planes (circled) are called support vectors and used to classify new, unlabeled data. that data points can be viewed as \"species\" that compete for resources, with each feature identified with a distinct resource, and the kernel function specifying the niche overlap between species/datapoints [11, 12] . This mapping allows us to reinterpret SVMs as complex ecosystems that self-organize into ecologically stable steady states defined by their support vectors. This new ecological perspective naturally leads to a new online algorithms based on ecological invasion for SVMs as well as for outlier detection kernel methods such as Support Vector Data Description (SVDD) [13, 14] . We also show that our ecological SVDD method is equivalent to the online algorithm derived in [15] . SVMS AS QP Consider a classification problem where each pdimensional data point x i (i = 1, 2, 3 . . . N ) comes with a binary label t i = ±1. A SVM fits a linear classifier to the data of the form y(x) = w T φ(x) + b (1) where φ : R p → R q , q p denotes a mapping to a highdimensional feature space. The scalar offset b and the q-dimensional weight vector w are tunable model parameters. A new data point x k is assigned to class t k = +1 if y(x k ) > 0 and to class t k = -1 if y(x k ) < 0. In the main text, we restrict our discussion to linearly separable datasets, i.e., datasets for which exists a hyperplane in the feature space φ(x) that partitions the dataset into two regions with every point in class +1 in one region and every point in class -1 in the other (see Fig 1) . However, our construction can be easily generalized to non-separable datasets (see Supporting Information). SVMs are trained by maximizing the margin, defined as the Euclidean distance from the line y(x) = 0 (the decision boundary) to the nearest data point. It is easy to show that distance from the point x i to the line y(x) = 0 is given by the expression t i y(xi) |w| . Maximizing the margin corresponds to choosing the parameters w and b so that w, b = arg max w,b 1 |w| min i [t i (w T φ(x i ) + b)] (2) The above maximization problem can be recast by noting that Equation (2) has a gauge degree of freedom: the decision surface is invariant under the scaling transformation w → Dw and b → Db [1, 3, 4] . We can fix this gauge by choosing the margin to be exactly 1. In this gauge, Equation ( 2 ) is equivalent to the following convex quadratic programming problem arg min w,b 1 2 |w| 2 subject to t i (w T φ(x i ) + b) ≥ 1 for all i, (3) where i labels the N data points in the training dataset. As with all constrained optimization problems, we can also solve the equivalent dual optimization problem by introducing generalized Lagrange multipliers a i (often called KKT multipliers in the optimization literature) corresponding to each of the inequality constraints in (3) [16] . Since there is one constraint per data point i, we can uniquely associate each a i with a data point in the training set. For data points that saturate the inequality in (3), a i is positive, and acts as an ordinary Lagrange multiplier to enforce the constraint. For the rest of the data points, no Lagrange multiplier is required, and a i = 0. These observations give rise to the Karush-Kuhn-Tucker conditions, which are necessary and sufficient to determine the optimum [1, 3, 4] : 0 = ∇ w,b L(w, b, a i ) 1 ≤ t i (w T φ(x i ) + b) 0 ≤ a i 0 = a i [t i (w T φ(x i ) + b) -1] (4) where the last three expressions hold for all i, with the SVM Lagrangian L(w, b, a i ) = 1 2 |w| 2 - N i=1 a i [t i (w T φ(x i ) + b) -1]. (5) Solving the first condition for w and b yields the equations w = N i=1 a i t i φ(x i ) and N i=1 a i t i = 0. Inserting these results into Equation (5) gives equations for optimal a i : argmax ai L(a i ) = N i=1 a i - 1 2 N i,j=1 a i a j t i t j K(x i , x j ) subject to 0 ≤ a i for all i and N i=1 a i t i = 1 (6) L(a i ) is called the dual SVM Lagrangian. In writing this equation, we have introduced the kernel function K(x i , x j ) ≡ φ T (x i )φ(x j ) which is just the dot product of the data points in the high-dimensional feature space φ. In this dual formulation, the support vectors correspond precisely to those data points x k for which the corresponding KKT multiplier is greater than zero a k > 0. The SVM can be used to classify a new point x using t = sign(y(x)) with y(x) = i∈S t i a i K(x, x i ) + b b = 1 |S| i∈S   t i - j∈S a j t j K(x i , x j )   and S the set of support vectors. THE ECOLOGY OF SVMS Consider the maximization of the dual Lagrangian L(a i ) given in Equation ( 6 ), subject to the constraints N i=1 a i t i = 0 and a i ≥ 0. Recently, it was shown there exists a duality between constrained optimization and ecological dynamics [8] . Using this duality, it is straightforward to show that the solution to this problem is encoded in the steady state of a generalized Lotka-Volterra equation of the form da i dt = a i   1 + λt i - N j=1 t i t j K(x i , x j )a j   dλ dt = - N i=1 a i t i , (7) This system of differential equations has a natural ecological interpretation as the dynamics of N species with SVM Ecology Data point Species KKT Multiplier Species Abundance Feature Space Trait Space Kernel Niche Overlap Support Vectors Species that survive in ecosystem TABLE I: Conceptual mapping between SVMs and ecology abundances a i (i = 1 . . . N ) whose interactions are represented by the matrix α ij with elements α ij = t i t j K(x i , x j ). ( 8 ) Since each a i corresponds to a data point, we can think of this as an ecological network where data points i and j from the same class ( t i = t j ) compete with each other (i.e. α ij > 0) whereas species of from different classes (t i = -t j ) are mutualistic (i.e. α ij < 0). The level of competition or mutualism depends on the overlap kernel K(x i , x j ) with similar data points having stronger interactions. Ecologically, a combination of competitive and mutualistic interactions such as these naturally occur in plantpollinator networks [17] . In such networks, different species of plants compete with each other for pollinators, pollinators compete with each other for plants, and plantpollinators interactions are beneficial for both kinds of species. The λ term corresponds to an abiotic environmental factor that is produced or consumed by different species. In this plant-pollinator analogy, λ could represent an environmental CO 2 concentration. Specifically, plants consume CO 2 and benefit from high CO 2 concentration while pollinators produce CO 2 and are harmed by high CO 2 concentration. Note that this interpretation differs from the consumer-resource interpretation given to a generic constrained optimization problem in [8] . The Lagrange multiplier λ plays the role of a \"resource,\" but is not required to be positive, since it is enforcing an equality constraint rather than an inequality. The variables a i of the optimization are now treated as the species rather than as resources. These observations suggest a new ecological interpretation of SVMs (see Table I ). Data points act like species that either compete or promote each others' survival. The abundance of each species is the value of KKT multiplier that enforces the corresponding constraint in (3). Since only the support vectors have non-zero KKT multipliers, the only data points that survive in the ecosystem are support vectors. As noted above, data points from the same category compete with each whereas data points from different categories are mutualistic. As is widely appreciated in the ecological literature, the ecological dynamics depends only on the overlap of resource utilization function encoded in the similarity kernel K(x i , x j ) between points [11] . The data points most likely to survive in the ecosystem are data points from one category that are similar to data points from the opposite category since they have large mutualistic interactions. For this reason, the data points that survive in the ecosystem are precisely those lie near the boundary between the two categories, that is, the support vectors. This mapping can also be easily generalized to the case where the data is not linearly separable, and to Support Vector Data Description (SVDD) algorithms [13] [14] [15] for outlier detection (see SI). ECOSVM: AN ONLINE ALGORITHM One interesting class of processes that has been extensively studied in the ecological literature is ecological invasion [9, [18] [19] [20] . In the context of SVMs, invasion by new species corresponds to addition of a new data point (x 0 , t 0 ) to our existing dataset. If we denote the existing support vectors by the set S, the condition for a successful invasion is the intuitive statement that the initial growth rate must be positive when the new data point is introduced into the ecosystem: 0 < 1 a 0 da 0 dt = 1 + λt 0 - j∈S t 0 t j K(x 0 , x j )a j . (9) When this equation is satisfied the new data point can successfully invade the ecosystem and fixate (i.e. become a support vector). If the condition is not satisfied, the point goes \"extinct\" and the set of support vectors does not change. If a data point can invade successfully, the species abundances \"a i \" are modified and can be found by solving for the steady state of (7) using either forward integration or quadratic programming [8] . This suggests a simple new approximate algorithm for online SVM learning we term the EcoSVM. In online learning, rather than seeing all the data at once, training data is presented in a sequential pattern. In the EcoSVM algorithm when a new training data point is presented, the invasion condition ( 9 ) is used to determine whether it can successfully invade the ecosystem. If it cannot, the training data point is discarded. If it can, we recompute the steady-states using Equation (7) . This algorithm can be easily generalized to the case of non-separable data (see Supporting Information and attached code implementing the algorithm). Because we use the ecologically inspired invasion condition, there is no need to recompute the support vectors at each learning step, resulting in a faster and more memory-efficient online algorithm than those that were previously suggested [21] [22] [23] [24] [25] [26] . The EcoSVM algorithm also reduces the amount of training data that needs to be stored in memory. Specifically, instead of needing to store all data points, we keep only the support vectors. Since the number of support vectors is in general a small subset of all the training data, this greatly reduces the memory requirements. This increased efficiency comes at the expense of introducing small errors that come from the contingent nature of ecological invasions. Occasionally, a successful invasion by a new species (new data point) will allow a species that could not previously invade the ecosystem (designated not a support vector) to become viable (a support vector). This kind of historical contingency introduces errors in our online algorithm since we discard all data points that do not fix in the ecosystem. In practice, we find that these errors are generally quite small for real-world datasets. NUMERICAL TESTS We tested EcoSVM using several numerical examples. We started with two toy datasets with N = 200 data points x = (c 1 , c 2 ) drawn uniformly from a twodimensional hypercube x ∈ [0, 1] 2 . We enforced linear separability using a decision boundary given by c 1 = 1/2 so that data points with c 1 < 1/2 were assigned to one class t = -1 and those with c 1 > 1/2 were assigned to a different class t = 1 (see Figure 2 ). We also created a dataset where the decision boundary was given by c 1 = 1 2 + 1 10 sin(2πc 2 ), which is not linearly separable in the given feature space. After initialization using the first 10 data points, we trained an EcoSVM using the online scheme described above. As can be seen visually in Figure 2 , the decision boundaries found by the EcoSVM were very similar to those found using an ordinary batch SVM algorithm. Furthermore, the final accuracy and number of support vectors of the EcoSVM algorithms and ordinary batch SVM algorithm were almost identical (See SI for more detail). Next, we tested the performance of EcoSVM algorithm on MNIST [27, 28] , a standard benchmark dataset in machine learning. The MNIST dataset consists of 6, 000 training images and 1, 000 test images of each of the handwritten digits '0'-'9'. To test the EcoSVM, we considered the binary classification task of distinguishing fours and nines. For this classification problem, we used a standard Gaussian (RBF) kernel given by K σ (x, y) = exp -1 2σ 2 (x -y) 2 , where the kernel width σ was determined via cross validation on the batch SVM. The performance of our EcoSVM algorithm was comparable to a traditional SVM trained on the full dataset ( 98.1% accuracy compared to 98.5% accuracy for traditional SVMs, as shown in Figure 3 ). The EcoSVM algorithm also ends up finding a similar number of support vectors as a traditional SVM: ∼ 750. We note that since the MNIST dataset is not completely linearly separable in the RBF feature space, in these numerical simulations we used the generalization of the EcoSVM algorithm for non-linearly separable datasets discussed in the SI. FIG. 3 : An ecologically inspired online SVM algorithm EcoSVM applied to digit classification of ones and fours from the MNIST dataset [27, 28] . Accuracy of EcoSVM for 25 different realizations. The average accuracy over all realizations is shown using the solid blue line and the accuracy of an SVM trained on the entire dataset is shown using the blue dotted line. The inset panel shows the number of active support vectors in each realization (black lines), the mean number of support vectors across all realizations (blue solid line) and the number of support vectors for SVM trained on full dataset (dotted blue line). CONCLUSION In this work, we have shown how we can think about kernel methods using ideas from ecology. This ecological mapping allowed us to formulate a new ecologically inspired online SVM algorithm, the EcoSVM. We have shown that the performance of EcoSVM is comparable to traditional SVMs that work with all data simultaneously. Our algorithm differs from previous online SVMs which must recompute the support vectors at each learning step [21] [22] [23] [24] [25] [26] allowing for faster implementation and smaller memory requirements. While in the main text we focus on linearly separable data, as shown in the SI these same ideas can be generalized to non-separable data and for outlier detection (SVDD) using unlabeled data. Our results suggest that ecological dynamics may provide a rich new setting for thinking about biologically-inspired machine learning. They also suggest the tantalizing possibility that it maybe possible to engineer synthetic ecosystems that implement sophisticated statistical learning algorithms [29] . ACKNOWLEDGMENTS OH acknowledges support from BU UROP student funding. The work was supported by NIH NIGMS grant 1R35GM119461, Simons Investigator in the Mathematical Modeling of Living Systems (MMLS) to PM. Supporting Information Code implementing the EcoSVM algorithm is found at https://github.com/owenhowell20/EcoSVM . ADDING THE SLACK In the main text we have focused on datasets that are linearly separable. For the majority of practical applications this is not the case. For overlapping class distributions, the primal SVM problem is modified so that points are allowed to be on the wrong side of the margin. Specifically, slack variables ζ i ≥ 0 are introduced with t i y(x i ) ≥ 1 -ζ i . This should be compared with the linearly separable case where the constraint is instead t i y(x i ) ≥ 1. The new minimization is weighted to penalize points that lie on the wrong side of the margin arg min w,b,ζi 1 2 |w| 2 + C N i=1 ζ i subject to t i (w T φ(x i ) + b) ≥ 1 for all i, (10) where the slack parameter C determines the extent to which points on the wrong side of the margin are tolerated. In practice, C is a hyper-parameter that is tuned to minimize generalization error. The KKT conditions for this new minimization problem are: 0 = ∇ w,b,ζi L(w, b, ζ i , a i , µ i ) 1 -ζ i ≤ t i y(x i ) 0 ≤ a i 0 = a i [t i y(x i ) -1 + ζ i ] 0 ≤ ζ i 0 ≤ µ i 0 = µ i ζ i ( 11 ) where the µ i are additional KKT multipliers enforcing the constraints ζ i ≥ 0, and the primal Lagrangian is L(w, b, ζ i , a i , µ i ) = 1 2 |w| 2 + N i=1 [Cζ i -a i (t i y(x i ) -1 + ζ i ) -µ i ζ i ] Minimizing the Lagrangian ∂L ∂wi = 0 , ∂L ∂b = 0 and ∂L ∂ζi = 0 gives equations w = N i=1 t i a i φ(x i ) , N i=1 a i t i = 0 , a i = C -µ i ( 12 ) Each µ i ≥ 0 so the last equation is equivalent to a i ≤ C. Inserting these results into the primal Lagrangian transforms the problem into maximization of the dual SVM Lagrangian argmax ai L(a i ) = N i=1 a i - 1 2 N i,j=1 a i a j t i t j K(x i , x j ) subject to 0 ≤ a i ≤ C for all i and N i=1 a i t i = 1 ( 13 ) We can enforce the second constraint by introducing a Lagrange multiplier λ, resulting in the following set of equations for the optimal a i : argmax ai,λ L(a i , λ) subject to 0 ≤ a i ≤ C for all i ( 14 ) with Lagrangian L(a i , λ) = N i=1 a i - 1 2 N i,j=1 a i a j t i t j K(x i , x j ) + λ N i=1 t i a i (15 ) Using the duality described in [8] , we can map the quadratic programming problem ( 14 ) to ecological dynamics da i dt = a i (C -a i )(1 + λt i - N j=1 t i t j K(x i , x j )a j ) dλ dt = - N i=1 a i t i . (16) where the prefactor a i (C -a i ) enforces the constraints on a i . Equation ( 16 ) has a similar interpretation to the Lotka-Volterra equations for the linearly separable case, with the additional (C -a i ) factor can be interpreted as each species having a maximum carrying capacity C [30] . Now consider the addition of new point P 0 = (x 0 , t 0 ). This point changes the set of support vectors if the initial growth rate is positive. 0 < 1 a 0 da 0 dt = 1 + λt 0 - N j=1 t 0 t j K(x 0 , x j )a j (17) Let x k be any \"active\" support vector, that is, a point whose KKT multiplier a k satisfies C > a k > 0. Then, solving the steady state equation ( 16 ) for auxiliary variable λ gives λ = -t k + N i=1 t i K(x i , x k )a i ( 18 ) Inserting this into Equation (17) gives the invasion condition 0 < 1 a 0 da 0 dt = 1 -t k t 0 + N i=1 t i t 0 (K(x i , x k ) -K(x i , x 0 ))a i (19) This invasion condition can be used to construct an online learning algorithm. Specifically, when a new data point is presented, the condition ( 19 ) can be used to determine whether the new point changes the set of support vectors without having to recompute the minimum of ( 13 ). The nonzero KKT multipliers a i > 0 and corresponding support vectors x i are kept in memory. A new point x is classified using t = sign(y(x)) with y(x) = i t i a i K(x, x i ) + b b = 1 |M | i∈M   t i - j∈S a j t j K(x i , x j )   where S is the full set of support vectors and M is the subset of active support vectors. Note that this formula requires at least one active support vector. PERFORMANCE ON TOY MODELS We test our proposed online learning algorithms on two toy datasets. We consider one dataset that is linearly separable in the feature space φ(x) = x. Specifically, we choose all data points to be drawn from the [0, 1] p p-dimensional hypercube. We then define the decision surface: B 1 : (x 1 = 1 2 , x 2 , ...x p ) We consider a second dataset that is not linearly separable. Specifically, we define the second dataset to have decision boundary given by: B 2 : (x 1 = 1 2 + 1 10 sin(2πx 2 ) sin(2πx 3 )... sin(2πx p ), x 2 , ...x p ) To test our proposed algorithm, we draw N points from the p dimensional hypercube. The minimum of the SVM Lagrangian is found for N s points with N s N . We require that N s is greater then p or else there are flat directions and our algorithm can become unstable. At each step, a new point is presented and the invasion condition ( 17 ) is used to determine whether the set of support vectors is changed. If ( 17 ) is satisfied, the steady state is recomputed using quadratic programming. This is continued for all N points. We find an excellent agreement between the predictions of our online algorithm and an batch SVM trained using all N points for both the linearly separable and non-linearly separable datasets. We study how the test accuracy and number of support vectors depend on the training epoch T . For this purpose, let us define the accuracy A(T ) = 1 - 1 |N test | x∈Ntest 1 2 |t T (x) -t Exact (x)| where N test is the set of testing data and t Exact (x) is the true label corresponding to point x. t T (x) denotes the prediction of the online SVM trained with T data points. In addition, let us define N (T ) =          Number of a i (T ) > 0 for linearly seperable case Number of C > a i (T ) > 0 for non-linearly seperable case where a i (T ) are the support vector coefficients of the online SVM trained with T data points. In both the linear and non-linear case N (T ) counts the number of active support vectors. Figure 4 and 5 show that in both the linear and nonlinear case for large T the online algorithm converges to an accuracy A(T ) that is just below the accuracy of a batch SVM. Furthermore the number of active support vectors that the the online method finds after training is slightly below the number of support vectors that the batch SVM has. ECOLOGY TO SVDD We show how the method presented in the main text can be used to derive an approximate online SVDD learning algorithm first constructed by Jiang et al. in 2017 [15] . The Support Vector Data Description (SVDD) problem is concerned with unsupervised location of outliers in single-class classification problems (see [13, 14] for a good overview). The SVDD problem consists finding a FIG. 6: Schematic showing instances of a supervised classification problem (left) and an outlier detection problem (right). In the supervised classification problem the goal is to partition the space into two distinct volumes, one for each label. The thick black line denotes the decision boundary. An incorrectly classified four (inside the red hexagon) is shown on the wrong side of the decision boundary. In the outlier detection problem, a sphere is created in the feature space to enclose the minimum volume while still containing all data points (in this case fours). Points on the boundary of this sphere are called outliers, they are starred in the schematic. The thick black line denotes the sphere boundary. sphere of minimum radius in kernel space that contains all data points. Figure 6 shows schematically a classification problem and an outlier detection problem. Points that lie on the surface of the sphere are called outliers and are analogous to active support vectors in the SVM problem. The problem is formulated as follows. Given a set of unlabeled data points D = (x i ) N i=1 : minimize R 2 subject to |φ(x i ) -µ| 2 ≤ R 2 for all i ( 20 ) where R is the sphere radius in feature space φ(x) and µ is the center of the sphere in the feature space. This problem is simplified by the introduction of KKT multipliers a i for each inequality constraint. The KKT conditions for the minimization problem are: 0 = ∇ R,µ L(R, µ, a i ) 0 ≤ a i |φ(x i ) -µ| 2 -R 2 ≤ 0 a i [|φ(x i ) -µ| 2 -R 2 ] = 0 (21) with L(R, µ, a i ) = R 2 + N i=1 a i (|φ(x i ) -µ| 2 -R 2 ) Minimizing L(R, µ, a i ) with respect to µ and R gives equations: N i=1 a i = 1 and µ = N i=1 a i φ(x i ) (22) Substituting Equation (22) into Equation ( 21 ) gives maximization problem for the optimal a i : argmax ai L(a i ) = N i=1 a i K(x i , x i ) - N i,j=1 a i a j K(x i , x j ) subject to 0 ≤ a i for all i and N i=1 a i = 1 (23) L(a i ) is called the dual SVDD Lagrangian and K(x i , x j ) = φ(x i ) T φ(x j ). For simplicity we set K(x i , x i ) = 1 for the diagonal elements in the rest of the derivation, although the results we present are easily generalized for arbitrary kernel. The Python code included in the supplemental material works for any choice of K(x, y). After the SVDD is trained, the sphere radius in feature space R can be determined via R 2 = max i |φ(x i ) -µ| 2 = max i (φ(x i ) -µ) T (φ(x i ) -µ) = max i (φ(x i ) T φ(x i ) -2µ T φ(x i ) + µ T µ) (24) The explict dependence on φ(x) in ( 24 ) can be removed using µ = N i=1 a i φ(x i ) : φ(x i ) T φ(x i ) -2µ T φ(x i ) + µ T µ = 1 -2 N j=1 K(x i , x j )a j + N j,k=1 K(x j , x k )a j a k where we have used φ(x i ) T φ(x j ) = K(x i , x j ) and K(x i , x i ) = 1. Thus, (24) can be written in terms of kernel function and support vectors as R 2 = max i 1 -2 N j=1 K(x i , x j )a j + N j,k=1 K(x j , x k )a j a k (25) We apply our method to the SVDD Lagrangian 23. As K(x i , x i ) = 1 and N i=1 a i = 1 the first term in the sum can be ignored. A Lagrange multiplier λ is introduced to enforce the latter constraint. The quantity to be maximized is then argmax ai,λ L(a i , λ) = - N i,j=1 a i a j K(x i , x j ) + λ( N i=1 a i -1) subject to 0 ≤ a i for all i (26) Using the quadratic programming-ecology duality, we can embed the solution to (23) as the steady state of the dynamical equations da i dt = a i (λ - N j=1 K(x i , x j )a j ) dλ dt = 1 - N i=1 a n (27) Now, suppose we are at the steady state of Equation ( 27 ) and consider the addition of a new point x 0 . The invasion condition is that the initial growth rate is positive 0 < 1 a 0 da 0 dt = λ - N j=1 K(x 0 , x j )a j (28) Let x k be any point which has non-zero support vector, a k > 0. Then, solving the steady state equation ( 27 ) for auxiliary variable λ gives λ = N i=1 K(x i , x k )a i (29) Inserting this into Equation (28) gives the invasion condition 0 < 1 a 0 da 0 dt = N i=1 a i [K(x k , x i ) -K(x 0 , x i )] (30) which is identical to Equation (2.9) derived in [15] (Note that they use notation z for our variable x 0 ). Equation (30) can be used to formulate an online learning algorithm in the same manner as the EcoSVM algorithm presented in the main text. The paper by Jiang et al. derives Equation (30) and calls the algorithm Fast Incremental Support Vector Data Description (FISVDD). They also numerically shows that this algorithm performs well on real-world datasets. The fact that this algorithm can be constructed simply and elegantly using a duality between quadratic programming and ecology suggests that there is a deeper connection between machine learning and ecological dynamics than previously realized and, more significantly, that ecologically inspired machine learning models are not just of theoretical interest but can be used for real world data analysis. We illustrate the ability of FISVDD/EcoSVDD numerically. We draw data points from a p-dimensional multinomial Gaussian distribution with identity covariance matrix and mean uniformly sampled from the pdimensional hypercube x ∈ [0, 1] p . Figure (7) shows the two-dimensional case. We define R(T ) to be the SVDD radius (25) of an FISVDD/EcoSVDD trained on T points. The left panel of Figure 7 shows R(T ) as a function of T . R(T ) converges to the batch SVDD radius (shown with a dashed blue line). Similarly, we define a similarity metric between the FISVDD/EcoSVDD kernel sphere center trained on T points µ(T ) (22) and the batch SVDD sphere center μ as FIG. 2 : 2 FIG. 2: Comparison of classical SVM and EcoSVM algorithms for a data set with N = 200 total training points points for (Left) a linearly separable dataset and (Right) a dataset where the data is not linearly separable (see SI for more detail on algorithm in non-linear case). The true decision boundary is given by dashed black line. Cyan regions show data that the full SVM and online SVM both identify as ti = 1. Blue regions show data that full SVM and online SVM both identify as ti = -1. Purple regions show area in which SVM and online SVM disagree. ti = 1 data points are shown as green plus symbols, ti = -1 data points are shown as red circles. Active support vectors are shown with larger symbols. FIG. 4 : 4 FIG. 4: Test accuracy and number of support vectors as a function of time for the linearly separable toy model, with true decision boundary B1 defined in the SI text. Left panel shows accuracy of online SVM algorithm A(T ) as a function of the number of points T that the online SVM has seen. Black lines show individual realization, blue line shows mean accuracy. Dotted blue line shows full SVM accuracy. Right panel shows the number of support vectors N (T ) as a function of the number of points T . Black lines show individual realization, blue line shows mean number of support vectors. Dotted blue line shows number of support vectors in SVM trained on entire dataset at once. The dimension of the data space is p = 100 and the online training is initialized with Ns = 30 data points. FIG. 5 : 5 FIG.5: Same as Figure4, but for the non-linearly separable toy model, with true decision boundary B2 defined in the SI text. The dimension is p = 30 and the initial number of points is Ns = 30. FIG. 7 : 7 FIG. 7: Comparison of batch SVDD and online SVDD algorithms for a data set with N = 100 total points (shown in black) drawn from a Gaussian distribution. Batch SVDD active support vectors are shown with red \"+\" symbols. EcoSVDD active support vectors are shown with green stars. The kernel function is Gaussian K(x, y) = exp( -12 (x-y) T (xy)). The EcoSVDD algorithm was started with 10 points. S (T ) = µ(T ) T μ µ(T ) T µ(T ) μT μ(31)S(T ) ≤ 1 with equality if and only if µ(T ) = μ. The right panel of Figure8shows S(T ) as a function of T . S(T ) converges to 1 illustrating the fact that the FISVDD/EcoSVDD and batch SVDD produce the same kernel sphere center and radius. FIG. 8 : 8 FIG. 8: Online SVDD radius and kernel sphere center similarity score as function of T . Left panel shows online SVDD radius R(T ) as a function of the number of points T that the online SVM has seen. Black lines show individual realization, blue line shows mean radius. Dotted blue line shows batch SVDD radius. Right panel shows the normalized dot product between µ(T ) and μ. Black lines show individual realizations, blue line shows average over realizations. The dimension of the data space is p = 15 and the online training is initialized with Ns = 30 data points."
}