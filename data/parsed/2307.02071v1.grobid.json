{
  "title": "A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables",
  "abstract": "High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set, or in other words, there are few data points per level. Machine learning methods can have difficulties with high-cardinality variables. In this article, we empirically compare several versions of two of the most successful machine learning methods, tree-boosting and deep neural networks, and linear mixed effects models using multiple tabular data sets with high-cardinality categorical variables. We find that, first, machine learning models with random effects have higher prediction accuracy than their classical counterparts without random effects, and, second, tree-boosting with random effects outperforms deep neural networks with random effects.",
  "introduction": "Introduction High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set or, equivalently, there is little data per level. Highcardinality categorical variables can pose difficulties for machine learning methods such as deep neural networks and tree-based models. A simple strategy for dealing with categorical variables is to use one-hot encoding or dummy variables. But this approach often does not work well for high-cardinality categorical variables due to the reasons described below. For neural networks, a frequently adopted solution is to use entity embeddings [Guo and Berkhahn, 2016 ] that map every level of a categorical variable into a low-dimensional Euclidean space. For tree-boosting, an alternative to one-hot encoding is to assign a number to every level of a categorical variable, and then consider this as a one-dimensional numeric variable. Another solution implemented in the LightGBM boosting library [Ke et al., 2017] works by partitioning all levels into two subsets using an approximate approach [Fisher, 1958] when finding splits in the tree-building algorithm. Further, the CatBoost boosting library [Prokhorenkova et al., 2018] implements an approach based on ordered target statistics calculated using random partitions of the training data for handling categorical predictor variables. Random effects [Laird et al., 1982, Pinheiro and Bates, 2006] can also be used as a tool for handling high-cardinality categorical variables. In a random effects model, it is assumed that a (potentially transformed) parameter µ ∈ R n of the response variable distribution equals the sum of fixed F (X) and random effects Zb: µ = F (X) + Zb, b ∼ N (0, Σ), where F (X) is the row-wise evaluation of a function F (•) : R p → R, F (X) = (F (X 1 ), . . . , F (X n )) T , X i = (X i1 . . . , X ip ) T ∈ R p is the i-th row of the fixed effects variables matrix X ∈ R n×p , i = 1, . . . , n, b ∈ R m , and Z ∈ R n×m . These models are called mixed effects models since they contain both fixed effects F (X) and random effects Zb. If the conditional response variable distribution is Gaussian and there is a single high-cardinality categorical variable, such a mixed effects model can also be written as y ij = F (x ij ) + b i + ϵ ij , b i iid ∼ N (0, σ 2 1 ), ϵ ij iid ∼ N (0, σ 2 ), (1) where j = 1, . . . , n i is the sample index within level i with n i being the number of samples for which the categorical variable attains level i, i = 1, . . . , q is the level index with q being the number of levels of the categorical variable, and x ij are the fixed effects predictor variables for observation ij. The total number of samples is n = q i=1 n i . Further, the random effects b i and ϵ ij are assumed to be independent. For this model, the matrix Z is simply a binary incidence matrix that maps every random effect b i to its corresponding observations and Σ = σ 2 I m . In (generalized) linear mixed effects models it is assumed that F (•) is a linear function: F (X) = Xβ. In the last years, linear mixed effects models have been extended to non-linear ones using single trees [Hajjem et al., 2011 , Sela and Simonoff, 2012 , Fu and Simonoff, 2015] , random forest [Hajjem et al., 2014] , tree-boosting [Sigrist, 2022 [Sigrist, , 2023]] , and most recently (in terms of first public preprint) deep neural networks [Simchoni and Rosset, 2021 , 2023 , Avanzi et al., 2023] . In contrast to classical independent machine learning models, the random effects introduce dependence among samples. 1.1 Why are random effects useful for high-cardinality categorical variables? For high-cardinality categorical variables, there is little data for every level. Intuitively, if the response variable has a different (conditional) mean for many levels, traditional machine learning models (with, e.g., one-hot encoding, embeddings, or simply one-dimensional numeric variables) may have problems with over-or underfitting for such data. From the point of view of a classical bias-variance trade-off, independent machine learning models may have difficulties balancing this trade-off and finding an appropriate amount of regularization. For instance, overfitting may occur which means that a model has a low bias but high variance. Broadly speaking, random effects act as a prior, or regularizer, which models the difficult part of a function, i.e., the part whose \"dimension\" is similar to the total sample size, and, in doing so, provide an effective way for finding a balance between over-and underfitting or bias and variance. For instance, for a single categorical variable, random effects models will shrink estimates of group intercept effects towards the global mean. This process is sometimes also called \"information pooling\". It represents a trade-off between completely ignoring the categorical variable (= underfitting / high bias and low variance) and giving every level in the categorical variable \"complete freedom\" in estimation (= overfitting / low bias and high variance). Importantly, the amount of regularization, which is determined by the variance parameters of the model, is learned from the data. Specifically, in the above single-level random effects model in (1), a (point) prediction ŷp for the response variable for a sample with predictor variables x p and categorical variable having level i is given by ŷp = F (x p ) + σ2 1 σ2 /n i + σ2 1 (ȳ i -Fi ), where F (x p ) is the trained function evaluated at x p , σ2 1 and σ2 are variance estimates, and ȳi and Fi are sample means of y ij and F (x ij ), respectively, for level i. Ignoring the categorical variable would give the prediction ŷp = F (x p ), and a fully flexible model without regularization gives ŷp = F (x p )+(ȳ i -Fi ). I.e., the difference between these two extreme cases and the random effects model is the shrinkage factor σ2 1 σ2 /ni+σ foot_1 1 (which goes to zero if the number of samples n i for level i is large). Related to this, random effects models allow for more efficient (i.e., lower variance) estimation of the fixed effects function F (•) [Sigrist, 2022] . See also Sigrist [2023, Section 1.1] for a discussion on why random effects are useful for modeling high-cardinality categorical variables. In line with the above argumentation, Sigrist [2023, Section 4 .1] find in empirical experiments that tree-boosting combined with random effects outperforms traditional independent tree-boosting the more, the lower the number of samples per level of a categorical variable, i.e., the higher the cardinality of a categorical variable. 2 Methods, data sets, and experimental settings In the following, we compare several methods using multiple real-world data sets with highcardinality categorical variables. We use all the publicly available tabular data sets from Simchoni and Rosset [2021, 2023] and also the same experimental setting as in Simchoni and Rosset [2021, 2023] . In addition, we include the Wages data set analyzed in Sigrist [2022] .",
  "body": "Introduction High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set or, equivalently, there is little data per level. Highcardinality categorical variables can pose difficulties for machine learning methods such as deep neural networks and tree-based models. A simple strategy for dealing with categorical variables is to use one-hot encoding or dummy variables. But this approach often does not work well for high-cardinality categorical variables due to the reasons described below. For neural networks, a frequently adopted solution is to use entity embeddings [Guo and Berkhahn, 2016 ] that map every level of a categorical variable into a low-dimensional Euclidean space. For tree-boosting, an alternative to one-hot encoding is to assign a number to every level of a categorical variable, and then consider this as a one-dimensional numeric variable. Another solution implemented in the LightGBM boosting library [Ke et al., 2017] works by partitioning all levels into two subsets using an approximate approach [Fisher, 1958] when finding splits in the tree-building algorithm. Further, the CatBoost boosting library [Prokhorenkova et al., 2018] implements an approach based on ordered target statistics calculated using random partitions of the training data for handling categorical predictor variables. Random effects [Laird et al., 1982, Pinheiro and Bates, 2006] can also be used as a tool for handling high-cardinality categorical variables. In a random effects model, it is assumed that a (potentially transformed) parameter µ ∈ R n of the response variable distribution equals the sum of fixed F (X) and random effects Zb: µ = F (X) + Zb, b ∼ N (0, Σ), where F (X) is the row-wise evaluation of a function F (•) : R p → R, F (X) = (F (X 1 ), . . . , F (X n )) T , X i = (X i1 . . . , X ip ) T ∈ R p is the i-th row of the fixed effects variables matrix X ∈ R n×p , i = 1, . . . , n, b ∈ R m , and Z ∈ R n×m . These models are called mixed effects models since they contain both fixed effects F (X) and random effects Zb. If the conditional response variable distribution is Gaussian and there is a single high-cardinality categorical variable, such a mixed effects model can also be written as y ij = F (x ij ) + b i + ϵ ij , b i iid ∼ N (0, σ 2 1 ), ϵ ij iid ∼ N (0, σ 2 ), (1) where j = 1, . . . , n i is the sample index within level i with n i being the number of samples for which the categorical variable attains level i, i = 1, . . . , q is the level index with q being the number of levels of the categorical variable, and x ij are the fixed effects predictor variables for observation ij. The total number of samples is n = q i=1 n i . Further, the random effects b i and ϵ ij are assumed to be independent. For this model, the matrix Z is simply a binary incidence matrix that maps every random effect b i to its corresponding observations and Σ = σ 2 I m . In (generalized) linear mixed effects models it is assumed that F (•) is a linear function: F (X) = Xβ. In the last years, linear mixed effects models have been extended to non-linear ones using single trees [Hajjem et al., 2011 , Sela and Simonoff, 2012 , Fu and Simonoff, 2015] , random forest [Hajjem et al., 2014] , tree-boosting [Sigrist, 2022 [Sigrist, , 2023]] , and most recently (in terms of first public preprint) deep neural networks [Simchoni and Rosset, 2021 , 2023 , Avanzi et al., 2023] . In contrast to classical independent machine learning models, the random effects introduce dependence among samples. 1.1 Why are random effects useful for high-cardinality categorical variables? For high-cardinality categorical variables, there is little data for every level. Intuitively, if the response variable has a different (conditional) mean for many levels, traditional machine learning models (with, e.g., one-hot encoding, embeddings, or simply one-dimensional numeric variables) may have problems with over-or underfitting for such data. From the point of view of a classical bias-variance trade-off, independent machine learning models may have difficulties balancing this trade-off and finding an appropriate amount of regularization. For instance, overfitting may occur which means that a model has a low bias but high variance. Broadly speaking, random effects act as a prior, or regularizer, which models the difficult part of a function, i.e., the part whose \"dimension\" is similar to the total sample size, and, in doing so, provide an effective way for finding a balance between over-and underfitting or bias and variance. For instance, for a single categorical variable, random effects models will shrink estimates of group intercept effects towards the global mean. This process is sometimes also called \"information pooling\". It represents a trade-off between completely ignoring the categorical variable (= underfitting / high bias and low variance) and giving every level in the categorical variable \"complete freedom\" in estimation (= overfitting / low bias and high variance). Importantly, the amount of regularization, which is determined by the variance parameters of the model, is learned from the data. Specifically, in the above single-level random effects model in (1), a (point) prediction ŷp for the response variable for a sample with predictor variables x p and categorical variable having level i is given by ŷp = F (x p ) + σ2 1 σ2 /n i + σ2 1 (ȳ i -Fi ), where F (x p ) is the trained function evaluated at x p , σ2 1 and σ2 are variance estimates, and ȳi and Fi are sample means of y ij and F (x ij ), respectively, for level i. Ignoring the categorical variable would give the prediction ŷp = F (x p ), and a fully flexible model without regularization gives ŷp = F (x p )+(ȳ i -Fi ). I.e., the difference between these two extreme cases and the random effects model is the shrinkage factor σ2 1 σ2 /ni+σ foot_1 1 (which goes to zero if the number of samples n i for level i is large). Related to this, random effects models allow for more efficient (i.e., lower variance) estimation of the fixed effects function F (•) [Sigrist, 2022] . See also Sigrist [2023, Section 1.1] for a discussion on why random effects are useful for modeling high-cardinality categorical variables. In line with the above argumentation, Sigrist [2023, Section 4 .1] find in empirical experiments that tree-boosting combined with random effects outperforms traditional independent tree-boosting the more, the lower the number of samples per level of a categorical variable, i.e., the higher the cardinality of a categorical variable. 2 Methods, data sets, and experimental settings In the following, we compare several methods using multiple real-world data sets with highcardinality categorical variables. We use all the publicly available tabular data sets from Simchoni and Rosset [2021, 2023] and also the same experimental setting as in Simchoni and Rosset [2021, 2023] . In addition, we include the Wages data set analyzed in Sigrist [2022] . Methods We consider the following methods: • 'Linear': linear mixed effects models • 'NN Embed': deep neural networks with embeddings • 'LMMNN': combining deep neural networks and random effects [Simchoni and Rosset, 2021, 2023] • 'LGBM Num': tree-boosting by assigning a number to every level of categorical variables and considering these as one-dimensional numeric variables • 'LGBM Cat': tree-boosting with the approach of LightGBM [Ke et al., 2017] for categorical variables • 'CatBoost': tree-boosting with the approach of CatBoost [Prokhorenkova et al., 2018] for categorical variables • 'GPBoost': combining tree-boosting and random effects [Sigrist, 2022 [Sigrist, , 2023] ] The MERF algorithm of Hajjem et al. [2014] is another promising mixed effects machine learning method, but its current implementation in the form of a Python package foot_0 is prohibitively slow for the sample sizes of the data sets considered here. Also note that, recently (starting with version 1.6), the XGBoost library [Chen and Guestrin, 2016] has also implemented the same approach as LightGBM for handling categorical variables. 2 We do not consider this as a separate approach here. Data sets Table 1 gives an overview of the data sets. For more details on the data, we refer to Simchoni and Rosset [2021, 2023] and Sigrist [2022] . For all methods with random effects, we include random effects for every categorical variable mentioned in and Wages data sets are longitudinal data sets. This means that the samples for every level or the categorical variables are repeated measurements over time t. As in Simchoni and Rosset [2023] , we additionally include random coefficients (= random slopes), for the time variables t and t 2 besides intercept random effects with no prior correlation among the random effects. I.e., these random effects models are given by y ij = F (x ij ) + b 0,i + b 1,i • t ij + b 2,i • t 2 ij + ϵ ij , b k,i iid ∼ N (0, σ 2 k ), k ∈ {1, 2, 3}, ϵ ij iid ∼ N (0, σ 2 ), where b 1,i and b 2,i are the random slopes. Experimental setting We use the same experimental setting as in Simchoni and Rosset [2023] to compare the different methods. This means that we perform 5-fold cross-validation with the test mean squared error (MSE) to measure prediction accuracy. For the longitudinal data sets, we use the \"random mode\" 5-fold CV setting of Simchoni and Rosset [2023] where in every fold, 80% of the data is used for training and 20% of the data for validation irrespective of the time variable t. The results for neural networks with embeddings and also neural networks with random effects (LMMNN) are taken from Simchoni and Rosset [2021, 2023] ; see Simchoni and Rosset [2021, 2023] for more details on the specifications of these two modeling approaches. For linear mixed effects models, LGBM Num, LGBM Cat, GPBoost, and GPBoost I, we use the GPBoost library version 1.2.1 which is built upon the LightGBM library. Further, we use version 1.1.1 of the CatBoost library. For all tree-boosting methods, we choose tuning parameters on every of the five training sets in the 5-fold CV by randomly splitting the training data into inner training data containing 80% of the outer training data and validation data consisting of the remaining 20%. We use a deterministic grid search with the mean squared error as a selection criterion and consider the following parameter combinations: number of boosting iterations M ∈ {1, . . . , 1000}, learning rate ∈ {1, 0.1, 0.01}, maximal tree-depth ∈ {1, 2, 3, 5, 10}, minimal number of samples per leaf ∈ {10, 100, 1000}, and L2 penalty on leaf values ∈ {0, 1, 10}. For the machine learning models with random effects, one can either exclude or include the high-cardinality categorical variables in the fixed effects function. When additionally including them, one allows for potential interaction between the categorical variables and other predictor variables. For GPBoost, we consider this as a tuning parameter option. However, the difference between these two modeling options is minor which is an indication that there is no interaction present; see Table 4 in the appendix, where we report the results when separately either excluding or including the high-cardinality categorical variables in the fixed effects tree ensemble function. Code for pre-processing the data with instructions on how to download the data and code for running the experiments can be found at https://github.com/fabsig/Compare_ML_HighCardinality_  Categorical_Variables . Pre-processed data for modeling can also be found on the above webpage for data sets for which the license of the original source permits it. Results The results are reported in Figure 1 and Table 2 . Table 2 reports test mean squared errors (MSE) and corresponding standard errors. To summarize the prediction accuracy over the different data sets, we report average relative differences to the best result and average ranks. The former is obtained by calculating the relative difference of a test MSE of a method compared to the lowest MSE for every data set and then taking the average over all data sets. In Figure 1 , we also report the average relative difference to the best result. In addition, Table 3 in the appendix reports average wall clock times. Figure 1 : Average relative difference (in %) to the lowest test MSE. The Wages data set is not included for calculating this since not all methods were run on it. The results in Table 2 show that combined tree-boosting and random effects (GPBoost) has the highest prediction accuracy. GPBoost has an average relative difference to the best result of 7.02% and an average rank of 2.14. Combined neural networks and random effects (LMMNN) have an average relative difference to the best result of 17.3% and an average rank of 3.57. LightGBM (LGBM Cat) has a similar average relative difference to the best result of 17.4% and an average rank of 2.71. Next, CatBoost has an average relative difference to the best method of 44.5% and an average rank of 2.86. Linear mixed effects models perform worse having an average relative difference of approximately 47.6% and an average rank of 4.71. Overall worst perform neural networks with embeddings having an average relative difference to the best result of 189%. Tree-boosting with the categorical variables transformed to one-dimensional numeric variables (LGBM Num) performs slightly better with an average relative difference to the best result of 111%. foot_3 Conclusion We empirically compare various versions of tree-boosting and deep neural networks as well as linear mixed effects models on multiple tabular data sets with high-cardinality variables. We find that, first, machine learning models with random effects have higher prediction accuracy than their classical independent counterparts without random effects and, second, tree-boosting with random effects performs better than deep neural networks with random effects. While there may be several reasons for the latter finding, this is in line with the recent work of Grinsztajn et al. [2022] who find that tree-boosting outperforms deep neural networks (and also random forest) on tabular data without high-cardinality categorical variables. Similarly, Shwartz-Ziv and Armon [2022] conclude that tree-boosting \"outperforms deep models on tabular data.\" Appendices A Additional results Dataset Linear LGBM Cat LGBM Num CatBoost GPBoost NN Embed LMMNN Airbnb 1.93 6.41 0.952 16.8 14.4 825. 95.4 IMDb 183. 1.82 1.63 19.8 236. 55.8 144. Spotify 99.1 5.06 1.10 7.07 90.6 12.7 57.1 News 138. 1.02 1.54 93.1 63.9 31.1 83.9 InstEval 95.3 4.64 0.773 25.5 682. 51.1 92.5 Rossmann 8.22 3.61 1.32 54.6 1.45 53.6 76.3 AUimport 37.7 8.87 0.604 17.7 45.9 107. 175. Wages 19.6 3.61 2.41 43.6 34.2 Table 4: Results when either excluding ('GPBoost E') or including ('GPBoost I') the categorical variables in the fixed effects tree ensemble function for GPBoost. Table 1 : 1 Table1with no prior correlation among random effects. The Rossmann, AUImport, Summary of data sets. n is the number of samples, p is the number of predictor variables (excl. high-cardinality categorical variables), K is the number of high-cardinality categorical variables, and 'Cat. var.' describes the categorical variable(s). Dataset n p K Cat. var. Nb. levels Response var. Airbnb 50K 196 1 host 39K price (log) IMDb 88K 159 2 director 38K avg. movie score movie type 1.7K Spotify 28K 14 4 artist 10K song danceability album 22K playlist 2.3K subgenre 553 News 81K 176 2 source 5.4K nb. shares (log) title 72K InstEval 73K 3 3 student 2.9K teacher rating teacher 1.1K department 14 Rossmann 33K 23 1 store 1.1K total sales (in 100K) AUimport 125K 8 1 commodity 5K total import (log) Wages 28K 52 1 person 4.7K wage (log) Table 2 : 2 Average test mean squared error (MSE) and standard errors in parentheses. Best results are bold. 'Avg. rank' denotes the average rank of a method. 'Avg. rel. dif.' denotes the average relative difference in % of a method compared to the best method. The Wages data set is not included for calculating ranks and relative differences since not all methods were run on it. Dataset Linear LGBM Cat LGBM Num CatBoost GPBoost NN Embed LMMNN Airbnb 0.268 0.131 0.132 0.131 0.125 0.158 0.142 (0.0748) (0.00283) (0.00285) (0.00267) (0.00264) (0.00158) (0.00167) IMDb 1.02 0.954 1.05 0.922 0.850 1.26 0.974 (0.00730) (0.00721) (0.00830) (0.0117) (0.00825) (0.124) (0.00933) Spotify 0.0111 0.00858 0.00880 0.00851 0.00910 0.0164 0.00926 (9.20e-05) (6.23e-05) (5.79e-05) (4.69e-05) (0.000164) (0.000540) (8.34e-05) News 1.88 1.89 2.37 1.85 1.72 1.89 1.81 (0.0146) (0.0147) (0.0157) (0.0160) (0.0131) (0.0215) (0.0190) InstEval 1.44 1.44 1.53 1.46 1.47 1.50 1.45 (0.00672) (0.00746) (0.00549) (0.00711) (0.0230) (0.00669) (0.00492) Rossmann 0.0153 0.00627 0.0124 0.01000 0.00877 0.0516 0.0105 (0.000552) (0.000188) (0.000848) (0.000267) (0.000284) (0.00600) (0.000162) AUimport 0.743 1.26 4.56 2.15 0.651 3.35 0.713 (0.0138) (0.0269) (0.0177) (0.0477) (0.00396) (0.455) (0.00700) Wages 0.321 0.102 0.0992 0.0920 0.0830 (0.0222) (0.00259) (0.00234) (0.00251) (0.00200) Avg. rank 4.71 2.71 5.57 2.86 2.14 6.43 3.57 Avg. rel. dif. 47.6 17.4 111. 44.5 7.02 189. 17.3 Table 3 : 3 Average wall-clock time in seconds. Note that except for the linear model, the wallclock time depends on the chosen tuning parameters. The experiments for 'NN Embed' and 'LMMNN' were run on Google Colab with NVIDIA Tesla V100 GPU machines (see Simchoni and Rosset, 2023) , and all for all other methods, a laptop with an Intel i7-12800H processor was used. Dataset Linear LGBM Cat LGBM Num CatBoost GPBoost E GPBoost I NN Embed LMMNN Airbnb 0.268 0.131 0.132 0.131 0.124 0.125 0.158 0.142 (0.0748) (0.00283) (0.00285) (0.00267) (0.00272) (0.00265) (0.00158) (0.00167) IMDb 1.02 0.954 1.05 0.922 0.883 0.850 1.26 0.974 (0.00730) (0.00721) (0.00830) (0.0117) (0.00837) (0.00825) (0.124) (0.00933) Spotify 0.0111 0.00858 0.00880 0.00851 0.00894 0.00986 0.0164 0.00926 (9.20e-05) (6.23e-05) (5.79e-05) (4.69e-05) (6.99e-05) (0.000227) (0.000540) (8.34e-05) News 1.88 1.89 2.37 1.85 1.78 1.72 1.89 1.81 (0.0146) (0.0147) (0.0157) (0.0160) (0.0143) (0.0131) (0.0215) (0.0190) InstEval 1.44 1.44 1.53 1.46 1.44 1.47 1.50 1.45 (0.00672) (0.00746) (0.00549) (0.00711) (0.00685) (0.0230) (0.00669) (0.00492) Rossmann 0.0153 0.00627 0.0124 0.01000 0.00910 0.00877 0.0516 0.0105 (0.000552) (0.000188) (0.000848) (0.000267) (0.000229) (0.000284) (0.00600) (0.000162) AUimport 0.743 1.26 4.56 2.15 0.714 0.651 3.35 0.713 (0.0138) (0.0269) (0.0177) (0.0477) (0.00550) (0.00396) (0.455) (0.00700) Wages 0.321 0.102 0.0992 0.0920 0.0829 0.0830 (0.0222) (0.00259) (0.00234) (0.00251) (0.00191) (0.00205) Avg. rank 5.71 3.29 6.43 3.71 2.43 2.71 7.43 4.29 Avg. rel. dif. 47.7 17.4 111. 44.5 9.63 8.36 189. 17.3 https://github.com/manifoldai/merf See https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html#optimal-partitioning (retrieved on June 30, 2023) In their online documentation, LightGBM recommends \"For a categorical feature with high cardinality, it often works best to treat the feature as numeric ...\"; see https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#categorical-feature-support (retrieved on June 30, 2023). We clearly come to a different conclusion."
}