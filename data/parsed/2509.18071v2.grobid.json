{
  "title": "Learning functions, operators and dynamical systems with kernels",
  "abstract": "This expository article presents the approach to statistical machine learning based on reproducing kernel Hilbert spaces. The basic framework is introduced for scalar-valued learning and then extended to operator learning. Finally, learning dynamical systems is formulated as a suitable operator learning problem, leveraging Koopman operator theory. The manuscript collects supporting material for the corresponding course taught at the CIME school \"Machine Learning: From Data to Mathematical Understanding\" in Cetraro.",
  "introduction": "Learning from data The most fundamental problem in machine learning is estimating a function f from data, i.e., pairs of inputs and outputs (x 1 , y 1 ), . . . , (x n , y n ). The key intuition is that the function f should give an estimate of the output for any new input, ideally satisfying f (x new ) ≈ y new . The function to be learned can be interpreted as a task and the data as experience from which solving the task can be learned. The term \"learning\" arises from this perspective. This idea is the so-called learning-from-examples paradigm. We illustrate the generality of this approach with three different examples. Example 1 (Regression) Regression is a supervised learning problem where the outputs are scalar-valued. More precisely, consider the following data model, y i = f * (x i ) + ϵ i i = 1, . . . , n. Here, for any i = 1, . . . , n, x i ∈ R d , d ≥ 1, y i ∈ R, f * : R d → R is a unknown function to be learned, and, for any i = 1, . . . , n, ϵ i ∼ N (0, σ 2 ). Both the function f * and the noise are fixed but unknown. Given the data (x i , y i ) n i=1 , the goal is to compute an estimate f of f * . This model is illustrated in Figure 1 . As an example, we can think of x i as a set of features describing a house (e.g., size, number of rooms, location) and y i as its market price. The task is to learn to predict house prices from given features. Example 2 (Operator learning) Operator learning is a supervised learning problem where inputs and outputs belong to Hilbert (or Banach) spaces. More precisely, consider the following model, y i = F * (x i ) + ϵ i i = 1, . . . , n. Here, for any i = 1, . . . , n, x i ∈ X and y i ∈ Y , where X and Y are Hilbert spaces. The function to be learned is a fixed but unknown linear operator F * : X → Y , and for any i = 1, . . . , n, the noise term ϵ i is a zero-mean random variable. Given the data (x i , y i ) n i=1 , the goal is to compute an estimate F of F * . As an example, we can think of y i as images and x i as their blurred, noisy versions. The task is to learn to reconstruct sharp images from their degraded versions. Example 3 (Dynamical systems learning) Dynamical systems learning is a supervised learning setting where the goal is to estimate a state transition function from observed trajectories. More precisely, consider the following model, for some initial condition s 0 , s t+1 = f * (s t , ω t ) t = 0, . . . , T -1. Here, for any t = 0, . . . , T -1, s t ∈ S = R d is the system state, ω t is a random variable modeling stochasticity, and f * : S × Ω → S is a fixed but unknown function describing the state evolution. Given a trajectory (s 0 , s 1 , . . . , s T -1 ), the goal is to compute an estimate f of f * . As an example, we can think of s t as the position and velocity of a robot and ω t as external disturbances, such as sensor noise or environmental factors. The task is to learn to predict the robot's next state based on past observations. The above examples show that Figure (1) provides a simple but potentially misleading illustration. The figure depicts a low-dimensional setting, real-world data can have tens, or even tens of thousands, of dimensions, and datasets can contain millions or even billions of samples. Beyond these examples and observations, ensuring that the learning problem is well-posed requires assumptions about the data. Available data must be informative about new data. As suggested in the previous examples, one possible assumption is the existence of a data-generating process. This is the perspective taken in the framework of statistical learning theory, which we describe next.",
  "body": "Learning from data The most fundamental problem in machine learning is estimating a function f from data, i.e., pairs of inputs and outputs (x 1 , y 1 ), . . . , (x n , y n ). The key intuition is that the function f should give an estimate of the output for any new input, ideally satisfying f (x new ) ≈ y new . The function to be learned can be interpreted as a task and the data as experience from which solving the task can be learned. The term \"learning\" arises from this perspective. This idea is the so-called learning-from-examples paradigm. We illustrate the generality of this approach with three different examples. Example 1 (Regression) Regression is a supervised learning problem where the outputs are scalar-valued. More precisely, consider the following data model, y i = f * (x i ) + ϵ i i = 1, . . . , n. Here, for any i = 1, . . . , n, x i ∈ R d , d ≥ 1, y i ∈ R, f * : R d → R is a unknown function to be learned, and, for any i = 1, . . . , n, ϵ i ∼ N (0, σ 2 ). Both the function f * and the noise are fixed but unknown. Given the data (x i , y i ) n i=1 , the goal is to compute an estimate f of f * . This model is illustrated in Figure 1 . As an example, we can think of x i as a set of features describing a house (e.g., size, number of rooms, location) and y i as its market price. The task is to learn to predict house prices from given features. Example 2 (Operator learning) Operator learning is a supervised learning problem where inputs and outputs belong to Hilbert (or Banach) spaces. More precisely, consider the following model, y i = F * (x i ) + ϵ i i = 1, . . . , n. Here, for any i = 1, . . . , n, x i ∈ X and y i ∈ Y , where X and Y are Hilbert spaces. The function to be learned is a fixed but unknown linear operator F * : X → Y , and for any i = 1, . . . , n, the noise term ϵ i is a zero-mean random variable. Given the data (x i , y i ) n i=1 , the goal is to compute an estimate F of F * . As an example, we can think of y i as images and x i as their blurred, noisy versions. The task is to learn to reconstruct sharp images from their degraded versions. Example 3 (Dynamical systems learning) Dynamical systems learning is a supervised learning setting where the goal is to estimate a state transition function from observed trajectories. More precisely, consider the following model, for some initial condition s 0 , s t+1 = f * (s t , ω t ) t = 0, . . . , T -1. Here, for any t = 0, . . . , T -1, s t ∈ S = R d is the system state, ω t is a random variable modeling stochasticity, and f * : S × Ω → S is a fixed but unknown function describing the state evolution. Given a trajectory (s 0 , s 1 , . . . , s T -1 ), the goal is to compute an estimate f of f * . As an example, we can think of s t as the position and velocity of a robot and ω t as external disturbances, such as sensor noise or environmental factors. The task is to learn to predict the robot's next state based on past observations. The above examples show that Figure (1) provides a simple but potentially misleading illustration. The figure depicts a low-dimensional setting, real-world data can have tens, or even tens of thousands, of dimensions, and datasets can contain millions or even billions of samples. Beyond these examples and observations, ensuring that the learning problem is well-posed requires assumptions about the data. Available data must be informative about new data. As suggested in the previous examples, one possible assumption is the existence of a data-generating process. This is the perspective taken in the framework of statistical learning theory, which we describe next. Statistical Learning In this section, we introduce the problem of supervised machine learning within the framework of statistical learning theory. For the time being we focus on scalar valued learning. The statistical learning problem. Let (X , A) be a measure space with σ -algebra A, and let Y ⊆ R with the corresponding Borel σ -algebra B. Let (X, Y ) be a pair of random variables taking values in (X × Y ) with law P . Let ℓ : Y × Y → [0, ∞) be a given measurable function. Consider M(X , R) ⊂ R X , the space of all measurable functions from X to R. Define L : M(X , R) → [0, ∞) as L(f ) = E[ℓ(Y , f (X))], ∀f ∈ M(X , R). The problem of learning is to solve min f ∈M(X ,R) L(f ), ( 1 ) where P is only known through a sample (x i , y i ) n i=1 of n independent and identically distributed copies of (X, Y ). This framework is referred to as statistical supervised machine learning. Given data (x i , y i ) n i=1 drawn from a distribution, the goal is to find a function with small error on future data sampled from the same distribution. The error is measured pointwise by the loss ℓ, while the expected risk L represents the error over all possible future data. In general, finding a perfect solution is infeasible since the data distribution is unknown; but one can find empirical solutions that become more accurate as more data become available. The problem is supervised since each input x i has a corresponding output y i . Next, we discuss the interpretation of the quantities introduced above and provide examples. Later, we describe how empirical solutions can be found. Data Space and Distribution We next discuss the interpretation and examples of the probability space (X ×Y , A⊗B, P ) where the data reside. Input and output spaces. The space X is called the input space. An example is X ⊆ R d with the corresponding Borel σ -algebra. More generally, X can be a metric space equipped with the corresponding Borel σ -algebra. For example, X could be the space of binary strings {0, 1} d with the Hamming distance d Ham (x, x) = d j=1 1 x j xj , or the simplex {x ∈ R d | d j=1 x j = 1 and x j ≥ 0 for j = 0, . . . , d} with the Hellinger distance d Hel (x, x) = 1 2 d i=1 ( x j -xj ) 2 , or another metric on probability distributions. The space Y ⊆ R is called the output space. A special case is when Y ⊂ R, and in particular, if Y takes only two values, such as {-1, 1}, the learning problem is referred to as binary classification. Data distribution. The distribution P is a probability measure on the space (X × Y , A ⊗ B). The marginal measure P X on (X , A) is defined for all A ∈ A as P X (A) = A×Y dP (x, y). For all x ∈ X , there exists a Borel probability measure P (• |x) on (Y , B), known as the conditional measure at x, such that for all B ∈ B, the function x → P (B |x) is measurable, and for all A ∈ A and B ∈ B, P (A × B) = P (B | x)dP X (x). In binary classification, the conditional distribution reduces to the point mass function {P (1 | x), P (-1 | x)}. The general proof of decomposition in marginal and conditional probability measures is technical, see bibliography. It becomes elementary for discrete random variables and for continuous random variables with a well-defined probability density function. In the latter case, the existence of a well-defined conditional distribution follows from Fubini's theorem. Training set. The set of pairs (x i , y i ) n i=1 is referred to as the training set. The acronym i.i.d. (independent and identically distributed) is commonly used to describe such samples. While the i.i.d. assumption is strong, it is the standard one used to derive fundamental results. Loss Function, Expected Risk, and Target Function Since we consider deterministic functions within a probabilistic setting, we must account for possible errors. This motivates the introduction of the loss function and the corresponding expected loss (risk). Loss function. The function ℓ is called the loss function. It can be viewed as a pointwise error measure for any pair of outputs. In particular, ℓ(y, f (x)) represents the error incurred when predicting f (x) instead of y. It is often assumed to be continuous and convex in the second argument. The primary loss function we consider is the least squares loss ℓ(y, y ′ ) = (yy ′ ) 2 for all y, y ′ ∈ Y . More generally, in regression, loss functions take the form ℓ(y, y ′ ) = V (yy ′ ) for all y, y ′ ∈ R, where V : R → [0, ∞). An example, besides the square loss, is the absolute value loss V (a) = |a|, a ∈ R. In binary classification, loss functions are of the form ℓ(y, y ′ ) = V (yy ′ ) for all y, y ′ ∈ R, with V : R → [0, ∞). Examples include the misclassification loss V (a) = 1 a<0 , the square loss V (a) = (1a) 2 , the logistic loss V (a) = log(1 + e -a ), the exponential loss V (a) = e -a , and the hinge loss V (a) = |1 -a| + = max{0, 1 -a}, for a ∈ R. Expected risk and target function. For all f ∈ M, the functional L : M(X , R) → [0, ∞) defined as L(f ) = E[ℓ(Y , f (X))] = ℓ(y, f (x))dP (x, y) (2) is called the expected loss or expected risk. Assume that there exists f P ∈ M(X , R) such that L(f P ) = min f ∈M(X ,R) L(f ), then f P is called a target function. The expected risk can be interpreted as the error over all possible input-output pairs as measured by the loss function, where errors are weighted more for pairs that are more likely to be sampled. The target function can be seen as an ideal solution achieving the best possible error on all future data. In practice, only empirical approximations of the target function can be computed. However, as describe next, analytic expressions can be derived in terms of the underlying probability distribution, providing insights into the problem. Inner risk and target function. For almost all x ∈ X , let L x : R → [0, ∞) be defined as L x (a) = ℓ(y, a)dP (y | x) for all a ∈ R. The function L x is referred to as the inner risk at x. Note that, for all f ∈ M(X , R), L(f ) = ℓ(y, f (x))dP (x, y) =       ℓ(y, f (x))dP (y|x)       dP X (x) = L x (f (x))dP X (x). The inner risk is useful for characterizing the minimizers of the risk analytically. Indeed, assume that, for almost all x ∈ X , there exists a x ∈ R such that L x (a x ) = min a∈R L x (a). (3) Then, defining f P (x) = a x , (4) for almost all x ∈ X , it is possible to show that f P ∈ M(X , R) and that f P is a target function. It is an exercise to prove that f P as in Equation ( 4 ) is indeed a target function. Proving that it is measurable relies on technical facts in measure theory, see bibliography. We illustrate the usefulness of the above results by deriving the target function for the square loss. Example 4 (Regression function) The target function for the square loss is called the regression function and is given, for almost all x ∈ X , by f P (x) = ydP (y | x), that is, the conditional mean of y given x. To see this, recalling (3) and (4), we verify that f P (x) = arg min a∈R (y -a) 2 dP (y|x) = ydP (y | x). Analogous computations can be considered for other loss functions and are left to the interested reader. Next, we move on to discuss the main approach to derive empirical solutions, namely empirical risk minimization. Empirical risk minimization and hypothesis spaces The basic intuition behind empirical risk minimization (ERM) is to take the problem in Equation ( 1 ), replace the expectation with an average over the available data, i.e., the empirical risk, and restrict the space of candidate functions from M(X , R) to a smaller space, allowing for efficient computations. Empirical risk. Given a training set (x i , y i ) n i=1 and a loss function ℓ, define L : M(X , R) → [0, ∞) by L(f ) = 1 n n i=1 ℓ(y i , f (x i )). ( 5 ) The functional L is referred to as the empirical risk, also known as the training error. Hypothesis space. Let H ⊂ M(X , R) denote a subspace of candidate functions (hypotheses) from which a solution is selected. H is referred to as the hypothesis space. Empirical risk minimization (ERM). Consider the problem min f ∈H L(f ). In general, both the computation of a solution and its properties depend on the chosen hypothesis space H. Next, we discuss some basic examples. Examples of hypothesis spaces We provide some basic examples of hypothesis spaces. Example 5 (Linear functions) Let X = R d and H = {f : X → R | ∃w ∈ R d s.t. f (x) = w ⊤ x, ∀x ∈ X }. (6) Example 6 (Dictionaries of features) For j = 1, . . . , p, let φ j : X → R, and define H = {f : X → R | ∃w ∈ R p s.t. f (x) = p j=1 w j φ j (x), ∀x ∈ X }. (7) The functions φ j are often referred to as atoms or features, and their collection is called a dictionary. Example 7 (Neural networks) Let X = R d , σ : R → R, and define H = {f : X → R | ∃β ∈ R p , w j ∈ R d , j = 1, . . . , p, s.t. f (x) = p j=1 β j σ (w ⊤ j x), ∀x ∈ X }. The function σ is called the activation function. For example, σ (a) = |a| + is known as the rectified linear unit (ReLU), while σ (a) = (1+e -a ) -1 is called the sigmoid function. A linear neural network corresponds to σ being the identity function. The vectors w j define the so-called hidden units σ (w ⊤ j •), and p is the number of hidden units. Often, additional offsets b j ∈ R, j = 1, . . . , p are considered, in which case the functional expression becomes f (x) = p j=1 β j σ (w ⊤ j x + b j ). Let W : R d → R p be the matrix with rows w 1 , . . . , w p , we can also write f (x) = β ⊤ σ (W x), where, by abuse of notation, the activation function applied to a vector is understood to act component-wise. If offsets are considered, then the matrix W is replaced by a suitable affine map. Example 8 (Deep neural networks) Let X = R d , σ : R → R. Let L ∈ N, and for each ℓ = 0, . . . , L -1, let d ℓ ∈ N with d 0 = d. Define H = {f : X → R | ∃β ∈ R d L-1 , W ℓ ∈ R d ℓ ×d ℓ-1 , ℓ = 1, . . . , L -1, s.t. f (x) = β ⊤ σ (W L-1 . . . W 2 σ (W 1 x)), ∀x ∈ X }. ( 8 ) The above function space describes deep neural networks (DNNs), also called multi-layer perceptrons or multilayer networks. Each matrix W ℓ corresponds to a hidden layer, and L represents the total number of layers. We conclude with two remarks. First, we note that, aside from linear functions, all the above examples are spaces of nonlinear functions. However, like linear functions, feature-based functions are linearly parameterized, whereas neural networks are nonlinearly parameterized. Second, all the examples above describe functions defined by a finite number of parameters. Next, we introduce reproducing kernel Hilbert spaces, which allow us to consider infinitely many parameters, and discuss the corresponding ERM problem. Bibliography Comprehensive treatments of statistical learning theory and empirical risk minimization are available in [25, 7, 11] , as well as [6, 5] . A standard reference on probability theory is [8] . Regularized ERM in reproducing kernel Hilbert spaces Kernel methods for supervised machine learning rely on ERM in a reproducing kernel Hilbert space (RKHS), a general class of possibly infinite-dimensional function spaces. Reproducing kernel Hilbert spaces A reproducing kernel Hilbert space (RKHS) can be defined in several different yet equivalent ways, as discussed next. Evaluation functionals and RKHS. Let X be a set. A reproducing kernel Hilbert space H ⊂ R X is a Hilbert space with inner product ⟨•, •⟩ H , such that for all x ∈ X , the evaluation functionals e x : H → R defined by e x (f ) = f (x) (9) are linear and continuous. Note that this implies, in particular, that |f (x)| ≲ ∥f ∥ H . We add two remarks. Remark 1 (The space of continuous functions is not an RKHS) Let X = R d , and consider C(X ) ⊂ R X , the space of continuous functions with the sup norm ∥f ∥ ∞ = sup x∈X |f (x)|, ∀f ∈ C(X ). Indeed, for all f ∈ C(X ) and for all x ∈ X , we have |f (x)| ≤ ∥f ∥ ∞ . However, C(X ) is not a Hilbert space, since the sup norm is not induced by an inner product. To verify this, we check that ∥f ∥ ∞ violates the parallelogram law. Remark 2 (The space of square-integrable functions is not an RKHS) Consider the Hilbert space L 2 (R d ) = {f : R d → R | ∥f ∥ 2 L 2 = |f (x)| 2 dx < ∞}, with inner product f , f ′ L 2 = f (x)f ′ (x)dx. Since the norm ∥f ∥ L 2 is only defined up to sets of measure zero, it does not control the value of f at every x. Hence, L 2 is not an RKHS. The definition in terms of evaluation functionals highlights the generality of the notion of RKHS. An equivalent definition reveals additional properties. RKHS and reproducing kernels. A reproducing kernel Hilbert space H ⊂ R X is a Hilbert space with inner product ⟨•, •⟩ H , such that there exists a function k : X × X → R, called the reproducing kernel, satisfying the following properties: • ∀x ∈ X , k x = k(x, •) ∈ H, (10) • ∀f ∈ H, ∀x ∈ X , f (x) = k x , f H . ( 11 ) The latter condition is called the reproducing property of the kernel. The following remark discusses the equivalence of the two above definitions. Remark 3 (Evaluation functionals and reproducing kernel) The existence of a reproducing kernel follows from the linearity and continuity of the evaluation functionals via the Riesz Representation Theorem. Conversely, the linearity and continuity of the evaluation functionals follow directly from Condition (10) and the reproducing property. Remark 4 (Regularity properties of an RKHS) It can be shown that the regularity properties of functions in an RKHS-e.g. measurability, continuity, or differentiability-are inherited from the corresponding kernel. We do not develop this discussion here. Another notion relevant for RKHS is that of a positive definite function, which leads to yet another equivalent definition. RKHS and positive definite kernels. Recall that a function k : X × X → R is called positive definite if, for all x 1 , . . . , x N ∈ X , given N ∈ N and c 1 , . . . , c N ∈ R, N i,j=1 k(x i , x j )c i c j ≥ 0. It is easy to see that every reproducing kernel k is symmetric and positive definite. Symmetry is straightforward, whereas positive definiteness can be proved using the reproducing property by noting that for all x i , x j with i, j = 1, . . . , N , we have k( x i , x j ) = k x i , k x j H , so that N i,j=1 k(x i , x j )c i c j = N i=1 c i k x i , N j=1 c j k x j H = N i=1 c i k x i 2 H ≥ 0. The converse of the above observation is known as the Moore-Aronszajn theorem. Remark 5 (Moore-Aronszajn theorem) It can be shown that given a symmetric and positive definite kernel k, there exists a unique RKHS associated with it, for which k is the reproducing kernel. The proof is constructive and is based on introducing the pre-Hilbert space H 0 = {f : X → R | ∃N ∈ N, c 1 , . . . , c N ∈ R, x 1 , . . . , x N ∈ X , s.t. f = N i=1 c i k x i } endowed with the scalar product f , f ′ H 0 = N i=1 N ′ j=1 k(x i , x ′ j )c i c ′ j , ∀f , f ′ ∈ H 0 . The inner product above can be shown to be well defined and independent of the choice of function representation. Then, the completion H of H 0 is a Hilbert space, and it can be easily verified that it is indeed an RKHS with reproducing kernel k. Finally, RKHSs are related to feature maps. RKHS and Feature Maps Let F be a Hilbert space with inner product ⟨•, •⟩ F . A feature map is a function Φ : X → F that embeds input points into F . Every reproducing kernel k : X × X → R defines a canonical feature map by setting F = H and Φ(x) = k x , ∀x ∈ X . ( 12 ) However, the same RKHS can define multiple feature maps. For example, given any orthonormal basis (a j ) j of H, we can set F = ℓ 2 and define Φ(x) = (a j (x)) j , ∀x ∈ X . ( 13 ) In turn, any feature map defines a corresponding RKHS. Indeed, given any Φ : X → F , define H Φ = {f : X → R | ∃w ∈ F s.t. f (x) = ⟨w, Φ(x)⟩ F , ∀x ∈ X }. ( 14 ) The norm in H Φ is given for all f ∈ H Φ by ∥f ∥ H Φ = inf{∥w∥ F | w ∈ F s.t. f (x) = ⟨w, Φ(x)⟩ F , ∀x ∈ X }. This space consists of functions that are linear in the feature representation Φ(x). Since Φ might not be injective, the correspondence between f ∈ H Φ and w ∈ F is not one-to-one, which justifies the infimum in the norm definition. It is possible to show that the space H Φ is an RKHS with reproducing kernel k(x, x ′ ) = Φ(x), Φ(x ′ ) F . ( 15 ) This proof, which is somewhat technical, is omitted. The above discussion shows how a RKHS can be defined taking many different perspectives. Before, discussing some examples we add a remark showing how kernels can be combined to build new ones. Remark 6 (Closure Properties of RKHSs Under Sum and Product of Kernels) Given two reproducing kernels k 1 , k 2 : X × X → R with corresponding RKHSs H 1 and H 2 , their sum and product define valid RKHSs. The sum k + (x, x ′ ) = k 1 (x, x ′ ) + k 2 (x, x ′ ) ( 16 ) is a reproducing kernel. The associated RKHS H + consists of functions f = f 1 + f 2 with f 1 ∈ H 1 and f 2 ∈ H 2 . The norm in H + is given by ∥f ∥ H + = inf{ ∥f 1 ∥ 2 H 1 + ∥f 2 ∥ 2 H 2 | f = f 1 + f 2 }. ( 17 ) The pointwise product of kernels, k × (x, x ′ ) = k 1 (x, x ′ ) k 2 (x, x ′ ), defines a reproducing kernel Hilbert space H × with kernel k × . The space H × contains functions that can be written as pointwise products f 1 f 2 with f 1 ∈ H 1 and f 2 ∈ H 2 , and in fact includes the algebraic span of such products. In general, H × is continuously embedded in the tensor product RKHS H 1 ⊗ H 2 . Examples of RKHSs We next discuss some basic examples of RKHSs and their corresponding reproducing kernels. Example 9 (Linear kernel) Let (X , ⟨•, •⟩ X ) be a real separable Hilbert space. In particular, we could take X = R d . For all x, x ′ ∈ X , the linear kernel is k(x, x ′ ) = ⟨x, x ′ ⟩ X . Then for all f ∈ H, there exists a unique w ∈ X such that f (x) = ⟨w, x⟩ X and ∥f ∥ H = ∥w∥ X . To see this, note that from Equation (10) , for all x ∈ X , k x ∈ H. Since for the linear kernel, k x (•) = ⟨x, •⟩ X ∈ L(X , R) = X * , then X * = H 0 ⊂ H. Thus, we can write H 0 = {f ∈ H | ∃!w ∈ X s.t. f (•) = ⟨w, •⟩ X }. Moreover, for all f , f ′ ∈ H 0 , f , f ′ H = ⟨w, •⟩ X , w ′ , • X H = w, w ′ X , so that ∥f ∥ H = ∥w∥ X . Since H 0 is a closed subspace, we have the decomposition H = H 0 + H ⊥ 0 . However, H ⊥ 0 = {0}, since by Equation (11), if h ∈ H ⊥ 0 , then for all x ∈ X , ⟨h, ⟨x, •⟩ X ⟩ H = h(x) = 0, which implies h = 0. Thus, H 0 = H, and the proof is finished. Example 10 (Gaussian kernel) Let X = R d and, for γ > 0, consider the Gaussian kernel k(x, x ′ ) = e -γ∥x-x ′ ∥ 2 for x, x ′ ∈ X . Then, for f ∈ H ⊂ L 2 (R d ), the RKHS norm satisfies ∥f ∥ 2 H ∝ R d | f (ω)| 2 e ∥ω∥ 2 γ dω, where f denotes the Fourier transform of f . In particular, functions in H must have rapidly decaying Fourier transforms. Example 11 (Random features and integral representation kernel) Let (B, π) be a probability space and define L 2 π = {g : B → R | ∥g∥ 2 π = |g(β)| 2 dπ(β) < ∞}. Let ψ : B × X → R be a measurable function such that for almost all x ∈ X , ψ(•, x) ∈ L 2 π . Then, for all x, x ′ ∈ X , define k(x, x ′ ) = ψ(β, x)ψ(β, x ′ )dπ(β). ( 18 ) Next, consider (β i ) M i=1 ∼ π M and define k M (x, x ′ ) = 1 M M i=1 ψ(β i , x)ψ(β i , x ′ ). The kernel k M is called the random features kernel. Regularized ERM in RKHS Let H be a RKHS. Recalling the definition of the empirical risk (5), for each λ > 0, define the regularized empirical risk L λ : H → [0, ∞), such that for all f ∈ H, L λ (f ) = L(f ) + λ ∥f ∥ 2 H . ( 19 ) Then, consider the regularized empirical risk minimization problem, min f ∈H L λ (f ). ( 20 ) The functional L λ is also called the Tikhonov functional; λ is the regularization parameter, and ∥•∥ 2 H is the regularizer. If for all y ∈ Y , ℓ(y, •) is continuous and convex, then for all λ > 0, L λ is continuous, strictly convex, and coercive, so that there exists a unique minimizer f λ ∈ H. Next, we discuss how it can be computed considering the square loss. Kernel ridge regression. Let ℓ(y, y ′ ) = (yy ′ ) 2 , for all y, y ′ ∈ R. Then, the regularized ERM problem in RKHS is called kernel ridge regression (KRR). If X = R d and k is the linear kernel, the method is called ridge regression (RR). It is useful to rewrite problem (20) to highlight its connection to linear inverse problems. Towards this end, we introduce the sampling and extension operators defined by the kernel and the data. Sampling and extension operators. Endow R n with the inner product a, a ′ n = 1 n n i=1 a i a ′ i , for a, a ′ ∈ R n . Given an RKHS H ⊂ R X with reproducing kernel k and a set of n input points x 1 , . . . , x n ∈ X , define S : H → R n for all f ∈ H by Sf = f , k x 1 H , . . . , f , k x n H . ( 21 ) Note that S is linear and bounded. Moreover, the adjoint S * : R n → H satisfies for c = (c 1 , . . . , c n ) ∈ R n . S * c = 1 n n i=1 c i k x i . We refer to S and S * as the sampling and extension operators, respectively. The former name follows from the fact that Sf = (f (x 1 ), . . . , f (x n )). The latter is explained by considering c = (g(x 1 ), . . . , g(x n )), the values of a function g at x 1 , . . . , x n . Then, for all x ∈ X , ( S * c)(x) = n i=1 g(x i )k(x i , x), which can be interpreted as extending the values of g to any other point through an averaging process performed via the kernel. KRR and inverse problems. Given a training set of n points, and a kernel k, let y = (y 1 , . . . , y n ) ∈ R n . Then for all λ > 0, and f ∈ H L λ (f ) = Sf -y 2 n + λ ∥f ∥ 2 H . ( 22 ) The minimization of the above functional is the regularized least squares problem associated to the linear inverse problem Sf = y. Then, taking the functional derivative w.r.t. f in (22) and setting it to zero we get that f λ = ( S * S + λI) -1 S * y. ( 23 ) Next we show how the above expression directly leads to computable quantities. Representer theorem for KRR. The solution f λ admits an alternative representation, f λ = S * ( S S * + λI) -1 y. ( 24 ) The above expression can be derived in multiple ways, such as by direct manipulation using the Woodbury matrix identity or the singular value decomposition of S. This derivation is left as an exercise. Here, we note that Equation ( 24 ) can be further developed to show that for all x ∈ X , f λ (x) = n i=1 k(x, x i ) c i , c = ( K + nλI) -1 y, ( 25 ) where c ∈ R n , K = n S S * is called the empirical kernel matrix and is symmetric, positive semi-definite, and such that K ij = k(x i , x j ) for all i, j = 1, . . . , n. To derive (25) , note that from Equation (24) , f λ = S * c = 1 n n i=1 k x i c i , where c = ( c 1 , . . . , c n ) ∈ R n satisfies c = ( S S * + λI) -1 y = K n + λI -1 y. Equation ( 25 ) then follows by factoring out n. The above expression shows that the kernel ridge regression (KRR) solution can be computed by solving a finite-dimensional linear system, even though the corresponding RKHS is infinite-dimensional. While feasible the above computations can become cumbersome for large kernel matrices. In the next section, we discuss how efficiency can be improved through approximate computations. Nyström approximation For M ≤ n, let { x 1 , . . . , x M } ⊂ {x 1 , . . . , x n }. The points ( x j ) M j=1 are called Nyström centers or inducing points. Let Z : H → R M be such that for all f ∈ H, Zf = f , k x 1 H , . . . , f , k x m H . Then, Z is linear and bounded. The adjoint Z * : R M → H is such that for a = (a 1 , . . . , a M ) ∈ R M , Z * a = M i=1 k x i a i . Let H be the subspace of H defined as H = {f ∈ H | f = Z * a, a ∈ R M }. Consider the regularized ERM problem on H, given by min f ∈ H L λ (f ). ( 26 ) We refer to this problem as the Nyström KRR. Note that this minimization problem is defined by a strongly convex and coercive functional, so there is a unique solution f λ ∈ H such that L λ ( f λ ) = min f ∈ H L λ (f ). For the square loss, we have that for all f ∈ H, L λ (f ) = S Z * a -y 2 n + λ a, Z Z * a R M , so that problem (26) is equivalent to min a∈R M S Z * a -y 2 n + λ a, Z Z * a R M . By a direct computation, the solution of the above problem is a = ( K ⊤ nM K nM + λn K MM ) -1 K ⊤ nM y, ( 27 ) where K nM = S Z * is the matrix with entries ( K nM ) ij = k(x i , x j ) for i = 1, . . . , n, j = 1, . . . , M, and K MM = Z Z * is the matrix with entries ( K MM ) ij = k( x i , x j ) for i, j = 1, . . . , M. It is easy to see that for all x ∈ X , f λ (x) = M i=1 k( x i , x) a i . ( 28 ) K K MM K nM Figure 2: The Nyström approximation greatly reduces the size of the kernel matrices involved in solving the learning problem. Instead of K, only K nM and K MM are needed. Remark 7 (Computational costs) Equations ( 27 ) and (28) show that an efficient approximate KRR solution can be computed using a small number of Nyström centers. Indeed, the cost for computing the exact KRR solution is O(n 3 ) in time and O(n 2 ) in space. The cost for computing the Nyström KRR solution is O(nM 2 + M 3 ) in time and O(nM) in space. Remark 8 (Column subsampling) As already, via the representer theorem, KRR reduces to solving a linear system, ( K + λnI)c = y, which can be seen as a regularized version of the linear system, Kc = y. Analogously, Nyström approximation can be seen to correspond to a regularized version of linear system K nM c = y. From this perspective Nyström approximation corresponds to so called column subsampling, a technique from randomized numerical linear algebra. See Figure 2 for a pictorial representation. Remark 9 (Regularization by projection) Nyström approximations can also be related to so called projection regularization methods. These methods naturally arise when the discretization of possible infinite dimensional problems need be considered. Here, the idea is to consider a subspace H and the corresponding orthogonal projection P : H → H. Then, the following regularized problem min f ∈H S P f -y 2 n + λ ∥f ∥ 2 H , can be shown to be equivalent to Problem (26) . This perspective suggests more general ways to choose a subspace H than the one discussed before. Remark 10 (Approaches to select the Nyström centers) The Nyström centers are often sampled at random from the training set. The simplest idea is to sample uniformly without replacement, but other strategies can be considered. Deterministic choices are also possible and are closely related to quadrature methods. Bibliography A classic reference for reproducing kernel hilbert spaces is [1] and the study of regularized learning problem in RKHS originate in a number of foundational contributions in the '60s see e.g. [27] and references therein. Ridge regression was proposed in [12] in the context of statistics, and is called Tikhonov regularization in inverse problems [24, 9] . It is a form of empirical risk minimization in statistical learning theory [25] . The formulation in terms of sampling operators was introduced in [26] and [22] . The Nyström approximation was introduced in [28] and [23] under the name of sparse greedy approximation, and its statistical learning properties were proved in [19] and [20] who also introduced a computationally efficient iterative algorithm, with connections to randomized linear algebra [15] . The use of General projection regularization was first considered in [9] . Learning operators Instead of Y = R, consider (Y , ⟨•, •⟩ Y ) as a real separable Hilbert space. The statistical learning problem naturally extends to this setting. Moreover, the approach based on kernels also generalizes provided a suitable notion of vector valued reproducing kernel Hilbert spaces is introduced. Vector valued statistical learning Let (X , A) be a measure space with σ -algebra A, and let (Y , ⟨•, •⟩ Y ) be a real separable Hilbert space with its corresponding Borel σ -algebra B Y . Let (X, Y ) be a pair of random variables taking values in (X × Y ) with law P . Let ℓ : Y × Y → [0, ∞) be a given measurable function. Consider M(X , Y ) ⊂ Y X , the space of all measurable functions from X to Y . Define L : M(X , Y ) → [0, ∞) as L(f ) = E[ℓ(Y , f (X))], ∀f ∈ M(X , Y ). The problem of learning is to solve min f ∈M(X ,Y ) L(f ), ( 29 ) where P is only known through a sample (x i , y i ) n i=1 of n independent and identically distributed copies of (X, Y ). Our primary example for the loss function ℓ is the squared loss, ℓ(y, y ′ ) = y -y ′ 2 Y , ∀y, y ′ ∈ Y . The case where Y = R T is a special case of this formulation. Vector valued RKHS The definition of vector valued reproducing kernel Hilbert space (vvRKHS) extend naturally to Hilbert spaces valued functions. In the following, we let (Y , ⟨•, •⟩ Y ) be a real separable Hilbert space. Evaluation functionals and RKHS. Let X be a set. A vector valued reproducing kernel Hilbert space H ⊂ Y X is a Hilbert space with inner product ⟨•, •⟩ H , such that for all x ∈ X , the evaluation operators e x : H → Y defined for all f ∈ H by e x (f ) = f (x) (30) are linear and continuous. Note that this implies, in particular, that for all x ∈ X and f ∈ H ∥f (x)∥ Y ≲ ∥f ∥ H . Similarly, an equivalent definition can be given in terms of reproducing kernels. However, for vector valued RKHS, reproducing kernels are operator valued. RKHS and reproducing kernels. A vector valued reproducing kernel Hilbert space H ⊂ R X is a Hilbert space with inner product ⟨•, •⟩ H , such that there exists Γ : X × X → L(Y ), called the operator valued reproducing kernel satisfying the following properties: • ∀x ∈ X ,∀y ∈ Y , Γ (x, •)y ∈ H, (31) • ∀f ∈ H, ∀x ∈ X , ∀y ∈ Y , Γ (x, •)y, f H = y, f (x) Y . ( 32 ) The latter condition is analogous to the reproducing property in the scalar valued setting. Remark 11 (Evaluation operator) In view of the above definition, we define the linear operator Γ x : Y → H such that Γ x = Γ (x, •). Then, the corresponding adjoint operator Γ * x : H → Y satisfies Γ * x f = f (x) by the reproducing property. Hence, Γ * x provides a representation of the evaluation operator in Equation (30). As in the scalar case, the two definitions above can once again be related through an extension of the Moore-Aronszajn theorem and an application of the Riesz representation theorem. Similarly, vvRKHSs can be characterized in terms of positive definite functions and feature maps, but we omit these developments here. Instead, we provide some relevant examples of operator-valued kernels. Example 12 (Operator valued linear kernel) Let (X , ⟨•, •⟩ X ) be a real separable Hilbert space and let Γ (x, x ′ ) = ⟨x, x ′ ⟩ X I Y , for x, x ′ ∈ X , where I Y :Y → Y is the identity operator. Then for all f ∈ H, there exists a unique W ∈ L 2 (X , Y ) such that for all x ∈ X , f (x) = W x and ∥f ∥ H = ∥W ∥ L 2 (X ,Y ) . Example 13 (Separable kernels I) Let X be a set and H k ⊂ R X a RKHS with corresponding scalar valued reproducing kernel k : X × X → R. Let Γ (x, x ′ ) = k(x, x ′ )I Y , for x, x ′ ∈ X , where I Y :Y → Y is the identity operator. Then for all f ∈ H, there exists a unique W ∈ L 2 (H k , Y ) such that for all x ∈ X , f (x) = W k x and ∥f ∥ H = ∥W ∥ L 2 (H k ,Y ) . We call these kernels separable, since the contribution of inputs and output space to the operator valued kernel is factorized. Example 14 (Separable kernels II) The above examples can be further developed considering operator valued kernels of the form Γ (x, x ′ ) = k(x, x ′ )A, for x, x ′ ∈ X , where A : Y → Y is a positive semi-definite operator. Example 15 (Integral representation and random features for operator valued kernels) Let (B, π) be a probability space and L 2 π (B, Y ) = {g : B → Y | ∥g∥ 2 π = |g(β)| 2 dπ(β) < ∞}. Then, let ψ : B × X → Y a measurable function such that for almost all x ∈ X , ψ(•, x) ∈ L 2 π (B, Y ). Then, for all x, x ′ ∈ X , let Γ (x, x ′ ) = ψ(β, x) ⊗ ψ(β, x ′ )dπ(β). Next, consider (β i ) M i=1 ∼ π M and let Γ M (x, x ′ ) = 1 M M i=1 ψ(β i , x) ⊗ ψ(β i , x ′ ). The latter kernel is called a operator valued random features kernel. Next, we discuss ERM in a vvRKHS. Regularized ERM in vvRKHS The (regularized) ERM approach seamlessly extends to functions with values in a Hilbert space. Vector valued Kernel ridge regression. Let ℓ(y, y ′ ) = yy ′ 2 Y , for y, y ′ ∈ Y , and consider the extension of Equations ( 19 ) and (20) . In this case, we call the regularized ERM in RKHS vector-valued kernel ridge regression (vvKRR). We now develop a discussion analogous to the one in the scalar setting. Sampling and extension operators. Given an operator valued reproducing kernel Γ with RKHS H, for all x ∈ X , let Γ x : Y → H be the linear bounded operator such that Γ * x : H → Y is given by Γ * x f = f (x) for f ∈ H, x ∈ X . These operators are well defined in view of Conditions (31), (32). Also, note that for all x, x ′ ∈ X , Γ * x Γ x ′ = Γ (x, x ′ ). Let Y n = ⊕ n i=1 Y , with the normalized inner product ⟨a, a ′ ⟩ Y n = 1 n n i=1 a i , a ′ i Y for a, a ∈ Y n . Given a training set, let x 1 , . . . , x n ∈ X be the corresponding input points. Define the sampling operator for vvRKH spaces as S : H → Y n , such that for all f ∈ H, Sf = Γ * x 1 f , . . . , Γ * x n f . Then, S is linear and bounded. Moreover, the corresponding extension operator is the adjoint S * : Y n → H such that for a = (a 1 , . . . , a n ) ∈ Y n S * a = 1 n n i=1 Γ x i a i . vvKRR solution and representer theorem. Given a training set, and an operator valued kernel Γ , let y = (y 1 , . . . , y n ) ∈ Y n . Then for all λ > 0 L λ (f ) = Sf -y 2 Y n + λ ∥f ∥ 2 H , f ∈ H. The same computations done for scalar functions show that f λ = ( S * S + λI Y ) -1 S * y = S * ( S S * + λI Y ) -1 y. Further, we can also write for all x ∈ X , f λ (x) = n i=1 Γ (x, x i ) c i , c = ( Γ + λnI Y ) -1 y, ( 33 ) where c = ( c 1 , . . . , c n ) ∈ Y n , Γ = n S S * is the corresponding empirical kernel operator. Example 16 (Computations with operator valued linear kernels) Let (X , ⟨•, •⟩ X ) be a real separable Hilbert space and H ⊂ Y X a vvRKHS with reproducing kernel Γ . If Γ = ⟨•, •⟩ X I Y , then the vvKRR problem can be written as, min W ∈L 2 (X ,Y ) 1 n Y -XW * 2 L 2 (Y ,R n ) + λ ∥W ∥ 2 L 2 (X ,Y ) , where Y : Y → R n and X : X → R n and the corresponding solution is W = Y * X( X * X + λIn) -1 Remark 12 (Discretization) The above computations in general need some discretization to be performed. For example, considering X d ⊂ X , Y T ⊂ Y subspaces of dimensions d and T , respectively. Remark 13 (Finite dimensional output spaces) Assume (X , ⟨•, •⟩ H ) and Y = R T . In particular, consider the operator (matrix) valued linear kernel Γ (x, x ′ ) = ⟨x, x ′ ⟩ X I Y . In this case, the empirical kernel operator can be identified with a nT × nT block diagonal matrix. Note that an analogous observation holds for separable kernels. Bibliography Vector valued RKHSs were introduced in [21] and we refer to [4] for more recent results and further references. Their potential use in machine learning was highlighted in [18] , while [3] proved the first results on vector valued KRR. We refer to [14] for more recent results and further references in the context of operator learning. Learning dynamical systems We next describe how discrete time stochastic dynamical systems can be learned using ideas from operator learning, by leveraging the Koopman operator theory. Dynamical systems and Markov processes The term dynamical system broadly refers to a quantity that evolves over time. For quantities represented by vectors in X ⊆ R d , a discrete-time evolution can be described by the iterative map x t+1 = f (x t ), for a given initial condition x 0 . Here, X is called the state space, and its elements are called states, while the function f : X → X is called the evolution function. Such dynamical systems are called autonomous because the evolution function does not depend on time. In many situations, it is useful to consider stochastic dynamics, given by x t+1 = f (x t , ω t ), (34) where the initial state x 0 is drawn from a given initial distribution ρ 0 on X . Here, (ω t ) t∈N are i.i.d. samples in some probability space (Ω, A, P), and encode the stochastic nature of the evolution. Stochasticity may arise as a perturbation to an underlying deterministic dynamics, or be intrinsic to the evolution itself. Stochastic dynamical systems can be equivalently described in terms of Markov processes, as described next. Markov processes. Let (X t ) t∈N be a stochastic process with values in a measurable space (X , A). Assume that for all measurable set A ∈ A and for every t ∈ N, P(X t+1 ∈ A | X t , . . . , X 1 ) = P(X t+1 ∈ A | X t ). (35) Then, (X t ) t∈N is called a Markov process. The Markov process is called time-homogeneous if the conditional probability in Equation ( 35 ) is the same for all t ∈ N. In this case, there exists a transition kernel p : X × A → [0, 1] such that for all t ∈ N, p(x, A) = P(X t+1 ∈ A | X t = x), for all x ∈ X and all A ∈ A. Conversely, given a transition kernel p and an initial distribution ρ 0 , a Markov process can be defined letting X 0 ∼ ρ 0 and for all x ∈ X , A ∈ A, P(X t+1 ∈ A | X t = x) = p(x, A). Remark 14 (Stochastic dynamical systems and Markov processes) It can be shown that every stochastic dynamical system (34) defines a Markov process and conversely, any Markov process can be realized by a stochastic dynamical system (34). Roughly speaking, the autonomous nature of the system and the i.i.d. nature of the samples in (34) translate into the Markovianity of the process. An important notion associated with a Markov process with transition kernel p is that of an invariant measure, which is a probability measure π on X such that π(A) = X p(x, A) dπ(x), ∀A ∈ A. In general, we cannot expect a Markov process to have an invariant measure, but existence is ensured in a number of relevant cases. A sufficient condition is given in the following remark. Remark 15 (Positive Harris recurrence and invariant measures) Let (X t ) t∈N be a Markov process with transition kernel p. For A ∈ A, define the return time to A as τ A = inf{t ≥ 1 : X t ∈ A}. The process is called Harris recurrent if there exists a σ -finite measure µ on X such that for all measurable A ⊆ X with µ(A) > 0 and all x ∈ X , P x (τ A < ∞) = 1. The process is positive Harris recurrent if it is Harris recurrent and there exists a set A with µ(A) > 0 such that E x [τ A ] < ∞, ∀x ∈ A. Under these conditions, the process admits a unique invariant probability measure π. Another important notion is time reversibility, characterized by the so-called detailed balance condition p(x, dx ′ )dπ(x) = p(x ′ , dx)dπ(x ′ ). (36) More precisely, the above shorthand notation means that, for all measurable sets A, B ⊆ X , A p(x, B)dπ(x) = B p(x ′ , A)dπ(x ′ ). Intuitively, this means that the Markov process behaves the same way forward and backward in time. Finally, it is useful to recall that linear dynamical systems have special properties that greatly simplify their study. Remark 16 (Linear dynamical system) Let A ∈ R d×d , and consider the dynamical system x t+1 = Ax t , ( 37 ) for some initial condition x 0 . The study of the system dynamics can be greatly simplified if A admits a spectral decomposition A = d j=1 λ j ψ j ⊗ ψ j , ( 38 ) for some suitable eigenvalues λ 1 , . . . , λ d ∈ R and eigenvectors ψ 1 , . . . , ψ d ∈ X . This is the case, for instance, if A is symmetric or normal. Using the spectral decomposition (38), the system (37) can be written as x t = A t x 0 = d j=1 λ t j ψ j , x 0 ψ j . This expression shows that the evolution of x t is governed by the eigenvalues λ j , with modes associated with |λ j | > 1 growing and those with |λ j | < 1 decaying over time. In the following, we focus on nonlinear systems and discuss how the so-called Koopman operator theory extends ideas from linear systems. Koopman operator theory Consider a Markov process with transition kernel p and invariant measure π. Let L 2 π = L 2 π (X , R) = {g : X → R | ∥g∥ π = |g| 2 dπ < ∞}. Define the operator A π : L 2 π → L 2 π as A π g(x) = E[g(X t+1 ) | X t = x], for all g ∈ L 2 π and almost all x ∈ X . Equivalently, writing the expectation explicitly, A π g(x) = X g(x ′ )p(x, dx ′ ). The operator A π is called the Koopman operator or Markov operator. It is linear and bounded with ∥A π ∥ ≤ 1. Moreover, for all τ ∈ N, it holds that A τ π g(x) = E[g(X t+τ ) | X t = x]. The interpretation is that any g ∈ L 2 π represents an observable of the system. Its evolution over time follows a linear transformation given by the Koopman operator. If the Markov process is time reversible, then the Koopman operator is self-adjoint, that is, A π = A * π . Remark 17 (Self-adjointness of the Koopman operator) For all g, h ∈ L 2 π , using the definition of the Koopman operator ⟨A π g, h⟩ L 2 π = X A π g(x)h(x)dπ(x) = X h(x) X g(x ′ )p(x, dx ′ ) dπ(x). Swapping the integrals and using the detailed balance condition (36), we get ⟨A π g, h⟩ L 2 π = X g(x ′ ) X h(x)p(x ′ , dx) dπ(x ′ ) = X g(x ′ )A π h(x ′ )dπ(x ′ ) = ⟨g, A π h⟩ L 2 π which shows that A π is self-adjoint. If the transition kernel is absolutely continuous with respect to the Lebesgue measure, with a square integrable density, then the Koopman operator is also compact. Remark 18 (Compactness of the Koopman operator) Let q(x, x ′ ) be the density of the transition kernel with respect to the Lebesgue measure, i.e., p(x, dx ′ ) = q(x, x ′ )dx ′ . Then, the Koopman operator A π is the integral operator given by A π g(x) = X q(x, x ′ )g(x ′ )dπ(x ′ ). It is a standard fact in functional analysis that, if X X q 2 (x, x ′ )dπ(x)dπ(x ′ ) < ∞, then the Koopman operator is a Hilbert-Schmidt operator and hence compact. In the following, we assume the Koopman operator A π to be self-adjoint and compact. Then, A π admits an orthonormal eigensystem (λ i , ψ i ) i∈N , with λ i ≥ 0 and ψ i ∈ L 2 π , satisfying A π ψ i = λ i ψ i , i = 1, 2, . . . , and by the spectral theorem, A π = ∞ i=1 λ i ψ i ⊗ ψ i . This expansion is known as the Koopman mode decomposition. The functions ψ i are called Koopman modes and represent spatial patterns of the system, while the eigenvalues λ i characterize their temporal evolution. Note that, for t ∈ N and g ∈ L 2 π A t π g = i λ t i ⟨g, ψ i ⟩ L 2 π ψ i . This decomposition provides a spectral perspective on nonlinear dynamical systems, analogous to the spectral expansion of linear systems discussed earlier in Remark 16. The above discussion highlights how nonlinear dynamical systems can be characterized in terms of corresponding operators. In practice, Koopman operators might be hard to compute exactly, but empirical approximations can be derived based on observations from the corresponding dynamical systems. We next show how considering observables in an RKHS allows casting the estimation of the Koopman operator as a suitable operator learning problem. Learning the Koopman operator with kernels Let H ⊂ R X be a scalar reproducing kernel Hilbert space with reproducing kernel k. The idea is to consider observables in H to estimate A π from data. We next provide an intuition for why restricting observables to an RKHS enables efficient algorithm design. Kernel Koopman regression. The idea is to find W ∈ L(H) such that for any f ∈ H and any t ∈ N f (X t+1 ) ≈ W f (X t ). A useful observation is that if f ∈ H, then ∀W ∈ L(H) and ∀t ∈ N, by the reproducing property E[(f (X t+1 ) -W f (X t )) 2 ] = E[ f , φ(X t+1 ) -W * φ(X t ) 2 H ], where φ(x) = k x = k(x, •) for all x ∈ X . Then, if (f j ) j is an orthonormal basis of H, then for all t ∈ N, j E[(f j (x t+1 ) -W f j (X t )) 2 ] = j E[ f j , φ(X t+1 ) -W * φ(X t ) 2 H ] = E[ φ(X t+1 ) -W * φ(X t ) 2 H ]. Finally, define the risk L : L(H) → R, for all W ∈ L(H) by L(W ) = E[ φ(X t+1 ) -W * φ(X t ) 2 H ]. (39) The above computations lead to several observations. First, they suggest that considering an RKHS enables studying the evolution of the kernel rather than that of all observables. Second, given samples x 1 , . . . , x T , we can estimate the expectation in (39) by the empirical risk L : L(H) → R for all W ∈ L(H) as L(W ) = 1 T T -1 t=0 φ(x t+1 ) -W * φ(x t ) 2 H . ( 40 ) Before proceeding further, we discuss some implications of restricting observables to an RKHS. RKHS restriction of the Koopman operator. Next, we develop the above dobservations, discussing how an operator W ∈ L(H) can approximate the Koopman operator, which naturally belongs to L(L 2 π ). We show that rather than the Koopman operator itself we are approximating its restriction to the RKHS. To show this, we define a suitable embedding operator defined by the kernel and the invariant measure. Assume that there exists κ > 0 such that for all x ∈ X , k(x, x) ≤ κ 2 . (41) Define S : H → L 2 π by Sf (x) = f , k x H , (42) for all f ∈ H and almost all x ∈ X . Several observations can be made. First, the operator S is linear and bounded by Assumption (41). Indeed, for all f ∈ H, we have ∥Sf ∥ π ≤ κ ∥f ∥ H , by the reproducing property (11) and Assumption (41). Second, S is the embedding operator from H to L 2 π , so that each function f ∈ H is seen as an element Sf ∈ L 2 π , with its norm changing from that in H to that in L 2 π . Third, if the support X π of the invariant measure is strictly contained in X , then S is not injective and acts as a restriction operator. From this latter perspective, it can be seen as a continuous analog of the sampling operator (21) . Finally, it is easy to check that the operator S is Hilbert-Schmidt. Let (f j ) j be an orthonormal basis of H. Then Given the above premise, we derive an identity providing insights into the nature of the approximation attainable by minimizing the risk (39), as well as the empirical risk (40). Define A H : H → L 2 π as A H = A π S, that is, the restriction of the Koopman operator to the RKHS. Note that, A H ∈ L 2 (H, L 2 π ) since the set of Hilbert-Schmidt operators forms a two-sided ideal in the algebra of bounded operators. Similarly, SW ∈ L 2 (H, L 2 π ) for all W ∈ L(H). Next we show that L(W ) = ∥A H -SW ∥ 2 L 2 (H,L 2 π ) + σ 2 , ( 43 ) for some suitable constant σ 2 . The above expression shows that when minimizing the risk (39) we are effectively finding an approximation to the Koopman operator restricted to the RKHS. To prove Equation (43) we begin noting that for all W ∈ L(H) L(W ) = E[ φ(X t+1 ) -W * φ(X t ) 2 H ] = E[ F * (X t ) -W * φ(X t ) 2 H ] + σ 2 , ( 44 ) where for almost all x ∈ X F * (x) = E[φ(X t+1 ) | X t = x] and 44 ) is a classic result that can be easily checked developing the square and taking the expectation. We omit this calculation to note that for all f ∈ H, by linearity of the expectation σ 2 = E[ φ(X t+1 ) -F * (X t ) 2 H ]. Equation ( F * (x), f H = E[ φ(X t+1 ), f H | X t = x] = E[Sf (X t+1 ) | X t = x] = A π Sf (x), and W * Φ(x), f H = Φ(x), W f H = SW f (x) for almost all x ∈ X . Moreover, recall that, for any orthonormal basis (f j ) j ∈ H and B ∈ L 2 (H, L 2 π ), ∥B∥ L 2 (H,L 2 π ) = j Bf j 2 L 2 π . Then, any orthonormal basis (f j ) j ∈ H the following equalities hold E[ F * (X t ) -W * φ(X t ) 2 H ] = j E[( F * (X t ) -W * φ(X t ), f j H ) 2 ] = j E[(A π S -SW )f j (X t )) 2 ] = j (A π S -SW )f j 2 L 2 π = ∥A π S -SW ∥ 2 L 2 (H,L 2 π ) Combining the above expression in Equation (44) leads to (43). Provided with the above discussion, we next discuss the computation of the empirical approximation of the Koopman operator restricted to a RKHS. KRR for the Koopman operator. Following the discussion in the previous section, given a sample trajectory x 1 , . . . , x T and an RKHS H with reproducing kernel k, consider the regularized empirical risk minimization problem min W ∈L 2 (H) 1 T T t=1 ∥φ(x t+1 ) -W * φ(x t )∥ 2 H + λ∥W ∥ 2 L 2 (H) . ( 45 ) Note that, we further restricted the class of operator considering L 2 (H) rather than L(H). As discussed in Section 4, this corresponds to a suitable operator valued reproducing kernel and allows the computation of the solution denoted by W λ . Let Y : H → R T be such that ( Y f ) t = k x t+1 , f H , and X : H → R T be such that ( Xf ) t = k x t , f H , t=1, . . . , T. Then, from the optimality condition of problem (45) the following formulas can be derived, W λ = Y * X( X * X + T λI) -1 = Y * ( X X * + T λI) -1 X Moreover, for all x ∈ X let k(x) = (k(x 1 , x), . . . , k(x T , x)) and α(x) = ( X X * + T λI) -1 κ(x) ∈ R T Note that, while W * λ is infinite dimensional, the coefficient vector α(x) is finite dimensional for all x ∈ X , and further its computations involves only finite dimensional quantities. Several observations a can be made. First, it is possible to predict any observable given a number of measurements f (x 1 ), . . . , f (x T -1 ). Indeed, for all f ∈ H, and x ∈ X W λ f (x) = W λ f , k x H = f , Y * ( X X * + T λI) -1 Xk x H = Y f , ( X X * + T λI) -1 Xk x R T = T t=1 f (x t+1 )α(x) t . Note that in particular, we could consider f (x) = x j for x = (x 1 , . . . , x d ) to forecast (the coordinate of) future states. Second, it possible to show that empirical Koopman modes can also be computed but the reasoning is more involved and is omitted here. Bibliography An introduction to several ideas related to dynamical systems and their empirical estimation can be found in [2] . A standard reference for Markov processes is for example [16] . Koopman operator theory is discussed in [10, 17, 17] . The learning approach to dynamical systems based on Koopman operators and reproducing kernel Hilbert spaces is discussed in [13] . Figure 1 : 1 Figure 1: Supervised learning is the problem of finding an estimate f of a function f * given data (x i , y i ) n i=1 . | k x , f j H | 2 dπ(x) = ∞ j=1 | k x , f j H | 2 dπ(x) = k(x, x)dπ(x) ≤ κ 2 ."
}