{
  "title": "WHEN PHYSICS MEETS MACHINE LEARNING: A SURVEY OF PHYSICS-INFORMED MACHINE LEARNING A PREPRINT",
  "abstract": "Physics-informed machine learning (PIML), referring to the combination of prior knowledge of physics, which is the high level abstraction of natural phenomenons and human behaviours in the long history, with data-driven machine learning models, has emerged as an effective way to mitigate the shortage of training data, to increase models' generalizability and to ensure the physical plausibility of results. In this paper, we survey an abundant number of recent works in PIML and summarize them from three aspects: (1) motivations of PIML, (2) physics knowledge in PIML, (3) methods of physics knowledge integration in PIML. We also discuss current challenges and corresponding research opportunities in PIML.",
  "introduction": "Introduction Machine learning/deep learning models have already achieved tremendous success in a number of domains such as computer vision [1, 2, 3, 4, 5] and natural language processing [6, 7, 8, 9, 10, 11, 12, 13, 14] , where large amounts of training data and highly expressive neural network architectures together give birth to solutions outperforming previously dominating methods. As a consequence, researchers have also started exploring the possibility of applying machine learning models to advance scientific discovery and to further improve traditional analytical modeling [15, 16, 17, 18, 19, 20, 21] . While given a set of input and output pairs, deep neural networks are able to extract the complicated relations between the input and output via appropriate optimization over adequate large amount of data, prior knowledge still acts as an important role in finding the optimal solution. As the high level extraction of data distributions and task properties, prior knowledge, if incorporated properly, can provide rich information not existing or hard to extract in limited training data, and helps improve the data efficiency, the ability to generalize, and the plausibility of resulting models. Physics knowledge, which has been collected and validated explicitly both theoretically and empirically in the long history, contains tremendous abstraction and summary of natural phenomena and human behaviours in many important scientific and engineering applications. Thus in this paper, we focus on the topic of integrating prior physics knowledge into machine learning models, i.e. physics-informed machine learning (PIML). Compared to the integration of other types of prior knowledge, such as knowledge graphs, logic rules and human feedback [22] , the integration of physics knowledge requires specific design due to its special properties and forms. In this paper, we survey a wide range of recent works in PIML and summarize them from three aspects. (1) Motivations of PIML, which can be further categorized to using machine learning to serve tasks in physics domains and incorporating physics principles to existing machine learning models for real-world tasks. (2) Physics knowledge in PIML, each type of which is a general principle covering a wide range of problems. (3) Methods of physics knowledge integration in PIML. Depending on the location of knowledge integration, we categorize the methods into data enhancement, neural network architecture design, and physics-informed optimization. The paper is organized as follows. Sec 2 analyzes two main categories of motivations using PIML: one mainly serves tasks in physics domain, while the other serves real-world problems. Sec 3 introduces several general physics principles widely used in PIML. Sec 4 investigates methods of physics knowledge integration. Sec 5 discusses challenges and potential future research directions of PIML. Sec 6 serves as the summary of the whole paper. 2 Motivations of PIML 2.1 ML for Physics: Enhancement of Physics Models via Data-Driven Methods Physical science problems involves various data-intensive tasks including spatiotemporal data modeling, causal reasoning, computer vision, probabilistic inference and so on. Since machine learning methods have achieved great success in these tasks, using machine learning models for furthering scientific discovery in physics has received increasing interest in recent years. Compared to existing numerical or pure physics based methods, physics-informed machine learning methods have advantages in flexibility, generalizability, and computation cost. Meanwhile, they still enforce physical plausibility. In this section, we introduce recent developments of exploiting machine learning for several physics related tasks, including surrogate simulation, data-driven PDE solvers, parameterization of physics models, reduced-order models, and knowledge discovery.",
  "body": "Introduction Machine learning/deep learning models have already achieved tremendous success in a number of domains such as computer vision [1, 2, 3, 4, 5] and natural language processing [6, 7, 8, 9, 10, 11, 12, 13, 14] , where large amounts of training data and highly expressive neural network architectures together give birth to solutions outperforming previously dominating methods. As a consequence, researchers have also started exploring the possibility of applying machine learning models to advance scientific discovery and to further improve traditional analytical modeling [15, 16, 17, 18, 19, 20, 21] . While given a set of input and output pairs, deep neural networks are able to extract the complicated relations between the input and output via appropriate optimization over adequate large amount of data, prior knowledge still acts as an important role in finding the optimal solution. As the high level extraction of data distributions and task properties, prior knowledge, if incorporated properly, can provide rich information not existing or hard to extract in limited training data, and helps improve the data efficiency, the ability to generalize, and the plausibility of resulting models. Physics knowledge, which has been collected and validated explicitly both theoretically and empirically in the long history, contains tremendous abstraction and summary of natural phenomena and human behaviours in many important scientific and engineering applications. Thus in this paper, we focus on the topic of integrating prior physics knowledge into machine learning models, i.e. physics-informed machine learning (PIML). Compared to the integration of other types of prior knowledge, such as knowledge graphs, logic rules and human feedback [22] , the integration of physics knowledge requires specific design due to its special properties and forms. In this paper, we survey a wide range of recent works in PIML and summarize them from three aspects. (1) Motivations of PIML, which can be further categorized to using machine learning to serve tasks in physics domains and incorporating physics principles to existing machine learning models for real-world tasks. (2) Physics knowledge in PIML, each type of which is a general principle covering a wide range of problems. (3) Methods of physics knowledge integration in PIML. Depending on the location of knowledge integration, we categorize the methods into data enhancement, neural network architecture design, and physics-informed optimization. The paper is organized as follows. Sec 2 analyzes two main categories of motivations using PIML: one mainly serves tasks in physics domain, while the other serves real-world problems. Sec 3 introduces several general physics principles widely used in PIML. Sec 4 investigates methods of physics knowledge integration. Sec 5 discusses challenges and potential future research directions of PIML. Sec 6 serves as the summary of the whole paper. 2 Motivations of PIML 2.1 ML for Physics: Enhancement of Physics Models via Data-Driven Methods Physical science problems involves various data-intensive tasks including spatiotemporal data modeling, causal reasoning, computer vision, probabilistic inference and so on. Since machine learning methods have achieved great success in these tasks, using machine learning models for furthering scientific discovery in physics has received increasing interest in recent years. Compared to existing numerical or pure physics based methods, physics-informed machine learning methods have advantages in flexibility, generalizability, and computation cost. Meanwhile, they still enforce physical plausibility. In this section, we introduce recent developments of exploiting machine learning for several physics related tasks, including surrogate simulation, data-driven PDE solvers, parameterization of physics models, reduced-order models, and knowledge discovery. Simulation Many physics-informed machine learning models have been proposed to act as surrogate solutions to numerical simulation in many domains, such as turbulence simulation [23, 24, 25] , climate simulation [26, 27, 28, 29] , and particle system simulation [30, 31, 32, 33] . Compared to numerical simulation, neural network based machine learning models enjoy following advantages. (1) Lower implementation costs. To build a high-quality numerical simulator, researchers usually need years of engineering effort and must choose numerous physically meaningful parameters depending on the task. Instead, machine learning models can be trained directly from a large amount of observed data. (2) Stronger ability to generalize. Machine learning models can share the same architecture for different types of problems in the same category, and then can be further specialized for each problem with observed data. For example, [30] proposes a general framework to learn particle simulation, and can generalize across fluid, rigid , and deformable material systems. (3) Lower computation costs. Existing neural network building blocks such as multilayer perceptrons and convolutions can be efficiently accelerated by various hardware including CPU, GPU, FPGA and ASIC, giving advantages of computation costs to neural networks composed of these blocks. In fluid flow prediction, the model proposed in [34] achieves prediction results close to ground truth simulation at a running speed 60x faster than the numerical method. Meanwhile, due to the large and complex optimization space of neural networks, it is critical to incorporate inductive bias of physics knowledge about the task into either the training data, the model architecture, or the optimization process. This ensures greater physical plausibility of resulting models, further improving the robustness in real-world settings. The detailed techniques of integration will be introduced in Section 4. Data-driven PDE Solver Many real-world problems from physical systems are mostly about how to describe observations based on partial differential equations and numerically solve the equations. There are many traditional methods to numerically solve PDEs such as spectral methods, finite-difference methods (FDM), finite element method (FEM), finite volume method (FVM), etc. All of these methods are numerical and they require proper discretization or a finite number of steps to approximate continuous solution. Combining machine learning techniques with PDE models has a long history in machine learning. [35] introduce a method to reconstruct the deterministic portion of the equations of motion directly from a data series. This approach employs an informational measure of model optimality to guide searching through the space of dynamical systems. [36] present a framework for computer-aided multi scale analysis, which enables models at a fine (microscopic/stochastic) level of description to perform modeling tasks at a coarse (macroscopic/systems) level. These macroscopic modeling tasks, yielding information over long time and large scales, are accomplished through approximately initialized calls to the microscopic similar for only short times and small spatial domains. More recently, [37, 38] introduce a solution to solve PDE in a data-driven manner. Rather than analytically solving a given equation, they infer solutions to targeted PDE via supervised learning. Given an input tuple (x, t), they compute spatial and time derivatives from black-box models and then the output are connected based on a form of PDE to update all learnable parameters in the black-box models. This method does not require any discretization and it is fully data-driven to find a surrogate model. [39] approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal dataset. [40] introduce a technique based on the singular vector canonical correlation analysis (SVCCA) for measuring the generality of neural network layers across a continuously-parameterized set of tasks. They illustrate this method by studying generality in neural networks trained to solve parameterized boundary value problems based on the Poisson partial differential equation. [41] directly model the mapping between a PDE's solution and its initial conditions via message passing in the spatial domain. The following work [42] further extends the modeling to the frequency domain, which inherently generalizes to multiple spatial resolutions. In many physical systems, the governing equations are known with high confidence, but direct numerical solution is prohibitively expensive. Often this situation is alleviated by writing effective equations to approximate dynamics below the grid scale. This process is often impossible to perform analytically and is often ad hoc. [43] propose data-driven discretization, a method that uses machine learning to systematically derive discretizations for continuous physical systems. [44] target the problem of reducing numerical errors of iterative PDE solvers and compare different learning approaches for finding complex correction functions. They integrate the PDE solver into the training loop and thereby allow the model to interact with the PDE during training. Downscaling Directly solving PDEs require spatial and temporal discretization and finer resolution is more desired to capture physically reliable solutions. However, it increases the computational cost and modeling complexity. Downscaling techniques have been widely used as a solution to capture physical variables that need to be modeled at a finer resolution from a coarser resolution. Recently, artificial neural networks have shown a lot of promise for this problem, given their ability to model non-linear relationships. [45] present a downscaling algorithms using neural networks to leverage the relationships between Satellite precipitation estimates (SPEs) and cloud optical and microphysical properties in northeast Austria. [46] present DeepSD (Statistically Downscaling), a generalized stacked super resolution convolutional neural network (SRCNN) framework for statistical downscaling of climate variables. DeepSD augments SRCNN with multi-scale input channels to maximize predictability in statistical downscaling. [47] introduce a data-driven framework for the identification of unavailable coarse-scale PDEs from microscopic observations via machine-learning algorithms. Specifically, using Gaussian processes, artificial neural networks, and/or diffusion maps, the proposed framework uncovers the relation between the relevant macroscopic space fields and their time evolution (the right-hand side of the explicitly unavailable macroscopic PDE) Parameterization A common technique used in numerical models describing complex physics phenomena is replacing dynamic systems that are hard to model with parameterized simple processes, which is named parameterization. While the traditional way to decide parameters is to minimize the discrepancy between models' output and observed data by conducting grid search or Bayesian inference within a certain parameter space, recent works have adopted advances in machine learning as the approximation of unknown dynamics. In geology, [48] study the application of Wasserstein GAN [49] for the parametrization of geological models. The effectiveness of the method is assessed for uncertainty propagation tasks using several test cases involving different permeability patterns and subsurface flow problems. [50] develop a new predictor for near-bed suspended sediment reference concentration under unbroken waves using genetic programming, a machine learning technique. In meteorological science, [51] show that a neural network-based parameterization is successfully trained using a nearglobal aqua-planet simulation with a 4-km resolution (NG-Aqua). The neural network predicts the apparent sources of heat and moisture averaged onto (160 km 2 ) grid boxes. [52] present a novel approach to convective parameterization based on machine learning, using an aquaplanet with prescribed sea surface temperatures as a proof of concept. A deep neural network is trained with a super-parameterized version of a climate model in which convection is resolved by thousands of embedded 2-D cloud resolving models. In chemistry, supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. [53] reformulate existing models into a single common framework we call Message Passing Neural Networks and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark. [54] utilize molecular graph data for property prediction based on spatial graph convolution neural networks. Reduced-Order Models Instead of directly modeling the complex dynamics in the observation space, researchers have developed simplified but more interpretable models (reduced-order models, ROMs) in the hidden space, which is usually derived from the observation space via dimensionality reduction. In recent works, machine learning models have emerged as effective tools to discover such hidden spaces and more information preserving transformations between them and the observation space. In dynamic system analysis, Koopman theory is a typical ROM. Koopman theory is based on the insight that the state space of a non-linear dynamic system can be encoded into an infinite-dimensional space where the dynamics is linear [55] . In practice, people assume the infinite-dimensional space can be approximated with a finite-dimensional space. The key problem is then to find a proper pair of encoder/decoder to map from/to the state space to/from the hidden space. Traditionally, people construct the encoder/decoder with hand-crafted functions, such as the identity function in Dynamic Mode Decomposition (DMD) [56] , nonlinear functions in Extended DMD (EDMD) [57] , and kernel functions in Kernel DMD (KDMD) [58] . However, hand-crafted functions may fail to fit complex dynamic systems and are hard to design without domain-specific knowledge. Thus, recent works [59, 60, 61] construct encoders/decoders using neural networks as trainable universal approximators. They demonstrate that the combination of neural networks and Koopman theory achieves comparable or even higher performance than the Koopman approximators with hand-crafted mapping functions, while enjoying the ability to generalize to multiple datasets with the same design. [59] further shows that the integration of Koopman theory allows the model to adapt to new systems with unknown dynamics faster than pure neural networks. In fluid dynamics, ROMs is largely considered due to the unprecedented physical insight into turbulence offered by high-fidelity computational fluid dynamics (CFD). [62] develop a dimensionality reduction method called Non-Intrusive Reduced Order Model (NIROM) for predicting the turbulent air flows found within an urban environment. [63] demonstrate a deep learning based approach to build a ROM using the POD basis of canonical DNS datasets, for turbulent flow control applications. They find that a type of Recurrent Neural Network, the Long Short Term Memory (LSTM) which has been primarily utilized for problems like speech modeling and language translation, shows attractive potential in modeling temporal dynamics of turbulence. Causality Causal Discovery and Causal Inference in Time Series How to discover the underlying causal structure is a fundamental problem and has still been actively studied. [64, 65, 66] introduce the problem and provided a mathematical framework for causal reasoning and inference under causal graphical models (also known as Bayesian networks (BN) [67] ). [68] formalize a concept of quantifiable causality in time series, called Granger causality. From the pioneering works, learning causal associations from time series has been an emerging topic in machine learning and deep learning community as well. [69] propose a method to distinguish direct from indirect dependencies and common drivers among multiple time series to reconstruct a causal network. [70] quantify causal associations in nonlinear time series and [71, 72] provide promising applications of the causal discovery in time series. [73] introduce a smooth acyclicity constraint to multivariate time series inspired by [74] that consider a causal discovery as a purely continuous optimization problem. For causal inference part, G-computation formula, g-estimation of structural nested mean models [75] , and inverse probability of treatment weighting in marginal structural models (MSMs) [76, 77] rely on linear predictors for estimation to estimate treatment effects. Recurrent marginal structural networks (RMSNs) [78] is proposed to further improve MSM's ability by capturing nonlinear dependencies. In addition, Gaussian process [79, 80] has been tailored to estimate treatment response in a continuous-time settings in order to incorporate non-deterministic quantification. Furthermore, [81] and [82] theoretically prove that observed proxy variables can be used to capture hidden confounders and estimate treatment effects. TSD [83] introduces recurrent neural networks in the factor model to estimate the dynamics of confounders. In a similar vein, [84] propose a sequential deconfounder to infer hidden confounders by using Gaussian process latent variable model. DTA [85] estimates treatment effects under dynamic setting using observed data as noisy proxies. Besides, DSW [86] infers the hidden confounders by using a deep recursive weighted neural network that combines current treatment assignment and historical information. DNDC [87] aims to learn how hidden confounders behave over time by using current network observation data and historical information. Although many works are successful to discover unknown causal structure from observational data directly and make effective causal inference, there are few works to leverage explicit causal relations from physical knowledge to improve data-driven models. Counterfactual Analysis of Physical Dynamics Counterfactual analysis in the physical world is typically concerned with analytically predicting the effects of various types of interventions/treatments, including the physical laws of current environment [88, 89, 90] , the actions of the agent itself [91, 92, 93, 94] , and the decision outcomes of other agents in multi-agent systems [95, 96] . With the development of 3D simulation in the field of machine learning, several benchmarks performing counterfactual analysis in physics have emerged. CLEVRER [97] asks agents to answer a counterfactual question after observing a video showing the movement and collision of a 3D object. Physics 101 [98] presents a video benchmark containing over 101 real-world objects physically interacting in four different physical scenarios. [99] proposes a counterfactual benchmark with two tasks: a scene where balls interact according to unknown interaction laws (e.g., gravity or elasticity), and a scene where clothes are folded by the wind. The agent needs to discover causal relationships between counterfactual variables and objects and then predicts future frames. CoPhy [100] separates the observed experiments from those of counterfactuals and includes three complex 3D scenarios involving rigid body dynamics. The counterfactual analysis of the physics-informed models relies on the separation between the physical information features and the remaining features [101, 102] and may incorporate additional information based on the available prior knowledge of the scene [103] . Recently, PhyDNet [104] explicitly disentangles PDE dynamics from unknown complementary information. The interpretable intuitive physical model [105] proposes an encoder-decoder framework for predicting future collision event frames. The encoder layer infers physical properties, such as mass and friction, from the input frames. Then, the decoder decomposes the potential physical vectors by outputting optical flow. PIP [106] uses a deep generative model to build approximate mental simulations by generating a framework for future physical interactions and then employs selective temporal attention in the form of spanwise selection to predict the outcome of physical interactions. CWMs [99] allow unsupervised modeling of relationships between the intervened observations and the alternative futures by learning an estimator of the latent confounding factors. Cophy [100] predicts alternative outcomes of a physical experiment by estimating the potential performance of confounding factors. Filter-Cophy [107] further learns and acts on a suitable hybrid potential representation based on a combination of dense features, sets of 2D keypoints, and an additional latent vector per keypoint. Physics for ML: Improvement of Data-Driven Models from External Knowledge Data-driven methods represented by machine learning and neural networks have achieved great success recently in a wide range of real-world problems due to the universal approximation ability of neural networks [108] , the increase of available data for model training, and the rapid development of hardware neural network accelerators. However, the optimization of neural networks is highly non-convex, and its convergence to global minima is hard to achieve in practice. Optimization processes converging to local minima without constraints may lead to models with limited generalization ability or results violating existing knowledge including commonsense, logic rules, and physics laws [22] . In this work, we focus on the integration of physics knowledge in machine learning, and the following sections will introduce the integration in multiple domains of machine learning respectively. Supervised Learning Typical supervised learning tasks including classification and forecasting cover various real-world applications from multi-agent systems, computer vision, time series analysis, and spatio-temporal data modeling. While deep neural network-based methods have been the dominating solution in these areas recently, people have also explored several novel approaches with the incorporation of physics knowledge in tasks on data from physics-related scenarios, such as object-centric data, spatio-temporal data, and geometry data. Object-centric Data Object-centric data is generated by systems composed of multiple discrete objects. Examples of real-world object-centric data include trajectory data from multi-agent systems [109, 110, 111] , position and velocity data from networks of motion sensors [110, 112] , and molecule data [53] . Graph neural network (GNN)-based methods [88, 113, 114, 111] achieve state-of-the-art results on most tasks with object-centric data due to the matching of its inductive bias [113] and the interacting physics property in object-centric data. [115] demonstrates that GNNs generalize well in many popular object-centric tasks because the forwarding process of GNNs aligns to the underlying reasoning process. In addition to this alignment, other types of physics knowledge are also integrated in recent works. [116] obtains a joint model for the total energy and interatomic forces of molecules that follows the energyconserving law. [117] proposes Deep Lagrangian Networks (DeLaN) for robotics, on which Lagrangian Mechanics is imposed and physical plausibility is maintained. [109] designs models for forecasting n-body systems that learn and respect exact conservation laws -Hamiltonian mechanics -in an unsupervised manner. Lagrangian Neural Networks (LNNs) in [118] further parameterize arbitrary Lagrangians using neural networks without canonical coordinates or restrictions of the functional form of learned energies, and can be applied to graphs and continuous systems. [59] learns compositional Koopman operators using GNNs and shows better efficiency and generalization than existing GNN baselines in multi-object systems such as ropes and soft robots. Spatio-Temporal Data Spatio-temporal data records the dynamics of values of interest in multiple locations within a period of time. The most common form of spatio-temporal data is video, where all locations are aligned to rectangular mesh grids in some specific area. Spatio-temporal data can also be generated from networks of irregularly spaced sensors in domains such as traffic [119] , weather [120] , and electricity [121] . Since the underlying processes of spatio-temporal data are usually governed by physics laws, physics-informed methods have potentials to further improve the performance of neural network models. [104] explicitly disentangles known PDE dynamics from unknown factors and performs PDE-constrained prediction in latent-space, both of which contributes to video forecasting performance. [122] mimics the pipeline of physical flow simulation, and evolves and accumulates point features in point clouds using flow velocities generated from a high-dimensional force field. It demonstrates the efficacy of the proposed method in various point cloud classification and segmentation problems. [123] learns finite differences of sparsely available data points inspired by physics equations, and shows the superiority in synthetic graph signal prediction and real-world weather forecasting tasks. [124] proposes a general continuous-time differential model for dynamical systems, which admits arbitrary space and time discretizations, and enables efficient neural PDE inference. Manifold Data Manifold data describes signals defined on non-planar surfaces such as spheres and surfaces of complex 3D objects, where the Euclidean geometry only holds locally near each point. Examples are magnetoencephalography (MEG) brain activity signal data [125] (on sphere), omnidirectional image data [126] (on 3D objects), and human scan data [127] (on 3D objects). Without the restriction that data points are distributed on planes, general manifold data comes with richer information about the space including local structures and symmetries of the space. Meanwhile, existing convolution-based neural network architectures are either no longer applicable in nonplanar manifolds (CNNs) or not capable of fully exploiting the spatial information (GNNs). Instead, physics-informed neural networks incorporate established knowledge of manifolds into the construction of new types of convolutions for manifolds and bridge the gap. [128] constructs intrinsic CNN-like architectures on non-planar surfaces under the geodesic polar coordinate system. [129] alternatively uses the principal curvature directions as fixed gauges to construct convolutions in corresponding tangent fields. [130] constructs convolution kernels as rotating filters and collects the strongest responses among all possible directions. [131, 132] further extends convolution filters to be gauge-equivariant via parallel transporting geometric features to the same vector space before applying the filters. Model-Based Control Due to the integration of physics knowledge, physics-informed machine learning models enjoy better physical plausibility, higher data efficiency and stronger generalization ability compared to pure neural network models, all of which are critical properties for constructing a good model describing the relation between the control input and the state transition of dynamic systems. Recent works have shown that physics-informed machine learning models achieve significant success in model predictive control and model-based reinforcement learning. [133] integrates a deep neural network (DNN) that learn high-order interactions into the dynamics model, and constrain the Lipschitz constant of the DNN to guarantee system stability. [117, 134] enforce Lagrangian and Hamiltonian dynamics in the modeling of underlying system dynamics respectively, and both outperform model learning approaches without physics knowledge in trajectory tracking error, learning speed, and robustness. [59] shows that dynamics learned via compositional Koopman operators can quickly adapt to new environments of unknown physical parameters in online learning. [135, 136] augment partial differential equations (PDEs) that approximately describe continuous physical systems with controllable force terms, and demonstrate that proposed methods successfully control the evolution of complex physical systems. Physics Knowledge in PIML In this section, we introduce several categories of general physics knowledge integrated in PIML. While there is much more domain/task-specific knowledge that can be incorporated for corresponding solutions, each category we introduce in this section covers a wide range of problems and inspires a series of works generally applicable to them instead of leading to only one or two task-specific solutions. Classical Mechanics and Energy Conservation Laws Newtonian, Lagrangian, and Hamiltonian mechanics are three typical approaches describing systems of classical mechanics. While the Newtonian mechanics has been widely used to describe the relations among locations, velocities, accelerations and forces, Lagrangian and Hamiltonian mechanics provide effective tools to enforce laws of conservation of energy in the modeling of dynamic systems. Since Newtonian mechanics described as the famous Newton's Three Laws has been widely known, here we only focus on Lagrangian and Hamiltonian mechanics. Lagrangian Mechanics The Lagrangian mechanics defines the Lagrangian function L of generalized coordinates q and its gradient w.r.t time q to fully describe dynamics of a mechanical system. Usually L is chose as the difference between the kinetic energy T and the potential energy V , i.e. L(q, q) ≡ T (q, q) -V (q). The defined Lagrangian function must satisfy the principle of stationary action: the real physical trajectory of a system will always take is the one in which the action happens to be stationary, which is mathematically expressed as: δ t2 t1 Ldt = 0. (1) We can further derive the important Euler-Lagrange equation from Eq 1: d dt ∇ q L = ∇ q L. (2) According to Noether's theorem, the energy of a system is conserved if the system has a time-translation symmetry, i.e. if the Lagrangian function does not explicitly depend on time ( ∂L ∂t = 0). Therefore, as long as we model the Lagrangian function as a function of q, q without the explicit time term t and satisfies Eq 2, it automatically satisfies the law of conservation of energy. With Eq 2 we can derive the expression of q with the chain rule [118] : (∇ q ∇ T q L) q + (∇ q ∇ T q L) q = ∇ q L. ( 3 ) When L is modeled as a differential function such as neural networks, we can solve q as: q = (∇ q ∇ T q L) -1 [∇ q L -(∇ q ∇ T q L) q]. (4) Usually one system's q and q can be observed as spatial coordinates and speeds of objects, thus the system can be solved with (q, q)| t=T = T 0 ( q, q)dt. For systems with non-conservative forces, Eq 2 can be extended as d dt ∇ q L -∇ q L = τ , (5) where τ are generalized forces and can be used for controllers such as a PD-Controller [117] . Hamiltonian Mechanics The Hamiltonian mechanics defines the Hamiltonian function H as the function of a pair of variables (q, p), where q is the vector of spatial coordinates of system objects and p is the vector of their momentum. (q, p) must satisfy the canonical condition: p = ∇ q L, (6) where L is the Lagrangian function of the system. The Hamiltonian function is defined as follows: H(q, p) = q • p -L. (7) Combine Eq 6, Eq 7 and Eq 2 we can derive: ṗ = -∇ q H, q = ∇ p H. (8) With Eq 8, we can verify that dH dt = q∇ q H + ṗ∇ p H = 0. (9) In the context of classical mechanics, we have p = M q, T = 1 2 qT M q, L = T -V , then Eq 7 can be rewritten as H(q, p) = qT M q -T + V = T + V , where M is the mass matrix of the system. Here the Hamiltonian function H is exactly the total energy of the system. Therefore, Eq 9 shows that systems described with Hamiltonian mechanics follows the law of conservation of energy. Symmetry, Invariant and Equivariant Functions A symmetry defined on an object or system is some transformation that keeps certain properties unchanged. Typical symmetries include shifts in visual object classification problems, rotations in molecule property prediction problems, and permutations in particle systems. For one object or system, its symmetries form a symmetry group, where the following rules are satisfied: (1) associativity, (2) identity, (3) inverse, (4) closure. Some common symmetry groups are: T (n) (n-dim translation group), O(n) (n-dim distance-perserving group including both rotation and reflection), SO(n) (n-dim rotation group), E(n) (n-dim Euclidean group, including translation, rotation and reflection). On a set Ω representing a domain, we can define a group action as a mapping (g, u) → g.u, where g is one element of a symmetry group G, u and g.u are two points in Ω. A group action shall satisfy associativity: g.(h.u) = (gh).u for all g, h ∈ G and u ∈ Ω. In comparison, group action on a signal space X : Ω → R n is defined as: (g.x)(u) = x(g -1 u). (10) We can verify that the above definition satisfies the associativity: (g.(h.x))(u) = (h.x)(g -1 (u)) = x((gh) -1 u) = ((gh).x)(u). (11) On a domain Ω ∈ R n , we can define an n-dimensional real representation of a group G as a map ρ : G → R n×n , connecting each g ∈ G to an invertible matrix ρ(g), and satisfying the associtivity ρ(gh) = ρ(g)ρ(h) for all g, h ∈ G. For a symmetry group G we have the definitions of G-invariant and G-equivariant. A function f : X (Ω) → Y is G-invariant if f (ρ(g)x) = f (x) for all g ∈ G and x ∈ X (Ω), and it is G-equivariant if f (ρ(g)x) = ρ(g)f (x). By stacking multiple neural network layers(functions), each of which satisfies either equivariance or invariance under some symmetry group, we can incorporate the knowledge of symmetries of domains into the resulting network. Table 1: Neural network architectures, symmetry groups and domains. Numerical Methods for Partial Differential Equations (PDEs) A generic form of PDEs describing the evolution of a continuous value v(x, t) is as follows: ∂v ∂t = F (t, x, v, ∂v ∂x i , ∂ 2 v ∂x i ∂x j , . . . ). (12) Finite Difference Method Finite difference methods approximate spatial derivatives on the right hand side of Eq 12 at a certain point x 0 as a linear combination of function values at N neighbors of x 0 . In 1D case, the approximation of the l-th order spatial derivative can be formulated as (here we omit t): ∂ l f ∂x l x=x0 = N n=1 α n f (x 0 + h n ). (13) α 1 , α 2 , . . . , α n can be solved via expanding α 1 f (x 0 + h 1 ), α 2 f (x 0 + h 2 ), . . . , α N f (x 0 + h N ) at x 0 to the N -th order:      α 1 f (x 0 + h 1 ) = α 1 N -1 k=0 1 k! ∂ k f ∂x k h k 1 + O(h N -1 1 ) . . . α N f (x 0 + h N ) = α N N -1 k=0 1 k! ∂ k f ∂x k h k N + O(h N -1 N ) , (14) which can be summed together as N n=1 α n f (x 0 + h n ) = N -1 k=0 ( 1 k! N i=1 α i h k i ) ∂ k f ∂x k + O(max i∈[N ] h N -1 i ) (15) To solve α 1 , α 2 , . . . , α n for a certain order l, we can let the multiplier of ∂ k f ∂x k be 1 and others be 0, and solve following linear equations:           1 1 . . . 1 h 1 1 h 1 2 . . . h 1 N . . . . . . . . . h l 1 h l 2 . . . h l N . . . . . . . . . h N -1 1 h N -1 2 . . . h N -1 N               α 1 α 2 . . . α N     =          0! • 0 1! • 0 . . . l! • 1 . . . (N -1)! • 0          , (16) the solution α * 1 , . . . , α * N of which satisfies ∂ k f ∂x k = N n=1 α * n f (x 0 + h n ) + O(max i∈[N ] h N -1 i ). (17) The above method can be extended to multi-dimensional cases with multi-dimensional Taylor expansion. After approximating spatial derivatives on the left hand side, Eq 12 can be solved with standard techniques of numerical integration. While Eq 16 can fully determine all coefficients, recent works [140, 141] relax it by removing some constraining equations and use neural networks to learn undetermined coefficients to combine prior knowledge with stronger expressivity. Finite Volume Method Unlike finite difference method, finite volume method represent the value v(x, t) with its averages over a grid cell: v i (t) = ∆x -1 xi+∆x/2 xi-∆x/2 v(x ′ , t)dx ′ . Similar to the finite difference method, spatial derivatives can also be estimated as the linear combination of averaged cell values: ∂ l v ∂x l = N i=1 α (l) i v i , and the coefficients are estimated in the same way as finite difference method. For finite volume methods, the equation must be able to be rewritten as: ∂v ∂t = ∂ ∂x J(t, x, v, ∂v ∂x i , ∂ 2 v ∂x i ∂x j , . . . ), (18) where J is called a flux and has an analytical form derived from the original PDE. The evolution along time can be carried out in following steps: (1) spatial derivatives are estimated on the boundary between grid cells; (2) the flux J is calculated with approximated derivatives using its analytical form; (3) the temporal derivative of averaged cell values is calculated via subtracting J ath the cell's left and right boundaries. The final step can be conducted with techniques that promote stability, such as monotone numerical fluxes and Godunov flux [43] . Recent works have integrated datadriven models in step (1) to improve the estimation of spatial derivatives. For example, [43] estimate coefficients with the combination of neural network results and numerical results, and [142] uses policies trained via reinforcement learning to determine coefficients for estimation. Finite Element Method Finite element method divide a space into small parts (elements) and approximate the PDE on each. As illustrated in [143] , we use the Poisson's equation to show the basic idea of finite element method. The formulation of the Poisson's equation is as follows: -∆u = λ in Ω u = 0 on Γ. (19) In finite element method, we multiply a test function v on both sides, integrate over Ω, and use integration in parts to derive the weak formulation: a(u, v) = l(λ, v), for all v ∈ V, (20) where a(u, v) = Ω ∇u • ∇vdx, l(λ, v) = Ω λvdx. (21) Then we construct a finite-dimensional subspace V h ⊂ V, where V h is a piece-wise polynomial function space spanned via {φ 1 , φ 2 , . . . , φ n }. To solve the weak formulation, we transform it to the Galerkin weak formulation, which is an approximation of Eq 20: Au = f , (22) where A ∈ R n×n , A ij = a(φ i , φ j ), u ∈ R n is the solution vector and f ∈ R n is the source vector with f i = l(φ i , λ). The solution of Eq 22 gives the best solution of the PDE in V h . Koopman Theory Given a non-linear dynamic system with its state vector at time t denoted as s t ∈ R m . The system can be described as s t+1 = F (s t ). As defined in [55] , the Koopman operator K F is a linear transformation defined on a function space F by K F g = g • F for every g : R m → R that belongs to the infinite-dimensional Hilbert space F . With the definition we have K F g(s t ) = g • F (s t ) = g(s t+1 ). The Koopman theory [55] guarantees the existence of K, but in practice we often assume the existence of an invariant finite-dimensional subspace G of F spanned by k bases {g 1 , g 2 , . . . , g k }. Define g t = [g 1 (s t ), g 2 (s t ), . . . , g k (s t )] T and g t+1 = [g 1 (s t+1 ), g 2 (s t+1 ), . . . , g k (s t+1 )] T , under the assumption we have g t , g t+1 ∈ G and there exists a Koopman matrix K ∈ R k×k s.t. g t+1 = Kg t . The key problem is to find the pair of mappings between the state space R m and the invariant subspace G ∈ R k : g : R m → R k and g -1 : R k → R m . Recent works [61, 59, 60] utilize neural networks as g and g -1 to find the mappings in a data-driven way. Methods of PIML Typical solutions to a problem with machine learning involve three key parts: data, model, and optimization, each of which can be integrated with prior physics knowledge. In the following parts, we will introduce existing techniques of incorporating physics knowledge to each part respectively. However, we should notice that these techniques are not mutually exclusive: physics knowledge can be integrated to more that one parts of the machine learning solution. Knowledge Form Integration Method Data Model Optimization Simulation Data Transfer Learning Auxiliary Tasks Computation Graph Fusion Knowledge Based Loss Regularization Domain Knowledge in Analytical Form Table 2 : Existing works classified based on forms of physics knowledge and integration methods. For certain types of knowledge forms, inductive bias integrated in reusable computation graphs have advantages over integration with data and optimization. In Table 2 , we classify existing works based on the forms of physics knowledge and the integration methods. We notice that for domain knowledge taking analytical forms, existing works integrate the knowledge into all three aspects including data, model, and optimization. However, research works on integrating other general types of physics knowledge, including energy conservation law, symmetry, numerical methods for PDEs, and Koopman theory, mainly focus on incorporating corresponding knowledge into computation graphs. The main reason is that such general physics knowledge is possible to be transformed to inductive bias in reusable network architectures, which has advantages over data augmentation and physics knowledge based loss functions in terms of prediction performance and data efficiency [137] . This is due to that (1) general physics knowledge applies to various problems and thus leads to general network architectures, and (2) has simpler forms that can be translated to combinations of a limited number of differentiable operators compared to complex numerical simulators designed for domain-specific problems such as weather and turbulence. Physics-Informed Data Enhancement Data Generated from Simulation The Universal Approximation Theorem [108] guarantees that multilayer neural networks with as few as one hidden layer can approximate any continuous function from one finite dimensional space to another to any desired degree of accuracy. Therefore, one straight forward method to incorporate physics knowledge into neural networks is to generate training data from the desired physics knowledge. When the data amount is abundant, the neural network is expressive enough and trained properly, the trained neural network will be able to approximate the behavior of the physics knowledge governing the data generation. Usually the neural network models can be accelerated with hardware such as GPU, FPGA and ASIC, thus they can act as good surrogate models with much lower computation costs while maintaining comparable accuracy to the numerical simulation. [26] present a benchmark dataset from results of numerical global weather simulation with high computation cost, and provide scores of deep learning models. Results show that data-driven models trained with simulation data can achieve competitive results compared to numerical solutions while enjoying lower computation costs. [31, 30, 144] utilize different variants of message passing graph neural networks [169, 113, 114] respectively and train them with the simulation data of particle systems. Compared to the simulation methods generating datasets, the trained surrogate model can accurately predict the dynamics of a wide range of physical systems within the same architecture, and runs orders of magnitude faster. Transfer Learning For real-world tasks suffering from data limitation or labeling difficulties, the integration of prior physics knowledge about the tasks is critical. Simulators constructed with such physics knowledge can provide large amount of data with high label quality, and can be used to pre-train models. However, the differences between the target real-world data distributions and simulation data distributions call for techniques of transfer learning to mitigate the gap. [145] present a system for training object detection models with synthetic images. This work adopts the technique of domain randomization, where important parameters of simulators -including lighting, pose, object textures -are randomized in non-realistic ways to encourage the model to learn essential features. [146] uses off-the-shelf simulators to render synthetic data for training a grasping system together with pixel-level domain adaptation between synthetic images and real-world ones. The utilization of synthetic data reduces the required amount of real-world samples by up to 50 times. [147] transfer driving policies trained from simulation to reality via modularity and abstraction, where the driving policy is exposed to segmentation results of input scenes and target way points, instead of raw perceptual input or low-level vehicle dynamics. [148] incorporate the task-specific prior knowledge into the model and pre-train it with synthetic data generated by imperfect physical models, which allows the model to get close enough to the target solution and only a small amount of real-world data is needed for refining. Multitask Learning and Meta Learning with Auxiliary Tasks Synthetic data generated from physics-based simulators can also be used to construct auxiliary learning tasks for improving the model's performance on the target task with techniques of multitask learning and meta learning. [116] constructs the auxiliary task as the prediction of interatomic forces for the main task of molecular energy prediction. The labels of the auxiliary task and the main task are generated from simulation at the same time, while the prediction of the auxiliary task is produced via differentiating the energy prediction model. Both tasks are used to train the model simultaneously. [162] adopts the multitask learning scheme by learning shared representations between multiple related PDEs, which are generated by varying coefficients, for better generalizability of the proposed neural network based PDE solver. [163] proposes a spatiotemporal forecasting model with decoupled spatial and temporal modules, where the spatial module is PDE-independent and are trained via model agnostic meta learning (MAML) [170] for fast adaptation on new tasks, while the task-dependent temporal module is trained from scratch for each task. 4.2 Physics-Informed Neural Network Architecture Design Physics-Informed Computation Graph A typical way of physics-informed neural network architecture design is to design computation graphs that mimic the behavior of physics knowledge based methods. While the specific method is highly dependent on the physics knowledge, a general idea is to start from some existing physics based solution, then replace difficult-to-estimate variables with outputs of neural networks, or relax some fixed parameters by enabling them to adapt to the data. In following paragraphs, we will introduce several knowledge-specific neural network designs as well as techniques to directly fuse deep learning models with physics based solutions as a hybrid model. Energy Conservation Laws As introduced in Sec 3.1, Lagrangian and Hamiltonian mechanics are powerful in enforcing energy conservation laws, thus a series of recent works develop neural network architectures based on them to incorporate the energy conservation property. [117] proposes a network topology named Deep Lagrangian Networks (DeLaN) encoding the Lagrange-Euler PDE originating from Lagrangian Mechanics, which can be trained with standard optimizers while maintaining physical plausibility. [118] designs Lagrangian Neural Networks (LNNs) to model arbitrary Lagrangian functions via neural networks, and solve the dynamics of the system with a numerical expression derived from the Euler-Lagrangian equation, where gradients from auto differentiation are utilized. Similarly, [109] (HNN) models the Hamiltonian function with a neural network. The derivatives of spatial coordinates and momentum with respect to time are derived from Eq 8. [154] proposes Neural Hamiltonian Flow (NHF), which is a powerful normalising flow model using Hamiltonian dynamics as the invertible function to model expressive densities. In NHF, the density is decomposed into the \"coordinate\" part and the \"momentum\" part in the hidden space, both of which are then propagated with Eq 8. The propagation is (1) invertible and (2) has the volume (\"energy\" with respect to the hidden space) preserving property, which satisfies the requirement of normalising flows. Compared to other flowbased approaches, NHF enjoys higher computational efficiency since it avoids the expensive step of calculating the trace of Jacobians. [134] designs the computation graph of the proposed neural network following the Hamiltonian dynamics with control to incorporate the corresponding inductive bias. It further proposes a parameterization that can enforce the Hamiltonian mechanics with coordinates embedded in a high-dimensional space or velocity data instead of momentum. Symmetry Table 1 in Sec 3.2 has shown the connection between some widely used neural network architectures and the corresponding symmetry groups. Here we introduce methods incorporating other types of symmetry groups into neural network architectures. Based on the representations of symmetry groups adopted, all the methods we introduce can be categorized to methods using (1) invariant treatment of coordinates (2) irreducible representations or (3) regular representations. Invariant Treatment of Coordinates Depending on the symmetry group, spatial coordinates should be properly processed instead of being used as raw inputs. [155] develops equivariant message passing to E(3) via letting messages passed among nodes only depend on distances, which follows the property that E(3) preserves distances between nodes. The same technique is also used in [116] . [156] propose a set of transformation invariant and equivariant GNN models by tweaking the definition of an adjacency matrix named isometric adjacency matrix, which can be viewed as a weighted adjacency matrix for each direction and reflects spatial information. Irreducible Representations All elements of a roto-translation group can be transformed into an irreducible form: a vector that is rotated by a block diagonal matrix. The full set of equivariant mappings for some symmetry group can be solved with equivariance constraint over convolution kernels. The solutions form a linear combination of equivariant basis matrices, which can be used for equivariant convolutions. [157] gives a general solution of the kernel space constraint for arbitrary representations of the Euclidean group E(2) and its subgroups, which forms a wide range of equivariant network architectures. [158] develops convolution filters locally equivariant to 3D rotations, translations, and permutations, which are built from spherical harmonics. [159] further enhances [158] with the self-attention mechanism. Regular Representations Regular representation approaches store copies of latent feature embeddings for all elements of a symmetry group. To mitigate this computational burden, some recent works such as [160, 161] use Lie groups as the tool for rapid prototyping across various symmetry groups. Only the exponential and logarithm maps are required for incorporating equivariance to a new symmetry group. Numerical Methods Sec 3.3 introduces several numerical methods for solving PDEs. In this paragraph, we introduce some recent works integrating numerical solutions for each method. Finite Difference Method [140, 141] propose learnable differential operators by learning convolution kernels to approximate unknown nonlinear responses in PDEs. All kernels are properly constrained by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters, which originates from wavelet theory. These constraints ensure both the model's ability to identify PDEs and its expressivity. [164] proposes an efficient convolution kernel on unstructured grids of spherical signals using parameterized finite difference operators. [123] leverages differences of sparsely available data from physical systems via the spatial difference layer (SDL). SDL is inspired by finite difference operators on graph and triangulated mesh and replaces fixed parameters in these operators with the output of a GNN capturing spatial information of the input data. Finite Volume Method [43] adopts CNNs to generate coefficients for approximating spatial derivatives, followed by the standard finite volume method. CNNs are optimized end-to-end to best satisfy the equations on low resolution grids, and produce accurate numerical results: it can be integrated at resolutions 4-8x coarser than is possible with standard finite volume methods. [142] creates new PDE solvers based on the WENO scheme [171] via generating its coefficients from a learned policy network trained with reinforcement learning. Finite Element Method [165] mimics the behavior of finite element analysis: it assigns nodes of a GNN to selected spatial locations and uses message passing on the graph to model the relationship between an initial function and a resulting function defined in the same space. Both the locations of nodes and their connectivity can be optimized to focus on the most important parts of the space. [143] proposes a two-stage optimization framework for PDE-constrained optimization problem. At the first stage, the framework obtains a surrogate model to prediction solutions of finite element method directly from control parameters. At the second stage, the framework performs gradient-based PDEconstrained optimization. [166] introduces convolution operators on unstructured point clouds based on Generalized Moving Least Squares (GMLS), which is a non-parametric technique in finite element method [172] for estimating linear bounded functionals from scattered data. Koopman Theory [61] first combines deep learning models with the Koopman operator. It utilizes the power of deep learning to identify nonlinear coordinates on which the dynamics are globally linear using the Koopman operator. The resulting method benefits from both the power and generality of deep learning models and the physical interpretability of Koopman embeddings. [168] proposes minimization of the residula sum of squares of linear leastsquares regression to estimate the encoders and decoders that maps data into the Koopman invariant subspaces where the linear regression fits well. [59] extends deep learning based Koopman operators to scenarios with multiple objects. It adopts GNNs to encode object-centric states and uses a block-wise linear state transition matrix (Koopman matrix) to enforce the shared structure among objects. [60] incorporates the consistency by penalizing the consistency mismatch of forward and backward Koopman matrices. Fusion of Deep Learning and Physics-Based Modules In addition to previously introduced methods that mimic the behaviour of physics based solutions in the computation graph design of neural network layers, deep learning and physics-based methods can also be fused in a higher level: modules constructed with standard neural network blocks and physics rules work interactively but only expose input/output to each other. [149] adopts a graphical model derived from equations of motion in a physics model to predict the next future state while adding it with the predicted residual part from a GNN module. [167] presents HybridNet, a framework combining data-driven deep learning and model-driven computation for reliable spatiotemporal evolution prediction. The deep learning part, Convolutional LSTM (ConvLSTM) works as the backbone to predict the evolution of external input to the system. The model-driven part, Cellular Neural Network (CeNN), transforms numerical computation in PDE solvers and is able to infer unknown physical parameters. [34] proposes a hybrid approach for fluid flow prediction containing two components: one GNN-based module operating directly on the original fine-grained non-uniform mesh used in CFD, and one CFD solver operating on a much coarser resolution. The output of the CFD solver is upsampled to the fine-grained mesh and then concatenated to the hidden embeddings from GNN layers. The hybrid model generalizes better than pure GNN-based approaches and is still faster than directly running CFD simulation on the original mesh. [140, 104] uses constrained convolution kernels to extract approximated spatial derivatives as features, which are fed as the input to the following neural network layers that capture unknown dynamics and give the final prediction. Physics-Informed Optimization Prior physics knowledge can also be integrated into the optimization process in the form of loss functions directly derived from task-specific knowledge or regularizations from physics principles. The integration of physics knowledge in optimization targets reshapes the optimization space and encourages the training process to converge to physical plausible solutions. Task-Specific Knowledge Based Loss Terms A variety of physics knowledge can be described in the form of PDEs, which provides connections between the spatial and temporal derivatives as well as constraints of values of interest on boundaries. [37, 38, 39, 150] adopts multi-layer perceptrons to directly model the mapping from input spatial and temporal coordinates to the value of PDE solutions. Instead of only minimizing the prediction error between the output and solutions from numerical methods, the series of works adds loss terms enforcing the equation structure, which penalizes the violation of PDEs using derivatives from auto-differentiation of the neural network solution. [23] optimizes the model for super-resolution of turbulent flows by minimizing a weighted combination of two losses: one is the norm of the difference between predictions and ground truth values, the other is the norm of residues of the governing PDEs. [151] predicts the traffic flow with a neural network taking time and coordinates as input, and constructs physical discrepancy loss terms with an existing second-order traffic model. [148] trains the proposed lake temperature prediction model with generalized loss function to include the physical consistency-based penalty, which encourages the consistency between lake energy and energy fluxes. Regularization [133] adopts a deep neural network to predict the unknown disturbance forces in the controller of drones. To guarantee the system's stability, the authors first derive the overall stability and robustness requirement indicating constraints on the Lipschitz constant, then minimizes its upper bound -the spectral norm of weights in each layer -together with the prediction error in the optimization. [135, 136] both augment purely physics laws/rule-based prediction models with learnable control signals to mitigate the approximation errors of prior knowledge. While they differ in terms of processes for solving PDEs, both have constraints minimizing the norm of control signals, which originates from the least action principle [173] . 5 Challenges and Future Directions Existing works require expertise of the domain-specific knowledge of tasks to incorporate the most appropriate physics knowledge. While this serves the purpose of leveraging domain-knowledge to mitigate the deficiencies of pure datadriven methods, it lacks the flexibility of identifying the correct physics knowledge depending on the task. For example, [23] directly uses the governing equation of the Rayleigh-Benard instability problem as the prior knowledge for turbulence super-resolution, while [24] chooses the derived Hybrid RANS(Reynolds-averaged Navier-Stokes)-LES(Large Eddy Simulation) Coupling method for turbulence prediction. Although both super-resolution and prediction tasks are defined on the same turbulence system governed by the same physics laws, choosing the form of physics laws (the original form/the derived approximation form) to incorporate is heuristic. Research Direction 1: Automatic Identification of Proper Physics Knowledge To Incorporate A promising research direction is to reach the middle ground between the domain-specific knowledge and the pure data-driven way. Here we discuss some potential approaches to realizing it. Neural Architecture Search (NAS) The development of NAS allows the automatic design of neural network architectures and NAS methods have outperformed manually designed architectures on many tasks including image classification and semantic segmentation [174] . By restricting the available physics knowledge within a pre-selected search space and enabling the model to discover the optimal knowledge or combination of knowledge using NAS techniques, the resulting architecture can reach the balance between exploiting prior knowledge and adapting to observed data. [175] presents a neural block dynamics design space of neural network components that encompasses various state-space models as the search space for NAS, and models given by NAS in such a space achieve highly accuracy with physically consistent results. Automatic Modularization of Network Architectures Modularization of network architectures plays a key part in physics-informed network architecture design. For example, DeLaN [117] and HNN [109] contain modules estimating the Lagrangian/Hamiltonian function of the system and following modules deriving the prediction. [43, 123] separate modules approximating spatial and temporal derivatives in the network. Modularity provides better generalization ability [115] , and some modules can further be trained in multiple tasks and benefit the overall performance on all tasks [176, 163] . However, existing works still require a certain selection of physics knowledge to guide the modularization. Some recent works have started exploring discovering functional modules automatically during the training process. For example, [177] divides models into reusable modules and task-specific modules by estimating the variance of parameters across tasks. [178] lets multiple groups of recurrent cells compete with each other so that they are only updated at time steps where they are most relevant. This enables the model to learn modular structures and leads to improved generalization. Challenge 2: Lack of Benchmarks and Evaluations of PIML Methods Comprehensive benchmarks have shown as great boosters for the development of corresponding research areas. Examples include ImageNet Large Scale Visual Recognition Challenge(ILSVRC) [179] and Common Objects in Context(COCO) [180] from computer vision, Workshop on Statistical Machine Translation (WMT) [181] and Stanford Question Answering Dataset (SQuAD) [182] from natural language processing. However, due to the complexity and heterogeneity of problem settings, PIML still lacks comprehensive benchmarks for evaluating various methods of knowledge integration, which creates barriers in the development of PIML. First, most problems in PIML come from physics or engineering applications, where acquiring the data and formalizing the task can be challenging for researchers without domain knowledge and experience. Second, existing works such as [117, 109, 123, 60, 140, 43, 34] heavily rely on heterogeneous domain-specific datasets, which greatly increases the difficulty of fairly comparing different PIML methods. Research Direction 2: Comprehensive Benchmarks for PIML Methods Constructing comprehensive benchmarks for PIML is of great need for boosting its development. According to the above discussion, ideal benchmarks (1) must provide publicly available and organized datasets, and formulate benchmark tasks as typical machine learning tasks, such as classification and regression; (2) must be general enough to accommodate various PIML methods as well as data-driven and pure physics-based methods. Recent development along this direction includes WeatherBench [26] and Open Graph Benchmark (OGB) [183] . WeatherBench provides processed weather data together with clearly defined tasks and evaluation metrics for medium-range weather forecasting. It also presents performance of purely data-driven models and numerical models as baselines. OGB offers data of protein structures and molecule structures, and construct node/link/graph property prediction tasks, where performance of PIML methods and data-driven baselines can be directly compared. Established theories and empirical conclusions of neural network architectures and optimization methods are mostly developed in areas where neural network methods first gain advantages, such as computer vision and natural language processing. However, they may no longer be effective in PIML. The reason is that PIML methods usually involve explicit use of gradients in forwarding processes and objective functions, leading to the existence of highorder derivatives in the backward process, which shapes optimization spaces significantly different from typical deep learning models. For example, [118] notices that regular parameter initialization methods such as Kaiming [184] and Xavier [185] are insufficient since the unusual optimization objective is very nonlinear. [186] also empirically demonstrates that the widely used Rectified Linear Unit (ReLU) activation is not effective in the physics-informed PINN architecture proposed by [37, 38] . Research Direction 3: Novel Neural Network Designs for PIML The drastic differences in network architecture and objectives between PIML and conventional deep learning tasks signify the importance of novel neural network designs for PIML from both the architecture and the optimization aspects. From the architecture perspective, since many PIML methods involve the utilization of gradients from the autodifferentiation of neural networks, designing new architectures/components better preserving gradient information is one promising direction. [187] leverages periodic activation functions for implicit neural representations and demonstrate that they are ideally suited for representing complex natural signals and their spatial/temporal derivatives. From the optimization perspective, multiple objectives (including both the task objective and the physics-informed constraints) may contradict with each other and lead to suboptimal results under vanilla optimization methods. [188] notices the discrepancy of updating directions between the boundary constraint loss and the approximation loss in PINN [37, 38] , and proposes the dynamic pulling method (DPM) to align their updating directions, which significantly improves the extrapolation performance of PINNs. 5. 1 1 Challenge 1: Handcrafted Selection of Physics Knowledge for Incorporation 5. 3 3 Challenge 3: Suboptimal Existing Neural Network Architectures and Optimization Methods for PIML Table 1 [ 1 137] connects some neural network architectures with their corresponding symmetry groups and domains. Architecture Symmetry group G Domain Ω CNN Spherical CNN [138] Intrinsic Mesh CNN [128, 129] Manifold Grid Sphere / SO(3) GNN Graph Deep Sets [139] Set Transformer Complete Graph LSTM 1D Grid Translation Rotation SO(3) Isometry Iso(Ω) / Gauge symmetry SO(2) Permutation Σ n Permutation Σ n Permutation Σ n Time warping"
}