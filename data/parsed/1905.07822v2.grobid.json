{
  "title": "Minimal Achievable Sufficient Statistic Learning",
  "abstract": "We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a training method for machine learning models that attempts to produce minimal sufficient statistics with respect to a class of functions (e.g. deep networks) being optimized over. In deriving MASS Learning, we also introduce Conserved Differential Information (CDI), an information-theoretic quantity that -unlike standard mutual information -can be usefully applied to deterministically-dependent continuous random variables like the input and output of a deep network. In a series of experiments, we show that deep networks trained with MASS Learning achieve competitive performance on supervised learning and uncertainty quantification benchmarks.",
  "introduction": "Introduction The representation learning approach to machine learning focuses on finding a representation Z of an input random variable X that is useful for predicting a random variable Y (Goodfellow et al., 2016) . What makes a representation Z \"useful\" is much debated, but a common assertion is that Z should be a minimal sufficient statistic of X for Y (Adragni, Kofi P. & Cook, R. Dennis, 2009; Shamir et al., 2010; James et al., 2017; Achille & Soatto, 2018b) . That is: 1. Z should be a statistic of X. This means Z = f (X) for some function f . 2. Z should be sufficient for Y . This means p(X|Z, Y ) = p(X|Z). 3. Given that Z is a sufficient statistic, it should be minimal with respect to X. This means for any measurable, non-invertible function g, g(Z) is no longer sufficient for Y . foot_0 In other words: a minimal sufficient statistic is a random variable Z that tells you everything about Y you could ever care about, but if you do any irreversible processing to Z, you are guaranteed to lose some information about Y . Minimal sufficient statistics have a long history in the field of statistics (Lehmann & Scheffe, 1950; Dynkin, 1951) . But the minimality condition (3, above) is perhaps too strong to be useful in machine learning, since it is a statement about any function g, rather than about functions in a practical hypothesis class like the class of deep neural networks. Instead, in this work we consider minimal achievable sufficient statistics: sufficient statistics that are minimal among some particular set of functions. Definition 1 (Minimal Achievable Sufficient Statistic). Let Z = f (X) be a sufficient statistic of X for Y . Z is minimal achievable with respect to a set of functions F if f ∈ F and for any Lipschitz continuous, non-invertible function g where g • f ∈ F, g(Z) is no longer sufficient for Y .",
  "body": "Introduction The representation learning approach to machine learning focuses on finding a representation Z of an input random variable X that is useful for predicting a random variable Y (Goodfellow et al., 2016) . What makes a representation Z \"useful\" is much debated, but a common assertion is that Z should be a minimal sufficient statistic of X for Y (Adragni, Kofi P. & Cook, R. Dennis, 2009; Shamir et al., 2010; James et al., 2017; Achille & Soatto, 2018b) . That is: 1. Z should be a statistic of X. This means Z = f (X) for some function f . 2. Z should be sufficient for Y . This means p(X|Z, Y ) = p(X|Z). 3. Given that Z is a sufficient statistic, it should be minimal with respect to X. This means for any measurable, non-invertible function g, g(Z) is no longer sufficient for Y . foot_0 In other words: a minimal sufficient statistic is a random variable Z that tells you everything about Y you could ever care about, but if you do any irreversible processing to Z, you are guaranteed to lose some information about Y . Minimal sufficient statistics have a long history in the field of statistics (Lehmann & Scheffe, 1950; Dynkin, 1951) . But the minimality condition (3, above) is perhaps too strong to be useful in machine learning, since it is a statement about any function g, rather than about functions in a practical hypothesis class like the class of deep neural networks. Instead, in this work we consider minimal achievable sufficient statistics: sufficient statistics that are minimal among some particular set of functions. Definition 1 (Minimal Achievable Sufficient Statistic). Let Z = f (X) be a sufficient statistic of X for Y . Z is minimal achievable with respect to a set of functions F if f ∈ F and for any Lipschitz continuous, non-invertible function g where g • f ∈ F, g(Z) is no longer sufficient for Y . Contributions: • We introduce Conserved Differential Information (CDI), an information-theoretic quantity that, unlike mutual information, is meaningful for deterministically-dependent continuous random variables, such as the input and output of a deep network. • We introduce Minimal Achievable Sufficient Statistic Learning (MASS Learning), a training objective based on CDI for finding minimal achievable sufficient statistics. • We provide empirical evidence that models trained by MASS Learning achieve competitive performance on supervised learning and uncertainty quantification benchmarks. Conserved Differential Information Before we present MASS Learning, we need to introduce Conserved Differential Information (CDI), on which MASS Learning is based. CDI is an information-theoretic quantity that addresses an oft-cited issue in machine learning (Bell & Sejnowski, 1995; Amjad & Geiger, 2018; Saxe et al., 2018; Nash et al., 2018; Goldfeld et al., 2018) , which is that for a continuous random variable X and a continuous, non-constant function f , the mutual information I(X, f (X)) is infinite. (See Supplementary Material 7.2 for details.) This makes I(X, f (X)) unsuitable for use in a learning objective when f is, for example, a standard deep network. The infinitude of I(X, f (X)) has been circumvented in prior works by two strategies. One is discretize X and f (X) (Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017) , though this is controversial (Saxe et al., 2018) . Another is to use a random variable Z with distribution p(Z|X) as the representation of X rather than using f (X) itself as the representation (Alemi et al., 2017; Kolchinsky et al., 2017; Achille & Soatto, 2018b) . In this latter approach, p(Z|X) is usually implemented by adding noise to a deep network that takes X as input. These are both reasonable strategies for avoiding the infinitude of I(X, f (X)). But another approach would be to derive a new information-theoretic quantity that is better suited to this situation. To that end we present Conserved Differential Information: Definition 2. For a continuous random variable X taking values in R d and a Lipschitz continuous function f : R d → R r , the Conserved Differential Information (CDI) is C(X, f (X)) := H(f (X)) -E X [log (J f (X))] (1) where H denotes the differential entropy H(Z) = -p(z) log p(z) dz and J f is the Jacobian determinant of f J f (x) = det ∂f (x) ∂x T ∂f (x) ∂x T T with ∂f (x) ∂x T ∈ R r×d the Jacobian matrix of f at x. Readers familiar with normalizing flows (Rezende & Mohamed, 2015) or Real NVP (Dinh et al., 2017) will note that the Jacobian determinant used in those methods is a special case of the Jacobian determinant in the definition of CDI. This is because normalizing flows and Real NVP are based on the change of variables formula for invertible mappings, while CDI is based in part on the more general change of variables formula for non-invertible mappings. More details on this connection are given in Supplementary Material 7.3. The mathematical motivation for CDI based on the recent work of Koliander et al. (2016) is provided in Supplementary Material 7.4. Figure 1 gives a visual example of what CDI measures about a function. 0 1 1 X p(X) 0 ½ 2 f 1 (X) p(f 1 (X)) f 1 (X) = ½ X f 2 (X) = ┃X -½┃ 0 ½ 2 f 2 (X) p(f 2 (X)) C(X, f 1 (X)) = 0 C(X, f 2 (X)) = -log 2 Figure 1 . CDI of two functions f1 and f2 of the random variable X. Even though the random variables f1(X) and f2(X) have the same distribution, C(X, f1(X)) is different from C(X, f2(X)). This is because f1 is an invertible function, while f2 is not. CDI quantifies, roughly speaking, \"how non-invertible\" f2 is. The conserved differential information C(X, f (X)) between continuous, deterministically-dependent random variables behaves much like mutual information does between discrete random variables. For example, when f is invertible, C(X, f (X)) = H(X), just like with the mutual information between discrete random variables. Most importantly for our purposes, though, C(X, f (X)) obeys the following data processing inequality: Theorem 1 (CDI Data Processing Inequality). For Lipschitz continuous functions f and g with the same output space, C (X, f (X)) ≥ C (X, g(f (X))) with equality if and only if g is invertible almost everywhere. The proof is in Supplementary Material 7.5. MASS Learning With CDI and its data processing inequality in hand, we can give the following optimization-based characterization of minimal achievable sufficient statistics: Theorem 2. Let X be a continuous random variable, Y be a discrete random variable, and F be any set of Lipschitz continuous functions with a common output space (e.g., different parameter settings of a deep network). If f ∈ arg min S∈F C(X, S(X)) s.t. I(S(X), Y ) = max S I(S (X), Y ) then f (X) is a minimal achievable sufficient statistic of X for Y with respect to F. Proof. First note the following lemma (Cover & Thomas, 2006) . Lemma 1. Z = f (X) is a sufficient statistic for a discrete random variable Y if and only if I(Z, Y ) = max S I(S (X), Y ). Lemma 1 guarantees that any f satisfying the conditions in Theorem 2 is sufficient. Suppose such an f was not minimal achievable. Then by Definition 1 there would exist a non-invertible, Lipschitz continuous g such that g(f (X)) was sufficient. But by Theorem 1, it would then also be the case that C(X, g(f (X))) < C(X, f (X)), which would contradict f minimizing C(X, S(X)). We can turn Theorem 2 into a learning objective over functions f by relaxing the strict constraint into a Lagrangian formulation with Lagrange multiplier 1/β for β > 0: C(X, f (X)) - 1 β I(f (X), Y ) The larger the value of β, the more our objective will encourage minimality over sufficiency. We can then simplify this formulation using the identity I(f (X), Y ) = H(Y ) -H(Y |f (X)) , which gives us the following optimization objective: L M ASS (f ) := H(Y |f (X)) + βH(f (X)) -βE X [log J f (X)]. (2) We refer to minimizing this objective as MASS Learning. Practical implementation In practice, we are interested in using MASS Learning to train a deep network f θ with parameters θ using a finite dataset {(x i , y i )} N i=1 of N datapoints sampled from the joint distribution p(x, y) of X and Y . To do this, we introduce a parameterized variational approximation q φ (f θ (x)|y) ≈ p(f θ (x)|y). Using q φ , we minimize the following empirical upper bound to L M ASS : L M ASS ≤ L M ASS (θ, φ) := 1 N N i=1 -log q φ (y i |f θ (x i )) -β log q φ (f θ (x i )) -β log J f θ (x i ), where the quantity q φ (f θ (x i )) is computed as y q φ (f θ (x i )|y)p(y) and the quantity q φ (y i |f θ (x i )) is computed with Bayes rule as q φ (f θ (xi)|yi)p(yi) y q φ (f θ (xi)|y)p(y) . When Y is discrete and takes on finitely many values, as in classification problems, and when we choose a variational distribution q φ that is differentiable with respect to φ (e.g. a multivariate Gaussian), then we can minimize L M ASS (θ, φ) using stochastic gradient descent (SGD). To perform classification using our trained network, we use the learned variational distribution q φ and Bayes rule: p(y i |x i ) ≈ p(y i |f θ (x i )) ≈ q φ (f θ (x i )|y i )p(y i ) y q φ (f θ (x i )|y)p(y) . Computing the J f θ term in L M ASS for every sample in an SGD minibatch is too expensive to be practical. For f θ : R d → R r , doing so would require on the order of r times more operations than in standard training of deep networks by, since computing the J f θ term involves computing the full Jacobian matrix of the network, which, in our implementation, involves performing r backpropagations. Thus to make training tractable, we use a subsampling strategy: we estimate the J f θ term using only a 1/r fraction of the datapoints in a minibatch. In practice, we have found this subsampling strategy to not noticeably alter the numerical value of the J f θ term during training. Subsampling for the J f θ term results in a significant training speedup, but it must nevertheless be emphasized that, even with subsampling, our implementation of MASS Learning is roughly eight times as slow as standard deep network training. (Unless β = 0, in which case the speed is the same.) This is by far the most significant drawback of (our implementation of) MASS Learning. There are many easierto-compute upper bounds or estimates of J f θ that one could use to make MASS Learning faster, and one could also potentially find non-invertible network architectures which admit more efficiently computable Jacobians, but we do not explore these options in this work. Related Work Connection to the Information Bottleneck The well-studied Information Bottleneck learning method (Tishby et al., 2000; Tishby & Zaslavsky, 2015; Strouse & Schwab, 2015; Alemi et al., 2017; Saxe et al., 2018; Amjad & Geiger, 2018; Goldfeld et al., 2018; Kolchinsky et al., 2019; Achille & Soatto, 2018b; a) is based on minimizing the Information Bottleneck Lagrangian L IB (Z) := βI(X, Z) -I(Y, Z) for β > 0, where Z is the representation whose conditional distribution p(Z|X) one is trying to learn. The L IB learning objective can be motivated based on pure information-theoretic elegance. But some works like (Shamir et al., 2010) also point out the connection between the L IB objective and minimal sufficient statistics, which is based on the following theorem: Theorem 3. Let X be a discrete random variable drawn according to a distribution p(X|Y ) determined by the discrete random variable Y . Let F be the set of deterministic functions of X to any target space. Then f (X) is a minimal sufficient statistic of X for Y if and only if f ∈ arg min S∈F I(X, S(X)) s.t. I(S(X), Y ) = max S ∈F I(S (X), Y ). The L IB objective can then be thought of as a Lagrangian relaxation of the optimization problem in this theorem. Theorem 3 only holds for discrete random variables. For continuous X it holds only in the reverse direction, so minimizing L IB for continuous X has no formal connection to finding minimal sufficient statistics, not to mention minimal achievable sufficient statistics. See Supplementary Material 7.6 for details. Nevertheless, the optimization problems in Theorem 2 and Theorem 3 are extremely similar, relying as they both do on Lemma 1 for their proofs. And the idea of relaxing the optimization problem in Theorem 2 into a Lagrangian formulation to get L M ASS is directly inspired by the Information Bottleneck. So while MASS Learning and Information Bottleneck learning entail different network architectures and loss functions, there is an Information Bottleneck flavor to MASS Learning. Jacobian Regularization The presence of the J f θ term in L M ASS is reminiscent of the contrastive autoencoder (Rifai et al., 2011) and Jacobian Regularization literature (Sokolic et al., 2017; Ross & Doshi-Velez, 2018; Varga et al., 2017; Novak et al., 2018; Jakubovitz & Giryes, 2018) . Both these literatures suggest that minimizing E X [ D f (X) F ], where D f (x) = ∂f (x) ∂x T ∈ R r×d is the Jacobian matrix, seems to improve generalization and adversarial robustness. This may seem paradoxical at first, since by applying the AM-GM inequality to the eigenvalues of D f (x)D f (x) T we have E X [ D f (X) 2r F ] = E X [Tr(D f (X)D f (X) T ) r ] ≥ E X [r r det(D f (X)D f (X) T )] = E X [r r J f (X) 2 ] ≥ log E X [r r J f (X) 2 ] ≥ 2E X [log J f (X)] + r log r and E X [log J f (X) ] is being maximized by L M ASS . So L M ASS might seem to be optimizing for worse generalization according to the Jacobian regularization literature. However, the entropy term in L M ASS strongly encourages minimizing E X [ D f (X) F ]. So overall L M ASS seems to be seeking the right balance of sensitivity (dependent on the value of β) in the network to its inputs, which is precisely in alignment with what the Jacobian regularization literature suggests. Experiments In this section we compare MASS Learning to other approaches for training deep networks. Code to reproduce all experiments is available online. foot_1 Full details on all experiments is in Supplementary Material 7.7. We use the abbreviation \"SoftmaxCE\" to refer to the standard approach of training deep networks for classification problems by minimizing the softmax cross entropy loss L Sof tmaxCE (θ) := - 1 N N i=1 log softmax(f θ (x i )) yi where softmax(f θ (x i )) yi is the y i th element of the softmax function applied to the outputs f θ (x i ) of the network's last linear layer. As usual, softmax(f θ (x i )) yi is taken to be the network's estimate of p(y i |x i ). We also compare against the Variational Information Bottleneck method (Alemi et al., 2017) for representation learning, which we abbreviate as \"VIB\". We use two networks in our experiments. \"SmallMLP\" is a feedforward network with two fully-connected layers of 400 and 200 hidden units, respectively, both with elu nonlinearities (Clevert et al., 2015) . \"ResNet20\" is the 20layer residual network of He et al. (2016) . We performed all experiments on the CIFAR-10 dataset (Krizhevsky, 2009) and implemented all experiments using PyTorch (Paszke et al., 2017) . Classification Accuracy and Regularization We first confirm that networks trained by MASS Learning can make accurate predictions in supervised learning tasks. We compare the classification accuracy of networks trained on varying amounts of data to see the extent to which MASS Learning regularizes networks. Classification accuracies for the SmallMLP network are shown in Table 1 , and for the ResNet20 network in Table 2 . For the SmallMLP network, MASS Learning performs slightly worse than SoftmaxCE and VIB training. For the .2 ± 0.9 MASS, β=1e-2, D 29.3 ± 1.2 41.7 ± 0.4 52.0 ± 0.6 MASS, β=1e-3, D 31.5 ± 0.6 43.7 ± 0.2 53.1 ± 0.4 MASS, β=1e-4, D 32.7 ± 0.8 43.4 ± 0.5 53.2 ± 0.1 MASS, β=0, D 32.2 ± 1.1 43.9 ± 0.4 52.7 ± 0.0 larger ResNet20 network, MASS Learning performs equivalently to the other methods. It is notable that with the ResNet20 network VIB and MASS Learning both perform well when β = 0, and neither perform significantly better than SoftmaxCE. This may be because the hyperparameters used in training the ResNet20 network, which were taken directly from the original paper (He et al., 2016) , are specifically tuned for SoftmaxCE training and are more sensitive to the specifics of the network architecture than to the loss function. Uncertainty Quantification We also evaluate the ability of networks trained by MASS Learning to properly quantify their uncertainty about their predictions. We assess uncertainty quantification in two ways: using proper scoring rules (Lakshminarayanan et al., 2017) , which are scalar measures of how well a network's predictive distribution p(y|f θ (x)) is calibrated, and by assessing performance on an out-of-distribution (OOD) detection task. Tables 3 through 8 show the uncertainty quantification performance of networks according to two proper scoring rules: the Negative Log Likelihood (NLL) and the Brier Score. The entropy and test accuracy of the predictive distributions are also given, for reference.  and 7 , MASS Learning provides the best combination of accuracy and proper scoring rule performance, though its performance falters when trained on only 2,500 datapoints in Table and 8 . These ResNet20 UQ results also show the trend that MASS Learning with larger β leads to better calibrated network predictions. Thus, as measured by proper scoring rules, MASS Learning can significantly improve the calibration of a network's predictions while maintaining the same accuracy. Tables 9 through 14 show metrics for performance on an OOD detection task where the network predicts not just the class of the input image, but whether the image is from its training distribution (CIFAR-10 images) or from another distribution (SVHN images (Netzer et al., 2011) ). Following Hendrycks & Gimpel (2017) and Alemi et al. (2018) , the metrics we report for this task are the Area under the ROC curve (AUROC) and Average Precision score (APR). APR depends on whether the network is tasked with identifying in-distribution or out-of-distribution images; we report values for both cases as APR In and APR Out, respectively. There are different detection methods that networks can use to identify OOD inputs. One way, applicable to all training methods, is to use the entropy of the predictive distribution p(y|f θ (x)): larger entropy suggests the input is OOD. For networks trained by MASS Learning, the variational distribution q φ (f θ (x)|y) is a natural OOD detector: a small value of max i q φ (f θ (x)|y i ) suggests the input is OOD. For networks trained by SoftmaxCE, a distribution q φ (f θ (x)|y) can be learned by MLE on the training set and used to detect OOD inputs in the same way. For both the SmallMLP network in Tables 9, 10, and 11 and the ResNet20 network in Tables 12 , 13 , and 14, MASS Learning performs comparably or better than SoftmaxCE and VIB. However, one should note that MASS Learning with β = 0 gives performance not significantly different to MASS Learning with β = 0 on these OOD tasks, which suggests that the good performance of MASS Learning may be due to its use of a variational distribution to produce predictions, rather than to the overall MASS Learning training scheme. 5.3. Does MASS Learning finally solve the mystery of why stochastic gradient descent with the cross entropy loss works so well in deep learning? We do not believe so. Figure 2 shows how the values of the three terms in L M ASS change as the SmallMLP network trains on the CIFAR-10 dataset using either the SoftmaxCE training or MASS Learning. Despite achieving similar accuracies, the SoftmaxCE training method does not seem to be implicitly performing MASS Learning, based on the differing values of the entropy (orange) and Jacobian (green) terms between the two methods as training progresses. Discussion MASS Learning is a new approach to representation learning that performs well on classification accuracy, regularization, and uncertainty quantification benchmarks, despite not being directly formulated for any of these tasks. It shows particularly strong performance in improving uncertainty quantification. There are several potential ways to improve MASS Learning. Starting at the lowest level: it is likely that we did not manage to minimize L M ASS anywhere close to the extent possible in our experiments, given the minimal hyperparameter tuning we performed. In particular, we noticed that the initialization of the variational distribution played a large role in performance, but we were not able to fully explore it. Moving a level higher, it may be that we are effectively minimized L M ASS , but that L M ASS is not a useful empirical approximation or upper bound to L M ASS . This could be due to an insufficiently expressive variational distribution, or simply that the quantities in L M ASS require more data to approximate well than our datasets contained. At higher levels still, it may be the case that the Lagrangian formulation of Theorem 2 as L M ASS is impractical for finding minimal achievable sufficient statistics. Or it may be that the difference between minimal and minimal achievable sufficient statistics is relevant for performance on machine learning tasks. Or it may simply be that framing machine learning as a problem of finding minimal sufficient statistics is not productive. Finally, while we again note that more work is needed to reduce the computational cost of our implementation of MASS Learning, we believe the concept of MASS learning, and the concepts of minimal achievability and Conserved Differential Information we introduce along with it, are beneficial to the theoretical understanding of representation learning.   The most common phrasing of the definition of minimal sufficient statistic is: Definition 3 (Minimal Sufficient Statistic). A sufficient statistic f (X) for Y is minimal if for any other sufficient statistic h(X) there exists a measurable function g such that f = g • h almost everywhere. Some references do not explicitly mention the \"measurability\" and \"almost everywhere\" conditions on g, but since we are in the probabilistic setting it is this definition of f = g • h that is meaningful. Our preferred phrasing of the definition of minimal sufficient statistic, which we use in our Introduction, is: Definition 4 (Minimal Sufficient Statistic). A sufficient statistic f (X) for Y is minimal if for any measurable function g, g(f (X)) is no longer sufficient for Y unless g is invertible almost everywhere (i.e. there exist a measurable function g -1 and a set A such that g -1 (g(x)) = x for all x ∈ A and the event {X ∈ A c } has probability zero). The equivalence of Definition 3 and Definition 4 is given by the following lemma: Lemma 2. Assume that there exists a minimal sufficient statistic h(X) for Y by Definition 3. Then a sufficient statistic f (X) is minimal in the sense of Definition 3 if and only if it is minimal in the sense of Definition 4. Proof. We first assume that f (X) is minimal in the sense of Definition 3. Let g be any measurable function such that g(f (X)) is sufficient for Y . By the minimality (Def. 3) of f there must exist a measurable function g such that g(g(f (x))) = f (x) almost everywhere. This proves that f is minimal in the sense of Definition 4. Now assume that f (X) is minimal in the sense of Definition 4 and let f (X) be another sufficient statistic. Because h is minimal (Def. 3), there exist g 1 such that h = g 1 • f almost everywhere and g 2 such that h = g 2 • f almost everywhere. Because f is minimal (Def. 4), g 2 must be one-to-one almost everywhere, i.e. there exists a g2 such that g2 • h = g2 • g 2 • f = f almost everywhere. In turn, we obtain that g2 • g 1 • f = f almost everywhere, and since f was arbitrary this proves the minimality of f in the sense of Definition 3. The Mutual Information Between the Input and Output of a Deep Network is Infinite Typically the mutual information between continuous random variables X and Y is given by I(X, Y ) = p(x, y) log p(x, y) p(x)p(y) dxdy, but this quantity is only defined when the joint density p(x, y) is integrable, which it is not in the case that Y = f (X). (The technical term for p(x, y) in this case is a \"singular distribution\".) Instead, to compute I(X, f (X)) we must refer to the \"master definition\" of mutual information (Cover & Thomas, 2006) , which is I(X, Y ) = sup P,Q I([X] P , [Y ] Q ), (3) where P and Q are finite partitions of the range of X and Y , respectively, and [X] P is the random variable obtained by quantizing X using partition P, and analogously for [Y ] Q . From this definition, we can prove the following Lemma: This includes all X and Y where Y = f (X) for an f that is continuous somewhere on its domain, e.g., any deep network (considered as a function from an input vector to an output vector). Proof. Suppose X and Y satisfy the conditions of the lemma. Let O X and O Y be open sets with f (O X ) = O Y and P[X ∈ O X ] =: δ > 0, which exist by the lemma's assumptions. Then let P n O Y be a partition of O Y into n disjoint sets. Because Y is continuous and hence does not have any atoms, we may assume that the probability of Y belonging to each element of P n O Y is equal to the same nonzero value δ/n. Denote by P n O X the partition of O X into n disjoint sets, where each set in P n O X is the preimage of one of the sets in P n O Y . We can construct partitions of the whole domains of X and Y as P n O X ∪ O c X and P n O Y ∪ O c Y , respectively. Using these partitions in (3), we obtain I(X, Y ) ≥ (1 -δ) log(1 -δ) + A∈[X] P n O X P[X ∈ A, Y ∈ f (A)] log P[X ∈ A, Y ∈ f (A)] P[X ∈ A]P[Y ∈ f (A)] = (1 -δ) log(1 -δ) + n δ n log δ n δ n δ n = (1 -δ) log(1 -δ) + δ log n δ . By letting n go to infinity, we can see that the supremum in Eq. 3 is infinity. The Change of Variables Formula for Non-invertible Mappings The change of variables formula is widely used in machine learning and is key to recent results in density estimation and generative modeling like normalizing flows (Rezende & Mohamed, 2015) , NICE (Dinh et al., 2014) , and Real NVP (Dinh et al., 2017) . But all uses of the change of variables formula in the machine learning literature that we are aware of use it with respect to bijective mappings between random variables, despite the formula also being applicable to non-invertible mappings between random variables. To address this gap, we offer the following brief tutorial. The familiar form of the change of variables formula for a random variable X with density p(x) and a bijective, differentiable function f : R d → R d is R d p(x)J f (x) dx = R d p(f -1 (y)) dy. (4) where J f (x) = det ∂f (x) ∂x T . A slightly more general phrasing of Equation 4 is f -1 (B) g(x)J f (x) dx = B g(f -1 (y)) dy. (5) where g : R d → R is any non-negative measurable function, and B ⊆ R d is any measurable subset of R d . We can extend Equation 5 to work in the case that f is not invertible. To do this, we must address two issues. First, if f is not invertible, then f -1 (y) is not a single point but rather a set. Second, if f is not invertible, then the Jacobian matrix ∂f (x) ∂x T may not be square, and thus has no well defined determinant. Both issues can be resolved and lead to the following change of variables theorem (Krantz & Parks, 2009) , which is based on the so-called coarea formula (Federer, 1969) . Theorem 4. Let f : R d → R r with r ≤ d be a differentiable function, g : R d → R a non-negative measurable function, B ⊆ R d a measurable set, and J f (x) = det ∂f (x) ∂x T ∂f (x) ∂x T T . Then f -1 (B) g(x)J f (x) dx = B f -1 (y) g(x) dH d-r (x) dy. ( 6 ) where H d-r is the (d -r)-dimensional Hausdorff measure (one can think of this as a measure for lower-dimensional structures in high-dimensional space, e.g. the area of 2-dimensional surfaces in 3-dimensional space). foot_2 We see in Theorem 4 that Equation 6 looks a lot like Equations 4 and 5, but with f -1 (y) replaced by an integral over the set f -1 (y), which for almost every y is a (d -r)-dimensional set. And if f in Equation 6 happens to be bijective, Equation 6 reduces to Equation 5 . We also see that the Jacobian determinant in Equation 5 was replaced by the so-called r-dimensional Jacobian det ∂f (x) ∂x T ∂f (x) ∂x T T in Equation 6 . A word of caution is in order, as the r-dimensional Jacobian does not have the same nice properties for concatenated functions as does the Jacobian in the bijective case. In particular, we cannot calculate J f2•f1 based on the values of J f1 and J f2 because the product ∂f2 (x) ∂x T ∂f1(x) ∂x T ∂f2(x) ∂x T ∂f1(x) ∂x T T does not decompose into a product of ∂f2(x) ∂x T ∂f2(x) ∂x T T and ∂f1(x) ∂x T ∂f1(x) ∂x T T . In other words, the trick used in techniques like normalizing flows and NICE to compute determinants of deep networks for use in the change of variables formula by decomposing the network's Jacobian into the product of layerwise Jacobians does not work straightforwardly in the case of non-invertible mappings. Motivation for Conserved Differential Information First, we present an alternative definition of conditional entropy that is meaningful for singular distributions (e.g., the joint distribution p(X, f (X)) for a function f ). More information on this definition can be found in Koliander et al. (2016) . SINGULAR CONDITIONAL ENTROPY Assume that the random variable X has a probability density function p X (x) on R d . For a given differentiable function f : R d → R r (r ≤ d), we want to analyze the conditional differential entropy H(X|f (X)). Following Koliander et al. (2016) , we define this quantity as: H(X|f (X)) = - R r p f (X) (y) f -1 (y) θ d-r Pr{X∈•|f (X)=y} (x) log θ d-r Pr{X∈•|f (X)=y} (x) dH d-r (x) dy (7) where H d-r denotes (d -r)-dimensional Hausdorff measure. The function p f (X) is the probability density function of the random variable f (X). Although θ d-r Pr{X∈•|f (X)=y} can also be interpreted as a probability density, it is not the commonly used density with respect to Lebesgue measure (which does not exist for X|f (X) = y) but a density with respect to a lower-dimensional Hausdorff measure. We will analyze the two functions p f (X) and θ d-r Pr{X∈•|f (X)=y} in more detail. The density p f (X) is defined by the relation f -1 (B) p X (x) dx = B p f (X) (y) dy , (8) which has to hold for every measurable set B ⊆ R r . Using the coarea formula (or the related change-of-variables theorem), we see that f -1 (B) p X (x) dx = B f -1 (y) p X (x) J f (x) dH d-r (x) dy , (9) where J f (x) = det ∂f (x) ∂x T ∂f (x) ∂x T T is the r-dimensional Jacobian determinant. Thus, we identified p f (X) (y) = f -1 (y) p X (x) J f (x) dH d-r (x) . ( 10 ) The second function, namely θ d-r Pr{X∈•|f (X)=y} , is the Radon-Nikodym derivative of the conditional probability Pr{X ∈ •|f (X) = y} with respect to H d-r restricted to the set where X|f (X) = y has positive probability (in the end, this will be the set f -1 (y)). To understand this function, we have to know something about the conditional distribution of X given f (X). Formally, a (regular) conditional probability Pr{X ∈ •|f (X) = y} has to satisfy three conditions: • Pr{X ∈ •|f (X) = y} is a probability measure for each fixed y ∈ R r . • Pr{X ∈ A|f (X) = •} is measurable for each fixed measurable set A ⊆ R d . • For measurable sets A ⊆ R d and B ⊆ R r , we have Pr{(X, f (X)) ∈ A × B} = B Pr{X ∈ A|f (X) = y}p f (X) (y) dy . (11) In our setting, (11) becomes where the final equality is again an application of the coarea formula. Thus, we identified θ d-r Pr{X∈•|f (X)=y} (x) = p X (x) J f (x) p f (X) (y) . (15) Although things might seem complicated up to this point, they simplify significantly once we put everything together. In particular, inserting (15) into (7), we obtain H(X|f (X)) = - R r p f (X) (y) f -1 (y) p X (x) J f (x) p f (X) (y) log p X (x) J f (x) p f (X) (y) dH d-r (x) dy = -R r f -1 (y) p X (x) J f (x) log p X (x) J f (x) p f (X) (y) dH d-r (x) dy = - R d p X (x) log p X (x) J f (x) p f (X) (f (x)) dx (16) = H(X) + R d p X (x) log J f (x)p f (X) (f (x)) dx = H(X) + R d p X (x) log p f (X) (f (x)) dx + R d p X (x) log J f (x) dx = H(X) + R r f -1 (y) p X (x) J f (x) log p f (X) (f (x)) dH d-r (x) dy + E log J f (X) (17) = H(X) + R r f -1 (y) p X (x) J f (x) dH d-r (x) log p f (X) (y) dy + E log J f (X) = H(X) + R r p f (X) (y) log p f (X) (y) dy + E log J f (X) = H(X) -H(f (X)) + E log J f (X) (18) where ( 16 ) and ( 17 ) hold by the coarea formula. So, altogether we have that for a random variable X and a function f , the singular conditional entropy between X and f (X) is H(X|f (X)) = H(X) -H(f (X)) + E log J f (X) . This quantity can loosely be interpreted as being the difference in differential entropies between X and f (X) but with an additional term that corrects for any \"uninformative\" scaling that f does. Figure 2 . 2 Figure 2. Estimated value of each term in the MASS Learning loss function, LMASS(f ) = H(Y |f (X))+βH(f (X))-βEX [log J f (X)],during training of the SmallMLP network on the CIFAR-10 dataset. The MASS training was performed with β = 0.001, though the plotted values are for the terms without being multiplied by the β coefficients. The values of these terms for SoftmaxCE training are estimated using a distribution q φ (f θ (x)|y), with the distribution parameters φ being estimated at each training step by MLE over the training data. Lemma 3 . 3 If X and Y are continuous random variables, and there are open sets O X and O Y in the support of X and Y , respectively, such that y = f (x) for x ∈ O X and y ∈ O Y , then I(X, Y ) = ∞. A∩f - 1 1 (B) p X (x) dx = B Pr{X ∈ A|f (X) = y}p f (X) (y) dy . (12) Choosing Pr{X ∈ A|f (X) = y} = 1 p f (X) (y) A∩f -1 (y) p X (x) J f (x) dH d-r (x) ,(13)the right-hand side in (12) becomesB Pr{X ∈ A|f (X) = y}p f (X) (y) dy = B A∩f -1 (y) p X (x) J f (x) dH d-r (x) dy = A∩f -1 (B)p X (x) dx , Table 1 . 1 Test-set classification accuracy (percent) on CIFAR-10 dataset using the SmallMLP network trained by various methods. Full experiment details are in Supplementary Material 7.7. Values are the mean classification accuracy over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened accuracies are those for which the maximum observed mean accuracy in the column was within one standard deviation. WD is weight decay; D is dropout. METHOD 2500 TRAINING SET SIZE 10,000 40,000 SoftmaxCE 34.2 ± 0.8 44.6 ± 0.6 52.7 ± 0.4 SoftmaxCE, WD 23.9 ± 0.9 36.4 ± 0.9 48.1 ± 0.1 SoftmaxCE, D 33.7 ± 1.1 44.1 ± 0.6 53.7 ± 0.3 VIB, β=1e-1 32.2 ± 0.6 40.6 ± 0.4 46.1 ± 0.5 VIB, β=1e-2 34.6 ± 0.4 43.8 ± 0.8 51.9 ± 0.8 VIB, β=1e-3 35.6 ± 0.5 44.6 ± 0.6 51.8 ± 0.8 VIB, β=1e-1, D 29.0 ± 0.6 40.1 ± 0.5 49.5 ± 0.5 VIB, β=1e-2, D 32.5 ± 0.9 43.9 ± 0.3 53.6 ± 0.3 VIB, β=1e-3, D 34.5 ± 1.0 44.4 ± 0.4 54.3 ± 0.2 MASS, β=1e-2 29.6 ± 0.4 39.9 ± 1.2 46.3 ± 1.2 MASS, β=1e-3 32.7 ± 0.8 41.5 ± 0.7 47.8 ± 0.8 MASS, β=1e-4 34.0 ± 0.3 41.5 ± 1.1 47.9 ± 0.8 MASS, β=0 34.1 ± 0.6 42.0 ± 0.6 48 Table 2 . 2 Test-set classification accuracy (percent) on CIFAR-10 dataset using the ResNet20 network trained by various methods. No data augmentation was used -full details in Supplementary Material 7.7. Values are the mean classification accuracy over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened accuracies are those for which the maximum observed mean accuracy in the column was within one standard deviation. METHOD 2500 TRAINING SET SIZE 10,000 40,000 SoftmaxCE 50.0 ± 0.7 67.5 ± 0.8 81.7 ± 0.3 VIB, β=1e-3 49.5 ± 1.1 66.9 ± 1.0 81.0 ± 0.3 VIB, β=1e-4 49.4 ± 1.0 66.4 ± 0.5 81.2 ± 0.4 VIB, β=1e-5 50.0 ± 1.1 67.9 ± 0.8 80.9 ± 0.5 VIB, β=0 50.6 ± 0.8 67.1 ± 1.0 81.5 ± 0.2 MASS, β=1e-3 38.2 ± 0.7 59.6 ± 0.8 75.8 ± 0.5 MASS, β=1e-4 49.9 ± 1.0 66.6 ± 0.4 80.6 ± 0.5 MASS, β=1e-5 50.1 ± 0.5 67.4 ± 1.0 81.6 ± 0.4 MASS, β=0 50. 2 ± 1.0 67.4 ± 0.3 81.5 ± 0.2 For the SmallMLP network in Tables3, 4, and 5 , VIB provides the best combination of high accuracy and low NLL and Brier score across all sizes of training set, despite SoftmaxCE with weight decay achieving the best scoring rule values. For the larger ResNet20 network in Tables 6 Table 3 . 3 Uncertainty quantification metrics (proper scoring rules) on CIFAR-10 using the SmallMLP network trained on 40,000 datapoints. Test Accuracy and Entropy of the network's predictive distribution are given for reference. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the minimum observed mean value in the column was within one standard deviation. WD is weight decay; D is dropout. Lower values are better. Method Test Accuracy Entropy NLL Brier Score SoftmaxCE 52.7 ± 0.4 0.211 ± 0.003 4.56 ± 0.07 0.0840 ± 0.0005 SoftmaxCE, WD 48.1 ± 0.1 1.500 ± 0.009 1.47 ± 0.01 0.0660 ± 0.0003 SoftmaxCE, D 53.7 ± 0.3 0.606 ± 0.005 1.79 ± 0.02 0.0681 ± 0.0005 VIB, β=1e-1 46.1 ± 0.5 0.258 ± 0.005 5.35 ± 0.15 0.0944 ± 0.0009 VIB, β=1e-2 51.9 ± 0.8 0.193 ± 0.004 5.03 ± 0.19 0.0861 ± 0.0015 VIB, β=1e-3 51.8 ± 0.8 0.174 ± 0.003 5.49 ± 0.20 0.0866 ± 0.0015 VIB, β=1e-1, D 49.5 ± 0.5 0.957 ± 0.005 1.62 ± 0.01 0.0660 ± 0.0003 VIB, β=1e-2, D 53.6 ± 0.3 0.672 ± 0.014 1.69 ± 0.01 0.0668 ± 0.0006 VIB, β=1e-3, D 54.3 ± 0.2 0.617 ± 0.007 1.75 ± 0.02 0.0677 ± 0.0005 MASS, β=1e-2 46.3 ± 1.2 0.203 ± 0.005 6.89 ± 0.16 0.0968 ± 0.0024 MASS, β=1e-3 47.8 ± 0.8 0.207 ± 0.004 5.89 ± 0.21 0.0935 ± 0.0017 MASS, β=1e-4 47.9 ± 0.8 0.212 ± 0.003 5.71 ± 0.16 0.0934 ± 0.0017 MASS, β=0 48.2 ± 0.9 0.208 ± 0.004 5.74 ± 0.20 0.0927 ± 0.0017 MASS, β=1e-2, D 52.0 ± 0.6 0.690 ± 0.013 1.85 ± 0.03 0.0694 ± 0.0005 MASS, β=1e-3, D 53.1 ± 0.4 0.649 ± 0.010 1.82 ± 0.04 0.0684 ± 0.0007 MASS, β=1e-4, D 53.2 ± 0.1 0.664 ± 0.020 1.79 ± 0.02 0.0680 ± 0.0002 MASS, β=0, D 52.7 ± 0.0 0.662 ± 0.003 1.82 ± 0.02 0.0690 ± 0.0003 Table 4 . 4 Uncertainty quantification metrics (proper scoring rules) on CIFAR-10 using the SmallMLP network trained on 10,000 datapoints. Test Accuracy and Entropy of the network's predictive distribution are given for reference. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the minimum observed mean value in the column was within one standard deviation. WD is weight decay; D is dropout. Lower values are better. Method Test Accuracy Entropy NLL Brier Score SoftmaxCE 44.6 ± 0.6 0.250 ± 0.004 5.33 ± 0.06 0.0974 ± 0.0011 SoftmaxCE, WD 36.4 ± 0.9 0.897 ± 0.033 2.44 ± 0.11 0.0905 ± 0.0019 SoftmaxCE, D 44.1 ± 0.6 0.379 ± 0.007 3.76 ± 0.04 0.0935 ± 0.0012 VIB, β=1e-1 40.6 ± 0.4 0.339 ± 0.011 4.86 ± 0.23 0.1017 ± 0.0016 VIB, β=1e-2 43.8 ± 0.8 0.274 ± 0.004 4.83 ± 0.16 0.0983 ± 0.0017 VIB, β=1e-3 44.6 ± 0.6 0.241 ± 0.004 5.50 ± 0.11 0.0983 ± 0.0005 VIB, β=1e-1, D 40.1 ± 0.5 0.541 ± 0.015 3.22 ± 0.09 0.0945 ± 0.0012 VIB, β=1e-2, D 43.9 ± 0.3 0.413 ± 0.009 3.43 ± 0.09 0.0927 ± 0.0011 VIB, β=1e-3, D 44.4 ± 0.4 0.389 ± 0.004 3.61 ± 0.06 0.0927 ± 0.0004 MASS, β=1e-2 39.9 ± 1.2 0.172 ± 0.008 10.06 ± 0.37 0.1109 ± 0.0020 MASS, β=1e-3 41.5 ± 0.7 0.197 ± 0.005 8.03 ± 0.28 0.1069 ± 0.0016 MASS, β=1e-4 41.5 ± 1.1 0.208 ± 0.008 7.55 ± 0.44 0.1054 ± 0.0023 MASS, β=0 42.0 ± 0.6 0.215 ± 0.009 7.21 ± 0.28 0.1043 ± 0.0015 MASS, β=1e-2, D 41.7 ± 0.4 0.399 ± 0.017 4.21 ± 0.17 0.0974 ± 0.0013 MASS, β=1e-3, D 43.7 ± 0.2 0.412 ± 0.010 3.71 ± 0.07 0.0930 ± 0.0006 MASS, β=1e-4, D 43.4 ± 0.5 0.435 ± 0.011 3.50 ± 0.05 0.0923 ± 0.0005 MASS, β=0, D 43.9 ± 0.4 0.447 ± 0.009 3.40 ± 0.03 0.0913 ± 0.0008 Table 5 . 5 Uncertainty quantification metrics (proper scoring rules) on CIFAR-10 using the SmallMLP network trained on 2,500 datapoints. Test Accuracy and Entropy of the network's predictive distribution are given for reference. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the minimum observed mean value in the column was within one standard deviation. WD is weight decay; D is dropout. Lower values are better. Method Test Accuracy Entropy NLL Brier Score SoftmaxCE 34.2 ± 0.8 0.236 ± 0.025 8.14 ± 0.84 0.1199 ± 0.0024 SoftmaxCE, WD 23.9 ± 0.9 0.954 ± 0.017 3.41 ± 0.07 0.1114 ± 0.0013 SoftmaxCE, D 33.7 ± 1.1 0.203 ± 0.006 9.68 ± 0.06 0.1219 ± 0.0013 VIB, β=1e-1 32.2 ± 0.6 0.247 ± 0.007 8.33 ± 0.50 0.1219 ± 0.0013 VIB, β=1e-2 34.6 ± 0.4 0.249 ± 0.004 7.36 ± 0.18 0.1175 ± 0.0005 VIB, β=1e-3 35.6 ± 0.5 0.217 ± 0.008 8.03 ± 0.37 0.1175 ± 0.0012 VIB, β=1e-1, D 29.0 ± 0.6 0.383 ± 0.011 6.32 ± 0.16 0.1219 ± 0.0010 VIB, β=1e-2, D 32.5 ± 0.9 0.260 ± 0.006 7.41 ± 0.25 0.1211 ± 0.0019 VIB, β=1e-3, D 34.5 ± 1.0 0.200 ± 0.002 9.44 ± 0.16 0.1203 ± 0.0020 MASS, β=1e-2 29.6 ± 0.4 0.047 ± 0.002 57.13 ± 1.60 0.1381 ± 0.0007 MASS, β=1e-3 32.7 ± 0.8 0.048 ± 0.004 46.40 ± 3.81 0.1322 ± 0.0018 MASS, β=1e-4 34.0 ± 0.3 0.052 ± 0.002 39.10 ± 1.96 0.1293 ± 0.0009 MASS, β=0 34.1 ± 0.6 0.061 ± 0.003 33.60 ± 1.34 0.1285 ± 0.0012 MASS, β=1e-2, D 29.3 ± 1.2 0.118 ± 0.008 20.51 ± 0.83 0.1349 ± 0.0018 MASS, β=1e-3, D 31.5 ± 0.6 0.145 ± 0.004 15.65 ± 0.71 0.1289 ± 0.0010 MASS, β=1e-4, D 32.7 ± 0.8 0.185 ± 0.010 11.21 ± 0.66 0.1245 ± 0.0011 MASS, β=0, D 32.2 ± 1.1 0.217 ± 0.008 9.70 ± 0.29 0.1236 ± 0.0021 Table 6 . 6 Uncertainty Method Test Accuracy Entropy NLL Brier Score SoftmaxCE 81.7 ± 0.3 0.087 ± 0.002 1.45 ± 0.04 0.0324 ± 0.0005 VIB, β=1e-3 81.0 ± 0.3 0.089 ± 0.003 1.51 ± 0.04 0.0334 ± 0.0005 VIB, β=1e-4 81.2 ± 0.4 0.092 ± 0.002 1.46 ± 0.05 0.0331 ± 0.0007 VIB, β=1e-5 80.9 ± 0.5 0.087 ± 0.005 1.58 ± 0.08 0.0339 ± 0.0008 VIB, β=0 81.5 ± 0.2 0.079 ± 0.001 1.70 ± 0.06 0.0331 ± 0.0007 MASS, β=1e-3 75.8 ± 0.5 0.139 ± 0.003 1.66 ± 0.07 0.0417 ± 0.0011 MASS, β=1e-4 80.6 ± 0.5 0.109 ± 0.002 1.33 ± 0.02 0.0337 ± 0.0008 MASS, β=1e-5 81.6 ± 0.4 0.095 ± 0.003 1.36 ± 0.03 0.0320 ± 0.0005 MASS, β=0 81.5 ± 0.2 0.092 ± 0.000 1.43 ± 0.04 0.0325 ± 0.0004 quantification metrics (proper scoring rules) on CIFAR-10 using the ResNet20 network trained on 40,000 datapoints. Test Accuracy and Entropy of the network's predictive distribution are given for reference. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the minimum observed mean value in the column was within one standard deviation. Lower values are better. Table 7 . 7 Uncertainty quantification metrics (proper scoring rules) on CIFAR-10 using the ResNet20 network trained on 10,000 datapoints. Test Accuracy and Entropy of the network's predictive distribution are given for reference. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the minimum observed mean value in the column was within one standard deviation. Lower values are better. Method Test Accuracy Entropy NLL Brier Score SoftmaxCE 67.5 ± 0.8 0.195 ± 0.011 2.19 ± 0.06 0.0557 ± 0.0012 VIB, β=1e-3 66.9 ± 1.0 0.193 ± 0.008 2.26 ± 0.13 0.0570 ± 0.0017 VIB, β=1e-4 66.4 ± 0.5 0.197 ± 0.009 2.30 ± 0.02 0.0577 ± 0.0007 VIB, β=1e-5 67.9 ± 0.8 0.166 ± 0.010 2.49 ± 0.13 0.0561 ± 0.0011 VIB, β=0 67.1 ± 1.0 0.162 ± 0.009 2.64 ± 0.11 0.0578 ± 0.0016 MASS, β=1e-3 59.6 ± 0.8 0.252 ± 0.007 2.61 ± 0.11 0.0688 ± 0.0014 MASS, β=1e-4 66.6 ± 0.4 0.209 ± 0.009 2.18 ± 0.05 0.0570 ± 0.0005 MASS, β=1e-5 67.4 ± 1.0 0.192 ± 0.007 2.22 ± 0.07 0.0561 ± 0.0017 MASS, β=0 67.4 ± 0.3 0.189 ± 0.004 2.30 ± 0.08 0.0562 ± 0.0007 Table 8 . 8 Uncertainty quantification metrics (proper scoring rules) on CIFAR-10 using the ResNet20 network trained on 2,500 datapoints. Test Accuracy and Entropy of the network's predictive distribution are given for reference. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the minimum observed mean value in the column was within one standard deviation. Lower values are better. Method Test Accuracy Entropy NLL Brier Score SoftmaxCE 50.0 ± 0.7 0.349 ± 0.005 2.98 ± 0.06 0.0833 ± 0.0012 VIB, β=1e-3 49.5 ± 1.1 0.363 ± 0.005 3.10 ± 0.11 0.0836 ± 0.0020 VIB, β=1e-4 49.4 ± 1.0 0.372 ± 0.016 3.02 ± 0.10 0.0833 ± 0.0016 VIB, β=1e-5 50.0 ± 1.1 0.306 ± 0.021 3.48 ± 0.15 0.0849 ± 0.0013 VIB, β=0 50.6 ± 0.8 0.271 ± 0.019 3.80 ± 0.15 0.0850 ± 0.0007 MASS, β=1e-3 38.2 ± 0.7 0.469 ± 0.012 3.75 ± 0.08 0.1010 ± 0.0017 MASS, β=1e-4 49.9 ± 1.0 0.344 ± 0.001 3.24 ± 0.08 0.0837 ± 0.0017 MASS, β=1e-5 50.1 ± 0.5 0.277 ± 0.008 3.81 ± 0.11 0.0859 ± 0.0005 MASS, β=0 50.2 ± 1.0 0.265 ± 0.009 3.96 ± 0.15 0.0861 ± 0.0020 Table 9 . 9 Out-of-distribution detection metrics for SmallMLP network trained on 40,000 CIFAR-10 images, with SVHN as the out-ofdistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the column was within one standard deviation. WD is weight decay; D is dropout. Higher values are better. Training Method Test Accuracy Detection Method AUROC APR In APR Out SoftmaxCE 52.7 ± 0.4 Entropy maxi q φ (f θ (x)|yi) 0.65 ± 0.01 0.38 ± 0.01 0.68 ± 0.01 0.42 ± 0.01 0.61 ± 0.01 0.43 ± 0.01 SoftmaxCE, WD 48.1 ± 0.1 Entropy maxi q φ (f θ (x)|yi) 0.65 ± 0.01 0.43 ± 0.01 0.69 ± 0.01 0.43 ± 0.01 0.59 ± 0.01 0.48 ± 0.02 SoftmaxCE, D 53.7 ± 0.3 Entropy maxi q φ (f θ (x)|yi) 0.71 ± 0.01 0.33 ± 0.00 0.75 ± 0.01 0.39 ± 0.00 0.65 ± 0.01 0.40 ± 0.00 VIB, β=1e-1 46.1 ± 0.5 Entropy Rate 0.62 ± 0.01 0.47 ± 0.02 0.66 ± 0.01 0.49 ± 0.01 0.57 ± 0.01 0.46 ± 0.01 VIB, β=1e-2 51.9 ± 0.8 Entropy Rate 0.64 ± 0.01 0.58 ± 0.03 0.67 ± 0.01 0.59 ± 0.02 0.59 ± 0.01 0.55 ± 0.02 VIB, β=1e-3 51.8 ± 0.8 Entropy Rate 0.65 ± 0.00 0.52 ± 0.03 0.67 ± 0.01 0.54 ± 0.03 0.61 ± 0.00 0.50 ± 0.03 VIB, β=1e-1, D 49.5 ± 0.5 Entropy Rate 0.68 ± 0.01 0.34 ± 0.01 0.74 ± 0.01 0.40 ± 0.01 0.60 ± 0.01 0.39 ± 0.00 VIB, β=1e-2, D 53.6 ± 0.3 Entropy Rate 0.69 ± 0.02 0.50 ± 0.03 0.73 ± 0.01 0.51 ± 0.02 0.62 ± 0.02 0.51 ± 0.03 VIB, β=1e-3, D 54.3 ± 0.2 Entropy Rate 0.69 ± 0.01 0.45 ± 0.01 0.73 ± 0.01 0.45 ± 0.01 0.62 ± 0.01 0.49 ± 0.01 MASS, β=1e-2 46.3 ± 1.2 Entropy maxi q φ (f θ (x)|yi) 0.64 ± 0.01 0.51 ± 0.03 0.67 ± 0.01 0.56 ± 0.05 0.61 ± 0.01 0.49 ± 0.01 MASS, β=1e-3 47.8 ± 0.8 Entropy maxi q φ (f θ (x)|yi) 0.63 ± 0.02 0.63 ± 0.07 0.65 ± 0.02 0.64 ± 0.08 0.60 ± 0.02 0.60 ± 0.05 MASS, β=1e-4 47.9 ± 0.8 Entropy maxi q φ (f θ (x)|yi) 0.63 ± 0.02 0.57 ± 0.06 0.65 ± 0.02 0.58 ± 0.05 0.60 ± 0.02 0.56 ± 0.05 MASS, β=0 48.2 ± 0.9 Entropy maxi q φ (f θ (x)|yi) 0.63 ± 0.02 0.58 ± 0.06 0.65 ± 0.02 0.58 ± 0.05 0.59 ± 0.02 0.56 ± 0.05 MASS, β=1e-2, D 52.0 ± 0.6 Entropy maxi q φ (f θ (x)|yi) 0.73 ± 0.01 0.75 ± 0.01 0.67 ± 0.01 0.65 ± 0.06 0.70 ± 0.06 0.58 ± 0.05 MASS, β=1e-3, D 53.1 ± 0.4 Entropy maxi q φ (f θ (x)|yi) 0.71 ± 0.02 0.64 ± 0.10 0.73 ± 0.01 0.66 ± 0.10 0.64 ± 0.02 0.60 ± 0.09 MASS, β=1e-4, D 53.2 ± 0.1 Entropy maxi q φ (f θ (x)|yi) 0.73 ± 0.01 0.75 ± 0.01 0.67 ± 0.01 0.65 ± 0.09 0.65 ± 0.08 0.61 ± 0.08 MASS, β=0, D 52.7 ± 0.0 Entropy maxi q φ (f θ (x)|yi) 0.71 ± 0.02 0.74 ± 0.01 0.65 ± 0.02 0.63 ± 0.09 0.65 ± 0.08 0.59 ± 0.09 Table 10 . 10 Out-of-distribution detection metrics for SmallMLP network trained on 10,000 CIFAR-10 images, with SVHN as the out-ofdistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the column was within one standard deviation. WD is weight decay; D is dropout. Higher values are better. Training Method Test Accuracy Detection Method AUROC APR In APR Out SoftmaxCE 44.6 ± 0.6 Entropy maxi q φ (f θ (x)|yi) 0.62 ± 0.00 0.36 ± 0.01 0.64 ± 0.01 0.40 ± 0.01 0.59 ± 0.00 0.42 ± 0.00 SoftmaxCE, WD 36.4 ± 0.9 Entropy maxi q φ (f θ (x)|yi) 0.62 ± 0.02 0.30 ± 0.01 0.62 ± 0.02 0.37 ± 0.00 0.60 ± 0.02 0.39 ± 0.01 SoftmaxCE, D 44.1 ± 0.6 Entropy maxi q φ (f θ (x)|yi) 0.66 ± 0.01 0.29 ± 0.01 0.69 ± 0.01 0.37 ± 0.00 0.62 ± 0.01 0.38 ± 0.00 VIB, β=1e-1 40.6 ± 0.4 Entropy Rate 0.60 ± 0.01 0.50 ± 0.02 0.64 ± 0.01 0.52 ± 0.02 0.56 ± 0.01 0.48 ± 0.01 VIB, β=1e-2 43.8 ± 0.8 Entropy Rate 0.62 ± 0.00 0.55 ± 0.03 0.64 ± 0.01 0.57 ± 0.02 0.59 ± 0.01 0.53 ± 0.02 VIB, β=1e-3 44.6 ± 0.6 Entropy Rate 0.62 ± 0.01 0.49 ± 0.04 0.64 ± 0.01 0.52 ± 0.04 0.59 ± 0.01 0.48 ± 0.03 VIB, β=1e-1, D 40.1 ± 0.5 Entropy Rate 0.62 ± 0.00 0.49 ± 0.02 0.65 ± 0.01 0.51 ± 0.02 0.57 ± 0.00 0.48 ± 0.01 VIB, β=1e-2, D 43.9 ± 0.3 Entropy Rate 0.67 ± 0.01 0.69 ± 0.01 0.60 ± 0.02 0.61 ± 0.02 0.62 ± 0.00 0.56 ± 0.01 VIB, β=1e-3, D 44.4 ± 0.4 Entropy Rate 0.67 ± 0.01 0.69 ± 0.01 0.50 ± 0.03 0.53 ± 0.03 0.63 ± 0.01 0.49 ± 0.02 MASS, β=1e-2 39.9 ± 1.2 Entropy maxi q φ (f θ (x)|yi) 0.63 ± 0.02 0.54 ± 0.03 0.64 ± 0.02 0.58 ± 0.04 0.60 ± 0.01 0.50 ± 0.02 MASS, β=1e-3 41.5 ± 0.7 Entropy maxi q φ (f θ (x)|yi) 0.61 ± 0.02 0.59 ± 0.07 0.62 ± 0.02 0.60 ± 0.06 0.59 ± 0.01 0.56 ± 0.06 MASS, β=1e-4 41.5 ± 1.1 Entropy maxi q φ (f θ (x)|yi) 0.60 ± 0.00 0.55 ± 0.05 0.61 ± 0.01 0.56 ± 0.04 0.58 ± 0.00 0.53 ± 0.04 MASS, β=0 42.0 ± 0.6 Entropy maxi q φ (f θ (x)|yi) 0.60 ± 0.02 0.55 ± 0.06 0.61 ± 0.02 0.57 ± 0.04 0.57 ± 0.01 0.54 ± 0.05 MASS, β=1e-2, D 41.7 ± 0.4 Entropy maxi q φ (f θ (x)|yi) 0.67 ± 0.01 0.68 ± 0.01 0.63 ± 0.01 0.63 ± 0.04 0.65 ± 0.04 0.57 ± 0.04 MASS, β=1e-3, D 43.7 ± 0.2 Entropy maxi q φ (f θ (x)|yi) 0.66 ± 0.05 0.67 ± 0.01 0.68 ± 0.01 0.63 ± 0.01 0.66 ± 0.04 0.61 ± 0.06 MASS, β=1e-4, D 43.4 ± 0.5 Entropy maxi q φ (f θ (x)|yi) 0.64 ± 0.07 0.65 ± 0.05 0.59 ± 0.08 0.68 ± 0.01 0.69 ± 0.01 0.64 ± 0.02 MASS, β=0, D 43.9 ± 0.4 Entropy maxi q φ (f θ (x)|yi) 0.65 ± 0.04 0.66 ± 0.03 0.60 ± 0.06 0.68 ± 0.00 0.69 ± 0.01 0.64 ± 0.00 Table 11 . 11 Out-of-distribution detection metrics for SmallMLP network trained on 2,500 CIFAR-10 images, with SVHN as the out-ofdistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the column was within one standard deviation. WD is weight decay; D is dropout. Higher values are better. Training Method Test Accuracy Detection Method AUROC APR In APR Out SoftmaxCE 34.2 ± 0.8 Entropy maxi q φ (f θ (x)|yi) 0.61 ± 0.01 0.30 ± 0.02 0.62 ± 0.01 0.38 ± 0.01 0.59 ± 0.01 0.39 ± 0.01 SoftmaxCE, WD 23.9 ± 0.9 Entropy maxi q φ (f θ (x)|yi) 0.70 ± 0.03 0.67 ± 0.03 0.71 ± 0.04 0.23 ± 0.02 0.36 ± 0.01 0.36 ± 0.01 SoftmaxCE, D 33.7 ± 1.1 Entropy maxi q φ (f θ (x)|yi) 0.60 ± 0.01 0.27 ± 0.01 0.62 ± 0.01 0.37 ± 0.00 0.58 ± 0.01 0.37 ± 0.00 VIB, β=1e-1 32.2 ± 0.6 Entropy Rate 0.58 ± 0.01 0.52 ± 0.02 0.60 ± 0.02 0.54 ± 0.02 0.56 ± 0.01 0.49 ± 0.02 VIB, β=1e-2 34.6 ± 0.4 Entropy Rate 0.60 ± 0.01 0.52 ± 0.04 0.62 ± 0.01 0.55 ± 0.04 0.57 ± 0.01 0.48 ± 0.03 VIB, β=1e-3 35.6 ± 0.5 Entropy Rate 0.59 ± 0.01 0.50 ± 0.04 0.60 ± 0.01 0.53 ± 0.03 0.56 ± 0.01 0.48 ± 0.03 VIB, β=1e-1, D 29.0 ± 0.6 Entropy Rate 0.57 ± 0.01 0.45 ± 0.02 0.60 ± 0.01 0.48 ± 0.02 0.53 ± 0.01 0.46 ± 0.01 VIB, β=1e-2, D 32.5 ± 0.9 Entropy Rate 0.62 ± 0.01 0.53 ± 0.05 0.63 ± 0.02 0.56 ± 0.04 0.59 ± 0.01 0.52 ± 0.04 VIB, β=1e-3, D 34.5 ± 1.0 Entropy Rate 0.63 ± 0.01 0.56 ± 0.05 0.64 ± 0.02 0.57 ± 0.03 0.60 ± 0.01 0.54 ± 0.05 MASS, β=1e-2 29.6 ± 0.4 Entropy maxi q φ (f θ (x)|yi) 0.59 ± 0.01 0.43 ± 0.03 0.61 ± 0.01 0.48 ± 0.03 0.56 ± 0.01 0.43 ± 0.01 MASS, β=1e-3 32.7 ± 0.8 Entropy maxi q φ (f θ (x)|yi) 0.57 ± 0.01 0.57 ± 0.04 0.59 ± 0.02 0.59 ± 0.04 0.55 ± 0.01 0.54 ± 0.03 MASS, β=1e-4 34.0 ± 0.3 Entropy maxi q φ (f θ (x)|yi) 0.57 ± 0.01 0.59 ± 0.03 0.57 ± 0.01 0.58 ± 0.03 0.55 ± 0.01 0.57 ± 0.03 MASS, β=0 34.1 ± 0.6 Entropy maxi q φ (f θ (x)|yi) 0.57 ± 0.01 0.61 ± 0.03 0.58 ± 0.01 0.59 ± 0.04 0.55 ± 0.00 0.59 ± 0.04 MASS, β=1e-2, D 29.3 ± 1.2 Entropy maxi q φ (f θ (x)|yi) 0.62 ± 0.02 0.50 ± 0.05 0.64 ± 0.03 0.54 ± 0.05 0.59 ± 0.02 0.47 ± 0.03 MASS, β=1e-3, D 31.5 ± 0.6 Entropy maxi q φ (f θ (x)|yi) 0.61 ± 0.02 0.62 ± 0.04 0.62 ± 0.03 0.63 ± 0.04 0.58 ± 0.01 0.58 ± 0.04 MASS, β=1e-4, D 32.7 ± 0.8 Entropy maxi q φ (f θ (x)|yi) 0.61 ± 0.02 0.65 ± 0.04 0.61 ± 0.03 0.63 ± 0.04 0.59 ± 0.01 0.62 ± 0.05 MASS, β=0, D 32.2 ± 1.1 Entropy maxi q φ (f θ (x)|yi) 0.65 ± 0.05 0.64 ± 0.05 0.63 ± 0.01 0.64 ± 0.02 0.61 ± 0.01 0.62 ± 0.06 Table 12 . 12 Out-of-distribution detection metrics for ResNet20 network trained on 40,000 CIFAR-10 images, with SVHN as the out-ofdistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the column was within one standard deviation. Higher values are better. Training Method Test Accuracy Detection Method AUROC APR In APR Out SoftmaxCE 81.7 ± 0.3 Entropy maxi q φ (f θ (x)|yi) 0.77 ± 0.02 0.59 ± 0.03 0.81 ± 0.02 0.62 ± 0.03 0.70 ± 0.02 0.55 ± 0.02 VIB, β=1e-3 81.0 ± 0.3 Entropy Rate 0.74 ± 0.02 0.55 ± 0.04 0.79 ± 0.02 0.57 ± 0.05 0.67 ± 0.02 0.51 ± 0.03 VIB, β=1e-4 81.2 ± 0.4 Entropy Rate 0.73 ± 0.02 0.50 ± 0.02 0.76 ± 0.03 0.54 ± 0.02 0.66 ± 0.02 0.48 ± 0.01 VIB, β=1e-5 80.9 ± 0.5 Entropy Rate 0.75 ± 0.02 0.18 ± 0.05 0.80 ± 0.02 0.34 ± 0.01 0.67 ± 0.02 0.34 ± 0.01 VIB, β=0 81.5 ± 0.2 Entropy Rate 0.79 ± 0.02 0.84 ± 0.02 0.73 ± 0.04 0.11 ± 0.03 0.32 ± 0.01 0.32 ± 0.01 MASS, β=1e-3 75.8 ± 0.5 Entropy maxi q φ (f θ (x)|yi) 0.74 ± 0.03 0.37 ± 0.04 0.77 ± 0.03 0.43 ± 0.02 0.69 ± 0.03 0.42 ± 0.02 MASS, β=1e-4 80.6 ± 0.5 Entropy maxi q φ (f θ (x)|yi) 0.76 ± 0.04 0.80 ± 0.04 0.70 ± 0.05 0.48 ± 0.06 0.53 ± 0.05 0.47 ± 0.04 MASS, β=1e-5 81.6 ± 0.4 Entropy maxi q φ (f θ (x)|yi) 0.77 ± 0.01 0.54 ± 0.03 0.82 ± 0.01 0.71 ± 0.02 0.58 ± 0.03 0.51 ± 0.02 MASS, β=0 81.5 ± 0.2 Entropy maxi q φ (f θ (x)|yi) 0.79 ± 0.03 0.83 ± 0.02 0.73 ± 0.03 0.49 ± 0.04 0.54 ± 0.04 0.47 ± 0.02 Table 13 . 13 Out-of-distribution detection metrics for ResNet20 network trained on 10,000 CIFAR-10 images, with SVHN as the out-ofdistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the column was within one standard deviation. Higher values are better. Training Method Test Accuracy Detection Method AUROC APR In APR Out SoftmaxCE 67.5 ± 0.8 Entropy maxi q φ (f θ (x)|yi) 0.64 ± 0.02 0.59 ± 0.03 0.68 ± 0.02 0.61 ± 0.03 0.58 ± 0.02 0.57 ± 0.04 VIB, β=1e-3 66.9 ± 1.0 Entropy Rate 0.59 ± 0.02 0.72 ± 0.05 0.73 ± 0.05 0.67 ± 0.05 0.63 ± 0.04 0.54 ± 0.02 VIB, β=1e-4 66.4 ± 0.5 Entropy Rate 0.59 ± 0.01 0.59 ± 0.07 0.63 ± 0.02 0.60 ± 0.07 0.54 ± 0.01 0.56 ± 0.06 VIB, β=1e-5 67.9 ± 0.8 Entropy Rate 0.61 ± 0.03 0.39 ± 0.07 0.65 ± 0.04 0.42 ± 0.03 0.56 ± 0.03 0.43 ± 0.04 VIB, β=0 67.1 ± 1.0 Entropy Rate 0.64 ± 0.01 0.32 ± 0.03 0.68 ± 0.01 0.39 ± 0.01 0.58 ± 0.01 0.39 ± 0.01 MASS, β=1e-3 59.6 ± 0.8 Entropy maxi q φ (f θ (x)|yi) 0.59 ± 0.02 0.49 ± 0.07 0.62 ± 0.03 0.46 ± 0.06 0.56 ± 0.02 0.48 ± 0.08 MASS, β=1e-4 66.6 ± 0.4 Entropy maxi q φ (f θ (x)|yi) 0.62 ± 0.02 0.61 ± 0.05 0.67 ± 0.02 0.61 ± 0.05 0.56 ± 0.03 0.60 ± 0.05 MASS, β=1e-5 67.4 ± 1.0 Entropy maxi q φ (f θ (x)|yi) 0.64 ± 0.02 0.61 ± 0.08 0.69 ± 0.03 0.61 ± 0.06 0.58 ± 0.01 0.61 ± 0.09 MASS, β=0 67.4 ± 0.3 Entropy maxi q φ (f θ (x)|yi) 0.64 ± 0.01 0.55 ± 0.05 0.68 ± 0.02 0.56 ± 0.04 0.58 ± 0.01 0.54 ± 0.05 Table 14 . 14 Out-of-distribution detection metrics for ResNet20 network trained on 2,500 CIFAR-10 images, with SVHN as the out-ofdistribution examples. Full experiment details are in Supplementary Material 7.7. Values are the mean over 4 training runs with different random seeds, plus or minus the standard deviation. Emboldened values are those for which the maximum observed mean value in the column was within one standard deviation. Higher values are better. Training Method Test Accuracy Detection Method AUROC APR In APR Out SoftmaxCE 50.0 ± 0.7 Entropy maxi q φ (f θ (x)|yi) 0.51 ± 0.01 0.63 ± 0.04 0.52 ± 0.02 0.62 ± 0.03 0.49 ± 0.01 0.63 ± 0.04 VIB, β=1e-3 49.5 ± 1.1 Entropy Rate 0.48 ± 0.05 0.68 ± 0.07 0.68 ± 0.05 0.66 ± 0.08 0.50 ± 0.05 0.47 ± 0.03 VIB, β=1e-4 49.4 ± 1.0 Entropy Rate 0.47 ± 0.05 0.66 ± 0.09 0.65 ± 0.08 0.66 ± 0.09 0.50 ± 0.05 0.47 ± 0.03 VIB, β=1e-5 50.0 ± 1.1 Entropy Rate 0.48 ± 0.05 0.59 ± 0.10 0.49 ± 0.05 0.55 ± 0.08 0.48 ± 0.03 0.61 ± 0.09 VIB, β=0 50.6 ± 0.8 Entropy Rate 0.51 ± 0.07 0.52 ± 0.20 0.54 ± 0.08 0.53 ± 0.15 0.50 ± 0.06 0.56 ± 0.17 MASS, β=1e-3 38.2 ± 0.7 Entropy maxi q φ (f θ (x)|yi) 0.48 ± 0.04 0.54 ± 0.11 0.50 ± 0.04 0.48 ± 0.06 0.47 ± 0.03 0.51 ± 0.08 MASS, β=1e-4 49.9 ± 1.0 Entropy maxi q φ (f θ (x)|yi) 0.72 ± 0.08 0.71 ± 0.08 0.73 ± 0.08 0.49 ± 0.04 0.51 ± 0.05 0.48 ± 0.03 MASS, β=1e-5 50.1 ± 0.5 Entropy maxi q φ (f θ (x)|yi) 0.69 ± 0.10 0.68 ± 0.10 0.70 ± 0.10 0.50 ± 0.06 0.51 ± 0.06 0.49 ± 0.04 MASS, β=0 50.2 ± 1.0 Entropy maxi q φ (f θ (x)|yi) 0.69 ± 0.07 0.68 ± 0.07 0.68 ± 0.07 0.51 ± 0.06 0.53 ± 0.06 0.50 ± 0.04 This is not the most common phrasing of statistical minimality, but we feel it is more understandable. For the equivalence of this phrasing and the standard definition see Supplementary Material 7.1. https://github.com/mwcvitkovic/ MASS-Learning In what follows, we will sometimes replace g by g/J f such that the Jacobian appears on the right-hand side. Furthermore, we will not only use non-negative g. This can be justified by splitting g into positive and negative parts provided that either part results in a finite integral."
}