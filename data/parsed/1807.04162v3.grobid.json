{
  "title": "THE THERMODYNAMICS OF MACHINE LEARNING",
  "abstract": "In this work we offer an information-theoretic framework for representation learning that connects with a wide class of existing objectives in machine learning. We develop a formal correspondence between this work and thermodynamics and discuss its implications. 1 Here we aim to invoke the same philosophy as in the introduction to Watanabe (2018). 2 That is, we imagine the data satisfies De Finetti's theorem, for which infinite exchangeable processes usually can be described by products of conditionally independent distributions, but don't want to worry too much about the complicated details since there are subtle special cases (Accardi, 2018). 3 Here and throughout H(A) is used to denote entropies H(A) =i p(A) log p(A).",
  "introduction": "INTRODUCTION Let X, Y be some paired data, for example: a set of images X and their labels Y . We imagine the data comes from some true, unknown data generating process Φ 1 , from which we have drawn a training set of N pairs: T N ≡ (x N , y N ) ≡ {x 1 , y 1 , x 2 , y 2 , . . . , x N , y N } ∼ φ(x N , y N ). (1) We further imagine the process is exchangeable 2 and the data is conditionally independent given the governing process Φ: p(x N , y N |φ) = i p(x i |φ)p(y i |x i , φ). (2) As machine learners, we believe that by studying the training set, we should be able to infer or predict new draws from the same data generating process. Call a set of M future draws from the data generating process T M ≡ {X M , Y M } the test set. The predictive information (Bialek et al., 2001) is the mutual information between the training set and a infinite test set, equivalently the amount of information the training set provides about the generative process itself: I pred (T N ) ≡ lim M →∞ I(T N ; T M ) = I(T N ; Φ) = I(X N , Y N ; Φ). (3) The predictive information measures the underlying complexity of the data generating process (Still, 2014) , and is fundamentally limited and must grow sublinearly in the dataset size (Bialek et al., 2001) . Hence, the predictive information is a vanishing fraction of the total information in the training set 3 : lim N →∞ I pred (T N ) H(T N ) = 0 (4) A vanishing fraction of the information present in our training data is in any way useful for future tasks. A vanishing fraction of the information contained in the training data is signal, the rest is noise. We claim the goal of learning is to learn a representation of data, both locally and globally that captures the predictive information while being maximally compressed: that separates the signal from the noise.",
  "body": "INTRODUCTION Let X, Y be some paired data, for example: a set of images X and their labels Y . We imagine the data comes from some true, unknown data generating process Φ 1 , from which we have drawn a training set of N pairs: T N ≡ (x N , y N ) ≡ {x 1 , y 1 , x 2 , y 2 , . . . , x N , y N } ∼ φ(x N , y N ). (1) We further imagine the process is exchangeable 2 and the data is conditionally independent given the governing process Φ: p(x N , y N |φ) = i p(x i |φ)p(y i |x i , φ). (2) As machine learners, we believe that by studying the training set, we should be able to infer or predict new draws from the same data generating process. Call a set of M future draws from the data generating process T M ≡ {X M , Y M } the test set. The predictive information (Bialek et al., 2001) is the mutual information between the training set and a infinite test set, equivalently the amount of information the training set provides about the generative process itself: I pred (T N ) ≡ lim M →∞ I(T N ; T M ) = I(T N ; Φ) = I(X N , Y N ; Φ). (3) The predictive information measures the underlying complexity of the data generating process (Still, 2014) , and is fundamentally limited and must grow sublinearly in the dataset size (Bialek et al., 2001) . Hence, the predictive information is a vanishing fraction of the total information in the training set 3 : lim N →∞ I pred (T N ) H(T N ) = 0 (4) A vanishing fraction of the information present in our training data is in any way useful for future tasks. A vanishing fraction of the information contained in the training data is signal, the rest is noise. We claim the goal of learning is to learn a representation of data, both locally and globally that captures the predictive information while being maximally compressed: that separates the signal from the noise. X Y Z Θ Φ (a) Graphical model for world P , the real world augmented with a local and global representation. The dashed lines emphasize that θ only depends on the first N data points, the training set. Blue denotes nodes outside our control, while red nodes are under our direct control. Z Y X Θ Φ (b) Graphical model for world Q, the world we desire. In this world, Z acts as a latent variable for X and Y jointly. Figure 1 : Graphical models. A TALE OF TWO WORLDS We are primarily interested in learning a stochastic local representation of X, call it Z, defined by some parametric distribution of our own design: p(z i |x i , θ) with its own parameters θ. A training procedure is a process that assigns a distribution p(θ|x N , y N ) to the parameters conditioned on the observed dataset. In this way, the parameters of our local parametric map are themselves a global representation of the dataset. With our augmentations, the world now looks like the graphical model in Figure 3a , denoted World P : Some data generating process Φ generates a dataset (X N , Y N ) which we perform some learning algorithm on to get some parameters p(θ|x N , y N ) which we can use to form a parametric local representation p(z i |x i , θ). World P is what we have. It is not necessarily what we want. What we have to contend with is an unknown distribution of our data. What we want is a world that corresponds to the traditional modeling assumptions in which Z acts as a latent factor for X and Y , rending them conditionally independent, leaving no correlations unexplained. Similarly, we would prefer if we could easily marginalize out the dependence on our universal (Φ) and model specific (Θ) parameters. World Q in Figure 3b is the world we want foot_0 . We can measure the degree to which the real world aligns with our desires by computing the minimum possible relative information foot_1 between our distribution p and any distribution consistent with the conditional dependencies encoded in graphical model Q foot_2 . It can be shown (Friedman et al., 2001) that this quantity is given by the difference in multi-informations between the two graphical models, as measured in World P : J ≡ min q∈Q D KL [p; q] = I P -I Q . ( 5 ) The multi-information (Slonim et al., 2005) of a graphical model is the KL divergence between the joint distribution and the product of all of the marginal distributions, which can be computed as a sum of mutual informations, one for each node in the graph, between itself and its parents: I G ≡ log p(g N ) i p(g i ) = i I(g i ; Pa(g i )) (6) In our case: J = I(Θ; X N , Y N ) + i [I(X i ; Φ) + I(Y i ; X i , Φ) + I(Z i ; X i , Θ) -I(X i ; Z i ) -I(Y i ; Z i )] . (7) This minimal relative information has two terms outside our control and we can take them to be constant, but which relate to the predictive information: i [I(X i ; Φ) + I(Y i ; X i , Φ)] ≥ i I(Y i ; X i ) + I pred (T N ). (8) These terms measure the intrinsic complexity of our data. The remaining four terms are: • I(X i ; Z i ) -which measures how much information our representation contains about the input (X). This should be maximized to ensure our local representation actually represents the input. • I(Y i ; Z i ) -which measures how much information our representation contains about our auxiliary data. This should be maximized as well to ensure that our local representation is predictive for the labels. • I(Z i ; X i , Θ) -which measures how much information the parameters and input determine about our representation. This should be minimized to ensure consistency between worlds, and ensure we learn compressed local representations. Notice that this is similar to, but distinct from the first term above. I(Z i ; X i , Θ) = I(Z i ; X i ) + I(Z i ; Θ|X i ) (9) by the Chain Rule for mutual information foot_3 . • I(Θ; X N , Y N ) -which measures how much information we store about our training data in the parameters of our encoder. This should also be minimized to ensure we learn compressed global representation, preveting overfitting. These mutual informations are all intractable in general, since we cannot compute the necessary marginals in closed form, given that we do not have access to the true data generating distribution. FUNCTIONALS Despite their intractability, we can compute variational bounds on these mutual informations. ENTROPY S ≡ log p(θ|x N , y N ) q(θ) P ≥ I(Θ; X N , Y N ) (10) The relative entropy in our parameters or just entropy for short measures the relative information between the distribution we assign our parameters in World P after learning from the data (X N , Y N ), with respect to some data independent q(θ) prior on the parameters. This is an upper bound on the mutual information between the data and our parameters and as such can measure our risk of overfitting our parameters. RATE R i ≡ log p(z i |x i , θ) q(z i ) P ≥ I(Z i ; X i , Θ) (11) The rate measures the complexity of our representation. It is the relative information of a sample specific representation z i ∼ p(z|x i , θ) with respect to our variational marginal q(z). It measures how many bits we actually encode about each sample, and can measure how our risk of overfitting our representation. We use R ≡ i R i . CLASSIFICATION ERROR C i ≡ -log q(y i |z i ) P ≥ H(Y i ) -I(Y i ; Z i ) = H(Y i |Z i ) (12) The classification error measures the conditional entropy of Y left after conditioning on Z. It is a measure of how much information about Y is left unspecified in our representation. This functional measures our supervised learning performance. We use C ≡ i C i . 2.1.4 DISTORTION D i ≡ -log q(x i |z i ) P ≥ H(X i ) -I(X i ; Z i ) = H(X i |Z i ) (13) The distortion measures the conditional entropy of X left after conditioning on Z. It is a measure of how much information about X is left unspecified in our representation. This functional measures our unsupervised learning performance. We use D ≡ i D i . GEOMETRY The distributions p(z|x, θ), p(θ|x N , y N ), q(z), q(x|z), q(y|z) can be chosen arbitrarily. Once chosen, the functionals R, C, D, S take on well described values. The choice of the five distributional families specifies a single point in a four-dimensional space. Importantly, the sum of these functionals is a variational upper bound (up to an additive constant) for the minimum possible relative information between worlds (Appendix D): S + R + C + D ≥ J + i H(X i , Y i |Φ) (14) Besides just the upper bound, we can consider the full space of feasible points. Notice that S and R are both themselves upper bounds on mutual informations, and so must be positive semidefinite. If our data is discrete, or if we have discretized it foot_4 , D and C which are both upper bounds on conditional entropies, must be positive as well. Along with Equation ( 14 ), given that i H(X i , Y i |Φ) is a positive constant outside our control, the space of possible (R, C, D, S) values is at least restricted to be points in the positive orthant with some minimum possible Manhattan distance to the origin: S + R + C + D ≥ i H(X i , Y i |Φ) R ≥ 0 S ≥ 0 D ≥ 0 C ≥ 0 (15) Even in the infinite model family limit, data-processing inequalities on mutual information terms all defined in a set of variables that satisfy some nontrivial conditional dependencies ensure that there are regions in this functional space that are wholly out of reach. The surface of the feasible region maps an optimal frontier, optimal in the degree to which it minimizes mismatch between our two worlds subject to constraints on the relative magnitudes of the individual terms. This convex polytope has edges, faces and corners that are identifiable as the optimal solutions for well known objectives. This story is a generalization of the story presented in Alemi et al. (2018) , which can be considered a two-dimensional projection of this larger space (onto R, D). Within our larger framework we can derive more specific bounds between subsets of the functionals. For instance: R i + D i ≥ H(X i ) + I(Z i ; Θ|X i ). ( 16 ) This mirrors the bound given in Alemi et al. (2018) where R + D ≥ H(X), which is still true given that all conditional mutual informations are positive semi-definite (H(X) + I(Z; Θ|X) ≥ H(X)), but here we obtain a tighter pointwise bound that has a term measuring how much information about our encoding is revealed by the parameters after conditioning on the input itself. This term I(Z i ; Θ|X i ) captures the degree to which our local representation is overly sensitive to the particular parameter settings 910 . GENERALIZATION We can evaluate how much information our representations capture about the true data generating process. For instance, I(Z i ; Φ) which measures how much information about the true data generating procedure our local representations capture. Notice that given the conditional dependencies in world P , we have the following Markov chain: Φ → (X i , Y i , Θ) → Z i (17) and so by the Data Processing Inequality (Cover & Thomas, 2012): I(Z i ; Φ) ≤ I(Z i ; Θ, X i , Y i ) = I(Z i ; X i , Θ) + ( ( ( ( ( ( ( I(Z i ; Y i |X i , Θ) ≤ R i . ( 18 ) The per-instance rate R i forms an upper bound on the mutual information between our encoding Z i and the true governing parameters of our data Φ. Similarly, we can establish that: Φ → (X N , Y N ) → Θ =⇒ I(Θ; Φ) ≤ I(Θ; X N , Y N ) ≤ S. (19) S upper bounds the amount of information our encoder's parameters Θ, the global representation of the dataset can contain about the true process Φ. At the same time: I(Θ; Φ) ≤ I(X N , Y N ; Φ) ≤ i I(X i , Y i ; Φ), (20) which sets a natural upper limit for the maximum S that might be useful. OPTIMAL FRONTIER As in Alemi et al. (2018) , under mild assumptions about the variational distributional families, it can be argued that the surface is monotonic in all of its arguments. The optimal surface in the infinite family limit can be characterized as a convex polytope (Equation ( 15 )). In practice we will be in the realistic setting corresponding to finite parametric families such as neural network approximators. We then expect that there is an irrevocable gap that opens up in the variational bounds. Any failure of the distributional families to model the correct corresponding marginal in P means that the space of all realizable R, C, D, S values will be some convex relaxation of the optimal feasible surface. This surface will be described some function f (R, C, D, S) = 0, which means we can identify points on the surface as a function of one functional with respect to the others (e.g. R = R(C, D, S)). Finding points on this surface equates to solving a constrained optimization problem, e.g. min q(z)q(x|z)q(y|z)p(z|x,θ)p(θ|{x,y}) R such that D = D 0 , S = S 0 , C = C 0 . (21) Equivalently, we could solve the unconstrained Lagrange multipliers problem: min q(z)q(x|z)q(y|z)p(z|x,θ)p(θ|{x,y}) R + δD + γC + σS. ( 22 ) Here δ, γ, σ are Lagrange multipliers that impose the constraints. They each correspond to the partial derivative of the rate at the solution with respect to their corresponding functional, keeping the others fixed. Notice that this single objective encompasses a wide range of existing techniques. • If we retain C alone, we are doing traditional supervised learning and our network will learn to be deterministic in its activations and parameters. • If δ = 0 we no longer require a variational reconstruction network q(x|z), and are doing some form of supervised learning generally. • If δ = 0, σ = 0 we exactly recover the Variational Information Bottleneck (VIB) objective of Alemi et al. ( 2016 ) (where β = 1/γ), a form of stochastically regularized supervised learning that imposes a bottleneck on how much information our representation can retain about the input, while simultaneously maximizing the amount of information the representation contains about the target. • If δ = 0 and σ, γ → ∞ but in such a way as to keep the ratio fixed β ≡ σ/γ (that is if we drop the R term and only keep C + βS as our objective) we recover the Information Bottleneck Lagrangian loss of Achille & Soatto (2017) , presented as an alternative way to do Information Bottleneck (Tishby et al., 1999) but being stochastic on the parameters rather than the activations as in VIB. • As a special case, if our objective is set to C + S (δ = 0, σ, γ → ∞, σ/γ → 1), we obtain the objective for a Bayesian neural network, ala Blundell et al. (2015) . • If we retain only D, we are training a stochastic autoencoder. • If σ = 0, γ = 0, δ = 1 the objective is equivalent to the ELBO used to train a VAE (Kingma & Welling, 2014) . • If σ = 0, γ = 0 more generally, the objective is equivalent to a β-VAE (Higgins et al., 2017) where β = 1/δ. • If γ = 0 all terms involving the auxiliary data Y drop out and we are doing some form of unsupervised learning without any variational classifier q(y|z). The presence of the S term makes this more general than a usual β-VAE and should offer better generalization properties and control of overfitting by bottle-necking how much information we allow the parameters of our encoder to extract from the training data. • σ = 0, γ = α, δ = 1 recovers the semi-supervised objective of Kingma et al. (2014) . • In its most general form, in common parlance the full objective might be described as a temperature-regulated Bayesian semi-supervised β-VAE, or a Variational Information Bottleneck Lagrangian Autoencoder (VIBLA). Examples of all of these objectives behavior on a simple toy model is shown in Appendix H. Notice that all of these previous approaches describe low dimensional sub-surfaces of the optimal three dimensional frontier. These approaches were all interested in different domains, some were focused on supervised prediction accuracy, others on learning a generative model. Depending on your specific problem, and downstream tasks, different points on the optimal frontier will be desirable. However, instead of choosing a single point on the frontier, we can now explore a region on the surface to see what class of solutions are possible within the modeling choices. By simply adjusting the three control parameters δ, γ, σ, we can smoothly move across the entire frontier and smoothly interpolate between all of these objectives and beyond. OPTIMIZATION So far we've considered explicit forms of the objective in terms of the four functionals. For S this would require some kind of tractable approximation to the posterior over the parameters of our encoding distribution foot_7 . Alternatively, we can formally describe the exact solution to our minimization problem: min S s.t. R = R 0 , C = C 0 , D = D 0 . ( 23 ) Recall that S measures the relative entropy of our parameter distribution with respect to the q(θ) prior. As such, the solution that minimizes the relative entropy subject to some constraints is a generalized Boltzmann distribution (Jaynes, 1957): p * (θ|{x, y}) = q(θ) Z e -(R+δD+γC)/σ . ( 24 ) Here Z is the partition function, the normalization constant for the distribution Z = dθ q(θ) e -(R+δD+γC)/σ (25) This suggests an alternative method for finding points on the optimal frontier. We could turn the unconstrained Lagrange optimization problem that required some explicit choice of tractable posterior distribution over parameters into a sampling problem for a richer implicit distribution. A naive way to draw samples from this posterior would be to use Stochastic Gradient Langevin Dynamics or its cousins (Welling & Teh, 2011; Chen et al., 2014; Ma et al., 2015) which, in practice, would look like ordinary stochastic gradient descent (or its cousins like momentum) for the objective R + δD + γC, with injected noise. By choosing the magnitude of the noise relative to the learning rate, the effective temperature σ can be controlled. There is increasing evidence that the stochastic part of stochastic gradient descent itself is enough to turn SGD less into an optimization procedure and more into an approximate posterior sampler (Mandt et al., 2017; Smith & Le, 2017; Achille & Soatto, 2017; Zhang et al., 2018; Chaudhari & Soatto, 2017) , where hyperparameters such as the learning rate and batch size set the effective temperature. If ordinary stochastic gradient descent is doing something more akin to sampling from a posterior and less like optimizing to some minimum, it would help explain improved performance through ensemble averages of different points along trajectories (Huang et al., 2017) . When viewed in this light, Equation 24 describes the optimal posterior for the parameters so as to ensure the minimal divergence between worlds P and Q. q(θ) plays the role of the prior over parameters, but our overall objective is minimized when q(θ) = p(θ) = p(θ|x N , y N ) p(x N ,y N ) . (26) That is, when our prior is the marginal of the posteriors over all possible datasets drawn from the true distribution. A fair draw from this marginal is to take a sample from the posterior obtained on a different but related dataset. Insomuch as ordinary SGD training is an approximate method for drawing a posterior sample, the common practice of fine-tuning a pretrained network on a related dataset is using a sample from the optimal prior as our initial parameters. The fact that fine-tuning approximates use of an optimal prior presumably helps explain its broad success. If we identify our true goal not as optimizing some objective but instead directly sampling from Equation 24 , we can consider alternative approaches to define our learning dynamics, such as parallel tempering or population annealing (Machta & Ellis, 2011) . Alternatively, we could, instead of adopting variational bounds on the mutual informations, consider other mutual information bounds such as those in Ishmael Belghazi et al. (2018); van den Oord et al. (2018). Perhaps our priors can be fit, providing we form estimates of the expectation over datasets (e.g. bootstrapping or jackknifing our dataset (DasGupta, 2008)). THERMODYNAMICS So far we have described a framework for learning that involves finding points that lie on the surface of a convex three-dimensional surface in terms of four functional coordinates R, C, D, S. Interestingly, this is all that is required to establish a formal connection to thermodynamics, which similarly is little more than the study of exact differentials (Sethna, 2006; Finn, 1993) . Whereas previous approaches connecting thermodynamics and learning (Parrondo et al., 2015; Still, 2017; Still et al., 2012) have focused on describing the thermodynamics and statistical mechanics of physical realizations of learning systems (i.e. the heat bath in these papers is a physical heat bath at finite temperature), in this work we make a formal analogy to the structure of the theory of thermodynamics, without any physical content. FIRST LAW OF LEARNING The optimal frontier creates an equivalence class of states, being the set of all states that minimize as much as possible the distortion introduced in projecting world P onto a set of distributions that respect the conditions in Q. The surface satisfies some equation f (R, C, D, S) = 0 which we can use to describe any one of these functionals in terms of the rest, e.g. R = R(C, D, S). This function is entire, and so we can equate partial derivatives of the function with differentials of the functionals 12 : dR = ∂R ∂C D,S dC + ∂R ∂D C,S dD + ∂R ∂S C,D dS. ( 27 ) Since the function is smooth and convex, instead of identifying the surface of optimal rates in terms of the functionals C, D, S, we could just as well describe the surface in terms of the partial derivatives by applying a Legendre transformation. We will name the partial derivatives: γ ≡ - ∂R ∂C D,S δ ≡ - ∂R ∂D C,S σ ≡ - ∂R ∂S C,D . (28) These measure the exchange rate for turning rate into reduced distortion, reduced classification error, or increased entropy, respectively. The functionals R, C, D, S are analogous to extensive thermodynamic variables such as volume, entropy, particle number, magnetic field, charge, surface area, length and energy which grow as the system grows, while the named partial derivatives γ, δ, σ are analogous to the intensive, generalized forces in thermodynamics corresponding to their paired state variable, such as pressure, temperature, chemical potential, magnetization, electromotive force, surface tension, elastic force, etc. Just as in thermodynamics, the extensive functionals are defined for any state, while the intensive partial derivatives are only well defined for equilibrium states, which in our language are the states lying on the optimal surface 13 . Recasting our total differential: dR = -γdC -δdD -σdS, (29) we create a law analogous to the First Law of Thermodynamics. In thermodynamics the First Law is often taken to be a statement about the conservation of energy, and by analogy here we could think about this law as a statement about the conservation of information. Granted, the actual content of the law is fairly vacuous, equivalent only to the statement that there exists a scalar function R = R(C, D, S) defining our surface and its partial derivatives. MAXWELL RELATIONS AND THERMODYNAMIC POTENTIALS Requiring that Equation 29 be an exact differential has mathematically trivial but intuitively nonobvious implications that relate various partial derivatives of the system to one another, akin to the Maxwell Relations in thermodynamics. For example, requiring that mixed second partial derivatives are symmetric establishes that: We can additionally take and name higher order partial derivatives, analogous to the susceptibilities of thermodynamics like bulk modulus, the thermal expansion coefficient, or heat capacities. For instance, we can define the analog of heat capacity for our system, a sort of rate capacity at constant distortion: ∂ 2 R ∂D∂C = ∂ 2 R ∂C∂D =⇒ ∂δ ∂C D = ∂γ ∂D C . ( 30 K D ≡ ∂R ∂σ D . ( 31 ) 12 ∂X ∂Y Z denotes the partial derivative of X with respect to Y holding Z constant. 13 For more discussion of equilibrium states, and how they connect with more intuitive notions of equilibrium, see Appendix G Just as in thermodynamics, these susceptibilities may offer useful ways to characterize and quantify the systematic differences between model families. Perhaps general scaling laws can be found between susceptibilities and network widths, or depths, or number of parameters or dataset size. Divergences or discontinuities in the susceptibilities are the hallmark of phase transitions in physical systems, and it is reasonable to expect to see similar phenomenon for certain models. A great deal of first, second and third order partial derivatives in thermodynamics are given unique names. This is because the quantities are particularly useful for comparing different physical systems. We expect a subset of the first, second and higher order partial derivatives of the base functionals will prove similarly useful for comparing, quantifying, and understanding differences between modeling choices. SECOND LAW OF LEARNING? Even when doing deterministic training, training is non-invertible (Maclaurin et al., 2015) , and we need to contend with and track the entropy (S) term. We set the parameters of our networks initially with a fair draw from some prior distribution q(θ). The training procedure acts as a Markov process on the distribution of parameters, transforming it from the prior distribution into some modified distribution, the posterior p(θ|x N , y N ). Optimization is a many-to-one function, that in the ideal limiting case, maps all possible initializations to a single global optimum. In this limiting case S would be divergent, and there is nothing to prevent us from memorizing the training set. The Second Law of Thermodynamics states that the entropy of an isolated system tends to increase. All systems tend to disorder, and this places limits on the maximum possible efficiency of heat engines. Formally, there are many statements akin to the Second Law of Thermodynamics that can be made about Markov chains generally (Cover & Thomas, 2012) . The central one is that for any for any two distributions p n , q n both evolving according to the same Markov process (n marks the time step), the relative entropy D KL [p n ; q n ] is monotonically decreasing with time. This establishes that for a stationary Markov chain, the relative entropy to the stationary state D KL [p n ; p ∞ ] monotonically decreases 14 . In our language, we can make strong statements about dynamics that target points on the optimal frontier, or dynamics that implement a relaxation towards equilibrium. There is a fundamental distinction between states that live on the frontier and those off of it, analogous to the distinction between equilibrium and non-equilibrium states in thermodynamics. Any equilibrium distribution can be expressed in the form Equation ( 24 ) and identified by its partial derivatives γ, δ, σ. If name the objective in Equation ( 22 ): J(γ, δ, σ) ≡ R + δD + γC + σS, (32) The value this objective takes for any equilibrium distribution can be shown to be given by the log partition function (Equation ( 25 )): min J(γ, δ, σ) = -σ log Z(γ, δ, σ) (33) and the KL divergence between any distribution over parameters p(θ) and an equilibrium distribution is: D KL [p(θ); p * (θ; γ, δ, σ)] = ∆J/σ (34) ∆J ≡ J noneq (p; γ, δ, σ) -J(γ, δ, σ) (35) Where J noneq is the non-equilibrium objective: J noneq (p; γ, δ, σ) = R + δD + γC + σS p(θ) . ( 36 ) For a stationary Markov process whose stationary distribution is an equilibrium distribution the KL divergence to the stationary distribution must monotonically decrease each step. This means the ∆J/σ must decrease monotonically, that is our objective J must decrease monotonically: J t=0 ≥ J t ≥ J t+1 ≥ J t=∞ . ( 37 ) Furthermore, if we use q(θ) as our prior over parameters, we know: J t=0 = R + δD + γC q(θ) (38) J t=∞ = -σ log Z. (39) CONCLUSION We have formalized representation learning as the process of minimizing the distortion introduced when we project the real world (World P ) onto the world we desire (World Q). The projection is naturally described by a set of four functionals which variationally bound relevant mutual informations in the real world. Relations between the functionals describe an optimal three-dimensional surface in a four dimensional space of optimal states. A single learning objective targeting points on this optimal surface can express a wide array of existing learning objectives spanning from unsupervised learning to supervised learning and everywhere in between. The geometry of the optimal frontier suggests a wide array of identities involving the functionals and their partial derivatives. This offers a direct analogy to thermodynamics independent of any physical content. By analogy to thermodynamics, we can begin to develop new quantitative measures and relationships amongst properties of our models that we believe will offer a new class of theoretical understanding of learning behavior. A RECONSTRUCTION FREE FORMULATION We can utilize the Chain Rule of Mutual Information (Equation ( 9 )): I(Z i ; X i , Θ) = I(Z i ; X i ) + I(Z i ; Θ|X i ), (40) to simplify our expression for the minimum possible KL between worlds (Equation ( 7 )), and consider a reduced set of functionals (compare to Section 2.1): • C i ≡ -log q(y i |z i ) P ≥ H(Y i ) -I(Y i ; Z i ) = H(Y i |Z i ) The classification error, as before. • S ≡ log p(θ|{x,y}) q(θ) P ≥ I(Θ; {X, Y }) The entropy as before. • V i ≡ log p(zi|xi,θ) q(zi|xi) P ≥ I(Z i ; Θ|X i ) The volume of the representation (for lack of a better term), which measures the mutual information between our representation Z and the parameters Θ, conditioned on the input X. That is, this functional bounds how much of the information in our representation can come from the learning algorithm, independent of the actual input. In principle, these three functionals still fully characterize the distortion introduced in our information projection. Notice that this new functional requires the variational approximation q(z i |x i ), a variational approximation to the marginal over our parameter distribution. Notice also that we no longer require a variational approximation to p(x i |z i ). That is, in this formulation we no longer require any form of decoder, or synthesis in our original data space X. While equivalent in its information projection, this more naturally corresponds to the model of our desired world Q: q(x, y, φ, z, θ) = q(φ)q(θ) i q(z i |x i )q(y i |z i ), (41) depicted below in Figure 2 . Here we desire, not the joint generative model X ← Z → Y , but the predictive model X → Z → Y . Z Y X Θ Φ Figure 2 : Modified graphical model for world Q, instead of Figure 3b , the world we desire which satisfies the joint density in Equation 41 . Notice that this graphical model encodes all of the same conditional independencies as the original. In this case we have: C + S + V ≥ J + i [H(Y i X i |Φ) -H(X i )] . (42) We can imagine tracing out this, now three dimensional, frontier that still explores a space consistent with our original graphical model, but wherein we no longer have to do any form of direct variational synthesis. B BAYESIAN INFERENCE Just as in A we can consider alternative graphical models for World P. In particular, we can consider a simplified scenario depicted in Figure 3 corresponding to the usual situation in Bayesian inference. X Θ Φ (a) Graphical model for world P , depicting Bayesian inference as learning a single global representation of data. X Θ Φ (b) Graphical model for world Q, the world we desire, the usual generative model of Bayesian inference. Here we have just data, generated by some process and we form a single global representation of the dataset. The world we desire, World Q, corresponds to the usual Bayesian modeling assumption, whereby our own global representation generates the data conditionally independently. For these sets of graphical models, we have the following information projection: J bayes = min q∈Q D KL [p; q] = I P -I Q = i I(X i ; Φ) + I(Θ; X n ) - i I(X i ; Θ) (43) And we can derive the simple variational bounds: S ≡ log p(θ|X n ) q(θ) ≥ I(Θ; X n ) (44) This entropy gives an upper bound on the mutual information between our parameters and the dataset, it requires a variational approximation to the true marginal of the posterior p(θ|X N ) over datasets: q(θ), a prior. U i ≡ -log q(x i |θ) ≥ H(X i |Θ) (45) The energy gives an upper bound on the conditional entropy of our data given our parameters, it is powered by a variational approximation to the factored inverse of our global representation, the likelihood in ordinary parlance. Our optimal frontier is set by those conditions above as well as: foot_9 : U + S ≥ J bayes + i H(X i |Φ) (46) Just as in our earlier paper (Alemi et al., 2018) we could trace out the frontier by doing the constrained optimization problem: min S + βU (47) The formal solution to this optimization problem takes the form: log p(θ|x N ) = log q(θ) + β i log q(x i |θ) -log Z. ( 48 ) Where Z is the partition function: Z = dθ q(θ)e β i log q(xi|θ) (49) This is the ordinary temperature regulated (Watanabe, 2009) Bayesian posterior: p(θ|x N ) ∝ q(θ) i q(x i |θ) β . ( 50 ) Using a temperature to regulate the relative contribution of the prior and posterior has been used broadly, but ordinarily doesn't have a well founded justification. Here we can unapologetically vary the relative contributions of the prior and likelihood since in the representational framework, those are both variational approximations that might have differing ability to better model the true distributions they approximate. By varying the β parameter here, just as in the β-VAE case (Alemi et al., 2018) we can smoothly explore the frontier within our modeling family, smoothly controlling the amount of information our model extracts from the dataset. This can help us control for overfitting in a principled way. Additionally, we could try to relax our variational approximations, and fit our prior, assuming we could estimate an expectation over datasets. One way to do that is with a bootstrap or jackknife procedure (DasGupta, 2008) . C DISCRIMINATIVE MODELS Similarly we could consider the situation depicting usual discriminative learning, depicted in  For these sets of graphical models, we have the following information projection: J d = I P -I Q = I(Θ; X N , Y N ) + i [I(X i ; Φ) + I(Y i ; X i , Φ) -I(Y i ; X i , Θ)] . (51) S ≡ log p(θ|X n ) q(θ) P ≥ I(Θ; X n ) (52) This entropy gives an upper bound on the mutual information between our parameters and the dataset, it requires a variational approximation to the true marginal of the posterior p(θ|X N ) over datasets: q(θ), a prior. U i ≡ -log q(y i |x i , θ) P ≥ H(Y i |X i , Θ) (53) The energy gives an upper bound on the conditional entropy of our targets given our parameters and input, it is powered by a variational approximation to the factored inverse of our global representation, the conditional likelihood in ordinary parlance. Our optimal frontier is set by those conditions above as well as: U + S ≥ J d + i H(Y i |X i , Φ) -I(X i ; Φ) (54) Just as previously in Appendix B solutions on the frontier can be specified by: log p(θ|x N , y N ) = log q(θ) + β i log q(y i |x i , θ) -log Z. (55) Here again we can smoothly explore the frontier set by the variationals approximations given by the prior and likelihood by simply adjusting β. We might additionally consider going beyond the fixed variational approximations and push the frontier by fitting the prior, or likelihood. F MAXWELL RELATIONS We can also define other potentials analogous to the alternative thermodynamic potentials such as enthalpy, free energy, and Gibb's free energy by performing partial Legendre transformations. For instance, we can define a free rate: F (C, D, σ) ≡ R + σS (78) dF = -γdC -δdD + Sdσ. (79) The free rate measures the rate of our system, not as a function of S (something difficult to keep fixed), but in terms of σ, a parameter in our loss or optimal posterior. The free rate gives rise to other Maxwell relations such as ∂S ∂C σ = - ∂γ ∂σ C , (80) which equates how much each additional bit of entropy (S) buys you in terms of classification error (C) at fixed effective temperature (σ), to a seemingly very different experiment where you measure the change in the effective supervised tension (γ, the slope on the R -C curve) versus effective temperature (σ) at a fixed classification error (C). F.1 COMPLETE ENUMERATION Here we enumerate a complete set of Maxwell Relations. First if we write R = R(D, C, S): dR = -γdC -δdD -σdS ∂γ ∂D C = ∂δ ∂C D (81) ∂δ ∂S D = ∂σ ∂D S (82) ∂γ ∂S C = ∂σ ∂C S (83) Next transforming to F = R + σS = F (D, C, σ) dF = -γdC -δdD + Sdσ ∂γ ∂σ C = - ∂S ∂C σ (84) ∂δ ∂S D = - ∂S ∂D σ (85) Next transforming to H = R + γC = H(D, γ, S) dH = Cdγ -δD -σdS (86) ∂C ∂D γ = - ∂δ ∂γ D (87) ∂C ∂S γ = - ∂σ ∂γ S (88) H EXPERIMENTS We show examples of models trained on a toy dataset for all of the different objectives we define above. The dataset has both an infinite data variant, where overfitting is not a problem, and a finite data variant, where overfitting can be clearly observed for both reconstruction and classification. Data generation. We follow the toy model from Alemi et al. ( 2018 ), but add an additional classification label in order to explore supervised and semi-supervised objectives. The true data generating distribution is as follows. We first sample a latent binary variable, z ∼ Ber(0.7), then sample a latent 1D continuous value from that variable, h|z ∼ N (h|µ z , σ z ), and finally we observe a discretized value, x = discretize(h; B), where B is a set of 30 equally spaced bins, and a discrete label, y = z (so the true label is the latent variable that generated x). We set µ z and σ z such that R * ≡ I(x; z) = 0.5 nats, in the true generative process, representing the ideal rate target for a latent variable model. For the finite dataset, we select 50 examples randomly from the joint p(x, y, z). For the infinite dataset, we directly supply the true full marginal p(x, y) at each iteration during training. When training on the finite dataset, we evaluate model performance against the infinite dataset so that there is no error in the evaluation metrics due to a finite test set. Model details. We choose to use a discrete latent representation with K = 30 values, with an encoder of the form q(z i |x j ) ∝ -exp[(w e i x j -b e i ) 2 ], where z is the one-hot encoding of the latent categorical variable, and x is the one-hot encoding of the observed categorical variable. We use a decoder of the same form, but with different parameters: q (x j |z i ) ∝ -exp[(w d i x j -b d i ) 2 ]. We use a classifier of the same form as well: q(y j |z i ) ∝ -exp[(w c i y j -b c i ) 2 ]. Finally, we use a variational marginal, q(z i ) = π i . Given this, the true joint distribution has the form p(x, y, z) = p(x)p(z|x)p(y|x), with marginal p(z) = x p(x, z), and conditionals p(x|z) = p(x, z)/p(z) and p(y|z) = p(y, z)/p(z). The encoder is additionally parameterized following Achille & Soatto (2017) by α, a set of learned parameters for a Log Normal distribution of the form log N (-α i /2, α i ). In total, the model has 184 parameters: 60 weights and biases in the encoder and decoder, 4 weights and biases in the classifier, 30 weights in the marginal, and an additional 30 weights for the α i parameterizing the stochastic encoder. We initialize the weights so that when σ = 0, there is no noticeable effect on the encoder during training or testing. Experiments. In Figure 5 , we show the optimal, hand-crafted model for the toy dataset, as well as a selection of parameterizations of the TherML objective that correspond to commonly-used objective functions and a few new objective functions not previously described. In the captions, the parameters are specified with γ, δ, σ as in the main text, as well as ρ, which is a corresponding Lagrange multiplier for R, in order to simplify the parameterization. It just parameterizes the optimal surface slightly differently. We train all objectives for 10,000 gradient steps. For all of the objectives described, the model has converged, or come close to convergence, by that point. Because the model is sufficiently powerful to memorize the dataset, most of the objectives are very susceptible to overfitting. Only the objective variants that are \"regularized\" by the S term (parameterized by σ) are able to avoid overfitting in the decoder and classifier. Figure 5 : Hand-crafted optimal model. Toy Model illustrating the difference between selected points on the three dimensional optimal surface defined by γ, δ, and σ. See Section 3 for more description of the objectives, and Appendix H for details on the experiment setup. Top (i): Three distributions in data space: the true data distribution, p(x), the model's generative distribution, g(x) = z q(z)q(x|z), and the empirical data reconstruction distribution, d(x) = x z p(x )q(z|x )q(x|z). Middle (ii): Four distributions in latent space: the learned (or computed) marginal q(z), the empirical induced marginal e(z) = x p(x)q(z|x), the empirical distribution over z values for data vectors in the set X0 = {xn : zn = 0}, which we denote by e(z0) in purple, and the empirical distribution over z values for data vectors in the set X1 = {xn : zn = 1}, which we denote by e(z1) in yellow. Bottom: Three K × K distributions: (iii) q(z|x), (iv) q(x|z) and (v) q(x |x) = z q(z|x)q(x |z). ) This equates the result of two very different experiments. In the experiment encoded in the partial derivative on the left, one would measure the change in the derivative of the R -D curve (δ) as a function of the classification error (C) at fixed distortion (D). On the right one would measure the change in the derivative of the R -C curve (γ) as a function of the distortion (D) at fixed classification error (C). As different as these scenarios appear, they are mathematically equivalent. A full set of Maxwell relations can be found in Appendix F. Figure 3 : 3 Figure 3: Graphical models for standard Bayesian inference. Graphic model Q depicting a discriminative generative model. Figure 4 : 4 Figure 4: Graphical models for the traditional discriminative case. (a) Deterministic Supervised Classifier: δ = ρ = σ = 0, γ = 1. (b) Entropy-regularized Deterministic Classifier: δ = ρ = 0, γ = 1, σ = 0.1. (c) Entropy-regularized IB: δ = 0, ρ = 0, γ = 1, σ = 0.01. (d) Bayesian Neural Network Classifier: δ = 0, ρ = 0, σ = γ = 1. Figure 6 : 6 Figure 6: Supervised Learning approaches. (a) VIB: δ = 0, σ = 0, γ = 1, ρ(β) = 0.5.(b) Entropy-regularized VIB: δ = 0, γ = 1, ρ = 0.9, σ = 0.1. Figure 7 : 7 Figure 7: VIB style objectives. ( a ) a Deterministic Autoencoder: γ = ρ = σ = 0, δ = 1. (b) Entropy-regularized Deterministic Autoencoder: γ = ρ = 0, δ = 1, σ = 0.01. Figure 8 : 8 Figure 8: Autoencoder objectives. (a) VAE: σ = 0, γ = 0, δ = ρ = 1. (b) β-VAE: σ = 0, γ = 0, δ = 1, ρ(β) = 0.5. (c) Entropy-regularized β-VAE: σ = 0.5, γ = 0, δ = 1, ρ(β) = 0.9. (d) Semi-supervised VAE: σ = 0, γ(α) = 0.5, δ = ρ = 1. Figure 9 : 9 Figure 9: VAE style objectives. Figure 10 : 10 Figure 10: Full Objective. σ = 0.5, γ = 1000, δ = 1, ρ = 0.9. Simple demonstration of the behavior with all terms present in the objective. We could consider different alternatives, deciding to relax some of the constraints we imposed in World Q, or generalizing World P by letting the representation depend on X and Y jointly, for instance. What follows demonstrates a general sort of calculus that we can invoke for any specified pair of graphical models. In particular Appendices A to C discuss alternatives. Also known as the KL divergence. Note that this is DKL [p; q * ] where q * is the well known reverse-information projection or moment projection: q * = argmin q∈Q DKL [p; q] (Csiszár & Matúš, 2003). Given this relationship, we could actually reduce the total number of functions we consider from 4 to 3, as discussed in Appendix A. More generally, if we choose some measure m(x), m(y) on both X and Y , we can define D and C in terms of that measure e.g. D ≡ -log q(x|z) m(x) P ≥ Hm(X) -I(X; Z) = Hm(X|Z) In Appendix A we consider taking this bound seriously to limit the space only only three functionals, S, C and V ≥ I(Zi; Θ|Xi) This could help explain the observation that often times putting additional modeling power on the prior rather than the encoder can give improvements in ELBO (Chen et al., 2016) . As in Blundell et al. (2015) ; Achille & Soatto (2017) For discrete state Markov chains, this implies that if the stationary distribution is uniform, the entropy of the distribution H(pn) is strictly increasing. U ≡ i Ui"
}