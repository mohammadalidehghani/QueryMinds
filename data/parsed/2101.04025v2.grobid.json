{
  "title": "Distributed Double Machine Learning with a Serverless Architecture",
  "abstract": "This paper explores serverless cloud computing for double machine learning. Being based on repeated cross-fitting, double machine learning is particularly well suited to exploit the high level of parallelism achievable with serverless computing. It allows to get fast on-demand estimations without additional cloud maintenance effort. We provide a prototype Python implementation DoubleML-Serverless for the estimation of double machine learning models with the serverless computing platform AWS Lambda and demonstrate its utility with a case study analyzing estimation times and costs. CCS CONCEPTS • Computer systems organization → Cloud computing; • Computing methodologies → Machine learning.",
  "introduction": "INTRODUCTION Double machine learning (DML) models [19] are becoming increasingly popular among statisticians, econometricians and data scientists with numerous methodological extensions [8, 18, 21, 27-29, 33, 34, 37] and applications in areas like finance [22] , COVID-19 research [20, 39] or economics [30, 38] . The DML models allow researchers to exploit the excellent prediction power of machine learning algorithms in a valid statistical framework for estimation and inference on causal parameters. Recently, the Python and R packages DoubleML with a flexible object-oriented structure for estimating double machine learning models have been published [6, 7] . Serverless cloud computing is predicted to be the dominating and default architecture of cloud computing in the coming decade (Berkley View on Serverless Computing [26] ) and is becoming increasingly adopted in the industry and by researchers. Its Function as a Service (FaaS) paradigm lowers the entry bar to cloud computing technologies as the cloud providers are responsible for almost every operational and maintenance task. A key advantage of serverless computing is the high elasticity in terms of an automated on-demand scaling depending on the actual amount of computing requests. A second key advantage of serverless computing is the pricing model: Only actually used resources are charged without provisioning costs. The management of computing clusters is usually not part of the daily business of econometricians or data scientists using DML for data analysis or in applied research. Nevertheless there is demand for a high level of scalability to speed up the estimation of models like DML in interactive data analysis tasks. In our experience, econometricians or data scientists who consider using cloud computing resources often want to achieve goals like the following: • A high level of parallelism. • A \"cloud button\": Easy deployment and if possible no ongoing maintenance tasks for the user. • A high level of elasticity: On-demand availability of a high level of parallelism, pay-per-request and ideally no costs when the systems are idle. The goal of this paper is to explore to what extent such goals are achievable with serverless cloud computing and we put special focus on DML models as an application. Our study is based on AWS Lambda and we made our prototype implementation DoubleML-Serverless publicly available. 1 We demonstrate the functionalities of the prototype with an experiment where we analyze estimation times and costs with different settings. The rest of the paper is organized as follows: Introductions to serverless computing and double machine learning are given in Sections 2 and 3. The prototype implementation DoubleML-Serverless is described in Section 4. Section 5 presents our experiment setup and results. In Section 6 we discuss our prototype implementation and give an outlook to potential future extensions. Section 7 concludes the paper.",
  "body": "INTRODUCTION Double machine learning (DML) models [19] are becoming increasingly popular among statisticians, econometricians and data scientists with numerous methodological extensions [8, 18, 21, 27-29, 33, 34, 37] and applications in areas like finance [22] , COVID-19 research [20, 39] or economics [30, 38] . The DML models allow researchers to exploit the excellent prediction power of machine learning algorithms in a valid statistical framework for estimation and inference on causal parameters. Recently, the Python and R packages DoubleML with a flexible object-oriented structure for estimating double machine learning models have been published [6, 7] . Serverless cloud computing is predicted to be the dominating and default architecture of cloud computing in the coming decade (Berkley View on Serverless Computing [26] ) and is becoming increasingly adopted in the industry and by researchers. Its Function as a Service (FaaS) paradigm lowers the entry bar to cloud computing technologies as the cloud providers are responsible for almost every operational and maintenance task. A key advantage of serverless computing is the high elasticity in terms of an automated on-demand scaling depending on the actual amount of computing requests. A second key advantage of serverless computing is the pricing model: Only actually used resources are charged without provisioning costs. The management of computing clusters is usually not part of the daily business of econometricians or data scientists using DML for data analysis or in applied research. Nevertheless there is demand for a high level of scalability to speed up the estimation of models like DML in interactive data analysis tasks. In our experience, econometricians or data scientists who consider using cloud computing resources often want to achieve goals like the following: • A high level of parallelism. • A \"cloud button\": Easy deployment and if possible no ongoing maintenance tasks for the user. • A high level of elasticity: On-demand availability of a high level of parallelism, pay-per-request and ideally no costs when the systems are idle. The goal of this paper is to explore to what extent such goals are achievable with serverless cloud computing and we put special focus on DML models as an application. Our study is based on AWS Lambda and we made our prototype implementation DoubleML-Serverless publicly available. 1 We demonstrate the functionalities of the prototype with an experiment where we analyze estimation times and costs with different settings. The rest of the paper is organized as follows: Introductions to serverless computing and double machine learning are given in Sections 2 and 3. The prototype implementation DoubleML-Serverless is described in Section 4. Section 5 presents our experiment setup and results. In Section 6 we discuss our prototype implementation and give an outlook to potential future extensions. Section 7 concludes the paper. SERVERLESS COMPUTING A core principle of serverless computing is that the user just writes a cloud function, often in a high-level programming language like Python, and all the server provisioning and administration is done by the cloud provider. These serverless cloud function offerings are often called Function as a Service (FaaS), because the user basically only specifies the function code to be executed and declares which events should trigger such function calls. There is especially no need for ex-ante provisioning of computing resources. It is in the hand of the cloud provider to automatically scale up resources depending on the number of requests sent to the FaaS. This is one of the key differences in comparison to a classical cloud server, where the user ex-ante needs to decide which requirements best match the upcoming computing tasks. General discussions of serverless computing, recent developments and challenges can be found in [9, 23, 26, 32, 41, 42] . Besides that, serverless computing is getting more and more adopted for various machine learning tasks, like for example to serve deep learning models [14, 24, 40] or more generally for ML model training and hyperparameter tuning [15, 16, 43] . Another core principle of serverless computing is the pricing model. The billing is usually done proportionally to the actually used resources and not proportionally to resources provisioned. In case of AWS Lambda it is proportional to the execution time and very fine grained as the duration billing granularity was recently lowered to per millisecond billing [2] . When using AWS Lambda there is one key parameter set by the user, which is the memory available to the function at runtime. AWS Lambda also scales other resources like CPU power proportionally to the allocated memory. In the past the maximum memory allocatable was regularly increased and recently there was an significant extension from a maximum of 3 GB to 10 GB [3] . According to AWS this translates to a maximum of 6 vCPUs accessible in a single FaaS request [3] . By its nature the enormous elasticity of serverless computing platforms comes at the cost of rather strict resource limits for a single request. When using AWS Lambda among others the maximum runtime is 15 minutes. However, the recent updates make serverless computing increasingly attractive for computationally intense tasks like machine learning. A BRIEF INTRODUCTION TO DOUBLE MACHINE LEARNING Double machine learning (DML) was developed in a series of papers [10] [11] [12] and introduced as a general framework in [19] . The application of DML for model classes like the partially linear regression model, the partially linear instrumental variable model, the interactive regression model and the interactive instrumental variable model is discussed in [19] . Recently the DML framework and related techniques have been extended to numerous model classes like for example reinforcement learning [27, 34] , transformation models [28] , generalized additive models [8] , continuous treatment effects [21, 37] , dynamic treatment effects [33] , Gaussian graphical models [29] , difference-in-differences models [18] and many more. In these applications of DML, one is usually interested in statistical inference for a causal parameter 𝜃 0 . The DML framework makes it possible to obtain valid statistical inference for 𝜃 0 while exploiting the excellent prediction quality of machine learning methods for estimating nuisance functions denoted as 𝜂 0 . As an example, we consider the partially linear regression (PLR) model as studied by [36] 𝑌 = 𝐷𝜃 0 + 𝑔 0 (𝑋 ) + 𝑈 , E(𝑈 |𝑋, 𝐷) = 0, (1) 𝐷 = 𝑚 0 (𝑋 ) + 𝑉 , E(𝑉 |𝑋 ) = 0, (2) with outcome variable 𝑌 , treatment/policy variable 𝐷 and the potentially high-dimensional vector of controls 𝑋 := (𝑋 1 , . . . , 𝑋 𝑝 ). The causal parameter of interest is 𝜃 0 . It measures the average treatment effect of 𝐷 on 𝑌 , if 𝐷 is conditionally exogenous. The confounding variables 𝑋 affect 𝐷 via the function 𝑚 0 (𝑋 ) and 𝑌 via the function 𝑔 0 (𝑋 ). Figure 1 visualizes the interpretation in a causal diagram. The DML framework allows to obtain valid statistical inference for 𝜃 0 while exploiting the excellent prediction quality of machine learning methods when estimating the nuisance functions 𝜂 0 = (𝑔 0 , 𝑚 0 ). In the DML framework the nuisance functions 𝜂 0 = (𝑔 0 , 𝑚 0 ) can be estimated with different ML-methods, e.g., [19] use random forests, regression trees, boosting, lasso, neural networks and ensembles of these methods. Depending on the structural assumptions on 𝜂 0 , different ML-methods are appropriate. 2 Y D V X A key component of the DML framework are so-called Neyman orthogonal score functions 𝜓 (𝑊 ; 𝜃, 𝜂). The score functions identify the causal parameter of interest 𝜃 0 as the unique solution to E(𝜓 (𝑊 ; 𝜃 0 , 𝜂 0 )) = 0. Neyman orthogonality of 𝜓 (𝑊 ; 𝜃, 𝜂) with respect to the nuisance functions 𝜂 guarantees that there are no first-order effects of estimation errors in the nuisance functions on the estimation of the causal parameter 𝜃 0 . A second key component of the DML framework is sample splitting to avoid biases caused by overfitting. The application of repeated cross-fitting is further recommended in [19] . This makes it particularly well suited for a distributed architecture where the computationally intense inference tasks run in parallel. Estimation of typical DML models often requires the estimation and prediction of several hundreds of ML models to approximate nuisance functions in different sample splits. An ambitious goal of a serverless DML implementation would be to achieve that the estimation of the whole DML model with repeated cross-fitting does not take much longer than the estimation of the nuisance functions on a single fold. The enormous elasticity of serverless cloud computing makes such a goal achievable in an on-demand setup with no need to start and maintain a large computing cluster, which is becoming costly if being idle. The DML algorithm with repeated cross-fitting can be summarized as follows (w.l.o.g. we assume that the number of observations 𝑁 is divisible by the number of folds 𝐾): ( ). (2) For each sample split, compute an estimate θ0,𝑚 of the causal parameter as the solution to the equation 1 𝑁 𝐾 ∑︁ 𝑘=1 ∑︁ 𝑖 ∈𝐼 𝑚,𝑘 𝜓 (𝑊 𝑖 ; θ0,𝑚 , η0,𝑘 ) = 0. The final estimate for the causal parameter is obtain via aggregation θ0 = Median(( θ0,𝑚 ) 𝑚 ∈ [𝑀 ] ). Note that the number of nuisance functions, which need to be estimated with ML methods, depends on the considered model, e.g., for the PLR model we have 𝐿 = 2 nuisance functions 𝜂 0 = (𝑔 0 , 𝑚 0 ). The total number of ML fits is 𝑀 × 𝐾 × 𝐿, i.e., one ML estimation in each fold, of each repeated sample splitting and for each nuisance function. For example [19] choose 𝐾 = 5 (or 𝐾 = 2) and 𝑀 = 100, which for the PLR model with 𝐿 = 2 nuisance functions amounts to 1000 (or 400) ML fits or for the partially linear instrumental variable model with 𝐿 = 3 nuisance functions it amounts to 1500 (or 600) ML fits. Note that for the interactive regression models, as considered in [19] , even more nuisance functions need to be estimated. As mentioned before, our prototype for serverless DML allows for parallelization of all these 1000 machine learning tasks and therefore potentially speeds up the estimation of DML models by a significant factor. Basically, the estimation time with repeated cross-fitting with five folds and 100 repetitions could be almost reduced to the time needed to estimate a single nuisance function for one fold in a single sample split. Note that we do not require to transfer the estimated ML models for the nuisance functions η0,𝑘 , instead it suffices to return the predictions on the test datasets (i.e., for the observations indexed with 𝑖 ∈ 𝐼 𝑚,𝑘 ) to evaluate the score function and solve for the causal parameter θ0,𝑚 in a second step. Neyman orthogonal score functions for many model classes, like for example the PLR model, can be written as linear functions in the parameter 𝜃 , i.e., 𝜓 (𝑊 ; 𝜃 ; 𝜂) = 𝜃𝜓 𝑎 (𝑊 ; 𝜂) + 𝜓 𝑏 (𝑊 ; 𝜂). This common property forms the basis for a very general objectoriented implementation of DML models in the Python package DoubleML [6] , which serves as a basis four our prototype DoubleML-Serverless. SERVERLESS DOUBLE MACHINE LEARNING Similar to PyWren [25] , our prototype implementation DoubleML-Serverless is intended to be used in an interactive fashion: The user runs a Python session on a local machine or server, but at the same time has access to a high level of parallelism with an ondemand and pay-per-request interface for the computationally most intense tasks during the estimation of DML models. 3 In comparison to PyWren, which allows to run more or less arbitrary parallel tasks, like for example map reduce, our implementation is more specialized to the specific use case of DML models. Many cloud providers have serverless FaaS offerings. Our prototype DoubleML-Serverless uses AWS Lambda and is developed in Python as an extension of the DoubleML package [6] . 4 4.1 The Architecture of DoubleML-Serverless The architecture of DoubleML-Serverless is summarized in Figure 2. As data storage we use the AWS S3 object storage. In the DoubleML-Serverless package, we implement a DoubleMLDataS3 DoubleML-Serverless S3 Bucket λ-Function • LambdaCVPredict λ-Layer • scikit-learn • pandas • numpy deploy upload data deploy pull data fit aws lambda() invoke lambdas return predictions λ λ λ λ λ λ λ λ λ λ λ λ . . . . . . . . . . . . LambdaCVPredict 1. Pull data from S3 2. Estimate ML-models 3. Compute predictions 4. Return predictions class, which serves as a data backend. It is inherited from the Dou-bleML class DoubleMLData and primarily extends it by methods to transfer datasets from and to AWS S3. The model classes, like for example DoubleMLPLRServerless for the PLR model, extend the corresponding classes from the DoubleML package by methods to perform the ML estimation and prediction step on AWS Lambda. In addition to the standard inputs for DoubleML model classes, the user needs to provide the name of the deployed lambda function and the AWS region on initialization. Then the DML model can be estimated with a call to the method fit_aws_lambda(). On invocation, each request consists of a reference to the dataset on S3, the nuisance-function-specific names of target variables and confounders and the sample splitting. The lambda function returns the predictions for the corresponding test indices. The Level of Scaling Our prototype implementation DoubleML-Serverless offers two different degrees of scaling. Figure 3 visualizes the level of scaling options for the PLR class DoubleMLPLRServerless. Per-samplesplit scaling is achieved by choosing scaling = 'n_rep'. It results in a lambda function invocation for each nuisance function and repeated sample split, i.e., for each blue rectangle in Figure 3 . In each such invocation, 𝐾 machine learning models are estimated and corresponding predictions for the test indices returned. As an alternative one can choose scaling = 'n_folds * n_rep' to invoke a separate lambda for each single fold, nuisance function and sample split, i.e., for each orange rectangle in Figure 3 . DoubleMLPLRServerless g0(X) Outcome Variable: Y Controls: X m0(X) Outcome Variable: D Controls: X Fold 1 Fold 2 Fold 3 . . . Fold K Split 1 . . . Fold 1 Fold 2 Fold 3 . . . Fold K Split M Fold 1 Fold 2 Fold 3 . . . Fold K Split 1 . . . If we again consider the above mentioned PLR model with 𝐾 = 5 folds, 𝑀 = 100 splits and 𝐿 = 2 nuisance functions, it means that we either sent 𝑀 × 𝐿 = 200 requests or 𝑀 × 𝐾 × 𝐿 = 1000 requests. Which level of scaling is favorable depends on the individual use case. First of all, the runtime limit of AWS Lambda implies that the per-sample-split scaling cannot be applied if the estimation of 𝐾 machine learning models takes longer than the maximum runtime, which might be the case, depending on the machine learning approach and the size of the dataset. Furthermore, there is always a cost vs. estimation-time tradeoff which the user controls via the scaling parameter and the allocated memory. Deployment with AWS SAM User-friendly deployment of the prototype is achieved with the AWS Serverless Application Model (AWS SAM) for deploying our FaaS to AWS Lambda. AWS SAM [4] allows for easy deployment of serverless applications to AWS Lambda and is configured via template files. We added an AWS SAM template to our prototype, which deploys the following components (see Figure 2 for a visualization of the architecture): • A lambda function called LambdaCVPredict. • A layer providing the Python libraries scikit-learn, pandas and numpy together with their dependencies. • An S3 bucket for the data transfer (can be optionally generated, or an existing bucket is used). • A role for the execution of the lambda function LambdaCVPredict which consists of the AWS-managed AWSLambdaBa-sicExecutionRole policy plus read access to the S3 bucket for data transfer. LambdaCVPredict is the main function being invoked when estimating DML models on AWS Lambda. The main advantage of AWS SAM is that the deployment process is simple with only two calls sam build and sam deploy -guided. Additionally, based on the same SAM template, even simpler deployment is offered directly from the AWS Serverless Application Repository. 5 The listing in the AWS Serverless Application Repository gives the user almost a \"bring me to the cloud\"-button for estimating DML models. 6 ESTIMATING DOUBLE MACHINE LEARNING MODELS WITH DOUBLEML-SERVERLESS To demonstrate our prototype implementation DoubleML-Serverless we revisit the Pennsylvania Reemployment Bonus experiment and estimate the effect of provisioning a cash bonus on the unemployment duration as studied in [19] . Experiment Setup We consider the previously discussed PLR model ( 1 )-( 2 ). The nuisance functions 𝑔 0 and 𝑚 0 are estimated using a random forest with 500 regression trees. 7 We choose 𝐾 = 5 folds and 𝑀 = 100 splits. At invocation, the following information is transferred to Lamb-daCVPredict: • The name of the outcome variable, e.g., for 𝑔 0 the 𝑌 column. • The names of the controls, e.g., for 𝑔 0 the 𝑋 columns. • The ML model to be estimated, e.g., random forest. • The set of indices 𝐼 𝑚,𝑘 . In Listing 1 we provide sample code which demonstrates the syntax to estimate the described DML model with DoubleML-Serverless for the bonus dataset. . Based on the evaluated score function, inference tasks like the computation of standard errors and confidence intervals that build on a multiplier bootstrap approach could be easily done locally using the functionalities of the DoubleML package. For further details, we refer to the paper introducing the DML framework [19] and the documentation of the DoubleML package [6] . 8 Timings and Costs To demonstrate the utility of our prototype DoubleML-Serverless we ran a couple of experiments on AWS Lambda with the above stated bonus data example. We especially focus on the two different settings for the scaling parameter, i.e., scaling = 'n_rep' for per-sample-split scaling and scaling = 'n_folds * n_rep' for per-fold scaling. With the above mentioned settings (𝐾 = 5 folds and 𝑀 = 100 splits) this amounts to 200 and 1000 invocations, respectively. Additionally, we also alter the memory available to the function at runtime which also impacts the CPU power, because AWS Lambda scales other resources proportionally to the allocated memory. All experiments are repeated 100 times and the estimation times and costs are visualized with boxplots in Figure 4 and 5 . In Figure 4 we can clearly see that the total estimation times for the DML models decrease if more memory is allocated. However, the marginal improvement in the estimation times is decreasing which is a typical behavior as for example documented in [1, 24] . It is also important to point out that faster estimation does not necessarily come at higher costs. In Figure 5 we see that by allocating more memory, 512 MB or 1024 MB instead of 256 MB, besides lowering the estimation time we could also lower the total costs for the estimation on AWS Lambda. The observation that too low or high memory allocations result in higher costs is also common for serverless computing with AWS Lambda and this observation has been used to propose cost optimization frameworks [1, 17] . When comparing the two different levels of scaling, we can see in Figure 4 that by choosing per-fold scaling the estimation times can be further decreased. It is important to note that the costs are only slightly increasing when going from per-sample-split to perfold scaling (see Figure 5 ). This is one of the benefits of serverless computing where one can increase the concurrency dramatically but still the billing is proportional to the actual computing time and therefore is often only slightly increased due to more overhead. Table 1 provides more detailed results for the cheapest case in our experiment which is the setting with 1024 MB memory allocated and per-sample-split scaling. We can see that in the 100 repetitions of our experiment the estimation time was on average 19.82 seconds. The response time from the invocation of the first lambda until we received the predictions from each of the 200 invocations took on average 19.09 seconds and the average computation time for a single invocation was 17.16 seconds. 9 Therefore, in this setting we are very close to the ambitious goal that using serverless computing the estimation of the DML model with repeated cross-fitting only takes a little bit more time than estimating with only a single sample split on a machine with similar CPU power as one lambda. In Table 1 we can further see that the average estimation costs amount to 3515.36 GB-seconds, which translates to roughly 0.05858 USD at the current price of 0.0000166667 USD per GB-second that AWS charges in eu-central-1 [5] . 10 DISCUSSIONS In the following, we discuss features, advantages and limits of the current prototype implementation DoubleML-Serverless and give an outlook to potential future extensions. Reproducibility and seeds: The prototype comes with a basic implementation of seeds to obtain reproducible results. We refer to Launch overhead & cold vs. warm invocations: It is well known that there is a launch overhead when using serverless computing which results in timing differences between so-called cold and warm starts. We report timings for warm starts and refer to [24, 26] for a discussion of the phenomenon. Transfer of ML models: The ML models are transferred at invocation using their string representation and only a subset of all scikit-learn ML-models is supported. To transfer more sophisticated learners, an alternative approach like pickling the learners similar to PyWren could be implemented. Data transfer via payloads: The prototype uses the payloads to transfer the test indices and to return the predictions. This implies some restrictions, which could be overcome by implementation of a data transfer via S3. Distributed storage: The datasets, which are loaded in every learning task, are stored in the Amazon S3 object storage. An alternative would be the AWS Elastic File System (EFS) which can be mounted directly for AWS Lambda calls [13] . Cost optimization: The main configuration parameter of AWS Lambda is the allocated memory. It is important to know that AWS Lambda allocates CPU power proportional to the amount of memory. Therefore, the memory allocation has an impact on the total execution time and the costs. Discussions and proposal for cost optimization of serverless applications are provided in [24, 43] and implementations of frameworks for cost optimization in [1, 17] . Similar approaches could also be used to cost-optimize our prototype DoubleML-Serverless. Limits on runtime and memory: Currently on AWS Lambda, there is an upper limit for execution time of 15 minutes. Obviously, our prototype cannot be used if the single fold estimation is not doable within this limit. Considering the previously discussed scenario with 𝐾 = 5 folds, 100 splits and two nuisance functions and assuming that the estimation of each task is of similar effort, this translates to a total estimation time limit of roughly 10.5 days (5 × 100 × 2 × 15 minutes). Note that in the past AWS Lambda regularly increased these limits. Limits on memory: Recently AWS Lambda announced a significant increase of their memory limit from 3 GB to 10 GB [3] . This implies that serverless computing is becoming increasingly suitable and attractive for memory-intense models and big data applications. For standard applications of DML these memory limits are not an issue. However, DML is particularly well suited for causal inference in high-dimensional settings and therefore also used for very big datasets. Realizing such estimations in very high-dimensional and big data sets with our prototype will be challenging. Parameter tuning for DML models: As usual in machine learning, hyperparameter tuning is also done for DML models. The prototype could be extended to also support hyperparameter tuning with an efficient serverless implementation. DML models with multiple treatment variables: The prototype implementation only supports a single treatment variable but an extension to multiple treatment variables, as supported by DoubleML, would be straightforward. CONCLUSION For many users like econometricians, statisticians and data scientists existing serverfull frameworks for distributed machine learning have a high entry barrier and are often expensive if being used infrequently or inefficiently. In this paper we explore serverless cloud computing for estimation of double machine learning models. Our prototype DoubleML-Serverless using AWS Lambda gives econometricians, statisticians and data scientists access to an enormous level of parallelism, it almost comes with a \"cloud button\" as it can be easily deployed via AWS SAM and it comes at the advantage of a pay-per-request pricing model. Figure 1 : 1 Figure 1: Causal Diagram for the PLR Model (1)-(2). 1 ) 1 For each 𝑚 ∈ [𝑀] := {1, . . . , 𝑀 } draw a 𝐾-fold random partition (𝐼 𝑚,𝑘 ) 𝑘 ∈𝐾 of observation indices [𝑁 ] := {1, . . . , 𝑁 } of size 𝑛 = 𝑁 /𝐾. Define 𝐼 𝑐 𝑚,𝑘 := [𝑁 ] \\ 𝐼 𝑚,𝑘 and for each 𝑘 construct a ML estimator η0,𝑘 = η0 ((𝑊 𝑖 ) 𝑖 ∈𝐼 𝑐 𝑚,𝑘 Figure 2 : 2 Figure 2: DoubleML-Serverless: Architecture. Figure 3 : 3 Figure 3: DoubleML-Serverless: Level of Scaling. Listing 1 : 1 Estimation of a Partially Linear Regression (PLR) Model with DoubleML-Serverless. from d o u b l e m l . d a t a s e t s import f e t c h _ b o n u s from d o u b l e m l _ s e r v e r l e s s import DoubleMLDataS3 , D o u b l e M L P L R S e r v e r l e s s from s k l e a r n . b a s e import c l o n e from s k l e a r n . e n s e m b l e import R a n d o m F o r e s t R e g r e s s o r d f _ b o n u s = f e t c h _ b o n u s ( ' DataFrame ' ) d m l _ d a t a _ b o n u s = DoubleMLDataS3 ( ' double ml -s e r v e r l e s s -d a t a ' , ' b o n u s _ d a t a . c s v ' , d f _ b o n u s , y _ c o l = ' i n u i d u r 1 ' , d _ c o l s = ' t g ' , x _ c o l s =[ ' f e m a l e ' , ' b l a c k ' , ' o t h r a c e ' , ' dep1 ' , ' dep2 ' , ' q2 ' , ' q3 ' , ' q4 ' , ' q5 ' , ' q6 ' , ' a g e l t 3 5 ' , ' a g e g t 5 4 ' , ' d u r a b l e ' , ' l u s d ' , ' husd ' ] ) d m l _ d a t a _ b o n u s . s t o r e _ a n d _ u p l o a d _ t o _ s 3 ( ) ml = R a n d o m F o r e s t R e g r e s s o r ( n _ e s t i m a t o r s = 5 0 0 , n _ j o b s = -1) ml_g = c l o n e ( ml ) ml_m = c l o n e ( ml ) d m l _ l a m b d a _ p l r _ b o n u s = D o u b l e M L P L R S e r v e r l e s s ( ' LambdaCVPredict ' , ' eu -c e n t r a l -1 ' , d m l _ d a t a _ b o n u s , ml_g , ml_m , n _ f o l d s = 5 , n _ r e p = 1 0 0 ) d m l _ l a m b d a _ p l r _ b o n u s . f i t _ a w s _ l a m b d a ( ) The FaaS function LambdaCVPredict returns predictions which are obtained by estimating the nuisance function based on the training indices 𝐼 𝑐 𝑚,𝑘 and then predictions are computed for all 𝑖 ∈ 𝐼 𝑚,𝑘 . When all requested predictions have been returned, the score function components for the PLR model at hand are obtained as 𝜓 𝑎 (𝑊 𝑖 ; η0 ) = -(𝐷 𝑖 -m0 (𝑋 𝑖 ))(𝐷 𝑖 -m0 (𝑋 𝑖 )), 𝜓 𝑏 (𝑊 𝑖 ; η0 ) = (𝑌 𝑖 -ĝ0 (𝑋 𝑖 ))(𝐷 𝑖 -m0 (𝑋 𝑖 )). Using the evaluated score function components, we can solve for the parameter estimate θ0 = -𝑁 𝑖=1 𝜓 𝑏 (𝑊 𝑖 ; η0 ) 𝑁 𝑖=1 𝜓 𝑎 (𝑊 𝑖 ; η0 ) Figure 4 : 4 Figure 4: Serverless Fit Times with Different Scaling and Allocated Memory. Figure 5 : 5 Figure 5: Serverless Costs with Different Scaling and Allocated Memory. Table 1 : 1 Serverless Fit Times and Costs with 1024 MB Memory and Per-Sample-Split Scaling (Mean, Min & Max in 100 Runs). Mean Min Max Fit Time (s) 19.82 19.53 21.49 Billed Duration (GB-s) 3515.36 3492.01 3571.42 Avg. Duration per Invocation (s) 17.16 17.05 17.44 Total Response Time (s) 19.09 18.81 20.76 the numpy documentation [35] for a discussion of parallel random number generation. GitHub: https://github.com/DoubleML/doubleml-serverless and AWS Serverless Application Repository: https://serverlessrepo.aws.amazon.com/applications/eu-central-1/839779594349/doubleml-serverless. We refer to [19, Section 3] for a discussion and the formal conditions for the quality of the nuisance estimators. As suggested by an anonymous referee, alternatively a fully serverless version could be implemented using services like AWS Step Functions to organize the serverless workflow. The prototype is tied to AWS Lambda. Adaptions of the data transfer and the deployment process would be necessary to make it compatible with other serverless platforms. https://serverlessrepo.aws.amazon.com/applications/eu-central-1/839779594349/ doubleml-serverless From the AWS Serverless Application Repository, the deployment can be done directly in the browser by clicking \"Deploy\" and following the steps in the AWS Management Console. In case of a binary treatment variable 𝐷, one can also use classifiers to estimate 𝑚 0 . https://docs.doubleml.org The maximum total response time of 20.76 seconds for 200 invocations, each with an computation time between 17.16 and 17.44 seconds (see Table1), also gives some indication that a high level of elasticity seems to be achievable. For an empirical evaluation of the elasticity of different FaaS platforms we refer to [31] . For comparison, the estimation of the same DML model on a virtual machine (AWS EC2 instance of type m5.2xlarge with 8 vCPUs) takes much longer with approximately 383.90 seconds and at the same time amounts to slightly lower costs of 0.04905 USD at the current price of 0.46 USD per hour when ignoring the additional costs from setup and teardown of the virtual machine."
}