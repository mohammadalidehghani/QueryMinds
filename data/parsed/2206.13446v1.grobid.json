{
  "title": "",
  "abstract": "We may have all heard the saying \"use it or lose it\". We experience it when we feel rusty in a foreign language or sports that we have not practised in a while. Practice is important to maintain skills but it is also key when learning new ones. This is a reason why many textbooks and courses feature exercises. However, the solutions to the exercises feel often overly brief, or are sometimes not available at all. Rather than an opportunity to practice the new skills, the exercises then become a source of frustration and are ignored. This book contains a collection of exercises with detailed solutions. The level of detail is, hopefully, sufficient for the reader to follow the solutions and understand the techniques used. The exercises, however, are not a replacement of a textbook or course on machine learning. I assume that the reader has already seen the relevant theory and concepts and would now like to deepen their understanding through solving exercises. While coding and computer simulations are extremely important in machine learning, the exercises in the book can (mostly) be solved with pen and paper. The focus on penand-paper exercises reduced length and simplified the presentation. Moreover, it allows the reader to strengthen their mathematical skills. However, the exercises are ideally paired with computer exercises to further deepen the understanding. The exercises collected here are mostly a union of exercises that I developed for the courses \"Unsupervised Machine Learning\" at the University of Helsinki and \"Probabilistic Modelling and Reasoning\" at the University of Edinburgh. The exercises do not comprehensively cover all of machine learning but focus strongly on unsupervised methods, inference and learning. I am grateful to my students for providing feedback and asking questions. Both helped to improve the quality of the exercises and solutions. I am further grateful to both universities for providing the research and teaching environment. My hope is that the collection of exercises will grow with time. I intend to add new exercises in the future and welcome contributions from the community. Latex source code is available at https://github.com/michaelgutmann/ml-pen-and-paper-exercises . Please use GitHub's issues to report mistakes or typos, and please get in touch if you would like to make larger contributions.",
  "introduction": "Contents",
  "body": "Contents Gram-Schmidt orthogonalisation (a) Given two vectors a 1 and a 2 in R n , show that u 1 = a 1 (1.1) u 2 = a 2 - u 1 a 2 u 1 u 1 u 1 (1.2) are orthogonal to each other. Solution. Two vectors u 1 and u 2 of R n are orthogonal if their inner product equals zero. Computing the inner product u 1 u 2 gives u 1 u 2 = u 1 (a 2 - u 1 a 2 u 1 u 1 u 1 ) (S.1.1) = u 1 a 2 - u 1 a 2 u 1 u 1 u 1 u 1 (S.1.2) = u 1 a 2 -u 1 a 2 (S.1.3) = 0. (S.1.4) Hence the vectors u 1 and u 2 are orthogonal. If a 2 is a multiple of a 1 , the orthogonalisation procedure produces a zero vector for u 2 . To see this, let a 2 = αa 1 for some real number α. We then obtain u 2 = a 2 - u 1 a 2 u 1 u 1 u 1 (S.1.5) = αu 1 - αu 1 u 1 u 1 u 1 u 1 (S.1.6) = αu 1 -αu 1 (S.1.7) = 0. (S.1.8) (b) Show that any linear combination of (linearly independent) a 1 and a 2 can be written in terms of u 1 and u 2 . Solution. Let v be a linear combination of a 1 and a 2 , i.e. v = αa 1 + βa 2 for some real numbers α and β. Expressing u 1 and u 2 in term of a 1 and a 2 , we can write v as v = αa 1 + βa 2 (S.1.9) = αu 1 + β(u 2 + u 1 a 2 u 1 u 1 u 1 ) (S.1.10) = αu 1 + βu 2 + β u 1 a 2 u 1 u 1 u 1 (S.1.11) = (α + β u 1 a 2 u 1 u 1 )u 1 + βu 2 , (S.1.12) Since α + β((u 1 a 2 )/(u 1 u 1 )) and β are real numbers, we can write v as a linear combination of u 1 and u 2 . Overall, this means that any vector in the span of {a 1 , a 2 } can be expressed in the orthogonal basis {u 1 , u 2 }. (c) Show by induction that for any k ≤ n linearly independent vectors a 1 , . . . , a k , the vectors u i , i = 1, . . . k, are orthogonal, where u i = a i - i-1 j=1 u j a i u j u j u j . (1. 3) The calculation of the vectors u i is called Gram-Schmidt orthogonalisation. Solution. We have shown above that the claim holds for two vectors. This is the base case for the proof by induction. Assume now that the claim holds for k vectors. The induction step in the proof by induction then consists of showing that the claim also holds for k + 1 vectors. Assume that u 1 , u 2 , . . . , u k are orthogonal vectors. The linear independence assumption ensures that none of the u i is a zero vector. We then have for u k+1 u k+1 = a k+1 - u 1 a k+1 u 1 u 1 u 1 - u 2 a k+1 u 2 u 2 u 2 -. . . - u k a k+1 u k u k u k , (S.1.13) and for all i = 1, 2, . . . , k u i u k+1 = u i a k+1 - u 1 a k+1 u 1 u 1 u i u 1 -. . . - u k a k+1 u k u k u i u k . (S.1.14) By assumption u i u j = 0 if i = j, so that u i u k+1 = u i a k+1 -0 -. . . - u i a k+1 u i u i u i u i -0 . . . -0 (S.1.15) = u i a k+1 -u i a k+1 (S.1.16) = 0, (S.1.17) which means that u k+1 is orthogonal to u 1 , . . . , u k . (d) Show by induction that any linear combination of (linear independent) a 1 , a 2 , . . . , a k can be written in terms of u 1 , u 2 , . . . , u k . Solution. The base case of two vectors was proved above. Using induction, we assume that the claim holds for k vectors and we will prove that it then also holds for k + 1 vectors: Let v be a linear combination of a 1 , a 2 , . . . , a k+1 , i.e. v = α 1 a 1 + α 2 a 2 + . . . + α k a k + α k+1 a k+1 for some real numbers α 1 , α 2 , . . . , α k+1 . Using the induction assumption, v can be written as v = β 1 u 1 + β 2 u 2 + . . . + β k u k + α k+1 a k+1 , (S.1.18) for some real numbers β 1 , β 2 , . . . , β k Furthermore, using equation (S.1.13), v can be written as v = β 1 u 1 + . . . + β k u k + α k+1 u k+1 + α k+1 u 1 a k+1 u 1 u 1 u 1 (S.1.19) + . . . + α k+1 u k a k+1 u k u k u k . (S.1.20) With γ i = β i + α k+1 (u i a k+1 )/(u i u i ), v can thus be written as v = γ 1 u 1 + γ 2 u 2 + . . . + γ k u k + α k+1 u k+1 , (S.1.21) which completes the proof. Overall, this means that the u 1 , u 2 , . . . , u k form an orthogonal basis for span(a 1 , . . . , a k ), i.e. the set of all vectors that can be obtained by linearly combining the a i . (e) Consider the case where a 1 , a 2 , . . . , a k are linearly independent and a k+1 is a linear combination of a 1 , a 2 , . . . , a k . Show that u k+1 , computed according to (1.3), is zero. Solution. Starting with (1.3), we have u k+1 = a k+1 - k j=1 u j a k+1 u j u j u j . (S.1.22) By assumption, a k+1 is a linear combination of a 1 , a 2 , . . . , a k . By the previous question, it can thus also be written as a linear combination of the u 1 , . . . , u k . This means that there are some β i so that a k+1 = k i=1 β i u i (S.1.23) holds. Inserting this expansion into the equation above gives u k+1 = k i=1 β i u i - k j=1 k i=1 β i u j u i u j u j u j (S.1.24) = k i=1 β i u i - k i=1 β i u i u i u i u i u i (S.1.25) because u j u i = 0 if i = j. We thus obtain the desired result: u k+1 = k i=1 β i u i - k i=1 β i u i (S.1.26) = 0 (S.1.27) This property of the Gram-Schmidt process in (1.3) can be used to check whether a list of vectors a 1 , a 2 , . . . , a d is linearly independent or not. If, for example, u k+1 is zero, a k+1 is a linear combination of the a 1 , . . . , a k . Moreover, the result can be used to extract a sublist of linearly independent vectors: We would remove a k+1 from the list and restart the procedure in (1.3) with a k+2 taking the place of a k+1 . Continuing in this way constructs a list of linearly independent a j and orthogonal u j , j = 1, . . . , r, where r is the number of linearly independent vectors among the a 1 , a 2 , . . . , a d . Linear transforms (a) Assume two vectors a 1 and a 2 are in R 2 . Together, they span a parallelogram. Use Exercise 1.1 to show that the squared area S 2 of the parallelogram is given by S 2 = (a T 2 a 2 )(a T 1 a 1 ) -(a T 2 a 1 ) 2  (1.4) Solution. Let a 1 and a 2 be the vectors that span the parallelogram. From geometry we know that the area of parallelogram is base times height, which is equivalent to the length of the base vector times the length of the height vector. Denote this by S 2 = ||a 1 || 2 ||u 2 || 2 , where is a 1 is the base vector and u 2 is the height vector which is orthogonal to the base vector. Using the Gram-Schmidt process for the vectors a 1 and a 2 in that order, we obtain the vector u 2 as the second output. u 2 a 1 a 2 a T 1 a 2 a T 1 a 1 a 1 Therefore ||u 2 || 2 equals ||u 2 || 2 = u 2 u 2 (S.1.28) = a 2 - a 1 a 2 a 1 a 1 a 1 a 2 - a 1 a 2 a 1 a 1 a 1 (S.1.29) = a 2 a 2 - (a 1 a 2 ) 2 a 1 a 1 - (a 1 a 2 ) 2 a 1 a 1 + a 1 a 2 a 1 a 1 2 a 1 a 1 (S.1.30) = a 2 a 2 - (a 1 a 2 ) 2 a 1 a 1 . (S.1.31) Thus, S 2 is: S 2 = ||a 1 || 2 ||u 2 || 2 (S.1.32) = (a 1 a 1 )(u 2 u 2 ) (S.1.33) = (a 1 a 1 ) a 2 a 2 - (a 1 a 2 ) 2 a 1 a 1 (S.1.34) = (a 2 a 2 )(a 1 a 1 ) -(a 1 a 2 ) 2 . (S.1.35) (b) Form the matrix A = (a 1 a 2 ) where a 1 and a 2 are the first and second column vector, respectively. Show that S 2 = (det A) 2 . (1.5) Solution. We form the matrix A, A = a 1 a 2 = a 11 a 12 a 21 a 22 . (S.1.36) The determinant of A is det A = a 11 a 22 -a 12 a 21 . By multiplying out (a 2 a 2 ), (a 1 a 1 ) and (a 1 a 2 ) 2 , we get a 2 a 2 = a 2 12 + a 2 22 (S.1.37) a 1 a 1 = a 2 11 + a 2 21 (S.1.38) (a 1 a 2 ) 2 = (a 11 a 12 + a 21 a 22 ) 2 = a 2 11 a 2 12 + a 2 21 a 2 22 + 2a 11 a 12 a 21 a 22 . (S.1.39) Therefore the area equals S 2 = (a 2 12 + a 2 22 )(a 2 11 + a 2 21 ) -(a 1 a 2 ) 2 (S.1.40) = a 2 12 a 2 11 + a 2 12 a 2 21 + a 2 22 a 2 11 + a 2 22 a 2 21 -(a 2 12 a 2 11 + a 2 21 a 2 22 + 2a 11 a 12 a 21 a 22 ) (S.1.41) = a 2 12 a 2 21 + a 2 22 a 2 11 -2a 11 a 12 a 21 a 22 (S.1.42) = (a 11 a 22 -a 12 a 21 ) 2 , (S.1.43) which equals (det A) 2 . (c) Consider the linear transform y = Ax where A is a 2 × 2 matrix. Denote the image of the rectangle U x = [x 1 x 1 + 1 ] × [x 2 x 2 + 2 ] under the transform A by U y . What is U y ? What is the area of U y ? Solution. U y is parallelogram that is spanned by the column vectors a 1 and a 2 of A, when A = (a 1 a 2 ). A rectangle with the same area as U x is spanned by vectors (∆ 1 , 0) and (0, ∆ 2 ). Under the linear transform A these spanning vectors become ∆ 1 a 1 and ∆ 2 a 2 . Therefore a parallelogram with the same area as U y is spanned by ∆ 1 a 1 and ∆ 2 a 2 as shown in the following figure. U x x 2 x 1 + ∆ 1 x 2 + ∆ 2 x 1 x 1 x 1 + ∆ 1 x 2 + ∆ 2 x 2 a 1 a 2 U y From the previous question, the A Uy of U y equals the absolute value of the determinant of the matrix (∆ 1 a 1 ∆ 2 a 2 ): where A is such that U x is an axis-aligned (hyper-) rectangle as in the previous question. A Uy = | det ∆ 1 a 11 ∆ 2 a 12 ∆ 1 a 21 ∆ Solution. We can think that, loosely speaking, the two integrals are limits of the following two sums y i ∈Uy f (y i )vol(∆ y i ) x i ∈Ux f (Ax i )| det A|vol(∆ x i ) (S.1.48) where x i = A -1 y i , which means that x and y are related by y = Ax. The set of function values f (y i ) and f (Ax i ) that enter the two sums are exactly the same. The volume vol(∆ x i ) of a small axis-aligned hypercube (in d dimensions) equals d i=1 ∆ i . The image of this small axis-aligned hypercube under A is a parallelogram ∆ y i with volume vol(∆ y i ) = | det A|vol(∆ x i ). Hence y i ∈Uy f (y i )vol(∆ y i ) = x i ∈Ux f (Ax i )| det A|vol(∆ x i ). (S.1.49) We must have the term | det A| to compensate for the fact that the volume of U x and U y are not the same. For example, let A be a diagonal matrix diag(10, 100) so that U x is much smaller than U y . The determinant det A = 1000 then compensates for the fact that the x i values are more condensed than the y i . Eigenvalue decomposition For a square matrix A of size n × n, a vector u i = 0 which satisfies Au i = λ i u i (1.7) is called a eigenvector of A, and λ i is the corresponding eigenvalue. For a matrix of size n × n, there are n eigenvalues λ i (which are not necessarily distinct). (a) Show that if u 1 and u 2 are eigenvectors with λ 1 = λ 2 , then u = αu 1 + βu 2 is also an eigenvector with the same eigenvalue. Solution. We compute Au = αAu 1 + βAu 2 (S.1.50) = αλu 1 + βλu 2 (S.1.51) = λ(αu 1 + βu 2 ) (S.1.52) = λu, (S.1.53) so u is an eigenvector of A with the same eigenvalue as u 1 and u 2 . (b) Assume that none of the eigenvalues of A is zero. Denote by U the matrix where the column vectors are linearly independent eigenvectors u i of A. Verify that (1.7) can be written in matrix form as AU = UΛ, where Λ is a diagonal matrix with the eigenvalues λ i as diagonal elements. Solution. By basic properties of matrix multiplication, we have AU = ( Au 1 Au 2 . . . Au n ) (S.1.54) With Au i = λ i u i for all i = 1, 2, . . . , n, we thus obtain AU = (λ 1 u 1 λ 2 u 2 . . . λ n u n ) (S.1.55) = UΛ. (S.1.56) (c) Show that we can write, with V T = U -1 , A = UΛV , A = n i=1 λ i u i v T i , (1.8) A -1 = UΛ -1 V , A -1 = n i=1 1 λ i u i v i , (1.9) where v i is the i-th column of V. Solution. (i) Since the columns of U are linearly independent, U is invertible. Because AU = UΛ, multiplying from the right with the inverse of U gives A = UΛU -1 = UΛV . (ii) Denote by u [i] the ith row of U, v (j) the jth column of V and v [j] the jth row of V and denote B = n i=1 λ i u i v i . Let e [i] be a row vector with 1 in the ith place and 0 elsewhere and e (j) be a column vector with 1 in the jth place and 0 elsewhere. Notice that because A = UΛV , the element in the ith row and jth column is (j)  (S.1.57) A ij = u [i] Λv = u [i] Λv [j]  (S.1.58) = u [i]    λ 1 V j1 . . . λ n V jn    (S.1.59) = n k=1 λ k V jk U ik . (S.1.60) On the other hand, for matrix B the element in the ith row and jth column is B ij = n k=1 λ k e [i] u k v k e (j) (S.1.61) = n k=1 λ k U ik V jk , (S.1.62) which is the same as A ij . Therefore A = B. (iii) Since Λ is a diagonal matrix with no zeros as diagonal elements, it is invertible. We have thus A -1 = (UΛU -1 ) -1 (S.1.63) = (ΛU -1 ) -1 U -1 (S.1.64) = UΛ -1 U -1 (S.1.65) = UΛ -1 V . (S.1.66) (iv) This follows from A = UΛV = i u i λ i v i , when λ i is replaced with 1/λ i . 1.4 Trace, determinants and eigenvalues where, in the last line, we have used that the determinant of a diagonal matrix is the product of its elements. Eigenvalue decomposition for symmetric matrices (a) Assume that a matrix A is symmetric, i.e. A = A. Let u 1 and u 2 be two eigenvectors of A with corresponding eigenvalues λ 1 and λ 2 , with λ 1 = λ 2 . Show that the two vectors are orthogonal to each other. Solution. Since Au 2 = λ 2 u 2 , we have u 1 Au 2 = λ 2 u 1 u 2 . (S.1.76) Taking the transpose of u 1 Au 2 gives (u 1 Au 2 ) = (Au 2 ) (u 1 ) = u 2 A u 1 = u 2 Au 1 (S.1.77) = λ 1 u 2 u 1 (S.1.78) because A is symmetric and Au 1 = λ 1 u 1 . On the other hand, the same operation gives (u 1 Au 2 ) = (λ 2 u 1 u 2 ) = λ 2 u 2 u 1 (S.1.79) Therefore λ 1 u 2 u 1 = λ 2 u 2 u 1 , which is equivalent to u 2 u 1 (λ 1 -λ 2 ) = 0. Because λ 1 = λ 2 , the only possibility is that u 2 u 1 = 0. Therefore u 1 and u 2 are orthogonal to each other. The result implies that the eigenvectors of a symmetric matrix A with distinct eigenvalues λ i forms an orthogonal basis. The result extends to the case where some of the eigenvalues are the same (not proven). (b) A symmetric matrix A is said to be positive definite if v T Av > 0 for all non-zero vectors v. Show that positive definiteness implies that λ i > 0, i = 1, . . . , M . Show that, vice versa, λ i > 0, i = 1 . . . M implies that the matrix A is positive definite. Conclude that a positive definite matrix is invertible. Solution. Assume that v Av > 0 for all v = 0. Since eigenvectors are not zero vectors, the assumption holds also for eigenvector u k with corresponding eigenvalue λ k . Now u k Au k = u k λ k u k = λ k (u k u k ) = λ k ||u k || > 0 (S.1.80) and because ||u k || > 0, we obtain λ k > 0. Assume now that all the eigenvalues of A, λ 1 , λ 2 , . . . , λ n , are positive and nonzero. We have shown above that there exists an orthogonal basis consisting of eigenvectors u 1 , u 2 , . . . , u n and therefore every vector v can be written as a linear combination of those vectors (we have only shown it for the case of distinct eigenvalues but it holds more generally). Hence for a nonzero vector v and for some real numbers α 1 , α 2 , . . . , α n , we have v Av = (α 1 u 1 + + . . . + α n u n ) A(α 1 u 1 + . . . + α n u n ) (S.1.81) = (α 1 u 1 + . . . + α n u n ) (α 1 Au 1 + . . . + α n Au n ) (S.1.82) = (α 1 u 1 + . . . + α n u n ) (α 1 λ 1 u 1 + . . . + α n λ n u n ) (S.1.83) = i,j α i u i α j λ j u j (S.1.84) = i α i α i λ i u i u i (S.1.85) = i (α i ) 2 ||u i || 2 λ i , (S.1.86) where we have used that u T i u j = 0 if i = j, due to orthogonality of the basis. Since (α i ) 2 > 0, ||u i || 2 > 0 and λ i > 0 for all i, we find that v Av > 0. Since every eigenvalue of A is nonzero, we can use Exercise 1.3 to conclude that inverse of A exists and equals i 1/λ i u i u i . Power method We here analyse an algorithm called the \"power method\". The power method takes as input a positive definite symmetric matrix Σ Σ Σ and calculates the eigenvector that has the largest eigenvalue (the \"first eigenvector\"). For example, in case of principal component analysis, Σ Σ Σ is the covariance matrix of the observed data and the first eigenvector is the first principal component direction. The power method consists in iterating the update equations where Λ is the diagonal matrix with eigenvalues λ i of Σ Σ Σ as diagonal elements. Let the eigenvalues be ordered λ 1 > λ 2 > . . . > λ n > 0 (and, as additional assumption, all distinct). v k+1 = Σ Σ Σw k , w k+1 = v k+1 ||v k+1 || 2 , ( 1 (b) Let ṽk = U T v k and wk = U T w k . Write the update equations of the power method in terms of ṽk and wk . This means that we are making a change of basis to represent the vectors w k and v k in the basis given by the eigenvectors of Σ Σ Σ. Solution. With v k+1 = Σ Σ Σw k (S.1.88) = UΛU w k (S.1.89) we obtain U v k+1 = ΛU w k . (S.1.90) Hence ṽk+1 = Λ wk . The norm of ṽk+1 is the same as the norm of v k+1 : ||ṽ k+1 || 2 = ||U v k+1 || 2 (S.1.91) = (U v k+1 ) (U v k+1 ) (S.1.92) = v k+1 UU v k+1 (S.1.93) = v k+1 v k+1 (S.1.94) = ||v k+1 || 2 . (S.1.95) Hence, the update equation, in terms of ṽk and wk , is ṽk+1 = Λ wk , wk+1 = ṽk+1 ||ṽ k+1 || . (S.1.96) (c) Assume you start the iteration with w0 . To which vector w * does the iteration converge to? Solution. Let w0 = α 1 α 2 . . . α n . Since Λ is a diagonal matrix, we obtain ṽ1 =      λ 1 α 1 λ 2 α 2 . . . λ n α n      = λ 1 α 1      1 α 2 α 1 λ 2 λ 1 . . . αn α 1 λn λ 1      (S.1.97) and therefore w1 = λ 1 α 1 c 1      1 α 2 α 1 λ 2 λ 1 . . . αn α 1 λn λ 1      , (S.1.98) where c 1 is a normalisation constant such that w1 = 1 (i.e. c 1 = ṽ1 ). Hence, for wk it holds that wk = ck        1 α 2 α 1 λ 2 λ 1 k . . . αn α 1 λn λ 1 k        , (S.1.99) where ck is again a normalisation constant such that || wk || = 1. As λ 1 is the dominant eigenvalue, |λ j /λ 1 | < 1 for j = 2, 3, . . . , n, so that lim k→∞ λ j λ 1 k = 0, j = 2, 3, . . . , n, (S.1.100) and hence lim k→∞        1 α 2 α 1 λ 2 λ 1 k . . . αn α 1 λn λ 1 k        =      1 0 . . . 0      . (S.1.101) For the normalisation constant ck , we obtain ck = 1 1 + n i=2 α i α 1 2 λ i λ 1 2k , (S.1.102) and therefore lim k→∞ ck = 1 1 + n i=2 α i α 1 2 lim k→∞ λ i λ 1 2k (S.1.103) = 1 1 + n i=2 α i α 1 2 • 0 (S.1.104) = 1. (S.1.105) The limit of the product of two convergent sequences is the product of the limits so that lim k→∞ wk = lim k→∞ ck lim k→∞        1 α 2 α 1 λ 2 λ 1 k . . . αn α 1 λn λ 1 k        =      1 0 . . . 0      . (S.1.106) (d) Conclude that the power method finds the first eigenvector. Solution. Since w k = U wk , we obtain lim k→∞ w k = U      1 0 . . . 0      = u 1 , (S.1.107) which is the eigenvector with the largest eigenvalue, i.e. the \"first\" or \"dominant\" eigenvector. Gradient of vector-valued functions For a function J that maps a column vector w ∈ R n to R, the gradient is defined as ∇J(w) =     ∂J(w) ∂w 1 . . . ∂J(w) ∂wn     , (2.1) where ∂J(w)/∂w i are the partial derivatives of J(w) with respect to the i-th element of the vector w = (w 1 , . . . , w n ) (in the standard basis). Alternatively, it is defined to be the column vector ∇J(w) such that J(w + h) = J(w) + (∇J(w)) h + O( 2 ) (2.2) for an arbitrary perturbation h. This phrases the derivative in terms of a first-order, or affine, approximation to the perturbed function J(w + h). The derivative ∇J is a linear transformation that maps h ∈ R n to R (see e.g. Rudin, 1976, Chapter 9, for a formal treatment of derivatives). Use either definition to determine ∇J(w) for the following functions where a ∈ R n , A ∈ R n×n and f : R → R is a differentiable function. (a) J(w) = a w. Solution. First method: J(w) = a w = n k=1 a k w k =⇒ ∂J(w) ∂w i = a i (S.2.1) Hence ∇J(w) =      a 1 a 2 . . . a n      = a. (S.2.2) Second method: J(w + h) = a (w + h) = a w J(w) + a h ∇J h (S.2.3) Hence we find again ∇J(w) = a. (b) J(w) = w Aw. Solution. First method: We start with J(w) = w Aw = n i=1 n j=1 w i A ij w j (S.2.4) Hence, ∂J(w) ∂w k = n j=1 A kj w j + n i=1 w i A ik (S.2.5) = n j=1 A kj w j + n i=1 w i (A ) ki (S.2.6) = m j=1 A kj + (A ) kj w j (S.2.7) where we have used that the entry in row i and column k of the matrix A equals the entry in row k and column i of its transpose A . It follows that ∇J(w) =    n j=1 A 1j + (A ) 1j w j . . . n j=1 A nj + (A ) nj w j    (S.2.8) = (A + A )w, (S.2.9) where we have used that sums like j B ij w j are equal to the i-th element of the matrix-vector product Bw. Second method: J(w + h) = (w + h) A(w + h) (S.2.10) = w Aw + w A( h) + h Aw + h A h O( 2 ) (S.2.11) = w Aw + (w Ah + w A h) + O( 2 ) (S.2.12) = w Aw J(w) + (w A + w A ∇J(w) )h + O( 2 ) (S.2.13) where we have used that h Aw is a scalar so that h Aw = (h Aw) = w A h. Hence ∇J(w) = w A + w A = w (A + A ) (S.2.14) and ∇J(w) = (A + A )w. (S.2.15) (c) J(w) = w w. Solution. The easiest way to calculate the gradient of J(w) = w w is to use the previous question with A = I (the identity matrix). Therefore ∇J(w) = Iw + I w = w + w = 2w. (S.2.16) (d) J(w) = ||w|| 2 . Solution. Note that ||w|| 2 = √ w w. First method: We use the chain rule ∂J(w) ∂w k = ∂ √ w w ∂w w ∂w w ∂w k (S.2.17) and that ∂ √ w w ∂w w = 1 2 √ w w (S.2.18) The derivatives ∂w w/∂w k were calculated in the question above so that ∇J(w) = 1 2 √ w w 2w = w ||w|| 2 (S.2.19) Second method: Let f (w) = w w. From the previous question, we know that f (w + h) = f (w) + 2w h + O( 2 ). (S.2.20) Moreover, z + u + O( 2 ) = √ z + 1 2 √ z ( u + O( 2 )) + O( 2 ) (S.2.21) = √ z + 1 2 √ z u + O( 2 ) (S.2.22) With z = f (w) and u = 2w h, we thus obtain J(w + h) = f (w + h) (S.2.23) = f (w) + 1 2 f (w) 2w h + O( 2 ) (S.2.24) = f (w) + w f (w) h + O( 2 ) (S.2.25) = J(w) + w ||w|| 2 h + O( 2 ) (S.2.26) so that ∇J(w) = w ||w|| 2 . (S.2.27) (e) J(w) = f (||w|| 2 ). Solution. Either the chain rule or the approach with the Taylor expansion can be used to deal with the outer function f . In any case: ∇J(w) = f (||w|| 2 )∇||w|| 2 = f (||w|| 2 ) w ||w|| 2 , (S.2.28) where f is the derivative of the function f . (f) J(w) = f (w a). Solution. We have seen that ∇ w a w = a. Using the chain rule then yields ∇J(w) = f (w a)∇(w a) (S.2.29) =f (w a)a (S.2.30) Newton's method Assume that in the neighbourhood of w 0 , a function J(w) can be described by the quadratic approximation f (w) = c + g (w -w 0 ) + 1 2 (w -w 0 ) H(w -w 0 ), (2.3) where c = J(w 0 ), g is the gradient of J with respect to w, and H a symmetric positive definite matrix (e.g. the Hessian matrix for J(w) at w 0 if positive definite). (a) Use Exercise 2.1 to determine ∇f (w). Solution. We first write f as f (w) = c + g (w -w 0 ) + 1 2 (w -w 0 ) T H(w -w 0 ) (S.2.31) = c -g w 0 + 1 2 w 0 Hw 0 + g w + 1 2 w Hw - 1 2 w 0 Hw - 1 2 w Hw 0 (S.2.32) Using now that w Hw 0 is a scalar and that H is symmetric, we have w Hw 0 = (w Hw 0 ) = w 0 H w = w 0 Hw (S.2.33) and hence f (w) = const + (g -w 0 H)w + 1 2 w Hw (S.2.34) With the results from Exercise 2.1 and the fact that H is symmetric, we thus obtain ∇f (w) = g -H w 0 + 1 2 (H w + Hw) (S.2.35) = g -Hw 0 + Hw (S.2.36) The expansion of f (w) due to the ww 0 terms is a bit tedious. It is simpler to note that gradients define a linear approximation of the function. We can more efficiently deal with ww 0 by changing the coordinates and determine the linear approximation of f as a function of v = ww 0 , i.e. locally around the point w 0 . We then have f (v) = f (v + w 0 ) (S.2.37) = c + g v + 1 2 v Hv (S.2.38) With Exercise 2.1, the derivative is ∇ v f (v) = g + Hv (S.2.39) and the linear approximation becomes f (v + h) = c + (g + Hv) h + O( 2 ) (S.2.40) The linear approximation for f determines a linear approximation of f around w 0 , i.e. f (w + h) = f (w -w 0 + h) = c + (g + H(w -w 0 )) h + O( 2 ) (S.2.41) so that the derivative for f is ∇ w f (w) = g + H(w -w 0 ) = g -Hw 0 + Hw, (S.2.42) which is the same result as before. (b) A necessary condition for w being optimal (leading either to a maximum, minimum or a saddle point) is ∇f (w) = 0. Determine w * such that ∇f (w) w=w * = 0. Provide arguments why w * is a minimiser of f (w). Solution. We set the gradient to zero and solve for w: g + H(w -w 0 ) = 0 ↔ w -w 0 = -H -1 g (S.2.43) so that w * = w 0 -H -1 g. (S.2.44) As we assumed that H is positive definite, the inverse H exists (and is positive definite too). Let us consider f as a function of v around w * , i.e. w = w * +v. With w * +v -w 0 = -H -1 g + v, we have f (w * + v) = c + g (-H -1 g + v) + 1 2 (-H -1 g + v) H(-H -1 g + v) (S.2.45) Since H is positive definite, we have that (- H -1 g + v) H(-H -1 g + v) > 0 for all v. Hence, as we move away from w * , the function increases quadratically, so that w * minimises f (w). (c) In terms of Newton's method to minimise J(w), what do w 0 and w * stand for? Solution. The equation w * = w 0 -H -1 g. (S.2.46) corresponds to one update step in Newton's method where w 0 is the current value of w in the optimisation of J(w) and w * is the updated value. In practice rather than determining the inverse H -1 , we solve Hp = g (S.2.47) for p and then set w * = w 0 -p. The vector p is the search direction, and it is possible include a step-length α so that the update becomes w * = w 0 -αp. The value of α may be set by hand or can be determined via line-search methods (see e.g. Nocedal and Wright, 1999). Gradient of matrix-valued functions For functions J that map a matrix W ∈ R n×m to R, the gradient is defined as ∇J(W) =     ∂J(W) ∂W 11 . . . ∂J(W) ∂W 1m . . . . . . . . . ∂J(W) ∂W n1 . . . ∂J(W) ∂Wnm     . (2.4) Alternatively, it is defined to be the matrix ∇J such that J(W + H) = J(w) + tr(∇J H) + O( 2 ) (2.5) = J(w) + tr(∇JH ) + O( 2 ) (2.6) This definition is analogue to the one for vector-valued functions in (2.2). It phrases the derivative in terms of a linear approximation to the perturbed objective J(W + H) and, more formally, tr ∇J is a linear transformation that maps H ∈ R n×m to R (see e.g. Rudin, 1976 , Chapter 9, for a formal treatment of derivatives). Let e (i) be column vector which is everywhere zero but in slot i where it is 1. Moreover let e [j] be a row vector which is everywhere zero but in slot j where it is 1. The outer product e (i) e [j] is then a matrix that is everywhere zero but in row i and column j where it is one. For H = e (i) e [j] , we obtain J(W + e (i) e [j] ) = J(W) + tr((∇J) e (i) e [j] ) + O( 2 ) (2.7) = J(W) + e [j] (∇J) e (i) + O( 2 ) (2.8) = J(W) + e [i] ∇Je (j) + O( 2 ) (2.9) Note that e [i] ∇Je (j) picks the element of the matrix ∇J that is in row i and column j, i.e. e [i] ∇Je (j) = ∂J/∂W ij . Use either of the two definitions to find ∇J(W) for the functions below, where u ∈ R n , v ∈ R m , A ∈ R n×m , and f : R → R is differentiable. (a) J(W) = u Wv. Solution. First method: With J(W) = n i=1 m j=1 u i W ij v j we have ∂J(W) W kl = u k v l = (uv ) kl (S.2.48) and hence ∇J(W) = uv (S.2.49) Second method: J(W + H) = u (W + H)v (S. 2.50) = J(W) + u Hv (S.2.51) = J(W) + tr(u Hv) (S.2.52) = J(W) + tr(vu H) (S.2.53) Hence: ∇J(W) = uv (S.2.54) (b) J(W) = u (W + A)v. Solution. Expanding the objective function gives J(W) = u Wv + u Av. The second term does not depend on W. With the previous question, the derivative thus is ∇J(W) = uv (S.2.55) (c) J(W) = n f (w n v) , where w n are the rows of the matrix W. Solution. First method: ∂J(W) ∂W ij = n k=1 ∂ ∂W ij f (w k v) (S.2.56) = f (w i v) ∂ ∂W ij w i v m j=1 W ij v j (S.2.57) = f (w i v)v j (S.2.58) Hence ∇J(W) = f (Wv)v , (S.2.59) where f operates element-wise on the vector Wv. Second method: J(W) = n k=1 f (w k v) (S.2.60) = n k=1 f (e [k] Wv), (S.2.61) where e [k] is the unit row vector that is zero everywhere but for element k which equals one. We now perform a perturbation of W by H. J(W + H) = n k=1 f (e [k] (W + H)v) (S.2.62) = n k=1 f (e [k] Wv + e [k] Hv) (S.2.63) = n k=1 (f (e [k] Wv) + f (e [k] Wv)e [k] Hv + O( 2 ) (S.2.64) = J(W) + n k=1 f (e [k] Wv)e [k] Hv + O( 2 ) (S.2.65) The term f (e [k] Wv)e [k] is a row vector that equals (0, . . . , 0, f (e [k] Wv), 0, . . . , 0). Hence, we have n k=1 f (e [k] Wv)e [k] ) = f (Wv) (S.2.66) where f operates element-wise on the column vector Wv. The perturbed objective function thus is J(W + H) = J(W) + f (Wv) Hv + O( 2 ) (S.2.67) = J(W) + tr f (Wv) Hv + O( 2 ) (S.2.68) = J(W) + tr vf (Wv) H + O( 2 ) (S.2.69) Hence, the gradient is the transpose of vf (Wv) , i.e. ∇J(W) = f (Wv)v (S.2.70) (d) J(W) = u W -1 v (Hint: (W + H) -1 = W -1 -W -1 HW -1 + O( 2 ).) Solution. We first verify the hint: W -1 -W -1 HW -1 + O( 2 ) (W + H) = I + W -1 H -W -1 H + O( 2 ) (S.2.71) = I + O( 2 ) (S.2.72) Hence the identity holds up to terms smaller than 2 , which is sufficient we do not care about terms of order 2 and smaller in the definition of the gradient in (2.5). Let us thus make a first-order approximation of the perturbed objective J(W + H): J(W + H) = u (W + Hv) -1 v (S.2.73) hint = u (W -1 -W -1 HW -1 + O( 2 ))v (S.2.74) = u W -1 v -u W -1 HW -1 v + O( 2 ) (S.2.75) = J(W) -tr u W -1 HW -1 v + O( 2 ) (S.2.76) = J(W) -tr W -1 vu W -1 H + O( 2 ) (S.2.77) Comparison with (2.5) gives ∇J = -W -1 vu W -1 (S.2.78) and hence ∇J = -W -uv W -, (S.2.79) where W -is the transpose of the inverse of W. Gradient of the log-determinant The goal of this exercise is to determine the gradient of J(W) = log | det(W)|. (2.10) (a) Show that the n-th eigenvalue λ n can be written as λ n = v n Wu n , (2.11) where u n is the nth eigenvector and v n the nth column vector of U -1 , with U being the matrix with the eigenvectors u n as columns. Solution. As in Exercise 1.3, let UΛV be the eigenvalue decomposition of W (with V = U -1 ). Then Λ = V WU and λ n = e [n] Λe (n) (S.2.80) = e [n] V WUe (n) (S.2.81) = (Ve (n) ) WUe (n) (S.2.82) = v n Wu n , (S.2.83) where e (n) is the standard basis (unit) vector with a 1 in the n-th slot and zeros elsewhere, and e [n] is the corresponding row vector. (b) Calculate the gradient of λ n with respect to W, i.e. ∇λ n (W). Solution. With Exercise 2.3, we have (ii) If W is a matrix with real entries, then Wu = λu implies Wū = λū, i.e. if λ is a complex eigenvalue, then λ (the complex conjugate of λ) is also an eigenvalue. ∇ W λ n (W) = ∇ W v n Wu n = v n u n . (S. Since |λ| 2 = λ λ, | det(W)| =   λ i ∈C λ i     λ j ∈R |λ j |   . (S.2.85) Now we can write J(W) in terms of the eigenvalues: J(W) = log | det(W)| (S.2.86) = log   λ i ∈C λ i     λ j ∈R |λ j |   (S.2.87) = log   λ i ∈C λ i   + log   λ j ∈R |λ j |   (S.2.88) = λ i ∈C log λ i + λ j ∈R log |λ j |. (S.2.89) Assume that the real-valued λ j are non-zero so that ∇ W log |λ j | = 1 |λ j | ∇ W |λ j | (S.2.90) = 1 |λ j | sign(λ j )∇ W λ j (S.2.91) Hence ∇J(W) = λ i ∈C ∇ W log λ i + λ j ∈R ∇ W log |λ j | (S.2.92) = λ i ∈C 1 λ i ∇ W λ i + λ i ∈R 1 |λ i | sign(λ i )∇ W λ i (S.2.93) = λ i ∈C v i u i λ i + λ i ∈R sign(λ i )v i u i |λ i | (S.2.94) = λ i ∈C v i u i λ i + λ i ∈R v i u i λ i (S.2.95) = i v i u i λ i . (S.2.96) (d) Show that ∇J(W) = (W -1 ) . (2.12) Solution. This follows from Exercise 1.3 where we have found that W -1 = i 1 λ i u i v i . (S.2.97) Indeed: ∇J(W) = i v i u i λ i = i 1 λ i (u i v i ) = W -1 . (S.2.98) Descent directions for matrix-valued functions Assume we would like to minimise a matrix valued function J(W) by gradient descent, i.e. the update equation is W ← W -∇J(W), (2.13) where is the step-length. The gradient ∇J(W) was defined in Exercise 2.3. It was there pointed out that the gradient defines a first order approximation to the perturbed objective function J(W + H). With (2.5), J(W -∇J(W)) = J(W) -tr(∇J(W) ∇J(W)) + O( 2 ) (2.14) For any (nonzero) matrix M, it holds that tr(M M) = i (M M) ii (2.15) = i j (M ) ij (M) ji (2.16) = i j M ji M ji (2.17) = ij (M ji ) 2 (2.18) > 0, (2.19) which means that tr(∇J(W) ∇J(W)) > 0 if the gradient is nonzero, and hence J(W -∇J(W)) < J(W) (2.20) for small enough . Consequently, ∇J(W) is a descent direction. Show that A A∇J(W)BB for non-zero matrices A and B is also a descent direction or leaves the leaves the objective invariant. Solution. As in the introduction to the question, we appeal to (2.5) to obtain J(W -∇J(W)A ABB ) = J(W) -tr(∇J(W) A A∇J(W)BB ) + O( 2 ) (S.2.99) = J(W) -tr(B ∇J(W) A A∇J(W)B) + O( 2 ), (S.2.100) where tr(B ∇J(W) A A∇J(W)B) takes the form tr(M M) with M = A∇J(W)B. With (2.19), we thus have tr(B ∇J(W) A A∇J(W)B) > 0 if A∇J(W)B is non-zero, and hence J(W -A A∇J(W)BB ) < J(W) (S.2.101) for small enough . We have equality if A∇J(W)B = 0, e.g. if the columns of B are all in the null space of ∇J. Chapter 3 Directed Graphical Models Exercises 3.1 Directed graph concepts . . . . . . . . . . . . . . . . . . . . . . . 3.2 Canonical connections . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Ordered and local Markov properties, d-separation . . . . . . . 3.4 More on ordered and local Markov properties, d-separation . 3.5 Chest clinic (based on Barber, 2012, Exercise 3.3) . . . . . . . 3.6 More on the chest clinic (based on Barber, 2012, Exercise 3.3) 3.7 Hidden Markov models . . . . . . . . . . . . . . . . . . . . . . . . 3.8 Alternative characterisation of independencies . . . . . . . . . . 3.9 More on independencies . . . . . . . . . . . . . . . . . . . . . . . 3.10 Independencies in directed graphical models . . . . . . . . . . . 3.11 Independencies in directed graphical models . . . . . . . . . . . Directed graph concepts Consider the following directed graph: a z q e h (a) List all trails in the graph (of maximal length) Solution. We have (a, q, e) (a, q, z, h) (h, z, q, e) and the corresponding ones with swapped start and end nodes. (b) List all directed paths in the graph (of maximal length) Solution. (a, q, e) (z, q, e) (z, h) Solution. nondesc(q) = {a, z, h, e} \\ {e} = {a, z, h} (e) Which of the following orderings are topological to the graph? • (a,z,h,q,e) • (a,z,e,h,q) • (z,a,q,h,e) • (z,q,e,a,h) Solution. • (a,z,h,q,e): yes • (a,z,e,h,q): no (q is a parent of e and thus has to come before e in the ordering) • (z,a,q,h,e): yes • (z,q,e,a,h): no (a is a parent of q and thus has to come before q in the ordering) Canonical connections We here derive the independencies that hold in the three canonical connections that exist in DAGs, shown in Figure 3 .1. x z y (a) For the serial connection, use the ordered Markov property to show that x ⊥ ⊥ y | z. Solution. The only topological ordering is x, z, y. The predecessors of y are pre y = {x, z} and its parents pa y = {z}. The ordered Markov property y ⊥ ⊥ (pre y \\ pa y ) | pa y (S.3.1) thus becomes y ⊥ ⊥ ({x, z} \\ z) | z. Hence we have y ⊥ ⊥ x | z, (S.3.2) which is the same as x ⊥ ⊥ y | z since the independency relationship is symmetric. This means that if the state or value of z is known (i.e. if the random variable z is \"instantiated\"), evidence about x will not change our belief about y, and vice versa. We say that the z node is \"closed\" and that the trail between x and y is \"blocked\" by the instantiated z. In other words, knowing the value of z blocks the flow of evidence between x and y. (b) For the serial connection, show that the marginal p(x, y) does generally not factorise into p(x)p(y), i.e. that x ⊥ ⊥ y does not hold. Solution. There are several ways to show the result. One is to present an example where the independency does not hold. Consider for instance the following model x ∼ N (x; 0, 1) (S.3.3) z = x + n z (S.3.4) y = z + n y (S.3.5) where n z ∼ N (n z ; 0, 1) and n y ∼ N (n y ; 0, 1), both being statistically independent from x. Here N (•; 0, 1) denotes the Gaussian pdf with mean 0 and variance 1, and x ∼ N (x; 0, 1) means that we sample x from the distribution N (x; 0, 1). Hence p(z|x) = N (z; x, 1), p(y|z) = N (y; z, 1) and p(x, y, z) = p(x)p(z|x)p(y|z) = N (x; 0, 1)N (z; x, 1)N (y; z, 1). Whilst we could manipulate the pdfs to show the result, it's here easier to work with the generative model in Equations (S.3.3) to (S.3.5). Eliminating z from the equations, by plugging the definition of z into (S.3.5) we have y = x + n z + n y , (S.3.6) which describes the marginal distribution of (x, y). We see that E[xy] is E[xy] = E[x 2 + xn z + xn y ] (S.3.7) = E[x 2 ] + E[x]E[n z ] + E[x]E[n y ] (S.3.8) = 1 + 0 + 0 (S.3.9) where we have use the linearity of expectation, that x is independent from n z and n y , and that x has zero mean. If x and y were independent (or only uncorrelated), we had E[xy] = E[x]E[y] = 0. However, since E[xy] = E[x]E[y], x and y are not independent. In plain English, this means that if the state of z is unknown, then evidence or information about x will influence our belief about y, and the other way around. Evidence can flow through z between x and y. We say that the z node is \"open\" and the trail between x and y is \"active\". As in the serial connection, if the state or value z is known, evidence about x will not change our belief about y, and vice versa. Knowing z closes the z node, which blocks the trail between x and y. (d) For the diverging connection, show that the marginal p(x, y) does generally not factorise into p(x)p(y), i.e. that x ⊥ ⊥ y does not hold. Solution. As for the serial connection, it suffices to give an example where x ⊥ ⊥ y does not hold. We consider the following generative model z ∼ N (z; 0, 1) (S.3.12) x = z + n x (S.3.13) y = z + n y (S.3.14) where n x ∼ N (n x ; 0, 1) and n y ∼ N (n y ; 0, 1), and they are independent of each other and the other variables. We have E[x] = E[z + n x ] = E[z] + E[n x ] = 0. On the other hand E[xy] = E[(z + n x )(z + n y )] (S.3.15) = E[z 2 + z(n x + n y ) + n x n y ] (S.3.16) = E[z 2 ] + E[z(n x + n y )] + E[n x n y ] (S.3.17) = 1 + 0 + 0 (S.3.18) Hence, E[xy] = E[x]E[y] and we do not have that x ⊥ ⊥ y holds. In a diverging connection, as in the serial connection, if the state of z is unknown, then evidence or information about x will influence our belief about y, and the other way around. Evidence can flow through z between x and y. We say that the z node is open and the trail between x and y is active. (e) For the converging connection, show that x ⊥ ⊥ y. Solution. We can here again use the ordered Markov property with the ordering y, x, z. Since pre x = {y} and pa x = ∅, we have x ⊥ ⊥ (pre x \\ pa x ) | pa x = x ⊥ ⊥ y. (S.3.19) Alternatively, we can use the basic definition of directed graphical models, i.e. p(x, y, z) = k(x)k(y)k(z | x, y) (S.3.20) together with the result that the kernels (factors) are valid (conditional) pdfs/pmfs and equal to the conditionals/marginals with respect to the joint distribution p(x, y, z), i.e. k(x) = p(x) (S.3.21) k(y) = p(y) (S.3.22) k(z|x, y) = p(z|x, y) (not needed in the proof below) (S.3.23) Integrating out z gives p(x, y) = p(x, y, z)dz (S.3.24) = k(x)k(y)k(z | x, y)dz (S.3.25) = k(x)k(y) k(z | x, y)dz 1 (S.3.26) = p(x)p(y) (S.3.27) Hence p(x, y) factorises into its marginals, which means that x ⊥ ⊥ y. Hence, when we do not have evidence about z, evidence about x will not change our belief about y, and vice versa. For the converging connection, if no evidence about z is available, the z node is closed, which blocks the trail between x and y. (f) For the converging connection, show that x ⊥ ⊥ y | z does generally not hold. Solution. We give a simple example where x ⊥ ⊥ y | z does not hold. Consider x ∼ N (x; 0, 1) (S.3.28) y ∼ N (y; 0, 1) (S.3.29) z = xy + n z (S.3.30) where n z ∼ N (n z ; 0, 1), independent from the other variables. From the last equation, we have xy = z -n z (S.3.31) We thus have E[xy | z] = E[z -n z | z] (S.3.32) = z -0 (S.3.33) On the other hand, E[xy] = E[x]E[y] = 0. Since E[xy | z] = E[xy], x ⊥ ⊥ y | z cannot hold. The intuition here is that if you know the value of the product xy, even if subject to noise, knowing the value of x allows you to guess the value of y and vice versa. More generally, for converging connections, if evidence or information about z is available, evidence about x will influence the belief about y, and vice versa. We say that information about z opens the z-node, and evidence can flow between x and y. Note: information about z means that z or one of its descendents is observed, see exercise 3.9. Ordered and local Markov properties, d-separation We continue with the investigation of the graph from Exercise 3.1 shown below for reference. a z q e h (a) The ordering (z, h, a, q, e) is topological to the graph. What are the independencies that follow from the ordered Markov property? Solution. A distribution that factorises over the graph satisfies the independencies x i ⊥ ⊥ (pre i \\ pa i ) | pa i for all i for all orderings of the variables that are topological to the graph. The ordering comes into play via the predecessors pre i = {x 1 , . . . , x i-1 } of the variables x i ; the graph via the parent sets pa i . For the graph and the specified topological ordering, the predecessor sets are pre z = ∅, pre h = {z}, pre a = {z, h}, pre q = {z, h, a}, pre e = {z, h, a, q} The parent sets only depend on the graph and not the topological ordering. They are: pa z = ∅, pa h = {z}, pa a = ∅, pa q = {a, z}, pa e = {q}, The ordered Markov property reads x i ⊥ ⊥ (pre i \\ pa i ) | pa i where the x i refer to the ordered variables, e.g. x 1 = z, x 2 = h, x 3 = a, etc. With pre h \\ pa h = ∅ pre a \\ pa a = {z, h} pre q \\ pa q = {h} pre e \\ pa e = {z, h, a} we thus obtain h ⊥ ⊥ ∅ | z a ⊥ ⊥ {z, h} q ⊥ ⊥ h | {a, z} e ⊥ ⊥ {z, h, a} | q The relation h ⊥ ⊥ ∅ | z should be understood as \"there is no variable from which h is independent given z\" and should thus be dropped from the list. Note that we can possibly obtain more independence relations for variables that occur later in the topological ordering. This is because the set pre \\ pa can only increase when the predecessor set pre becomes larger. (b) What are the independencies that follow from the local Markov property? Solution. The non-descendants are nondesc(a) = {z, h} nondesc(z) = {a} nondesc(h) = {a, z, q, e} nondesc(q) = {a, z, h} nondesc(e) = {a, q, z, h} With the parent sets as before, the independencies that follow from the local Markov property are x i ⊥ ⊥ (nondesc(x i ) \\ pa i ) | pa i , i.e. a ⊥ ⊥ {z, h} z ⊥ ⊥ a h ⊥ ⊥ {a, q, e} | z q ⊥ ⊥ h | {a, z} e ⊥ ⊥ {a, z, h} | q (c) The independency relations obtained via the ordered and local Markov property include q ⊥ ⊥ h | {a, z}. Verify the independency using d-separation. Solution. The only trail from q to h goes through z which is in a tail-tail configuration. Since z is part of the conditioning set, the trail is blocked and the result follows. (d) Use d-separation to check whether a ⊥ ⊥ h | e holds. Solution. The trail from a to h is shown below in red together with the default states of the nodes along the trail. a z q e h closed open Conditioning on e opens the q node since q in a collider configuration on the path. a z q e h open open The trail from a to h is thus active, which means that the relationship does not hold because a ⊥ ⊥ h | e for some distributions that factorise over the graph. (e) Assume all variables in the graph are binary. How many numbers do you need to specify, or learn from data, in order to fully specify the probability distribution? Solution. The graph defines a set of probability mass functions (pmf) that factorise as p(a, z, q, h, e) = p(a)p(z)p(q|a, z)p(h|z)p(e|q) To specify a member of the set, we need to specify the (conditional) pmfs on the right-hand side. The (conditional) pmfs can be seen as tables, and the number of elements that we need to specified in the tables are: -1 for p(a) -1 for p(z) -4 for p(q|a, z) -2 for p(h|z) -2 for p(e|q) In total, there are 10 numbers to specify. This is in contrast to 2 5 -1 = 31 for a distribution without independencies. Note that the number of parameters to specify could be further reduced by making parametric assumptions. More on ordered and local Markov properties, d-separation We continue with the investigation of the graph below a z q e h (a) Why can the ordered or local Markov property not be used to check whether a ⊥ ⊥ h | e may hold? Solution. The independencies that follow from the ordered or local Markov property require conditioning on parent sets. However, e is not a parent of any node so that the above independence assertion cannot be checked via the ordered or local Markov property. (b) The independency relations obtained via the ordered and local Markov property include a ⊥ ⊥ {z, h}. Verify the independency using d-separation. Solution. All paths from a to z or h pass through the node q that forms a headhead connection along that trail. Since neither q nor its descendant e is part of the conditioning set, the trail is blocked and the independence relation follows. (c) Determine the Markov blanket of z. Solution. The Markov blanket is given by the parents, children, and co-parents. Hence: MB(z) = {a, q, h}. (d) Verify that q ⊥ ⊥ h | {a, z} holds by manipulating the probability distribution induced by the graph. Solution. A basic definition of conditional statistical independence x 1 ⊥ ⊥ x 2 | x 3 is that the (conditional) joint p(x 1 , x 2 | x 3 ) equals the product of the (conditional) marginals p(x 1 | x 3 ) and p(x 2 | x 3 ). In other words, for discrete random variables, x 1 ⊥ ⊥ x 2 | x 3 ⇐⇒ p(x 1 , x 2 | x 3 ) = x 2 p(x 1 , x 2 | x 3 ) x 1 p(x 1 , x 2 | x 3 ) (S.3.34) We thus answer the question by showing that (use integrals in case of continuous random variables) p(q, h|a, z) = h p(q, h|a, z) q p(q, h|a, z) (S.3.35) First, note that the graph defines a set of probability density or mass functions that factorise as p(a, z, q, h, e) = p(a)p(z)p(q|a, z)p(h|z)p(e|q) We then use the sum-rule to compute the joint distribution of (a, z, q, h), i.e. the distribution of all the variables that occur in p(q, h|a, z) p(a, z, q, h) = e p(a, z, q, h, e) (S. 3.36) = e p(a)p(z)p(q|a, z)p(h|z)p(e|q) (S.3.37) = p(a)p(z)p(q|a, z)p(h|z) e p(e|q) 1 (S.3.38) = p(a)p(z)p(q|a, z)p(h|z), (S.3.39) where e p(e|q) = 1 because (conditional) pdfs/pmfs are normalised so that the integrate/sum to one. We further have p(a, z) = q,h p(a, z, q, h) (S.3.40) = q,h p(a)p(z)p(q|a, z)p(h|z) (S.3.41) = p(a)p(z) q p(q|a, z) h p(h|z) (S.3.42) = p(a)p(z) (S.3.43) so that p(q, h|a, z) = p(a, z, q, h) p(a, z) (S.3.44) = p(a)p(z)p(q|a, z)p(h|z) p(a)p(z) (S.3.45) = p(q|a, z)p(h|z). (S.3.46) We further see that p(q|a, z) and p(h|z) are the marginals of p(q, h|a, z), i.e. p(q|a, z) = h p(q, h|a, z) (S.3.47) p(h|z) = q p(q, h|a, z). (S.3.48) This means that p(q, h|a, z) = h p(q, h|a, z) q p(q, h|a, z) , (S.3.49) which shows that q ⊥ ⊥ h|a, z. We see that using the graph to determine the independency is easier than manipulating the pmf/pdf. 3.5 Chest clinic (based on Barber, 2012, Exercise 3.3) The directed graphical model in Figure 3 .2 is about the diagnosis of lung disease (t=tuberculosis or l=lung cancer). In this model, a visit to some place \"a\" is thought to increase the probability of tuberculosis. (a) Explain which of the following independence relationships hold for all distributions that factorise over the graph. 1. t ⊥ ⊥ s | d x d e t l b a s Positive X-ray Dyspnea (shortness of breath) Either tuberculosis or lung cancer Tuberculosis Lunc cancer Bronchitis Visited place a Smoker Solution. • There are two trails from t to s: (t, e, l, s) and (t, e, d, b, s). • The trail (t, e, l, s) features a collider node e that is opened by the conditioning variable d. The trail is thus active and we do not need to check the second trail because for independence all trails needed to be blocked. • The independence relationship does thus generally not hold.  (a) Explain which of the following independence relationships hold for all distributions that factorise over the graph. a ⊥ ⊥ s | l Solution. • There are two trails from a to s: (a, t, e, l, s) and (a, t, e, d, b, s) • The trail (a, t, e, l, s) features a collider node e that blocks the trail (the trail is also blocked by l). • The trail (a, t, e, d, b, s) is blocked by the collider node d. • All trails are blocked so that the independence relation holds. a ⊥ ⊥ s | l, d Solution. • There are two trails from a to s: (a, t, e, l, s) and (a, t, e, d, b, s) • The trail (a, t, e, l, s) features a collider node e that is opened by the conditioning variable d but the l node is closed by the conditioning variable l: the trail is blocked • The trail (a, t, e, d, b, s) features a collider node d that is opened by conditioning on d. On this trail, e is not in a head-head (collider) configuration) so that all nodes are open and the trail active. • Hence, the independence relation does generally not hold. (b) Let g be a (deterministic) function of x and t. Is the expected value E[g(x, t) | l, b] equal to E[g(x, t) | l]? Solution. The question boils down to checking whether x, t ⊥ ⊥ b | l. For the independence relation to hold, all trails from both x and t to b need to be blocked by l. • For x, we have the trails Hidden Markov models This exercise is about directed graphical models that are specified by the following DAG: y 1 y 2 y 3 y 4 x 1 x 2 x 3 x 4 These models are called \"hidden\" Markov models because we typically assume to only observe the y i and not the x i that follow a Markov model. (a) Show that all probabilistic models specified by the DAG factorise as p(x 1 , y 1 , x 2 , y 2 , . . . , x 4 , y 4 ) = p(x 1 )p(y 1 |x 1 )p(x 2 |x 1 )p(y 2 |x 2 )p(x 3 |x 2 )p(y 3 |x 3 )p(x 4 |x 3 )p(y 4 |x 4 ) Solution. From the definition of directed graphical models it follows that p(x 1 , y 1 , x 2 , y 2 , . . . , x 4 , y 4 ) = 4 i=1 p(x i |pa(x i )) 4 i=1 p(y i |pa(y i )). The result is then obtained by noting that the parent of y i is given by x i for all i, and that the parent of x i is x i-1 for i = 2, 3, 4 and that x 1 does not have a parent (pa(x 1 ) = ∅). (b) Derive the independencies implied by the ordered Markov property with the topological ordering (x 1 , y 1 , x 2 , y 2 , x 3 , y 3 , x 4 , y 4 ) Solution. y i ⊥ ⊥ x 1 , y 1 , . . . , x i-1 , y i-1 | x i x i ⊥ ⊥ x 1 , y 1 , . . . , x i-2 , y i-2 , y i-1 | x i-1 (c) Derive the independencies implied by the ordered Markov property with the topological ordering (x 1 , x 2 , . . . , x 4 , y 1 , . . . , y 4 ). Solution. For the x i , we use that for i ≥ 2: pre (x i ) = {x 1 , . . . , x i-1 } and pa(x i ) = x i-1 . For the y i , we use that pre(y 1 ) = {x 1 , . . . , x 4 }, that pre(y i ) = {x 1 , . . . , x 4 , y 1 , . . . , y i-1 } for i > 1, and that pa(y i ) = x i . The ordered Markov property then gives: x 3 ⊥ ⊥ x 1 | x 2 x 4 ⊥ ⊥ {x 1 , x 2 } | x 3 y 1 ⊥ ⊥ {x 2 , x 3 , x 4 } | x 1 y 2 ⊥ ⊥ {x 1 , x 3 , x 4 , y 1 } | x 2 y 3 ⊥ ⊥ {x 1 , x 2 , x 4 , y 1 , y 2 } | x 3 y 4 ⊥ ⊥ {x 1 , x 2 , x 3 , y 1 , y 2 , y 3 } | x 4 (d) Does y 4 ⊥ ⊥ y 1 | y 3 hold? Solution. The trail y 1 -x 1 -x 2 -x 3 -x 4 -y 4 is active: none of the nodes is in a collider configuration, so that their default state is open and conditioning on y 3 does not block any of the nodes on the trail. While x 1 -x 2 -x 3 -x 4 forms a Markov chain, where e.g. x 4 ⊥ ⊥ x 1 | x 3 holds, this not so for the distribution of the y's. Alternative characterisation of independencies We The characterisation in Equation (3.2) is particularly important for undirected graphical models. Solution. We first show the equivalence of p(x, y|z) = p(x|z)p(y|z) and p(x, y, z) = p(x|z)p(y|z)p(z): By the product rule, we have p(x, y, z) = p(x, y|z)p(z). If p(x, y|z) = p(x|z)p(y|z), it follows that p(x, y, z) = p(x|z)p(y|z)p(z). To show the opposite direction assume that p(x, y, z) = p(x|z)p(y|z)p(z) holds. By comparison with the decomposition in the product rule, it follows that we must have p(x, y|z) = p(x|z)p(y|z) whenever p(z) > 0 (it suffices to consider this case because for z where p(z) = 0, p(x, y|z) may not be uniquely defined in the first place). More on independencies This exercise is on further properties and characterisations of statistical independence. We have to show that x ⊥ ⊥ y|z and x ⊥ ⊥ w|z. For simplicity, we assume that the variables are discrete valued. If not, replace the sum below with an integral. To show that x ⊥ ⊥ y|z, we marginalise p(x, y, w|z) over w to obtain p(x, y|z) = which means that x ⊥ ⊥ y|z. To show that x ⊥ ⊥ w|z, we similarly marginalise p(x, y, w|z) over y to obtain p(x, w|z) = p(x|z)p(w|z), which means that x ⊥ ⊥ w|z. (b) For the directed graphical model below, show that the following two statements hold without using d-separation: x ⊥ ⊥ y and (3.3) x ⊥ ⊥ y | w (3.4) x z w y The exercise shows that not only conditioning on a collider node but also on one of its descendents activates the trail between x and y. You can use the result that x ⊥ ⊥ y|w ⇔ p(x, y, w) = a(x, w)b(y, w) for some non-negative functions a(x, w) and b(y, w). Solution. The graphical model corresponds to the factorisation p(x, y, z, w) = p(x)p(y)p(z|x, y)p(w|z). For the marginal p(x, y) we have to sum (integrate) over all (z, w) p(x, y) = Independencies in directed graphical models Consider the following directed acyclic graph. x 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8 x 9 For each of the statements below, determine whether it holds for all probabilistic models that factorise over the graph. Provide a justification for your answer. (a) p(x 7 |x 2 ) = p(x 7 ) Solution. Yes, it holds. x 2 is a non-descendant of x 7 , pa(x 7 ) = ∅, and hence, by the local Markov property, x 7 ⊥ ⊥ x 2 , so that p(x 7 |x 2 ) = p(x 7 ). (b) x 1 ⊥ ⊥ x 3 Solution. No, does not hold. x 1 and x 3 are d-connected, which only implies independence for some and not all distributions that factorise over the graph. The graph generally only allows us to read out independencies and not dependencies. (c) p(x 1 , x 2 , x 4 ) ∝ φ 1 (x 1 , x 2 )φ 2 (x 1 , x 4 ) for some non-negative functions φ 1 and φ 2 . Solution. Yes, it holds. The statement is equivalent to x 2 ⊥ ⊥ x 4 | x 1 . There are three trails from x 2 to x 4 , which are all blocked: 1. x 2 -x 1 -x 4 : this trail is blocked because x 1 is in a tail-tail connection and it is observed, which closes the node. 2. x 2 -x 3 -x 6 -x 5 -x 4 : this trail is blocked because x 3 , x 6 , x 5 is in a collider configuration, and x 6 is not observed (and it does not have any descendants). 3. x 2 -x 3 -x 6 -x 8 -x 7 -x 4 : this trail is blocked because x 3 , x 6 , x 8 is in a collider configuration, and x 6 is not observed (and it does not have any descendants). Hence, by the global Markov property (d-separation), the independency holds. (d) x 2 ⊥ ⊥ x 9 | {x 6 , x 8 } Solution. No, does not hold. Conditioning on x 6 opens the collider node x 4 on the trail x 2 -x 1 -x 4 -x 7 -x 9 , so that the trail is active. (e) x 8 ⊥ ⊥ {x 2 , x 9 } | {x 3 , x 5 , x 6 , x 7 } Solution. Yes, it holds. {x 3 , x 5 , x 6 , x 7 } is the Markov blanket of x 8 , so that x 8 is independent of remaining nodes given the Markov blanket. (f) E[x 2 • x 3 • x 4 • x 5 • x 8 | x 7 ] = 0 if E[x 8 | x 7 ] = 0 Solution. Yes, it holds. {x 2 , x 3 , x 4 , x 5 } are non-descendants of x 8 , and x 7 is the parent of x 8 , so that x 8 ⊥ ⊥ {x 2 , x 3 , x 4 , x 5 } | x 7 . This means that E[x 2 • x 3 • x 4 • x 5 • x 8 | x 7 ] = E[x 2 • x 3 • x 4 • x 5 | x 7 ]E[x 8 | x 7 ] = 0. Independencies in directed graphical models Consider the following directed acyclic graph: m 1 s 1 u 1 v 1 x 1 y 1 θ 1 m 2 s 2 u 2 v 2 x 2 y 2 θ 2 For each of the statements below, determine whether it holds for all probabilistic models that factorise over the graph. Provide a justification for your answer. (a) x 1 ⊥ ⊥ x 2 Solution. Does not hold. The trail x 1 -θ 1 -θ 2 -x 2 is active (unblocked) because none of the nodes is in a collider configuration or in the conditioning set. (b) p(x 1 , y 1 , θ 1 , u 1 ) ∝ φ A (x 1 , θ 1 , u 1 )φ B (y 1 , θ 1 , u 1 ) for some non-negative functions φ A and φ B Solution. Holds. The statement is equivalent to x 1 ⊥ ⊥ y 1 | {θ 1 , u 1 }. The conditioning set {θ 1 , u 1 } blocks all trails from x 1 to y 1 because they are both only in serial configurations in all trails from x 1 to y 1 , hence the independency holds by the global Markov property. Alternative justification: the conditioning set is the Markov blanket of x 1 , and x 1 and y 1 are not neighbours which implies the independency. (c) v 2 ⊥ ⊥ {u 1 , v 1 , u 2 , x 2 } | {m 2 , s 2 , y 2 , θ 2 } Solution. Holds. The conditioning set is the Markov blanket of v 2 (the set of parents, children, and co-parents): the set of parents is pa(v 2 ) = {m 2 , s 2 }, y 2 is the only child of v 2 , and θ 2 is the only other parent of y 2 . And v 2 is independent of all other variables given its Markov blanket. (d) E[m 2 | m 1 ] = E[m 2 ] Solution. Holds. There are four trails from m 1 to m 2 , namely via x 1 , via y 1 , via x 2 , via y 2 . In all trails the four variables are in a collider configuration, so that each of the trails is blocked. By the global Markov property (d-separation), this means that m 1 ⊥ ⊥ m 2 which implies that E[m 2 | m 1 ] = E[m 2 ]. Alternative justification 1: m 2 is a non-descendent of m 1 and pa(m 2 ) = ∅. By the directed local Markov property, a variable is independent from its non-descendents given the parents, hence m 2 ⊥ ⊥ m 1 . Alternative justification 2: We can choose a topological ordering where m 1 and m 2 are the first two variables. Moreover, their parent sets are both empty. By the directed ordered Markov, we thus have m 1 ⊥ ⊥ m 2 . Chapter 4 Undirected Graphical Models Visualising and analysing Gibbs distributions via undirected graphs We here consider the Gibbs distribution p(x 1 , . . . , x 5 ) ∝ φ 12 (x 1 , x 2 )φ 13 (x 1 , x 3 )φ 14 (x 1 , x 4 )φ 23 (x 2 , x 3 )φ 25 (x 2 , x 5 )φ 45 (x 4 , x 5 ) (a) Visualise it as an undirected graph. Solution. We draw a node for each random variable x i . There is an edge between two nodes if the corresponding variables co-occur in a factor. x 1 x 2 x 3 x 4 x 5 (b) What are the neighbours of x 3 in the graph? Solution. The neighbours are all the nodes for which there is a single connecting edge. Thus: ne(x 3 ) = {x 1 , x 2 }. (Note that sometimes, we may denote ne(x 3 ) by ne 3 .) (c) Do we have x 3 ⊥ ⊥ x 4 | x 1 , x 2 ? Solution. Yes. The conditioning set {x 1 , x 2 } equals ne 3 , which is also the Markov blanket of x 3 . This means that x 3 is conditionally independent of all the other variables given {x 1 , x 2 }, i.e. x 3 ⊥ ⊥ x 4 , x 5 | x 1 , x 2 , which implies that x 3 ⊥ ⊥ x 4 | x 1 , x 2 . (One can also use graph separation to answer the question.) (d) What is the Markov blanket of x 4 ? Solution. The Markov blanket of a node in a undirected graphical model equals the set of its neighbours: MB(x 4 ) = ne(x 4 ) = ne 4 = {x 1 , x 5 }. This implies, for example, that x 4 ⊥ ⊥ x 2 , x 3 | x 1 , x 5 . (e) On which minimal set of variables A do we need to condition to have x 1 ⊥ ⊥ x 5 | A? Solution. We first identify all trails from x 1 to x 5 . There are three such trails: (x 1 , x 2 , x 5 ), (x 1 , x 3 , x 2 , x 5 ), and (x 1 , x 4 , x 5 ). Conditioning on x 2 blocks the first two trails, conditioning on x 4 blocks the last. We thus have: x 1 ⊥ ⊥ x 5 | x 2 , x 4 , so that A = {x 2 , x 4 }. Factorisation and independencies for undirected graphical models Consider the undirected graphical model defined by the graph in Figure 4 .1. x 1 x 2 x 3 x 4 x 5 x 6 (a) What is the set of Gibbs distributions that is induced by the graph? Solution. The graph in Figure 4 .1 has four maximal cliques: (x 1 , x 2 , x 4 ) (x 1 , x 3 , x 4 ) (x 3 , x 4 , x 5 ) (x 4 , x 5 , x 6 ) The Gibbs distributions are thus p(x 1 , . . . , x 6 ) ∝ φ 1 (x 1 , x 2 , x 4 )φ 2 (x 1 , x 3 , x 4 )φ 3 (x 3 , x 4 , x 5 )φ 4 (x 4 , x 5 , x 6 ) (b) Let p be a pdf that factorises according to the graph. Does p(x 3 |x 2 , x 4 ) = p(x 3 |x 4 ) hold? Solution. p(x 3 |x 2 , x 4 ) = p(x 3 |x 4 ) means that x 3 ⊥ ⊥ x 2 | x 4 . We can use the graph to check whether this generally holds for pdfs that factorise according to the graph. There are multiple trails from x 3 to x 2 , including the trail (x 3 , x 1 , x 2 ), which is not blocked by x 4 . From the graph, we thus cannot conclude that x 3 ⊥ ⊥ x 2 | x 4 , and p(x 3 |x 2 , x 4 ) = p(x 3 |x 4 ) will generally not hold (the relation may hold for some carefully defined factors φ i ). (c) Explain why x 2 ⊥ ⊥ x 5 | x 1 , x 3 , x 4 , x 6 holds for all distributions that factorise over the graph. Solution. Distributions that factorise over the graph satisfy the pairwise Markov property. Since x 2 and x 5 are not neighbours, and x 1 , x 3 , x 4 , x 6 are the remaining nodes in the graph, the independence relation follows from the pairwise Markov property. (d) Assume you would like to approximate E(x 1 x 2 x 5 | x 3 , x 4 ), i.e. the expected value of the product of x 1 , x 2 , and x 5 given x 3 and x 4 , with a sample average. Do you need to have joint observations for all five variables x 1 , . . . , x 5 ? Solution. In the graph, all trails from {x 1 , x 2 } to x 5 are blocked by {x 3 , x 4 }, so that x 1 , x 2 ⊥ ⊥ x 5 | x 3 , x 4 . We thus have E(x 1 x 2 x 5 | x 3 , x 4 ) = E(x 1 x 2 | x 3 , x 4 )E(x 5 | x 3 , x 4 ). Hence, we only need joint observations of (x 1 , x 2 , x 3 , x 4 ) and (x 3 , x 4 , x 5 ). Variables (x 1 , x 2 ) and x 5 do not need to be jointly measured. Factorisation and independencies for undirected graphical models Consider the undirected graphical model defined by the following graph, sometimes called a diamond configuration. Solution. We can generate the independencies by conditioning on progressively larger sets. Since there is a trail between any two nodes, there are no unconditional independencies. If we condition on a single variable, there is still a trail that connects the remaining ones. Let us thus consider the case where we condition on two nodes. By graph separation, we have w ⊥ ⊥ y | x, z x ⊥ ⊥ z | w, y (S.4.2) These are all the independencies that hold for the model, since conditioning on three nodes does not lead to any independencies in a model with four variables. Factorisation from the Markov blankets I Assume you know the following Markov blankets for all variables x 1 , . . . , x 4 , y 1 , . . . y 4 of a pdf or pmf p(x 1 , . . . , x 4 , y 1 , . . . , y 4 ). MB(x 1 ) = {x 2 , y 1 } MB(x 2 ) = {x 1 , x 3 , y 2 } MB(x 3 ) = {x 2 , x 4 , y 3 } MB(x 4 ) = {x 3 , y 4 } (4.1) MB(y 1 ) = {x 1 } MB(y 2 ) = {x 2 } MB(y 3 ) = {x 3 } MB(y 4 ) = {x 4 } (4.2) Assuming that p is positive for all possible values of its variables, how does p factorise? Solution. In undirected graphical models, the Markov blanket for a variable is the same as the set of its neighbours. Hence, when we are given all Markov blankets we know what local Markov property p must satisfy. For positive distributions we have an equivalence between p satisfying the local Markov property and p factorising over the graph. Hence, to specify the factorisation of p it suffices to construct the undirected graph H based on the Markov blankets and then read out the factorisation. We need to build a graph where the neighbours of each variable equals the indicated Markov blanket. This can be easily done by starting with an empty graph and connecting each variable to the variables in its Markov blanket. We see that each y i is only connected to x i . Including those Markov blankets we get the following graph: y 1 y 2 y 3 y 4 x 1 x 2 x 3 x 4 Connecting the x i to their neighbours according to the Markov blanket thus gives: y 1 y 2 y 3 y 4 x 1 x 2 x 3 x 4 The graph has maximal cliques of size two, namely the x i -y i for i = 1, . . . , 4, and the x i -x i+1 for i = 1, . . . , 3. Given the equivalence between the local Markov property and factorisation for positive distributions, we know that p must factorise as p(x 1 , . . . , x 4 , y 1 , . . . , y 4 ) = 1 Z 3 i=1 m i (x i , x i+1 ) 4 i=1 g i (x i , y i ), (S.4.3) where m i (x i , x i+1 ) > 0, g(x i , y i ) > 0 are positive factors (potential functions). The graphical model corresponds to an undirected version of a hidden Markov model where the x i are the unobserved (latent, hidden) variables and the y i are the observed ones. Note that the x i form a Markov chain. Factorisation from the Markov blankets II We consider the same setup as in Exercise 4.4 but we now assume that we do not know all Markov blankets but only MB(x 1 ) = {x 2 , y 1 } MB(x 2 ) = {x 1 , x 3 , y 2 } MB(x 3 ) = {x 2 , x 4 , y 3 } MB(x 4 ) = {x 3 , y 4 } (4.3) Without inserting more independencies than those specified by the Markov blankets, draw the graph over which p factorises and state the factorisation. (Again assume that p is positive for all possible values of its variables). Solution. We take the same approach as in Exercise 4.4. In particular, the Markov blankets of a variable are its neighbours in the graph. But since we are not given all Markov blankets and are not allowed to insert additional independencies, we must assume that each y i is connected to all the other y's. For example, if we didn't connect y 1 and y 4 we would assert the additional independency y 1 ⊥ ⊥ y 4 | x 1 , x 2 , x 3 , x 4 , y 2 , y 3 . We thus have a graph as follows: y 1 y 2 y 3 y 4 x 1 x 2 x 3 x 4 The factorisation thus is p(x 1 , . . . , x 4 , y 1 , . . . , y 4 ) = 1 Z g(y 1 , . . . , y 4 ) 3 i=1 m i (x i , x i+1 ) 4 i=1 g i (x i , y i ), (S.4.4) where the m i (x i , x i+1 ), g i (x i , y i ) and g(y 1 , . . . , y 4 ) are positive factors. Compared to the factorisation in Exercise 4.4, we still have the Markov structure for the x i , but only a single factor for (y 1 , y 2 , y 3 , y 4 ) to avoid inserting independencies beyond those specified by the given Markov blankets. Undirected graphical model with pairwise potentials We here consider Gibbs distributions where the factors only depend on two variables at a time. The probability density or mass functions over d random variables x 1 , . . . , x d then take the form p(x 1 , . . . , x d ) ∝ i≤j φ ij (x i , x j ) Such models are sometimes called pairwise Markov networks. (a) Let p(x 1 , . . . , x d ) ∝ exp -1 2 x Axb x where A is symmetric and x = (x 1 , . . . , x d ) . What are the corresponding factors φ ij for i ≤ j? Solution. Denote the (i, j)-th element of A by a ij . We have x Ax = ij a ij x i x j (S.4.5) = i<j 2a ij x i x j + i a ii x 2 i (S.4.6) where the second line follows from A = A. Hence, - 1 2 x Ax -b x = - 1 2 i<j 2a ij x i x j - 1 2 i a ii x 2 i - i b i x i (S.4.7) so that φ ij (x i , x j ) = exp (-a ij x i x j ) if i < j exp -1 2 a ii x 2 i -b i x i if i = j (S.4.8) For x ∈ R d , the distribution is a Gaussian with A equal to the inverse covariance matrix. For binary x, the model is known as Ising model or Boltzmann machine. For x i ∈ {-1, 1}, x 2 i = 1 for all i, so that the a ii are constants that can be absorbed into the normalisation constant. This means that for x i ∈ {-1, 1}, we can work with matrices A that have zeros on the diagonal. (b) For p(x 1 , . . . , x d ) ∝ exp -1 2 x Ax -b x , show that x i ⊥ ⊥ x j | {x 1 , . . . , x d } \\ {x i , x j } if the (i, j)-th element of A is zero. Solution. The previous question showed that we can write p(x 1 , . . . , x d ) ∝ i≤j φ ij (x i , x j ) with potentials as in Equation (S.4.8). Consider two variables x i and x j for fixed (i, j). They only appear in the factorisation via the potential φ ij . If a ij = 0, the factor φ ij becomes a constant, and no other factor contains x i and x j , which means that there is no edge between x i and x j if a ij = 0. By the pairwise Markov property it then follows that The restricted Boltzmann machine is an undirected graphical model for binary variables v = (v 1 , . . . , v n ) and h = (h 1 , . . . , h m ) with a probability mass function equal to x i ⊥ ⊥ x j | {x 1 , . . . , x d } \\ {x i , x j }. p(v, h) ∝ exp v Wh + a v + b h , (4.4) where W is a n × m matrix. Both the v i and h i take values in {0, 1}. The v i are called the \"visibles\" variables since they are assumed to be observed while the h i are the hidden variables since it is assumed that we cannot measure them. (a) Use graph separation to show that the joint conditional p(h|v) factorises as p(h|v) = m i=1 p(h i |v). Solution. Figure 4 .2 on the left shows the undirected graph for p(v, h) with n = 3, m = 2. We note that the graph is bi-partite: there are only direct connections between the h i and the v i . Conditioning on v thus blocks all trails between the h i (graph on the right). This means that the h i are independent from each other given v so that (b) Show that p(h|v) = m i=1 p(h i |v). h 1 h 2 v 1 v 2 v 3 h 1 h 2 v 1 v 2 v 3 p(h i = 1|v) = 1 1 + exp -b i -j W ji v j (4.5) where W ji is the (ji)-th element of W, so that j W ji v j is the inner product (scalar product) between the i-th column of W and v. Solution. For the conditional pmf p(h i |v) any quantity that does not depend on h i can be considered to be part of the normalisation constant. A general strategy is to first work out p(h i |v) up to the normalisation constant and then to normalise it afterwards. We begin with p(h|v): p(h|v) = p(h, v) p(v) (S.4.9) ∝ p(h, v) (S.4.10) ∝ exp v Wh + a v + b h (S.4.11) ∝ exp v Wh + b h (S.4.12) ∝ exp   i j v j W ji h i + i b i h i   (S.4.13) As we are interested in p(h i |v) for a fixed i, we can drop all the terms not depending on that h i , so that p(h i |v) ∝ exp   j v j W ji h i + b i h i   (S.4.14) Since h i only takes two values, 0 and 1, normalisation is here straightforward. Call the unnormalised pmf p(h i |v), p(h i |v) = exp   j v j W ji h i + b i h i   . (S.4.15) We then have p(h i |v) = p(h i |v) p(h i = 0|v) + p(h i = 1|v) (S.4.16) = p(h i |v) 1 + exp j v j W ji + b i (S.4.17) = exp j v j W ji h i + b i h i 1 + exp j v j W ji + b i , (S.4.18) so that p(h i = 1|v) = exp j v j W ji + b i 1 + exp j v j W ji + b i (S.4.19) = 1 1 + exp -j v j W ji -b i . (S.4.20) The probability p(h = 0|v) equals 1 -p(h i = 1|v), which is p(h i = 0|v) = 1 + exp j v j W ji + b i 1 + exp j v j W ji + b i - exp j v j W ji + b i 1 + exp j v j W ji + b i (S.4.21) = 1 1 + exp j W ji v j + b i (S. 4.22) The function x → 1/(1 + exp(-x)) is called the logistic function. It is a sigmoid function and is thus sometimes denoted by σ(x). For other versions of the sigmoid function, see https://en.wikipedia.org/wiki/Sigmoid_function . -6 -4 -2 2 4 6 0.2 0.4 0.6 0.8 x σ(x) With that notation, we have p(h i = 1|v) = σ   j W ji v j + b i   . (c) Use a symmetry argument to show that p(v|h) = i p(v i |h) and p(v i = 1|h) = 1 1 + exp -a i -j W ij h j Solution. Since v Wh is a scalar we have (v Wh) = h W v = v Wh, so that p(v, h) ∝ exp v Wh + a v + b h (S.4.23) ∝ exp h W v + b h + a v . (S.4.24) To derive the result, we note that v and a now take the place of h and b from before, and that we now have W rather than W. In Equation (4.5), we thus replace h i with v i , b i with a i , and W ji with W ij to obtain p(v i = 1|h). In terms of the sigmoid function, we have p(v i = 1|h) = σ   j W ij h j + a i   . Note that while p(v|h) factorises, the marginal p(v) does generally not. The marginal p(v) can here be obtained in closed form up to its normalisation constant. p(v) = h∈{0,1} m p(v, h) (S.4.25) = 1 Z h∈{0,1} m exp v Wh + a v + b h (S.4.26) = 1 Z h∈{0,1} m exp   ij v i h j W ij + i a i v i + j b j h j   (S.4.27) = 1 Z h∈{0,1} m exp   m j=1 h j i v i W ij + b j + i a i v i   (S.4.28) = 1 Z h∈{0,1} m m j=1 exp h j i v i W ij + b j exp i a i v i (S.4.29) = 1 Z exp i a i v i h∈{0,1} m m j=1 exp h j i v i W ij + b j (S.4.30) = 1 Z exp i a i v i h 1 ,...,hm m j=1 exp h j i v i W ij + b j (S.4.31) Importantly, each term in the product only depends on a single h j , so that by sequentially applying the distributive law, we have h 1 ,...,hm m j=1 exp h j i v i W ij + b j =   h 1 ,...,h m-1 m-1 j=1 exp h j i v i W ij + b j   • hm exp h m i v i W im + b m (S.4.32) = . . . = m j=1   h j exp h j i v i W ij + b j   (S.4.33) Since h j ∈ {0, 1}, we obtain h j exp h j i v i W ij + b j = 1 + exp i v i W ij + b j (S.4.34) and thus p(v) = 1 Z exp i a i v i m j=1 1 + exp i v i W ij + b j . (S.4.35) Note that in the derivation of p(v) we have not used the assumption that the visibles v i are binary. The same expression would thus obtained if the visibles were defined in another space, e.g. the real numbers. While p(v) is written as a product, p(v) does not factorise into terms that depend on subsets of the v i . On the contrary, all v i are present in all factors. Since p(v) does not factorise, computing the normalising Z is expensive. For binary visibles v i ∈ {0, 1}, Z equals Z = v∈{0,1} n exp i a i v i m j=1 1 + exp i v i W ij + b j (S.4.36) where we have to sum over all 2 n configurations of the visibles v. This is computationally expensive, or even prohibitive if n is large (2 20 = 1048576, 2 30 > 10 9 ). Note that different values of a i , b i , W ij yield different values of Z. (This is a reason why Z is called the partition function when the a i , b i , W ij are free parameters.) It is instructive to write p(v) in the log-domain, log p(v) = log Z + n i=1 a i v i + m j=1 log 1 + exp i v i W ij + b j , (S.4.37) and to introduce the nonlinearity f (u), f (u) = log [1 + exp(u)] , (S.4.38) which is called the softplus function and plotted below. The softplus function is a smooth approximation of max(0, u), see e.g. https://en.wikipedia.org/wiki/  Rectifier_(neural_networks) -6 -4 -2 2 4 6 2 4 6 u f (u) With the softplus function f (u), we can write log p(v) as log p(v) = log Z + n i=1 a i v i + m j=1 f i v i W ij + b j . (S.4.39) The parameter b j plays the role of a threshold as shown in the figure below. The terms f ( i v i W ij + b j ) can be interpreted in terms of feature detection. The sum i v i W ij is the inner product between v and the j-th column of W, and the inner product is largest if v equals the j-th column. We can thus consider the columns of W to be feature-templates, and the f ( i v i W ij + b j ) a way to measure how much of each feature is present in v. Further, i v i W ij + b j is also the input to the sigmoid function when computing p(h j = 1|v). Thus, the conditional probability for h j to be one, i.e. \"active\", can be considered to be an indicator of the presence of the j-th feature (j-th column of W) in the input v. If v is such that i v i W ij + b j is large for many j, i.e. if many features are detected, then f ( i v i W ij + b j ) will be non-zero for many j, and log p(v) will be large. -6 -4 -2 2 4 6 2 4 6 8 f (u) f (u + 2) f (u -2) u f (u) Hidden Markov models and change of measure Consider the following undirected graph for a hidden Markov model where the y i correspond to observed (visible) variables and the x i to unobserved (hidden/latent) variables. x 1 x 2 x 3 . . . . . . x t y 1 y 2 y 3 y t The graph implies the following factorisation p(x 1 , . . . , x t , y 1 , . . . , y t ) ∝ φ y 1 (x 1 , y 1 ) t i=2 φ x i (x i-1 , x i )φ y i (x i , y i ), (4.6) where the φ x i and φ y i are non-negative factors. Let us consider the situation where t i=2 φ x i (x i-1 , x i ) equals f (x) = t i=2 φ x i (x i-1 , x i ) = f 1 (x 1 ) t i=2 f i (x i |x i- 1 ), (4.7) with x = (x 1 , . . . , x t ) and where the f i are (conditional) pdfs. We thus have p(x 1 , . . . , x t , y 1 , . . . , y t ) ∝ f 1 (x 1 ) t i=2 f i (x i |x i-1 ) t i=1 φ y i (x i , y i ). (4.8) (a) Provide a factorised expression for p(x 1 , . . . , x t |y 1 , . . . , y t ) Solution. For fixed (observed) values of the y i , p(x 1 , . . . , x t |y 1 , . . . , y t ) factorises as p(x 1 , . . . , x t |y 1 , . . . , y t ) ∝ f 1 (x 1 )g 1 (x 1 ) t i=1 f i (x i |x i-1 )g i (x i ). (S.4.40) where g i (x i ) is φ y i (x i , y i ) for a fixed value of y i . (b) Draw the undirected graph for p(x 1 , . . . , x t |y 1 , . . . , y t ) Solution. Conditioning corresponds to removing nodes from an undirected graph. We thus have the following Markov chain for p(x 1 , . . . , x t |y 1 , . . . , y t ). x 1 x 2 x 3 . . . x t (c) Show that if φ y i (x i , y i ) equals the conditional pdf of y i given x i , i.e. p(y i |x i ), the marginal p(x 1 , . . . , x t ), obtained by integrating out y 1 , . . . , y t from (4.8), equals f (x). Solution. In this setting all factors in (4.8) are conditional pdfs and we are dealing with a directed graphical model that factorises as p(x 1 , . . . , x t , y 1 , . . . , y t ) = f 1 (x 1 ) t i=2 f i (x i |x i-1 ) t i=1 p(y i |x i ). (S.4.41) By integrating over the y i , we have p(x 1 , . . . , x t ) = p(x 1 , . . . , x t , y 1 , . . . , y t )dy 1 . . . dy t (S.4.42) = f 1 (x 1 ) t i=2 f i (x i |x i-1 ) t i=1 p(y i |x i )dy 1 . . . dy t (S.4.43) = f 1 (x 1 ) t i=2 f i (x i |x i-1 ) t i=1 p(y i |x i )dy i 1 (S.4.44) = f 1 (x 1 ) t i=2 f i (x i |x i-1 ) (S.4.45) = f (x) (S. 4.46) (d) Compute the normalising constant for p(x 1 , . . . , x t |y 1 , . . . , y t ) and express it as an expectation over f (x). Solution. With p(x 1 , . . . , x t , y 1 , . . . , y t ) ∝ f 1 (x 1 ) t 2=1 f i (x i |x i-1 ) t i=1 φ y i (x i , y i ). (S.4.47) The normalising constant is given by Z = f 1 (x 1 ) t 2=1 f i (x i |x i-1 ) t i=1 g i (x i )dx 1 . . . dx t (S.4.48) = E f t i=1 g i (x i ) (S.4.49) Since we can use ancestral sampling to sample from f , the above expectation can be easily computed via sampling. (e) Express the expectation of a test function h(x) with respect to p(x 1 , . . . , x t |y 1 , . . . , y t ) as a reweighted expectation with respect to f (x). Solution. By definition, the expectation over a test function h(x) is E p(x 1 ,...,xt|y 1 ,...,yt) [h(x)] = 1 Z h(x)f 1 (x 1 ) t 2=1 f (x i |x i-1 ) t i=1 g i (x i )dx 1 . . . dx t (S.4.50) = E f [h(x) i g i (x i )] E f [ i g i (x i )] (S.4.51) Both the numerator and denominator can be approximated using samples from f . Since the g i (x i ) = φ y i (x i , y i ) involve the observed variables y i , this has a nice interpretation: We can think we have two models for x: f (x) that does not involve the observations and p(x 1 , . . . , x t |y 1 , . . . , y t ) that does. Note, however, that unless φ y i (x i , y i ) is the conditional pdf p(y i |x i ), f (x) is not the marginal p(x 1 , . . . , x t ) that you would obtain by integrating out the y's from the joint model . We can thus generally think it is a base distribution that got \"enhanced\" by a change of measure in our expression for p(x 1 , . . . , x t |y 1 , . . . , y t ). If φ y i (x i , y i ) is the conditional pdf p(y i |x i ), the change of measure corresponds to going from the prior to the posterior by multiplication with the likelihood (the terms g i ). From the expression for the expectation, we can see that the \"enhancing\" leads to a corresponding introduction of weights in the expectation that depend via g i on the observations. This can be particularly well seen when we approximate the expectation as a sample average over n samples x (k) ∼ f (x): E p(x 1 ,...,xt|y 1 ,...,yt) [h(x)] ≈ n k=1 W (k) h(x (k) ) (S.4.52) W (k) = w (k) n k=1 w (k) (S.4.53) w (k) = i g i (x (k) i ) (S.4.54) where x (k) i is the i-th dimension of the vector x (k) . Chapter 5 Expressive Power of Graphical Models  Solution. The skeleton of graph 3 is different from the skeleton of graphs 1 and 2, so that graph 3 cannot be I-equivalent to graph 1 or 2, and we do not need to further check the immoralities for graph 3. Graph 1 and 2 have the same skeleton, and they also have the same immorality. Hence, graph 1 and 2 are I-equivalent. Note that node w in graph 1 is in a collider configuration along trail v -w -x but it is not an immorality because its parents are connected (covering edge); equivalently for node v in graph 2. x 1 x 2 I-equivalence x 3 x 4 x 5 x 6 x 7 Graph 0 For each of the three graphs below, explain whether the graph is a perfect map, an I-map, or not an I-map for U. x 1 x 2 x 3 x 4 x 5 x 6 x 7 Graph 1 x 1 x 2 x 3 x 4 x 5 x 6 x 7 Graph 2 x 1 x 2 x 3 x 4 x 5 x 6 x 7 Graph 3 Solution. • Graph 1 has an immorality x 2 → x 5 ← x 7 which graph 0 does not have. The graph is thus not I-equivalent to graph 0 and can thus not be a perfect map. Moreover, graph 1 asserts that x 2 ⊥ ⊥ x 7 |x 4 which is not case for graph 0. Since graph 0 is a perfect map for U, graph 1 asserts an independency that does not hold for U and can thus not be an I-map for U. • Graph 2 has an immorality x 1 → x 3 ← x 7 which graph 0 does not have. Graph 2 thus asserts that x 1 ⊥ ⊥ x 7 , which is not the case for graph 0. Hence, for the same reason as for graph 1, graph 2 is not an I-map for U. • Graph 3 has the same skeleton and set of immoralities as graph 0. It is thus I-equivalent to graph 0, and hence also a perfect map. Minimal I-maps (a) Assume that the graph G in Figure 5 .1 is a perfect I-map for p(a, z, q, e, h). Determine the minimal directed I-map using the ordering (e, h, q, z, a). Is the obtained graph I-equivalent to G? Solution. Since the graph G is a perfect I-map for p, we can use G to check whether p satisfies a certain independency. This gives the following recipe to construct the minimal directed I-map: 1. Assume an ordering of the variables. Denote the ordered random variables by x 1 , . . . , x d . 2. For each i, find a minimal subset of variables π i ⊆ pre i such that x i ⊥ ⊥ {pre i \\ π i } | π i is in I(G) (only works if G is a perfect I-map for I(p)) 3. Construct a graph with parents pa i = π i . Note: For I-maps G that are not perfect, if the graph does not indicate that a certain independency holds, we have to check that the independency indeed does not hold for p. If we don't, we won't obtain a minimal I-map but just an I-map for I(p). This is because p may have independencies that are not encoded in the graph G. Given the ordering (e, h, q, z, a), we build a graph where e is the root. From Figure 5 .1 (and the perfect map assumption), we see that h ⊥ ⊥ e does not hold. We thus set e as parent of h, see first graph in Figure 5 .2. Then: • We consider q: pre q = {e, h}. There is no subset π q of pre q on which we could condition to make q independent of pre q \\ π q , so that we set the parents of q in the graph to pa q = {e, h}. (Second graph in Figure 5 .2.) • We consider z: pre z = {e, h, q}. From the graph in Figure 5 .1, we see that for π z = {q, h} we have z ⊥ ⊥ pre z \\ π z |π z . Note that π z = {q} does not work because z ⊥ ⊥ e, h|q does not hold. We thus set pa z = {q, h}. (Third graph in Figure 5 .2.) • We consider a: pre a = {e, h, q, z}. This is the last node in the ordering. To find the minimal set π a for which a ⊥ ⊥ pre a \\ π a |π a , we can determine its Markov blanket MB(a). The Markov blanket is the set of parents (none), children (q), and co-parents of a (z) in Since the skeleton in the obtained minimal I-map is different from the skeleton of G, we do not have I-equivalence. Note that the ordering (e, h, q, z, a) yields a denser graph (Figure 5 .2) than the graph in Figure 5 .1. Whilst a minimal I-map, the graph does e.g. not show that a ⊥ ⊥ z. Furthermore, the causal interpretation of the two graphs is different. (b) For the collection of random variables (a, z, h, q, e) you are given the following Markov blankets for each variable: For positive distributions, the set of distributions that satisfy the local Markov property relative to a graph (as given by the Markov blankets) is the same as the set of Gibbs distributions that factorise according to the graph. Given the I-map, we can now easily find the Gibbs distribution p(a, z, h, q, e) = 1 Z φ 1 (a, z, q)φ 2 (q, e)φ 3 (z, h), • MB(a) = {q,z} • MB(z) = {a,q,h} • MB(h) = {z} • MB(q) = {a, where the φ i must take positive values on their domain. Note that we used the maximal clique (a, z, q). First, note that both graphs share the same skeleton and the only reason that they are not fully connected is the missing edge between x and z. I-equivalence between directed and undirected graphs For the DAG, there is also only one ordering that is topological to the graph: x, u, y, z. The missing edge between x and y corresponds to the only independency encoded by the graph: z ⊥ ⊥ pre z \\ pa z |pa z , i.e. z ⊥ ⊥ x|u, y. This is the same independency that we get from the directed local Markov property. For the undirected graph, z ⊥ ⊥ x|u, y holds because u, y block all paths between z and x. All variables but z and x are connected to each other, so that no further independency can hold. Hence both graphs only encode z ⊥ ⊥ x|u, y and they are thus I-equivalent. (b) Are the following two graphs, which are directed and undirected hidden Markov models, I-equivalent? y 1 y 2 y 3 y 4 x 1 x 2 x 3 x 4 y 1 y 2 y 3 y 4 x 1 x 2 x 3 x 4 Solution. The skeleton of the two graphs is the same and there are no immoralities. Hence, the two graphs are I-equivalent. (c) Are the following two graphs I-equivalent? y 1 y 2 y 3 y 4 x 1 x 2 x 3 x 4 y 1 y 2 y 3 y 4 x 1 x 2 x 3 x 4 Solution. The two graphs are not I-equivalent because x 1 -x 2 -x 3 forms an immorality. Hence, the undirected graph encodes x 1 ⊥ ⊥ x 3 |x 2 which is not represented in the directed graph. On the other hand, the directed graph asserts x 1 ⊥ ⊥ x 3 which is not represented in the undirected graph. Moralisation: Converting DAGs to undirected minimal I-maps The following recipe constructs undirected minimal I-maps for I(p): • Determine the Markov blanket for each variable x i • Construct a graph where the neighbours of x i are given by its Markov blanket. We can adapt the recipe to construct an undirected minimal I-map for the independencies I(G) encoded by a DAG G. What we need to do is to use G to read out the Markov blankets for the variables x i rather than determining the Markov blankets from the distribution p. Show that this procedure leads to the following recipe to convert DAGs to undirected minimal I-maps: 1. For all immoralities in the graph: add edges between all parents of the collider node. 2. Make all edges in the graph undirected. The first step is sometimes called \"moralisation\" because we \"marry\" all the parents in the graph that are not already directly connected by an edge. The resulting undirected graph is called the moral graph of G, sometimes denoted by M(G). Solution. The Markov blanket of a variable x is the set of its parents, children, and co-parents, as shown in the graph below in sub-figure (a). The parents and children are connected to x in the directed graph, but the co-parents are not directly connected to x. Hence, according to \"Construct a graph where the neighbours of x i are its Markov blanket.\", we need to introduce edges between x and all its co-parents. This gives the intermediate graph in sub-figure (b). Now, considering the top-left parent of x, we see that for that node, the Markov blanket includes the other parents of x. This means that we need to connect all parents of x, which gives the graph in sub-figure (c) . This is sometimes called \"marrying\" the parents of x. Continuing in this way, we see that we need to \"marry\" all parents in the graph that are not already married. Finally, we need to make all edges in the graph undirected, which gives sub-figure (d). A simpler approach is to note that the DAG specifies the factorisation p(x) = i p(x i |pa i ). We can consider each conditional p(x i |pa i ) to be a factor φ i (x i , pa i ) so that we obtain the Gibbs distribution p(x) = i φ i (x i |pa i ). Visualising the distribution by connecting all variables in the same factor φ i (x i |pa i ) leads to the \"marriage\" of all parents of x i . This corresponds to the first step in the recipe because x i is in a collider configuration with respect to the parent nodes. Not all parents form an immorality but this does here not matter because those that do not form an immorality are already connected by a covering edge in the first place. Moralisation exercise For the DAG G below find the minimal undirected I-map for I(G). x 3 x 4 x 5 x 6 x 7 Solution. To derive an undirected minimal I-map from a directed one, we have to construct the moralised graph where the \"unmarried\" parents are connected by a covering edge. This is because each conditional p(x i |pa i ) corresponds to a factor φ i (x i , pa i ) and we need to connect all variables that are arguments of the same factor with edges. Statistically, the reason for marrying the parents is as follows: An independency x ⊥ ⊥ y|{child, other nodes} does not hold in the directed graph in case of collider connections but would hold in the undirected graph if we didn't marry the parents. Hence links between the parents must be added. It is important to add edges between all parents of a node. Here, p(x 4 |x 1 , x 2 , x 3 ) corresponds to a factor φ(x 4 , x 1 , x 2 , x 3 ) so that all four variables need to be connected. Just adding edges x 1 -x 2 and x 2 -x 3 would not be enough. The moral graph, which is the requested minimal undirected I-map, is shown below. x 2 x 1 x 3 x 4 x 5 x 6 x 7 Moralisation exercise Consider the DAG G: y z 1 z 2 x 1 x 2 x 3 x 4 x 5 x 6 A friend claims that the undirected graph below is the moral graph M(G) of G. Is your friend correct? If not, state which edges needed to be removed or added, and explain, in terms of represented independencies, why the changes are necessary for the graph to become the moral graph of G. y z 1 z 2 x 1 x 2 x 3 x 4 x 5 x 6 Solution. The moral graph M(G) is an undirected minimal I-map of the independencies represented by G. Following the procedure of connecting \"unmarried\" parents of colliders, we obtain the following moral graph of G: y z 1 z 2 x 1 x 2 x 3 x 4 x 5 x 6 We can thus see that the friend's undirected graph is not the moral graph of G. The edge between x 1 and x 6 can be removed. This is because for G, we have e.g. the independencies x 1 ⊥ ⊥ x 6 |z 1 , x 1 ⊥ ⊥ x 6 |z 2 , x 1 ⊥ ⊥ x 6 |z 1 , z 2 which is not represented by the drawn undirected graph. We need to add edges between x 1 and x 3 , and between x 4 and x 6 . Otherwise, the undirected graph makes the wrong independency assertion that x 1 ⊥ ⊥ x 3 |x 2 , z 1 (and equivalent for x 4 and x 6 ). Triangulation: Converting undirected graphs to directed minimal I-maps In Exercise 5.4 we adapted a recipe for constructing undirected minimal I-maps for I(p) to the case of I(G), where G is a DAG. The key difference was that we used the graph G to determine independencies rather than the distribution p. We can similarly adapt the recipe for constructing a directed minimal I-map for I(p) to build a directed minimal I-map for I(H), where H is an undirected graph: 1. Choose an ordering of the random variables. 2. For all variables x i , use H to determine a minimal subset π i of the predecessors pre i such that x i ⊥ ⊥ (pre i \\ π i ) | π i holds. 3. Construct a DAG with the π i as parents pa i of x i . Remarks: (1) Directed minimal I-maps obtained with different orderings are generally not I-equivalent. (2) The directed minimal I-maps obtained with the above method are always chordal graphs. Chordal graphs are graphs where the longest trail without shortcuts is a triangle ( https://en.wikipedia.org/wiki/Chordal_graph ). They are thus also called triangulated graphs. We obtain chordal graphs because if we had trails without shortcuts that involved more than 3 nodes, we would necessarily have an immorality in the graph. But immoralities encode independencies that an undirected graph cannot represent, which would make the DAG not an I-map for I(H) any more. (a) Let H be the undirected graph below. Determine the directed minimal I-map for I(H) with the variable ordering x 1 , x 2 , x 3 , x 4 , x 5 . x 1 x 2 x 3 x 4 x 5 Solution. We use the ordering x 1 , x 2 , x 3 , x 4 , x 5 and follow the conversion procedure: • x 2 is not independent from x 1 so that we set pa 2 = {x 1 }. See first graph in Figure 5 .4. • Since x 3 is connected to both x 1 and x 2 , we don't have x 3 ⊥ ⊥ x 2 , x 1 . We cannot make x 3 independent from x 2 by conditioning on x 1 because there are two paths from x 3 to x 2 and x 1 only blocks the upper one. Moreover, x 1 is a neighbour of x 3 so that conditioning on x 2 does make them independent. Hence we must set pa 3 = {x 1 , x 2 }. See second graph in Figure 5 .4. • For x 4 , we see from the undirected graph, that x 4 ⊥ ⊥ x 1 | x 3 , x 2 . The graph further shows that removing either x 3 or x 2 from the conditioning set is not possible and conditioning on x 1 won't make x 4 independent from x 2 or x 3 . We thus have pa 4 = {x 2 , x 3 }. See fourth graph in Figure 5 .4. • The same reasoning shows that pa 5 = {x 3 , x 4 }. See last graph in Figure 5 .4. This results in the triangulated directed graph in Figure 5 .4 on the right. x 1 x 2 x 3 x 4 x 5 x 1 x 2 x 3 x 4 x 5 x 1 x 2 x 3 x 4 x 5 x 1 x 2 x 3 x 4 x 5 Figure 5 .4: . Answer to Exercise 5.7, Question (a). To see why triangulation is necessary consider the case where we didn't have the edge between x 2 and x 3 as in Figure 5 .5. The directed graph would then imply that x 3 ⊥ ⊥ x 2 | x 1 (check!). But this independency assertion does not hold in the undirected graph so that the graph in Figure 5 .5 is not an I-map. (b) For the undirected graph from question (a) above, which variable ordering yields the directed minimal I-map below? x 1 x 2 x 3 x 4 x 5 Figure 5 .5: Not a directed I-map for the undirected graphical model defined by the graph in Exercise 5.7, Question (a). x 1 x 2 x 3 x 4 x 5 Solution. x 1 is the root of the DAG, so it comes first. Next in the ordering are the children of x 1 : x 2 , x 3 , x 4 . Since x 3 is a child of x 4 , and x 4 a child of x 2 , we must have x 1 , x 2 , x 4 , x 3 . Furthermore, x 3 must come before x 5 in the ordering since x 5 is a child of x 3 , hence the ordering used must have been: x 1 , x 2 , x 4 , x 3 , x 5 . I-maps, minimal I-maps, and I-equivalency Consider the following probability density function for random variables x 1 , . . . , x 6 . p a (x 1 , . . . , x 6 ) = p(x 1 )p(x 2 )p(x 3 |x 1 , x 2 )p(x 4 |x 2 )p(x 5 |x 1 )p(x 6 |x 3 , x 4 , x 5 ) For each of the two graphs below, explain whether it is a minimal I-map, not a minimal I-map but still an I-map, or not an I-map for the independencies that hold for p a . x 1 x 2 x 3 x 4 x 5 x 6 graph 1 x 1 x 2 x 3 x 4 x 5 x 6 graph 2 Solution. The pdf can be visualised as the following directed graph, which is a minimal I-map for it. x 1 x 2 x 3 x 4 x 5 x 6 Graph 1 defines distributions that factorise as p b (x) = p(x 1 )p(x 2 )p(x 3 |x 1 , x 2 )p(x 4 |x 2 , x 3 )p(x 5 |x 1 , x 3 )p(x 6 |x 3 , x 4 , x 5 ). (S.5.1) Comparing with p a (x 1 , . . . , x 6 ), we see that only the conditionals p(x 4 |x 2 , x 3 ) and p(x 5 |x 1 , x 3 ) are different. Specifically, their conditioning set includes x 3 , which means that Graph 1 encodes fewer independencies than what p a (x 1 , . . . , x 6 ) satisfies. In particular x 4 ⊥ ⊥ x 3 |x 2 and x 5 ⊥ ⊥ x 3 |x 1 are not represented in the graph. This means that we could remove x 3 from the conditioning sets, or equivalently remove the edges x 3 → x 4 and x 3 → x 5 from the graph without introducing independence assertions that do not hold for p a . This means graph 1 is an I-map but not a minimal I-map. Graph 2 is not an I-map. To be an undirected minimal I-map, we had to connect variables x 5 and x 4 that are parents of x 6 . Graph 2 wrongly claims that x 5 ⊥ ⊥ x 4 | x 1 , x 3 , x 6 . Limits of directed and undirected graphical models We here consider the probabilistic model p(y 1 , y 2 , x 1 , x 2 ) = p(y 1 , y 2 |x 1 , x 2 )p(x 1 )p(x 2 ) where p(y 1 , y 2 |x 1 , x 2 ) factorises as p(y 1 , y 2 |x 1 , x 2 ) = p(y 1 |x 1 )p(y 2 |x 2 )φ(y 1 , y 2 )n(x 1 , x 2 ) (5.1) with n(x 1 , x 2 ) equal to n(x 1 , x 2 ) = p(y 1 |x 1 )p(y 2 |x 2 )φ(y 1 , y 2 )dy 1 dy 2 -1 . (5.2) In the model, x 1 and x 2 are two independent inputs that each control the interacting variables y 1 and y 2 (see graph below). However, the nature of the interaction between y 1 and y 2 is not modelled. In particular, we do not assume a directionality, i.e. y 1 → y 2 , or y 2 → y 1 . some interaction x 1 x 2 y 1 y 2 (a) Use the basic characterisations of statistical independence u ⊥ ⊥ v|z ⇐⇒ p(u, v|z) = p(u|z)p(v|z) (5.3) u ⊥ ⊥ v|z ⇐⇒ p(u, v|z) = a(u, z)b(v, z) (a(u, z) ≥ 0, b(v, z) ≥ 0) (5.4) to show that p(y 1 , y 2 , x 1 , x 2 ) satisfies the following independencies x 1 ⊥ ⊥ x 2 x 1 ⊥ ⊥ y 2 | y 1 , x 2 x 2 ⊥ ⊥ y 1 | y 2 , x 1 Solution. The pdf/pmf is p(y 1 , y 2 , x 1 , x 2 ) = p(y 1 |x 1 )p(y 2 |x 2 )φ(y 1 , y 2 )n(x 1 , x 2 )p(x 1 )p(x 2 ) For x 1 ⊥ ⊥ x 2 We compute p(x 1 , x 2 ) as p(x 1 , x 2 ) = p(y 1 , y 2 , x 1 , x 2 )dy 1 dy 2 (S.5.2) = p(y 1 |x 1 )p(y 2 |x 2 )φ(y 1 , y 2 )n(x 1 , x 2 )p(x 1 )p(x 2 )dy 1 dy 2 (S.5.3) = n(x 1 , x 2 )p(x 1 )p(x 2 ) p(y 1 |x 1 )p(y 2 |x 2 )φ(y 1 , y 2 )dy 1 dy 2 (S.5.4) (5.2) = n(x 1 , x 2 )p(x 1 )p(x 2 ) 1 n(x 1 , x 2 ) (S.5.5) = p(x 1 )p(x 2 ). (S.5.6) Since p(x 1 ) and p(x 2 ) are the univariate marginals of x 1 and x 2 , respectively, it follows from (5.3) that x 1 ⊥ ⊥ x 2 . For x 1 ⊥ ⊥ y 2 | y 1 , x 2 We rewrite p(y 1 , y 2 , x 1 , x 2 ) as p(y 1 , y 2 , x 1 , x 2 ) = p(y 1 |x 1 )p(y 2 |x 2 )φ(y 1 , y 2 )n(x 1 , x 2 )p(x 1 )p(x 2 ) (S.5.7) = [p(y 1 |x 1 )p(x 1 )n(x 1 , x 2 )] [p(y 2 |x 2 )φ(y 1 , y 2 )p(x 2 )] (S.5.8) = φ A (x 1 , y 1 , x 2 )φ B (y 2 , y 1 , x 2 ) (S.5.9) With (5.4), we have that x 1 ⊥ ⊥ y 2 | y 1 , x 2 . Note that p(x 2 ) can be associated either with φ A or with φ B . For x 2 ⊥ ⊥ y 1 | y 2 , x 1 We use here the same approach as for x 1 ⊥ ⊥ y 2 | y 1 , x 2 . (By symmetry considerations, we could immediately see that the relation holds but let us write it out for clarity). We rewrite p(y 1 , y 2 , x 1 , x 2 ) as p(y 1 , y 2 , x 1 , x 2 ) = p(y 1 |x 1 )p(y 2 |x 2 )φ(y 1 , y 2 )n(x 1 , x 2 )p(x 1 )p(x 2 ) (S.5.10) = [p(y 2 |x 2 )n(x 1 , x 2 )p(x 2 )p(x 1 ))] [p(y 1 |x 1 )φ(y 1 , y 2 )]) (S.5.11) = φA (x 2 , x 1 , y 2 ) φB (y 1 , y 2 , x 1 ) (S.5.12) With (5.4), we have that x 2 ⊥ ⊥ y 1 | y 2 , x 1 . (b) Is there an undirected perfect map for the independencies satisfied by p(y 1 , y 2 , x 1 , x 2 )? Solution. We write p(y 1 , y 2 , x 1 , x 2 ) = p(y 1 |x 1 )p(y 2 |x 2 )φ(y 1 , y 2 )n(x 1 , x 2 )p(x 1 )p(x 2 ) as a Gibbs distribution p(y 1 , y 2 , x 1 , x 2 ) = φ 1 (y 1 , x 1 )φ 2 (y 2 , x 2 )φ 3 (y 1 , y 2 )φ 4 (x 1 , x 2 ) with (S.5.13) φ 1 (y 1 , x 1 ) = p(y 1 |x 1 )p(x 1 ) (S.5.14) φ 2 (y 2 , x 2 ) = p(y 2 |x 2 )p(x 2 ) (S.5.15) φ 3 (y 1 , y 2 ) = φ(y 1 , y 2 ) (S.5.16) φ 4 (x 1 , x 2 ) = n(x 1 , x 2 ). (S.5.17) Visualising it as an undirected graph gives an I-map: x 1 x 2 y 1 y 2 While the graph implies x 1 ⊥ ⊥ y 2 | y 1 , x 2 and x 2 ⊥ ⊥ y 1 | y 2 , x 1 , the independency x 1 ⊥ ⊥ x 2 is not represented. Hence the graph is not a perfect map. Note further that removing any edge would result in a graph that is not an I-map for I(p) anymore. Hence the graph is a minimal I-map for I(p) but that we cannot obtain a perfect I-map. (c) Is there a directed perfect map for the independencies satisfied by p(y 1 , y 2 , x 1 , x 2 )? Solution. We construct directed minimal I-maps for p(y 1 , y 2 , x 1 , x 2 ) = p(y 1 , y 2 |x 1 , x 2 )p(x 1 )p(x 2 ) for different orderings. We will see that they do not represent all independencies in I(p) and hence that they are not perfect I-maps. To guarantee unconditional independence of x 1 and x 2 , the two variables must come first in the orderings (either x 1 and then x 2 or the other way around). If we use the ordering x 1 , x 2 , y 1 , y 2 , and that • x 1 ⊥ ⊥ x 2 • y 2 ⊥ ⊥ x 1 |y 1 , x 2 , which is y 2 ⊥ ⊥ pre(y 2 ) \\ π|π for π = (y 1 , x 2 ) are in I(p), we obtain the following directed minimal I-map: x 1 x 2 y 1 y 2 The graphs misses x 2 ⊥ ⊥ y 1 | y 2 , x 1 . If we use the ordering x 1 , x 2 , y 2 , y 1 , and that • x 1 ⊥ ⊥ x 2 • y 1 ⊥ ⊥ x 2 |x 1 , y 2 , which is y 1 ⊥ ⊥ pre(y 1 ) \\ π|π for π = (x 1 , y 2 ) are in I(p), we obtain the following directed minimal I-map: x 1 x 2 y 1 y 2 The graph misses x 1 ⊥ ⊥ y 2 | y 1 , x 2 . Moreover, the graphs imply a directionality between y 1 and y 2 , or a direct influence of x 1 on y 2 , or of x 2 on y 1 , in contrast to the original modelling goals. (d) (advanced) The following factor graph represents p(y 1 , y 2 , x 1 , x 2 ): p(x 1 ) x 1 p(x 2 ) x 2 p(y 1 |x 1 ) p(y 2 |x 2 ) y 1 y 2 n(x 1 x 2 ) φ(y 1 y 2 ) Use the separation rules for factor graphs to verify that we can find all independence relations. The separation rules are (see Barber, 2012 , Section 4.4.1), or the original paper by Frey (2003) : \"If all paths are blocked, the variables are conditionally independent. A path is blocked if one or more of the following conditions is satisfied: 1. One of the variables in the path is in the conditioning set. 2. One of the variables or factors in the path has two incoming edges that are part of the path (variable or factor collider), and neither the variable or factor nor any of its descendants are in the conditioning set.\" Remarks: • \"one or more of the following\" should best be read as \"one of the following\". • \"incoming edges\" means directed incoming edges • the descendants of a variable or factor node are all the variables that you can reach by following a path (containing directed or directed edges, but for directed edges, all directions have to be consistent) • In the graph we have dashed directed edges: they do count when you determine the descendants but they do not contribute to paths. For example, y 1 is a descendant of the n(x 1 , x 2 ) factor node but x 1 -n -y 2 is not a path. Solution. x 1 ⊥ ⊥ x 2 There are two paths from x 1 to x 2 marked with red and blue below: p(x 1 ) x 1 p(x 2 ) x 2 p(y 1 |x 1 ) p(y 2 |x 2 ) y 1 y 2 n(x 1 x 2 ) φ(y 1 y 2 ) Both the blue and red path are blocked by condition 2. x 1 ⊥ ⊥ y 2 | y 1 , x 2 There are two paths from x 1 to y 2 marked with red and blue below: p(x 1 ) x 1 p(x 2 ) x 2 p(y 1 |x 1 ) p(y 2 |x 2 ) y 1 y 2 n(x 1 x 2 ) φ(y 1 y 2 ) The observed variables are marked in blue. For the red path, the observed x 2 blocks the path (condition 1). Note that the n(x 1 , x 2 ) node would be open by condition 2. The blue path is blocked by condition 1 too. In directed graphical models, the y 1 node would be open, but here while condition 2 does not apply, condition 1 still applies (note the one or more of ... in the separation rules), so that the path is blocked. x 2 ⊥ ⊥ y 1 | y 2 , x 1 There are two paths from x 2 to y 1 marked with red and blue below: p(x 1 ) x 1 p(x 2 ) x 2 p(y 1 |x 1 ) p(y 2 |x 2 ) y 1 y 2 n(x 1 x 2 ) φ(x 1 x 2 ) The same reasoning as before yields the result. Finally note that x 1 and x 2 are not independent given y 1 or y 2 because the upper path through n(x 1 , x 2 ) is not blocked whenever y 1 or y 2 are observed (condition 2). Credit: this example is discussed in the original paper by B. Frey (Figure 6 ). Chapter 6 Factor Graphs and Message Passing Conversion to factor graphs (a) Draw an undirected graph and an undirected factor graph for p(x 1 , x 2 , x 3 ) = p(x 1 )p(x 2 )p(x 3 |x 1 , x 2 ) Solution. x 1 x 2 x 3 x 1 x 2 p(x 3 |x 1 x 2 ) x 3 p(x 1 ) p(x 2 ) (b) Draw an undirected factor graph for the directed graphical model defined by the graph below. y 1 y 2 y 3 y 4 x 1 x 2 x 3 x 4 Solution. The graph specifies probabilistic models that factorise as p(x 1 , . . . , x 4 , y 1 , . . . , y 4 ) = p(x 1 )p(y 1 |x 1 ) 4 i=2 p(y i |x i )p(x i |x i-1 ) It is the graph for a hidden Markov model. The corresponding factor graph is shown below. y 1 p(y 1 |x 1 ) y 2 p(y 2 |x 2 ) y 3 p(y 3 |x 3 ) y 4 p(y 4 |x 4 ) p(x 1 ) x 1 p(x 2 |x 1 ) x 2 p(x 3 |x 2 ) x 3 p(x 4 |x 3 ) x 4 (c) Draw the moralised graph and an undirected factor graph for directed graphical models defined by the graph below (this kind of graph is called a polytree: there are no loops but a node may have more than one parent). x 1 x 2 x 3 x 4 x 5 x 6 Solution. The moral graph is obtained by connecting the parents of the collider node x 4 . See the graph on the left in the figure below. For the factor graph, we note that the directed graph defines the following class of probabilistic models p(x 1 , . . . x 6 ) = p(x 1 )p(x 2 )p(x 3 |x 1 )p(x 4 |x 1 , x 2 )p(x 5 |x 4 )p(x 6 |x 4 ) This gives the factor graph on right in the figure below. x 1 x 2 x 3 x 4 x 5 x 6 p(x 1 ) x 1 p(x 2 ) x 2 p(x 3 |x 1 ) x 3 p(x 4 |x 1 x 2 ) x 4 p(x 5 |x 4 ) x 5 p(x 6 |x 4 ) x 6 Note: • The moral graph contains a loop while the factor graph does not. The factor graph is still a polytree. This can be exploited for inference. • One may choose to group some factors together in order to obtain a factor graph with a particular structure (see factor graph below) x 1 x 2 p(x 3 |x 1 ) x 3 p(x 4 |x 1 x 2 )p(x 1 )p(x 2 ) x 4 p(x 5 |x 4 )p(x 6 |x 4 ) x 5 x 6 Sum-product message passing We here consider the following factor tree: φ A x 1 φ C x 2 φ B x 3 φ D x 4 φ E x 5 φ F Let all variables be binary, x i ∈ {0, 1}, and the factors be defined as follows: x 1 φ A 0 2 1 4 x 2 φ B 0 4 1 4 x 1 x 2 x 3 φ C 0 0 0 4 1 0 0 2 0 1 0 2 1 1 0 6 0 0 1 2 1 0 1 6 0 1 1 6 1 1 1 4 x 3 x 4 φ D 0 0 8 1 0 2 0 1 2 1 1 6 x 3 x 5 φ E 0 0 3 1 0 6 0 1 6 1 1 3 x 5 φ F 0 1 1 8 (a) Mark the graph with arrows indicating all messages that need to be computed for the computation of p(x 1 ). Solution. φ A x 1 φ C x 2 φ B x 3 φ D x 4 φ E x 5 φ F → ← ↓ ↓ ← ← ← ← ← ← (b) Compute the messages that you have identified. Assuming that the computation of the messages is scheduled according to a common clock, group the messages together so that all messages in the same group can be computed in parallel during a clock cycle. Solution. Since the variables are binary, each message can be represented as a two-dimensional vector. We use the convention that the first element of the vector corresponds to the message for x i = 0 and the second element to the message for x i = 1. For example, µ φ A →x 1 µ φ A →x 1 µ φ A →x 1 = 2 4 (S.6.1) means that the message µ φ A →x 1 (x 1 ) equals 2 for x 1 = 0, i.e. µ φ A →x 1 (0) = 2. The following figure shows a grouping (scheduling) of the computation of the messages. φ A x 1 φ C x 2 φ B x 3 φ D x 4 φ E x 5 φ F [1] → ← [5] [2]↓ [1]↓ ← [4] [ 2 ] ← [1] ← ← [ 3 ] ← [2] ← [1] Clock cycle 1: µ φ A →x 1 µ φ A →x 1 µ φ A →x 1 = 2 4 µ φ B →x 2 µ φ B →x 2 µ φ B →x 2 = 4 4 µ x 4 →φ D µ x 4 →φ D µ x 4 →φ D = 1 1 µ φ F →x 5 µ φ F →x 5 µ φ F →x 5 = 1 8 (S.6.2) Clock cycle 2: µ x 2 →φ C µ x 2 →φ C µ x 2 →φ C = µ φ B →x 2 µ φ B →x 2 µ φ B →x 2 = 4 4 µ x 5 →φ E µ x 5 →φ E µ x 5 →φ E = µ φ F →x 5 µ φ F →x 5 µ φ F →x 5 = 1 8 (S.6.3) Message µ φ D →x 3 is defined as µ φ D →x 3 (x 3 ) = x 4 φ D (x 3 , x 4 )µ x 4 →φ D (x 4 ) (S.6.4) so that µ φ D →x 3 (0) = 1 x 4 =0 φ D (0, x 4 )µ x 4 →φ D (x 4 ) (S.6.5) = φ D (0, 0)µ x 4 →φ D (0) + φ D (0, 1)µ x 4 →φ D (1) (S.6.6) = 8 • 1 + 2 • 1 (S.6.7) = 10 (S.6.8) µ φ D →x 3 (1) = 1 x 4 =0 φ D (1, x 4 )µ x 4 →φ D (x 4 ) (S.6.9) = φ D (1, 0)µ x 4 →φ D (0) + φ D (1, 1)µ x 4 →φ D (1) (S.6.10) = 2 • 1 + 6 • 1 (S.6.11) = 8 (S.6.12) and thus µ φ D →x 3 µ φ D →x 3 µ φ D →x 3 = 10 8 . (S.6.13) The above computations can be written more compactly in matrix notation. Let φ D φ D φ D be the matrix that contains the outputs of φ D (x 3 , x 4 ) φ D φ D φ D = φ D (x 3 = 0, x 4 = 0) φ D (x 3 = 0, x 4 = 1) φ D (x 3 = 1, x 4 = 0) φ D (x 3 = 1, x 4 = 1) = 8 2 2 6 . (S.6.14) We can then write µ φ D →x 3 µ φ D →x 3 µ φ D →x 3 in terms of a matrix vector product, µ φ D →x 3 µ φ D →x 3 µ φ D →x 3 = φ D φ D φ D µ x 4 →φ D µ x 4 →φ D µ x 4 →φ D . (S.6.15) Clock cycle 3: Representing the factor φ E as matrix φ E φ E φ E , φ E φ E φ E = φ E (x 3 = 0, x 5 = 0) φ E (x 3 = 0, x 5 = 1) φ E (x 3 = 1, x 5 = 0) φ E (x 3 = 1, x 5 = 1) = 3 6 6 3 , (S.6.16) we can write µ φ E →x 3 (x 3 ) = x 5 φ E (x 3 , x 5 )µ x 5 →φ E (x 5 ) (S.6.17) as a matrix vector product, µ φ E →x 3 µ φ E →x 3 µ φ E →x 3 = φ E φ E φ E µ x 5 →φ E µ x 5 →φ E µ x 5 →φ E (S. 6.18) = 3 6 6 3 1 8 (S.6.19) = 51 30 . (S.6.20) Clock cycle 4: Variable node x 3 has received all incoming messages, and can thus output µ x 3 →φ C , µ x 3 →φ C (x 3 ) = µ φ D →x 3 (x 3 )µ φ E →x 3 (x 3 ). (S.6.21) Using to denote element-wise multiplication of two vectors, we have µ x 3 →φ C µ x 3 →φ C µ x 3 →φ C = µ φ D →x 3 µ φ D →x 3 µ φ D →x 3 µ φ E →x 3 µ φ E →x 3 µ φ E →x 3 (S. 6.22) = 10 8 51 30 (S.6.23) = 510 240 . (S.6.24) Clock cycle 5: Factor node φ C has received all incoming messages, and can thus output µ φ C →x 1 , If we also computed the messages toward the leaf factor nodes, we needed six cycles, but they are not necessary for computation of the marginals so they are omitted. µ φ C →x 1 (x 1 ) = x 2 ,x 3 φ C (x 1 , x 2 , x 3 )µ x 2 →φ C (x 2 )µ x 3 →φ C (x 3 ). (S.6.25) φ A x 1 φ C x 2 φ B x 3 φ D x 4 φ E x 5 φ F [1] → ← [5] [2] → [2]↓ ↑[5] [1]↓ ← [4] [3] → [ 2 ] ← → [ 4 ] [1] ← → [5] ← [ 3 ] [ 4 ] → ← [2] [5] → ← [1] which is in vector notation p(x 1 = 0) p(x 1 = 1) ∝ µ φ A →x 1 µ φ A →x 1 µ φ A →x 1 µ φ C →x 1 µ φ C →x 1 µ φ C →x 1 (S. 6.48) ∝ 2 4 19920 25920 (S.6.49) ∝ 39840 103680 . (S.6.50) Normalisation gives p(x 1 = 0) p(x 1 = 1) = 1 39840 + 103680 39840 103680 (S.6.51) = 0.2776 0.7224 (S.6.52) so that p(x 1 = 1) = 0.7224. Note the relatively large numbers in the messages that we computed. In other cases, one may obtain very small ones depending on the scale of the factors. This can cause numerical issues that can be addressed by working in the logarithmic domain. (d) Draw the factor graph corresponding to p(x 1 , x 3 , x 4 , x 5 |x 2 = 1) and provide the numerical values for all factors. Solution. The pmf represented by the original factor graph is p(x 1 , . . . , x 5 ) ∝ φ A (x 1 )φ B (x 2 )φ C (x 1 , x 2 , x 3 )φ D (x 3 , x 4 )φ E (x 3 , x 5 )φ F (x 5 ) The conditional p(x 1 , x 3 , x 4 , x 5 |x 2 = 1) is proportional to p(x 1 , . . . , x 5 ) with x 2 fixed to x 2 = 1, i.e. p(x 1 , x 3 , x 4 , x 5 |x 2 = 1) ∝ p(x 1 , x 2 = 1, x 3 , x 4 , x 5 ) (S.6.53) ∝ φ A (x 1 )φ B (x 2 = 1)φ C (x 1 , x 2 = 1, x 3 )φ D (x 3 , x 4 )φ E (x 3 , x 5 )φ F (x 5 ) (S.6.54) ∝ φ A (x 1 )φ x 2 C (x 1 , x 3 )φ D (x 3 , x 4 )φ E (x 3 , x 5 )φ F (x 5 ) (S.6.55) where φ x 2 C (x 1 , x 3 ) = φ C (x 1 , x 2 = 1, x 3 ). The numerical values of φ x 2 C (x 1 , x 3 ) can be read from the table defining φ C (x 1 , x 2 , x 3 ), extracting those rows where x 2 = 1, x 1 x 2 x 3 φ C 0 0 0 4 1 0 0 2 → 0 1 0 2 → 1 1 0 6 0 0 1 2 1 0 1 6 → 0 1 1 6 → 1 1 1 4 so that x 1 x 3 φ x 2 C 0 0 2 1 0 6 0 1 6 1 1 4 The factor graph for p(x 1 , x 3 , x 4 , x 5 |x 2 = 1) is shown below. Factor φ B has disappeared since it only depended on x 2 and thus became a constant. Factor φ C is replaced by φ x 2 C defined above. The remaining factors are the same as in the original factor graph. φ A x 1 φ x 2 C x 3 φ D x 4 φ E x 5 φ F (e) Compute p(x 1 = 1|x 2 = 1), re-using messages that you have already computed for the evaluation of p(x 1 = 1). Solution. The message µ φ A →x 1 is the same as in the original factor graph and µ x 3 →φ x 2 C = µ x 3 →φ C . This is because the outgoing message from x 3 corresponds to the effective factor obtained by summing out all variables in the sub-trees attached to x 3 (without the φ x 2 C branch), and these sub-trees do not depend on x 2 . The message µ φ x 2 C →x 1 needs to be newly computed. We have µ φ x 2 C →x 1 (x 1 ) = x 3 φ x 2 C (x 1 , x 3 )µ x 3 →φ x 2 C (S.6.56) or in vector notation µ φ x 2 C →x 1 µ φ x 2 C →x 1 µ φ x 2 C →x 1 = φ x 2 C φ x 2 C φ x 2 C µ x 3 →φ x 2 C µ x 3 →φ x 2 C µ x 3 →φ x 2 C (S.6.57) = φ x 2 C (x 1 = 0, x 3 = 0) φ x 2 C (x 1 = 0, x 3 = 1) φ x 2 C (x 1 = 1, x 3 = 0) φ x 2 C (x 1 = 1, x 3 = 1) µ x 3 →φ x 2 C µ x 3 →φ x 2 C µ x 3 →φ x 2 C (S. 6.58) = 2 6 6 4 510 240 (S.6.59) = 2460 4020 (S.6.60) We thus obtain for the marginal posterior of x 1 given x 2 = 1: p(x 1 = 0|x 2 = 1) p(x 1 = 1|x 2 = 1) ∝ µ φ A →x 1 µ φ A →x 1 µ φ A →x 1 µ φ x 2 C →x 1 µ φ x 2 C →x 1 µ φ x 2 C →x 1 (S. 6.61) ∝ 2 4 2460 4020 (S.6.62) ∝ 4920 16080 . (S.6.63) Normalisation gives p(x 1 = 0|x 2 = 1) p(x 1 = 1|x 2 = 1) = 0.2343 0.7657 (S.6.64) and thus p(x 1 = 1|x 2 = 1) = 0.7657. The posterior probability is slightly larger than the prior probability, p(x 1 = 1) = 0.7224. Sum-product message passing The following factor graph represents a Gibbs distribution over four binary variables x i ∈ {0, 1}. φ a x 1 φ b x 2 φ c x 3 x 4 φ d φ e The factors φ a , φ b , φ d are defined as follows: x 1 φ a 0 2 1 1 x 1 x 2 φ b 0 0 5 1 0 2 0 1 2 1 1 6 x 3 φ d 0 1 1 2 and φ c (x 1 , x 3 , x 4 ) = 1 if x 1 = x 3 = x 4 , and is zero otherwise. For all questions below, justify your answer: (a) Compute the values of µ x 2 →φ b (x 2 ) for x 2 = 0 and x 2 = 1. Solution. Messages from leaf-variable nodes to factor nodes are equal to one, so that µ x 2 →φ b (x 2 ) = 1 for all x 2 . (b) Assume the message µ x 4 →φc (x 4 ) equals µ x 4 →φc (x 4 ) = 1 if x 4 = 0 3 if x 4 = 1 Compute the values of φ e (x 4 ) for x 4 = 0 and x 4 = 1. Solution. Messages from leaf-factors to their variable nodes are equal to the leaffactors, and variable nodes with single incoming messages copy the message. We thus have µ φe→x 4 (x 4 ) = φ e (x 4 ) (S.6.65) µ x 4 →φc (x 4 ) = µ φe→x 4 (x 4 ) (S.6.66) and hence φ e (x 4 ) = 1 if x 4 = 0 3 if x 4 = 1 (S.6.67) (c) Compute the values of µ φc→x 1 (x 1 ) for x 1 = 0 and x 1 = 1. Solution. We first compute µ x 3 →φc (x 3 ): µ x 3 →φc (x 3 ) = µ φ d →x 3 (x 3 ) (S.6.68) = 1 if x 3 = 0 2 if x 3 = 1 (S.6.69) The desired message µ φc→x 1 (x 1 ) is by definition µ φc→x 1 (x 1 ) = x 3 ,x 4 φ c (x 1 , x 3 , x 4 )µ x 3 →φc (x 3 )µ x 4 →φc (x 4 ) (S.6.70) Since φ c (x 1 , x 3 , x 4 ) is only non-zero if x 1 = x 3 = x 4 , where it equals one, the computations simplify: µ φc→x 1 (x 1 = 0) = φ c (0, 0, 0)µ x 3 →φc (0)µ x 4 →φc (0) (S.6.71) = 1 • 1 • 1 (S.6.72) = 1 (S.6.73) µ φc→x 1 (x 1 = 1) = φ c (1, 1, 1)µ x 3 →φc (1)µ x 4 →φc (1) (S.6.74) = 1 • 2 • 3 (S.6.75) = 6 (S.6.76) (d) The message µ φ b →x 1 (x 1 ) equals µ φ b →x 1 (x 1 ) = 7 if x 1 = 0 8 if x 1 = 1 What is the probability that x 1 = 1, i.e. p(x 1 = 1)? Solution. The unnormalised marginal p(x 1 ) is given by the product of the three incoming messages p(x 1 ) ∝ µ φa→x 1 (x 1 )µ φ b →x 1 (x 1 )µ φc→x 1 (x 1 ) (S.6.77) With µ φ b →x 1 (x 1 ) = x 2 φ b (x 1 , x 2 ) (S.6.78) it follows that µ φ b →x 1 (x 1 = 0) = x 2 φ b (0, x 2 ) (S.6.79) = 5 + 2 (S.6.80) = 7 (S.6.81) µ φ b →x 1 (x 1 = 1) = x 2 φ b (1, x 2 ) (S. 6.82) = 2 + 6 (S.6.83) = 8 (S.6.84) Hence, we obtain p(x 1 = 0) ∝ 2 • 7 • 1 = 14 (S.6.85) p(x 1 = 1) ∝ 1 • 8 • 6 = 48 (S.6.86) and normalisation yields the desired result p(x 1 = 1) = 48 14 + 48 = 48 62 = 24 31 = 0.774 (S.6.87) Max-sum message passing We here compute most probable states for the factor graph and factors below. φ A x 1 φ C x 2 φ B x 3 φ D x 4 φ E x 5 φ F Let all variables be binary, x i ∈ {0, 1}, and the factors be defined as follows: x 1 φ A 0 2 1 4 x 2 φ B 0 4 1 4 x 1 x 2 x 3 φ C 0 0 0 4 1 0 0 2 0 1 0 2 1 1 0 6 0 0 1 2 1 0 1 6 0 1 1 6 1 1 1 4 x 3 x 4 φ D 0 0 8 1 0 2 0 1 2 1 1 6 x 3 x 5 φ E 0 0 3 1 0 6 0 1 6 1 1 3 x 5 φ F 0 1 1 8 (a) Will we need to compute the normalising constant Z to determine argmax x p(x 1 , . . . , x 5 )? Solution. This is not necessary since argmax x p(x 1 , . . . , x 5 ) = argmax x cp(x 1 , . . . , x 5 ) for any constant c. Algorithmically, the backtracking algorithm is also invariant to any scaling of the factors. (b) Compute argmax x 1 ,x 2 ,x 3 p(x 1 , x 2 , x 3 |x 4 = 0, x 5 = 0) via max-sum message passing. Solution. We first derive the factor graph and corresponding factors for p(x 1 , x 2 , x 3 |x 4 = 0, x 5 = 0). For fixed values of x 4 , x 5 , the two variables are removed from the graph, and the factors φ D (x 3 , x 4 ) and φ E (x 3 , x 5 ) are reduced to univariate factors φ x 4 D (x 3 ) and φ x 5 D (x 3 ) by retaining those rows in the table where x 4 = 0 and x 5 = 0, respectively: x 3 φ x 4 D 0 8 1 2 x 3 φ x 5 E 0 3 1 6 Since both factors only depend on x 3 , they can be combined into a new factor φ(x 3 ) by element-wise multiplication. x 3 φ 0 24 1 12 Moreover, since we work with an unnormalised model, we can rescale the factor so that the maximum value is one, so that x 3 φ 0 2 1 1 Factor φ F (x 5 ) is a constant for fixed value of x 5 and can be ignored. The factor graph for p(x 1 , x 2 , x 3 |x 4 = 0, x 5 = 0) thus is φ A x 1 φ C x 2 φ B x 3 φ Let us fix x 1 as root towards which we compute the messages. The messages that we need to compute are shown in the following graph φ A x 1 φ C x 2 φ B x 3 φ → ← ↓ ↓ ← ← Next, we compute the leaf (log) messages. We only have factor nodes as leaf nodes so that λ φ A →x 1 = log φ A (x 1 = 0) log φ A (x 1 = 1) = log 2 log 4 (S.6.88) and similarly λ φ B →x 2 = log φ B (x 2 = 0) log φ B (x 2 = 1) = log 4 log 4 λ φ→x 3 = log φ(x 3 = 0) log φ(x 3 = 1) = log 2 log 1 (S.6.89) Since the variable nodes x 2 and x 3 only have one incoming edge each, we obtain λ x 2 →φ C = λ φ B →x 2 = log 4 log 4 λ x 3 →φ C = λ φ→x 3 = log 2 log 1 (S.6.90) The message λ φ C →x 1 (x 1 ) equals λ φ C →x 1 (x 1 ) = max x 2 ,x 3 log φ C (x 1 , x 2 , x 3 ) + λ x 2 →φ C (x 2 ) + λ x 3 →φ C (x 3 ) (S.6.91) where we wrote the messages in non-vector notation to highlight their dependency on the variables x 2 and x 3 . We now have to consider all combinations of x 2 and x 3 x 2 x 3 log φ C (x 1 = 0, x 2 , x 3 ) 0 0 log 4 1 0 log 2 0 1 log 2 1 1 log 6 x 2 x 3 log φ C (x 1 = 1, x 2 , x 3 ) 0 0 log 2 1 0 log 6 0 1 log 6 1 1 log 4 Furthermore x 2 x 3 λ x 2 →φ C (x 2 ) + λ x 3 →φ C (x 3 ) 0 0 log 4 + log 2 = log 8 1 0 log 4 + log 2 = log 8 0 1 log 4 1 1 log 4 Hence for x 1 = 0, we have x 2 x 3 log φ C (x 1 = 0, x 2 , x 3 ) + λ x 2 →φ C (x 2 ) + λ x 3 →φ C (x 3 ) 0 0 log 4 + log 8 = log 32 1 0 log 2 + log 8 = log 16 0 1 log 2 + log 4 = log 8 1 1 log 6 + log 4 = log 24 The maximal value is log 32 and for backtracking, we also need to keep track of the argmax which is here x2 = x3 = 0. For x 1 = 1, we have x 2 x 3 log φ C (x 1 = 1, x 2 , x 3 ) + λ x 2 →φ C (x 2 ) + λ x 3 →φ C (x 3 ) 0 0 log 2 + log 8 = log 16 1 0 log 6 + log 8 = log 48 0 1 log 6 + log 4 = log 24 1 1 log 4 + log 4 = log 16 The maximal value is log 48 and the argmax is (x 2 = 1, x3 = 0). So overall, we have λ φ C →x 1 = λ φ C →x 1 (x 1 = 0) λ φ C →x 1 (x 1 = 1) = log 32 log 48 (S.6.92) and the argmax back-tracking function is λ * φ C →x 1 (x 1 ) = (x 2 = 0, x3 = 0) if x 1 = 0 (x 2 = 1, x3 = 0) if x 1 = 1 (S.6.93) We now have all incoming messages to the assigned root node x 1 . Ignoring the normalising constant, we obtain γ = γ * (x 1 = 0) γ * (x 1 = 1) = λ φ A →x 1 + λ φ C →x 1 (S. 6.94) = log 2 log 4 + log 32 log 48 = log 64 log 192 (S.6.95) The value x 1 for which γ * (x 1 ) is largest is thus x1 = 1. Plugging x1 = 1 into the backtracking function λ * φ C →x 1 (x 1 ) gives (x 1 , x2 , x3 ) = argmax x 1 ,x 2 ,x 3 p(x 1 , x 2 , x 3 |x 4 = 0, x 5 = 0) = (1, 1, 0). (S.6.96) In this low-dimensional example, we can verify the solution by computing the unnormalised pmf for all combinations of x 1 , x 2 , x 3 . This is done in the following table where we start with the table for φ C and then multiply-in the further factors φ A , φ and φ B . x 1 x 2 x 3 φ C φ C φ A φ C φ A φ φ C φ A φφ B 0 0 0 4 8 16 16 • 4 1 0 0 2 8 16 16 • 4 0 1 0 2 8 16 16 • 4 1 1 0 6 24 48 48 • 4 0 0 1 2 8 8 8 • 4 1 0 1 6 24 24 24 • 4 0 1 1 6 12 12 12 • 4 1 1 1 4 16 16 16 • 4 For example, for the column φ c φ A , we multiply each value of φ C (x 1 , x 2 , x 3 ) by φ A (x 1 ), so that the rows with x 1 = 0 get multiplied by 2, and the rows with x 1 = 1 by 4. The maximal value in the final column is achieved for x 1 = 1, x 2 = 1, x 3 = 0, in line with the result above (and 48 • 4 = 192). Since φ B (x 2 ) is a constant, being equal to 4 for all values of x 2 , we could have ignored it in the computation. The formal reason for this is that since the model is unnormalised, we are allowed to rescale each factor by an arbitrary (factor-dependent) constant. This operation does not change the model. So we could divide φ B by 4 which would give a value of 1, so that the factor can indeed be ignored. (c) Compute argmax x 1 ,...,x 5 p(x 1 , . . . , x 5 ) via max-sum message passing with x 1 as root. Solution. As discussed in the solution to the answer above, we can drop factor φ B (x 2 ) since it takes the same value for all x 2 . Moreover, we can rescale the individual factors by a constant so they are more amenable to calculations by hand. We normalise them such that the largest value is one, which gives the following factors. Note that this is entirely optional. x 1 φ A 0 1 1 2 x 1 x 2 x 3 φ C 0 0 0 2 1 0 0 1 0 1 0 1 1 1 0 3 0 0 1 1 1 0 1 3 0 1 1 3 1 1 1 2 x 3 x 4 φ D 0 0 4 1 0 1 0 1 1 1 1 3 x 3 x 5 φ E 0 0 1 1 0 2 0 1 2 1 1 1 x 5 φ F 0 1 1 8 The factor graph without φ B together with the messages that we need to compute is: φ A x 1 φ C x 2 x 3 φ D x 4 φ E x 5 φ F → ← ↓ ← ← ← ← ← ← The leaf (log) messages are (using vector notation where the top element corresponds to x i = 0 and the bottom one to x i = 1): λ φ A →x 1 = 0 log 2 λ x 2 →φ C = 0 0 λ x 4 →φ D = 0 0 λ φ F →x 5 = 0 log 8 (S.6.97) The variable node x 5 only has one incoming edge so that λ x 5 →φ E = λ φ F →x 5 . The message λ φ E →x 3 (x 3 ) equals λ φ E →x 3 (x 3 ) = max x 5 log φ E (x 3 , x 5 ) + λ x 5 →φ E (x 5 ) (S.6.98) Writing out log φ E (x 3 , x 5 ) + λ x 5 →φ E (x 5 ) for all x 5 as a function of x 3 we have x 5 log φ E (x 3 = 0, x 5 ) + λ x 5 →φ E (x 5 ) 0 log 1 + 0 = 0 1 log 2 + log 8 = log 16 x 5 log φ E (x 3 = 1, x 5 ) + λ x 5 →φ E (x 5 ) 0 log 2 + 0 = log 2 1 log 1 + log 8 = log 8 Taking the maximum over x 5 as a function of x 3 , we obtain λ φ E →x 3 = log 16 log 8 (S.6.99) and the backtracking function that indicates the maximiser x5 = argmax x 5 log φ E (x 3 , x 5 )+ λ x 5 →φ E (x 5 ) as a function of x 3 equals λ * φ E →x 3 (x 3 ) = x5 = 1 if x 3 = 0 x5 = 1 if x 3 = 1 (S.6.100) We perform the same kind of operation for λ φ D →x 3 (x 3 ) λ φ D →x 3 (x 3 ) = max x 4 log φ D (x 3 , x 4 ) + λ x 4 →φ D (x 4 ) (S.6.101) Since λ x 4 →φ D (x 4 ) = 0 for all x 4 , the table with all values of log φ D (x 3 , x 4 ) + λ x 4 →φ D (x 4 ) is x 3 x 4 log φ D (x 3 , x 4 ) + λ x 4 →φ D (x 4 ) 0 0 log 4 + 0 = log 4 1 0 log 1 + 0 = 0 0 1 log 1 + 0 = 0 1 1 log 3 + 0 = log 3 Taking the maximum over x 4 as a function of x 3 we thus obtain λ φ D →x 3 = log 4 log 3 (S.6.102) and the backtracking function that indicates the maximiser x4 = argmax x 4 log φ D (x 3 , x 4 )+ λ x 4 →φ D (x 4 ) as a function of x 3 equals λ * φ D →x 3 (x 3 ) = x4 = 0 if x 3 = 0 x4 = 1 if x 3 = 1 (S.6.103) For the message λ x 3 →φ C (x 3 ) we add together the messages λ φ E →x 3 (x 3 ) and λ φ D →x 3 (x 3 ) which gives λ x 3 →φ C = log 16 + log 4 log 8 + log 3 = log 64 log 24 (S.6.104) Next we compute the message λ φ C →x 1 (x 1 ) by maximising over x 2 and x 3 , λ φ C →x 1 (x 1 ) = max x 2 ,x 3 log φ C (x 1 , x 2 , x 3 ) + λ x 2 →φ C (x 2 ) + λ x 3 →φ C (x 3 ) (S.6.105) Since λ x 2 →φ C (x 2 ) = 0, the problem becomes λ φ C →x 1 (x 1 ) = max x 2 ,x 3 log φ C (x 1 , x 2 , x 3 ) + λ x 3 →φ C (x 3 ) (S. 6.106) Building on the table for φ C , we form a table with all values of log φ C (x 1 , x 2 , x 3 ) + λ x 3 →φ C (x 3 ) x 1 x 2 x 3 log φ C (x 1 , x 2 , x 3 ) + λ x 3 →φ C (x 3 ) 0 0 0 log 2 + log 64 = log 128 1 0 0 0 + log 64 = log 64 0 1 0 0 + log 64 = log 64 1 1 0 log 3 + log 64 = log 192 0 0 1 log 24 1 0 1 log 3 + log 24 = log 72 0 1 1 log 3 + log 24 = log 72 1 1 1 log 2 + log 24 = log 48 The maximal value as a function of x 1 are highlighted in the table, which gives the message λ φ C →x 1 = log 128 log 192 (S.6.107) and the backtracking function λ * φ C →x 1 (x 1 ) = (x 2 = 0, x3 = 0) if x 1 = 0 (x 2 = 1, x3 = 0) if x 1 = 1 (S.6.108) We now have all incoming messages to the assigned root node x 1 . Ignoring the normalising constant, we obtain γ = γ * (x 1 = 0) γ * (x 1 = 1) = 0 + log 128 log 2 + log 192 (S.6.109) We can now start the backtracking to compute the desired argmax x 1 ,...,x 5 p(x 1 , . . . , x 5 ). Starting at the root we have x1 = argmax x 1 γ * (x 1 ) = 1. Plugging this value into the look-up table λ * φ C →x 1 (x 1 ), we obtain (x 2 = 1, x3 = 0). With the look-up table λ * φ E →x 3 (x 3 ) we find x5 = 1 and λ * φ D →x 3 (x 3 ) gives x4 = 0 so that overall argmax x 1 ,...,x 5 p(x 1 , . . . , x 5 ) = (1, 1, 0, 0, 1). (S.6.110) (d) Compute argmax x 1 ,...,x 5 p(x 1 , . . . , x 5 ) via max-sum message passing with x 3 as root. Solution. With x 3 as root, we need the following messages: φ A x 1 φ C x 2 x 3 φ D x 4 φ E x 5 φ F → → ↓ → ← ← ← ← ← The following messages are the same as when x 1 was the root: λ φ D →x 3 = log 4 log 3 λ φ E →x 3 = log 16 log 8 λ φ A →x 1 = 0 log 2 λ x 2 →φ C = 0 0 (S.6.111) Since x 1 has only one incoming message, we further have λ x 1 →φ C = λ φ A →x 1 = 0 log 2 . (S.6.112) We next compute λ φ C →x 3 (x 3 ), λ φ C →x 3 (x 3 ) = max x 1 ,x 2 log φ C (x 1 , x 2 , x 3 ) + λ x 1 →φ C (x 1 ) + λ x 2 →φ C (x 2 ). (S.6.113) We first form a table for log φ C (x 1 , x 2 , x 3 ) + λ x 1 →φ C (x 1 ) + λ x 2 →φ C (x 2 ) noting that λ x 2 →φ C (x 2 ) = 0 x 1 x 2 x 3 log φ C (x 1 , x 2 , x 3 ) + λ x 1 →φ C (x 1 ) + λ x 2 →φ C (x 2 ) 0 0 0 log 2 + 0 = log 2 1 0 0 0 + log 2 = log 2 0 1 0 0 + 0 = 0 1 1 0 log 3 + log 2 = log 6 0 0 1 0 + 0 = 0 1 0 1 log 3 + log 2 = log 6 0 1 1 log 3 + 0 = log 3 1 1 1 log 2 + log 2 = log 4 The maximal value as a function of x 3 are highlighted in the table, which gives the message λ φ C →x 3 = log 6 log 6 (S.6.114) and the backtracking function λ * φ C →x 3 (x 3 ) = (x 1 = 1, x2 = 1) if x 3 = 0 (x 1 = 1, x2 = 0) if x 3 = 1 (S.6.115) We have now all incoming messages for x 3 and can compute γ * (x 3 ) up the normalising constant -log Z (which is not needed if we are interested in the argmax only: γ = γ * (x 3 = 0) γ * (x 3 = 1) = λ φ C →x 3 + λ φ D →x 3 + λ φ E →x 3 (S. 6.116) = log 6 + log 4 + log 16 = log 384 log 6 + log 3 + log 8 = log 144 (S.6.117) We can now start the backtracking which gives: x3 = 0, so that λ * φ C →x 3 (0) = (x 1 = 1, x2 = 1). The backtracking functions λ * φ E →x 3 (x 3 ) and λ * φ D →x 3 (x 3 ) are the same for question (c) , which gives λ * φ E →x 3 (0) = x5 = 1 and λ * φ D →x 3 (0) = x4 = 0. Hence, overall, we find argmax x 1 ,...,x 5 p(x 1 , . . . , x 5 ) = (1, 1, 0, 0, 1). (S.6.118) Note that this matches the result from question (c) where x 1 was the root. This is because the output of the max-sum algorithm is invariant to the choice of the root. Choice of elimination order in factor graphs Consider the following factor graph, which contains a loop: x 1 φ A x 2 x 3 φ B x 4 φ C x 5 x 6 φ D Let all variables be binary, x i ∈ {0, 1}, and the factors be defined as follows: x 1 x x 3 φ A 0 0 4 1 0 2 0 0 2 1 0 6 0 1 2 1 1 6 0 1 6 1 1 4 x 2 x 3 x 4 φ B 0 0 0 2 1 0 0 2 0 1 0 4 1 1 0 2 0 0 1 6 1 0 1 8 0 1 1 4 1 1 1 2 x 4 x 5 φ C 0 0 8 1 0 2 0 1 2 1 1 6 x 4 x 6 φ D 0 0 3 1 0 6 0 1 6 1 1 3 (a) Draw the factor graph corresponding to p(x 2 , x 3 , x 4 , x 5 | x 1 = 0, x 6 = 1) and give the tables defining the new factors φ x 1 =0 A (x 2 , x 3 ) and φ x 6 =1 D (x 4 ) that you obtain. Solution. First condition on x 1 = 0: Factor node φ A (x 1 , x 2 , x 3 ) depends on x 1 , thus we create a new factor φ x 1 =0 A (x 2 , x 3 ) from the table for φ A using the rows where x 1 = 0. φ x 1 =0 A x 2 x 3 φ B x 4 φ C x 5 x 6 φ D x 1 x 2 x 3 φ A → 0 0 0 4 1 0 0 2 → 0 1 0 2 1 1 0 6 → 0 0 1 2 1 0 1 6 → 0 1 1 6 1 1 1 4 so that x 2 x 3 φ x 1 =0 A 0 0 4 1 0 2 0 1 2 1 1 6 Next condition on x 6 = 1: Factor node φ D (x 4 , x 6 ) depends on x 6 , thus we create a new factor φ x 6 =1 D (x 4 ) from the table for φ D using the rows where x 6 = 1. φ x 1 =0 A x 2 x 3 φ B x 4 φ C x 5 φ x 6 =1 D x 4 x 6 φ D 0 0 3 1 0 6 → 0 1 6 → 1 1 3 so that x 4 φ x 6 =1 D 0 6 1 3 (b) Find p(x 2 | x 1 = 0, x 6 = 1) using the elimination ordering (x 4 , x 5 , x 3 ): (i) Draw the graph for p(x 2 , x 3 , x 5 | x 1 = 0, x 6 = 1) by marginalising x 4 Compute the table for the new factor φ4 (x 2 , x 3 , x 5 ) (ii) Draw the graph for p(x 2 , x 3 | x 1 = 0, x 6 = 1) by marginalising x 5 Compute the table for the new factor φ45 (x 2 , x 3 ) (iii) Draw the graph for p(x 2 | x 1 = 0, x 6 = 1) by marginalising x 3 Compute the table for the new factor φ453 (x 2 ) Solution. Starting with the factor graph for p(x 2 , x 3 , x 4 , x 5 | x 1 = 0, x 6 = 1) φ x 1 =0 A x 2 x 3 φ B x 4 φ C x 5 φ x 6 =1 D Marginalising x 4 combines the three factors φ B , φ C and φ x 6 =1 D φ x 1 =0 A x 2 x 3 φ4 x 5 Marginalising x 5 modifies the factor φ4 φ x 1 =0 A x 2 x 3 φ45 Marginalising x 3 combines the factors φ x 1 =0 A and φ45 x 2 φ453 We now compute the tables for the new factors φ4 , φ45 , φ453 . First find φ4 (x 2 , x 3 , x 5 ) x 2 x 3 x 4 φ B 0 0 2 0 0 2 1 0 4 1 0 2 0 1 6 0 1 8 1 1 4 1 1 2 x 4 x 5 φ C 0 0 8 1 0 2 0 1 2 1 1 6 x 4 φ x 6 =1 D 0 6 1 3 so that φ * (x 2 , x , x 4 , x 5 ) = φ B (x 2 , x 3 , x 4 )φ C (x 4 , x 5 )φ x 6 =1 D (x 4 ) equals x 2 x 3 x 4 x 5 φ * (x 2 , x 3 , x 4 , x 5 ) 0 0 0 0 2 * 8 * 6 1 0 0 0 2 * 8 * 6 0 1 0 0 4 * 8 * 6 1 1 0 0 2 * 8 * 6 0 0 1 0 6 * 2 * 3 1 0 1 0 8 * 2 * 3 0 1 1 0 4 * 2 * 3 1 1 1 0 2 * 2 * 3 0 0 0 1 2 * 2 * 6 1 0 0 1 2 * 2 * 6 0 1 0 1 4 * 2 * 6 1 1 0 1 2 * 2 * 6 0 0 1 1 6 * 6 * 3 1 0 1 1 8 * 6 * 3 0 1 1 1 4 * 6 * 3 1 1 1 1 2 * 6 * 3 and x 2 x x 5 x 4 φ B (x 2 , x 3 , x 4 )φ C (x 4 , x 5 )φ x 6 =1 D (x 4 ) φ4 0 0 (2 * 8 * 6) + (6 * 2 * 3) = 132 1 0 (2 * 8 * 6) + (8 * 2 * 3) = 144 0 0 (4 * 8 * 6) + (4 * 2 * 3) = 216 1 0 (2 * 8 * 6) + (2 * 2 * 3) = 108 0 1 (2 * 2 * 6) + (6 * 6 * 3) = 132 1 1 (2 * 2 * 6) + (8 * 6 * 3) = 168 0 1 (4 * 2 * 6) + (4 * 6 * 3) = 120 1 1 (2 * 2 * 6) + (2 * 6 * 3) = 60 Next find φ45 (x 2 , x 3 ) x 2 x 3 x 5 φ4 0 0 0 132 1 0 0 144 0 1 0 216 1 1 0 108 0 0 1 132 1 0 1 168 0 1 1 120 1 1 1 60 so that x 2 x 3 x 5 φ4 (x 2 , x 3 , x 5 ) φ45 0 0 132 + 132 = 264 1 0 144 + 168 = 312 0 1 216 + 120 = 336 1 1 108 + 60 = 168 Finally find φ453 (x 2 ) x 2 x 3 φ x 1 =0 A 0 0 4 1 0 2 0 1 2 1 1 6 x 2 x 3 φ45 0 0 264 1 0 312 0 1 336 1 1 168 so that x 2 x 3 φ45 (x 2 , x 3 )φ x 1 =0 A (x 2 , x 3 ) φ453 0 (4 * 264) + (2 * 336) = 1728 1 (2 * 312) + (6 * 168) = 1632 The normalising constant is Z = 1728 + 1632. Our conditional marginal is thus: p(x 2 | x 1 = 0, x 6 = 1) = 1728/Z 1632/Z = 0.514 0.486 (S.6.119) (c) Now determine p(x 2 | x 1 = 0, x 6 = 1 ) with the elimination ordering (x 5 , x 4 , x 3 ): (i) Draw the graph for p(x 2 , x 3 , x 4 , | x 1 = 0, x 6 = 1) by marginalising x 5 Compute the table for the new factor φ5 (x 4 ) (ii) Draw the graph for p(x 2 , x 3 | x 1 = 0, x 6 = 1) by marginalising x 4 Compute the table for the new factor φ54 (x 2 , x 3 ) (iii) Draw the graph for p(x 2 | x 1 = 0, x 6 = 1) by marginalising x 3 Compute the table for the new factor φ543 (x 2 ) Solution. Starting with the factor graph for p(x 2 , x 3 , x 4 , x 5 | x 1 = 0, x 6 = 1) φ x 1 =0 A x 2 x 3 φ B x 4 φ C x 5 φ x 6 =1 D Marginalising x 5 modifies the factor φ C φ x 1 =0 A x 2 x 3 φ B x 4 φ5 φ x 6 =1 D Marginalising x 4 combines the three factors φ B , φ5 and φ x 6 =1 D φ x 1 =0 A x 2 x 3 φ54 Marginalising x 3 combines the factors φ x 1 =0 A and φ54 x 2 φ543 We now compute the tables for the new factors φ5 , φ54 , and φ543 . First find φ5 (x 4 ) x 4 x 5 φ C 0 0 8 1 0 2 0 1 2 1 1 6 so that x 4 x 5 φ C (x 4 , x 5 ) φ5 0 8 + 2 = 10 1 2 + 6 = 8 Next find φ54 (x 2 , x 3 ) x 2 x 3 x 4 φ B 0 0 0 2 1 0 0 2 0 1 0 4 1 1 0 2 0 0 1 6 1 0 1 8 0 1 1 4 1 1 1 2 x 4 φ5 0 10 1 8 x 4 φ x 6 =1 D 0 6 1 3 so that φ * (x 2 , x 3 , x 4 ) = φ B (x 2 , x 3 , x 4 ) φ5 (x 4 )φ x 6 =1 D (x 4 ) equals x 2 x 3 x 4 φ * (x 2 , x 3 , x 4 ) 0 0 0 2 * 10 * 6 1 0 0 2 * 10 * 6 0 1 0 4 * 10 * 6 1 1 0 2 * 10 * 6 0 0 1 6 * 8 * 3 1 0 1 8 * 8 * 3 0 1 1 4 * 8 * 3 1 1 1 2 * 8 * 3 and x 2 x 3 x 4 φ B (x 2 , x 3 , x 4 ) φ5 (x 4 )φ x 6 =1 D (x 4 ) φ54 0 0 (2 * 10 * 6) + (6 * 8 * 3) = 264 1 0 (2 * 10 * 6) + (8 * 8 * 3) = 312 0 1 (4 * 10 * 6) + (4 * 8 * 3) = 336 1 1 (2 * 10 * 6) + (2 * 8 * 3) = 168 Finally find φ543 (x 2 ) x 2 x 3 φ x 1 =0 A 0 0 4 1 0 2 0 1 2 1 1 6 x 2 x 3 φ54 0 0 264 1 0 312 0 1 336 1 1 168 so that x 2 x 3 φ54 (x 2 , x 3 )φ x 1 =0 A (x 2 , x 3 ) φ543 0 (4 * 264) + (2 * 336) = 1728 1 (2 * 312) + (6 * 168) = 1632 As with the ordering in the previous part, we should come to the same result for our conditional marginal distribution.The normalising constant is Z = 1728 + 1632, so that the conditional marginal is p(x 2 | x 1 = 0, x 6 = 1) = 1728/Z 1632/Z = 0.514 0.486 (S.6.120) (d) Which variable ordering, (x 4 , x 5 , x 3 ) or (x 5 , x 4 , x 3 ) do you prefer? Solution. The ordering (x 5 , x 4 , x 3 ) is cheaper and should be preferred over the ordering (x 4 , x 5 , x 3 ) . The reason for the difference in the cost is that x 4 has three neighbours in the factor graph for p(x 2 , x 3 , x 4 , x 5 | x 1 = 0, x 6 = 1). However, after elimination of x 5 , which has only one neighbour, x 4 has only two neighbours left. Eliminating variables with more neighbours leads to larger (temporary) factors and hence a larger cost. We can see this from the tables that were generated during the computation (or numbers that we needed to add together): for the ordering (x 4 , x 5 , x 3 ), the largest table had 2 4 entries while for (x 5 , x 4 , x 3 ), it had 2 3 entries. Choosing a reasonable variable ordering has a direct effect on the computational complexity of variable elimination. This effect becomes even more pronounced when the domain of our discrete variables has a size greater than 2 (binary variables), or if the variables are continuous. φ x 1 =0 A x 2 x 3 φ B x 4 φ C x 5 φ x 6 =1 D Choice of elimination order in factor graphs We would like to compute the marginal p(x 1 ) by variable elimination for a joint pmf represented by the following factor graph. All variables x i can take K different values. x 1 φ a x 2 φ b x 3 φ c x 4 x 5 φ d x 6 φ e x 7 φ f (a) A friend proposes the elimination order x 4 , x 5 , x 6 , x 7 , x 3 , x 2 , i.e. to do x 4 first and x 2 last. Explain why this is computationally inefficient. Solution. According to the factor graph, p(x 1 , . . . , x 7 ) factorises as p(x 1 , . . . , x 7 ) ∝ φ a (x 1 , x 4 )φ b (x 2 , x 4 )φ c (x 3 , x 4 )φ d (x 5 , x 4 )φ e (x 6 , x 4 )φ f (x 7 , x 4 ) (S.6.121) If we choose to eliminate x 4 first, i.e. compute p(x 1 , x 2 , x 3 , x 5 , x 6 , x 7 ) = x 4 p(x 1 , . . . , x 7 ) (S.6.122) ∝ x 4 φ a (x 1 , x 4 )φ b (x 2 , x 4 )φ c (x 3 , x 4 )φ d (x 5 , x 4 )φ e (x 6 , x 4 )φ f (x 7 , x 4 ) (S.6.123) we cannot pull any of the factors out of the sum since each of them depends on x 4 . This means the cost to sum out x 4 for all combinations of the six variables (x 1 , x 2 , x 3 , x 5 , x 6 , x 7 ) is K 7 . Moreover, the new factor φ(x 1 , x 2 , x 3 , x 5 , x 6 , x 7 ) = x 4 φ a (x 1 , x 4 )φ b (x 2 , x 4 )φ c (x 3 , x 4 )φ d (x 5 , x 4 )φ e (x 6 , x 4 )φ f (x 7 , x 4 ) (S.6.124) does not factorise anymore so that subsequent variable eliminations will be expensive too. (b) Propose an elimination ordering that achieves O(K 2 ) computational cost per variable elimination and explain why it does so. Solution. Any ordering where x 4 is eliminated last will do. At any stage, elimination of one of the variables x 2 , x 3 , x 5 , x 6 , x 7 is then a O(K 2 ) operation. This is because e.g. p(x 1 , . . . , x 6 ) = x 7 p(x 1 , . . . , x 7 ) (S.6.125) ∝ φ a (x 1 , x 4 )φ b (x 2 , x 4 )φ c (x 3 , x 4 )φ d (x 5 , x 4 )φ e (x 6 , x 4 ) x 7 φ f (x 7 , x 4 ) φ7 (x 4 ) (S.6.126) ∝ φ a (x 1 , x 4 )φ b (x 2 , x 4 )φ c (x 3 , x 4 )φ d (x 5 , x 4 )φ e (x 6 , x 4 ) φ7 (x 4 ) (S.6.127) where computing φ7 (x 4 ) for all values of x 4 is O(K 2 ). Further, p(x 1 , . . . , x 5 ) = x 6 p(x 1 , . . . , x 6 ) (S.6.128) ∝ φ a (x 1 , x 4 )φ b (x 2 , x 4 )φ c (x 3 , x 4 )φ d (x 5 , x 4 ) φ7 (x 4 ) x 6 φ e (x 6 , x 4 ) (S.6.129) ∝ φ a (x 1 , x 4 )φ b (x 2 , x 4 )φ c (x 3 , x 4 )φ d (x 5 , x 4 ) φ7 (x 4 ) φ6 (x 4 ), (S.6.130) where computation of φ6 (x 4 ) for all values of x 4 is again O(K ). Continuing in this manner, one obtains p(x 1 , x 4 ) ∝ φ a (x 1 , x 4 ) φ2 (x 4 ) φ3 (x 4 ) φ5 (x 4 ) φ6 (x 4 ) φ7 (x ). (S.6.131) where each derived factor φ has O(K 2 ) cost. Summing out x and normalising the pmf is again a O(K 2 ) operation. Chapter 7 Inference for Hidden Markov Models Predictive distributions for hidden Markov models For the hidden Markov model p(h 1:d , v 1:d ) = p(v 1 |h 1 )p(h 1 ) d i=2 p(v i |h i )p(h i |h i-1 ) assume you have observations for v i , i = 1, . . . , u < d. (a) Use message passing to compute p(h t |v 1:u ) for u < t ≤ d. For the sake of concreteness, you may consider the case d = 6, u = 2, t = 4. Solution. The factor graph for d = 6, u = 2, with messages that are required for the computation of p(h t |v 1:u ) for t = 4, is as follows. φ 1 h 1 φ 2 h 2 p(h3|h2) h 3 p(h4|h3) h 4 p(h 5 |h 4 ) h 5 p(h 6 |h 5 ) h 6 v 3 p (v3|h3) v 4 p(v4|h4) v 5 p(v5|h5) v 6 p(v6|h6) → → → → → → → ← ← ← ← ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ The messages from the unobserved visibles v i to their corresponding h i , e.g. v 3 to h 3 , are all one. Moreover, the message from the p(h 5 |h 4 ) node to h 4 equals one as well. This is because all involved factors, p(v i |h i ) and p(h i |h i-1 ), sum to one. Hence the factor graph reduces to a chain: φ 1 h 1 φ 2 h 2 p(h 3 |h 2 ) h 3 p(h 4 |h 3 ) h 4 → → → → Since the variable nodes copy the messages in case of a chain, we only show the factor-to-variable messages. The graph shows that we are essentially in the same situation as in filtering, with the difference that we use the factors p(h s |h s-1 ) for s ≥ u + 1. Hence, we can use filtering to compute the messages until time s = u and then compute the further messages with the p(h s |h s-1 ) as factors. This gives the following algorithm: 1. Compute α(h u ) by filtering. For s = u + 1, . . . , t, compute α(h s ) = h s-1 p(h s |h s-1 )α(h s-1 ) (S.7.1) 3. The required predictive distribution is p(h t |v 1:u ) = 1 Z α(h t ) Z = ht α(h t ) (S.7.2) For s ≥ u + 1, we have that hs α(h s ) = hs h s-1 p(h s |h s-1 )α(h s-1 ) (S.7.3) = h s-1 α(h s-1 ) (S.7.4) since p(h s |h s-1 ) is normalised. This means that the normalising constant Z above equals Z = hu α(h u ) = p(v 1:u ) (S.7.5) which is the likelihood. For filtering, we have seen that α(h s ) ∝ p(h s |v 1:s ), s ≤ u. The α(h s ) for all s > u are proportional to p(h s |v 1:u ). This may be seen by noting that the above arguments hold for any t > u. (b) Use message passing to compute p(v t |v 1:u ) for u < t ≤ d. For the sake of concreteness, you may consider the case d = 6, u = 2, t = 4. Solution. The factor graph for d = 6, u = 2, with messages that are required for the computation of p(v t |v 1:u ) for t = 4, is as follows. φ 1 h 1 φ 2 h 2 p(h3|h2) h 3 p(h4|h3) h 4 p(h 5 |h 4 ) h 5 p(h 6 |h 5 ) h 6 v 3 p(v3|h3) v 4 p(v4|h4) v 5 p(v5|h5) v 6 p(v6|h6) → → → → → → → ← ← ← ← ↑ ↑ ↓ ↓ ↑ ↑ ↑ ↑ Due to the normalised factors, as above, the messages to the right of h t are all one. Moreover the messages that go up from the v i to the h i , i = t, are also all one. Hence the graph simplifies to a chain. φ 1 h 1 φ 2 h 2 p(h 3 |h 2 ) h 3 p(h 4 |h 3 ) h 4 p(v 4 |h 4 ) v 4 → → → → → The message in blue is proportional to p(h t |v 1:u ) computed in question (a). Thus assume that we have computed p(h t |v 1:u ). The predictive distribution on the level of the visibles thus is p(v t |v 1:u ) = ht p(v t |h t )p(h t |v 1:u ). (S.7.6) This follows from message passing since the last node (h 4 in the graph) just copies the (normalised) message and the next factor equals p(v t |h t ). An alternative derivation follows from basic definitions and operations, together with the independencies in HMMs: (sum rule) p(v t |v 1:u ) = ht p(v t , h t |v 1:u ) (S.7.7) (product rule) = ht p(v t |h t , v 1:u )p(h t |v 1:u ) (S.7.8) (vt ⊥ ⊥ v 1:u | ht) = ht p(v t |h t )p(h t |v 1:u ) (S.7.9) Viterbi algorithm For the hidden Markov model p(h 1:t , v 1:t ) = p(v 1 |h 1 )p(h 1 ) t i=2 p(v i |h i )p(h i |h i-1 ) assume you have observations for v i , i = 1, . . . , t. Use the max-sum algorithm to derive an iterative algorithm to compute ĥ = argmax h 1 ,...,ht p(h 1:t |v 1:t ) (7.1) Assume that the latent variables h i can take K different values, e.g. h i ∈ {0, . . . , K -1}. The resulting algorithm is known as Viterbi algorithm. Solution. We first form the factors φ 1 (h 1 ) = p(v 1 |h 1 )p(h 1 ) φ 2 (h 1 , h 2 ) = p(v 2 |h 2 )p(h 2 |h 1 ) (S.7.10) . . . φ t (h t-1 , h t ) = p(v t |h t )p(h t |h t-1 ) (S.7.11) where the v i are known and fixed. The posterior p(h 1 , . . . , h t |v 1 , . . . , v t ) is then represented by the following factor graph (assuming t = 4). φ 1 h 1 φ 2 h 2 φ 3 h 3 φ 4 h 4 For the max-sum algorithm, we here choose h t to be the root. We thus initialise the algorithm with γ φ 1 →h 1 (h 1 ) = log φ 1 (h 1 ) = log p(v 1 |h 1 ) + log p(h 1 ) and then compute the messages from left to right, moving from the leaf φ 1 to the root h t . Since we are dealing with a chain, the variable nodes, much like in the sum-product algorithm, just copy the incoming messages. It thus suffices to compute the factor to variable messages shown in the graph, and then backtrack to h 1 . φ 1 h 1 φ 2 h 2 φ 3 h 3 φ 4 h 4 → → → → With γ h i-1 →φ i (h i-1 ) = γ φ i-1 →h i-1 (h i-1 ), the factor-to-variable update equation is γ φ i →h i (h i ) = max h i-1 log φ i (h i-1 , h i ) + γ h i-1 →φ i (h i-1 ) (S.7.12) = max h i-1 log φ i (h i-1 , h i ) + γ φ i-1 →h i-1 (h i-1 ) (S.7.13) To simplify notation, denote γ φ i →h i (h i ) by V i (h i ). We thus have V 1 (h 1 ) = log p(v 1 |h 1 ) + log p(h 1 ) (S.7.14) V i (h i ) = max h i-1 log φ i (h i-1 , h i ) + V i-1 (h i-1 ) i = 2, . . . , t (S.7.15) In general, V 1 (h 1 ) and V i (h i ) are functions that depend on h 1 and h i , respectively. Assuming that the h i can take on the values 0, . . . , K -1, the above equations can be written as v 1,k = log p(v 1 |k) + log p(k) k = 0, . . . , K -1 (S.7.16) v i,k = max m∈0,...,K-1 log φ i (m, k) + v i-1,m k = 0, . . . , K -1, i = 2, . . . , t, (S.7.17) At the end of the algorithm, we thus have a t × K matrix V with elements v i,k . The maximisation can be performed by computing the temporary matrix A (via broadcasting) where the (m, k)-th element is log φ i (m, k) + v i-1,m . Maximisation then corresponds to determining the maximal value in each column. To support the backtracking, when we compute V i (h i ) by maximising over h i-1 , we compute at the same time the look-up table γ * i (h i ) = argmax h i-1 log φ i (h i-1 , h i ) + V i-1 (h i-1 ) (S.7.18) When h i takes on the values 0, . . . , K -1, this can be written as γ * i,k = argmax m∈0,...,K-1 log φ i (m, k) + v i-1,m (S.7.19) This is the (row) index of the maximal element in each column of the temporary matrix A. After computing v t,k and γ * t,k , we then perform backtracking via ĥt = argmax k v t,k (S.7.20) ĥi = γ * i+1, ĥi+1 i = t -1, . . . , 1 (S.7.21) This gives recursively ĥ = ( ĥ1 , . . . , ĥt ) = argmax h 1 ,...,ht p(h 1:t |v 1:t ). Forward filtering backward sampling for hidden Markov models Consider the hidden Markov model specified by the following DAG. h 1 . . . . . . h t-1 h t . . . . . . h n v 1 v t-1 v t v n We assume that have already run the alpha-recursion (filtering) and can compute p(h t |v 1:t ) for all t. The goal is now to generate samples p(h 1 , . . . , h n |v 1:n ), i.e. entire trajectories (h 1 , . . . , h n ) from the posterior. Note that this is not the same as sampling from the n filtering distributions p(h t |v 1:t ). Moreover, compared to the Viterbi algorithm, the sampling approach generates samples from the full posterior rather than just returning the most probable state and its corresponding probability. (a) Show that p(h 1 , . . . , h n |v 1:n ) forms a first-order Markov chain. Solution. There are several ways to show this. The simplest is to notice that the undirected graph for the hidden Markov model is the same as the DAG but with the arrows removed as there are no colliders in the DAG. Moreover, conditioning corresponds to removing nodes from an undirected graph. This leaves us with a chain that connects the h i . h 1 h 2 h 3 . . . h n By graph separation, we see that p(h 1 , . . . , h n |v 1:n ) forms a first-order Markov chain so that e.g. h 1:t-1 ⊥ ⊥ h t+1:n |h t (past independent from the future given the present). (b) Since p(h 1 , . . . , h n |v 1:n ) is a first-order Markov chain, it suffices to determine p(h t-1 |h t , v 1:n ), the probability mass function for h t-1 given h t and all the data v 1:n . Use message passing to show that p(h t-1 , h t |v 1:n ) ∝ α(h t-1 )β(h t )p(h t |h t-1 )p(v t |h t ) (7.2) Solution. Since all visibles are in the conditioning set, i.e. assumed observed, we can represent the conditional model p(h 1 , . . . , h n |v 1:n ) as a chain factor tree, e.g. as follows in case of n = 4 φ 1 h 1 φ 2 h 2 φ 3 h 3 φ 4 h 4 Combining the emission distributions p(v s |h s ) (and marginal p(h 1 )) with the transition distributions p(h s |h s-1 ) we obtain the factors φ 1 (h 1 ) = p(h 1 )p(v 1 |h 1 ) (S.7.22) φ s (h s-1 , h s ) = p(h s |h s-1 )p(v s |h s ) for t = 2, . . . , n (S.7.23) We see from the factor tree that h t-1 and h t are neighbours, being attached to the same factor node φ t (h t-1 , h t ), e.g. φ 3 in case of p(h 2 , h 3 |v 1:4 ). By the rules of message passing, the joint p(h t-1 , h t |v 1:n ) is thus proportional to φ t times the messages into φ t . The following graph shows the messages for the case of p(h 2 , h 3 |v 1:4 ). φ 1 h 1 φ 2 h 2 φ 3 h 3 φ 4 h 4 → ← Since the variable nodes only receive single messages from any direction, they copy the messages so that the messages into φ t are given by α(h t-1 ) and β(h t ) shown below in red and blue, respectively. φ 1 h 1 φ 2 h 2 φ 3 h 3 φ 4 h 4 → ← Hence, p(h t-1 , h t |v 1:n ) ∝ α(h t-1 )β(h t )φ t (h t-1 , h t ) (S.7.24) ∝ α(h t-1 )β(h t )p(h t |h t-1 )p(v t |h t ) (S.7.25) which is the result that we want to show. (c) Show that p(h t-1 |h t , v 1:n ) = α(h t-1 ) α(ht) p(h t |h t-1 )p(v t |h t ). Solution. The conditional p(h t-1 |h t , v 1:n ) can be written as the ratio p(h t-1 |h t , v 1:n ) = p(h t-1 , h t |v 1:n ) p(h t |v 1:n ) . (S.7.26) Above, we have shown that the numerator satisfies p(h t-1 , h t |v 1:n ) ∝ α(h t-1 )β(h t )p(h t |h t-1 )p(v t |h t ). (S.7.27) The denominator p(h t |v 1:n ) is proportional to α(h t )β(h t ) since it is the smoothing distribution than can be determined via the alpha-beta recursion. Normally, we needed to sum the messages over all values of (h t-1 , h t ) to find the normalising constant of the numerator. For the denominator, we had to sum over all values of h t . Next, I will argue qualitatively that this summation is not needed; the normalising constants are both equal to p(v 1:t ). A more mathematical argument is given below. We started with a factor graph and factors that represent the joint p(h 1:n , v 1:n ). The conditional p(h 1:n , v 1:n ) equals p(h 1:n |v 1:n ) = p(h 1:n , v 1:n ) p(v 1:n ) (S.7.28) Message passing is variable elimination. Hence, when computing p(h t |v 1:n ) as α(h t )β(h t ) from a factor graph for p(h 1:n , v 1:n ), we only need to divide by p(v 1:n ) for normalisation; explicitly summing out h t is not needed. In other words, p(h t |v 1:n ) = α(h t )β(h t ) p(v 1:n ) . (S.7.29) Similarly, p(h t-1 , h t |v 1:n ) is also obtained from (S.7.28) by marginalisation/variable elimination. Again, when computing p(h t-1 , h t |v 1:n ) as α(h t-1 )β(h t )p(h t |h t-1 )p(v t |h t ) v 1 p(v 1 |h 1 ) v 2 p(v 2 |h 2 ) v 3 p(v 3 |h 3 ) p(h 1 ) h 1 p(h 2 |h 1 ) h 2 p(h 3 |h 2 ) h 3 This question is about computing the predictive probability p(v 3 = 1|v 1 = 1). (a) The factor graph below represents p(h 1 , h 2 , h 3 , v 2 , v 3 | v 1 = 1) . Provide an equation that defines φ A in terms of the factors in the factor graph above. v 2 p(v 2 |h 2 ) v 3 p(v 3 |h 3 ) h 1 φ A h 2 p(h 3 |h 2 ) h 3 Solution. φ A (h 1 , h 2 ) ∝ p(v 1 |h 1 )p(h 1 )p(h 2 |h 1 ) with v 1 = 1. (b) Assume further that all variables are binary, h i ∈ {0, 1}, v i ∈ {0, 1}; that p(h 1 = 1) = 0.5, and that the transition and emission distributions are, for all i, given by: p(h i+1 |h i ) h i+1 h i 0 0 0 1 1 0 1 0 1 0 1 1 p(v i |h i ) v i h i 0.6 0 0 0.4 1 0 0.4 0 1 0.6 1 1 Compute the numerical values of the factor φ A . (c) Given the definition of the transition and emission probabilities, we have φ A (h 1 , h 2 ) = 0 if h 1 = h 2 . For h 1 = 0, h 2 = 1, we obtain φ A (h 1 = 0, h 2 = 1) = p(v 1 = 1|h 1 = 0)p(h 1 = 0)p(h 2 = 1|h 1 = 0) (S.7.52) = 0.4 • 0.5 • 1 (S.7.53) = 4 10 • 1 2 (S.7.54) = 2 10 = 0.2 (S.7.55) For h 1 = 1, h 2 = 0, we obtain φ A (h 1 = 1, h 2 = 0) = p(v 1 = 1|h 1 = 1)p(h 1 = 1)p(h 2 = 0|h 1 = 1) (S.7.56) = 0.6 • 0.5 • 1 (S.7.57) = 6 10 • 1 2 (S.7.58) = 3 10 = 0.3 (S.7.59) Hence φ A (h 1 , h 2 ) h 1 h 2 0 0 0 0.3 1 0 0.2 0 1 0 1 1 (d) Denote the message from variable node h 2 to factor node p(h 3 |h 2 ) by α(h 2 ). Use message passing to compute α(h 2 ) for h 2 = 0 and h 2 = 1. Report the values of any intermediate messages that need to be computed for the computation of α(h 2 ). Solution. The message from h 1 to φ A is one. The message from φ A to h 2 is µ φ A →h 2 (h 2 = 0) = h 1 φ A (h 1 , h 2 = 0) (S.7.60) = 0.3 (S.7.61) µ φ A →h 2 (h 2 = 1) = h 1 φ A (h 1 , h 2 = 1) (S.7.62) = 0.2 (S.7.63) Since v 2 is not observed and p(v 2 |h 2 ) normalised, the message from p(v 2 |h 2 ) to h 2 equals one. This means that the message from h 2 to p(h 3 |h 2 ), which is α(h 2 ) equals µ φ A →h 2 (h 2 ), i.e. α(h 2 = 0) = 0.3 (S.7.64) α(h 2 = 1) = 0.2 (S.7.65) (e) With α(h 2 ) defined as above, use message passing to show that the predictive probability p(v 3 = 1|v 1 = 1) can be expressed in terms of α(h 2 ) as p(v 3 = 1|v 1 = 1) = xα(h 2 = 1) + yα(h 2 = 0) α(h 2 = 1) + α(h 2 = 0) (7.4) and report the values of x and y. Solution. Given the definition of p(h 3 |h 2 ), the message µ p(h 3 |h 2 )→h 3 (h 3 ) is µ p(h 3 |h 2 )→h 3 (h 3 = 0) = α(h 2 = 1) (S.7.66) µ p(h 3 |h 2 )→h 3 (h 3 = 1) = α(h 2 = 0) (S.7.67) The variable node h 3 copies the message so that we have µ p(v 3 |h 3 )→v 3 (v 3 = 0) = h 3 p(v 3 = 0|h 3 )µ p(h 3 |h 2 )→h 3 (h 3 ) (S.7.68) = p(v 3 = 0|h 3 = 0)α(h 2 = 1) + p(v 3 = 0|h 3 = 1)α(h 2 = 0) (S.7.69) = 0.6α(h 2 = 1) + 0.4α(h 2 = 0) (S.7.70) µ p(v 3 |h h 3)→v 3 (v 3 = 1) = h 3 p(v 3 = 1|h 3 ))µ p(h 3 |h 2 )→h 3 (h 3 ) (S.7.71) = p(v 3 = 1|h 3 = 0)α(h 2 = 1) + p(v 3 = 1|h 3 = 1)α(h 2 = 0) (S.7.72) = 0.4α(h 2 = 1) + 0.6α(h 2 = 0) (S.7.73) We thus have p(v 3 = 1|v 1 = 1) = 0.4α(h 2 = 1) + 0.6α(h 2 = 0) 0.4α(h 2 = 1) + 0.6α(h 2 = 0) + 0.6α(h 2 = 1) + 0.4α(h 2 = 0) (S.7.74) = 0.4α(h 2 = 1) + 0.6α(h 2 = 0) α(h 2 = 1) + α(h 2 = 0) (S.7.75) The requested x and y are thus: x = 0.4, y = 0.6. (f) Compute the numerical value of p(v 3 = 1|v 1 = 1). Solution. Inserting the numbers gives α(h 2 = 0) + α(h 2 = 1) = 5/10 = 1/2 so that p(v 3 = 1|v 1 = 1) = 0.4 • 0.2 + 0.6 • 0.3 1 2 (S.7.76) = 2 • 4 10 • 2 10 + 6 10 3 10 (S.7.77) = 4 10 • 4 10 + 6 10 6 10 (S.7.78) = 1 100 (16 + 36) (S.7.79) = 1 100 52 (S.7.80) = 52 100 = 0.52 (S.7.81) Integrating out x 2 , . . . , x n gives p 1 (x 1 ) = p 1 (x 1 , . . . , x n )dx 2 . . . dx n (S.7.91) ∝ p(x 1 ) n i=2 p(x i |x i-1 )g 1 (x 1 )dx 2 . . . dx n (S.7.92) ∝ p(x 1 )g 1 (x 1 ) n i=2 p(x i |x i-1 )dx 2 . . . dx n (S.7.93) ∝ p(x 1 )g 1 (x 1 ) n i=2 p(x i |x i-1 )dx i =1 (S.7.94) ∝ p(x 1 )g 1 (x 1 ) (S.7.95) The normalising constant is Z 1 = p(x 1 )g 1 (x 1 )dx 1 (S.7.96) This establishes the result for t = 1. From (7.8), we further have p t-1 (x 1 , . . . , x n ) = p(x 1 , . . . , x n |y 1 , . . . , y t-1 ) (S.7.97) ∝ p(x 1 ) n i=2 p(x i |x i-1 ) t-1 i=1 g i (x i ) (S.7.98) Integrating out x t+1 , . . . , x n thus gives p t-1 (x 1 , . . . , x t ) = p t-1 (x 1 , . . . , x n )dx t+1 . . . dx n (S.7.99) ∝ p(x 1 ) n i=2 p(x i |x i-1 ) t-1 i=1 g i (x i )dx t+1 . . . dx n (S.7.100) ∝ p(x 1 ) t i=2 p(x i |x i-1 ) t-1 i=1 g i (x i ) n i=t+1 p(x i |x i-1 )dx t+1 . . . dx n (S.7.101) ∝ p(x 1 ) t i=2 p(x i |x i-1 ) t-1 i=1 g i (x i ) n i=t+1 p(x i |x i-1 )dx i (S.7.102) ∝ p(x 1 ) t i=2 p(x i |x i-1 ) t-1 i=1 g i (x i ) (S.7.103) Noting that the product over the g i does not involve x t and that p(x t |x t-1 ) is a pdf, we have further p t-1 (x 1 , . . . , x t-1 ) = p t-1 (x 1 , . . . , x t )dx t (S.7.104) ∝ p(x 1 ) t-1 i=2 p(x i |x i-1 ) t-1 i=1 g i (x i ) (S.7.105) Solution. Let t > 1. With (7.9), we have p t-1 (x t-1 , x t ) = p t-1 (x 1 , . . . , x t )dx 1 . . . dx t-2 (S.7.112) = p t-1 (x 1 , . . . , x t-1 )p(x t |x t-1 )dx 1 . . . dx t-2 (S.7.113) = p(x t |x t-1 ) p t-1 (x 1 , . . . , x t-1 )dx 1 . . . dx t-2 (S.7.114) = p(x t |x t-1 )p t-1 (x t-1 ) (S.7.115) which proves the \"extension\". With (7.10), we have p t (x t ) = p t (x 1 , . . . , x t )dx 1 , . . . dx t-1 (S.7.116) = 1 Z t p t-1 (x 1 , . . . , x t )g t (x t )dx 1 , . . . dx t-1 (S.7.117) = 1 Z t g t (x t ) p t-1 (x 1 , . . . , x t )dx 1 , . . . dx t-1 (S.7.118) = 1 Z t g t (x t )p t-1 (x t ) (S.7.119) which proves the \"change of measure\". Moreover, the normalising constant Z t is the same as before. Hence completing the iteration until t = n yields the likelihood p(y 1 , . . . , y n ) = Z n as a by-product of the recursion. The initialisation of the recursion with p 0 (x 1 ) = p(x 1 ) is also the same as above. Kalman filtering We here consider filtering for hidden Markov models with Gaussian transition and emission distributions. For simplicity, we assume one-dimensional hidden variables and observables. We denote the probability density function of a Gaussian random variable x with mean µ and variance σ 2 by N (x|µ, σ 2 ), N (x|µ, σ 2 ) = 1 √ 2πσ 2 exp - (x -µ) 2 2σ 2 . (7.16) The transition and emission distributions are assumed to be p(h s |h s-1 ) = N (h s |A s h s-1 , B 2 s ) (7.17) p(v s |h s ) = N (v s |C s h s , D 2 s ). (7.18) The distribution p(h 1 ) is assumed Gaussian with known parameters. The A s , B s , C s , D s are also assumed known. (a) Show that h s and v s as defined in the following update and observation equations h s = A s h s-1 + B s ξ s (7.19) v s = C s h s + D s η s (7.20) follow the conditional distributions in (7.17) and (7.18). The random variables ξ s and η s are independent from the other variables in the model and follow a standard normal Gaussian distribution, e.g. ξ s ∼ N (ξ s |0, 1). Hint: For two constants c 1 and c 2 , y = c 1 + c 2 x is Gaussian if x is Gaussian. In other words, an affine transformation of a Gaussian is Gaussian. The equations mean that h s is obtained by scaling h s-1 and by adding noise with variance B 2 s . The observed value v s is obtained by scaling the hidden h s and by corrupting it with Gaussian observation noise of variance D 2 s . Solution. By assumption, ξ s is Gaussian. Since we condition on h s-1 , A s h s-1 in (7.19) is a constant, and since B s is a constant too, h s is Gaussian. What we have to show next is that (7.19) defines the same conditional mean and variance as the conditional Gaussian in (7.17): The conditional expectation of h s given h s-1 is E(h s |h s-1 ) = A s h s-1 + E(B s ξ s ) (since we condition on h s-1 ) (S.7.120) = A s h s-1 + B s E(ξ s ) (by linearity of expectation) (S.7.121) = A s h s-1 (since ξ s has zero mean) (S.7.122) The conditional variance of h s given h s-1 is V(h s |h s-1 ) = V(B s ξ s ) (since we condition on h s-1 ) (S.7.123) = B 2 s V(ξ s ) (by properties of the variance) (S.7.124) = B 2 s (since ξ s has variance one) (S.7.125) We see that the conditional mean and variance of h s given h s-1 match those in (7.17). And since h s given h s-1 is Gaussian as argued above, the result follows. Exactly the same reasoning also applies to the case of (7.20). Conditional on h s , v s is Gaussian because it is an affine transformation of a Gaussian. The conditional mean of v s given h s is: E(v s |h s ) = C s h s + E(D s η s ) (since we condition on h s ) (S.7.126) = C s h s + D s E(η s ) (by linearity of expectation) (S.7.127) = C s h s (since η s has zero mean) (S.7.128) The conditional variance of v s given h s is V(v s |h s ) = V(D s η s ) (since we condition on h s ) (S.7.129) = D 2 s V(η s ) (by properties of the variance) (S.7.130) = D 2 s (since η s has variance one) (S.7.131) Hence, conditional on h s , v s is Gaussian with mean and variance as in (7.18). (b) Show that N (x|µ, σ 2 )N (y|Ax, B 2 )dx ∝ N (y|Aµ, A 2 σ 2 + B 2 ) (7.21) Hint: While this result can be obtained by integration, an approach that avoids this is as follows: First note that N (x|µ, σ 2 )N (y|Ax, B 2 ) is proportional to the joint pdf of x and y. We can thus consider the integral to correspond to the computation of the marginal of y from the joint. Using the equivalence of Equations (7.17)-(7.18) and (7.19)-(7.20), and the fact that the weighted sum of two Gaussian random variables is a Gaussian random variable then allows one to obtain the result. Solution. We follow the procedure outlined above. The two Gaussian densities correspond to the equations x = µ + σξ (S. 7.132) y = Ax + Bη (S.7.133) where ξ and η are independent standard normal random variables. The mean of y is E(y) = AE(x) + BE(η) (S.7.134) = Aµ (S.7.135) where we have use the linearity of expectation and E(η) = 0. The variance of y is V(y) = V(Ax) + V(Bη) (since x and η are independent) (S.7.136) = A 2 V(x) + B 2 V(η) (by properties of the variance) (S.7.137) = A 2 σ 2 + B 2 (S.7.138) Since y is the (weighted) sum of two Gaussians, it is Gaussian itself, and hence its distribution is completely defined by its mean and variance, so that y ∼ N (y|Aµ, A 2 σ 2 + B 2 ). (S.7.139) Now, the product N (x|µ, σ 2 )N (y|Ax, B 2 ) is proportional to the joint pdf of x and y, so that the integral can be considered to correspond to the marginalisation of x, and hence its result is proportional to the density of y, which is N (y|Aµ, A 2 σ 2 + B 2 ). (c) Show that N (x|m 1 , σ 2 1 )N (x|m 2 , σ 2 2 ) ∝ N (x|m 3 , σ 2 3 ) (7.22) where σ 2 3 = 1 σ 2 1 + 1 σ 2 2 -1 = σ 2 1 σ 2 2 σ 2 1 + σ 2 2 (7.23) m 3 = σ 2 3 m 1 σ 2 1 + m 2 σ 2 2 = m 1 + σ 2 1 σ 2 1 + σ 2 2 (m 2 -m 1 ) (7.24) Hint: Work in the negative log domain. Solution. We show the result using a classical technique called \"completing the square\", see e.g. https://en.wikipedia.org/wiki/Completing_the_square . We work in the (negative) log-domain and use that -log N (x|m, σ 2 ) = (x -m) 2 2σ 2 + const (S.7.140) = x 2 2σ 2 -x m σ 2 + m 2 2σ 2 + const (S.7.141) = x 2 2σ 2 -x m σ 2 + const (S.7.142) where const indicates terms not depending on x. We thus obtain -log N (x|m 1 , σ 2 1 )N (x|m 2 , σ 2 2 ) = -log N (x|m 1 , σ 2 1 ) -log N (x|m 2 , σ 2 2 ) (S.7.143) = (x -m 1 ) 2 2σ 2 1 + (x -m 2 ) 2 2σ 2 2 + const (S.7.144) = x 2 2σ 2 1 -x m 1 σ 2 1 + x 2 2σ 2 2 -x m 2 σ 2 2 + const (S.7.145) = x 2 2 1 σ 2 1 + 1 σ 2 2 -x m 1 σ 2 1 + m 2 σ 2 2 + const (S.7.146) = x 2 2σ 2 3 - x σ 2 3 σ 2 3 m 1 σ 2 1 + m 2 σ 2 2 + const, (S.7.147) where 1 σ 2 3 = 1 σ 2 1 + 1 σ 2 2 . (S.7.148) Comparison with (S.7.142) shows that we can further write x 2 2σ 2 3 - x σ 2 3 σ 2 3 m 1 σ 2 1 + m 2 σ 2 2 = (x -m 3 ) 2 2σ 2 3 + const (S.7.149) where m 3 = σ 2 3 m 1 σ 2 1 + m 2 σ 2 2 (S.7.150) so that -log N (x|m 1 , σ 2 1 )N (x|m 2 , σ 2 2 ) = (x -m 3 ) 2 2σ 2 3 + const (S.7.151) and hence N (x|m 1 , σ 2 1 )N (x|m 2 , σ 2 2 ) ∝ N (x|m 3 , σ 2 3 ). (S.7.152) Note that the identity m 3 = σ 2 3 m 1 σ 2 1 + m 2 σ 2 2 = m 1 + σ 2 1 σ 2 1 + σ 2 2 (m 2 -m 1 ) (S.7.153) is obtained as follows σ 2 3 m 1 σ 2 1 + m 2 σ 2 2 = σ 2 1 σ 2 2 σ 2 1 + σ 2 2 m 1 σ 2 1 + m 2 σ 2 2 (S.7.154) = m 1 σ 2 2 σ 2 1 + σ 2 2 + m 2 σ 2 1 σ 2 1 + σ 2 2 (S.7.155) = m 1 1 - σ 2 1 σ 2 1 + σ 2 2 + m 2 σ 2 1 σ 2 1 + σ 2 2 (S.7.156) = m 1 + σ 2 1 σ 2 1 + σ 2 2 (m 2 -m 1 ) (S.7.157) (d) We can use the \"alpha-recursion\" to recursively compute p(h t |v 1:t ) ∝ α(h t ) as follows. α(h 1 ) = p(h 1 ) • p(v 1 |h 1 ) α(h s ) = p(v s |h s ) h s-1 p(h s |h s-1 )α(h s-1 ). (7.25) For continuous random variables, the sum above becomes an integral so that α(h s ) = p(v s |h s ) p(h s |h s-1 )α(h s-1 )dh s-1 . (7.26) For reference, let us denote the integral by I(h s ), I(h s ) = p(h s |h s-1 )α(h s-1 )dh s-1 . (7.27) Note that I(h s ) is proportional to the predictive distribution p(h s |v 1:s-1 ). For a Gaussian prior distribution for h 1 and Gaussian emission probability p(v 1 |h 1 ), α(h 1 ) = p(h 1 ) • p(v 1 |h 1 ) ∝ p(h 1 |v 1 ) is proportional to a Gaussian. We denote its mean by µ 1 and its variance by σ 2 1 so that α(h 1 ) ∝ N (h 1 |µ 1 , σ 2 1 ). (7.28) Assuming α(h s-1 ) ∝ N (h s-1 |µ s-1 , σ 2 s-1 ) (which holds for s = 2), use Equation (7.21) to show that I(h s ) ∝ N (h s |A s µ s-1 , P s ) (7.29) where P s = A 2 s σ 2 s-1 + B 2 s . (7.30) Solution. We can set α(h s-1 ) ∝ N (h s-1 |µ s-1 , σ 2 s-1 ). Since p(h s |h s-1 ) is Gaussian, see Equation (7.17), Equation (7.27) becomes I(h s ) ∝ N (h s |A s h s-1 , B 2 s )N (h s-1 |µ s-1 , σ 2 s-1 )dh s-1 . (S.7.158) Equation (7.21) with x ≡ h s-1 and y ≡ h s yields the desired result, I(h s ) ∝ N (h s |A s µ s-1 , A 2 s σ 2 s-1 + B 2 s ). (S.7.159) We can understand the equation as follows: To compute the predictive mean of h s given v 1:s-1 , we forward propagate the mean of h s-1 |v 1:s-1 using the update equation (7.19). This gives the mean term A s µ s-1 . Since h s-1 |v 1:s-1 has variance σ 2 s-1 , the variance of h s |v 1:s-1 is given by A 2 s σ 2 s-1 plus an additional term, B 2 s , due to the noise in the forward propagation. This gives the variance term A 2 s σ 2 s-1 + B 2 s . (e) Use Equation (7.22) to show that α(h s ) ∝ N h s |µ s , σ 2 s (7.31) where µ s = A s µ s-1 + P s C s C 2 s P s + D 2 s (v s -C s A s µ s-1 ) (7.32) σ 2 s = P s D 2 s P s C 2 s + D 2 s (7.33) Solution. Having computed I(h s ), the final step in the alpha-recursion is α(h s ) = p(v s |h s )I(h s ) (S.7.160) With Equation (7.18) we obtain α(h s ) ∝ N (v s |C s h s , D 2 s )N (h s |A s µ s-1 , P s ). (S.7.161) We further note that N (v s |C s h s , D 2 s ) ∝ N h s |C -1 s v s , D 2 s C 2 s (S.7.162) so that we can apply Equation (7.22) (with m 1 = Aµ s-1 , σ 2 1 = P s ) α(h s ) ∝ N h s |C -1 s v s , D 2 s C 2 s N (h s |A s µ s-1 , P s ) (S.7.163) ∝ N h s , µ s , σ 2 s (S.7.164) with µ s = A s µ s-1 + P s P s + D 2 s C 2 s C -1 s v s -A s µ s-1 (S.7.165) = A s µ s-1 + P s C 2 s C 2 s P s + D 2 s C -1 s v s -A s µ s-1 (S.7.166) = A s µ s-1 + P s C s C 2 s P s + D 2 s (v s -C s A s µ s- 1 ) (S.7.167) σ 2 s = P s D 2 s C 2 s P s + D 2 s C 2 s (S.7.168) = P s D 2 s P s C 2 s + D 2 s (S.7.169) (S.7.170) (f) Show that α(h s ) can be re-written as α(h s ) ∝ N h s |µ s , σ 2 s (7.34) where µ s = A s µ s-1 + K s (v s -C s A s µ s-1 ) (7.35) σ 2 s = (1 -K s C s )P s (7.36) K s = P s C s C 2 s P s + D 2 s (7.37) These are the Kalman filter equations and K s is called the Kalman filter gain. Solution. We start from µ s = A s µ s-1 + P s C s C 2 s P s + D 2 s (v s -C s A s µ s-1 ) , (S.7.171) and see that P s C s C 2 s P s + D 2 s = K s (S.7.172) so that µ s = A s µ s-1 + K s (v s -C s A s µ s-1 ) . (S.7.173) For the variance σ 2 s , we have σ 2 s = P s D 2 s P s C 2 s + D 2 s (S.7.174) = D 2 s P s C 2 s + D 2 s P s (S.7.175) = 1 -P s C 2 s P s C 2 s + D 2 s P s (S.7.176) = (1 -K s C s )P s , (S.7.177) which is the desired result. The filtering result generalises to vector valued latents and visibles where the transition and emission distributions in (7.17) and (7.18) become p(h s |h s-1 ) = N (h s |Ah s-1 , Σ Σ Σ h ), (S.7.178) p(v s |h s ) = N (v s |C s h s , Σ Σ Σ v ), (S.7.179) where N () denotes multivariate Gaussian pdfs, e.g. N (v s |C s h s , Σ Σ Σ v ) = 1 | det(2πΣ Σ Σ v )| 1/2 exp - 1 2 (v s -C s h s ) (Σ Σ Σ v ) -1 (v s -C s h s ) . (S.7.180) We then have p(h t |v 1:t ) = N (h t |µ µ µ t , Σ Σ Σ t ) (S.7.181) where the posterior mean and variance are recursively computed as µ µ µ s = A s µ µ µ s-1 + K s (v s -C s A s µ µ µ s-1 ) (S.7.182) Σ Σ Σ s = (I -K s C s )P s (S.7.183) P s = A s Σ Σ Σ s-1 A s + Σ Σ Σ h (S.7.184) K s = P s C s C s P s C s + Σ Σ Σ v -1 (S.7.185) and initialised with µ µ µ 1 and Σ Σ Σ 1 equal to the mean and variance of p(h 1 |v 1 ). The matrix K s is then called the Kalman gain matrix. The Kalman filter is widely applicable, see e.g. https://en.wikipedia.org/wiki/  Kalman_filter , and has played a role in historic events such as the moon landing, see e.g. (Grewal and Andrews, 2010). An example of the application of the Kalman filter to tracking is shown in Figure 7 .1. (g) Explain Equation (7.35) in non-technical terms. What happens if the variance D 2 s of the observation noise goes to zero? Solution. We have already seen that A s µ s-1 is the predictive mean of h s given v 1:s-1 . The term C s A s µ s-1 is thus the predictive mean of v s given the observations so far, v 1:s-1 . The difference v s -C s A s µ s-1 is thus the prediction error of the observable. Since α(h s ) is proportional to p(h s |v 1:s ) and µ s its mean, we thus see that the posterior mean of h s |v 1:s equals the posterior mean of h s |v 1:s-1 , A s µ s-1 , updated by the prediction error of the observable weighted by the Kalman gain. For D 2 s → 0, K s → C -1 s and µ s = A s µ s-1 + K s (v s -C s A s µ s-1 ) (S.7.186) = A s µ s-1 + C -1 s (v s -C s A s µ s-1 ) (S.7.187) = A s µ s-1 + C -1 s v s -A s µ s-1 (S.7.188) = C -1 s v s , ( S.7.189) so that the posterior mean of p(h s |v 1:s ) is obtained by inverting the observation equation. Moreover, the variance σ 2 s of h s |v 1:s goes to zero so that the value of h s is known precisely and equals C -1 s v s . The posterior is p(µ|D) ∝ L(θ)p(µ; µ 0 , σ 2 0 ) (S.8.25) ∝ N (µ; x, σ 2 /n)N (µ; µ 0 , σ 2 0 ) (S.8.26) so that with (8.4), we have p(µ|D) ∝ N (µ; µ n , σ 2 n ) (S.8.27) σ 2 n = 1 σ 2 /n + 1 σ 2 0 -1 (S.8.28) = σ 2 0 σ 2 /n σ 2 0 + σ 2 /n (S.8.29) µ n = σ 2 n x σ 2 /n + µ 0 σ 2 0 (S.8.30) = 1 σ 2 0 + σ 2 /n σ 2 0 x + (σ 2 /n)µ 0 (S.8.31) = σ 2 0 σ 2 0 + σ 2 /n x + σ 2 /n σ 2 0 + σ 2 /n µ 0 . (S.8.32) As n increases, σ 2 /n goes to zero so that σ 2 n → 0 and µ n → x. This means that with an increasing amount of data, the posterior of the mean tends to be concentrated around the maximum likelihood estimate x. From (8.7), we also have that µ n = µ 0 + σ 2 0 σ 2 /n + σ 2 0 (x -µ 0 ), (S.8.33) which shows more clearly that the value of µ n lies on a line with end-points µ 0 (for n = 0) and x (for n → ∞). As the amount of data increases, µ n moves form the mean under the prior, µ 0 , to the average of the observed sample, that is the MLE x. Maximum likelihood estimation of probability tables in fully observed directed graphical models of binary variables We assume that we are given a parametrised directed graphical model for variables x 1 , . . . , x d , p(x; θ) = d i=1 p(x i |pa i ; θ i ) x i ∈ {0, 1} (8.8) where the conditionals are represented by parametrised probability tables, For example, if pa 3 = {x 1 , x 2 }, p(x 3 |pa 3 ; θ 3 ) is represented as p(x 3 = 1|x 1 , x 2 ; θ 1 3 , . . . , θ 4 3 )) x 1 x 2 θ 1 3 0 0 θ 2 3 1 0 θ 3 3 0 1 θ 4 3 1 1 with θ 3 = (θ 1 3 , θ 2 3 , θ 3 3 , θ 4 3 ), and where the superscripts j of θ j 3 enumerate the different states that the parents can be in. (a) Assuming that x i has m i parents, verify that the table parametrisation of p(x i |pa i ; θ i ) is equivalent to writing p(x i |pa i ; θ i ) as p(x i |pa i ; θ i ) = S i s=1 (θ s i ) 1(x i =1,pa i =s) (1 -θ s i ) 1(x i =0,pa i =s) (8.9) where S i = 2 m i is the total number of states/configurations that the parents can be in, and 1(x i = 1, pa i = s) is one if x i = 1 and pa i = s, and zero otherwise. Solution. The number of configurations that m binary parents can be in is given by S i . The questions thus boils down to showing that p( x i = 1|pa i = k; θ i ) = θ k i for any state k ∈ {1, . . . , S i } of the parents of x i . Since 1(x i = 1, pa i = s) = 0 unless s = k, we have indeed that p(x i = 1|pa i = k; θ i ) =   s =k (θ s i ) 0 (1 -θ s i ) 0   (θ k i ) 1(x i =1,pa i =k) (1 -θ k i ) 1(x i =0,pa i =k) (S.8.34) = 1 • (θ k i ) 1(x i =1,pa i =k) (1 -θ k i ) 0 (S.8.35) = θ k i . (S.8.36) (b) For iid data D = {x (1) , . . . , x (n) } show that the likelihood can be represented as p(D; θ) = d i=1 S i s=1 (θ s i ) n s x i =1 (1 -θ s i ) n s x i =0 (8.10) where n s x i =1 is the number of times the pattern (x i = 1, pa i = s) occurs in the data D, and equivalently for n s x i =0 . Solution. Since the data are iid, we have p(D; θ) = n j=1 p(x (j) ; θ) (S. 8.37) (S.8.38) where each term p(x (j) ; θ) factorises as in (8.8), p(x (j) ; θ) = d i=1 p(x (j) i |pa (j) i ; θ i ) (S.8.39) with x (j) i denoting the i-th element of x (j) and pa (j) i the corresponding parents. The conditionals p(x (j) i |pa (j) i ; θ i ) factorise further according to (8.9), p(x (j) i |pa (j) i ; θ i ) = S i s=1 (θ s i ) 1(x (j) i =1,pa (j) i =s) (1 -θ s i ) 1(x (j) i =0,pa (j) i =s) , (S.8.40) so that p(D; θ) = n j=1 d i=1 p(x (j) i |pa (j) i ; θ i ) (S.8.41) = n j=1 d i=1 S i s=1 (θ s i ) 1(x (j) i =1,pa (j) i =s) (1 -θ s i ) 1(x (j) i =0,pa (j) i =s) (S.8.42) Swapping the order of the products so that the product over the data points comes first, we obtain p(D; θ) = d i=1 S i s=1 n j=1 (θ s i ) 1(x (j) i =1,pa (j) i =s) (1 -θ s i ) 1(x (j) i =0,pa (j) i =s) (S.8.43) We next split the product over j into two products, one for all j where x (j) i = 1, and one for all j where x (j) i = 0 p(D; θ) = d i=1 S i s=1 j: x (j) i =1 j: x (j) i =0 (θ s i ) 1(x (j) i =1,pa (j) i =s) (1 -θ s i ) 1(x (j) i =0,pa (j) i =s) (S.8.44) = d i=1 S i s=1 j: x (j) i =1 (θ s i ) 1(x (j) i =1,pa (j) i =s) j: x (j) i =0 (1 -θ s i ) 1(x (j) i =0,pa (j) i =s) (S.8.45) = d i=1 S i s=1 (θ s i ) n j=1 1(x (j) i =1,pa (j) i =s) (1 -θ s i ) n j=1 1(x (j) i =0,pa (j) i =s) (S.8.46) = d i=1 S i s=1 (θ s i ) n s x i =1 (1 -θ s i ) n s x i =0 (S.8.47) where n s x i =1 = n j=1 1(x (j) i = 1, pa (j) i = s) n s x i =0 = n j=1 1(x (j) i = 0, pa (j) i = s) (S.8.48) is the number of times x i = 1 and x i = 0, respectively, with its parents being in state s. (c) Show that the log-likelihood decomposes into sums of terms that can be independently optimised, and that each term corresponds to the log-likelihood for a Bernoulli model. Solution. The log-likelihood ℓ(θ) equals ℓ(θ) = log p(D; θ) (S.8.49) = log d i=1 S i s=1 (θ s i ) n s x i =1 (1 -θ s i ) n s x i =0 (S.8.50) = d i=1 S i s=1 log (θ s i ) n s x i =1 (1 -θ s i ) n s x i =0 (S.8.51) = d i=1 S i s=1 n s x i =1 log(θ s i ) + n s x i =0 log(1 -θ s i ) (S.8.52) Since the parameters θ s i are not coupled in any way, maximising ℓ(θ) can be achieved by maximising each term ℓ is (θ s i ) individually, ℓ is (θ s i ) = n s x i =1 log(θ s i ) + n s x i =0 log(1 -θ s i ). (S.8.53) Moreover, ℓ is (θ s i ) corresponds to the log-likelihood for a Bernoulli model with success probability θ s i and data with n s x i =1 number of ones and n s x i =0 number of zeros. (d) Determine the maximum likelihood estimate θ for the Bernoulli model p(x; θ) = θ x (1 -θ) 1-x , θ ∈ [0, 1], x ∈ {0, 1} (8.11) and iid data x 1 , . . . , x n . Solution. The log-likelihood function is ℓ(θ) = n i=1 log p(x i ; θ) (S.8.54) = n i=1 x i log(θ) + 1 -x i log(1 -θ). (S.8.55) Since log(θ) and log(1 -θ) do not depend on i, we can pull them outside the sum and the log-likelihood function can be written as There are multiple ways to solve the problem. One option is to determine the unconstrained optimiser and then check whether it satisfies the constraint. The first derivative equals ℓ(θ) = n x=1 log(θ) + n x=0 log(1 -θ) (S. ℓ (θ) = n x=1 θ - n x=0 1 -θ (S.8.58) and the second derivative is ℓ (θ) = - n x=1 θ 2 - n x=0 (1 -θ) 2 (S.8.59) The reason for this is as follows: Let J(η) = ℓ(g -1 (η)) be the log-likelihood seen as a function of η. Since g and g -1 are invertible, we have that max θ∈[0,1] ℓ(θ) = max η J(η) (S.8.70) argmax θ∈[0,1] ℓ(θ) = g -1 argmax η J(η) . (S.8.71) (e) Returning to the fully observed directed graphical model, conclude that the maximum likelihood estimates are given by θs i = n s x i =1 n s x i =1 + n s x i =0 = n j=1 1(x (j) i = 1, pa (j) i = s) n j=1 1(pa (j) i = s) (8.12) Solution. Given the result from question (c), we can optimise each term ℓ is (θ s i ) separately. Each term formally corresponds to a log-likelihood for a Bernoulli model, so that we can use the results from question (d) to obtain θs i = n s x i =1 n s x i =1 + n s x i =0 . (S.8.72) Since n s x i =1 = n j=1 1(x (j) i = 1, pa (j) i = s) and n s x i =1 + n s x i =0 = n j=1 1(x (j) i = 1, pa (j) i = s) + n j=1 1(x (j) i = 0, pa (j) i = s) (S.8.73) = n j=1 1(pa (j) i = s), (S.8.74) we further have θs i = n j=1 1(x (j) i = 1, pa (j) i = s) n j=1 1(pa (j) i = s) . (S.8.75) Hence, to determine θs i , we first count the number of times the parents of x i are in state s, which gives the denominator, and then among them, count the number of times x i = 1, which gives the numerator. Cancer-asbestos-smoking example: MLE Consider the model specified by the DAG a s c The distribution of a and s are Bernoulli distributions with parameter (success probability) θ a and θ s , respectively, i.e. p(a; θ a ) = θ a a (1 -θ a ) 1-a p(s; θ s ) = θ s s (1 -θ s ) 1-s , (8.13) and the distribution of c given the parents is parametrised as specified in the following table p(c = 1|a, s; θ 1 c , . . . , θ 4 c )) a s θ 1 c 0 0 θ 2 c 1 0 θ 3 c 0 1 θ 4 c 1 1 The free parameters of the model are (θ a , θ s , θ 1 c , . . . , θ 4 c ). Assume we observe the following iid data (each row is a data point). a s c 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 (a) Determine the maximum-likelihood estimates of θ a and θ s Solution. The maximum likelihood estimate (MLE) θa is given by the fraction of times that a is 1 in the data set. Hence θa = 1/5. Similarly, the MLE θs is 2/5. (b) Determine the maximum-likelihood estimates of θ 1 c , . . . , θ 4 c . Solution. With (S.8.75), we have p(c = 1|a, s) a s θ1 c = 0 0 0 θ2 c = 1/1 1 0 θ3 c = 1/2 0 1 θ4 c not defined 1 1 This because, for example, we have two observations where (a, s) = (0, 0), and among them, c = 1 never occurs, so that the MLE for p(c = 1|a, s) is zero. This example illustrates some issues with maximum likelihood estimates: We may get extreme probabilities, zero or one, or if the parent configuration does not occur in the observed data, the estimate is undefined. where B(α, β) denotes the Beta function and where the Gamma function Γ(t) is defined as Γ(t) = ∞ o f t-1 exp(-f )df (8.17) and satisfies Γ(t + 1) = tΓ(t). Hint: It will be useful to represent the partition function in terms of the Beta function. Solution. We first write the partition function of p(f ; α, β) in terms of the Beta function Z(α, β) = 1 0 f α-1 (1 -f ) β-1 (S.8.84) = B(α, β). (S.8.85) We then have that the mean E[f ] is given by E[f ] = 1 0 f p(f ; α, β)df (S.8.86) = 1 B(α, β) 1 0 f f α-1 (1 -f ) β-1 df (S.8.87) = 1 B(α, β) 1 0 f α+1-1 (1 -f ) β-1 df (S.8.88) = B(α + 1, β) B(α, β) (S.8.89) = Γ(α + 1)Γ(β) Γ(α + 1 + β) Γ(α + β) Γ(α)Γ(β) (S.8.90) = αΓ(α)Γ(β) (α + β)Γ(α + β) Γ(α + β) Γ(α)Γ(β) (S.8.91) = α α + β (S.8.92) where we have used the definition of the Beta function in terms of the Gamma function and the property Γ(t + 1) = tΓ(t). (c) Show that the predictive posterior probability p(x = 1|D) for a new independently observed data point x equals the posterior mean of p(θ|D), which in turn is given by E(θ|D) = α 0 + n x=1 α 0 + β 0 + n . ( 8 .18) Solution. We obtain p(x = 1|D) = 1 0 p(x = 1, θ|D)dθ (sum rule) (S.8.93) = 1 0 p(x = 1|θ, D)p(θ|D)dθ (product rule) (S.8.94) = 1 0 p(x = 1|θ)p(θ|D)dθ (x ⊥ ⊥ D|θ) (S.8.95) = 1 0 θp(θ|D)dθ (S. 8.96) = E[θ|D] (S.8.97) From the previous question we know the mean of a Beta random variable. Since θ ∼ B(θ; α n , β n ), we obtain p(x = 1|D) = E[θ|D] (S.8.98) = α n α n + β n (S.8.99) = α 0 + n x=1 α 0 + n x=1 + β 0 + n x=0 (S.8.100) = α 0 + n x=1 α 0 + β 0 + n (S.8.101) where the last equation follows from the fact that n = n x=0 + n x=1 . Note that for n → ∞, the posterior mean tends to the MLE n x=1 /n. 8.6 Bayesian inference of probability tables in fully observed directed graphical models of binary variables This is the Bayesian analogue of Exercise 8.3 and the notation follows that exercise. We consider the Bayesian model p (x|θ) = d i=1 p(x i |pa i , θ i ) x i ∈ {0, 1} (8.19) p(θ; α 0 , β 0 ) = d i=1 S i s=1 B(θ s i ; α s i,0 , β s i,0 ) (8.20) where p(x i |pa i , θ i ) is defined via (8.9), α 0 is a vector of hyperparameters containing all α s i,0 , β 0 the vector containing all β s i,0 , and as before B denotes the Beta distribution. Under the prior, all parameters are independent. (a) For iid data D = {x (1) , . . . , x (n) } show that p(θ|D) = d i=1 S i s=1 B(θ s i , α s i,n , β s i,n ) (8.21) where 8.22) and that the parameters are also independent under the posterior. α s i,n = α s i,0 + n s x i =1 β s i,n = β s i,0 + n s x i =0 ( Solution. We start with p(θ|D) ∝ p(D|θ)p(θ; α 0 , β 0 ). (S.8.102) Inserting the expression for p(D|θ) given in (8.10) and the assumed form of the prior gives p(θ|D) ∝ d i=1 S i s=1 (θ s i ) n s x i =1 (1 -θ s i ) n s x i =0 d i=1 S i s=1 B(θ s i ; α s i,0 , β s i,0 ) (S.8.103) ∝ d i=1 S i s=1 (θ s i ) n s x i =1 (1 -θ s i ) n s x i =0 B(θ s i ; α s i,0 , β s i,0 ) (S.8.104) ∝ d i=1 S i s=1 (θ s i ) n s x i =1 (1 -θ s i ) n s x i =0 (θ s i ) α s i,0 -1 (1 -θ s i ) β s i,0 -1 (S.8.105) ∝ d i=1 S i s=1 (θ s i ) α s i,0 +n s x i =1 -1 (1 -θ s i ) β s i,0 +n s x i =0 -1 (S.8.106) ∝ d i=1 S i s=1 B(θ s i ; α s i,0 + n s x i =1 , β s i,0 + n s x i =0 ) (S.8.107) It can be immediately verified that B(θ s i ; α s i,0 + n s x i =1 , β s i,0 + n s x i =0 ) is proportional to the marginal p(θ s i |D) so that the parameters are independent under the posterior too. (b) For a variable x i with parents pa i , compute the posterior predictive probability p(x i = 1|pa i , D) Solution. The solution is analogue to the solution for question (c), using the sum rule, independencies, and properties of beta random variables: p(x i = 1|pa i = s, D) = p(x i = 1, θ s i |pa i = s, D)dθ s i (S.8.108) = p(x i = 1|θ s i , pa i = s, D)p(θ s i |pa i = s, D) (S.8.109) = p(x i = 1|θ s i , pa i = s)p(θ s i |D) (S.8.110) = θ s i p(θ s i |D) (S.8.111) = E[θ s i |D)] (S.8.112) (S.8.92) = α s i,n α s i,n + β s i,n (S.8.113) = α s i,0 + n s x i =1 α s i,0 + β s i,0 + n s (S.8.114) where n s = n s x i =0 + n s x i =1 denotes the number of times the parent configuration s occurs in the observed data D. The distribution of a and s are Bernoulli distributions with parameter (success probability) θ a and θ s , respectively, i.e. Cancer-asbestos-smoking example: Bayesian inference p(a|θ a ) = θ a a (1 -θ a ) 1-a p(s|θ s ) = θ s s (1 -θ s ) 1-s , (8.23) and the distribution of c given the parents is parametrised as specified in the following table p(c = 1|a, s, θ 1 c , . . . , θ 4 c )) a s θ 1 c 0 0 θ 2 c 1 0 θ 3 c 0 1 θ 4 c 1 1 We assume that the prior over the parameters of the model, (θ a , θ s , θ 1 c , . . . , θ 4 c ), factorises and is given by beta distributions with hyperparameters α 0 = 1 and β 0 = 1 (same for all parameters). Assume we observe the following iid data (each row is a data point). (1 + 0)/(1 + 1 + 2) = 1/4 0 0 (1 + 1)/(1 + 1 + 1) = 2/3 1 0 (1 + 1)/(1 + 1 + 2) = 1/2 0 1 (1 + 0)/(1 + 1) = 1/2 1 1 Compared to the MLE solution in Exercise (b) question (b), we see that the estimates are less extreme. This is because they are a combination of the prior knowledge and the observed data. Moreover, when we do not have any data, the posterior equals the prior, unlike for the mle where the estimate is not defined. Learning parameters of a directed graphical model We consider the directed graphical model shown below on the left for the four binary variables t, b, s, x, each being either zero or one. Assume that we have observed the data shown in the table on the right. x s t b 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 We assume the (conditional) pmf of s|t, b is specified by the following parametrised probability table: p(s = 1|t, b; θ 1 s , . . . , θ 4 s )) t b θ 1 s 0 0 θ 2 s 1 0 θ 3 s 0 1 θ 4 s 1 1 (a) What are the maximum likelihood estimates for p(s = 1|b = 0, t = 0) and p(s = 1|b = 0, t = 1), i.e. the parameters θ 1 s and θ 3 s ? the score matching objective function becomes a quadratic form, which can be optimised efficiently (see e.g. Barber, 2012, Appendix A.5.3). The set of models above are called the (continuous) exponential family, or also log-linear models because the models are linear in the parameters θ k . Since the exponential family generally includes probability mass functions as well, the qualifier \"continuous\" may be used to highlight that we are here considering continuous random variables only. The functions F k (x) are assumed to be known (they are called the sufficient statistics). (a) Denote by K(x) the matrix with elements K kj (x), K kj (x) = ∂F k (x) ∂x j , k = 1 . . . K, j = 1 . . . m, (8.34) and by H(x) the matrix with elements H kj (x), H kj (x) = ∂ 2 F k (x) ∂x 2 j , k = 1 . . . K, j = 1 . . . m. (8.35) Furthermore, let h j (x) = (H 1j (x), . . . , H Kj (x)) be the j-th column vector of H(x). Show that for the continuous exponential family, the score matching objective in Equation (8.32) becomes J(θ) = θ r + 1 2 θ Mθ, (8.36) where the first derivative with respect to x j , the j-th element of x, is r = 1 n n i=1 m j=1 h j (x i ), M = 1 n n i=1 K(x i )K(x i ) . ( 8 ψ j (x; θ) = ∂ log p(x; θ) ∂x j (S.8.155) = K k=1 θ k ∂F k (x) ∂x j (S.8.156) = K k=1 θ k K kj (x). (S.8.157) The second derivative is ∂ j ψ j (x; θ) = ∂ 2 log p(x; θ) ∂x 2 j (S.8.158) = K k=1 θ k ∂ 2 F k (x) ∂x 2 j (S.8.159) = K k=1 θ k H kj (x), (S.8.160) Inserting the expressions into Equation (8.32) gives J(θ) = 1 n n i=1 m j=1 ∂ j ψ j (x i ; θ) + 1 2 ψ j (x i ; θ) 2 (S.8.170) = 1 n n i=1 m j=1 ∂ j ψ j (x i ; θ) + 1 2 1 n n i=1 m j=1 ψ j (x i ; θ) 2 (S.8.171) = 1 n n i=1 m j=1 θ h j (x i ) + 1 2 1 n n i=1 θ K(x i )K(x i ) θ (S.8.172) = θ   1 n n i=1 m j=1 h j (x i )   + 1 2 θ 1 n n i=1 K(x i )K(x i ) θ (S.8.173) = θ r + 1 2 θ Mθ, (S.8.174) which is the desired result. (b) The pdf of a zero mean Gaussian parametrised by the variance σ 2 is p(x; σ 2 ) = 1 √ 2πσ 2 exp - x 2 2σ 2 , x ∈ R. (8.38) The (multivariate) Gaussian is a member of the exponential family. By comparison with Equation (8.33), we can re-parametrise the statistical model {p(x; σ 2 )} σ 2 and work with p(x; θ) = 1 Z(θ) exp θx 2 , θ < 0, x ∈ R, (8.39) instead. The two parametrisations are related by θ = -1/(2σ 2 ). Using the previous result on the (continuous) exponential family, determine the score matching estimate θ, and show that the corresponding σ2 is the same as the maximum likelihood estimate. This result is noteworthy because unlike in maximum likelihood estimation, score matching does not need the partition function Z(θ) for the estimation. Solution. By comparison with Equation (8.33), the sufficient statistics F (x) is x 2 . We first determine the score matching objective function. For that, we need to determine the quantities r and M in Equation (8.37). Here, both r and M are scalars, and so are the matrices K and H that define r and M. By their definitions, we obtain K(x) = ∂F (x) ∂x = 2x (S.8.175) H(x) = ∂ 2 F (x) ∂x 2 = 2 (S. 8.176) r = 2 (S.8.177) M = 1 n n i=1 K(x i ) 2 (S.8.178) = 4m 2 (S.8.179) 9.1 Importance sampling to estimate tail probabilities (based on Robert and Casella, 2010, Exercise 3.5) We would like to use importance sampling to compute the probability that a standard Gaussian random variable x takes on a value larger than 5, i.e P(x > 5) = ∞ 5 1 √ 2π exp - x 2 2 dx (9.1) We know that the probability equals P(x > 5) = 1 - 5 -∞ 1 √ 2π exp - x 2 2 dx (9.2) = 1 -Φ(5) (9.3) ≈ 2.87 • 10 -7 (9.4) where Φ(.) is the cumulative distribution function of a standard normal random variable. (a) With the indicator function 1 x>5 (x), which equals one if x is larger than 5 and zero otherwise, we can write P(x > 5) in form of the expectation P(x > 5) = E[1 x>5 (x)], (9.5) where the expectation is taken with respect to the density N (x; 0, 1) of a standard normal random variable, N (x; 0, 1) = 1 √ 2π exp -x 2 2 . (9.6) This suggests that we can approximate P(x > 5) by a Monte Carlo average P(x > 5) ≈ 1 n n i=1 1 x>5 (x i ), x i ∼ N (x; 0, 1). (9.7) Explain why this approach does not work well. Solution. In this approach, we essentially count how many times the x i are larger than 5. However, we know that the chance that x i > 5 is only 2.87 • 10 -7 . That is, we only get about one value above 5 every 20 million simulations! The approach is thus very sample inefficient. (b) Another approach is to use importance sampling with an importance distribution q(x) that is zero for x < 5. We can then write P(x > 5) as P(x > 5) = ∞ 5 1 √ 2π exp - x 2 2 dx (9.8) = ∞ 5 1 √ 2π exp - x 2 2 q(x) q(x) dx (9.9) = E q(x) 1 √ 2π exp - x 2 2 1 q(x) (9.10) and estimate P(x > 5) as a sample average. We here use an exponential distribution shifted by 5 to the right. It has pdf q(x) = exp(-(x -5)) if x ≥ 5 0 otherwise (9.11) For background on the exponential distribution, see e.g. https://en.wikipedia.  org/wiki/Exponential_distribution . Provide a formula that approximates P(x > 5) as a sample average over n samples x i ∼ q(x). Solution. The provided equation P(x > 5) = E q(x) 1 √ 2π exp - x 2 2 1 q(x) (S.9.1) can be approximated as a sample average as follows: P(x > 5) ≈ 1 n n i=1 1 √ 2π exp - x 2 i 2 1 q(x i ) (S.9.2) = 1 n n i=1 1 √ 2π exp - x 2 i 2 + x -5 (S.9.3) with x i ∼ q(x). Plot the estimate against the sample size and compare with the ground truth value. Solution. The following figure shows the importance sampling estimate as a function of the sample size (numbers do depend on the random seed used). We can see that we can obtain a good estimate with a few hundred samples already. Python code is as follows. Ihat[k] = mean(w.(x[1:k])); end # plot plt=plot(Ihat, label=\"approximation\"); hline!([p], color=:red, label=\"ground truth\") xlabel!(\"number of samples\") Monte Carlo integration and importance sampling A standard Cauchy distribution has the density function (pdf) p(x) = 1 π 1 1 + x 2 (9.12) with x ∈ R. A friend would like to verify that p(x)dx = 1 but doesn't quite know how to solve the integral analytically. They thus use importance sampling and approximate the integral as p(x)dx ≈ 1 n n i=1 p(x i ) q(x i ) x i ∼ q (9.13) where q is the density of the auxiliary/importance distribution. Your friend chooses a standard normal density for q and produces the following figure: The figure shows two independent runs. In each run, your friend computes the approximation with different sample sizes by subsequently including more and more x i in the approximation, so that, for example, the approximation with n = 2000 shares the first 1000 samples with the approximation that uses n = 1000. Your friend is puzzled that the two runs give rather different results (which are not equal to one), and also that within each run, the estimate very much depends on the sample size. Explain these findings. The Cauchy pdf has much heavier tails than a Gaussian so that the Gaussian pdf is already \"small\" when the Cauchy pdf is still \"large\". where p x denotes the pdf of the random variable x (u is here a dummy variable). Note that F x maps the domain of x to the interval [0, 1]. For simplicity, we here assume that F x is invertible. For a continuous random variable x with cdf F x show that the random variable y = F x (x) is uniformly distributed on [0, 1]. Importantly, this implies that for a random variable y which is uniformly distributed on [0, 1], the transformed random variable F -1 x (y) has cdf F x . This gives rise to a method called \"inverse transform sampling\" to generate n iid samples of a random variable x with cdf F x . Given a target cdf F x , the method consists of: • calculating the inverse F -1 x • sampling n iid random variables uniformly distributed on [0, 1]: y (i) ∼ U(0, 1), i = 1, . . . , n. • transforming each sample by F -1 x : x (i) = F -1 x (y (i) ), i = 1, . . . , n. By construction of the method, the x (i) are n iid samples of x. Solution. We start with the cumulative distribution function (cdf) F y for y, F y (β) = P(y ≤ β). (S.9.11) Since F x (x) maps x to [0, 1], F y (β) is zero for β < 0 and one for β > 1. We next consider β ∈ [0, 1]. Let α be the value of x that F x maps to β, i.e. F x (α) = β, which means α = F -1 x (β). Since F x is a non-decreasing function, we have F y (β) = P(y ≤ β) = P(F x (x) ≤ β) = P(x ≤ F -1 x (β)) = P(x ≤ α) = F x (α). (S.9.12) 0 0.2 0.4 0.6 0.8 1 y 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 g(y) To generate n iid samples from x, we first generate n iid samples y (i) that are uniformly distributed on [0, 1], and then compute for each F -1 x (y (i) ). The properties of inverse transform sampling guarantee that the x (i) , x (i) = F -1 x (y (i) ), (S.9.44) are independent and Laplace distributed. We can sample from it by sampling a Laplace variable with variance 1 as in Exercise 9.5 and then scaling the sample by √ 2b. Rejection sampling then repeats the following steps: • Generate x ∼ q(x; b) • Accept x with probability f (x) = 1 M p(x) q(x) , i.e. generate u ∼ U (0, 1) and accept x if u ≤ f (x).   for some function g(θ). If d is small, e.g. d ≤ 3, deterministic numerical methods can be used to approximate the integral to high accuracy, see e.g. https://en.wikipedia.org/  wiki/Numerical_integration . But for higher dimensions, these methods are generally not applicable any more. The expectation, however, can be approximated as a sample average if we have samples θ (i) from p(θ | D): E p(θ|D) [g(θ)] ≈ 1 S S i=1 g(θ (i) ) (9.25) Note that in MCMC methods, the samples θ ( foot_0 ) , . . . , θ (S) used in the above approximation are typically not statistically independent. Metropolis-Hastings is an MCMC algorithm that generates samples from a distribution p(θ), where p(θ) can be any distribution on the parameters (and not only posteriors). The algorithm is iterative and at iteration t, it uses: • a proposal distribution q(θ; θ (t) ), parametrised by the current state of the Markov chain, i.e. θ (t) ; • a function p * (θ), which is proportional to p(θ). In other words, p * (θ) is unnormalised 1 and the normalised density p(θ) is p(θ) = p * (θ) p * (θ)dθ . (9.26) For all tasks in this exercise, we work with a Gaussian proposal distribution q(θ; θ (t) ), whose mean is the previous sample in the Markov chain, and whose variance is 2 . That is, at iteration t of our Metropolis-Hastings algorithm, q(θ; θ (t-1) ) = • p_star: a function on θ that is proportional to the density of interest p(θ); • param_init: the initial sample -a value for θ from where the Markov chain starts; • num_samples: the number S of samples to generate; • vari: the variance 2 for the Gaussian proposal distribution q; and return [θ (1) , . . . , θ (S) ] -a list of S samples from p(θ) ∝ p * (θ).  Ideally, the time series covers the whole domain of the target distribution and it is hard to \"see\" any structure in it so that predicting values of future samples from the current one is difficult. If so, the samples are likely independent from each other and the chain is said to be well \"mixed\". (a) Consider the trace plots in Figure 9 .6: Is the variance vari used in Figure 9 .6b larger or smaller than the value of vari used in Figure 9 .6a? Is vari used in Figure 9 .6c larger or smaller than the value used in Figure 9 .6a? In both cases, explain the behaviour of the trace plots in terms of the workings of the Metropolis Hastings algorithm and the effect of the variance vari. Solution. MCMC methods are sensitive to different hyperparameters, and we usually need to carefully diagnose the inference results to ensure that our algorithm adequately approximates the target posterior distribution. (i) Figure 9 .6b uses a small variance (vari was set to 0.001) . The trace plots show that the samples for β are very highly correlated and evolve very slowly through time. This is because the introduced randomness is quite small compared to the scale of the posterior, thus the proposed sample at each MCMC iteration will be very close to the current sample and hence likely accepted. More mathematical explanation: for a symmetric proposal distribution, the acceptance ratio a becomes a = p * (θ * ) p * (θ) , (S.9.77) where θ is the current sample and θ * is the proposed sample. For variances that are small compared to the (squared) scale of the posterior, a is close to one and the proposed sample θ * gets likely accepted. This then gives rise to the slowly changing time series shown in Figure 9 .6b. (ii) In Figure 9 .6c, the variance is larger than the reference (vari was set to 50) . The trace plots suggest that many iterations of the algorithm result in the proposed sample being rejected, and thus we end up copying the same sample over and over again. This is because if the random perturbations are large compared to the scale of the posterior, p * (θ * ) may be very different from p * (θ) and a may be very small. (b) In Metropolis-Hastings, and MCMC in general, any sample depends on the previously generated sample, and hence the algorithm generates samples that are generally statistically dependent. The effective sample size of a sequence of dependent samples is the number of independent samples that are, in some sense, equivalent to our number of dependent samples. A definition of the effective sample size (ESS) is ESS = S 1 + 2 ∞ k=1 ρ(k) (9.34) where S is the number of dependent samples drawn and ρ(k) the correlation coefficient between two samples in the Markov chain that are k time points apart. We can 10.1 Mean field variational inference I Let L x (q) be the evidence lower bound for the marginal p(x) of a joint pdf/pmf p(x, y), L x (q) = E q(y|x) log p(x, y) q(y|x) . (10.1) Mean field variational inference assumes that the variational distribution q(y|x) fully factorises, i.e. q(y|x) = d i=1 q i (y i |x), (10.2) when y is d-dimensional. An approach to learning the q i for each dimension is to update one at a time while keeping the others fixed. We here derive the corresponding update equations. (a) Show that the evidence lower bound L x (q) can be written as L x (q) = E q 1 (y 1 |x) E q(y \\1 |x) [log p(x, y)] -d i=1 E q i (y i |x) [log q i (y i |x)] (10.3) where q(y \\1 |x) = d i=2 q i (y i |x) is the variational distribution without q 1 (y 1 |x). Solution. This follows directly from the definition of the ELBO and the assumed factorisation of q(y|x). We have L x (q) = E q(y|x) log p(x, y) -E q(y|x) log q(y|x) (S.10.1) = E d i=1 q i (y i |x) log p(x, y) -E d i=1 q i (y i |x) d i=1 log q i (y i |x) (S.10.2) = E d i=1 q i (y i |x) log p(x, y) -d i=1 E q i (y i |x) log q i (y i |x) (S.10.3) = E q 1 (y 1 |x) E d i=2 q i (y i |x) log p(x, y) -d i=1 E q i (y i |x) log q i (y i |x) (S.10.4) = E q 1 (y 1 |x) E q(y \\1 |x) [log p(x, y)] -d i=1 E q i (y i |x) [log q i (y i |x)] (S.10.5) We have here used the linearity of expectation. In case of continuous random variables, for instance, we have q i (y i |x) log q i (y i |x)dy i j =i q j (y j |x)dy j =1 (S.10.8) = d i=1 E q i (y i |x) log q i (y i |x) (S.10.9) For discrete random variables, the integral is replaced with a sum and leads to the same result. (b) Assume that we would like to update q 1 (y 1 |x) and that the variational marginals of the other dimensions are kept fixed. Show that argmax q 1 (y 1 |x) L x (q) = argmin where Z is the normalising constant. Note that variables y 2 , . . . , y d are marginalised out due to the expectation with respect to q(y \\1 |x). Solution. Starting from L x (q) = E q 1 (y 1 |x) E q(y \\1 |x) [log p(x, y)] -d i=1 E q i (y i |x) [log q i (y i |x)] (S.10.10) we drop terms that do not depend on q 1 . We then obtain J(q 1 ) = E q 1 (y 1 |x) E q(y \\1 |x) [log p(x, y)] -E q 1 (y 1 |x) [log q 1 (y 1 |x)] (S.10.11) = E q 1 (y 1 |x) log p(y 1 |x) -E q 1 (y 1 |x) [log q 1 (y 1 |x)] + const (S.10.12) = E q 1 (y 1 |x) log p(y 1 |x) q 1 (y 1 |x) (S.10.13) = -KL(q 1 (y 1 |x)||p(y 1 |x)) (S.10.14) The covariance between y 2 and x is computed in the same way and equals 1 too. We thus obtain the covariance matrix Σ Σ Σ, Since x is the sum of three random variables that have the same distribution, it makes intuitive sense that the mean assigns 1/3 of the observed value of x to y 1 and y 2 . Moreover, y 1 and y 2 are negatively corrected since an increase in y 1 must be compensated with a decrease in y 2 . Let us now approximate the posterior p(y 1 , y 2 |x) with mean field variational inference. Determine the optimal variational distribution using the method and results from Exercise 10.1. You may use that p(y 1 , y 2 , x) = N ((y 1 , y 2 , x); 0, Σ Σ Σ) Σ Σ Σ =   1 0 1 0 1 1 0 1 3   Σ Σ Σ -1 =   2 1 -1 1 2 -1 -1 -1 1   (10.11) Solution. The mean field assumption means that the variational distribution is assumed to factorise as q(y 1 , y 2 |x) = q 1 (y 1 |x)q 2 (y 2 |x) (S.10.23) From Exercise 10.1, the optimal q 1 (y 1 |x) and q 2 (y 2 |x) satisfy q 1 (y 1 |x) = p(y 1 |x), p(y 1 |x) = 1 Z exp E q 2 (y 2 |x) [log p(y 1 , y 2 , x)] (S.10.24) q 2 (y 2 |x) = p(y 2 |x), p(y 2 |x) = 1 Z exp E q 1 (y 1 |x) [log p(y 1 , y 2 , x)] (S.10.25) Note that these are coupled equations: q 2 features in the equation for q 1 via p(y 1 |x), and q 1 features in the equation for q 2 via p(y 2 |x). But we have two equations for two unknowns, which for the Gaussian joint model p(x, y 1 , y 2 ) can be solved in closed form. Given the provided equation for p(y 1 , y 2 , x), we have that log p(y 1 , y 2 , x) = -1 2   y 1 y 2 x     2 1 -1 1 2 -1 -1 -1 1     y 1 y 2 x   + const (S.10.26) = -1 2 2y 2 1 + 2y 2 2 + x 2 + 2y 1 y 2 -2y 1 x -2y 2 x + const (S.10.27) We further have E q log N (x i ; λ 2 ) = E q log 1 √ 2πλ 2 exp -x 2 i 2λ 2 (S.10.53) = log 1 √ 2πλ 2 -E q x 2 i 2λ 2 (S.10.54) = -log λ -λ 2 2λ 2 + const (S.10.55) = -log λ -1 2 + const (S.10.56) = -log λ + const (S.10.57) where we have used that for zero mean x i , E q [x 2 i ] = V(x i ) = λ 2 . We similarly obtain E q log N (x i ; σ 2 i ) = E q log   1 2πσ 2 i exp - x 2 2 Optimisation 2 . 1 221 -Schmidt orthogonalisation . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Linear transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Eigenvalue decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4 Trace, determinants and eigenvalues . . . . . . . . . . . . . . . . . . . . . . 1.5 Eigenvalue decomposition for symmetric matrices . . . . . . . . . . . . . . . 1.6 Power method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Gradient of vector-valued functions . . . . . . . . . . . . . . . . . . . . . . . 2.2 Newton's method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Gradient of matrix-valued functions . . . . . . . . . . . . . . . . . . . . . . . 2.4 Gradient of the log-determinant . . . . . . . . . . . . . . . . . . . . . . . . . 2.5 Descent directions for matrix-valued functions . . . . . . . . . . . . . . . . . 3 Directed Graphical Models 3.1 Directed graph concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Canonical connections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Ordered and local Markov properties, d-separation . . . . . . . . . . . . . . 3.4 More on ordered and local Markov properties, d-separation . . . . . . . . . 3.5 Chest clinic (based on Barber, 2012, Exercise 3.3) . . . . . . . . . . . . . . . . 3.6 More on the chest clinic (based on Barber, 2012, Exercise 3.3) . . . . . . . . . 3.7 Hidden Markov models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . -Schmidt orthogonalisation . . . . . . . . . . . . . . . . . . 1.2 Linear transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Eigenvalue decomposition . . . . . . . . . . . . . . . . . . . . . . 1.4 Trace, determinants and eigenvalues . . . . . . . . . . . . . . . . 1.5 Eigenvalue decomposition for symmetric matrices . . . . . . . . 1.6 Power method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . = |∆ 1 ∆ 2 a 11 a 22 -∆ 1 ∆ 2 a 12 a 21 | (S.1.45) = |∆ 1 ∆ 2 (a 11 a 22 -a 12 a 21 )| (S.1.46) = ∆ 1 ∆ 2 | det A| (S.1.47) Therefore the area of U y is the area of U x times | det A|. (d) Give an intuitive explanation why we have equality in the change of variables formula Uy f (y)dy = Ux f (Ax)| det A|dx.(1.6) (a) Use Exercise 1.3 to show that tr(A) = i A ii = i λ i . (You can use tr(AB) = tr(BA).) Solution. Since tr(AB) = tr(BA and A = UΛU -1 tr(A) = tr(UΛU -1 ) (S.1.67) = tr(ΛU -1 U) Use Exercise 1.3 to show that det A = i λ i . (Use det A -1 = 1/(det A) and det(AB) = det(A) det(B) for any A and B.) Solution. We use the eigenvalue decomposition of A to obtain det(A) = det(UΛU -1 ) (S.1.71) = det(U) det(Λ) det(U -1 ) (S.1.72) .10) where ||v k+1 || 2 denotes the Euclidean norm. (a) Let U the matrix with the (orthonormal) eigenvectors u i of Σ Σ Σ as columns. What is the eigenvalue decomposition of the covariance matrix Σ Σ Σ? Solution. Since the columns of U are orthonormal (eigen)vectors, U is orthogonal, i.e. U -1 = U . With Exercise 1.3 and Exercise 1.5, we obtain Σ Σ Σ = UΛU , (S.1.87) 2.84) (c) Write J(W) in terms of the eigenvalues λ n and calculate ∇J(W). Solution. In Exercise 1.4, we have shown that det(W) = i λ i and hence |det(W )| = i |λ i |.(i) If W is positive definite, its eigenvalues are positive and we can drop the absolute values so that | det(W )| = i λ i . ( c ) c What are the descendants of z? Solution. desc(z) = {q, e, h} (d) What are the non-descendants of q? Figure 3 . 1 : 31 Figure 3.1: The three canonical connections in DAGs. ( c ) c For the diverging connection, use the ordered Markov property to show that x ⊥ ⊥ y | z. Solution. A topological ordering is z, x, y. The predecessors of y are pre y = {x, z} and its parents pa y = {z}. The ordered Markov property y ⊥ ⊥ (pre y \\ pa y ) | pa y (S.3.10) thus becomes again y ⊥ ⊥ x | z, (S.3.11) which is, since the independence relationship is symmetric, the same as x ⊥ ⊥ z | z. Figure 3 . 2 : 32 Figure 3.2: Graphical model for Exercise 3.5 (Barber Figure 3.15). 2. l ⊥ ⊥ b | s Solution. • There are two trails from l to b: (l, s, b) and (l, e, d, b) • The trail (l, s, b) is blocked by s (s is in a tail-tail configuration and part of the conditioning set) • The trail (l, e, d, b) is blocked by the collider configuration for node d. • All trails are blocked so that the independence relation holds. (b) Can we simplify p(l|b, s) to p(l|s)? Solution. Since l ⊥ ⊥ b | s, we have p(l|b, s) = p(l|s). 3. 6 6 More on the chest clinic (based on Barber, 2012, Exercise 3.3)Consider the directed graphical model in Figure3.2. (x, e, l, s, b) and (x, e, d, b) • Trail (x, e, l, s, b) is blocked by l • Trail (x, e, d, b) is blocked by the collider configuration of node d. • For t, we have the trails (t, e, l, s, b) and (t, e, d, b) • Trail (t, e, l, s, b) is blocked by l. • Trail (t, e, d, b) is blocked by the collider configuration of node d. As all trails are blocked we have x, t ⊥ ⊥ b | l and E[g(x, t) | l, b] = E[g(x, t) | l]. have seen that x ⊥ ⊥ y|z is characterised by p(x, y|z) = p(x|z)p(y|z) or, equivalently, by p(x|y, z) = p(x|z). Show that further equivalent characterisations are p(x, y, z) = p(x|z)p(y|z)p(z) and (3.1) p(x, y, z) = a(x, z)b(y, z) for some non-neg. functions a(x, z) and b(x, z). (3.2) Equation ( 3 3 .1) implies (3.2) with a(x, z) = p(x|z) and b(y, z) = p(y|z)p(z). We now show the inverse. Let us assume that p(x, y, z) = a(x, z)b(y, z). By the product rule, we havep(x, y|z)p(z) = a(x,z)b(y, z). (S.3.50) (S.3.51) Summing over y gives y p(x, y|z)p(z) = p(z) y p(x, y|z) (S.3.52) = p(z)p(x|z) of p(x|z) over x equals one we have x a(x, z) = p(z) y b(y, z) . (S.3.57) Now, summing p(x, y|z)p(z) over x yields x p(x, y|z)p(z) = p(z) z)b(y, z) (S.3.60) = b(y, z) x a(x, z) (S.3.61) (S.3.57) = b(y, z) p(z) y b(y, z) (S.3.62) so that p(y|z)p(z) = p(z) b(y, z) y b(y, z) (S.3.63) We thus have p(x, y, z) = a(x, z)b(y, z) ( a ) a Without using d-separation, show that x ⊥ ⊥ {y, w} | z implies that x ⊥ ⊥ y | z and x ⊥ ⊥ w | z. Hint: use the definition of statistical independence in terms of the factorisation of pmfs/pdfs. Solution. We consider the joint distribution p(x, y, w|z). By assumption p(x, y, w|z) = p(x|z)p(y, w|z) (S.3.68) w p(y, w|z) is the marginal p(y|z), we have p(x, y|z) = p(x|z)p(y|z), (S.3.72) x)p(y) (S.3.77) Since p(x, y) = p(x)p(y) we have x ⊥ ⊥ y. For x ⊥ ⊥ y|w, compute p(x, y, w) and use the result x ⊥ ⊥ y|w ⇔ p(x, y, w) = a(x, w)b(y, w). p(x, y, w) = z p(x, y, z, w) (S.3.78) = z p(x)p(y)p(z|x, y)p(w|z) (S.3.79) = p(x) p(y) z p(z|x, y)p(w|z) k(x,y,w) (S.3.80) Since p(x, y, w) cannot be factorised as a(x, w)b(y, w), the relation x ⊥ ⊥ y|w cannot generally hold. Exercises 4. 1 1 Visualising and analysing Gibbs distributions via undirected graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Factorisation and independencies for undirected graphical models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Factorisation and independencies for undirected graphical models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Factorisation from the Markov blankets I . . . . . . . . . . . . . 4.5 Factorisation from the Markov blankets II . . . . . . . . . . . . 4.6 Undirected graphical model with pairwise potentials . . . . . . 4.7 Restricted Boltzmann machine (based on Barber, 2012, Exercise 4.4) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.8 Hidden Markov models and change of measure . . . . . . . . . Figure 4 . 1 : 41 Figure 4.1: Graph for Exercise 4.2 How do the pdfs/pmfs of the undirected graphical model factorise?Solution. The maximal cliques are (x, w), (w, z), (z, y) and (x, y). The undirected graphical model thus consists of pdfs/pmfs that factorise as follows p(x, w, z, y) ∝ φ 1 (x, w)φ 2 (w, z)φ 3 (z, y)φ 4 (x, y) (S.4.1) (b) List all independencies that hold for the undirected graphical model. 4. 7 7 Restricted Boltzmann machine (based on Barber, 2012, Exercise 4.4) Figure 4 . 2 : 42 Figure 4.2: Left: Graph for p(v, h). Right: Graph for p(h|v) Exercises 5.1 I-equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Minimal I-maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 I-equivalence between directed and undirected graphs . . . . . 5.4 Moralisation: Converting DAGs to undirected minimal I-maps 5.5 Moralisation exercise . . . . . . . . . . . . . . . . . . . . . . . . . 5.6 Moralisation exercise . . . . . . . . . . . . . . . . . . . . . . . . . 5.7 Triangulation: Converting undirected graphs to directed minimal I-maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.8 I-maps, minimal I-maps, and I-equivalency . . . . . . . . . . . . 5.9 Limits of directed and undirected graphical models . . . . . . . (a) Which of three graphs represent the same set of independencies? Explain.To check whether the graphs are I-equivalent, we have to check the skeletons and the immoralities. All have the same skeleton, but graph 1 and graph 2 also have the same immorality. The answer is thus: graph 1 and 2 encode the same independencies.Which of three graphs represent the same set of independencies? Explain. Assume the graph below is a perfect map for a set of independencies U. Figure 5 5 Figure 5.1: Perfect I-map G for Exercise 5.2, question (a). Figure 5 . 2 : 52 Figure 5.2: Exercise 5.2, Question (a):Construction of a minimal directed I-map for the ordering (e, h, q, z, a). z,e} • MB(e) = {q} (i) Draw the undirected minimal I-map representing the independencies. (ii) Indicate a Gibbs distribution that satisfies the independence relations specified by the Markov blankets.Solution. Connecting each variable to all variables in its Markov blanket yields the desired undirected minimal I-map. Note that the Markov blankets are not mutually disjoint. ( a ) a Verify that the following two graphs are I-equivalent by listing and comparing the independencies that each graph implies. Figure 5 5 Figure 5.3: Answer to Exercise 5.4: Illustrating the moralisation process Exercises 6. 1 1 Conversion to factor graphs . . . . . . . . . . . . . . . . . . . . . 6.2 Sum-product message passing . . . . . . . . . . . . . . . . . . . . 6.3 Sum-product message passing . . . . . . . . . . . . . . . . . . . . 6.4 Max-sum message passing . . . . . . . . . . . . . . . . . . . . . . 6.5 Choice of elimination order in factor graphs . . . . . . . . . . . 6.6 Choice of elimination order in factor graphs . . . . . . . . . . . Figure 6 . 1 : 61 Figure 6.1: Answer to Exercise 6.2 Question (b): Computing all messages in five clock cycles.If we also computed the messages toward the leaf factor nodes, we needed six cycles, but they are not necessary for computation of the marginals so they are omitted. Exercises 7. 1 1 Predictive distributions for hidden Markov models . . . . . . . 7.2 Viterbi algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Forward filtering backward sampling for hidden Markov models115 7.4 Prediction exercise . . . . . . . . . . . . . . . . . . . . . . . . . . 7.5 Hidden Markov models and change of measure . . . . . . . . . 7.6 Kalman filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . Figure 7 . 1 : 2 s 712 Figure 7.1: Kalman filtering for tracking of a moving object. The blue points indicate the true positions of the object in a two-dimensional space at successive time steps, the green points denote noisy measurements of the positions, and the red crosses indicate the means of the inferred posterior distributions of the positions obtained by running the Kalman filtering equations. The covariances of the inferred positions are indicated by the red ellipses, which correspond to contours having one standard deviation. (Bishop, 2006, Figure 13.22) x i = 1) and n x=0 = n -n x=1 are the number of ones and zeros in the data. Since θ ∈ [0, 1], we have to solve the constrained optimisation problem Consider the model specified by the DAG a s c Determine the posterior predictive probabilities p(a = 1|D) and p(s = 1|D). Solution. With Exercise 8.5 question (c), we have p(a = 1|D) = E(θ a |D) Determine the posterior predictive probabilities p(c = 1|pa, D) for all possible parent configurations. Solution. The parents of c are (a, s). With Exercise 8.6 question (b), we have p(c = 1|a, s, D) a s has tuberculosis b = 1 has bronchitis s = 1 has shortness of breath x = 1 has positive x-ray Observed data: .37)Solution. For log p(x; θ) = K k=1 θ k F k (x) -log Z(θ)(S.8.154) ( c ) c Numerically compute the importance estimate for various sample sizes n ∈ [0, 1000]. Figure 9 . 1 : 91 Figure 9.1: Exercise 9.2. Comparison of the log pdf of a standard normal (blue) and the Cauchy random variable (red) for positive inputs. The Cauchy pdf has much heavier tails than a Gaussian so that the Gaussian pdf is already \"small\" when the Cauchy pdf is still \"large\". 9. 6 8 ) 68 Rejection sampling (based on Robert and Casella, 2010, Exercise 2.Most compute environments provide functions to sample from a standard normal distribution. Popular algorithms include the Box-Muller transform, see e.g. https://en. wikipedia.org/wiki/Box-Muller_transform. We here use rejection sampling to sample from a standard normal distribution with density p(x) using a Laplace distribution as our proposal/auxiliary distribution.The density q(x) of a zero-mean Laplace distribution with variance 2b 2 is q(x; b) (a) Compute the ratio M (b) = max x p(x) q(x;b) . Density represented by 10, 000 samples. Figure 9 . 3 : 93 Figure 9.3: Density and samples from p(x, y) = N (x; 0, 1)N (y; 0, 1). used with this proposal distribution, the algorithm is called Random Walk Metropolis-Hastings algorithm. (a) Read Section 27.4 of Barber (2012) to familiarise yourself with the Metropolis-Hastings algorithm. (b) Write a function mh implementing the Metropolis Hasting algorithm, as given in Algorithm 27.3 in Barber (2012), using the Gaussian proposal distribution in (9.27) above. The function should take as arguments For example: def mh(p_star, param_init, num_samples=5000, vari=1.0): # your code here return samples Solution. Below is a Python implementation. def mh(p_star, param_init, num_samples=5000, vari=1.0): x = [] x_current = param_init for n in range(num_samples): # proposal x_proposed = multivariate_normal.rvs(mean=x_current, cov=vari) # MH step a = multivariate_normal.pdf(x_current, mean=x_proposed, cov=vari) * p_star(x_proposed) a = a / (multivariate_normal.pdf(x_proposed, mean=x_current, cov= vari) * p_star(x_current)) # accept or not if a >= 1: x_next = np.copy(x_proposed) elif uniform.rvs(0, 1) < a: x_next = np.copy(x_proposed) else: x_next = np.copy(x_current) # keep record x.append(x_next) x_current = x_next through time, i.e. they are a time-series of the samples generated by the Markov chain. Figure 9 . 9 6 shows examples of trace plots obtained by running the Metropolis Hastings algorithm for different values of the hyperparameters vari and param_init. (( i (y i |x)d i=1 log q i (y i |x) = q 1 (y 1 |x) • . . . • q d (y d |x) d i=1 log q i (y i |x)dy 1 . . . dy d y 1 |x) • . . . • q d (y d |x) log q i (y i |x)dy 1 . . . dy d q 1 1 (y 1 |x) KL(q 1 (y 1 |x)||p(y 1 |x))(10.4) with log p(y 1 |x) = E q(y \\1 |x) [log p(x, y)] + const, (10.5)where const refers to terms not depending on y 1 . That is,p(y 1 |x) = 1 Z exp E q(y \\1 |x) [log p(x, y)] ,(10.6) Σ The conditional p(y 1 , y 2 |x) is Gaussian with mean m and covariance C, σ 2 2 ( 2 x; λ 2 ||p(x)) = -2 log λ + λ 2 Determine the value of λ that minimises J(λ) = KL(q(x; λ 2 )||p(x)). Interpret the result and relate it to properties of the Kullback-Leibler divergence.Solution. Taking derivatives of J(λ) with respect to λ gives S.10.65) We here follow the notation of Barber (2012) ; p or φ are often to denote unnormalised models too."
}