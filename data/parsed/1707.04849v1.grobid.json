{
  "title": "Minimax deviation strategies for machine learning and recognition with short learning samples",
  "abstract": "The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws.",
  "introduction": "Introduction The small learning sample problem has been around in machine learning under different names during its whole life. The learning sample is used to compensate for the lack of knowledge about the recognized object when its statistical model is not completely known. Naturally, the longer the learning sample is, the better is the subsequent recognition. However, when the learning sample becomes too small (2, 3, 5 elements) an effect of small samples becomes evident. In spite of the fact that any learning sample (even a very small one) provides some additional information about the object, it may be better to ignore the learning sample than to utilize it with the commonly used methods. Example 1. Let us consider an object that can be in one of two random states y = 1 and y = 2 with equal probabilities. In each state the object generates two independent Gaussian random signals x 1 and x 2 with variances equal 1. Mean values of signals depend on the state as it is shown on Fig. 1. In the first state the mean value is (2, 0). In the second state the mean value depends on an unknown parameter θ and is (0, θ). Even if no learning sample is given a minimax strategy can be used to make a decision about the state y. The minimax strategy ignores the second signal and makes decision y * = 1 when x 1 > 1 and decision y * = 2 when x 1 ≤ 1. Now let us assume that there is a sample of signals generated by an object in the second state but with higher variance 16. A maximum likelihood strategy estimates the unknown parameter θ and then makes a decision about y as if the estimated value of the parameter is its true value. Fig. 2 shows how the probability of a wrong decision (called the risk) depends on parameter θ for different sizes of the learning sample. If the learning sample is sufficiently long, the risk of maximum likelihood strategy may become arbitrarily close to the minimum possible risk. Naturally, when the length of the sample decreases the risk becomes worse and worse. Furthermore, when it becomes as small as 3 or 2 elements the risk of the maximum likelihood strategy becomes worse than the risk of the minimax strategy that uses neither the learning sample nor the signal x 2 at all. Hence, it is better to ignore available additional data about the recognized object than to try to make use of it in a conventional way. It demonstrates a serious theoretical flaw of commonly used methods, and definitely not that short samples are useless. Any learning sample, no mater how long or short it is, provides some, may be not a lot information about the recognized object and a reasonable method has to use it. θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) n = 1 n = 2 θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) n = 3 n = 10 Figure 2 : Example 1. Probability of a wrong decision (risk) for different sizes n of the learning sample. The curve R(q M L , θ) is the risk of a maximum likelihood strategy. The curve R(q minmax , θ) is the risk of a minimax strategy. The curve min q R(q, θ) is the minimum possible risk for each model. Example 2. This is a simple example that has been used by H.Robbins in his seminal article [5] where he initiated empirical Bayessian approach and explaned its main idea. An object can be in one of two possible states y = 1 and y = 2. In each state the object generates a univariate Gaussian signal x with variance 1. The mean value of the generated signal depends on the state y so that p(x|y = 1) = 1 √ 2π e -(x+1) 2 2 , p(x|y = 2) = 1 √ 2π e -(x-1) 2 2 . Only a priori probabilities of states are unknown and θ is the probability of the first state so that p(y = 1) = θ and p(y = 2) = 1θ. A minimax strategy for such incomplete statistical model makes decision y * based on the sign of the observed signal and ensures probability of correct recognition 0.84 independently of a priori probabilities of states. Let not only a single object, but a collection of mutually independant objects be available for recognition. Each object is in its own hidden state and is presented with its own signal. Let us also assume that the decision about x y * = 2 y * = 1 p(x|y = 1) p(x|y = 2) 1 -1 Figure 3: Example 2. x ∈ R -signal, y ∈ {1, 2} -state. each object's state does not have to be made immediately when the object is observed and can be postponed until the whole collection is observed. In this case maximum likelihood estimations of a priori probabilities of states can be computed and then each object of the collection is recognized as if the estimated values of probabilities were the true values. When the presented collection is sufficiently long the probability of a wrong decision can be made as close to the minimum as possible (Fig. 4 ). However, when the collection is too short, the probability of a wrong decision can be much worse than that of the minimax strategy. The considered examples lead to a difficult and up to now an unanswered question. What should be done when a fixed sample of 2-3 elements is given and no additional elements can be obtained? Is it really the best way to ignore these data or is it possible to make use of them? We want to fill up this gap between maximum likelihood and minimax strategies and develop a strategy that covers teh whole range of learning samples lengths including zero length. However, this gap, and it is infact a gap, shows a theoretical imperfection of the commonly used learning procedures, namely, of maximum likelihood learning. The short sample problem in whole follows from the fact that maximum likelihood learning as well as many other learning procedures have not been deduced from any explicit risk-oriented requirement to the quality of post-learning recognition. We will formulate such risk-oriented requirements a priori and will see what type of learning procedures follow. 2 Basic definitions Definition 1. An object is represented with a tuple X, Y, Θ, p XY : X × Y × Θ → R θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 n = 1 n = 2 θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 n = 5 n = 10 Figure 4 : Example 2. Probability of a wrong decision (risk) for different sizes n of the learning sample. The curve R(q M L , θ) shows the risk of a maximum likelihood strategy, R(q minmax , θ) is the risk of a minimax strategy, min q R(q, θ) is the minimal possible risk. where X is a finite set of signal values x ∈ X; Y is a finite set of states y ∈ Y ; Θ is a finite set of models θ ∈ Θ; p XY (x, y; θ) is a probability of a pair (x ∈ X, y ∈ Y ) for a model θ ∈ Θ. A signal x is an observable parameter of recognized object whereas a state y is its hidden parameter. A pair (x, y) is random and for each pair (x ∈ X, y ∈ Y ) its probability p XY (x, y; θ) exists. However, this probability is not known because it depends on an unknown model θ. As for the model θ it is not random, it takes a fixed but unknown value. Only the set Θ is known that the value θ belongs to. Let z be some random data that depend on a model θ and take values from a finite set Z. The data is specified with a tuple Z, p Z : Z × Θ → R where p Z (z; θ) is a probability of data z ∈ Z for model θ ∈ Θ. Definition 2. A random data Z, p Z : Z ×Θ → R that depends on a model is called a learning data for an object X, Y, Θ, p XY : X × Y × Θ → R if p XY Z (x, y, z; θ) = p XY (x, y; θ) • p Z (z; θ) for all x ∈ X, y ∈ Y, z ∈ Z, θ ∈ Θ. A learning sample ((x i , y i )|i = 1, 2, . . . , n) used for supervised learning is a special cases of learning data when Z = (X × Y ) n and p Z (z; θ) = n i=1 p XY (x i , y i ; θ). A learning sample (x i |i = 1, 2, . . . , n) for unsupervised learning is another special case of learning data when Z = X n and p Z (z; θ) = n i=1 y∈Y p XY (x i , y; θ). Any expert knowledge about the true model is also learning data. One can even consider the case when |Z| = 1 and therefore p Z (z; θ) = 1, which is equivalent to the absence of any learning data at all. We do not restrict the learning data in any way except that for any fixed model the learning data z depend neither on the current signal x nor on the current state y so that p XY Z (x, y, z; θ) = p XY (x, y; θ) • p Z (z; θ) for all x ∈ X, y ∈ Y, z ∈ Z, θ ∈ Θ. Definition 3. A non-negative function q : X ×Y ×Z → R is called a strategy if y∈Y q(y |x, z) = 1 for all x ∈ X, z ∈ Z. Value q(y |x, z) of a strategy q : X × Y × Z → R is a probability of a randomized decision that the current state of an object is y, given the current observed signal x and the available learning data z. The set of all strategies q : X × Y × Z → R is denoted Q. Let ω : Y × Y be a loss function whose value ω(y, y ′ ) is the loss of a decision y ′ when the true state is y. Definition 4. Risk R(q, θ) of a strategy q on a model θ is expected loss R(q, θ) = z∈Z x∈X y∈Y p XY (x, y; θ)p Z (z; θ) y ′ ∈Y q(y ′ |x, z)ω(y, y ′ ). Let us be reminded that throughout the paper the sets X, Y , Z and Θ are assumed to be finite. This allows a much more transparent formulation of main results. Allowing some of the sets to be infinite would require finer mathematical tools and the results might be obscured by unnecessary technical details. 3 Improper and Bayesian strategies. One can see that the risk of a strategy depends not only on the strategy itself but also on the model that the strategy is applied to. Therefore, in a general case it is not possible to prefer some strategy q 1 to another strategy q 2 . The risk of q 1 may be better than the risk of q 2 on some models and worse on the others. However, it is possible to prefer strategy q 2 to strategy q 1 if the risk of q 1 is greater than the risk of q 2 on all models. In this case we will say that q 2 dominates q 1 and q 1 is dominated by q 2 . Definition 5. A strategy q 0 is called improper if a strategy q * exists such that R(q 0 , θ) > R(q * , θ) for all θ ∈ Θ. We want to exclude all improper from consideration strategies and derive a common form of all the rest. Let T denote the set of all non-negative functions τ : Θ → R such that θ∈Θ τ (θ) = 1. Functions of such type will be refferred to as weight functions. Definition 6. A strategy q * is called Bayesian if there exists a weight function τ ∈ T such that q * = arg min q∈Q θ∈Θ τ (θ)R(q, θ). Theorem 1. Each strategy q 0 ∈ Q is either Bayesian or improper, but never both. Proof. For a given strategy q 0 let us define a function F : T × Q → R, F (τ, q) = θ∈Θ τ (θ) R(q, θ) -R(q 0 , θ) . According to Definition 4, for any fixed θ the risk R(q, θ) is a linear function of probabilities q(y |x, z). Consequently, for any fixed τ the function F is also a linear function of probabilities q(y |x, z). Similarly, function F is a linear function of weights τ (θ) for any fixed strategy q. The set Q of strategies and the set T of weight functions are both closed convex sets. Consequently, due to the known duality theorem [1, 2, 4] function F has a saddle point (τ * ∈ T, q * ∈ Q) such that max τ ∈T min q∈Q F (τ, q) = F (τ * , q * ) = min q∈Q max τ ∈T F (τ, q), where q * = argmin q∈Q max τ ∈T F (τ, q), τ * = argmax τ ∈T min q∈Q F (τ, q). It is obvious that F (τ, q 0 ) = 0 for any τ ∈ T . Therefore, the inequality min q∈Q F (τ, q) ≤ 0 holds for every τ ∈ T and, consequently, max τ ∈T min q∈Q F (τ, q) = F (τ * , q * ) ≤ 0. Therefore, there are two mutually exclusive cases: either F (τ * , q * ) < 0 or F (τ * , q * ) = 0. In such way the proof of the theorem is reduced to proving the following four propositions: Proposition 1. If the strategy q 0 is Bayessian then F (τ * , q * ) = 0. Proposition 2. If F (τ * , q * ) = 0 then the strategy q 0 is Bayessian. Proposition 3. If the strategy q 0 is improper then F (τ * , q * ) < 0. Proposition 4. If F (τ * , q * ) < 0 then the strategy q 0 is improper. Proof of Proposition 1. If the strategy q 0 is Bayessian then according to Definition 6 a weight function τ 0 exists such that inequality θ∈Θ τ 0 (θ)R(q, θ) ≥ θ∈Θ τ 0 (θ)R(q 0 , θ) is valid for all q ∈ Q. Consequently, for all q ∈ Q the chain 0 ≤ θ∈Θ τ 0 (θ)[R(q, θ) -R(q 0 , θ)] = F (τ 0 , q) ≤ max τ ∈T F (τ, q) is also valid. Since all numbers max τ ∈T F (τ, q), q ∈ Q, are not negative the least of them is also not negative and min q∈Q max τ ∈T F (τ, q) = F (τ * , q * ) ≥ 0 From this inequality it follows that F (τ * , q * ) = 0 because a case F (τ * , q * ) > 0 is impossible. Proof of Proposition 2. Let F (τ * , q * ) = 0 then 0 = F (τ * , q * ) = max τ ∈T min q∈Q F (τ, q) = min q∈Q F (τ * , q) = = min q∈Q θ∈Θ τ * (θ) R(q, θ) -R(q 0 , θ) = = min q∈Q θ∈Θ τ * (θ)R(q, θ) - θ∈Θ τ * (θ)R(q 0 , θ). It implies the equality min q∈Q θ∈Θ τ * (θ)R(q, θ) = θ∈Θ τ * (θ)R(q 0 , θ) and therefore, q 0 = arg min q∈Q θ∈Θ τ * (θ)R(q, θ), which means that q 0 is Bayesian according to Definition 6. Proof of Proposition 3. If the strategy q 0 is improper then according to Definition 5 a strategy q 1 exists such that inequality R(q 1 , θ) < R(q 0 , θ) holds for all θ. The set of models is finite and therefore, a value ε < 0 exists such that for any θ inequality R(q 1 , θ) -R(q 0 , θ) ≤ ε holds and a chain 0 > ε ≥ θ∈Θ τ (θ)[R(q 1 , θ) -R(q 0 , θ)] = F (τ, q 1 ) ≥ min q∈Q F (τ, q) is valid for any τ ∈ T . Since all numbers min q∈Q F (τ, q), τ ∈ T , are not greater then ε the greatest of them is also not greater then ε and max τ ∈T min q∈Q F (τ, q) = F (τ * , q * ) ≤ ε < 0. Proof of Proposition 4. Let F (τ * , q * ) < 0 then F (τ * , q * ) = min q∈Q max τ ∈T F (τ, q) = max τ ∈T F (τ, q * ) = = max τ ∈T θ∈Θ τ (θ) R(q * , θ) -R(q 0 , θ) = max θ∈Θ R(q * , θ) -R(q 0 , θ) and therefore max θ∈Θ R(q * , θ) -R(q 0 , θ) < 0. Consequently, the inequality R(q * , θ) < R(q 0 , θ) holds for all models θ ∈ Θ and q 0 is improper according to Definition 5. The theorem gives good reasons to reappraise lot of well-known methods that are commonly used as something self-evident. Let us illustrate this criticism with two simple examples. The first example considers a certain method of recognition without learning and the second relates to maximum likelihood learning. In both examples the loss function is ω(y, y ′ ) = 0, if y = y ′ , 1, if y = y ′ . Example 3. Let x be an image of a letter, y be its name and θ be a position of the letter in a field of vision. Let the function p XY : X × Y × Θ → R be constructively defined so that probability p XY (x, y; θ) can be calculated for each triple x, y, θ. In this case when an image x with an unknown position θ is observed the decision y * (x) about the name of the letter has to be of the form y * (x) = argmax y∈Y θ∈Θ τ (θ)p XY (x, y; θ). (1) Theorem 1 reveals a certain weakness of the commonly used form argmax y∈Y max θ∈Θ p XY (x, y; θ). (2) The strategy (2) could be represented in the form (1) if the weights τ (θ) in (1) could be chosen individually for each observation x ∈ X. However, each Bayessian strategy is specified with its own weight function τ : Θ → R so that weights are assigned to elements of the set Θ, not of the set Θ × X. As a rule, the strategy (2) cannot be represented in the form (1) with fixed weights τ (θ) that do not depend on x. It means that the strategy (2) is not Bayessian and is dominated by some other strategy that for each position of the letter recognizes its name better then strategy (2). Example 4. Let the sets X, Y and Θ be specified for the recognized object as well as a function p XY : X × Y × Θ → R. Let the learning information be a random learning sample z = (( x i , y i )|i = 1, 2, . . . , n) such that p Z (z; θ) = n i=1 p XY (x i , y i ; θ). Then the decision y * about the current state y 0 based on the current signal x 0 and available learning sample z has to be of the form y * = arg max y 0 ∈Y θ∈Θ τ (θ) n i=0 p(x i , y i ; θ) (3) for some fixed τ that does not depend on z. One can see that the commonly used maximum likelihood strategy y * = arg max y 0 p(x 0 , y 0 ; θ M L (z)), (4) θ M L (z) = arg max θ∈Θ n i=1 p(x i , y i ; θ) can almost never be represented in the form (3) with constant weights and therefore is not Bayessian. It means that some other strategy exists that makes a decision about the current state based both on current signal and learning information and for each model makes it better than strategy (4).",
  "body": "Introduction The small learning sample problem has been around in machine learning under different names during its whole life. The learning sample is used to compensate for the lack of knowledge about the recognized object when its statistical model is not completely known. Naturally, the longer the learning sample is, the better is the subsequent recognition. However, when the learning sample becomes too small (2, 3, 5 elements) an effect of small samples becomes evident. In spite of the fact that any learning sample (even a very small one) provides some additional information about the object, it may be better to ignore the learning sample than to utilize it with the commonly used methods. Example 1. Let us consider an object that can be in one of two random states y = 1 and y = 2 with equal probabilities. In each state the object generates two independent Gaussian random signals x 1 and x 2 with variances equal 1. Mean values of signals depend on the state as it is shown on Fig. 1. In the first state the mean value is (2, 0). In the second state the mean value depends on an unknown parameter θ and is (0, θ). Even if no learning sample is given a minimax strategy can be used to make a decision about the state y. The minimax strategy ignores the second signal and makes decision y * = 1 when x 1 > 1 and decision y * = 2 when x 1 ≤ 1. Now let us assume that there is a sample of signals generated by an object in the second state but with higher variance 16. A maximum likelihood strategy estimates the unknown parameter θ and then makes a decision about y as if the estimated value of the parameter is its true value. Fig. 2 shows how the probability of a wrong decision (called the risk) depends on parameter θ for different sizes of the learning sample. If the learning sample is sufficiently long, the risk of maximum likelihood strategy may become arbitrarily close to the minimum possible risk. Naturally, when the length of the sample decreases the risk becomes worse and worse. Furthermore, when it becomes as small as 3 or 2 elements the risk of the maximum likelihood strategy becomes worse than the risk of the minimax strategy that uses neither the learning sample nor the signal x 2 at all. Hence, it is better to ignore available additional data about the recognized object than to try to make use of it in a conventional way. It demonstrates a serious theoretical flaw of commonly used methods, and definitely not that short samples are useless. Any learning sample, no mater how long or short it is, provides some, may be not a lot information about the recognized object and a reasonable method has to use it. θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) n = 1 n = 2 θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) n = 3 n = 10 Figure 2 : Example 1. Probability of a wrong decision (risk) for different sizes n of the learning sample. The curve R(q M L , θ) is the risk of a maximum likelihood strategy. The curve R(q minmax , θ) is the risk of a minimax strategy. The curve min q R(q, θ) is the minimum possible risk for each model. Example 2. This is a simple example that has been used by H.Robbins in his seminal article [5] where he initiated empirical Bayessian approach and explaned its main idea. An object can be in one of two possible states y = 1 and y = 2. In each state the object generates a univariate Gaussian signal x with variance 1. The mean value of the generated signal depends on the state y so that p(x|y = 1) = 1 √ 2π e -(x+1) 2 2 , p(x|y = 2) = 1 √ 2π e -(x-1) 2 2 . Only a priori probabilities of states are unknown and θ is the probability of the first state so that p(y = 1) = θ and p(y = 2) = 1θ. A minimax strategy for such incomplete statistical model makes decision y * based on the sign of the observed signal and ensures probability of correct recognition 0.84 independently of a priori probabilities of states. Let not only a single object, but a collection of mutually independant objects be available for recognition. Each object is in its own hidden state and is presented with its own signal. Let us also assume that the decision about x y * = 2 y * = 1 p(x|y = 1) p(x|y = 2) 1 -1 Figure 3: Example 2. x ∈ R -signal, y ∈ {1, 2} -state. each object's state does not have to be made immediately when the object is observed and can be postponed until the whole collection is observed. In this case maximum likelihood estimations of a priori probabilities of states can be computed and then each object of the collection is recognized as if the estimated values of probabilities were the true values. When the presented collection is sufficiently long the probability of a wrong decision can be made as close to the minimum as possible (Fig. 4 ). However, when the collection is too short, the probability of a wrong decision can be much worse than that of the minimax strategy. The considered examples lead to a difficult and up to now an unanswered question. What should be done when a fixed sample of 2-3 elements is given and no additional elements can be obtained? Is it really the best way to ignore these data or is it possible to make use of them? We want to fill up this gap between maximum likelihood and minimax strategies and develop a strategy that covers teh whole range of learning samples lengths including zero length. However, this gap, and it is infact a gap, shows a theoretical imperfection of the commonly used learning procedures, namely, of maximum likelihood learning. The short sample problem in whole follows from the fact that maximum likelihood learning as well as many other learning procedures have not been deduced from any explicit risk-oriented requirement to the quality of post-learning recognition. We will formulate such risk-oriented requirements a priori and will see what type of learning procedures follow. 2 Basic definitions Definition 1. An object is represented with a tuple X, Y, Θ, p XY : X × Y × Θ → R θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 n = 1 n = 2 θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 n = 5 n = 10 Figure 4 : Example 2. Probability of a wrong decision (risk) for different sizes n of the learning sample. The curve R(q M L , θ) shows the risk of a maximum likelihood strategy, R(q minmax , θ) is the risk of a minimax strategy, min q R(q, θ) is the minimal possible risk. where X is a finite set of signal values x ∈ X; Y is a finite set of states y ∈ Y ; Θ is a finite set of models θ ∈ Θ; p XY (x, y; θ) is a probability of a pair (x ∈ X, y ∈ Y ) for a model θ ∈ Θ. A signal x is an observable parameter of recognized object whereas a state y is its hidden parameter. A pair (x, y) is random and for each pair (x ∈ X, y ∈ Y ) its probability p XY (x, y; θ) exists. However, this probability is not known because it depends on an unknown model θ. As for the model θ it is not random, it takes a fixed but unknown value. Only the set Θ is known that the value θ belongs to. Let z be some random data that depend on a model θ and take values from a finite set Z. The data is specified with a tuple Z, p Z : Z × Θ → R where p Z (z; θ) is a probability of data z ∈ Z for model θ ∈ Θ. Definition 2. A random data Z, p Z : Z ×Θ → R that depends on a model is called a learning data for an object X, Y, Θ, p XY : X × Y × Θ → R if p XY Z (x, y, z; θ) = p XY (x, y; θ) • p Z (z; θ) for all x ∈ X, y ∈ Y, z ∈ Z, θ ∈ Θ. A learning sample ((x i , y i )|i = 1, 2, . . . , n) used for supervised learning is a special cases of learning data when Z = (X × Y ) n and p Z (z; θ) = n i=1 p XY (x i , y i ; θ). A learning sample (x i |i = 1, 2, . . . , n) for unsupervised learning is another special case of learning data when Z = X n and p Z (z; θ) = n i=1 y∈Y p XY (x i , y; θ). Any expert knowledge about the true model is also learning data. One can even consider the case when |Z| = 1 and therefore p Z (z; θ) = 1, which is equivalent to the absence of any learning data at all. We do not restrict the learning data in any way except that for any fixed model the learning data z depend neither on the current signal x nor on the current state y so that p XY Z (x, y, z; θ) = p XY (x, y; θ) • p Z (z; θ) for all x ∈ X, y ∈ Y, z ∈ Z, θ ∈ Θ. Definition 3. A non-negative function q : X ×Y ×Z → R is called a strategy if y∈Y q(y |x, z) = 1 for all x ∈ X, z ∈ Z. Value q(y |x, z) of a strategy q : X × Y × Z → R is a probability of a randomized decision that the current state of an object is y, given the current observed signal x and the available learning data z. The set of all strategies q : X × Y × Z → R is denoted Q. Let ω : Y × Y be a loss function whose value ω(y, y ′ ) is the loss of a decision y ′ when the true state is y. Definition 4. Risk R(q, θ) of a strategy q on a model θ is expected loss R(q, θ) = z∈Z x∈X y∈Y p XY (x, y; θ)p Z (z; θ) y ′ ∈Y q(y ′ |x, z)ω(y, y ′ ). Let us be reminded that throughout the paper the sets X, Y , Z and Θ are assumed to be finite. This allows a much more transparent formulation of main results. Allowing some of the sets to be infinite would require finer mathematical tools and the results might be obscured by unnecessary technical details. 3 Improper and Bayesian strategies. One can see that the risk of a strategy depends not only on the strategy itself but also on the model that the strategy is applied to. Therefore, in a general case it is not possible to prefer some strategy q 1 to another strategy q 2 . The risk of q 1 may be better than the risk of q 2 on some models and worse on the others. However, it is possible to prefer strategy q 2 to strategy q 1 if the risk of q 1 is greater than the risk of q 2 on all models. In this case we will say that q 2 dominates q 1 and q 1 is dominated by q 2 . Definition 5. A strategy q 0 is called improper if a strategy q * exists such that R(q 0 , θ) > R(q * , θ) for all θ ∈ Θ. We want to exclude all improper from consideration strategies and derive a common form of all the rest. Let T denote the set of all non-negative functions τ : Θ → R such that θ∈Θ τ (θ) = 1. Functions of such type will be refferred to as weight functions. Definition 6. A strategy q * is called Bayesian if there exists a weight function τ ∈ T such that q * = arg min q∈Q θ∈Θ τ (θ)R(q, θ). Theorem 1. Each strategy q 0 ∈ Q is either Bayesian or improper, but never both. Proof. For a given strategy q 0 let us define a function F : T × Q → R, F (τ, q) = θ∈Θ τ (θ) R(q, θ) -R(q 0 , θ) . According to Definition 4, for any fixed θ the risk R(q, θ) is a linear function of probabilities q(y |x, z). Consequently, for any fixed τ the function F is also a linear function of probabilities q(y |x, z). Similarly, function F is a linear function of weights τ (θ) for any fixed strategy q. The set Q of strategies and the set T of weight functions are both closed convex sets. Consequently, due to the known duality theorem [1, 2, 4] function F has a saddle point (τ * ∈ T, q * ∈ Q) such that max τ ∈T min q∈Q F (τ, q) = F (τ * , q * ) = min q∈Q max τ ∈T F (τ, q), where q * = argmin q∈Q max τ ∈T F (τ, q), τ * = argmax τ ∈T min q∈Q F (τ, q). It is obvious that F (τ, q 0 ) = 0 for any τ ∈ T . Therefore, the inequality min q∈Q F (τ, q) ≤ 0 holds for every τ ∈ T and, consequently, max τ ∈T min q∈Q F (τ, q) = F (τ * , q * ) ≤ 0. Therefore, there are two mutually exclusive cases: either F (τ * , q * ) < 0 or F (τ * , q * ) = 0. In such way the proof of the theorem is reduced to proving the following four propositions: Proposition 1. If the strategy q 0 is Bayessian then F (τ * , q * ) = 0. Proposition 2. If F (τ * , q * ) = 0 then the strategy q 0 is Bayessian. Proposition 3. If the strategy q 0 is improper then F (τ * , q * ) < 0. Proposition 4. If F (τ * , q * ) < 0 then the strategy q 0 is improper. Proof of Proposition 1. If the strategy q 0 is Bayessian then according to Definition 6 a weight function τ 0 exists such that inequality θ∈Θ τ 0 (θ)R(q, θ) ≥ θ∈Θ τ 0 (θ)R(q 0 , θ) is valid for all q ∈ Q. Consequently, for all q ∈ Q the chain 0 ≤ θ∈Θ τ 0 (θ)[R(q, θ) -R(q 0 , θ)] = F (τ 0 , q) ≤ max τ ∈T F (τ, q) is also valid. Since all numbers max τ ∈T F (τ, q), q ∈ Q, are not negative the least of them is also not negative and min q∈Q max τ ∈T F (τ, q) = F (τ * , q * ) ≥ 0 From this inequality it follows that F (τ * , q * ) = 0 because a case F (τ * , q * ) > 0 is impossible. Proof of Proposition 2. Let F (τ * , q * ) = 0 then 0 = F (τ * , q * ) = max τ ∈T min q∈Q F (τ, q) = min q∈Q F (τ * , q) = = min q∈Q θ∈Θ τ * (θ) R(q, θ) -R(q 0 , θ) = = min q∈Q θ∈Θ τ * (θ)R(q, θ) - θ∈Θ τ * (θ)R(q 0 , θ). It implies the equality min q∈Q θ∈Θ τ * (θ)R(q, θ) = θ∈Θ τ * (θ)R(q 0 , θ) and therefore, q 0 = arg min q∈Q θ∈Θ τ * (θ)R(q, θ), which means that q 0 is Bayesian according to Definition 6. Proof of Proposition 3. If the strategy q 0 is improper then according to Definition 5 a strategy q 1 exists such that inequality R(q 1 , θ) < R(q 0 , θ) holds for all θ. The set of models is finite and therefore, a value ε < 0 exists such that for any θ inequality R(q 1 , θ) -R(q 0 , θ) ≤ ε holds and a chain 0 > ε ≥ θ∈Θ τ (θ)[R(q 1 , θ) -R(q 0 , θ)] = F (τ, q 1 ) ≥ min q∈Q F (τ, q) is valid for any τ ∈ T . Since all numbers min q∈Q F (τ, q), τ ∈ T , are not greater then ε the greatest of them is also not greater then ε and max τ ∈T min q∈Q F (τ, q) = F (τ * , q * ) ≤ ε < 0. Proof of Proposition 4. Let F (τ * , q * ) < 0 then F (τ * , q * ) = min q∈Q max τ ∈T F (τ, q) = max τ ∈T F (τ, q * ) = = max τ ∈T θ∈Θ τ (θ) R(q * , θ) -R(q 0 , θ) = max θ∈Θ R(q * , θ) -R(q 0 , θ) and therefore max θ∈Θ R(q * , θ) -R(q 0 , θ) < 0. Consequently, the inequality R(q * , θ) < R(q 0 , θ) holds for all models θ ∈ Θ and q 0 is improper according to Definition 5. The theorem gives good reasons to reappraise lot of well-known methods that are commonly used as something self-evident. Let us illustrate this criticism with two simple examples. The first example considers a certain method of recognition without learning and the second relates to maximum likelihood learning. In both examples the loss function is ω(y, y ′ ) = 0, if y = y ′ , 1, if y = y ′ . Example 3. Let x be an image of a letter, y be its name and θ be a position of the letter in a field of vision. Let the function p XY : X × Y × Θ → R be constructively defined so that probability p XY (x, y; θ) can be calculated for each triple x, y, θ. In this case when an image x with an unknown position θ is observed the decision y * (x) about the name of the letter has to be of the form y * (x) = argmax y∈Y θ∈Θ τ (θ)p XY (x, y; θ). (1) Theorem 1 reveals a certain weakness of the commonly used form argmax y∈Y max θ∈Θ p XY (x, y; θ). (2) The strategy (2) could be represented in the form (1) if the weights τ (θ) in (1) could be chosen individually for each observation x ∈ X. However, each Bayessian strategy is specified with its own weight function τ : Θ → R so that weights are assigned to elements of the set Θ, not of the set Θ × X. As a rule, the strategy (2) cannot be represented in the form (1) with fixed weights τ (θ) that do not depend on x. It means that the strategy (2) is not Bayessian and is dominated by some other strategy that for each position of the letter recognizes its name better then strategy (2). Example 4. Let the sets X, Y and Θ be specified for the recognized object as well as a function p XY : X × Y × Θ → R. Let the learning information be a random learning sample z = (( x i , y i )|i = 1, 2, . . . , n) such that p Z (z; θ) = n i=1 p XY (x i , y i ; θ). Then the decision y * about the current state y 0 based on the current signal x 0 and available learning sample z has to be of the form y * = arg max y 0 ∈Y θ∈Θ τ (θ) n i=0 p(x i , y i ; θ) (3) for some fixed τ that does not depend on z. One can see that the commonly used maximum likelihood strategy y * = arg max y 0 p(x 0 , y 0 ; θ M L (z)), (4) θ M L (z) = arg max θ∈Θ n i=1 p(x i , y i ; θ) can almost never be represented in the form (3) with constant weights and therefore is not Bayessian. It means that some other strategy exists that makes a decision about the current state based both on current signal and learning information and for each model makes it better than strategy (4). A gap between maximum likelihood and minimax strategies. We consider maximum likelihood and minimax strategies and specify a gap between them. Let us define for each θ ∈ Θ a strategy q opt (θ) = argmin q∈Q R(q, θ) that assigns a probability q opt (y |x, z; θ) for each triple (x, y, z). The strategy q opt (θ) is the best possible strategy that should be used if a true model were known. Since the model is known no learning data are needed. For any fixed model θ a strategy q(θ) : X × Y × Z → R can be replaced with a strategy q X (θ) : X × Y → R with the same risk. Probabilities q(y |x, z; θ) have to be transformed into probabilities q X (y |x; θ) according to expression q X (y |x; θ) = z∈Z p Z (z; θ)q(y |x, z; θ) and so the chain R(q, θ) = z∈Z x∈X y∈Y p XY (x, y; θ)p Z (z; θ) y ′ ∈Y q(y ′ |x, z; θ)ω(y, y ′ ) = = x∈X y∈Y p XY (x, y; θ) y ′ ∈Y ω(y, y ′ ) z∈Z p Z (z; θ)q(y ′ |x, z; θ) = = x∈X y∈Y p XY (x, y; θ) y ′ ∈Y q X (y ′ |x; θ)ω(y, y ′ ) = R(q X , θ). is valid for each model θ. Consequently, for each θ the equality min q∈Q R(q, θ) = min q X ∈Q X R(q X , θ) (5) is valid. The symbol Q X in (5) designates a set of all strategies of the form q X : X × Y → R that do not use the learning data. Definition 7. A strategy q M L : X × Y × Z → R is called a maximum likelihood strategy if for each triple (x, y, z) it specifies a probability q M L (y |x, z) = q opt X (x|y; θ M L (z)), where q opt X (θ) = argmin q X ∈Q X R(q X , θ) and θ M L (z) = argmax θ∈Θ p Z (z; θ). In other words, maximum likelihood strategies use the learning data z to estimate a model θ and make a decision that minimizes the expected loss with an assumption that the estimated model is the true model. As it has been quoted for Examples 3 and 4, as a rule, maximum likelihood strategies cannot be represented in a form of a Bayessian strategy q B = argmin q∈Q θ∈Θ τ (θ)R(q, θ) with fixed weights τ (θ) that do not depend on the learning data. In such cases the maximum likelihood strategy q M L may be dominated with another strategy of the form X × Y × Z → R. Minimax strategies are free of this flaw. Definition 8. Strategy argmin q∈Q max θ∈Θ R(q, θ) is called a minimax strategy. Theorem 2. No minimax strategy is improper. Proof. Let us prove an equivalent statement that any improper strategy q 0 is not minimax. Indeed, as far as q 0 is improper another strategy q 1 exists such that R(q 1 , θ) < R(q 0 , θ) for all θ. Therefore, max θ R(q 1 , θ) < max θ R(q 0 , θ) and min q max θ R(q, θ) < max θ R(q 0 , θ) and q 0 is not argmin q max θ R(q 0 , θ). Though maximum likelihood strategy may be improper whereas minimax strategy is never improper the first one has an essential advantage over the second. There is a rather wide class of learning data such that the maximum likelihood strategy is in a sense consistent for any recognized object whereas there is a rather wide class of recognized objects such that the minimax strategy is not consistent for any learning data. Let us exactly formulate these statements and prove them. Let z ∈ Z be a random variable that depends on model θ and let for each z ∈ Z and θ ∈ Θ a probabillity p Z (z; θ) be given. We will say that this dependence is essential if for each two different models θ 1 = θ 2 a value z * exists such that p Z (z * ; θ 1 ) = p Z (z * ; θ 2 ). Let z n = (z i |i = 1, 2, . . . , n) ∈ Z n be a learning sample, p Z n (z n ; θ * ) = n i=1 p Z (z i ; θ * ) be a probability of a sample and θ M L (z n ) = argmax θ p Z n (z n ; θ) be a maximum likelihood estimation of the model. Consistency is a generally known property of maximum likelihood estimate. In the considered case this property may be formulated in a simple way that the probability of inequality θ M L (z n ) = θ * converges to zero when n increases or, formally, lim n→∞ z n ∈Z n err n i=1 p Z (z i ; θ * ) = 0 ( 6 ) where Z n err = {z n ∈ Z n |θ M L (z n ) = θ * }. ( 7 ) The consistency of a maximum likelihood estimations is a base for a proof of the following theorem about consistency of maximum likelihood strategy. Theorem 3. Let z be random variable that takes values from a set Z according to probability distribution p Z (z; θ) that essentially depends on θ; let n be a positive integer and z n = (z i |i = 1, 2, . . . , n) ∈ Z n be a random learning sample with probability distribution p Z n (z n ; θ) = n i=1 p Z (z i ; θ); let q M L n : X × Y × Z n → R be a maximum likelihood strategy for an object X, Y, Θ, p XY : X × Y × Θ → R and learning data Z n , p Z n : Z n × Θ → R . Then lim n→∞ max θ∈Θ R(q M L n , θ) -min q∈Q R(q, θ) = 0. Proof. As far as a set Θ is finite the proof of the theorem is reduced to proof of the equality lim n→∞ R(q M L n , θ) -min q∈Q R(q, θ) = 0 (8) for any θ. The subsequent proof is based on equality (5), on equalities ( 6 ) and ( 7 ) that express consistency of maximum likelihood estimates and on equality R(q M L n , θ) = z n ∈Z n p Z n (z n ; θ) min q X ∈Q X R(q X , θ M L (z n )), where θ M L (z n ) = argmax θ∈Θ p Z n (z n ; θ), that follows from Definition 7. The following chain is valid: lim n→∞ [R(q M L n , θ) -min q∈Q R(q, θ)] = lim n→∞ [R(q M L n , θ) -min q X ∈Q X R(q X , θ)] = = lim n→∞ [ zn∈Z n p Z n (z n ; θ) min q X ∈Q X R(q X , θ M L (z n )) -min q X ∈Q X R(q X , θ)] = lim n→∞ zn∈Z n p Z n (z n ; θ)[ min q X ∈Q X R(q X , θ M L (z n )) -min q X ∈Q X R(q X , θ)] = lim n→∞ z n ∈Z n err p Z n (z n ; θ)[ min q X ∈Q X R(q X , θ M L (z n )) -min q X ∈Q X R X (q X , θ)] ≤ lim n→∞ z n ∈Z n err p Z n (z n ; θ)[max y∈Y max y ′ ∈Y w(y, y ′ ) -min y∈Y min y ′ ∈Y w(y, y ′ )] = lim n→∞ {[max y∈Y max y ′ ∈Y w(y, y ′ ) -min y∈Y min y ′ ∈Y w(y, y ′ )] z n ∈Z n err p Z n (z n ; θ)} = [max y∈Y max y ′ ∈Y w(y, y ′ ) -min y∈Y min y ′ ∈Y w(y, y ′ )] lim n→∞ z n ∈Z n err p Z n (z n ; θ) = 0. It follows from a chain that for any θ an inequality lim n→∞ R(q M L n , θ) -min q∈Q R(q, θ) ≤ 0 holds. The difference R(q M L n , θ) -min q∈Q R(q, θ) is never negative and so (8) is proved. So, with the increasing length of learning sample the risk function of maximum likelihood strategy becomes arbitrarily close to the minimum possible risk function. Minimax strategy has not this property. Moreover, for certain class of objects minimax strategies simply ignore the learning sample, no matter how long it is. Theorem 4. Let for an object X, Y, Θ, p XY : X × Y × Θ → R a pair (θ * , q * X ) exists such that q * X = argmin q X ∈Q X R(q X , θ * ), θ * = argmax θ∈Θ R(q * X , θ). Then the inequality max θ∈Θ R(q, θ) ≥ max θ∈Θ R(q * X , θ) (9) is valid for any learning data Z, p Z : Z × Θ → R and any strategy q : X × Y × Z → R. Proof. For any strategy q ∈ Q we have the chain max θ∈Θ R(q, θ) ≥ R(q, θ * ) ≥ min q∈Q R(q, θ * ) = = min q X ∈Q X R(q X , θ * ) = R(q * X , θ * ) = max θ∈Θ R(q * X , θ). The theorem shows that for some objects the minimax approach is particularly inappropriate because it enforces to ignore any learning data. There is nothing unusual in conditions of the Theorem 4. Examples 1 and 2 in Introduction show just the cases when these conditions are satisfied. So, there is a following gap between maximum likelihood and minimax strategies. Maximum likelihood strategy may be dominated with other strategy. In this case it can be improved and, consequently, it is not optimal from any point of view. However, for wide class of learning data maximum likelihood strategies are consistent and so their chortage does not become apparent when learning sample of an arbitrary size may be obtained. Cases of learning samples of fixed sizes, especially, short samples form an area of improper application of maximum likelihood strategies. This area is not covered with minimax strategies. Though minimax strategies are dominated with no strategy, for rather wide class of objects minimax requirement enforces to ignore any learning sample, no matter how long it is. Minimax deviation strategies. This section is aimed at developing a Bayesian consistent strategy that has to fill a gap between maximum likelihood and minimax strategies. Definition 9. A strategy argmin q∈Q max θ∈Θ R(q, θ) -min q ′ ∈Q R(q ′ , θ) is called mini- max deviation strategy. Minimax deviation strategies do not have the drawback of the minimax strategies. A theorem that is similar to Theorem 3 for maximum likelihood strategies is also valid for minimax deviation strategies. Theorem 5. Let z be random variable that takes values from a set Z according to probability distribution p Z (z; θ) that essentially depends on θ; let n be a positive integer and z n = (z i |i = 1, 2, . . . , n) ∈ Z n is a random learning sample with probability distribution p Z n (z n ; θ) = n i=1 p Z (z i ; θ); let q * n : X × Y × Z n → R be a minimax deviation strategy for an object X, Y, Θ, p XY : X × Y × Θ → R and learning data Z n , p Z n : Z n × Θ → R . Then lim n→∞ max θ∈Θ R(q * n , θ) -min q∈Q R(q, θ) = 0. ( 10 ) Proof. The Theorem is a straighforward consequence of Definition 9 and the Theorem 3. Let q M L n be a maximum likelihood strategy for an object X, Y, Θ, p XY : X × Y × Θ → R and learning data Z n , p Z n : Z n × Θ → R . It follows from Definition 9 that max θ∈Θ R(q * n , θ) -min q∈Q R(q, θ) ≤ max θ∈Θ R(q M L n , θ) -min q∈Q R(q, θ) for any n. It follows from Theorem 3 that lim n→∞ max θ∈Θ R(q * n , θ) -min q∈Q R(q, θ) ≤ ≤ lim n→∞ max θ∈Θ R(q M L n , θ) -min q∈Q R(q, θ) = 0. As far as the difference [R(q * n , θ) -min q∈Q R(q, θ) is negative for no model the equality (10) is proved. Let us note that the proof of the Theorem 10 shows not only a consistency of minimax deviation strategy. It shows also that minimax deviation strategy converges to desired result not slower than maximum likelihood strategy. Similarly, one can show that this advantage of minimax deviation strategy holds as compared with any consistent strategy and from this point of view it is the best of all consistent strategies. Following theorem states that minimax deviation strategies are also inappropriate for recognition of certain type of objects. Theorem 6. Let for an object X, Y, Θ, p : X × Y × Θ → R a model θ * and a strategy q * X exist such that q * X = argmin q X ∈Q X [R X (q X , θ * ) -min q ′ X ∈Q X R X (q ′ X , θ * )], (11) θ * = argmax θ∈Θ [R X (q * X , θ) -min q ′ X ∈Q X R X (q ′ X , θ)]. (12) Then the inequality max θ∈Θ [R(q, θ) -min q X ∈Q X R(q X , θ)] ≥ max θ∈Θ [R(q * X , θ) -min q X ∈Q X R(q X , θ)] holds for any learning data Z, p Z : Z × Θ → R and any strategy q ∈ Q. Proof. In fact, proof of the theorem does not differ from the proof of the Theorem 4. However, the consequences of this theorem for minimax deviation strategies are not so destructive as those of Theorem 4 for minimax strategies. In fact, conditions (11) and (12) imply that a strategy q * X ∈ Q X exists that does not use learning information and assures minimal possible risk for each model, R(q * X , θ) = min q X ∈Q X R(q X , θ) for all θ ∈ Θ. In this case, any learning data are needless and has to be omitted by any strategy. Evidently, minimax deviation strategy is not improper and, consequently, is Bayessian. The following theorem shows how the corresponding weight function has to be obtained. Theorem 7. Minimax deviation strategy q * = argmin q∈Q max θ∈Θ R(q, θ) -min q X ∈Q X R(q X , θ) is a Bayesian strategy argmin q∈Q θ∈Θ τ * (θ)R(q, θ) with respect to weight function τ * = arg max τ ∈T min q∈Q θ∈Θ τ (θ)R(q, θ) - θ∈Θ τ (θ) min q X ∈Q X R(q X , θ) . (13) Proof. Let us define a function F : T × Q → R, F (τ, q) = θ∈Θ τ (θ)R(q, θ) - θ∈Θ τ (θ) min q X ∈Q X R(q X , θ) and express q * and τ * in terms of F , q * = argmin q∈Q max θ∈Θ R(q, θ) -min q X ∈Q X R(q X , θ) = argmin q∈Q max τ ∈T θ∈Θ τ (θ) R(q, θ) -min q X ∈Q X R(q X , θ) = argmin q∈Q max τ ∈T F (τ, q), τ * = arg max τ ∈T min q∈Q F (τ, q). The function F is a linear function of q for fixed τ and a linear function of τ for fixed q and is defined on a Cartesian product of two closed convex sets T and Q. In such case a pair (τ * , q * ) is a saddle point [1, 2, 4] , min q∈Q max τ ∈T F (τ, q) = F (τ * , q * ) = max τ ∈T min q∈Q F (τ, q), that implies F (τ * , q * ) = min q∈Q F (τ * , q) and q * = arg min q∈Q F (τ * , q) = = arg min q∈Q θ∈Θ τ * (θ)R(q, θ) - θ∈Θ τ * (θ) min q X ∈Q X R(q X , θ) = = arg min q∈Q θ∈Θ τ * (θ)R(q, θ). In such way developing minimax deviation strategy is reduced to calculating weights τ (θ) of models that maximize concave function (13). In described below experiments general purpose methods of non-smooth optimization [6] were used. Experiments Minimax deviation strategies have been built for objects considered in Introduction in Examples 1 and 2. Minimax deviation strategies have been compared with maximum likelihood and minimax strategies. Results are presented on Figures 5 and 6 that show risk R(q, θ) of the strategies as a function of a model for several learning sample sizes. Figure 5 relates to Example 1 and Figure 6 to Example 2. θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) n = 1 n = 2 θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) n = 3 n = 10 Figure 5 : Example 1. Probability of making a wrong decision for different sizes n of the learning sample. The dashed line shows the risk of a minimax deviation strategy. The curve R(q M L , θ) is the risk of a maximum likelihood strategy. The curve R(q minmax , θ) is the risk of a minimax strategy. The curve min q R(q, θ) is the minimum possible risk for each model. Conclusion The paper analyzes the problem when for given object that differs from (14). Moreover, any decision that differs from (15) can be replaced with a decision of the form (15) with the better recognition quality. There is nothing in decision (15) that could be treated as a selecting some best model of the model set and so no question stands what estimator θ est : Z → Θ has to be used. No model has to be selected, on the contrary, all models have to take part in decision with their weights. It is essential that the weights do not depend on learning data, they are determined by requirement to searched strategy for concrete applied situation. The paper shows a way for computing these weights for minimax deviation strategy that is appropriate for learning samples of any length and in such way fills a gap between maximum likelihood and minimax startegies. Minimax deviation strategy is not at all a single strategy that is reasonable in such or other application. Many other strategies are appropriate too, for example, a strategies of the form argmin q∈Q max θ∈Θ R(q, θ)α(θ) β(θ) with predefined numbers α(θ) and β(θ) > 0. Minimax strategy is a special case of (16) when α(θ) = 0, β(θ) = 1, minimax deviation strategy is a case when α(θ) = min q∈Q R(q, θ), β(θ) = 1. A reasonable modification of minimax deviation strategy is a case when α(θ) = 0, β(θ) = min q∈Q R(q, θ). The numbers α(θ) may be risks of some already developed strategy and this is a case when the developer wants to check whether the better strategy is possible. At last, numbers α(θ) may be simply desired values of risks in concrete applied situation. Requirements of the form (16) together with various loss functions determine various applied situations and obtained results show the way to cope with all them. It has become quite clear now that each strategy of the form (16) may be represented in the form (15) because, obviously, no of them is improper. Obtained results imply unexpected conclusion that learning data take part in a decision (15) in a unified form that depends neither on applied situation nor on recognized object. So, no question stands more how the learning data have to influence the decision about current state when the current signal is observed. Learning data influence the decision via and only via probabilities p Z ; (z; θ), not via choise of some best model of the model set. Figure 1 : 1 Figure 1: Example 1. (x 1 , x 2 ) ∈ R 2 -signal, y ∈ {1, 2} -state. X , Y, Θ, p XY : X × Y × Θ → R , loss function w : Y × Y → R, learning data source Z, p Z : Z × Θ → R , observed current signal x and available learning data z a decision y * about to the quality of post-learning recognition implies the decision of the form y * = argmin y ′ ∈Y θ∈Θ τ (θ)p Z (z; θ) y∈Y p XY (x, y; θ)w(y, y ′ ) (15)"
}