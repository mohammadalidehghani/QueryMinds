{
  "title": "A COMBINATORIAL CONJECTURE FROM PAC-BAYESIAN MACHINE LEARNING",
  "abstract": "We present a proof of a combinatorial conjecture from the second author's Ph.D. thesis [9] . The proof relies on binomial and multinomial sums identities. We also discuss the relevance of the conjecture in the context of PAC-Bayesian machine learning.",
  "introduction": "1. PAC-Bayes bounds in machine learning. In machine learning, ensemble methods are used to build stronger models by combining several base models. In the setting of classification problems, the model, called classifier, can be combined using a weighted majority vote of the base classifiers. Examples of learning algorithms built using such classifiers include Bagging [1] and Boosting [4] . The PAC-Bayes theorem, introduced by McAllester in the 1990's ( [13] , [14] , [15] ), provides bounds on the expectation of the risk of the base classifiers among a majority vote classifier. These bounds are computed from the empirical risk of the majority classifier made during the training phase of a learning algorithm, and can then be used to provide guarantees on existing algorithms or to design new ones. The version of the PAC-Bayes theorem in [7] involves a function ξ : N → R defined by ξ(m) := m k=0 m k k m k 1 - k m m-k and improves upon other versions, such as the ones in [11] and [12] for instance, which provide the bounds m + 1 and 2 √ m respectively using approximations. The version in [7] , on the other hand, is based on a direct calculation in order to obtain ξ(m). Bounds on the risk of the majority vote itself can be deduced from the bounds given by the PAC-Bayes theorem. Such bounds, however, turn out to be greater than the bounds on the base voters. In order to circumvent this issue and obtain better bounds, a new version of the PAC-Bayes theorem was given in [10] . This new theorem can be seen as a two-dimensional version, since it gives bounds simultaneously on the expectation of the joint errors and on the joint success of pairs of voters in a majority vote. As shown in [10] , this approach does indeed lead to better bounds on the risk of the majority vote. The method of [10] is based on techniques from [7] and involves another combinatorial sum function ξ 2 : N → R defined by ξ 2 (m) := m j=0 m-j k=0 m j m -j k j m j k m k 1 - j m - k m m-j-k . The function ξ 2 was also used in [8] and [9] .",
  "body": "1. PAC-Bayes bounds in machine learning. In machine learning, ensemble methods are used to build stronger models by combining several base models. In the setting of classification problems, the model, called classifier, can be combined using a weighted majority vote of the base classifiers. Examples of learning algorithms built using such classifiers include Bagging [1] and Boosting [4] . The PAC-Bayes theorem, introduced by McAllester in the 1990's ( [13] , [14] , [15] ), provides bounds on the expectation of the risk of the base classifiers among a majority vote classifier. These bounds are computed from the empirical risk of the majority classifier made during the training phase of a learning algorithm, and can then be used to provide guarantees on existing algorithms or to design new ones. The version of the PAC-Bayes theorem in [7] involves a function ξ : N → R defined by ξ(m) := m k=0 m k k m k 1 - k m m-k and improves upon other versions, such as the ones in [11] and [12] for instance, which provide the bounds m + 1 and 2 √ m respectively using approximations. The version in [7] , on the other hand, is based on a direct calculation in order to obtain ξ(m). Bounds on the risk of the majority vote itself can be deduced from the bounds given by the PAC-Bayes theorem. Such bounds, however, turn out to be greater than the bounds on the base voters. In order to circumvent this issue and obtain better bounds, a new version of the PAC-Bayes theorem was given in [10] . This new theorem can be seen as a two-dimensional version, since it gives bounds simultaneously on the expectation of the joint errors and on the joint success of pairs of voters in a majority vote. As shown in [10] , this approach does indeed lead to better bounds on the risk of the majority vote. The method of [10] is based on techniques from [7] and involves another combinatorial sum function ξ 2 : N → R defined by ξ 2 (m) := m j=0 m-j k=0 m j m -j k j m j k m k 1 - j m - k m m-j-k . The function ξ 2 was also used in [8] and [9] . 2. A combinatorial conjecture. In [9] , the second author posed the following conjecture, based on numerical evidence. Conjecture 2.1 (Lacasse [9] ). For every m ∈ N, we have ξ 2 (m) = ξ(m) + m. In this paper, we present a proof of Conjecture 2.1 due to the first author that appeared in a an unpublished manuscript on the arXiv [24] . The proof uses binomial and multinomial sums identities in order to obtain simpler expressions for the functions ξ and ξ 2 . It has been cited in several publications related to machine learning, such as [3] , [6] , [8] , [16] , [17] , [18] , [19] , [22] . We also mention that although other proofs of Conjecture 2.1 were subsequently obtained (see e.g. [2] , [5] , [20] , [23] ), the proof we present remains, as far as we know, the only one providing equivalent expressions for the functions ξ and ξ 2 that are simpler and more convenient from a numerical perspective. Furthermore, none of the aforementioned papers [2] , [5] , [20] , [23] discuss the context and relevance of Conjecture 2.1 in the theory of machine learning. Proof of Conjecture 2.1 In this section, we present the proof of Conjecture 2.1 from [24] . 3.1. An equivalent formulation. It will be more convenient to express the conjecture in terms of binomial and multinomial type sums. We therefore introduce the following functions: γ(m) := m m ξ(m) = m k=0 m k k k (m -k) m-k (m ∈ N) and γ 2 (m) := m m ξ 2 (m) = m j=0 m-j k=0 m j m -j k j j k k (m -j -k) m-j-k (m ∈ N). Note that Conjecture 2.1 is equivalent to the identity γ 2 (m) -γ(m) = m m+1 (m ∈ N). It turns out that the functions γ and γ 2 have considerably simpler expressions. Proof. The proof consists of an elementary calculation. We have γ 2 (m) -γ(m) = m j=0 m m-j m j (j + 1)! - m j=0 m j m! j! = m k=0 m k m m -k (m -k + 1)! - m j=0 m j m! j! = m k=0 m k m! k! (m -k + 1) - m j=0 m j m! j! = m k=0 m k m! k! (m -k) = m m k=0 m k m! k! - m k=0 km k m! k! = m+1 j=1 m j m! (j -1)! - m k=1 m k m! (k -1)! = m m+1 as required. 3.2. Proof of Lemma 3.1. For the proof of Lemma 3.1, we need a binomial sum identity first proved by Abel as well as its generalization to the multinomial case by Hurwitz. More precisely, for m ∈ N, x, y ∈ R, p, q ∈ Z, define A m (x, y; p, q) := m k=0 m k (x + k) k+p (y + m -k) m-k+q . Note that the special case p = 0, q = -1, y = 0 corresponds to the classical Abel binomial theorem: A m (x, y; 0, -1) = 1 y (x + y + m) m . Moreover, we have (1) A m (0, 0; 0, 0) = γ(m) (m ∈ N). The other function γ 2 can be expressed in terms of a multinomial version of A m . More precisely, for x 1 , . . . , x n ∈ R and p 1 , . . . , p n ∈ Z, define B m (x 1 , . . . , x n ; p 1 , . . . , p n ) := m! k 1 ! • • • k n ! n j=1 (x j + k j ) kj +pj , where the sum is taken over all non-negative integers k 1 , . . . , k n such that k 1 + • • • + k n = m. A simple calculation then shows that (2) B m (0, 0, 0; 0, 0, 0) = γ 2 (m) (m ∈ N). We can now proceed with the proof of Lemma 3.1. Proof. In [21, p.21] , we find the following identity: A m (x, y; 0, 0) = m k=0 m k k! (x + y + m) m-k Setting x = y = 0 gives γ(m) = A m (0, 0; 0, 0) = m k=0 m k k! m m-k = m j=0 m! j! m j , where we used Equation ( 1 ). This proves the first equality. For the second equality, we use [21, Equation (35) , p.25]: B m (x 1 , . . . , x n ; 0, . . . , 0) = m k=0 m k (x 1 + . . . x n + m) m-k α k (n -1) where  Remark. Theorem 3.2 not only proves Conjecture 2.1, but also gives simpler expressions for the functions ξ and ξ 2 that are more convenient from a numerical perspective. As discussed in Section 1, this might be of interest for the computation of PAC-Bayes bounds in machine learning. α k (r) := (r + k -1)! (r - Lemma 3 . 1 . 1 , 311 For m ∈ N, we have γ(m) = we can now prove Conjecture 2.1. Theorem 3 . 2 . 32 1)! .Note that if n = 3, then α k (n -1) = (k + 1)!. Setting x 1 = x 2 = x 3 = 0 then givesγ 2 (m) = B m (0, 0, 0; 0, 0, 0) = m k=0 m k m m-k (k + 1)!,where we used Equation (2). This completes the proof of the lemma.3.3. Conclusion.We now summarize what we have proved in the following theorem.For m ∈ N, define ξ(m) j m j (j + 1)! (m ∈ N). Furthermore, ξ 2 2 (m) = ξ(m) + m (m ∈ N)."
}