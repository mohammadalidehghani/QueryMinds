{
  "title": "On conditional parity as a notion of non-discrimination in machine learning",
  "abstract": "We identify conditional parity as a general notion of nondiscrimination in machine learning. In fact, several recently proposed notions of non-discrimination, including a few counterfactual notions, are instances of conditional parity. We show that conditional parity is amenable to statistical analysis by studying randomization as a general mechanism for achieving conditional parity and a kernel-based test of conditional parity.",
  "introduction": "As automated decision systems permeate our world, the problem of implicit biases in these systems have become more serious. Machine learning algorithms are routinely used to make decisions in credit, criminal justice, and education, all of which are domains protected by anti-discrimination law. Although automated decision systems seem to eliminate the biases of a human decision maker, they may perpetuate or even exacerbate biases in the data. For example, consider an advertising platform which uses demographic information of visitors to a website to decide which credit card offers to show first-time visitors. If the system is trained on historical data where minority visitors were given less advantageous offers, the system may steer similar visitors to less advantageous offers, which is illegal (Steel and Angwin, 2010) . In response, the scientific community has proposed several formal definitions of non-discrimination and various approaches to ensure algorithms are non-discriminatory. Unfortunately, the myriad of definitions and approaches hinders the adoption of this work by practitioners: they must choose from the growing list of definitions and approaches, and there is often no clear choice. In light of this plethora of definitions, we identify a general notion of nondiscrimination in Section 2 that not only includes many recently proposed definitions but also suggests new definitions. In Sections 3 and 4, we study randomization as a general mechanism for achieving conditional parity and a kernel-based test of conditional parity. Finally, in Section 5, we apply this test to determine whether insurance companies charge higher premiums to insure cars in minority neighborhoods. Table 1 A graphical representation of the groups implicit in the gender discrimination example. a = female a = male z = analyst z = associate . . . z = VP",
  "body": "As automated decision systems permeate our world, the problem of implicit biases in these systems have become more serious. Machine learning algorithms are routinely used to make decisions in credit, criminal justice, and education, all of which are domains protected by anti-discrimination law. Although automated decision systems seem to eliminate the biases of a human decision maker, they may perpetuate or even exacerbate biases in the data. For example, consider an advertising platform which uses demographic information of visitors to a website to decide which credit card offers to show first-time visitors. If the system is trained on historical data where minority visitors were given less advantageous offers, the system may steer similar visitors to less advantageous offers, which is illegal (Steel and Angwin, 2010) . In response, the scientific community has proposed several formal definitions of non-discrimination and various approaches to ensure algorithms are non-discriminatory. Unfortunately, the myriad of definitions and approaches hinders the adoption of this work by practitioners: they must choose from the growing list of definitions and approaches, and there is often no clear choice. In light of this plethora of definitions, we identify a general notion of nondiscrimination in Section 2 that not only includes many recently proposed definitions but also suggests new definitions. In Sections 3 and 4, we study randomization as a general mechanism for achieving conditional parity and a kernel-based test of conditional parity. Finally, in Section 5, we apply this test to determine whether insurance companies charge higher premiums to insure cars in minority neighborhoods. Table 1 A graphical representation of the groups implicit in the gender discrimination example. a = female a = male z = analyst z = associate . . . z = VP CONDITIONAL PARITY: A NOTION OF NON-DISCRIMINATION Intuitively, any claim of discrimination or non-discrimination depends on a comparison: a comparison between the outcome of two groups that differ only by a sensitive attribute. For instance, a claim of gender discrimination by a female employee of a consulting firm implies she was treated differently from male employees of the company in her position. Here the two groups are female and male employees that share her position. By only comparing herself to male employees in her position, she is implicitly permitting the firm to treat male employees in other positions differently. In other words, the firm is allowed to discriminate on an employee's position (eg. paying senior employees higher salaries). We see that in order to fully specify the two groups, we must not only specify the protected attribute (eg. gender) but also specify the discriminatory attribute (eg. position). Definition 2.1 (conditional parity (CP)). A random variable x satisfies parity with respect to a conditioned on z = z if the distribution of x | a, {z = z} is constant in a: L(x | a = a, z = z) = L(x | a = a , z = z) for any a, a ∈ A. Similarly, x satisfies parity with respect to a conditioned on z (without specifying a value of z) if it satisfies parity with respect to a conditioned on z = z for any z ∈ Z. In terms of independence, conditional parity is x ⊥ a | {z = z}. Table 1 is a graphical representation of the groups in the running gender discrimination example. As we shall see, many existing notions of non-discrimination such as demographic parity, equalized odds, equalized opportunity, and counterfactual fairness are all instances of CP. We remark that the definition of CP is invariant under post-processing: if x satisfies CP with respect to a conditioned on z = z, then so does f (x) for an arbitrary function f . This is especially desirable because it leads to a simple way of eliminating bias in machine learning algorithms. The intuition of identical conditional distributions that Definition 2.1 formalizes extends easily to yield approximate notions of non-discrimination. To keep things simple, we assume a is discrete. Definition 2.2 ( -conditional parity). Let d be a metric on distributions. foot_0 A random variable x satisfies -conditional parity with respect to a ∈ A conditioned on z = z if max a,a ∈A d(L(x | a = a, z = z), L(x | a = a , z = z)) ≤ for any a, a ∈ A. To wrap up, we compare CP to two other notions of non-discrimination: functional blindness and individual fairness. Definition 2.3 (functional blindness). A decision rule δ : A × Z → X satisfies functional blindness with respect to a iff δ(a, z) = δ(a , z) for any a, a ∈ A and z ∈ Z. In other words, the decision rule has no functional dependence on the protected attribute. Functional blindness, also known as fairness through unawareness, is a rudimentary but widely used notion of non-discrimination. Although intuitive, it is a weak notion that is easily circumvented because it does not rule out implicit dependence of the decision on the protected attribute. For decades, insurance companies have charged drivers in predominantly minority neighborhoods higher premiums than drivers in majority white neighborhoods. Although insurers have justified their pricing by citing a higher risk of accidents in minority neighborhoods, consumer advocates suspect the practice is merely a way around laws that ban discriminatory rate-setting: a driver's zip code is a good proxy for his or her race in segregated areas. We remark that functional blindness implies parity conditioned on z. However, if z includes attributes that are proxies for the protected attribute (eg. zip code is a proxy for race in the preceding example), enforcing CP is vacuous. After all, by including an attribute in z, we are allowing the decision rule to discriminate based on it. Definition 2.4 (individual fairness (Dwork et al., 2012) ). Let d be a metric on distributions and D be a metric on the space of individuals X . A (possibly randomized) decision rule δ satisfies -individual fairness iff it is -Lipschitz in x: d(L(δ(x 1 )), L(x 2 )) ≤ D(x 1 , x 2 ) for any x 1 , x 2 ∈ X . Individual fairness is based on the principle that two similar individuals should be treated similarly by the decision rule. The precise definition of individual fairness depends crucially on the choice of the metrics d and D. Dwork et al. (2012) suggest the metrics be chosen by a regulatory body or proposed by civil rights organizations and left open to discussion and continual refinement. Both CP and individual fairness formalize the intuition that similar individuals should be treated similarly. In CP, similar individuals are those that share discriminatory attributes. In individual fairness, similar individuals are determined by the choice of the the metric on individuals. Although Dwork et al. (2012) does not distinguish between disriminatory and protected attributes, it is possible to encode the distinction into the choice of metric on X . Demographic parity and equalized odds In this subsection, we describe several factual (as opposed to counterfactual) notions of non-discrimination and show that they are instances of CP. Definition 2.5 (demographic parity (DP)). The outcome x satisfies demographic parity if L(x | a = a) = L(x | a = a ) for any a, a ∈ A. As we can see, there is no discriminatory attribute in DP, and it is required that individuals from the group {a = a} has to be treated equally as individuals from the group {a = a }. Although in general, DP seems too coarse a notion of non-discrimination, there are some scenarios where it is suitable. For example, in the allocation of public resources, DP is a fitting notion of non-discrimination. A concrete example is public secondary school admission. Due to the public service nature of public secondary education, parents should be allowed to send their children to any school in their neighborhood, regardless of their background. In reality, such goal is often attained by lottery, meaning that random selections in the pool of applicants are made. Example 2.6 (War on Drugs). According to the American Civil Liberties Union (ACLU), \"an African American adult is 2.8 times as likely to have a misdemeanor cannabis charge filed against him or her than does an Anglo American adult\" in Washington State (Jensen and Roussell, 2016) . By comparing the likelihood of being charged without stratifying the population (eg. by prevalence of cannabis consumption), the ACLU is claiming the War on Drugs violates DP. This is an example where DP is not a suitable notion of non-discrimination: the disparity between the likelihood of being charged may be due to disparities between prevalence of cannabis consumption. Thus it is incorrect to conclude the targeting of African Americans by law enforcement from violation of DP. To avoid the problems of DP, we first segment the population by certain discriminatory attributes (eg. prevalence of cannibis consumption in the War on Drugs example) and then apply DP to each segment of the population. This led us to the notion of CP. In supervised learning, a natural instantiation of CP is equalized odds, which appeared in Hardt et al. (2016) and Zafar et al. (2017) . In terms of CP, EO is equivalent to the prediction y satisfying parity with respect to the protected attributed a conditioned on the outcome y. In other words, EO requires the individuals with the same y but differing a to be treated equally. If both y and y are binary, then in standard terminology, EO means that the probabilities of false alarms and the detection probabilities are the same under all possible values of a. Note that typically the optimal ROC curve depends on the value of a, and oftentimes, we sacrifice some efficiency to achieve EO through randomization Hardt et al. (2016) . Example 2.8 (Example 2.6 continued). In the War on Drugs example, y is whether an individual is charged, a is an individual's race, and y may be whether an individual consumes cannabis. If there is a discrepancy between the prevalence of cannibis consumption among African and Anglo Americans, then EO is more suitable notion of non-discrimination in law enforcement. Since y depends on a, even perfect prediction y = y violates DP, but it is hardly discriminatory for law enforcement to charge anyone who consumes cannabis with a misdemeanor. On the other hand, it is easy to check that the perfect predictor satisfies EO. In some applications, one of the outcomes y ∈ Y is considered \"advantaged\". For example, consider the use of historical repayment data to predict default. If the historical data contains biases against minority groups, the prediction system may echo the bias in its predictions. A possible relaxation of EO is to require people who will not default to have equal chance of getting a loan, regardless of their race. Definition 2.9 (equal opportunity Hardt et al. (2016) ). Let y = 1 be the \"advantaged\" outcome. A prediction y satisfies equalized opportunity with respect to protected attribute a and outcome y if L( y | a = a, y = 1) = L( y | a = a , y = 1) for any a, a ∈ A. It is easy to see how equalized odds leads to the more general notion of CP. The key idea of comparing segments of the population that share discriminatory attributes but differ in the protected attribute is clear. In classification, there is a natural discriminatory attribute: the outcome y. However, it is worth considering other ways of segmenting the population, even in supervised learning. Example 2.10 (gender bias in UC Berkeley admissions Bickel, Hammel and O'Connell (1977) ). In the autumn of 1973, the graduate division of UC Berkeley admitted 44% of male applicants but only 35% of female applicants, prompting allegations of gender bias in the admissions process. However, adjusting the admissions outcome by department reveals a \"small but statistically significant bias in favor of women\". Bickel, Hammel and O'Connell (1977) concluded that women tended to apply to highly competitive departments, which admit a smaller percentage of applicants, while men tended to apply to less competitive departments. In this example, including department as a discriminatory attribute leads to a qualitatively different conclusion. As an aside, this example also shows that CP generally does not imply DP. Even if the admission rates of male and female applicants are identical in all departments, the admission rates to the graduate division may still differ if male and female applicants apply to departments at different rates. Conversely, even if the admission rates of male and female applicants to the graduate division are identical, the admission rates to each department may differ. This reveals another problem of DP: it permits disparate treatment within segments of the population as long as the disparities \"cancel out\" on average. Although this is rare in practice, we point it out to emphasize CP and DP are generally incomparable. Finally, to highlight the generality of CP, we describe an application of CP in representation learning. In machine learning, feature or representation learning is the task of learning a transformation of raw data to a feature vector that is amenable to machine learning algorithms. By letting x ∈ R d be the learned feature vector, CP readily leads to a notion of non-discrimination in representation learning: L(x | a = a, z = z) = L(x | a = a , z = z) for any a, a ∈ A and z ∈ Z. As we shall see, this notion of non-discrimination has been implicitly used in natural language processing (NLP). To wrap up, we describe a post-processing method that returns a new feature vector that satisfies CP. To keep things simple, we assume x ∈ R d , a ∈ R k 1 , and z ∈ R k 2 are jointly Gaussian. Without loss of generality, let T = x T -z T B -a T Γ, where B ∈ R k 2 ×d and Γ ∈ R k 1 ×d are chosen so that E | z, a = 0. Rearranging, we have x T = a T B + z T Γ + T . A new feature vector x that satisfies x ⊥ a | z is x = (I d -P B )x, P B = B T (B T ) † . One way to estimate R(B T ) is to select a subset of feature vectors that are similar in z and compute their principal components. This is essentially the approach proposed by Bolukbasi et al. (2016) to remove gender bias in word embeddings. Example 2.11 (debiasing word embeddings (Bolukbasi et al., 2016) ). A word embedding is a representation of words by vectors in R d . Word embeddings enable machine learning algorithms to reason semantically by performing arithmetic operations on the word embeddings; eg. grandfatherman + woman = grandmother. They are learned from text corpus and inherit implicit biases in the texts. For example, according to the the popular word2vec embedding, which is trained on a corpus of Google News articles, we have engineerman + woman = homemaker. To remove gender bias in word embeddings, Bolukbasi et al. (2016) propose a method that identifies a gender subspace and projects the embedding onto the orthocomplement of the gender subspace to obtain a debiased word embedding. To identify the gender subspace, the method takes pairs of words whose meanings differ only in gender (eg. (actor, actress), (father, mother)) and estimates the principal compoments of the pairwise differences. By the invariance of CP under post-processing, the output of a machine learning algorithm based on features that satisfy CP inherits the property. This suggests using non-discriminatory features as a simple approach to eliminating bias in machine learning algorithms. Counterfactual notions of non-discrimination In order to work with counterfactuals, we must impose some modeling assumptions on the data generating process. In the rest of this subsection, we assume the data is generated by a structural equations model (SEM). A SEM consists of (i) a set of random variables, (ii) a set of (deterministic) equations that assign values to some random variables, (iii) a probability distribution that assigns values to the rest of the variables. The variables whose values are assigned by the probability distribution are called exogenous. SEM's are conveniently represented as directed acyclic graphs (DAG). The nodes represent random variables, and the edges represent direct causal relationships between variables: there is an edge from node i to node j if the equation that assigns value to variable j takes variable i as input. The nodes that have no parents represent the exogenous variables. To sample from an SEM, we start by assigning values to the root nodes by sampling from the probability distribution and recursively assign values to the other nodes by the equations. Thus the nodes whose values are assigned by equations are random variables on the probability space \"generated by\" the exogenous variables. In this setting, counterfactuals are defined as random variables whose values are assigned by a modified SEM, where the equations and/or the probability distribution are modified according to the premise of the counterfactual. We wrap up our brief overview of counterfactuals with an example and refer to Pearl, Glymour and Jewell (2016) , Chapter 4 for further details. Example 2.12. Consider the intervention a ← a and the counterfactual y a←a in the SEM depicted in Figure 1a . The counterfactual is the counterpart of y in the modified SEM depicted on the right of Figure 1b , in which the equation that assigns the value of a is replaced by the equation a = a. We see that the value of y a←a ultimately depends on the values of the exogeneous variables in the SEM, making it a random variable on the same probability space as y. Thus it is possible to evaluate \"cross-SEM\" probabilities such as L(y a←a | y = y). We remark that this SEM formalism allows us to study the effects of more sophisticated interventions such as a ∼ P a (cf. Figure 1c ). In the rest of this subsection, we describe two counterfactual notions of nondiscrimination. The first was proposed recently by Kusner et al. (2017) , while the second is suggested by CP. To keep things simple, we specialize to supervised learning and focus on prediction. u a z y (a) u a z a←a y a←a (b) u a a∼Pa z a∼Pa y a∼Pa (c) In Definition 2.13, e is the evidence we observe in the real world. Although it plays the part of z in CP, we call it evidence and denote it by e to emphasize it is observed. To see that CF is an instance of CP, let y a , a a be the counterparts of y, a in a modified SEM, where the step that assigns value to a is replaced by a ∼ unif(A), and note that (2.1) is equivalent to We remark that the law of the intervention is unimportant because we condition on the value of a. We pick a ∼ unif(A) to keep things concrete. The notion of CF is best illustrated by the following case on employment discrimination. In (1996) , the judges wrote \"the central question in any employmentdiscrimination case is whether the employer would have taken the same action had the employee been of a different race (age, sex, religion, national origin, etc.) and everything else had been the same\". In other words, to ascertain whether discrimination occurred, the judges compared the employee with his counterpart in a counterfactual world, rather than a similar employee in the real world. We remark that it may not be possible to follow the judges directive literally and keep all other attributes the same: the intervention a ← a may propagate in the modified SEM and lead to discrepancies with the evidence. For example, consider a female employee who is homosexual. In a counterfactual world where she is male, it is not possible to keep both her sexual orientation and the gender she is attracted the same as hers in the real world. As we saw, CF is an instance of CP where we segment the population by observable evidence. A related notion of non-discrimination is equalized counterfactual odds (ECO): it is an instance of CP that segments the population by counterfactual attributes. It is motivated by Example 2.14. Example 2.14. Consider a system that predicts a driver's accident risk from his or her driving record. The prediction y depends directly on a driver's driving record z, which in turn depends on the driver's driving ability u. Driving ability also directly affects a driver's accident risk y and whether he or she is disabled a (poor drivers tend to get into accidents, which cause disabilities). Figure 2 is a DAG that depicts the functional dependencies among the variables. This example does not satisfy EO: the path a ← u → z → y between a and y is not blocked. However, y is intuitively non-discriminatory: there is only dependence between a and y because driving ability is a parent of disability and driving record. the prediction y has no causal dependence on a does not penalize good drivers that happen to be disabled. We see that EO is too stringent a condition in this scenario. It not only prohibits the prediction from treating disabled drivers differently because of their disability, but also prohibits the prediction from happening to put disabled drivers in an disadvantaged position due to the presence of a confounder. In this scenario, disabled people happen to have worse driving records because driving ability affects one's driving record and causes disability. Example 2.14 shows that EO is too stringent because it prohibits probabilistic dependence between y and a, which may arise due to confounding. The notion of equalized counterfactual odds is an amendment of EO that only prohibits causal relationships between y and a. If (2.3) holds for all y ∈ Y, we say y satisfies equalized counterfactual odds with respect to a. In a nutshell, ECO is EO on a modified SEM, in which the step that assigns value to a is replaced by a ∼ P a . The graph of the modified SEM is identical to that of the original SEM, except all the edges that point to a are removed. This removes all back door paths between y and a, which typically represent the effects of confounders. Thus ECO only prohibits probabilistic dependence between y and a in the original SEM through front door paths. This leads to a simple way of verifying ECO. Lemma 2.16. A prediction y of y satisfies ECO with respect to protected attribute a if any front door paths from a to y are blocked by y. ECO is also closely related to CF: both compare the law of the counterfactual prediction y a on segments of the population. ECO segments the population by the counterfactual target y a , while CF segments the population by (observable) evidence e. In practice, CF is a fairly stringent notion of non-discrimination. As Kusner et al. (2017) point out, there are instances in which perfect prediction does not satisfy CF. On the other hand, perfect prediction always satisfies ECO. To wrap up, we present another example that highlights the difference between the two notions. Example 2.17. Consider an SEM of a church's priest hiring process. The church's hiring decision y = 1{z ≥ 1.8} depends on an applicants score z = au, where u ∼ unif(0, 2) is the applicant's propensity for priest work and a ∈ {0, 1} is whether the applicant is Christian. Figure 3 depicts an SEM of the priest hiring process. Consider an atheist applicant whose talents are well-suited to priest work (eg. charismatic, persuasive, u = 1.9). He applied for the position, but was rejected. Since he would have been hired if he was a Christian, the hiring process is not counterfactually fair with respect to a in light of evidence z. Graphically, conditioning on z in the unmodified SEM does not block the path between a a and y a in the graph of the modified SEM. On the other hand, the hiring process clearly satisfies ECO: z blocks the only front door path between a and y in the graph of the (unmodified) SEM. Before moving on, we mention a few recently proposed counterfactual notions of non-discrimination. Zhang, Wu and Wu (2016) and Nabi and Shpitser (2017) formalize discrimination as the presence of path specific effects (cf. Pearl (2009) , §4.5.3). Although path-specific notions of non-discrimination are also instances of CP, we skip the details here. Kilbertus et al. (2017) addresses the difficulty of modeling and determining the effect of intervening on protected attributes by considering non-discrimination with respect to proxies of protected attributes. To wrap up, we cite a few related works on non-discrimination in machine learning. DP as a notion of non-discrimination was studied in Zemel et al. (2013) . Friedler, Scheidegger and Venkatasubramanian (2016) extends the notion of individual fairness to distinguish between constructs, which are unobservable attributes (eg. intelligence), and observations (eg. score on IQ test), which are proxies of constructs that enter into the algorithm. Berk et al. (2017) reviews various notions of non-discrimination in the criminal justice system. CONDITIONAL PARITY BY RANDOMIZATION In supervised learning, we observe realizations of (a, s, y), where a ∈ {0, 1} is the protected attribute, s ∈ R d is a score, y is the outcome. In general, s is dependent on a. If we wish to obtain a prediction that does not depend on the protected attribute we have to sacrifice some efficiency and use a randomized procedure. In this section we consider the construction of a (randomized) decision rule y = y(s, a) such that L( y | a = a, y = y) does not depend on a. Assume for simplicity that s is discrete. In that case let f ya be the probability vector corresponding to L(s | a = a, y = y). A non-discriminatory randomization is a pair of Markov kernels, K 0 , K 1 ∈ R k×k 1 satisfying f y1 K 1 = f y0 K 0 , y ∈ {0, 1} K 0 (i, j), K 1 (i, j) ≥ 0, i = 1, . . . , k, j = 1, . . . , k 1 k 1 j=1 K m (i, j) = 1, i = 1, . . . , k, m = 0, 1. (3.1) The minor difficulty is due to the fact that the conditional density given y should be checked, while the randomization cannot depend on y which is unobserved at the time of the randomization. Lemma 3.1. In general, we need to randomize the score of both categories to achieve EO. Proof. The lemma follows from the fact that a Markov kernel is a contraction in two measures, and in general they are unrelated. Consider two sets of densities such that f 10 -f 00 1 f 11 -f 01 1 , but on a small interval, small enough such that it has a little contribution to the L 1 distance, f 10 (x)/f 00 (x) f 11 (x)/f 01 (x). If it was that f y0 K = f y1 f 11 -f 01 1 = j i f 10 (i) -f 00 (i) K 1 (i, j) ≤ i f 11 (i) -f 01 (i) j K(i, j) = f 11 -f 01 1 . which contradict the assumption. On the other hand, if it was that f y1 K = f y0 max j f 10 (j) f 00 (j) = max j i f 11 (i)K 1 (i, j) i f 01 (i)K 1 (i, j) = max j i f 11 (i) f 01 (i) f 01 (i)K 1 (i, j) i f 01 (i)K 1 (i, j) ≤ max i f 11 (i) f 01 (i) , which contradict the other assumption. Hence neither it is that f y0 K = f y1 nor that f y0 K = f y1 . The set (3.1) has 2(k + k 1 -1) equality constraints with 2kk 1 undefined parameters. It always has the trivial solution. Adding a linear cost function, eg. (3.2) for example j(i) = ik 1 /k , turns the feasibility problem into a linear program. m i j c i |j -j(i)| α K m (i, j), To avoid sparse solutions, we add another set of constraints (3.3) j jK m (i, j) is monotone non-decreasing in i for m = 1, 2. That is, the rows of K m are increasing in the mean. Example 3.2. Consider the following model in which there is a never observed latent variable z a ∈ {0, 1} z | a = a ∼ N (µ z a, τ 2 z ) s | a = a, z = z ∼ N (z + µ s a, σ 2 s ) y ∈ {0, 1} P (y = 1 | a, s, z) = p z (z). In words z is the ability of the random subject, the two groups are of different ability. Let assume that µ z , µ s ≥ 0 We have a noisy observations s which is biased in favor of the stronger group. Since z | a = a, s = s ∼ N ( σ 2 s σ 2 s + τ 2 z aµ z + τ 2 z σ 2 s + τ 2 z (s -aµ s ), σ 2 s τ 2 z σ 2 s + τ 2 z ), if we have µ s = (σ 2 s /τ 2 z )µ z then the Bayes estimator of z given a, s is the same for the two groups. In reality, biasing the score of the stronger group is not done explicitly. However, in order to improve the Bayes estimator, a culturally dependent criteria may be introduced, which in effect biases the score. This situation may seem fair. The score (e.g., the SAT) is used in the same way independently of the attribute and it is the optimal way to use the score as it is Bayes. Removal of the bias would result in an inferior selection procedure. However, this is a discriminatory policy by other criteria. In Figure 4 we present the situation. The score distributions of the two groups are different, but worse, between two subjects with the same latent \"ability\", z, the subject from the stronger group gets, on the average, a higher score s. Table 2 The (minimal) impact of the type of decision on the Breir score Decision a = 0 a = 1 Bayes decision 0.1898 0.1820 Bayes decision based on 2 categories 0.2064 0.1990 The non-discrimantory decision 0.2092 0.2001 The minimizer of (3.2) subject to the (3.1) and (3.3) constraints is presented in Figure 5 . The Breir score is presented in Table 2 . Finally, in Figure 6 we present the output distribution of the Markov kernel when k 1 = 500. The situation is more complicated when the outcome is more than binary valued, or continuous. However, it is simple enough if the outcome and raw prediction are jointly normal conditioned on the protected attribute. Consider, for example, the situation of Example 3.2. The concluding model is that s | y, a ∼ N (µ a + A a y, Σ a ). Lemma 3.3. Suppose s ∈ R ns , y ∈ R ny and A a ∈ R ns×ny , a ∈ {0, 1}, are full rank. Then, without loss of generality, we can assume µ a = 0, a = {0, 1}, and A 0 = A 1 . A minimal randomized procedure that equates the conditional distribution of s is given by y|a, s = N (s, T a ), where T a = (a/2-1)λ i <0 λ i ξ i ξ i , and (λ 1 , ξ 1 ), . . . , (λ ns , ξ ns ) is an orthonormal eigen-system of Σ 1 -Σ 0 . Proof. We can always transform s when a = 0 by the transformation s → A 1 A † 0 (s -µ 0 ), if a = 0 s -µ 1 if a = 1, which equate the conditional mean of s under a ∈ {0, 1}. Now, since T 0 -T 1 = Σ 1 -Σ 0 , L( y | y, a = a) does not depend on a. Finally, since A 1 has a full rank, E K 1 (s, •) -K 0 (s, •) | y) = 0, iff K 1 -K 0 = 0 ( the shift is a complete sufficient statistics in the multivariate normal distribution. This is achieved by the randomization given above. TESTING CONDITIONAL PARITY In this section, we describe a kernel-based approach to testing CP developed by Zhang et al. (2012) . We begin by characterizing the conditional independence condition x ⊥ a | z in terms of cross-covariance operators. Let k x , k a , and k z be positive definite kernels on X , A, and Z respectively and H x , H a , and H z be the respective reproducing kernel Hilbert spaces (RKHS). Throughout this section, we assume the kernels satisfy E k x (x, x) < ∞, E k a (a, a) < ∞, E k z (z, z) < ∞. For any probability distribution on X , its RKHS embedding is the unique µ x ∈ H x such that µ x , f x = E f (x) for any f ∈ H x for any f ∈ H x . It is well-defined because the assumptions on k x imply f → E f (x) is a bounded linear functional. By the reproducing property, we see that µ x has the explicit form µ x (x) = k x (x, •), µ x x = E k x (x, x) . The cross-covariance operator of (x, a) is an operator from H a to H x such that f, Σ x,a g x = cov f (x), g(a) for any f ∈ H x and g ∈ H a . It is the functional analogue of the covariance matrix a pair of random vectors. In terms of the kernel and the RKHS embeddings of the marginal distributions of x and a, it has the form f, Σ x,a g x = E f, k x (x, •) -µ x x k a (a, •) -µ a , g a . Letting f = k(x, •), we see that Σ x,a g is (Σ x,a g)(x) = k(x, •), Σ x,a g x = E (k x (x, x) -µ x (x)) g(a) -E g(a) . The cross-covariance operator of (x, x) is a positive self-adjoint operator and is called the covariance operator of x. The conditional cross-covariance operator of (x, a) given z is Σ x,a|z = Σ x,a -Σ x,z Σ -1 z,z Σ z,a . Under some technical conditions, Fukumizu et al. (2008) show that it is an operator from H a to H x such that f, Σ x,a|z g x = E cov f (x), g(a) | z . Before we state the functional characterization of conditional independence, we define some additional notation. The tensor product H x ⊗ H a is an RKHS equipped with the inner product f ⊗ g, x ⊗ y = f, x x g, y a . We extend this inner product to all of H x ⊗ H a by bilinearity. We see that the representer of evaluation in H x ⊗ H a is the outer product of the representers of evaluation in H x and H a : k(x, •) ⊗ k(y, •), f ⊗ g = f (x)g(y), and the kernel is the pointwise product of k x and k y : (k x • k y )((x 1 , y 1 ), (x 2 , y 2 )) = k(x 1 , •) ⊗ k(y 1 , •), k(x 2 , •) ⊗ k(y 2 , •) = k x (x 1 , x 2 )k y (y 1 , y 2 ). We are ready to state the functional characterization of conditional independence by Fukumizu et al. (2008) . Theorem 4.1 (Fukumizu, Bach and Jordan (2004) ; Fukumizu et al. (2008) ). Let k x,z := k x • k z be a kernel on X × Z and H x ⊗ H z be its RKHS. As long as k x,z • k a is a characteristic kernel foot_2 on X × Z × A and H z ⊕ R is dense in L 2 (P z ), where ⊕ denotes direct sum and R is the space of constant functions, we have Σ (x,z),a|z = 0 ⇐⇒ x ⊥ a | z. The non-trivial implication in Theorem 4.1 is the \"only if\" implication. If Σ (x,z),a|z = 0, we have 0 = E cov f (x)h(z), g(a) | z = E cov f (x), g(a) | z h(z) for any h ∈ H z , which implies cov f (x), g(a) | z = 0 as long as H z is rich enough. The assumption H z + R is dense in L 2 (P z ) ensures H z is rich enough. In light of Theorem 4.1, a natural test statistic is the Hilbert-Schmidt (HS) norm of a plug in estimator of the conditional cross-covariance operator. Let the empirical cross-covariance operator of (x, a) be Σ x,a := 1 n n i=1 k x (x i , •) ⊗ k a (a i , •) -µ x ⊗ µ a , where µ x := 1 n n i=1 k x (x i , •) (resp. µ a ) . The empirical conditional cross-covariance operator is Σ (x,z),a|z := Σ (x,z),a -Σ (x,z),z ( Σ z,z + λI z ) -1 Σ z,a , where λ > 0 is a regularization parameter. It is possible to express its HS norm in terms of the kernel matrices G x , G a , G z , where G x i,j = k x (x i , x j ) (resp. G a , G z ). Lemma 4.2. We have Σ (x,z),a|z 2 HS = 1 n 2 tr(K x,z K a ) - 2 n 2 tr(K x,z K z (K z + λM n ) † K y ) + 1 n 2 tr((K z + λM n ) † K z K x,z K z (K z + λM n ) † K y ), where Zhang et al. (2012) show that the test statistic is asymptotically a mixture of independent χ 2 1 random variables K x,z = K x • K z . n Σ (x,z),a|z 2 HS d → ∞ i=1 λ i z 2 i , , z i i.i.d. ∼ N (0, 1), and proposed two ways to approximate the asymptotic distribution. We refer to their paper for the details. DO MINORITY NEIGHBORHOODS PAY HIGHER INSURANCE PREMIUMS? It has been observed that drivers from predominantly minority zip codes are often charged higher insurance premiums than drivers from non-minority zip codes (Feltner and Heller, 2015) . The insurance industry has justified the higher premiums by arguing that drivers from minority neighborhoods have higher risk of accidents. In this section, we examine the claim of the insurance industry using the proposed framework and the data obtained by Jeff Larson (2017) . Before presenting the results, we briefly describe the data, which was obtained by Jeff Larson (2017) from Quadrant Information Services and S&P Global Inc. It consists of 98,441 insurance quotes for drivers fitting a single profile: A 30year-old female teacher with a bachelor's degree, excellent credit, no accidents or moving violations, and who is purchasing a policy for $100,000 of property damage coverage and $100,000 to cover medical bills per person up to $300,000 per accident for the first time. She drives a 2016 Toyota Camry, has a 15 mile daily commute, and drives 13,000 miles a year. The quotes are associated with the zip code of the driver, and by fixing the profile and letting zip code change, we control for factors outside of geography. The risk of drivers in a zip code is measured by the ratio of dollars paid out for liability claims to the number of insured cars. In California, this ratio is called average loss and is a measure of the cost to the insurer of insuring a car in a zip code. Ideally, we would have data on the claims from drivers that fit aforementioned profile, but, unfortunately, we do not have such fine-grained data. We refer to Jeff Larson (2017) for further details regarding the data. We tested two hypotheses in California: the quotes were independent of the percent minority population given the risk in the associated zip codes (H 1 ); the quotes were independent of whether the associated zip code is underserved given the risk (H 2 ). The California Department of Insurance defines \"underserved\" zip codes as zip codes where (i) the fraction of uninsured drivers exceeds the statewide average by at least 10%, (ii) the per capita income is below the statewide median, (iii) minorities are at least two-thirds of the population. Among the 1,648 Californian zip codes recorded in the data, there are 145 such underserved zip codes. The results are reported in Table 3 . We examine the data on Progressive Group more closely because there is a discrepancy between the results of the test of H 1 (its quotes are independent of the percent minority population given the risk) and that of H 2 (its quotes are independent of whether the associated zip code is underserved given the risk). To comprehend this discrepancy, we redefine underserved zip codes as zip codes where the percent minorities population is at least q for various values of q and test H 2 again. The results are reported in Table 4 . We see that although the percentage of minority population, as a continuous variable, does not pass the conditional independence test at 0.05 level, the minority indicator derived from it sometimes does. Mathematically, the discrepancy between the tests is unsurprising: x ⊥ a does not generally imply f (x) ⊥ y. However, its practical implication is noteworthy because it exposes one problem of thresholding a continuous protected attribute. Thresholding tolerates discrimination within subgroups (discrimination within the minority/non-minority subgroup), as long as there is no discrimination across different subgroups. We also note that the p-value when q = 0.6 in 4 is quite different from that for H 1 in 3. This is because only 33% of the zip codes where minority percentage exceeds 60% are truly underserved. In Illinois and Texas, we tested the hypothesis that the quotes were independent of the percent minority population given the risk in the associated zip codes. In Illinois, we excluded the zip codes in Chicago because Chicago has a law that require insurers to charge the same price for bodily injury insurance. The results are reported in Table 5 . Finally, we apply the randomization procedure described in Section 3 to adjust the premium. We concentrate on one insurer (Garrison Property and Casualty Insurance Company) and only on the property damage policy premium. There are two protected group. The white-non-Hispanics and the rest. However, this   attribute is not given (to the insurer and in the data) and is derived from the proportions of the the two groups within any zip-code area. Let x be the premium in the zip code, z the state defined risk and q the proportion of whites (non-Hispanic) within the zip code area. Linear regression of x on z, q, and q • z finds all coefficients to be significant at the 0.001 level. To proceed we make the (clearly unrealistic) assumption that all zip code areas have the same number of car insured by the discussed insurer. Under this assumption the distribution of the excess premiums paid by the two group (after controling for the risk) is plotted in Figure 7 (a). In this semi-artificial setup, a white customer pays, on the average 1.54 USD less than predicted, while a minority group members pays 1.65 USD more with standard deviations equal to 25.7 and 29.2 USD respectively. Since the group membership is concealed from the insurer, this cannot be corrected directly. We suggest that in such a situation a cross-subsidization between zip code areas, where the premium would have a component which is proportional to the deviation of q from its mean and a randomized component proportional to q. Practically, randomization can be achieved by making the premium depend slightly on hardly relevant information about the customer or the location. In Figure 7 (b) we present the result of this process. Both the white and non-white groups have the same mean and standard deviation (0.0 and 29.8 USD respectively). SUMMARY AND DISCUSSION We identified conditional parity as a general notion of non-discrimination in machine learning. It formalizes the implicit comparison in claims of discrimination and is applicable beyond supervised learning. It also includes many recently proposed notions of non-discrimination, including counterfactual ones. The main takeaway for practitioners is the necessity of specifying not only protected attribute but also discriminatory ones in any rigorous notion of nondiscrimination. Ignoring the discriminatory attributes may lead to ambiguous definitions. Consider the recent debate on whether no sex-based discrimination implies no discrimination based on sexual orientation (Thayer, 2017) . In Hively v. Ivy Tech, the majority opinion expressed \"common-sense reality [is] that it is actually impossible to discriminate on the basis of sexual orientation without discriminating on the basis of sex\". However, by letting the gender to which one Definition 2.7 (equalized odds (EO) Hardt et al. (2016)). A prediction y of y ∈ Y satisfies equalized odds with respect to protected attribute a and outcome y if L( y | a = a, y = y) = L( y | a = a , y = y) for any a, a ∈ A and y ∈ Y. Definition 2.13 (counterfactual fairness (CF) Kusner et al. (2017)). A prediction y is counterfactually fair with respect to sensitive attribute a in light of evidence {e = e} iff (2.1) L( y a←a | e = e) = L( y a←a | e = e) for any a, a ∈ A.If (2.1) holds for all e ∈ E, y is counterfactually fair with respect to sensitive attribute a in light of evidence e. (2.2) L( y a | e = e, a a = a) = L( y a | e = e, a a = a ) for any a, a ∈ A, Fig 2 : 2 Fig 2: SEM of accident risk prediction (Example 2.14) Definition 2.15 (equalized counterfactual odds (ECO)). A prediction y of y ∈ Y satisfies equalized counterfactual odds with respect to protected attribute a conditioned on y a = y iff (2.3) L( y a | y a = y, a a = a) = L( y a | y a = y, a a = a ) for all a, a ∈ A. Fig 3 : 3 Fig 3: SEM of priest hiring process (Example 2.17) Fig 4: (a) The conditional densities given y and a. (b) P (ŷ = 1|z) vs. z. Among two candidates with the same latent ability, the one who comes from the weaker group gets on the average a smaller Bayes decision. Fig 5 : 5 Fig 5: The Bayes estimator and the solution of the linear program nondiscrimantory algorithm. Fig 6 : 6 Fig 6: The output of the linear program. (a) The conditional density given y; (b) The expected value of the output given s conditional on a and the Bayes estimator, which is independent of a. Fig 7 : 7 Fig 7: Car insurance premium distribution before and after correction. The red broken line is for the white-non-Hispanic group. The blue full line are the rest. Table 3 3 Table of p-values for 9 major insurance companies in California Company H1 p-value H2 p-value Allstate < 0.001 < 0.001 Berkshire Hathaway 0.488 0.456 Farmers < 0.001 < 0.001 Liberty Mutual < 0.001 < 0.001 Mercury 0.925 0.648 Nationwide < 0.001 < 0.001 Progressive < 0.001 0.365 State Farm < 0.001 < 0.001 USAA < 0.001 < 0.001 Table 4 4 Table of p-values for Progressive Group Threshold (q) p-value 0.1 0.297 0.2 0.778 0.3 0.698 0.4 0.002 0.5 < 0.001 0.6 0.010 0.7 0.110 0.8 0.061 0.9 0.009 Table 5 5 Table of p-values for 9 major insurance companies in Illinois and Texas Insurance group Illinois Texas Allstate < 0.001 < 0.001 American Family < 0.001 Auto Owners < 0.001 Berkshire Hathaway < 0.001 < 0.001 Country Financial < 0.001 Erie < 0.001 Farmers < 0.001 < 0.001 Hartford Fire < 0.001 Liberty Mutual < 0.001 < 0.001 Metropolitan < 0.001 Nationwide < 0.001 Pekin < 0.001 Progressive < 0.001 < 0.001 State Farm < 0.001 < 0.001 Traverlers < 0.001 USAA < 0.001 < 0.001 Formally, the metric must satisfy d(P, Q) = d(Q, P) and d(P, Q) ≥ 0 for any pair of distributions P and Q as well as d(P, P) = 0 for any distribution P. RITOV, SUN, ZHAO We call kernel is characteristic if the RKHS embedding P → E P k(x, •) is injective. In other words, E P f (x) = E Q f (x) for all f ∈ H implies P = Q."
}