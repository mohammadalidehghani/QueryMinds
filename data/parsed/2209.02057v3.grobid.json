{
  "title": "APPLYING MACHINE LEARNING TO LIFE INSURANCE: SOME KNOWLEDGE SHARING TO MASTER IT PREPRINT",
  "abstract": "Machine Learning permeates many industries, which brings new sources of benefits for companies. However, within the life insurance industry, Machine Learning is not widely used in practice as over the past years statistical models have shown their efficiency for risk assessment. Insurers may thus face difficulties assessing the value of the artificial intelligence. Focusing on the modification of the life insurance industry over time highlights the stake of using Machine Learning for insurers and benefits that it can bring by unleashing data value. This paper reviews traditional actuarial methodologies for survival modeling and extends them with Machine Learning techniques. It points out differences with regular machine learning models and emphasizes the importance of specific implementations to face censored data with the Machine Learning models family. In complement to this article, a Python library has been developed. Different open-source Machine Learning algorithms have been adjusted to adapt the specificities of life insurance data, namely censoring and truncation. Such models can be easily applied from this SCOR library to accurately model life insurance risks. This library is briefly presented in section 5 Why consider Machine Learning for risk modeling? The life insurance industry became more complex as the modern life insurance industry now offers all kinds of products covering death events or other hazards related to the health condition of the insured. The main products are covering Mortality, Critical Illness, Disability, Medical Expenses, and Longevity. Besides, thanks to technological advances and data storage capacity improvements, information considered for risk assessment has increased a lot and is still increasing. For instance, nowadays, life insurance applicants are asked to share, besides their age, part of their medical history, financial situation, and their profession. In addition to this new information, thanks to the seniority of the industry, insurers may also have access to centuries of data accumulation to deepen their knowledge.",
  "introduction": "As the information available is increasing over time, new challenges for insurance companies and actuaries are arising, namely efficiently extracting and analyzing information from a large amount of data to assess risk better. • Machine Learning as a solution to deal with huge databases For many years, only a limited amount of information about the applicants was collected. For instance, at a time, only age, gender, and smoking status of an insured were used to assess some biometric risks. Therefore, simple models such as linear regressions or classifications were considered sufficient to grasp the risk. However, by using such simple models the potential of the current increasing amount of data in the insurance industry may not be optimally tapped into. Indeed, insurers are handling larger and larger databases, both vertically (very large amount of policies sold) and horizontally (numerous features collected for an insured). This may lead to difficulties in the calibration of simple models such as linear regressions. • And to tackle new types of data In addition to collecting more data, insurers are also collecting new types of data that cannot be handled by traditional statistical models. This is the case for textual data, or badly structured data, such as large databases that integrate correlated and non-linear relationships between variables. One can also mention real-time data collected on distributed storage systems in the cloud via connected objects. The number of steps in a day is an example of such real-time data. Thanks to the increased computing power, the insurance industry can use Machine Learning models to capture the increasingly complex information contained in these new and larger datasets.",
  "body": "As the information available is increasing over time, new challenges for insurance companies and actuaries are arising, namely efficiently extracting and analyzing information from a large amount of data to assess risk better. • Machine Learning as a solution to deal with huge databases For many years, only a limited amount of information about the applicants was collected. For instance, at a time, only age, gender, and smoking status of an insured were used to assess some biometric risks. Therefore, simple models such as linear regressions or classifications were considered sufficient to grasp the risk. However, by using such simple models the potential of the current increasing amount of data in the insurance industry may not be optimally tapped into. Indeed, insurers are handling larger and larger databases, both vertically (very large amount of policies sold) and horizontally (numerous features collected for an insured). This may lead to difficulties in the calibration of simple models such as linear regressions. • And to tackle new types of data In addition to collecting more data, insurers are also collecting new types of data that cannot be handled by traditional statistical models. This is the case for textual data, or badly structured data, such as large databases that integrate correlated and non-linear relationships between variables. One can also mention real-time data collected on distributed storage systems in the cloud via connected objects. The number of steps in a day is an example of such real-time data. Thanks to the increased computing power, the insurance industry can use Machine Learning models to capture the increasingly complex information contained in these new and larger datasets. Which insurance departments can benefit from Machine Learning risk modeling? By leveraging on the increasing amount of data collected, Machine Learning allows insurers to refine their risk modeling and understanding. This benefits all insurance departments such as -to name a few -• Underwriting: A better understanding of the main risk drivers allows a thinner assessment of the applicants' risk profiles and therefore a better accuracy in the acceptance, rejection or loading granted. In addition, the newly collected data can have a really good predictive power while being less intrusive for the applicant who provide it or less expensive/easier to access to (for instance, asking for the age of an applicant is easier than asking for blood measurements). This leads to an enhancement of the customer journey. • Pricing: Machine Learning modeling can be directly used to predict incidence/morbidity rates. Some insights from the modeling such as variable importance can help decide on the pricing risk factors to use (provided that they are in line with local regulations and ethics). • Experience Analysis: By being a tool to study complex data (correlations, ...), Machine Learning techniques allow to highlight new insights from the experience data and thus to better understand risk drivers. In particular, partial dependence, variable importance or SHAP graphs (that are underlying components of Machine Learning) are powerful tools to precisely understand risks. Such insights can then be considered to better monitor the in-force portfolios. • Product development: Machine Learning allows the use of new types of data for risk modeling and therefore the consideration of new insurance products. For instance, this is the case for the SCOR BAM products using continuous physical activity data to revamp mortality and critical illness products, making them more inclusive and simplifying the customer journey. Modeling risk in life insurance Biometric risks require to model the duration until the occurrence of an event. Such an event depends on the insurance product and what it covers. For instance: • Mortality / Longevity products require to model the lifespan which is the duration before the death of the insured, • Disability products require to model: -For the incidence rates assessment: the duration until the occurrence of disability -The termination rates assessment: the disability duration which is the period an insured will remain disabled i.e. duration before death or recovery • Critical Illness products require to model the duration before the occurrence of a covered condition • Long Term Care products require to model: -The autonomy duration which is the duration for non-disabled individuals before death or the loss of autonomy -The non-autonomy duration which is the duration for a disabled (non-autonomous) individual before death (we do not consider recovery) The underwriting field of a life insurance company aims at classifying insurance applicants from bad to good risk based on their risk profile. That means being able to rank applicants based on their probability of claim occurrence (and claim duration for risks with a termination component such as disability or long term care). Besides, the development of more precise models contributes to being more inclusive, as we may derive a price even for a very risky individual. Thus it enables to sell products to clients who would have been excluded from the portfolio in the past. For the computation of premiums and reserves, one may not limit the modeling to a binary classification, indicating whether the event has occurred or not. Indeed, it is important to be able to compute the event occurrence probability at any time for the whole duration of the contract. In other words, we seek to predict the event probability for a given period of time. Predicting time to event requires a specific modeling approach called Survival Analysis mainly due to the underlying data structure. Survival Analysis is a branch of statistics for analyzing the duration until one or more events happen. Formally, it is a collection of statistical techniques used to describe and quantify the time to a specific event such as death, disease incidence, termination of an insurance contract, recovery, or any designated event of interest that may happen to an individual. The time may be measured on different scales such as years, months, or days from the beginning of the follow-up of an individual until the occurrence of the event. The time variable is usually referred to as survival time because it gives the time that an individual has \"survived\" in his/her initial state over some follow-up period. 3 Where can I find data to model duration? Survival datasets are precious for the life insurance industry and have many purposes, such as refining risk knowledge, making life insurance more inclusive, supporting the development of new products, innovating, ... Such data can be even more important than the model itself when dealing with Machine Learning modeling. In addition, in insurance, data science will value such data even more if it is already correctly integrated in a process. Whether the data used be internal or external, the quality of the data should be correctly assessed before starting a project and collecting business requirements. Indeed, the quality of the solution that will be created is directly driven by the quality of the data. A good understanding of the business and the data is therefore crucial and should be reviewed by a qualified actuary. Internal data Insurance data is the aggregation of all the data that the company possesses related to the business: claims, underwriting questionnaires, portfolio follow-ups, ... Those data sources are frequently separated in different systems, locations and environments. If necessary, the consolidation of those systems is the very first step of an internal project. This step can be time consuming but the performance of the models will heavily depend on the quality of the data preparation . Note that this stage is sometimes not even possible. This is the case, for instance, if a primary key is missing or not recorded to link the information between different databases or tables. Proxies between common variables can be proposed but the success of the project can be compromised. External data Public institutions such as hospitals, universities, governments, ... may record relevant medical data about the population as well as its health status. More recently, innovative technological companies have started collecting new features such as physical activity or genomics to predict the life expectancy of a person or the risk of certain illnesses (cancer, heart attack, stroke, ...). Insurers and reinsurers are more than ever willing to build partnerships with those companies to collaborate on the development of new innovative models. 4 What is the specificity of survival data? Most of the time, survival duration is only partially observed. Because of it, standard Machine Learning approaches can not be directly applied. In general, survival data contain a distinctive characteristic making it impossible to directly measure survival. Indeed, while gathering the information, undesired events can occur during the observation period that pollute the records and prevent the observation of the full survival duration for all individuals. One may think of events such as Lapses, Hospital transfers, IT system failures during recordings, etc. Besides, the observation time is limited, so the event of interest, such as death, may occur outside of the time window. These characteristics are called Censoring and Truncation. Censoring and Truncation Censoring defines a situation in which the information is only partially known or observed, while Truncation corresponds to a situation in which the information is totally unknown. There are two types for each: Right and Left. • Right Censoring: Right censoring refers to an event that occurs after the end of the observation period or to the loss of the trace of a subject due to other independent reasons. In Figure 1 , subjects 3 and 4 are subject to right censoring. Even if the exact duration is not observed, right censoring still reveals partial information: the event of interest occurs after the observed time and thus the life duration is at least as long as the censoring duration. Let's consider the study of a life duration after the purchase of a life insurance contract. If the insured ends her/his contract t years after the subscription, then for this observation, we can only say that the insured survived at least t years. But her/his survival time, i.e. period until death occurs, is unknown. • Left Censoring: Left censoring is quite the opposite of right censoring. It occurs when the trigger point of the duration measure is before the observation period as it is the case for the subject 6 in Figure 1 . Once again, we only know that the real duration is longer than the one observed. One may face left censoring when a study includes individuals who have already a contract at the beginning of the observation time. The purchasing date is thus unknown. When studying life span, left censoring isn't an impediment provided that the date of birth is known. However, if the study is on the duration before lapse for instance, one can see that the left censoring is indeed making the observed information only partial. • Right Truncation: Right truncation corresponds to individuals who are completely excluded from a study because the starting event that includes them in the study happens after the end of the observation period. Considering the same example as before, right truncation is observed when an individual buys insurance after the observation period. He/she is thus totally excluded from the study. • Left Truncation: Left truncation is the opposite of Right truncation. An individual is excluded because the event of interest occurs before the beginning of the observation period. For mortality risk modeling, all individuals who are insured and whose death occurs before the observation period are left truncated and thus excluded of the study. Most of the time, when studying biometric risks in life insurance, left censoring and right truncation don't occur. Left truncation is more likely to happen when modeling life risks, but hereinafter, we will only deal with right censoring, which is the most common scenario. It is worth noting that it is possible and quite easy to consider left truncation by enhancing the modeling a bit. Why is considering censoring and truncation important? When dealing with survival data, a common mistake could be to simply ignore any censoring or truncation effects (referred to as Mistake1 below). This approach would lead to an underestimation of the probability of the event of interest. Another common mistake would be to restrict the study to observations that are complete by removing any censored or truncated records (referred to as Mistake2 below). Here as well, the estimation would be extremely biased. Let's consider the four following individuals to understand the intuition behind the importance of taking censoring into account: 1 2 3 4 Duration observed 7.1 4.9 3.4 3 Real duration 7.1 6 7 3 Status D C C D Based on the data illustrated above, not considering censoring appropriately would lead to: • Mistake1: the average survival duration is 4.6 when considering censored time (i.e Duration observed), • Mistake2: the average survival duration is 5.05 when removing the censored observations (i.e removing observations 2 and 3), • While the real average survival duration is 5.775. In both cases of this example, life expectancy is underestimated when the partial information coming from censored individuals is ignored. SCOR Python survival models library A library in Python was developed to apply all the presented methods. This library adapts the existing open-source library of Machine Learning and deep learning to our needs in actuarial science. This package was developed with the Data Analytics team at SCOR and is distributed for all SCOR collaborators. The modules customize some existing statistical and Machine Learning models: sklearn, pygam, statsmodels, lifelines, scikit-survival, lightgbm and catboost. In addition, elements helping the interpretation of models, such as Shapley values, partial dependences, ..., have been adapted to our survival models. Figure 4: Time-to-event model classes Predicting life duration in practice This section provides a high level overview of techniques to model duration and is addressed to non-expert readers. Deeper explanations for each step of the process are available in the following sections. We will focus on the use of the methods implemented within the Python library and their interest on a business matter to automate and facilitate the mortality modeling of an insurance portfolio. The calculations are performed on the NHANES database as it is an open-source mortality experience database that contains a significant amount of information. 6 Dataset presentation 6.1 NHANES overview NHANES is the acronym for National Health and Nutrition Examination Survey, which is a study program designed to assess the health and nutritional status of adults and children in the United States. In this program, information is collected through interviews and examinations. Interviews gather declarative information about a participant's lifestyle (e.g. drinking and smoking habits) and selfassessment of health status. The information from such interviews is rather qualitative. For their part, physical examinations allow to collect objective quantitative information about a participant's characteristics and health status -such as height, weight, blood pressure or cholesterol, to name a few. Ultimately, more than 5,000 variables are recorded and they can be categorized into 5 classes: demography, dietary, laboratory, examination and questionnaire. Besides these different risk factors, the observed duration in months is indicated for each individual. An indicator variable specifies whether the end of the observation period is due to the death or to another independent event. The following figure illustrates the structure of the NHANES dataset for six individuals with the extraction of eight risk factors. Figure 5 : NHANES information extraction NHANES subset used for our study The aim of our study is to predict the duration before death from multiple risk factors (using observation duration and death indicator variables available for each individual). To that extent, we initially used a subset of the entire NHANES dataset which included more than 65,000 observed individuals and 106 explanatory variables. Then, following our analysis, we only consider the most meaningful twenty-nine variables described in Appendix 26 (in addition to the duration and death indicator). Some of the variables indeed contain too many missing values or redundant information. Therefore we excluded them. The NHANES program aims at being representative of the whole non-institutionalized US population. To that extent, a weight is associated to each person studied in order to readjust the sample to the non-institutionalized national population. In the following, we will ignore the weights and consider each line as an individual. The purpose of our study is to highlight how some factors may impact mortality. We do not have an interest in the representativeness in terms of the American population, therefore it is acceptable to ignore the weights. However, we want the insights of this study to be valid in a life insurance context. Consequently, and based on the available variables, we try to replicate a simplified life insurance underwriting process to create a fictive life insurance portfolio: only participants who would have been accepted upon life insurance application have been kept. For instance individuals with too high BMIs or some pre-existing conditions were excluded. Finally 29, 870 individuals are available to derive mortality patterns based on twenty-nine risk factors. 7 Data pre-processing The quality of the data is essential to any project. As such, we begin by pre-processing the data. How should I tackle missing information? The proportion of missing values is quite significant for some of our variables, which means that we have to apply missing value imputation to properly train the models. Two approaches have been considered, depending on the variable type. For numerical variables, the median of the series was assigned to all unspecified values. For categorical variables, a new category was created for each variable as 'Missing value'. This might enable us to highlight potential patterns specific to individuals with a given missing variable. From raw data 5, we get the following complete table: How can I include categorical variable into models? As some models cannot deal with categorical features, we apply one-hot encoding to all factor variables. One-hot encoding is a representation of categorical variables as binary vectors. This method produces a vector with length equal to the number of categories in the data set. If a data point belongs to the i th category, then components of this vector are assigned the value 0 except for the i th component, which is assigned a value of 1. In this way, one can keep track of the categories in a numerically meaningful way. Figure 7 illustrates the process for the marital status variable of the raw table 5: Creation of the \"Pseudo data tables\" Pseudo data tables are detailed in section 17.3.1. In the original dataset, the follow-up time in months, as well as a death indicator, are available, which corresponds to a traditional survival data table. However depending on the modeling strategy (cf Section 17), using pseudo data tables may be necessary. It consists in discretizing the data and computing exposures to risk. Such a transformation of the data into pseudo survival data tables is an important step in the pre-processing of survival data. In our study, we consider the survival time in years rather than in months. A yearly basis seems granular enough for life insurance mortality modeling purposes. Besides, this choice enables us to considerably reduce the number of rows in the discetization and thus the computation time. Considering the approach described in section 17.3.1, for each observation (i.e. individual) we create as many rows as the number of years the individual was observed for. For each one of the created rows, we add an explanatory variable: duration. \"Duration\" represents the time interval for which exposures (the initial and the central) are computed. The final step is to modify the time-varying variables. In this case, age and year are the only time-varying variables. Thus, we only have to increment them by one in each interval. This means computing the Attained Age (respectively Attained Year) by summing up the age (respectively year) at the start of observation and the yearly duration. Note that in order to ease calculations, we assume the same day and month for the date of birth and the date of observation. Within our SCOR survival library, a specific class has been created to transform of a survival data table into a pseudo data table: The application of this function to the last record of the data in Figure 5 (seventy-year-old married man) produces the following output: Splitting the data into train and test sets Before calibrating different models, we split our data into a train and a test sample by making sure the split is stratified and the mortality contained within the two subsets is equivalent to the global mortality of 9%. Model calibration In data science, the calibration of model hyper-parameters is essential as it impacts the model performance. It is advised to use a separate hold-out to perform the research of hyper-parameters with k-fold cross-validation. The calibration may be challenging as it often implies long and tedious calculations. This is especially the case for tree-based models (hyper-parameters are depth, leaf sizes, estimator numbers, learning rate, etc.) The lack of computation resources to train a model may lead to a too tiny grid of parameters to test (for instance, when an XGBoost is trained with an early stop on the number of estimators). This could lead to over-fitting the model, which users should be aware of, with the risk of reaching local optimums (while testing larger ranges of parameters minimizes this risk). Based on this thought, within the library, every model is extended to implement a scikit-learn interface. This allows parametrization before training a model, and to keep the standard syntax commonly used by Python users. A default grid of parameters is also suggested as a first very parsimonious model, based on SCOR experience. Thus, it eases the implementation of survival models by using default settings that minimize risk of mis-parametrisation. As illustrated in Figure 9 for the CatBoost model, the parameter choices imply very different performance results. We applied a grid search method to calibrate them based on two performance metrics. Based on the AUC (cf Section 21), the best model is obtained for a fit with 150 estimators of depth 8. To keep the SMR close to 100% (cf Section 18), we retained a learning rate of 0.07. This calibration appears to us as a good balance, as it provides good results on both metrics considered, AUC and SMR. To avoid overfitting, it is important to tune a model with a similar AUC performance on the train and test datasets. The presence of over-fitting is highlighted in the figure, as for many deep trees and high learning rates, the AUC is decreasing on the test set. All the models presented in the following have been calibrated through a k-fold cross-validation grid search based on the weighted AUC (respectively the C-Index (cf Section 19)) for discrete models (respectively continuous models). 9 How can I ensure that a model is well fitted? A survival model is an estimator that should respect certain mathematical properties. The complexity of the models can lead to biased or inadequate models. Using inaccurate models in insurance is a threat to the business. Constant monitoring of the development process and the usage of the model is mandatory to reduce the risk. Our main focus is to obtain a non-biased model. Mathematically, the non-bias property for parametric models is: E X [E Y [Y |X]] = E Y [Y ] (1) This equation states that the weighted average of the predictions is equal to the actual risk measured on the data. The model is expected to replicate the average mortality of the dataset. This equation consideration should always hold on the training set. This is the very first test to conduct after the calibration of a model. Within the library developed, a function implements this test for all continuous (cf Section 13) and discrete models (cf Section 17). Continuous models For continuous models, this test is the comparison of the average of the survival curves predicted by the model with the Kaplan-Meier (cf Section 15.1) estimation (cf Figure 10 ). Even if it seems that on average the mortality is a little overestimated by our models for the high duration, we can still consider that all our models are acceptable. Only the Cox-XGBoost model is not contained within the Kaplan-Meier confidence interval for the last duration. The increase of the gap (predicted survival VS Kaplan-Meier survival) over duration is expected. Indeed, the survival curve being cumulative, small errors from previous durations are added successively. To go beyond graphical validation, one can opt for statistical tests to determine whether two survival curves (here the average of the survival curves predicted VS the Kaplan-Meier survival curves) can be considered identical or not. Such tests include but are not limited to the Log-Rank test, the Wilcoxon test, or the Kolmogorov-Smirnov test. Note that specific care is needed when comparing crossing survival curves. Indeed, in this context, traditional tests may lead to misleading results and thus alternative statistical tests should be considered (eg. weighted Log-Rank test or modified Kolmogorov-Smirnov test to name a few). 9.2 Discrete models For discrete models, the validation test consists in computing the ratio of the number of observed deaths to the number of deaths predicted by the model: this is the SMR. Table 1: Validation metrics for discrete models Models SMR Train SMR Test Binomial Regression 0.99 0.99 Poisson Regression 1.00 0.98 logistic GAM 1.00 0.96 Random Forest 0.99 0.99 LightGBM 0.99 0.98 XGBoost 0.99 0.94 Catboost 1.01 0.99 The SMR must be close to 1 on the train set if the model is well trained. Based on the table above, all our models seem acceptable as the SMR on the train and test sets are close enough to 1. A deeper comparison will be conducted to define which model has the best predictive performance. However, another criteria to consider is the difference in SMR between the train and the test sets. If such a difference is significant, this highlights that the model is over-fitting. The XGBoost model appears to be less effective than the other ones as it highly relies on the hyper-parameter choice. As it requires more computation capacity than the others, it is quite difficult to test a large grid of hyper-parameters. The logistic GAM model seems to be over-fitting as well. Indeed, the model fits perfectly the train set but is biased on the test set. As the standard error on the test set is equal to 5.3%, this bias on the test set can be acceptable (as a perfect accuracy of 100% still belongs to the confidence interval of the SMR on test set) 10 How can I understand a model? The Partial Dependence above showcases a decreasing marginal effect of the number of steps on the predicted mortality (with potentially a flattening of the effect after a steps threshold). It seems that the random forest model is less effective in capturing this trend as its Partial Dependence curve is almost flat. It highlights that the effect seems to be non-linear, which lets us conclude that the mortality will be better-modeled with Machine Learning or non-linear models. Variable importance For all our models, the variable Age is the one with the highest impact. As the impact of age is way higher than the others, we remove it from the charts for the sake of visibility. In the figures below, the Variable Importance (cf Section 24) is plotted for all our implemented models. We can observe that even if the values are little different from a model to another, the ranking of the variables is quite similar. Since the models are fitted on different data, (pseudo data tables or survival data tables depending if the model is continuous or discrete) two specific figures are provided by type of model. Regarding the variable YEAR, we can note that it is considered only in the discrete models as duration is embedded in the continuous model. from s c o r _ s u r v i v a l import a n a l y s i s m o d e l s = [ l o g i s t i c , p o i s s o n , r f , l i g h t g b m , x g b o o s t , gam , c a t b o o s t ] a n a l y s i s . p l o t _ v a r _ i m p o r t a n c e ( models , X, e x p o s i t i o n , e v e n t ) Figure 13: Variable Importance for discrete models For both continuous and discrete models, methods based on the bagging of trees seem to be accentuating the importance of some risk factors compared to other models. Based on the figures, it seems that the highest impact of mortality comes from the socio-economics and general health features, which may be seen through the importance of the family income and the general health conditions and habits, such as being active and consuming alcohol or cigarettes. This phenomenon has been highlighted in some social science studies, such as the one published by Galea et al. (2011) , which concludes that the number of deaths attributable to social factors in the United States is comparable to the number attributed to pathophysiological causes. 11 How can I choose the most suitable model? The area under a ROC curve, which is named AUC, represents an estimation of the performance level. A perfectly predicting model (perfect ROC curve) would have an AUC of 1 whereas a randomly predicting model (ROC curve is the diagonal from origin to top right corner) would have an AUC of 0.5. The figure 14 highlights that all our models have quite good performance levels. As expected, all of them show a better performance on the train dataset than on the test one. The performance and, consequently, the ranking of our models, are assessed based on the test data set. If the XGBoost model seems to be the best one on the train set, it actually may be subject to over-fitting as it is no longer the best one on the test set. Based on the test dataset, the AUC is highest for the CatBoost model. 11.1.2 C-Index Refer to section 4.2 for details on C-Index For continuous models, the C-index measure could be considered as an equivalent to the AUC as it also gives insights on the risk ranking capacity of a model. Besides, it has the same order of magnitude as the AUC. An important gap between the metrics obtained on the train and test datasets indicates that the model is not perfectly calibrated. The Cox-XGBoost model seems to be slightly over-fitted, even if the C-Index on the test set is still the best one. Based on this criterion, a Cox-Net model would be preferred as it seems to be more robust.  It may be interesting to compare the predictive capacities of our model on different subgroups rather than globally. Indeed, as explained at the beginning, an insurer needs to produce consistent predictions for each category as the data used to calibrate the model is not necessarily representative of his portfolio. In other words, we expect that the prediction is contained within the confidence interval of the observed mortality for each subgroup. When considering the mortality prediction by year of entry into the study program, it seems that it is the case for all our models. When focusing on mortality prediction by sleep hours, it seems that some models, mainly the random forest and the lightgbm, may underestimate the observed mortality of the dataset. However, the selection of a model should not rely only on performance metrics, especially because model performance highly depends on the dataset on which the model is evaluated. From a business angle, many other factors can influence the model selection. Convergence issue For some models, such as GAM or GLM, a gradient descent algorithm is used to estimate the parameters. Such a gradient descent algorithm can face convergence issues, especially in the situations depicted below: • If the objective function is not strictly convex, the optimization algorithm can descend into a local solution. • If some variables are correlated, the problem can be intractable, and the algorithm will return an error. • A large number of features (and a high degree of freedom for GAM) increases the dimension of the parameters space. The algorithm may struggle to find the optimal solution to the problem. Within the library, the logistic GLM and GAM regressions return the logs of the internal function and raise a warning to the user in case of non convergence of the gradient descent algorithm. Edge effect issue A model is efficient where the data have been observed. When extrapolating the data with a model, the user should be very careful. In survival analysis, when the survival dataset is converted into a pseudo data table, the attained year is a numeric information. If a GAM or a GLM can regress against this variable and propose an average trend, a classification tree will only locally approximate the mortality. As such, for classification trees, the shape of the mortality prediction by attained year will flatten with increasing unobserved attained years. 11.4 The potential of extending the goodness of fit of models: transfer learning Within life insurance, products differ in every country. However, depending on market maturity, experience might not always be available. One could use market benchmark data as initial input for pricing, but if it is not available, leveraging on similar market data could be useful. Nevertheless, it is not always compliant to transfer one dataset from one country to another. An alternative is then to export the knowledge accumulated on a specific dataset through the built risk model. The practice of using such a model as a starting point, called transfer learning, is a common practice in Machine Learning to face the lack of data or capacity to fit one model. Usually, when it only consists in applying a model in a context that differs from its initial purpose, it could lead to bad performance. However, when models are fitted to capture actual interactions between biometric parameters and studying the influence of explanatory features on a life event (such as death), we could expect a generalization capacity. To benefit from this generalization effect, Machine Learning processes should then be controlled to not only reach high predictive performance but also to demonstrate a certain interpretability and robustness level. In practice, such control is feasible by using not only a larger panel of survival models (as presented in this document) but also transparency tooling as presented in Delcaillau et al. (2020) . This enhances expert review (with doctors, underwriters,etc.) and the capacity to explain models and control the interactions models can capture. Hence, this document helps in better understanding how to leverage on Machine Learning and adapt models to make them applicable to survival modeling. However, readers should be aware that applying such models cannot be reduced to only using a Python library (even if this one has been developed to limit operational risks). It is necessary to combine business knowledge and expert experience with the outcomes the models provide. Business constraints The model calibration easiness, mainly the model sensitivity to hyper-parameters and the computation time, are important factors on the operational angle. Models may be subject to frequent updates thus could be often re-calibrated. As seen above, Random Forest or XGBoost models require expensive computing resources to be very well calibrated while regression models are generally more straightforward. Other important factors are the robustness and interpretability of the model. Regulation requires insurers to be able to explain every decision as well as their assessment of risk factors. In this respect, a simple Binomial model could be preferred compared to the XGBoost algorithm. However, this could change in the future thanks to the development of interpretability methods. More complex models, such as Random Forest or XGBoost, have better predictive power as they can capture variables interactions and non-linear patterns. But this comes with costs regarding the calibration easiness and interpretability. An insurer has to find the right balance between model precision and business constraints. Based on these considerations, it seems that all our models have their strengths and weaknesses. On the NHANES dataset, the CatBoost model appears as the best compromise in terms of prediction performance, computation time, and calibration. Let's indeed remember that a CatBoost model can deal with categorical feature without prior transformation, which facilitates the process compared to all other models.  To give better insights into the comparison of the models based on this fictive portfolio, we will measure the financial impact of the different modeling strategies for an insurer. 12 How to derive the premium price based on estimated mortality Survival models are used to forecast mortality rates for the insurance contract duration according to applicant characteristics. To illustrate the importance of a good mortality prediction for a life insurance company, we will consider the pricing of a policy on a competitive market. Through a simulated competitive market between two insurers, we will give insights into the capacity of a model to price correctly simple products. For our purpose, we consider a simplified contract similar to mortality products that can be found in the US market (cf Figure 16 ). Given a premium π paid at time 0, the underwriting date, the insured's beneficiary will earn C, fixed at $ 100.000, in case of death in the following ten years. For simplicity matters, we will consider a constant interest rate i = 1.5% and we denote, v, the discount rate so that v = 1 1+i . However, for business matters, an interest rate curve should be considered. The pure premium of this life insurance contract is defined as the expected claim cost given the applicant's characteristics, that is to say: π(X) = E[Cv T | X] = C 10 t=1 P(T = t | X)v t = C 10 t=1 S(t | X)h(t | X) × v t Thus, the premium can be defined as a function of the survival curve: π(X) = f v (S(.|X)). The function f v is decreasing: the higher the survival probability, the lower the insurance price. The insurer is indeed less likely to pay the claim if the death probability is low. From the previous formula, it becomes obvious that a binary classification (deaths VS alive) model is not enough to price life insurance contracts as we have to be able to predict survival at several periods. Also, insurers need to precisely price insurance products given life insurance purchaser characteristics. To compare the different premium estimations between the models, the figure below represents the average premium for ages from twenty to eighty years old. Based on it, the age seems to impact the premium level exponentially for all models. The event of interest being death in the ten coming years, it indeed highly depends on one's age. Ultimately, the premium increases until almost reaching the sum insured. Indeed the oldest applicants are very unlikely to survive the insurance coverage period (10 years) and thus the insurer is likely to pay the claim. With a quasi-certainty to pay the claim, the expected claim cost for the insurer -which is the pure premium -is nearing the sum insured. Survival analysis theory Two main modeling strategies exist to take censoring into account: fitting specific models to raw survival data or modifying the data structure to be able to apply standard models. This chapter will introduce the survival analysis theory: the data challenges and the modeling strategies, on which Machine Learning modeling relies. Survival Data Before introducing the different models, the structure of survival data should be introduced. The whole theory relies on two random variables: T the time to event and C the censoring time. They are assumed to be independent. The censoring time is a random variable modeling the observation period of an individual, that is to say, the time between the start of the observation of the individual and the time of withdrawal or loss of tracking. The time to event is a random variable modeling the observation period between the start of observation and the studied event occurrence, for instance, the death. However, in practice, the information available in the datasets is the stopping time of observation (because of death or censoring) and an indicator of whether the observation is censored. That is to say: Y = min(T, C) δ = 1 {T ≤C} (2) Considering Age and Gender as explanatory variables at t = 0, a survival data table can be built as in Table 5 for the four previous observed individuals in Figure 2 . The columns are divided into three categories, the first two columns representing the risk factors, Y being the end of the follow-up period observed, and δ an indicator of death. From this example, we can deduce that the first individual, who is a forty-year-old woman dies after 7 years whereas the second individual is censored after 4.9 years. Quantity of interest The theory focuses on two functions as the quantity of interest to estimate: the survival function S and the hazard function h. Having an estimation of one of them allows to fully model the survival of an individual. The survival function S represents the probability that the time to event is not earlier than a specific time t: S(t) = P r(T t) (3) The survival function decreases from 1 to 0. The meaning of a probability equal to 1 at the starting time is that 100% of the observed subjects are alive when the study starts: none of the events of interest have occurred. From this quantity, we can define the cumulative death distribution function F (t) = 1 -S(t) and the density function f (t) = dF (t) dt = -dS(t) dt for continuous cases and f (t) = [F (t+∆t)-F (t)] ∆t for discrete cases. The relationships between these functions is shown in Figure 18 . 2017 )) The second quantity of interest is the hazard function h. It indicates the occurrence rate of an event at time t, given that no event occurred before. Formally, the hazard rate function is defined as: h(t) = lim ∆t→0 P r(t T t + ∆t|T t) ∆t = lim ∆t→0 F (t + ∆t) -F (t) ∆tS(t) = - d log S(t) d t (4) From this equation we can easily derive that S(t) = exp(- t 0 h(s)ds) = exp(-H(t)) where H(t) = -t 0 h(s)ds is called the cumulative hazard function. Using the same notation as before, we can define a likelihood function taking into account censoring: L = i P (T = t i ) δi P (T > t i ) 1-δi = i h(t i ) δi S(t i ) (5) The intuition of the function comes from the contribution to the likelihood function between a censored and a fullobserved individual: • If an individual dies at time t i , its contribution to the likelihood function is indeed the density that can be written as S(t i )h(t i ). • If the individual is still alive at t i , all we know is that the lifetime exceeds t i , which means that the contribution to the likelihood function is S(t i ). 15 Non-parametric survival model Non-parametric models present many advantages. Contrary to parametric ones, for which convergence issues may appear during the optimization steps due to correlation or non convexity, non-parametric models will always produce the desired estimator. Then, due to the reliance on fewer assumptions, non-parametric methods are quite robust and may be applied even in situations where little is known about the application in question. Finally they are less time and memory consuming, which presents a real goal when dealing with large amounts of data. Kaplan-Meier estimator When we have no censored observations in the data, the empirical survival function is estimated by: Ŝ(t) = 1 n n i=1 1 ti≤t (6) This estimation is no longer viable in presence of censoring as we do not observe the death time t i but the end of observation time y i . Thus Kaplan and Meier (1958) extended the non-parametric estimation to censored data (see Appendix 26 for details): Ŝ(t) = ti≤t (1 - d i N i ) δi (7) Based on the four individuals of Figure 2 , the survival probability at time 5 is equal to Ŝ(5) = (1 -1 4 ) 1 × (1 -0 3 ) 0 × (1 -0 2 ) 0 = 3 4 Computing this value for each period enables us to obtain a step function for the survival function where the jumps are observed at the empirical observed death times. As introduced before, ignoring censoring leads to an underestimation of the life duration. Figure 19 highlights this underestimation. Three 'Kaplan-Meier' survival curves are plotted on different datasets: the real one, the one relying only on fully observed individuals, and the one for which censored and dead individuals are not distinguished. As the two last curves are below the real one, it means that at each time the survival probability is lower and thus that risks have been overestimated. Kaplan-Meier estimator is the most widely used because of its simplicity to compute. It is implemented in many survival libraries and packages of statistical and mathematical software. Besides, this estimator doesn't rely on any assumption and can thus easily be used as a reference model or to test hypothesis. It is effective to get the survival curve of the global population. However, the precision of the estimation relies on the number of observations. If we want to take into account individuals' characteristics, we need to recompute the estimator for each chosen subset, which reduces the number of observations and thus the accuracy. On the business side, it is indeed important to have a good prediction among different subgroups rather than on the global level. The insurer portfolio may indeed have an over-representation of some individuals compared to the population used to build the model, keeping in mind that the insured population has lower mortality compared to the global population. Nelson-Aalen Instead of estimating the survival function, another method has been developed by Aalen (1978) and Nelson (1972) to estimate the cumulative hazard function. Using the previous notations, it is defined as: Ĥ(t) = ti≤t d i N i δ i To get an estimator of the survival function, one only has to plug-in the cumulative hazard estimator into the formula S(t) = e -H(t) , in the example, Ŝ(5) = exp -1 4 = 0.77 Ŝ(t) = e -Ĥ(t) = ti≤t (e - d i N i ) δi ≈ ti≤t (1 - d i N i ) δi If the number of deaths is small compared to the number of people at risk at each time, the Nelson-Aalen plug-in survival function can be approximated by the Kaplan-Meier estimator. The two estimators are thus numerically close, but they have different properties, which implies different confidence intervals or median times (see Rodrìguez (2001) for details on estimator properties). Cox Model Cox's model Cox (1972) allows to take the effect of covariates into account and to measure their impacts through the estimation of the hazard function. It then possible becomes to rank people's risk according to their characteristics. This model may be considered as a regression model for survival data, which is a particular case of the proportional hazard models. Such models are expressed as a multiplicative effect of the covariates on the hazard function through the expression: h(t | X) = h 0 (t) × g(β X) (8) X the vector of covariates which must be time-independent g a positive function β the parameter of interest h 0 the baseline hazard function for individuals with X=0. Given two individuals A and B with covariates X A and X B respectively, the ratio of their hazard functions is assumed to be unchanged over time. That is the reason why such models are said to be proportional hazard models. For the particular case of the Cox model, the function g is an exponential function as the Cox model is defined by h(t | X) = h 0 (t) × exp(β X). The main interest of the model is the possibility to rank people on their risk level without computing the survival function. The relative risk is introduced to this end: RR = h(t|X A ) h(t|X B ) = exp(β(X A -X B )) (9) The estimation can be divided into two steps. Depending on the purpose of the study, one may stop at the first one. Estimation of the risk parameter β We compute the estimator β by maximizing the partial likelihood function defined by Cox (cf Appendix 26) : L(β) = m i=1 e X j (i) β j∈Ri e X j β (10) m the total of uncensored individuals j (i) the individuals who died at time t (i) t (i) , ..., t (m) the ordered time of observed death events R i the risk set, which is a set of indices of the subjects that were still alive just before t (i) : R i = {j : t j ≤ t (i) } Based on the same four observed individuals in Figure 2 and considering Female as the baseline category, L(β) = e 60β2 e 60β2 + e β1+52β2 + e β1+30β2 + e 40β2 × e 40β2 e 40β2 (11) The maximum likelihood estimators are obtained for β1 = -10.17 and β2 = 0.84. When one is only interested in comparing the survival curve to classify individuals according to their survival probabilities, only the estimation of the risk parameter β is needed. The baseline hazard h 0 (t) does not have any effect on the relative risk. Indeed, in this case we can deduce that a Woman will have a higher mortality compared to a Man, and that the mortality increases with the Age. Estimation of the baseline function If one needs the survival function for every individual, for premium computation for instance, the hazard baseline h 0 (t) is needed in addition of the β parameters. The survival function can be computed as follows: Ŝ(t) = exp(-H 0 (t)exp(X β)) = S 0 (t) exp(X β) (12) where H 0 (t) is the cumulative baseline hazard function, and S 0 (t) = exp(-H 0 (t)) represents the baseline survival function. Breslow's estimator is the most widely used method to estimate Ŝ0 (t) = exp(-Ĥ0 (t)) where Ĥ0 (t) = ti<t ĥ0 (t i ) with ĥ0 (t i ) = 1 j∈Ri e Xj β if t i is a time event, 0 otherwise Based on our example, with the observed individuals of Figure 2 , we can deduce: Ŝ0 (5) = 1 and thus for a 43 year-old-man Ŝ(5) = 1 exp(-10.17+43×0.84) = 0.99 and for a 43 year-old-woman Ŝ(5) = 1 exp(-10.17) = 0.56. The average mortality between the two individuals is 0.77, which is indeed the global mortality found with the Kaplan-Meier estimator. 17 Exposures: an actuarial approach Another method, widely used in actuarial science, specifically to build mortality tables, consists in discretizing the data into small time intervals. The discretization enables us to apply traditional methodologies to predict mortality, as it removes the lack of information due to censoring thanks to the exposure to risk. Withdrawal from the study of the censored subjects introduces bias if we compute traditional estimators. The mortality rate, q j , within a time interval [τ j , τ j + 1], denoted interval j, can no longer be estimated with the ratio of the deaths, d j , on the number of alive subjects at the beginning of the interval, l j . The quantity dj lj is indeed an inaccurate estimation as deaths that occur after withdrawal will not be known. Therefore, withdrawing life is only exposed to the risk of death. To compensate for the withdrawal, the number of alive subjects, l j , is replaced by the number of subjects exposed to risk. Depending on the hypothesis made on mortality, several types of exposure can be considered. Three exposures are considered by actuaries: Distributed exposure, Initial exposure and Central exposure. However, we will only focus on the last two as the distributed exposure method relying on uniform distribution of deaths is not currently a widely-used one (see Atkinson and McGarry (2016) for details). Initial Exposure and Balducci hypothesis We denote initial exposure the quantity EI j , which represents the global amount of time each life was exposed to the risk of death during the interval j. As the exposure is based on the lives at the start of the interval the exposure can be referred to as initial. EI j is the aggregation of the following individual exposure, ei j : • Alive at the start and the end of the interval are assigned 1 • Deaths during the time interval are assigned 1 • Censored are assigned the fraction of the interval during which they were observed Formally, if we respectively denote c i,j and t i,j the censoring and death time of the individual i in interval j, w j the number of withdrawals and l j the number of alive subjects, the initial exposure is expressed as (see Appendix ?? for intuition on the formula): EI j = lj i 1 {ti,j >1} × 1 {ci,j >1} + 1 {ti,j <1} + c i,j 1 {ci,j <1} = lj i 1 -1 {ci,j <1} + c i,j 1 {ci,j <1} = l j -w j + wj i=1 c i,j To understand the idea behind the quantity, let's define the two following notations for the rate of mortality: • q j = P(T ≤ τ j + 1|T > τ j ) in interval j • ci,j q j = P(T ≤ τ j + c i,j |T > τ j ) for the one in the interval [τ j , c i,j ] The number of deaths can be expressed as the sum of deaths observed within the interval and deaths expected for the censored subjects. Formally: d j = (l j -w j )q j + wj i=1 ci,j q j = l j q j - wj i=1 1-ci,j q j+ci,j (13) The Balducci hypothesis supposes that mortality rates decrease over the interval and are defined as: 1-ci,j q j+ci,j = P (T i ≤ τ j + 1|T i > τ j + c i,j ) = (1 -c i,j )P (T i ≤ τ j + 1|T i > τ j ) = (1 -c i,j )q j Injecting it in the previous equation gives: d j = l j q j -q j M i (1 -c i,j ) Solving the formula for q j : qj = d j l j - wj i=1 (1 -c i,j ) = d j l j -w j + wj i=1 c i,j = d j EI j We finally get the rate of mortality estimator corrected for censoring with the previous definition of initial exposure as expected. This approach relies on the Balducci assumtion, which generally does not fit for mortality, as mortality rates increase with time. However, withdrawals are usually small compared to the population, which allows to ignore these errors. Central Exposure and constant hazard function assumption Depending on the mortality observed within a dataset, one may prefer to use another assumption: the constant hazard function over each time interval. In this case, another exposure should be used. The central exposure, EC j is the time individuals are observed within the interval. The difference with the initial exposure is that only individuals who survived during the whole time interval are assigned 1. The constant hazard function assumption implies that the hazard is constant over each time interval. For e ∈ [0, 1], we denote h j the hazard rate over the interval [τ j , τ j + 1]: h(τ j + e) = h j (14) As long as we consider small enough time intervals, this hypothesis is acceptable. When h j is known for each j, the survival function is easy to compute: S(τ j + e) = exp(- τj +e 0 h(s)ds) = exp(- j-1 s=1 h s + eh j ) ( 15 ) The goal is then to estimate each h j . Let ec i,j be the individual central exposure, which corresponds to the time one is observed within an interval. In addition, δ i,j is a death indicator in [τ j , τ j+1 ] (1 if death is observed, 0 otherwise). The likelihood can then be written as: L = i S(τ j + ec i,j )h(τ j + ec i,j ) δi,j (16) Using the constant hazard function assumption and considering the logarithm of the likelihood, we get: log(L) = i [ec i,j h j + δ i,j log(h j ) - j-1 s=1 h s ] (17) The maximum likelihood estimator ĥj so that d dhj log(L) = 0 is then the ratio of the number of deaths observed within the interval divided by the exposure: ĥj = i δ i,j i ec i,j = d j EC j (18) By definition, we can write qj = 1 -exp(-ĥj ). As for the initial exposure, the central exposure is interesting because it can be expressed through a closed formula. However, it also relies on a death distribution assumption, which is generally not verified in practice. Modeling using exposure The main advantage of discretization is that it allows to consider classical modeling approaches, by predicting the number of deaths for each time interval. In practice, we will model the random variable d j describing the number of deaths using the exposure as weights or offset. Exposures are easy to compute and take into account censoring. However, this approach can generate a high number of lines in the dataset as we need to create a pseudo data table, slowing down the computation. Pseudo data tables Models can be applied to pseudo data tables, which are alternative data structures in survival analysis modeling. Often, in the actuarial field, the information of a portfolio is directly presented in pseudo data tables. If not, we can easily transform traditional survival data tables into pseudo data tables. In practice, we have to generate for each individual as many rows as time intervals and for each of them compute individual exposure. The size of the intervals is fixed in advance: months, quarters, years, etc. The size choice depends on how granular and accurate the output is needed. The last time interval includes the time of death or censoring, which means that δ is always equal to 0 except for the last time interval. Regarding the exposure, it is always equal to 1 except for the last time interval, where it represents the time observed in any case when we compute the central exposure or only in case of censoring when we compute the initial exposure. Finally, we build the dataset for every individual in the study as illustrated for the four following observations in Table 6 . It is worth noticing that this approach allows to consider covariates that vary with the time interval. It is the case for time-varying covariates such as smoking habits. It is one advantage of this method in opposition to previous approaches considering only information at the start of observation. The time interval j is added to the feature variables. That is to say, that the same individual is seen as two different ones depending on the time interval j considered. Performance computation and model validation Due to the very nature of survival data, classic metrics such as the Area Under the ROC curve (AUC) or the Mean Squared Error (MSE) might not be adapted to measure the model performances. Also, censoring prevents us from directly applying usual metrics. Statisticians proposed several metrics to deal with survival data along with estimators when the observed survival is censored. In this section, we describe some of these metrics. Standardized Mortality Ratio One of the most common and widely used metrics is the Standardized Mortality Ratio (SMR). Also known as Actual to Expected ratio in the actuarial field, the SMR can be used to measure the prediction accuracy of a model. It is defined as the ratio between the number of observed deaths and the number of deaths predicted by the model. SMR = i δ i i pred i (19) with δ i = 1 if we observe the death of individual i and δ i = 0 otherwise. The total number of dexpected eaths is obtained by summing pred i defined as the model predicted probability to observe the death of individual i. A SMR close to 1 indicates that the model fits the observations well. Different values indicate that the model may have a bias. A SMR lower than 1 shows that the model overestimates the mortality, while a SMR higher than 1 indicates that the model underestimates the mortality. Censoring must be taken into account when estimating the death probabilities pred i . Indeed, it is unlikely to observe the death of an individual that has left the study after only a few days, while for an individual observed several years the probability should be higher. Using the law of large numbers, the probability to observe a death within the study period can be approximated by the sum of the observed death divided by the number of observations. But we must add the number of non-observed deaths because of censoring. Let's start with the following equation: P(T ≤ τ ) 1 N N i=1 δ i + 1 N N i=1 (1 -δ i )P(T ≤ τ | T > t i ) ( 20 ) with t i the observation period for person i, τ the maximum observation period and T the random variable modeling the survival time. After a few simplifications we end up with the following equality: i δ i i (1 -S(τ )) -(1 -δ i ) 1 - S(τ ) S(t i ) (21) We recall the survival function definition: S(t) = P(T > t). Considering Ŝ(t| X i ) the survival probability predicted by the model, we can define pred i as follows: pred i = 1 -Ŝ(τ | X i ) -(1 -δ i ) 1 - Ŝ(τ | X i ) Ŝ(t i | X i ) (22) 19 Concordance Index The Concordance Index, also called C-Index, has been introduced by Harrell et al. (1982) . Mainly used to measure the relevancy of the bio-marker for the survival estimation, this metric is also used to assess the predictive performance of survival models. This metric allows us to measure the model's ability to rank individuals according to their survival. This metric is very relevant when the main goal of the model is to classify individuals according to their mortality risk, i.e. rank individuals from the ones with the lowest mortality to the ones with the highest mortality. This metric measures the classification ability of the model, but it does not measure fit quality. Thus the potential bias of a model would not be detected by this metric, which is thus complementary to the SMR. The C-Index is defined as a conditional probability: the model survival predictions (M i , M j ) of the two individuals i and j are ranked in the same way as their respective survival observations (T i , T j ). C - Index = P (M i < M j | T i < T j ) (23) As the survival model predicts M i , one could consider the predicted life span M i = E[T | X i ] or the survival up to the end of the study period probability M i = Ŝ(τ | X i ). Note that, the classic definition is C -Index = P (M i > M j | T i < T j ) as the model score M i is considered. As the higher the score, the higher mortality, the score and survival observation of the pairs must be ranked in opposite orders. However, in this study, we found more convenient ways to consider the expected survival period. A C-Index close to 1 indicates good performance of the model while a C-Index close to 0.5 indicates poor performance. The C-Index can be estimated only on the pairs of observations (i, j) that are comparable. Indeed, due to censorship, some pairs are not comparable. In Figure 20 we provide an illustration of this issue. In this example, in pair (2, 3) both survival observations are censored, therefore we are not able to tell which of the two individuals has the highest survival period. Similarly, in pair (1, 2) we cannot tell who has survived longer, as we are losing track of individual 2 after year 5. Figure 20 : Censorship illustration, only pairs (1, 3), (1, 4), (2, 4) and (3, 4) are comparable. Let Ω denote the ensemble of comparable pairs (i, j) where T i < T j , then we can estimate the C-Index as follows: C -Index = 1 Card(Ω) (i,j)∈Ω 1 {Mi<Mj } (24) However, this estimator depends on the study-specific censoring distribution. While this should have limited impact when using the C-Index to compare model performances in a study, this prevents an accurate comparison of the C-Index from one study to the other. In order to have a better estimation of the C-Index, Uno et al. (2011) proposed an estimator based on the ICPW foot_0 approach. They proposed a free of censoring distribution estimator as follows: C -Index = i j ∆ j G(t j ) -2 1 {ti<tj } 1 {Mi<Mj } i j ∆ j G(t j ) -2 1 {ti<tj } (25) where G(t) denotes the probability of not having censoring up to time t, and ∆ j = 1 if no censoring, 0 otherwise. Brier Score Initially, the Brier Score was introduced by Brier (1950) to measure the accuracy of meteorological forecasts. Then, Graf et al. (1999) proposed to use this metric in the bio-statistics field for assessing survival model performance. As the interpretation of it is quite difficult, it is mainly used for model comparison. The Brier Score, denoted BS, is defined as the average of squared difference between the survival probabilities and the survival observations a a given time t. BS(t) = 1 N i 1 {Ti>t} -Ŝ(t| X i ) 2 (26) with Ŝ(t| X i ) the survival probability predicted by the model. Because of censoring the BS cannot be estimated with the previous formula. Indeed, if censoring occurred before the fixed time t, we cannot know if the individual has survived longer than t. As for the C-Index, the authors considered an ICPW approach and they proposed the following estimator: BS(t) = 1 N i Ŝ(t| X i ) 2 G(t i ) 1 {ti≤t ; dδi=1} + 1 -Ŝ(t| X i ) 2 G(t) 1 {ti>t} (27) where G(t) is the probability of not observing censoring up to time t. Like the Mean Squared Error, the lower the BS the better. Usually, for model comparison, the Brier Skill Score, BSS, is considered. It is defined as the reduction of the BS compared to the BS obtained on a reference model. BSS(t) = 1 - BS(t) BS ref (t) (28) One needs to specify the time t to compute the Brier Score. Depending on the purpose, a specific time t can be more relevant, for instance, we are studying the survival up to 5 years. Alternatively, the time-independent metric Integrated Brier Score, IBS, could be considered. It is defined as the average Brier Score: BSS = 1 τ τ 0 BS(t)dt (29) with τ the study period. Exposure weighted AUC When the time of observation is cut into intervals, the problem becomes a binary classification weighted by the exposure. In this case, the weighted AUC is a good measure of the performance of the model. This metric aims at evaluating the ability of the binary classification between dead and alive, when the integration of the initial exposure as weight implies giving a bigger importance to the observed individuals rather to the censored ones. The importance given to a mistake on a censored subject increases with the observation time as the information increases as well. It is indeed worse to classify a censored individual as dead if he was observed 90% of the interval compared to one observed 10% of it because the first one is less likely to die in the resting time rather than the second one. Let I d = {i : δ i = 1} and I a = {i : δ i = 0} be respectively the sets of dead and alive individuals. Considering a threshold function f as follows: f τ ( δ) = 1 if δ ≥ τ 0 if δ < τ (30) We then define the two following quantities: Weighted true positive rate: T P R(τ ) = 1 i∈I d ei i i∈I d 1 {fτ ( δi) =0} × ei i (31) Weighted false positive rate: F P R(τ ) = 1 i∈Ia ei i i∈Ia 1 {fτ ( δi)=1} × ei i (32) A weighted ROC curve is drawn by plotting F P R(τ ) and T P R(τ ) for all thresholds τ ∈ R. The weighted AUC is the value of the area under this ROC curve. The AUC ranges from 0.5 to 1 like the C-index, where 1 means that the model is perfect and 0.5 means that the prediction is equivalent to a random classification. The interpretation is quite equivalent to the C-index metric in terms of model quality. However, thanks to the exposures every observation, even the censored ones, may be included to compute the AUC. Survival Modeling Many of the current Machine Learning models have been adapted to survival analysis problems. In this chapter, the theory and the intuition behind the models that have been implemented in the Python library will be given. It is essential to review and understand the theory to make coherent implementation choices. Besides, there is almost no literature about the application of Machine Learning models to discrete data. A deep study of the theory is thus necessary to justify their correct adaptation to predict durations, which is required to derive insurance policy prices. As mentioned before, we considered two approaches to model survival analysis: the models built on the survival dataset and the ones built on the dataset obtained after discretization. In the Python implementation, a main mother class Model Discrete and Model Continuous has been created for each approach, to gather the common functions of all models (cf Figure 3 ). These functions are evaluation metrics computation or prediction. Each model, defined in a specific class, finally inherits from the corresponding mother class. As the different implemented models rely on different existing Python packages, such as statsmodel, lifelines or scikit-learn, having a class for each one enables us to deal with all specific constraints. Besides, through the different classes, a homogenization step is included, which contributes to simplifying the library use. Each model can be fitted, evaluated, or can predict mortality thanks to the following pattern: 22 Time to event framework Cox-Model adaptation Several Machine Learning methods have been adapted to Cox's Proportional-Hazard models such as Trees, Neural-Networks, Generalized Additive Models, etc. In this section, we present Elastic Net and Gradient Boosting Machine adaptation, as they are the most widely used in practice. Cox-ElasticNet Tibshirani et al. (2011) proposed to apply the Elastic Net regularization to the Cox proportional hazard model. In the Elastic Net approach, we add a penalization during the parameter estimation process. The goal is to put aside the less relevant features by penalizing the models with a high number of parameters. Decreasing the number of features allows to diminish the signal noise and consequently increase model accuracy. This approach is very useful in high dimensions, i.e. when the number of features is close to the number of observations. This might occur in some epidemiological studies where a significant amount of information is available for each patient, but a limited number of patients is observed. In practice, this approach is often used to quickly identify variables that have the biggest explanatory power and to put aside the non-relevant ones. In this approach, the model parameters β are estimated by optimizing the following: β = argmax β log(L(β)) -λ α β 1 + (1 -α) 1 2 β 2 2 (33) with L the likelihood of the Cox model (cf equation 10) and hyper-parameters λ and α. The penalization intensity is controlled thanks to the hyper-parameter λ. If λ equals 0 then we are performing a classic Cox regression. The higher the value of λ the higher the penalization, and the lower the number of non-null parameters. When the hyper-parameter α equals 0, it is called LASSO foot_1 regression, and when α equals 1, it is named Ridge regression. Hyper-parameter α is in [0, 1] and balances between LASSO and Ridge regression. This approach has the same issues as in the classic Cox model. Namely, it heavily relies on the proportional force of mortality assumption that might not be verified. Besides, non-linear effects and unspecified interactions will not be captured by this model. Cox -Gradient Boosting Machine To integrate non-linear effects within the Cox framework, a gradient boosting adaptation may be considered. Gradient boosting consists in building a complex model thanks to the aggregation of several simple models called weak learners (Friedman (2001) &Friedman ( 2002 )). The weak learners are all the same base learners throughout the process, but they are successively trained on the residual errors made by the predecessor. Thus, each model relies on previous steps constructed models. Gradient Boosting Machine is a mix between gradient boosting and gradient descent, which is an optimisation process to minimize a loss function. The adaptation of the GBM to a Cox's proportional hazard model Ridgeway (1999) is possible by choosing the opposite of Cox partial likelihood as loss function: LL(β) = - m i=1 [X j (i) β -log( j∈Ri e X j (i) β )] (34) Generally speaking, the algorithm is presented as the process below: Initialisation: F 0 (x) = argmin β LL(β) For m = 1 to M (number of weak learners): • Computation of the pseudo-residuals: r m = -dL(Fm-1(X)) Fm-1(X) • Fitting a new weak learner on pseudo-residuals: f m (X) = r m • Finding the best γ m by solving γ m = argmin γ L(F m-1 (X) + γ × f m (X)) • Update the new model: F m = F m-1 + v × γ m f m Thus, at each iteration, until the stopping condition is satisfied, we try to reduce the global error by fitting each specificity of the residuals. A learning rate, γ m , is introduced to control how much we adjust the weights of our base learner. This parameter may be constant and chosen at the beginning of the process or optimized at each step. A large value may reduce computing time but may cause divergence, whereas a small one ensures convergence and getting an optimum but makes learning time more consuming. The shrinkage parameter v, a scalar between 0 and 1, allows to regularize the model and ensure its convergence. The real advantage of gradient boosting is that it can be adapted to any weak learner. Most of the time, trees are chosen. Cox Gradient Boosting Machine is a way of building classic regression trees by taking into account censoring within the loss function and assuming the proportional hazard hypothesis. In this case, trees are constructed consecutively, and the gradient shows the best path so that each tree is constructed on the previous one in such a way that it leads to the biggest error reduction. Figure 22 : Gradient Tree Boosting Survival Tree Another method to build specific trees for survival analysis has been developed. Compared to Cox-Gradient Boosting, it enables to create predictor trees, which may be directly interpreted. The real advantage of trees is their simplicity compared to other Machine Learning techniques, which contributes to short computation time. Survival trees have been explained by Bou-Hamad et al. (2011) and Le Blanc and Crowley (1992) . It is the direct adaption of decision trees for survival analysis. Traditional decision trees are also called CART (i.e. classification and regression tree), which is the fundamental algorithm of the tree-based methods family. The CART algorithm was developed by Breiman Breiman et al. (1984) and makes the use of trees popular to solve regression and multi-class classification problems. Like CART, survival trees are binary trees grown by a recursive splitting of tree nodes. Starting from the root node composed of all the data, the observations are partitioned into two daughter nodes according to a predetermined criterion. Using a recursive process, each node is split again into two nodes until reaching the stopping criterion. The best split for a node is found by searching over all possible variables and values, the one that maximizes survival difference. The difference between CART and Survival trees relies on the splitting criterion used to build the tree. When dealing with survival data, the criterion must explicitly involve survival time and censoring information. Either it aims at maximizing the between-node heterogeneity or at minimizing the within-node homogeneity. Log-rank criterion The most widely used criterion is the maximization of the log-rank statistic (cf Appendix 26) between the two sub-samples of the nodes, which contributes to creating splits that distinguish the most the mortality. As it is impossible to measure the similarity of the mortality within a group, the idea behind it is that by sequentially creating splits with distinct mortality, we assume we will obtain homogeneous groups at the end as the dissimilar cases become separated at each node. Hyper-parameters should be introduced to optimize the number of splits: a minimum occurrence of events within a leaf or a lower threshold of the log-rank statistic to make a split. The intuition behind these stopping criteria is to ensure the quality of the split. The first one forces the splitting criterion to be computed on enough data to make sure that the log-rank statistic is consistent. The lower bound for the second one comes from the reject region bound of the underlying log-rank test, which means a node should not be split if the mortality is not statistically different with respect to any variable. The main advantage of this method is that it does not rely on major assumptions to build the tree, even if the statistic considered to measure the difference in mortality between groups is questionable. Indeed, the log-rank test performance may be poor in some situations. Once the tree is built, the model assumes that individuals within a leaf have the same common survival curve and thus a global survival curve is computed based on the individuals within each final leaves. In open-source packages, the Nelson-Aalen estimator is used to compute the cumulative hazard function, from which we can deduce the survival curve or the expected lifetime duration. Experimental studies have shown that using the Kaplan-Meier estimator to directly estimate the survival curve gives similar results. Thanks to the binary nature of survival trees, individuals with characteristics x i fall under a unique leaf f composed of observations (x i , δ i ) with i ∈ I f . The prediction of the cumulative hazard function is the estimator for x i 's terminal node: Ĥ(t|x i ) = Ĥf (t) = ti<t i∈I f d i N i δ i (35) Some other criteria have also been studied such as C-index maximization Schmid et al. (2016) or deviance minimization within one node. Deviance criterion The deviance minimization is based on a likelihood estimation relying on the proportional hazard function to partition the observations. Under this hypothesis, the hazard function within a leaf f composed of observations (x i , δ i ) with i ∈ I f , is expressed as follows: h f (t) = h 0 (t) × θ f Using the formula 5, the likelihood can thus be rewritten as L = f i∈I f h f (t i ) δi S f (t i ) = f i∈I f h f (t i ) δi e -H f (ti) = f i∈I f (h 0 (t)θ f ) δi e -H0(ti)θ f (36) Where H 0 (t) and h 0 (t) are respectively the baseline cumulative hazard function and the baseline hazard function, and θ f is the parameter to estimate by likelihood maximisation. When H 0 is known, we can get the maximum likelihood estimator: θf = i∈I f δ i i∈I f H 0 (t i ) In practice, the cumulative hazard function is unknown and we plug in the Breslow estimator Ĥ0 (t) = i:ti≤t δ i f i:ti≥t;i∈I f θ f The deviance is finally defined as: R(f ) = 2[L f (saturated) -L f ( θf )] (37) where L f (saturated) is the log-likelihood for the saturated model that allows one parameter for each observation and L f ( θf ) is the maximal log-likelihood. The algorithm to build the tree adopts the principle of the CART algorithm: it will split the observation and covariate space into regions that maximize the reduction of the deviance realized by the split by testing all possible splits for each of the covariates. In this approach a stopping criterion regarding the minimum size of a node is also considered, since the likelihood estimation converges when it relies on a large amount of data. Simulation experiments have shown that the performance is similar to the log-rank statistic. However, this method is not assumption-free and may not be applied to all datasets. According to the trees built with a C-index maximization, the results are quite similar to the ones obtained with trees based on the log-rank statistic and are also assumption-free, but the first ones require much more computation time. Thus, trees using the log-rank criterion should be privileged, which has been shown in several experimental studies. Random Survival Forest Random survival forest extends the random forest Breiman (2001) method to right-censored survival data. Random Forest Random forest is an ensemble method inspired by the bagging of decision trees. Bagging, which means Bootstrap Aggregating, is an ensemble learning method that enables the creation of more robust predictors thanks to the aggregation of several ones trained on different subsets. A bagging process consists in generating B random samples with replacement to train B trees f b on these subsamples. Finally, the prediction of a new input X is defined as: fBagging (X) = 1 B B i=1 f b (X) Random forest differs from simple bagging of trees as randomization is not only applied to draw samples but also to select features. At certain nodes, rather than considering all variables, a random subset of the attributes is selected to compute the splitting criterion. The introduction of randomization enables the reduction of the correlation among the trees and the improvement of the predictive performance. The training process is illustrated in Figure 23 . to consider afterwards the aggregation of the prediction of the death indicator of everyone, which means considering a model as follows: δ i,j |X ∼ P(ec i,j h i,j ) Using a log-link function, the model becomes equivalent to a classic Poisson regression model with the exposure in offset: log(E[d j |X]) = log(EC j ) + X β = log(EC j ) + log(h j ) (40) which means log(h j ) = log( E[d j |X] EC j ) = X β (41) We then apply the classic generalized linear model with a Poisson distribution to a pseudo data table with exposure. Through the likelihood optimisation with respect to β, we get the risk parameters: L(β|X, D, E) = j (EC j e X j β ) dj e -ECj e X j β d j ! (42) It is also possible to consider an extension and add a regularisation factor to only consider the variables with a high explanatory power. When the probabilities are small, if the central exposure is not available in the data, approximating the model with initial exposure predicts similar results. Using the maximum likelihood estimator, the hazard function is estimated ĥj = exp(X βj ), from which the rate of mortality is derived: q j = 1 -exp(-ĥj ) = 1 -exp(-exp(X βj )) Binomial regression In the case where we try to individually model for each subject the time of death and we aggregate it afterward, one may think of using a logistic regression instead of a Poisson one. Within each interval, j, we try to predict the indicator of death δ i,j . Considering a traditional binomial model δ i,j ∼ B(q j ), where δ i,j means the death indicator that individual i will die before τ j + 1 given he has survived up to τ j and q j = P (T ≤ τ j + 1|T > τ j ), will not enable to take censoring into account. Thus we will weigh the model with the individual initial exposure, ei i,j , representing the time where i is observed between [τ j , τ j + 1] and equals to 1 if survival exceeds τ j or if the death is observed. The intuition behind this comes from the Balducci hypothesis to make the estimator matches, as under this assumption qj = i δi,j i eii,j . Indeed the weighted likelihood of the model can be expressed as: L(q j ) = lj i=1 q δi,j eii,j j (1 -q j ) (1-δi,j )eii,j (43) Then the log likelihood is: l(q j ) = lj i=1 δ i,j ei i,j log(q j ) + ei i,j (1 -δ i,j )log(1 -q j ) (44) Deriving the previous equation with respect to q j : dl(q j ) dq j = lj i=1 δ i,j ei i,j q j -ei i,j -δ i,j ei i,j 1 -q j (45) Thus, (1 -qj ) lj i=1 δ i,j ei i,j = qj lj i=1 ei i,j -ei i,j δ i,j (46) And finally we get the desired estimator l j i=1 δi,j l j i=1 eii,j = qj , as ei i,j equals 1 for dead subjects, ie those with δ i,j equals 1. The previous consideration leads us to consider the initial exposure as a weight in the logistic regression to model the impact of the covariate vector X on the mortality rate. We will thus estimate a risk parameter β such as logit(E[δ|X]) = q(X) 1-q(X) = X β, which means q(X) = 1 1+e -X β Injecting it in the log-likelihood implies: l(β) = i,j δ i,j ei i,j log(q j (X i,j )) + ei i,j (1 -δ i,j )log(1 -q j (X i,j )) = i,j {i,j|δi,j =0} (-ei i,j log(1 + e X i,j β )) + i,j {i,j|δi,j =1} (-ei i,j log(1 + e -X i,j β )) If we change the feature space from {0, 1} to {-1, 1}, we can write (by a simple variable change δ i,j = 2δ i,j -1) : l(β) = i,j {i,j|δ i,j =-1} (-ei i,j log(1 + e X i,j β )) + i,j {i,j|δ i,j =1} (-ei i,j log(1 + e -X i,j β )) = i,j (-ei i,j log(1 + e -δ i,j X i,j β )) It enables us to get the formula often used in Machine Learning literature and implemented in existing Python packages such as scikit-learn, on which our implementation can thus rely. This is then the formula to be maximized. Deriving this equation with respect to β: dl(β) dβ = i,j ei i,j δ i,j X i,j e -δ i,j X i,j β 1 + e -δ i,j X i,j β (47) From this equation, we can deduce the first order condition by making it equal to zero. There is no explicit formula, but it can be solved numerically. Generalized Additive Model One limit of the two previous models is that only linear interactions are modeled. A first possibility to capture non-linear patterns is to use generalized additive models (GAM). One strength of GAMs is that they produce a regularized and interpretable solution. However, interpretability has a cost as interactions are not detected by the model. Indeed, one should specify the interactions of the variables to be considered. In other words, GAMs strike a nice balance between the interpretable, yet biased, linear model, and the extremely flexible, \"black box\" learning algorithms. GAMs may be seen as an extension of GLMs. A Generalized Additive Model (semi-parametric GLM) is a GLM where the linear predictor linearly depends on unknown smoothing functions, s. Any function could be considered, but in practice, splines are the most widely-used one as they perform well in such circumstances (see Hastie et al. (2008) for more details about GAMs). B-splines B-splines are curves, made up of sections of polynomials of the degree of the splines, joined together so that they are continuous in value as well as their first and second derivatives. It is a piecewise function built from a polynomial of degree d, where d is a hyper-parameter. To set it, one may use the grid-search method to find the one leading to the best prediction performance. The points at which the sections join are named knots. The locations of the knots must be chosen, for example, the knots would either be evenly spaced throughout the range of observed x values or placed at quantiles of the distribution of unique x values. Finally, B-splines are local functions, that is to say, they are zero everywhere outside the range of their knots. Figure 25 is an illustration of cubic splines of four polynomials, that is to say s(x) = P 1 (x) + P 2 (x) + P 3 (x) + P 4 (x), with polynomials of degree three. The regularity conditions imply at each junction point that the value of the two polynomials and their first and second derivatives are equal. These constraints imply that a spline only has one degree of freedom to be estimates. In the same way, as previously discussed GLMs, Poisson and Binomial, considering exposures allows using GAMs for survival time modeling. Poisson GAM The previous Poisson regression introduced before will thus be modified to capture the non-linear pattern. The model will still be stated as follows: d j |X ∼ P(EC j h j (X)) (48) A log-link function and the use of exposure as offset is kept, however splines are applied to the covariates: log(E[d j |X]) = log(EC j ) + p k=1 θ k S k (X) = log(EC j ) + log(h j ) (49) which means log(h j ) = log( E[d j |X] EC j ) = p k=1 θ k S k (X) = S θ (50) where θ = [θ 1 , .., θ p ] is the vector of regression coefficients and S is the regression matrix, which is the matrix of the B-spline transformation of the covariates: S j = [s 1 , ..., s n ] , where s i = (S 1 (X i ), .., S p (X i )) The model then becomes equivalent to the Poisson linear regression if we consider the matrix S instead of the covariates. The θ parameter is once again obtained through a numerical likelihood maximization. Binomial GAM Based on the binomial regression section, the initial exposure will be introduced as a weight in the binomial GAM to model the impact of the covariate vector X on the mortality rate. Using the same notations as in the Poisson GAM section, we want to estimate a risk parameter θ such that logit (E[δ|X]) = p k=1 θ k S k (X) = S θ, which means q(X) = 1 1+e -S θ The problem is once again equivalent to the linear one as long as we consider the matrix of the B-spline transformation of the covariates. The weighted log-likelihood maximization with respect to θ enables to find an estimator of the risk parameter, from which it is possible to derive the mortality. The estimation of Survival GAM was based on the package PyGam. However, this package requires a manual specification of all the variables, their interaction, and the associated spline level. An automatization step had to be introduced compared to the other implemented models to be called in the same manner. Decision tree The binomial regression aims at predicting the death indicator, δ, using initial exposure as weights. As we are predicting a binary variable, it seems interesting to consider a weighted classification problem, such as a weighted decision tree for classification. Classification Tree For our purpose, we only need to focus on the intuition behind two-class classification trees. The goal of a binary classification tree is to successively divide observations into two groups with respect to the variable that creates the best split. The partition of the individuals is repeated on each subsample until reaching the stopping criterion or having only pure groups at the final step, that is to say, groups in which all instances have the same label: δ. The algorithm seeks to create at each step two groups as homogeneous as possible by reducing the variance within a group, or equivalently to create two groups as different as possible by increasing the variance between groups. To determine the best split among the features and finding the partition rule, a splitting criterion is introduced to measure the quality of a split. In theory, all possible splits should be tested to find the best one, that is to say, the one giving the biggest reduction of variance. However, as it would be time-consuming, some randomization techniques are used in practice. Two criteria are considered to build trees. The first one is the entropy, H(x), which aims at measuring the disorder quantity. The second one is the Gini index, G(x), which measures the impurity within a group. To make a split from a given node composed of n 0 observations (x i , δ i ) with i ∈ I 0 , we partition our data into two branches I l and I r , with respectively n l and n l observations, in order to have the biggest information gain I(x 0 ) -[ nr n0 × I(x r ) + n l n0 × I(x l )], where I(x) is G(x) or H(x). Weighted Tree To prevent the bias due to censoring, we need to adapt the CART algorithm by injecting the initial exposure as weights in the splitting criterion. Let's define p m , the proportion of deaths observed in node m, I m the set of individuals of the node with characteristics x m and qm the death rate estimator for individuals with x m under the Balducci hypothesis: p m = 1 i∈Im ei i i∈Im ei i δ i = i∈Im δ i i∈Im ei i = qm as ei i = 1 if δ i = 0 Two splitting criterion may thus be adapted: Entropy: H(x m ) = -q m × log 2 (q m ) -(1 -qm ) × log 2 (1 -qm ) Gini index: G(x m ) = 2 × qm × (1 -qm ) Let's illustrate the intuition behind the Gini index with the ten following individuals within a node m: 1 2 3 4 5 6 7 8 9 10 δ 1 1 1 1 1 0 0 0 0 0 ei 1 1 1 1 1 1 1 0.2 0.8 0.5 Using the formulas, we have: qm = 5 8.5 = 0.59 and G = 2q m (1 -qm ) = 0.48. Thus the information gain is maximal for the first split as it creates two pure leaves. First split As shown with the second and the third splits, censoring is indeed taken into account thanks to the exposure. Without adjusting the probability with the initial exposure, both splits would have been equivalent (as it becomes equivalent to consider all exposures equal to one and thus there is no difference between censored and alive people for the Gini index). However, in our case, mixing censored and dead observations within a leaf is better than mixing alive and dead ones. This can be explained as the Gini index increases if we reduce the global exposure, keeping the number of dead observations the same and as the dominant label of the leaf. We can deduce the same conclusion on the ascendant side of the Gini curve by interchanging dead and alive observations. Censored and dead VS alive splits are indeed preferable compared to alive and dead VS censored. For censored people we know for sure that they survive until a certain point of time. However the probability of dying before the end of the observation period is not null compared to the survivors. Thus the mistake is less certain. Figure 27 : Impurity curve One may also think of building a tree based on the deviance criterion such as described for the survival tree. In this case, the weighted log-likelihood of the binomial regression should be considered to measure the deviance: R(x m ) = 2[L m (saturated) -L m ( βm )] = 2 i∈Im ei i log( 1 + e -δiX i βm 1 + e -δiX i βs ) The gradient g i = ∂l(y i , ŷi (t-1) ) ∂ ŷi (t-1) = -ei i y i e -ŷi (t-1) 1 + e -ŷi (t-1) + ei i (1 -y i ) e ŷi (t-1) 1 + e ŷi (t-1) = -ei i y i e -ŷi (t-1) 1 + e -ŷi (t-1) + ei i (1 -y i ) 1 1 + e -ŷi (t-1) = -ei i y i + ei i 1 1 + e -ŷi (t-1) = ei i qx (t-1) (x i ) -ei i y i With the log likelihood as the loss function, we see that the gradient for each instance i is proportional to the difference between the predicted dead and the observed ones weighted by the initial exposure. More importance is given to the observations with a bad mortality prediction. The hessian h i = ∂ 2 l(y i , ŷi (t-1) ) ∂ ŷi (t-1) = ei i e -ŷi (t-1) (1 + e -ŷi (t-1) ) 2 = ei i qx (t-1) (x i )(1 -qx (t-1) (x i )) (54) The optimal weight w * j = - i∈Ij g i i∈Ij h i + λ = - i∈Ij ei i qx (t-1) (x i ) -ei i y i i∈Ij ei i qx (t-1) (x i )(1 -qx (t-1) (x i )) + λ (55) The value of the weight w * j can be high and the initial second-order Taylor approximation can thus be no longer sustainable. A shrinkage factor η can be introduced here to diminish the importance of the current learner. The scoring function for the weak learner t is L t (q) = - 1 2 T j=1 ( i∈Ij ei i qx (t-1) (x i ) -ei i y i ) 2 i∈Ij ei i qx (t-1) (x i )(1 -qx (t-1) (x i )) + λ + γT (56) The literature also recommends using some subsampling when constructing the tree to diminish the greediness of the trees. In a word, the XGBoost corrected with the initial exposure is a model that will correctly integrate the partial information for each duration where an individual is observed. The learners recursively segment the space with weights dependant on the performance of the precedent learners and the regularisation parameters. This dependency leads to difficulties in the analysis of the structure of the trees. However, the celebrity of the XGBoost is established and the model has demonstrated high performances in many data science competitions. CatBoost CatBoost is an open-source Machine Learning algorithm developed by Yandex Dorogush et al. (2016) that uses gradient boosting on decision trees. The difference with other gradient boosting algorithms is that it successfully handles categorical features and uses a new schema for calculating leaf values when selecting the tree structure, which helps to reduce overfitting. Most popular implementations of gradient boosting use decision trees as base predictors. However, decision trees are convenient for numerical features, but, in practice, many datasets include categorical features, which are also important for prediction. Categorical features are not necessarily comparable with each other and most of the time, to deal with categorical features in gradient boosting, a pre-processing step is needed to convert the categorical values into numbers before training. With Catboost, this step is no longer needed but the categorical features cannot contain missing values. Within the developed library, a pre-processing step is thus still implemented to replace all missing values with the label 'Missing value', when some are contained in a categorical variable. LightGBM Microsoft researchers Ke et al. (2017) have proposed a novel open-source gradient boosting algorithm called LightGBM. This new implementation aims at optimizing the process for large datasets with a high feature dimension. Contrary to other gradient boosting implementations, LightGBM does not scan all the data instances to estimate the information gain of all possible split points thanks to two novel techniques: Gradient-based One-Side Sampling and Exclusive Feature Bundling to deal with a large number of data instances and a large number of features respectively. The principle of Gradient-based One-Side Sampling is to exclude a significant proportion of data instances with small gradients as they play a less important role in the information gain, and only use the rest to estimate the information gain. The principle of Exclusive Feature Bundling is to reduce the number of features by mutually bundling exclusive features (i.e., they rarely take nonzero values simultaneously). The experimental results show that LightGBM can significantly outperform XGBoost in terms of computational speed and memory consumption while achieving almost the same accuracy. Interpretation Some of the above-presented models are \"black box\" Machine Learning models, which cannot be directly interpreted. However, in many cases, understanding the model remains essential for different reasons: respecting the regulatory requirement to justify every decision taken, maintaining stakeholder trust by understanding the produced results, etc. For that purpose, three complementary methods of interpretation have been implemented within the library. The presented methods are post-hoc interpretation methods, i.e. once the model has been fitted, and agnostic to the model, i.e. independent of the algorithm to be explained (see Delcaillau et al. (2020) for more details). Permutation variable importance Feature importance can be assessed using the permutation method which has been introduced by Fisher (2018) . In this method, we measure the increase in the prediction error of the model after we permuted the values of the features. The permutation breaks the relationship between the feature and the true outcome thus it increases the prediction error. The higher the increase in the prediction error, the higher the importance of the feature. In practice, after having trained a model and assessed the model performance, for each feature j one can proceed as follows: • For each observation, replace the feature with a randomly generated variable. This enables to break the relationship between the feature and the true outcome to be predicted. • Produce model forecast and assess the loss in model performance L j . It can either be the difference or the ratio between the original error of the model and the one after permutation. A high loss in performance shows that the model heavily relied on the feature to produce the predictions. If performances are unchanged after shuffling values of one feature, it means that the model ignored the feature for the prediction. Thus, the importance of the features is ranked by their descending L j . This approach provides a global insight into model behavior. By permuting a feature, we do not only break the relationship with the outcome variable but also the interaction effects with all the other variables. The inclusion of the interaction effect within the measure not only has advantages but also disadvantages. The importance of the interaction is indeed measured twice, in the two associated features. The big advantage of this approach is that it is less time-consuming, as we do not need to retrain the model. This method only requires the application of the trained model to different observations and an error computation. Within the library, every model class inherits a permutation variable importance method. This method shuffles the modalities of a variable and assesses the loss in the performance of the model on the noisy data. Partial Dependence This method is the oldest one to interpret Machine Learning models and has been introduced by Friedman (2001) . The partial dependence plot shows the marginal effect that one or two features have on the predicted outcome. In practice, more features could be considered but we limit to one or two for representation purposes, i.e. producing 2D or 3D plots. The plot can show whether the relationship between the target and a feature is linear, monotonic, or more complex. The Partial Dependence function is defined as the average model prediction for a given value of a feature. When dealing The main idea behind this estimator is that surviving after a given time t means being alive just before t and do not die at the given time t. Consequently, with t 0 < t 1 < t we get : S(t) = P(T > t) = P(T > t 1 , T > t) = P(T > t | T > t 1 ) × P(T > t 1 ) = P(T > t | T > t 1 ) × P(T > t 1 | T > t 0 ) × P(T > t 0 ) In the end, by considering all the distinct times t i , (i = 1, ..., n) where an event occurred ranked by increasing order (whatever it is a death or censorship) we get: S(t j ) = P(T > t j ) = j i=1 P(T > t i | T > t i+1 ) , with t 0 = 0. Considering the following: d j the number of deaths that occurred in t j N j the number of individuals alive just before t j The probability q j = P(T ≤ t j | T > t j-1 ) of dying in the time interval ]t j-1 , t j ] knowing the individual was alive in t j-1 can be assessed by : qj = dj Nj Let δ i be the censorship indicator of each observation; the Kaplan-Meier estimator is then defined as: Ŝ(t) = ti≤t (1 - d i N i ) δi (57) Cox likelihood Cox model is defined as h(t | X) = h 0 (t) × exp(β X). Let consider 3 individuals A,B and C to give the intuition behind the formula. Considering that there is no tie and that t (i) is the ranked sequence of death time, for i ∈ [1; 3]. Let R i be the risk set, which is a set of indices of the subjects that are still alive just before t (i) : R i = {j : t j ≤ t (i) } = {A, B, C} The contribution to the likelihood will be the conditional probability : P(what happened at t (i) | one event occurs at t (i) and the information up to t (i) ) Considering without loss of generality that A dies at t (i) : P ti = P(A died; B,C survived | A,B,C in the risk set and one died) = P(A died; B,C survived) P(A died; B,C survived) + P(B died; A,C survived) + P(C died; A,B survived) The events being independent using the previous notation: P A = P(A died; B,C survived) = P(t A = t (i) , t B > t (i) , t C > t (i) ) = f A (t (i) ) × S B (t (i) ) × S C (t (i) ) = h A (t (i) ) × S A (t (i) ) × S B (t (i) ) × S C (t (i) ) = h A (t (i) ) × C where C = S A (t (i) ) × S B (t (i) ) × S C (t (i) ) Injecting P A in P ti and deducing the same formula for B and C : P ti = h A (t (i) ) × C h A (t (i) ) × C + h B (t (i) ) × C + h C (t (i) ) × C = h 0 (t (i) ) × exp(β X A ) h 0 (t (i) ) × exp(β X A ) + h 0 (t (i) ) × exp(β X B ) + h 0 (t (i) ) × exp(β X C ) Figure 1 : 1 Figure 1: Illustration of censoring. Figure 2 : 2 Figure 2: Illustration to highlight the impact of censor Figure 3 : 3 Figure 3: Discrete model classes Figure 6 : 6 Figure 6: Missing value imputation illustration Figure 7 : 7 Figure 7: One-hot encoding illustration from s c o r _ s u r v i v a l . d i s c r e t i z e r import e x p o s u r e _ e x p a n s i o n p s e u d o _ d a t a = e x p o s u r e _ e x p a n s i o n ( s u r v i v a l _ d a t a , t i m e , e v e n t , i n d i v i d u a l _ k e y , e n t r y _ a g e , e n t r y _ y e a r ) Figure 8 : 8 Figure 8: Transformation to pseudo data table : illustration Figure 9 : 9 Figure 9: Catboost hyper-parameters selection Figure 10: Validation for continuous model 10. 1 1 Figure 11: Partial Dependence for discrete models Figure 12: Variable Importance for continuous models 21 for details and illustrations on ROC curves and AUCA comparison of multiple classifiers is usually straight-forward thanks to ROC curves -Receiver Operating Characteristic curve -, especially when such curves are not crossing. Curves close to the perfect ROC curve have a better performance level than the ones close to the baseline. The baseline is achieved by a classifier with a random performance level and corresponds to the straight line from the origin to the top right corner. Figure 14: ROC Curve for the train (left) and test (right) datasets from s c o r _ s u r v i v a l . m o d e l s . c o n t i n u o u s import model model . c i (X, e v e n t , e x p o s i t i o n ) from s c o r _ s u r v i v a l . m o d e l s . d i s c r e t e import model model . a u c (X, e v e n t , e x p o s i t i o n ) Figure 15: Mortality prediction by year of entry into the study program (left) and sleep hours (right) Figure 16 : 16 Figure 16: Life insurance contract Figure 17 : 17 Figure 17: Discrete Model premium evaluation based on individuals' age Figure 18 : 18 Figure 18: Relationships between f (t), F (t) and S(t) (source Wang et al. (2017)) Figure 19 : 19 Figure 19: Impact of ignoring censoring in life duration study Figure 21 : 21 Figure 21: ROC Curve and AUC illustration import s c o r _ s u r v i v a l . m o d e l s # T r a i n i n g model . t r a i n (X, e v e n t , e x p o s i t i o n ) # E v a l u a t i o n model . a u c (X, e v e n t , e x p o s i t i o n ) model . c i (X, e v e n t , e x p o s i t i o n ) # P r e d i c t i n g model . p r e d i c t _ p r o b a (X) model . p r e d i c t _ s u r v (X) Figure 23 : 23 Figure 23: Random forest process illustration (source nanowiki (2016)) Figure 25 : 25 Figure 25: B-splines of degree 3 Figure 26: Split representation Table 2 : 2 C-Index and AUC value Continuous Models C-Index Train C-Index Test Cox 0.86 0.85 Cox-Net 0.86 0.85 Cox Tree 0.80 0.79 Cox XGBoost 0.89 0.85 Survival Tree 0.84 0.83 Random Survival Forest 0.85 0.83 Discrete Models AUC Train AUC Test Binomial Regression 0.83 0.83 Poisson Regression 0.85 0.85 Random Forest 0.86 0.84 LightGBM 0.86 0.85 XGBoost 0.88 0.86 Logistic GAM 0.86 0.86 CatBoost 0.86 0.85 Table 3 : 3 Discrete models summary table Strengths Weaknesses Binomial Regression • No parameter calibration • Interpretability • Possibility to add regularization • Model only linear patterns • May not converge Poisson Regression • No parameter calibration • Interpretability • Possibility to add regularization • Model only linear patterns • May not converge logistic GAM • Model variable interactions and non linear patterns • Interpretability • Possibility to add regularization • Need to specify the interactions • May not converge • Model variable interactions and non linear patterns • Subject to overfitting Random Forest • May be parallalized • Sensitivity to hyperparameter • Robust • Difficulty to capture duration effect LightGBM • Model variable interaction and non linear pattern • Can adapt to large dataset with lots of features • Speeder gradient boosting algorithm • Sensitivity to hyperparameters • No interpretability XGBoost • Model variable interactions and non linear patterns • More robust compared to other gradient boosting • Subject to overfitting • Sensitive to hyperparameters • No interpretability Catboost • Model variable interactions and non linear patterns • Handle categorical variable without prior transformation • Less subject to overfitting compared to other gradient boosting • Sensitivity to hyperparameters • No interpretability Table 4 : 4 Time-to-event models summary table Strengths Weaknesses • No parameter calibration • Model only linear patterns Cox -Net • Interpretability • May not converge • Regularization • Rely on proportional hazard assumption • Model variable interactions and non linear patterns • Subject to overfitting Cox -XGBoost • Robust • Sensitive to hyperparameters • Good predictive performance • No interpretability Cox Tree • Short computation time • Interpretability • Rely on proportional hazard assumption • May not converge • Difficulty to capture duration effect Survival Tree • Short computation time • No underlying assumption • Interpretability • Questionability of the heterogeneity measure • Difficulty to capture duration effect • Model variables interactions and non linear patterns • Subject to overfitting Random Survival Forest • May be parallalized • Sensitivity to hyperparameter • Robust • Difficulty to capture duration effect Table 5 : 5 Example of survival data table Age Gender Y δ S 1 40 Female 7.1 1 S 2 30 Male 4.9 0 S 3 52 Male 3.4 0 S 4 60 Female 3 1 Table 6 : 6 Example of survival data table exposure transformation Age Gender I ec ei δ S 10 40 Female 0 1 1 0 S 11 41 Female 1 1 1 0 S 12 42 Female 2 1 1 0 S 13 43 Female 3 1 1 0 S 14 44 Female 4 1 1 0 S 15 45 Female 5 1 1 0 S 16 46 Female 6 1 1 0 S 17 47 Female 7 0.1 1 1 S 20 30 Male 0 1 1 0 S 21 31 Male 1 1 1 0 S 22 32 Male 2 1 1 0 S 23 33 Male 3 1 1 0 S 24 34 Male 4 0.9 0.9 0 S 30 52 Male 0 1 1 0 S 31 53 Male 1 1 1 0 S 32 55 Male 2 1 1 0 S 33 55 Male 3 0.4 0.4 0 S 40 60 Female 0 1 1 0 S 41 61 Female 1 1 1 0 S 42 62 Female 2 1 1 0 S 43 63 Female 3 0 1 1 Inverse Censorship Probability Weighting LASSO stands for Least Absolute Shrinkage and Selection Operator"
}