{
  "title": "When Machine Learning Meets Privacy: A Survey and Outlook",
  "abstract": "The newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This paper surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field. CCS Concepts: ‚Ä¢ Security and privacy ‚Üí Privacy protections; Social network security and privacy.",
  "introduction": "INTRODUCTION Since Facebook data privacy scandal in 2018 [154] , privacy has once again become a dominant feature in people's minds. This motivates revisiting privacy challenges, particularly with the emergence of intelligent technologies thanks to the big data revolution. For example, newly emerged machine learning (ML) techniques, especially the unprecedented powerful deep learning, will have paradigm-shifting impacts on privacy preservation. A critical question that needs to be well investigated is: What are the privacy challenges and solutions associated with ML? Some initial work has appeared in the literature with an emphasis on mitigating privacy risks during the machine learning process by paying special attention to the privacy challenges and risks associated with the ML models. In this regard, possible attack models [8, 38, 98, 141, 144, 155] have been discussed and protection schemes [2, 12, 118, 126, 140] have been proposed. These works demonstrated both ML models and training datasets can be the target of privacy attacks, leading to sensitive information leakage. Meanwhile, researchers have also tried to use ML for privacy protection. As an example, the authors of [174] have developed a method for automatic recognition of privacy-sensitive object classes and adjust users' privacy preference settings. In addition, there are also several works that develop new privacy protection schemes in the scenarios where ML is used for attacks [80, 81] . Overall, the current research has only scratched the surface, and there are major issues that require further investigation: ‚Ä¢ ML could play different roles in a privacy protection problem, e.g., protection target, attack tool, and/or protection tool. It may even play multiple roles in the same problem. ‚Ä¢ ML systems and models have different types, each facing different privacy risks and requires different protection schemes. ‚Ä¢ There does not exist a unified privacy metric or notion. Although differential privacy (DP) [32] is widely accepted in traditional privacy studies, it still has limitations in the context of ML, especially when considering unstructured data, such as text, image, and video. In this context, a systematic study of privacy and ML is essential for future research efforts. Although there are several surveys on this topic [1, 64, 85, 177] , The focus has been on a certain type of ML model or specific methods. This study attempts to provide the first comprehensive survey on privacy in ML by investigating different scenarios/applications of privacy and ML. The main contributions of the paper are as follows: ‚Ä¢ We divide the works in this area by the different roles of ML, i.e., ML as protection target (private ML), protection tool (ML enhanced privacy protection), attack tool (ML-based attack), and analyze the problems and solutions in each category. ‚Ä¢ For private ML, we categorize the attacks and protection schemes and then compare their difference. ‚Ä¢ For ML aided privacy protection and ML-based privacy attack, we not only discuss the existing works, but also provide insights on new techniques to achieve privacy preservation. ‚Ä¢ The study concludes with a discussion on the directions of future research in ML and privacy. Through this comprehensive overview, we wish to prepare a solid ground for future research in this field. The rest of the paper is organized as follows. Section 2 reviews basic concepts of machine learning system and models, and discusses the relationship between privacy and ML. In Section 3, we compare and classify existing privacy attacks and protection schemes in ML systems. Section 4 focuses on ML aided privacy protections, followed by the discussion of ML-based attack and corresponding privacy preservation schemes in Section 5. We present our outlook and propose some future directions for this promising research topic in Section 6. Finally, we conclude our work with a summary in Section 7. Moreover, the abbreviations used in this paper are listed in Table 1 .",
  "body": "INTRODUCTION Since Facebook data privacy scandal in 2018 [154] , privacy has once again become a dominant feature in people's minds. This motivates revisiting privacy challenges, particularly with the emergence of intelligent technologies thanks to the big data revolution. For example, newly emerged machine learning (ML) techniques, especially the unprecedented powerful deep learning, will have paradigm-shifting impacts on privacy preservation. A critical question that needs to be well investigated is: What are the privacy challenges and solutions associated with ML? Some initial work has appeared in the literature with an emphasis on mitigating privacy risks during the machine learning process by paying special attention to the privacy challenges and risks associated with the ML models. In this regard, possible attack models [8, 38, 98, 141, 144, 155] have been discussed and protection schemes [2, 12, 118, 126, 140] have been proposed. These works demonstrated both ML models and training datasets can be the target of privacy attacks, leading to sensitive information leakage. Meanwhile, researchers have also tried to use ML for privacy protection. As an example, the authors of [174] have developed a method for automatic recognition of privacy-sensitive object classes and adjust users' privacy preference settings. In addition, there are also several works that develop new privacy protection schemes in the scenarios where ML is used for attacks [80, 81] . Overall, the current research has only scratched the surface, and there are major issues that require further investigation: ‚Ä¢ ML could play different roles in a privacy protection problem, e.g., protection target, attack tool, and/or protection tool. It may even play multiple roles in the same problem. ‚Ä¢ ML systems and models have different types, each facing different privacy risks and requires different protection schemes. ‚Ä¢ There does not exist a unified privacy metric or notion. Although differential privacy (DP) [32] is widely accepted in traditional privacy studies, it still has limitations in the context of ML, especially when considering unstructured data, such as text, image, and video. In this context, a systematic study of privacy and ML is essential for future research efforts. Although there are several surveys on this topic [1, 64, 85, 177] , The focus has been on a certain type of ML model or specific methods. This study attempts to provide the first comprehensive survey on privacy in ML by investigating different scenarios/applications of privacy and ML. The main contributions of the paper are as follows: ‚Ä¢ We divide the works in this area by the different roles of ML, i.e., ML as protection target (private ML), protection tool (ML enhanced privacy protection), attack tool (ML-based attack), and analyze the problems and solutions in each category. ‚Ä¢ For private ML, we categorize the attacks and protection schemes and then compare their difference. ‚Ä¢ For ML aided privacy protection and ML-based privacy attack, we not only discuss the existing works, but also provide insights on new techniques to achieve privacy preservation. ‚Ä¢ The study concludes with a discussion on the directions of future research in ML and privacy. Through this comprehensive overview, we wish to prepare a solid ground for future research in this field. The rest of the paper is organized as follows. Section 2 reviews basic concepts of machine learning system and models, and discusses the relationship between privacy and ML. In Section 3, we compare and classify existing privacy attacks and protection schemes in ML systems. Section 4 focuses on ML aided privacy protections, followed by the discussion of ML-based attack and corresponding privacy preservation schemes in Section 5. We present our outlook and propose some future directions for this promising research topic in Section 6. Finally, we conclude our work with a summary in Section 7. Moreover, the abbreviations used in this paper are listed in Table 1 . PRIVACY THREATS AND MACHINE LEARNING In this section, we discuss the privacy threats in the context of machine learning, and further point out various roles of machine learning in the studies of user privacy. The Machine Learning System and Models ML refers to algorithms and statistical models used by computer systems to efficiently perform specific tasks without the use of explicit instructions. It relies on an automated learning process. The ML algorithm constructs a mathematical model of sample data called a \"training set\" to make predictions or decisions [10] . Depending on if the output is labelled in the training set, ML models can be divided into three different groups: supervised, unsupervised, and semi-supervised. As supervised learning is used by most practical machine learning algorithms, it will be explained here as an example. A supervised ML model is a parameterized function ùëì ùúÉ that maps input data √¨ ùë• ‚àà X ùëë (generally a vector of features) to output data ùë¶ ‚àà Y (label). For a classification problem, X ùëë is a ùëë-dimensional vector space and Y is the set of classes. This function is trained to accurately predict the label of new data that have not seen before. Moreover, we can divide the ML process into two stages: (1) Model training: The training process of a machine learning model is to find the optimal parameters that can accurately capture the relationship between X and Y. To achieve this, a training dataset ùê∑ = { √¨ ùë• ùëñ , ùë¶ ùëñ } ùëÅ ùëñ=1 with ùëÅ samples is needed. Then a loss function ùêø is adopted to quantify the difference between two outputs, i.e. the ground-truth one ùë¶ ùëñ and the predicted one ùëì ùúÉ ( √¨ ùë• ùëñ ). The goal of training a model is to minimize this loss function, i.e., ùúÉ ‚òÖ = arg min ùúÉ ( ‚àëÔ∏Å ùëñ ùêø(ùë¶ ùëñ , ùëì ùúÉ ( √¨ ùë• ùëñ )) + Œ©(ùúÉ )), (1) where Œ© is a regularization term to penalize model complexity and avoid overfitting. (2) Model inference/prediction: After the model training is completed and the optimal parameters ùúÉ ‚òÖ are obtained, given an input √¨ ùë•, the corresponding output can be calculated as ùë¶ = ùëì ‚òÖ ùúÉ ( √¨ ùë• ùëñ ). This prediction process is called inference. We can calculate the prediction accuracy of the model over a testing dataset ùê∑ ùë° to measure the model's performance. Furthermore, according to the architecture of the ML systems, there are two different models, as shown in Fig. 1: ‚Ä¢ Centralized learning: The training data is centralized in a machine or in a data center, and the centralized entity trains and hosts the models. For example, a researcher could use a cloud platform, to host datasets and train an AI model based on them. It goes without saying that the availability of all data in such a centralized method leads to high efficiency and accuracy [55] . However, because the centralized operator has direct access to sensitive data, user privacy might be violated. As the learning tasks become more and more complicated, many companies start to outsource the training process, i.e., outsourced learning, or ML-as-a-service. In this case, each user owns his/her training data while the service providers own the models and algorithms. The data holder outsources model creation to a cloud service such as Microsoft Azure ML and Amazon AWS ML, which automate the process of ML. \"Users upload datasets, perform training, and make the resulting models available for use\" [144] . During this process, the users do not have any understanding of the details of model creation. The \"ML provider is the entity that provides ML training codes to data holders\" [144] . ‚Ä¢ Distributed learning: Centralized learning is sometimes not a good option for several reasons: (i) data is inherently distributed in some scenarios; (ii) data is too large to be stored in a single machine; (iii) users are not willing to share raw data; and (iv) users want to train the neural network with different instances to achieve better predication accuracy. In this case, ML can be conducted in a distributed manner, i.e., distributed learning. In general, distributed learning is used in a scenario of distributed training data sources and a centralized server. There are several variations of distributed learning: -Collaborative learning: Distributed learning involving such collaborations is known as collaborated learning. But the settings could be quite different in the literature. For example, the authors of [145] proposed a collaborative learning framework that trains several classifiers \"simultaneously on the same training data\" to achieve better performance. On the other hand, in the collaborative learning model defined in [55] , each participant uses its device to train a local AI model. It then shares a fraction of the parameters/coefficients of the model with the other users. Service operators can create a composite model by collecting these parameters and achieve almost the same accuracy as a model built using a centralized approach. The collaborative approach is \"more privacy-friendly\" because the dataset is not directly exposed. Also, if only a small part of the model parameters is shared and the parameters are truncated and/or obfuscated by DP mechanisms, the model exhibits convergence through experiments [140] . -Federated learning: A popular framework for collaborative learning is Federated learning [69] introduced by Google. There are currently two different federated learning settings: cross-device and cross-silo [68] . The cross-device setting normally involves a very large number of mobile or IoT devices, while in the cross-silo settings it \"might involve only a small number of relatively reliable clients\" [68] , e.g., multiple organizations. In a broader definition of federated learning that covers both settings, each device downloads the current model from a centralized server, improves it by learning from data on a local device, and then sums up the changes in a focused update. Here, \"focused updates are updates\" containing \"the minimum information necessary for the specific learning task\" [68] . And then the shared model is updated by averaging all users' updates. Since all the training data will not leave local devices, and no updates from individual users are stored in the cloud, the privacy risk has been greatly reduced. -Split learning: Another collaborative learning framework is Split learning, in which each user trains the network up to a certain layer known as the cut layer and sends the weights to server. Mathematically speaking, these weights represent and compress the input data to some intermediate feature vectors. The server then trains the network for rest of the layers, and generates the gradients for the final layer, followed by error back-propagation until the cut layer. The gradient is then passed over to the users. The rest of the back-propagation is completed by the users [159] . In split learning, \"client-side communication costs are significantly reduced as the data to be transmitted is restricted to first few layers of the split neural network prior to the split\". Although some collaborative learning models consider shared training data [145] , which presents a significant privacy risk. In this survey, however, we consider the case that the local raw training data are not shared with the server or amongst users. In this learning process, the users can collaboratively learn a shared ML model, thus decoupling ML tasks from the storage of the data in a single device. Overall, centralized learning is characterized by \"globally stored data\" and \"globally trained model\", as shown in Fig. 1 (a), while the distributed learning is characterized by \"locally stored data\" and \"locally trained model\", as shown in Fig. 1(b) . Although there will be a global model in distributed learning, it is not trained globally, at least part of the model is trained by individual clients. Relationship of privacy and machine learning In contrast to traditional privacy-related research frameworks, ML techniques open new challenges and opportunities to privacy protection. There has been some initial research embarking on this journey. The existing works can be divided into three categories according to the roles of ML in privacy. First, making ML system private, i.e., ML system is the target of privacy protection. As shown in Fig. 2 (a), this category 1 includes making both the ML system (model parameter) and data (training/test dataset and output data) private, since the privacy threat may happen in any stage of a data cycle, e.g. the training, publishing, or prediction of data. Most of the research in this group relies on the use of differential privacy in ML and deep learning models [44] . For example, Shokri et al. [140] developed a differentially private SGD algorithm and a distributed deep learning model training system. In such way, multiple entities can cooperatively learn a neural network. Second, using ML to enhance privacy protection. As shown in Fig. 2 (b), the privacy protection target is the data in this category 2 and ML is a tool to help privacy protection. For example, Liu et al. [82] utilized ML to enhance private decision-making experience through ML. Orekondy et al. [114] proposed an approach to categorize personal information in images and predict information leakage directly from images. Yuan et al. [175] presented an ML approach to decide whether to share a picture with a specific requester for a particular context. Third, ML-based privacy attack, i.e., ML is used as an attack tool of the adversary, as shown in Fig. 2(c ). For example, recent researches have shown that deep learning methods can be used to detect object types, people's identities, and landmarks, from images posted on Internet. When the adversaries use this kind of powerful tools, conventional privacy protection methods would be over-powered, especially being challenged by the mighty deep learning tools. There have been very few works in this category. Liu et al. [80] proposed schemes of applying adversarial perturbations images, so that ML systems cannot get private information from them. Table 2 summaries three categories of privacy protection problems involving ML systems. It is worth mentioning that one technique might belong to more than one category. For instance, ML might be used as attack and protection tools at the same time, which makes the problem more complicated. We will discuss this in more detail in the reminder of the paper. Fig. 3 summarizes the general taxonomy of the research papers presented in this work. We divide them according to the above mentioned three categories. In each category, we discuss the attack and threat models first and then analyze the works on privacy protection schemes. PRIVATE MACHINE LEARNING In this section, we will discuss the challenges and existing solutions in privacy preservation in ML, or simply stated, private ML. We will first discuss attack and threat models, followed by detailed analysis of privacy preservation schemes, along with some comparisons at the end. Attack and Threat Models In this subsection, we analyze the attack models from three perspectives: the attack targets, the knowledge of the adversary, and the attack methods. First, as we can see in Section 2, model and data are two important components in ML that correspond to two different categories of privacy attack targets, as shown in Fig. 4 : (1) Training data privacy: In many cases, a user wants to keep the training data private while using a ML service. For example, for a medical study or a hospital having a model built out of the private medical profiles of some patients. A patient may want to use the model to make a prediction about whether she is likely to contract a certain disease, or the hospital may want to use the model to predict the probability of readmittance. In these cases, the training data is sensitive medical profiles and should not be revealed. Similar cases exist in other areas such as financial records. More specifically, training data privacy includes exact data value, certain features, statistical properties, or membership (whether a certain data is in the training set).  model which can accurately predict stock prices or insurance rates. The model is an important commercial and intellectual property. Another example is the commercial ML API services currently provided by Google, Amazon, Microsoft, and other companies. They charge the customers per API access. Revealing their models or algorithms will cause loss of revenue. In summary, the attack target can either be the model structure or parameters. Second, the adversaries have different levels of knowledge according to their access to the information. ‚Ä¢ White-box access: The adversary has access to the trained model, especially the model parameters. ‚Ä¢ Black-box access: The adversary is an end-user and is only allowed to query the prediction model on his/her inputs through an appropriate interface. Finally, the adversary can adopt different attack methods. Existing attack methods include model inversion (reverse engineering), shadow training models, and encoding information into models. Next, we will group existing popular attack models by attack targets and analyze them from the above mentioned three aspects. An illustrative diagram of the attack models is presented in Fig. 5 . Model Extraction Attack. The model extraction attack targets at the duplication of (i.e., \"steal\") the AI model [155] . The outcome of the attack will be a function ùëì ‚Ä≤ that is approximately the same as the initial function ùëì . An illustration of such an attack can be found in Fig. 5(a) . In this attack, the adversary only has black-box access with no prior knowledge of the ML model parameters or training data. Tram√®r et al. [155] used a shadow training scheme that can \"extract target ML models with near-perfect fidelity for popular ML models\" including logistic regression, decision trees, and neural networks, by equation-solving, path finding, or extending the Lowd-Meek approach [90] . There are several other works following this path. Oh et al. [111] built meta-models to extract more model details such as the neural network architecture. Wang et al. [161] designed an attack to steal the hyperparameters of the machine learning model. A hyperparameter is \"used to balance the loss function and regularization term in the objective function\". The adversary can obtain this value from the training set and model. Hua et al. [56] \"investigated reverse-engineering attacks on CNN models exploiting information leaks through memory and timing side-channels\". (a) Model Extraction Attack (b) Feature Estimation Attack (c) Membership Inference Attack (d) Model Memorization Attack [38, 39] . In practice, it can be implemented by model inversion attack, shadow model attack or power side-channel attack. An illustration of such an attack can be found in Fig. 5(b) . First, Model Inversion Attack mostly works in a white-box model, although it also can use blackbox attack [38] with lower effectiveness. Fredrikson et al. [39] showed a white-box attack that can \"learn sensitive genomic information about individuals\". The basic idea of [39] is to complete the target feature vector \"with each of the possible values, and then computes a weighted probability estimate that this is the correct value\", given the knowledge of a linear regression model ùëì . Then in [38] they extended the attack to facial recognition models to achieve two different targets: the reconstruction attack that produces \"an image of the person associated with a given label\" and the deblurring attack that generates the deblurred image of a certain individual given \"an image containing a blurred-out face\". The idea behind these attacks is \"to use gradient descent (GD) to minimize a cost function involving ùëì \". Overall, the model inversion attack works with a simple philosophy: we can reverse-engineer (find ùëì -1 ) by following the gradient in a trained network to adjust the weights and obtain the features for all classes in the network. Even for classes that we do not have prior information, we can still reproduce the prototype example. This type of attack suggests that any accurate deep learning machine, regardless of training methods, may leak information on the distinguishable classes. Extensive research has shown that generative adversarial network (GAN) generated sample data are similar to the training data. And thus, the results given by the model inversion attack may even \"reveal more private information about the training data compared to the average samples\" [8] . Second, Shadow Model Attack means the attacker trains other ML models to achieve the target. It can happen in either black-box or white-box way. For example, Ateniese et al. [8] designed a \"meta-classifier that can be trained to hack into other ML classifiers to infer patterns or private information from the training set\", e.g. they were able to extract accent information from trained speech recognition systems. Hitaj et al. [55] designed an attack in the context of collaborative learning. They consider the adversary is an insider of the collaborative learning process who wants to infer sensitive information from the peers. The adversary can see and use internal parameters of the model, so it is a white-box attack. The adversary uses GANs [45] to extract and reconstruct information of the victim. \"This process is similar to facial composite imaging used by the police to identify suspects, where the composite artist generates sketches based on eyewitness identification of the suspect's face. Although the composite artist (GAN) has never seen a real face, the final image is based on eyewitness feedback\" [55] . Finally, Wei et al. [164] proposed to use power side-channel attack on an FPGA-based convolutional neural network accelerator, which can successfully recover the input image using the power traces at the inference stage. Membership Inference Attack. Membership inference attack refers to acquiring the knowledge about whether a certain data record ( √¨ ùë• ‚òÖ , ùë¶ ‚òÖ ) belongs to the model's training dataset ùê∑ or not [98, 141] . An illustration of such an attack can be found in Fig. 5(c ). Shokri et al. [141] introduced a \"black-box membership inference\" that used a shadow training technique to imitate the behavior of the target model. The trained inference model is used \"to recognize differences in the target model's predictions\" on training and non-training inputs. They also found that overfitting, the structure and type of the model are the main factors that cause a model to be vulnerable to membership inference attack. Long et al. [89] and Yeom et al. [173] investigated \"the relationship between overfitting and privacy leakage\". Salem et al. [134] proposed a membership inference attack method using an unsupervised binary classification, \"which does not need to train any shadow model and does not assume knowledge of model or data distribution\". Membership inference attacks are also studied in Generative Adversarial Networks (GANs). For example, Liu et al. [84] trained an attacker network to launch membership attacks against Variational Autoencoders (VAEs) and GANs. Hayes et al. [52] focused on \"generative models in ML-as-a-service applications and train GANs to recognize training inputs\". Melis et al. [98] studied membership inference in collaborative learning. The attack is achieved by \"analyzing periodic updates to the shared model during training\". The reason that this attack is effective is that the gradients in neural networks are based on features, \"thus observations of the participants' gradient updates can be used to infer the feature values, which are in turn based on these participants' private training data\". Wang et al. [163] considered membership inference attack \"against the user-level privacy on the federated learning framework by the attack from a malicious server. The proposed attack framework exploits GAN with a multi-task discriminator, which simultaneously discriminates category, reality and client identity of input samples, and doing so recovers user-specific private data\". 3.1.4 Model Memorization Attack. Song et al. [144] first proposed the model memorization attack that targets recovering the exact feature values on individual samples. They consider a \"malicious ML provider\" specialized in model-training for the customers. In such a business model, the provider does not observe the training, but has access to the resulting model. He can steal the sensitive samples and encode the values into the model parameters or outputs. Another malicious party can retrieve sensitive information from the model during model serving. An illustration of such an attack can be found in Fig. 5(d) . Model memorization attack can happen both in white-box and black-box cases. In the white-box case, Song et al. [144] proposed several techniques for the adversary to encode sensitive data into the models. (1) LSB encoding: the adversary can encode the \"training dataset in the least significant (lower) bits of the model parameters\". (2) Correlated value encoding: the adversary can \"gradually encode information while training model parameters\". For instance, \"the adversary can add a malicious term to the loss function which maximizes the correlation between the parameters and the data he wants to encode\". (3) Sign encoding: similar to correlated value encoding, the adversary can use \"the sign of model parameters to interpret as bit strings\", e.g., positive parameters represent 1 and negative parameters represent 0. In the black-box case, the adversary is assumed to have no access to the model parameters. They designed a scheme in which the adversary can \"augment the training dataset with synthetic inputs whose labels encode the critical information\". Then the information is leaked via the outputs of these added inputs. Model memorization attack studies how malicious training algorithms deliberately create models that leak information about their training data sets. \"This threat model is more generous to the adversary, so it can extract more information about the training data than any other attack\" [144] . Private Machine Learning Schemes In this subsection, we present several private ML schemes, including encryption, obfuscation, and aggregation. Encryption. Encryption or cryptography-based methods can be divided into two groups: ‚Ä¢ Encrypting training data. The mainstream technique is homomorphic encryption. As adding homomorphic encryption to the process will make the process at least an order of magnitude slower, initially it is applied on training data for relatively simple classifiers [13, 15, 47] . For example, Graepel et al. [47] found that training over encrypted data is possible when the training algorithm can be expressed as a low degree polynomial. Bost et al. [13] applied this technique in three classifiers: hyperplane decision, Naive Bayes and decision trees. Then researchers try to extend the work to deep neural networks (DNN). Dowlin et al. [29] proposed CryptoNets which demonstrates how to efficiently convert learned neural networks to make it applicable to encrypted input data. While Hesamifard et al. [54] proposed a framework to train the neural network over encrypted data. Li et al. [77] investigate the case of collaborative learning where datasets are encrypted with different keys, and propose a solution based on multi-key fully homomorphic encryption (FHE). ‚Ä¢ Encrypting ML model. The encryption technique is also used to protect the model privacy. Phong et al. [126] proposed to use \"additively homomorphic encryption on the gradients\". The scheme can prevent information leakage to the \"honest-but-curious cloud server\" in the condition of collaborative deep learning. Overall, training neural networks especially DNNs over encrypted data is still challenging. Computational complexity is a major challenge. The network is slow even when trained on plaintext. Adding homomorphic encryption to a process will make it at least an order of magnitude slower. Since the level of the computed polynomial is proportional to the number of backpropagation steps done, the deceleration is more likely to get worse. Another challenging aspect of encryption is the lack of data scientists' ability to examine data and train models, correct mislabelled items, add functionality, and further tune the network [29] . Secure multi-party computation (SMC) is the extension of encryption under the multiparty setting. In SMC, multiple non-colluding parties use a combination of encryption and oblivious transfer to privately finish the computation without seeing the individual components. For ML, it means to compute model updates without having access to both the data and the model. SMC has been used for a variety of traditional ML models, including decision trees [7] , linear regression [30, 66, 107, 135, 136] , logistic regression [143, 170] , Naive Bayes classifiers [157] , and ùëò-means clustering [16, 62] . In general, SMC techniques impose non-trivial computational overheads and their application to privacy-preserving neural networks especially deep learning remains a challenging task. Se-cureML [101] is a recent example of SMC. It uses \"two-party computations to privately train logistic regression models and neural networks\". In summary, SMC based method can cover both data/model privacy concerns, at the cost of communication overhead. Obfuscation/Perturbation (Differential Private Learning). Obfuscation mechanisms in the context of privacy protection in ML aim at reducing the precision of the data or model. It is can be achieved by adding noises to the model parameters or the original dataset. It is very popular because the DP scheme is usually implemented by obfuscation in practical applications. The obfuscation can be applied to the model or data. When obfuscation mechanism is for the model, it has another name in the community, i.e., differentially private machine learning. There are some early works on traditional machine learning with differential privacy. For example, Rubinstein et al. [132] proposed differentially-private support vector machine (SVM) learning mechanisms by adding noise to the output classifier and they yield close approximations to the non-private SVM. Chaudhuri et al. [18] provided the model objective perturbation to produce deferentially private empirical risk minimization (ERM) classifier. Song et al. [146] derived differentially private SGD for general convex objectives and validated the effectiveness of the approach using logistic regression for classification. One of the well-known early methods of implementing differential privacy in deep learning is [140] . They trained the ML model \"in a distributed manner by updating the selected local gradients and adding noise to them within the privacy budget of each parameter\". Based on this work, Abadi et al. [2] introduced \"a simpler differential private SGD (DPSGD) algorithm that ensures DP by cutting the gradients to a maximum ùëô 2 norm for each layer\". And then add the noise bounded by the \"ùëô 2 norm-clipping-bound\". It was shown that \"high-quality models can be trained through privacy under a moderate privacy budget\" with the DPSGD algorithm. In DPSGD, the DP noise is added to the gradients and the whole training process involves multiple iterations. Therefore, it is important to compute the overall privacy loss of the training, i.e, privacy accounting. Although the composition theorem [33] can be used to generate the overall privacy loss, it can be quite loose. Abadi et al. [2] introduced a moments accountant method that can track privacy loss across multiple training iterations and generate a tighter bound. Another closely related notion is R√©nyi differential privacy, which \"offers quantitatively accurate way of tracking cumulative privacy loss\" throughout a multi-round DP mechanisms [100] . Prior to [95] , all considered methods used \"record-level differential privacy as a framework to protect private information\". In many real-world work environments, users have multiple data sources. They may be relevant and should be protected as a whole. Therefore, in some cases, the DPSGD method results in a loss of privacy at a higher level (e.g., user level). McMahan et al. [95] introduced a \"user level differential private algorithm called the DP-FedAvg algorithm to protect all the data of a user\". Instead of limiting the \"contribution of a single record\", the DP-FedAvg algorithm limits the contribution of the user data set to the learning model. The DPSGD algorithm was \"combined with the FederatedAveraging algorithm\" from [14] which uses a server that performs model averaging. Obfuscation on training data has not been investigated extensively in the context of ML, because it has been deemed as similar to traditional big data privacy. One notable research from Zhang et al. [179] proposed an obfuscate function and applied it to the training data before feeding them to the model training task. This function adds random noise to existing samples, or augments the dataset with new samples. By doing so, sensitive information about the properties of individual samples, or statistical properties of a group of samples, is hidden. Meanwhile, the model trained from the obfuscated dataset can still achieve high accuracy. Apart from the above-mentioned works, there are other research works in the closely relevant area, such as tensor/matrix factorizations and functional optimization schemes. In more detail, the authors of [59, 60] discussed differentially private algorithms for tensor decomposition, in both centralized and distributed settings [60] . The authors of [40] applied a DP framework in the matrix factorization process with four different possible perturbation: input perturbation, private stochastic gradient perturbation, alternating least squares (ALS) with output perturbation, and output perturbation. The authors of [178] proposed a functional mechanism framework to achieve an ùúñ-DP in analyses, which involves solving an optimization problem with a perturbed objective function. Aggregation. Aggregation is a technique that generally comes along with distributed/collaborated learning, in which multiple parties join a machine learning task while wishing to keep their respective dataset private. Aggregation can be applied both in and after the training process. It often works together with the encryption scheme (especially SMC) when used during the training process. For example, Pathak et al. [121] proposed an aggregation scheme for independently trained classifiers. They average the parameters using DP and SMC. But they do not consider the accuracy of their approach formally. The first part of later research [140] also focuses on aggregation. They reduce the communication costs and improve the model accuracy by selectively \"sharing a subset of parameters in each round of communication\". Another popular framework using aggregation for collaborative learning is federated learning [69, 94] introduced by Google, which has been described before. Compared with [140] , federated learning considers different constraints on the training dataset, i.e., Non-IID, unbalanced, and massively distributed, which is claimed to be more practical in some scenarios such as using mobile devices for the local training. Federated learning algorithm introduces techniques for quickly and safely aggregating gradients. This scheme focuses on optimizing the communication efficiency of the aggregation process and making the protocol robust against adversaries. However, it lacks guarantees on the amount of user information leakage during training. Bonawitz et al. [12] enhance the privacy of federated learning by leveraging SMC to compute sums of model parameter updates, i.e., federated Learning with secure aggregation. On the other hand, using aggregation schemes for privacy protection in ML after the training process, i.e., using ensembles of models is also reasonable. If an ensemble contains enough of models, and each model is trained with disjoint subsets of the training data in a distributed manner, then \"any predictions made by most of the models should not be based on any particular part of the training data\" [1] . The private aggregation of teacher ensembles (PATE) is based on this idea [116] . In more detail, the ensemble is seen as a set of \"teachers\" for a new \"student\" model. The student is linked to the teachers only by their prediction capabilities. And the student is trained by \"querying the teachers about unlabelled examples\". The prediction result is disjoined from the training data through this process. Therefore the data privacy can be protected. The privacy budget for PATE is much lower than traditional DP ML approaches. But it may not work in many practical scenarios as it relies on an unlabelled public dataset. Until now, the above works consider aggregation from the perspective of the model. Dwork et al. [34] proposed a scheme that aggregates the prediction output rather than the model. In more details, they partition the dataset ùê∑ into several subsamples ùê∑ 1 , . . . , ùê∑ ùëü and run a nonprivate learning algorithm on each of those subsamples to obtain predictors ùëì 1 , . . . , ùëì ùëü , then use a differentially private aggregation technique on values ùëì 1 (ùë•), . . . , ùëì ùëü (ùë•) and output the result. This subsample-and-aggregate technique is easy to implement as it does not require a new learning algorithm. It focuses on training data privacy via private prediction. Summary on Private ML In this subsection, we sum up the key points on private ML. Discussions of attack models. We summarize the attack models and related papers in Table 3 and Table 4 . The attack models listed in Section 3.1 are not interdependent. For example, many attacks might be launched on top of the model extraction attack, because it converts the condition from black-box to white-box. Once the black-box attack is finished, the adversary can continue to launch the white-box attack, e.g., a model inversion attack followed by a model extraction attack. 3.3.2 Attack models and protection schemes. Table 5 summarizes the private ML schemes and their effectiveness against different attacks in different situations. Generally speaking, encryption can maintain the adversary's knowledge to a black-box case, thus it is effective to white-box attacks like model inversion attack. Obfuscation [179] influences most attacks as it blurs the information to reduce the privacy risk at the cost of utility. Aggregation is mostly used in distributed systems and often comes along with the other schemes. Another important question is the relationship of attack models, protection schemes and DP. Among all the mentioned attack schemes, the membership inference attack works along with DP, because the DP definition makes individuals indistinguishable. The other attack models cannot be well countered and evaluated by DP. For example, model inversion uses the output of a model to infer certain features of the hidden input. From a DP perspective, it does not necessarily lead to privacy breaches. For example, in a face recognition scenario, a single person is associated with an output class of the model. As all training images for this class include various photos of the same person, an adversary can orchestra a model inversion attack by creating an artificial image capturing the average information from the person's photos. In most of the cases, this average can be identified as that person. In summary, the average of the features produced by the model inversion can represent the entire output class at most. It does not construct a particular member of the training data set. Moreover, given an input and a model, it determines whether to use that particular input to train the model. Therefore, model inversion attack is even effective with DP applied collaborative learning [140] and Federated learning [69, 94] . Because DP is being applied to the parameters of the model, and the granularity is set at the record/instance level. However, once the model becomes accurate, it must eventually contain noise added to the learning parameters. Model inversion attack works as long as the model can accurately classify the class and will generate representations of that class. It should be noted that the DP scheme proposed in [140] can only prevent the recovery of specific elements, that is, membership inference attack. Overall, the DP criterion cannot provide comprehensive privacy evaluation in private machine learning, due to the complexity of the data (unstructured and multimedia data) and privacy protection target (not only membership, but also features of the dataset). Therefore, defining new privacy metrics and criteria is still an open question. Privacy in Distributed Learning Systems. Training ML in a distributed manner can naturally provide a certain level of privacy protection, as the local training data points are usually not shared among users. Moreover, different privacy protection schemes in centralised learning, such as encryption, perturbation, can be easily extended to the distributed learning settings [169] . In this sense, private ML in distributed systems have a lot in common with that of centralised ML. But there are several special features. ‚Ä¢ Distributed ML requires some forms of data sharing among the training nodes because distributed ML is fundamentally different from stand-alone ML. Such shared data, albeit not raw data, could take the forms of model parameters, feature vectors, classification results, etc., and such data would still reveal users' privacy from an information theory point of view. Hence, we need to carefully design the data sharing mechanism in distributed ML. ‚Ä¢ SMC and aggregation are quite often adopted in the distributed ML systems. However, the above mechanisms are not adequate to protect users' privacy, especially when there exist inside attackers [55, 105] . 3.3.4 Backdoor attacks and privacy. Some recent work raised the awareness of backdoor attacks against machine learning and deep learning systems, where misclassification behaviours are hidden in models and can be triggered by specific inputs. Gu et al. [49] introduced BadNets that builds a backdoor in DNN models by injecting a square-like trigger with a fixed location to some training data with a target label. Ahmed et al. [133] extended this work by using dynamic trigger patterns and locations. Liu et al. [87] proposed a backdoor attack called the Trojan attack, which reverseengineers the target model to synthesize training data so that it does not require access to the original training set. Yao et al. [172] proposed a latent backdoor attack method in which they embed the backdoors in teacher models to survive the transfer learning process. In general, current backdoor attacks are mostly considered to be security risks, e.g., it may cause various severe consequences in critical ML applications like autonomous driving. But we can also expect potential privacy risks in the future, for example, backdoor attacks against authentication systems that might enable an adversary to access sensitive information. MACHINE LEARNING AIDED PRIVACY PROTECTION In this section, we will focus on the case that ML is used to help privacy protection. We will first discuss traditional data privacy risks and threats. These threats have existed for a while, but the newly emerging ML gives us new tools to combat them. Attack and Threat Models Along with the proliferation of the mobile network, people spend more and more time on the Internet, using web-based applications, mobile applications and social networks. These all pose privacy risks. For example, online photo sharing has become more popular than any time before. Users are increasingly sharing their images on various social media, such as Facebook, Google+ and Flickr. Shared images can reveal sensitive information about people and their surroundings [148, 176] . Consider a person sharing a photo of a family gathering. Not only this photo can expose the people who may or may not wanted to be in the picture, but it can also reveal sensitive information about the family such as religious beliefs, traditions, and food habits. Therefore, sharing photos online can severely violate privacy and disclose sensitive information [37] . Major traditional privacy attacks include identification attacks, inference attacks, and linkage attacks, as shown in Fig. 6 . (1) Identification attack: Identification attack identifies a user's name or identity-based on some public dataset [76] . It is also called re-identification [53, 58] when anonymisation is reversed. Such kind of attack is illustrated in Fig. 6 (a). ( 2 ) Inference attack: This type of attack aims at \"analyzing data in order to illegitimately gain knowledge about a subject\" [70] . Such an attack is illustrated in Fig. 6 (b). ( 3 ) Linkage attack: The adversary aims to achieve a target's information by correlating multiple data sources. For example, Narayanan et al. [104] showed that an adversary \"can identify a subscriber's record in the Netflix Prize dataset\", linking it to an Internet Movie Database. Such an attack is illustrated in Fig. 6(c ). Machine Learning Aided Privacy Protection Schemes Many privacy protection schemes have been introduced. Obfuscation/perturbation [31, 140] , anonymization [5, 6] , reducing information sharing [82, 142] , and cryptographic mechanisms [43, 127] are the major technologies. However, the traditional privacy protection schemes focus on structured data, such as an entry in the databases [162] . With the introduction of new applications such as Internet of Things (IoT) and vehicular networks, both the volume and the complexity of the data is increasing. Traditional protection schemes cannot handle all cases and it also becomes more difficult for both common users and even data curators to understand the risk, select correct schemes and manage their privacy. Under these circumstances, ML has been introduced to enhance privacy protection during the past few years. The efforts including research in several aspects. ‚Ä¢ Privacy risk assessment and prediction: Assess and predict the privacy risk for the user during the processes of \"access\" and \"sharing\". As shown in Fig. 7 (a), ML is used to evaluate both the input and output data streams to find the risk and then privacy protection schemes can be deployed accordingly. ‚Ä¢ Personal privacy management assistant: This includes privacy policy evaluation, user preference prediction and management, as shown in Fig. 7(b ). ‚Ä¢ Private data release: Publish datasets with privacy guarantee. The schemes are generally adopted by data curators rather than an individual user, as shown in Fig. 7(c ). Privacy Risk Assessment and Prediction. The privacy risk exists either when the user is just accessing the application (passively collected information by malicious attackers) or sharing on social networks (actively sharing information). In both cases, ML can help to prevent the loss of sensitive information. An illustration of such a defence mechanism is shown in Fig. 7(a) . Website and application privacy risk prediction: ML can make browsing the websites safer. The proposed browser extension in [137] collects information about websites that users visit and provides feedback to users based on ML to let them know the privacy quality of the site. Manek et al. [92] proposed a method based on a Bayesian classifier to detect and identify websites that can be malicious or threatening to the privacy of users. The proposed approach analyzes online reviews written for websites to decide whether they are reliable or not. The work in [42] uses an SVM classifier to rate the privacy risks of applications. The results indicate that privacy risks can be identified with over 90% accuracy. Understanding the privacy risks of mobile phone applications with the aid of ML have been considered in [9, 46] . Identifying sensitive information when sharing: Identifying sensitive information in multimedia data has been difficult in the past. With the help of the state-of-the-art ML techniques, users can prevent loss of their personal information while sharing their photos on social media. Squicciarini et al. [147] considered visual-content features and images' metadata to develop and contrast several learning models. The ML models can classify the photos and evaluate the degree of sensitivity so as to make the decision based on past decisions of the users. Yu et al. [174] proposed a tool called \"iPrivacy (image privacy)\" to reduce the burden of specifying privacy setting by users when they are sharing photos online. iPrivcy utilizes ML to automate the process. It finds privacysensitive objects from images and classifies them according to their privacy sensitivity. Based on the classification, iPrivacy notifies the users if there are objects, which should be suppressed/masked due to privacy concerns before sharing. Moreover, iPrivacy provides privacy settings recommendation based on user preferences and shared images. Orekondy et al. [113] proposed the first large-scale private images dataset, with pixel and instance level annotations. And they proposed the first model to automatically redaction various private information. Hasan et al. [51] proposed a method to automatically identify bystanders \"solely based on the visual information present in an image\". Personal Privacy Management Assistant. As the user connectivity increases and web applications become ubiquitous, the responsibility of privacy management transfers more and more to individuals. Unfortunately, given the complexity of the environments and the lack of awareness about privacy attacks by adversaries, it is improbable that the users can manage and fine-tune their privacy preferences correctly [93] . Therefore, there is an immediate need to develop automated privacy management systems to help users in protecting their privacy. An illustration of such a defence mechanism is shown in Fig. 7(b) . The authors of [3] indicate that users continuously modify their privacy requirements to reach their expected level of privacy, and also, appropriately change their privacy preferences. Moreover, mobile and web applications are attempting to customize their services according to individual preferences to grant personalized experience to customers. Such a customized service results in potential risk for the users [122] . This evidence points to the fact that it is crucial to develop assistants to help users with the management of their privacy configurations. ML can be an invaluable asset in this regard. For example, it can help users to manage their privacy configurations and reduce the burden of time and human resources required to ensure the preservation of privacy. We have divided the applications of ML for privacy management in two broad categories: (i) privacy policy evaluation, and (ii) user preference prediction and management. Privacy policy evaluation: Users are usually prompted to agree with the provider's privacy policies when almost using any software and web applications. Privacy policies provide complete information on the collection, storage, and sharing of personal data. Therefore, they are critical to the privacy of users. Unfortunately, most of this information is written using technical jargon and challenging to read terms. Hence, most of the readers prefer to accept the policy unconditionally without thoroughly realizing the consequences [24] . To help users with the decision making, Costante et al. [25] developed a system to evaluate the completeness of privacy policies based on preferences of the users. The system uses natural language processing to analyze and verify the existence of the privacy measures that users specify, and also, assess the level of completeness. Nugent et al. [108] graded the privacy policies that the users encounter based on factors such as security, cookies, and purpose which helps users to check the results and identify if their desired privacy requirements are satisfied. Tesfay et al. [153] proposed an ML approach to \"summarize the long privacy policies\" into a short paragraph so that it is readable and understandable for users. Shayegh et al. [139] considered methods to improve the privacy notices given to users in IoT networks. With the aid of ML, the authors extract notice and choice statements from the privacy policies for IoT devices, so as to help users to better understand the implications of privacy notices. Lebanoff et al. [73] investigated automatic detection of vague contents on privacy policies and used GANs to characterize the vagueness of sentences. User Privacy Preference Prediction and Management: Another difficulty in user privacy protection is caused by the fact that each user has a different privacy sensitivity and preference. Nowadays, applications often provide many functionalities with different levels of privacy guarantees. While installing the applications, users are usually prompted for permissions to access resources that have an impact on their privacy. It is important that the users can well coordinate their own privacy preference with the actual privacy risk. ML techniques are implemented to predict user privacy preferences and help decision making. It was initially proved feasible as some early studies found that user privacy preferences are related to some statistical and environmental parameters. For example, the quantitative research in [166] uncovered that a significant number of users would rather prevent at least one permission request involved in the study. Also, several works have shown that the context of the applications is highly related to user privacy preferences [112, 165] . Lee et al. [75] surveyed 172 participants and uncovered contextual factors that violate the privacy of users in IoT. Based on the contextual factors and features, ML models can be developed to predict user privacy preferences and take privacy management decision. Mehrpouyan et al. [97] used openness, conscientiousness, neuroticism, extroversion, and agreeableness as inputs to ML models to predict desired users' preferences. Das et al. [26] generated ML models of people's privacy preferences and expectations. Wijesekera et al. [165] proposed a run-time permission system to infer privacy requirements of users automatically. The proposed system grants the resource allocation permission based on the type of the application requesting the permissions, the request time, and in what circumstances it is requested. Liu et al. [82] investigated ML to enhance privacy decision-making experience. The results show that providing users with \"recommendations based on clusters of like-minded users and using predictive models of people's privacy preferences work to the users' satisfaction\". Wijesekera et al. [166, 167] built a classifier to work as a middle-man and make privacy decisions on behalf of users. The classifier adjusts and preserves privacy by changes that happen in the context predicated on the past behaviors of the users. Orekondy et al. [114] proposed a method named \"Visual Privacy Advisor\" that \"extends this concept to image\" contents. They classify \"personal information in images into 68 attributes and train models that directly predict such information from images\". A user study has been done to understand the privacy preferences with respect to these attributes. They also proposed models that \"predict user specific privacy score from images\". Yuan et al. [175] presented an ML approach to decide whether to share a picture with a specific requester at a particular context, and if yes, at which granularity. Private Data Release. Database release is currently an important process in data analytic applications. Different entities generate different types of data, e.g., health data from medical centers. Then, such data will be transmitted to data custodians such as government agencies. Then, the data custodian maintains a platform that organizes, stores and provides data access to data consumers, such as other government departments, individuals, analysts, etc. Privacy preservation processing is highly required when the data custodians release the data. An illustration of such a defence mechanism is shown in Fig. 7 (c). A frequently used traditional private data release mechanism is obfuscation by adding noise to the original dataset. Whereas the ML techniques provide a new solution to this problem, i.e., using a generative neural network (GNN) or generative adversarial network (GAN) [45] to generate synthetic dataset [4] . Although the technique of GNN itself has existed for a while, using it for private data release has just been linked to privacy preservation very recently. Denton et al. [27] used the GAN framework in the context of image processing to generate natural synthetic images. Gregor et al. [48] introduced a model called \"Deep Recurrent Attentive Writer (DRAW)\" to create synthetic images. The principal idea of the approach is to use two recurrent neural networks as encoder and decoder trained endto-end with SGD. Vinyals et al. [160] proposed a generative model predicated on recurrent neural network architecture. The approach combines the natural processing ML tools with computer vision for the generation of natural scenes. Using generative models has also been considered for the generation of audios. Oord et al. [158] introduced a DNN model to produce raw audios and applied the approach to \"text-to-speech and validated by human listeners for natural sounding\". A modified version of the proposed model is used for singing synthesis in [11] . Kulkarni et al. [71] created spatiotemporal trajectories in large scale by training the models based on realistic data, and then, creating synthetic data using the trained models. The authors investigate the utility-privacy trade-off of the approach by experiments. Ouyang et al. [115] proposed a non-sequential nonparametric generative model for spatiotemporal trajectories. The authors generate \"synthetic data by training a generative adversarial neural network, which can learn geographic patterns\". Liu et al. [86] aim at the addition of geo-privacy protection layer for publication of spatiotemporal datasets based on synthetic trajectory generation. Choi et al. [23] proposed an approach for the generation of synthetic patient records based on GANs and autoencoders. In this work, the performance of the proposed generative model is examined by comparing the generated synthetic patient records with the real data. Cheung et al. [21] used GNNs for the transformation of sensitive images so that they can preserve privacy of individuals. The authors focus on the generation of synthetic facial images and how they can be used for classification of actual images. Zhang et al. [180] proposed a novel approach based on GNNs to increase privacy of users while releasing semantic rich data such as text, image, and video. Triastcyn et al. [156] used GAN to generating artificial data that retain statistical properties of the real data while reducing the risk of information disclosure. Sun et al. [149] proposed GAN-based head inpainting obfuscation technique to preserve the identity of users when sharing their photos online. Huai et al. [57] considered the differentially private release of crowdsourcing data. They proposed the PrisCrowd approach \"in which the data collector learns about underlying patterns of the data and then samples a set of candidate synthetic data from the learned density. The synthetic data are subjected to a privacy test and the ones that pass will be released\". Overall, the latest deep learning techniques show the ability to synthesize fake dataset that is statistically similar to the original one. This technique can be used for private data release. Fig. 8 presents the generative model framework used for privacy preservation of rich semantic data. The process can best be explained by an example. Consider a clinical data sharing scenario, in which the data curator instead of directly releasing the data, trains a deep generative model using the original data in a differentially private manner, and then publishes synthetic dataset generated by the model. In a more general case, the data curator may publish the deep generative model from which \"an unlimited amount of synthetic data for arbitrary analysis tasks\" can be produced [171] . The use of generative models can significantly increase the privacy of users as the training process of the models can be conducted based on synthetic data instead of the real data belonging to individuals. Meanwhile, the utility of the dataset can be guaranteed as the statistical similarity of models trained based on synthetic data and realist data has been shown repeatedly in the literature. For example, Park et al. [120] proved the statistical similarity of the generated synthetic tabular data and original data. Xu et al. [171] developed training deep neural networks for the generation of synthetic data that closely resemble the actual medical records of patients. Although research in GNNs for privacy preservation is in its initial stages, the outlook of the approach is promising. Generation of synthetic data is particularly crucial as traditional methods such as anonymity and obfuscation are ineffective for privacy preservation of semantic-rich data. Moreover, this approach is not associated with the drawbacks of other traditional anonymization approaches such as having background knowledge or linking the data to other sources. Summary on ML-aided Privacy Protection The three different groups of ML aided privacy protection schemes introduced in Subsection 4.2 work in various stages of privacy protection. Privacy risk assessment and prediction is a pre-process before privacy protection, that identifies what do we need to protect. Personal privacy management assistants help to improve access control over sensitive information. Private data release can be applied directly to the data. These protection schemes do not have a one-on-one relationship with the attack models listed in Subsection 4.1. They can be effective against multiple attack models and will work best if combined correctly in specific scenarios. The two main types of ML models used for privacy protection are classification and object detection. Classification is used for privacy risk prediction and assessment. Object detection is used for identifying sensitive information. Additionally, schemes discussed in 4.2.1 do not directly provide privacy protection. They are currently playing a supporting role, and other subsequent privacy protection schemes are still needed. GNN opens a new direction for privacy protection research, especially for unstructured data such as image and video. But it is still challenging, as there are no unified metrics for privacy measurements in those complicated cases. MACHINE LEARNING-BASED PRIVACY ATTACKS AND CORRESPONDING PROTECTION SCHEMES Besides serving as a privacy protection tool, ML can also be used as an attack tool. It urges us to revisit the definition and scope of privacy. In particular, the emerging deep learning technique can \"automatically collect and process millions of photos or videos to extract private/sensitive information from social networks\" [80] . Traditional privacy-preserving methods are over-powered when combating deep learning tools. It is time to seriously discuss new threats and corresponding solutions. Attack and Threat Models The riskiest personal information leakage source is the social network. While there are a variety of social network platforms enriching people's interactivity and relationship, the shared posts including check-ins, activities, thoughts (tweets, status updates, etc.), pictures, videos often come along with sensitive information. The information poses high privacy risks and they are likely to hand over their privacy unintentionally. A growing number of companies and start-ups specialize in analyzing shared pictures on social media to exploit them for commercial purposes or selling them to other companies. Therefore, the most advanced DNNs have been used to launch privacy attacks. For example, the adversary can use geo-location information to initiate a localized attack that focuses on finding the position and time information of the person. Gu et al. and Mahmud et al. [50, 91] showed a dangerous attack that is designed to \"find important locations such as homes and workplaces\". There have been some researches discussing the home location identification problem, either based on the \"content of the posts\" [20] , or the \"geo-tags in the check-ins\" [22] . And \"the research shows that the identification accuracy might be over 90% in many cases\" [81] . Besides the simple location information, multimedia data poses more risk under the attack of ML tools. Companies apply advanced DNNs to cluster photos or infer preference of users to facilitate marketers to send targeted ads [99, 130] . DNNs are considered one of the most practical tools in ML as they take advantage of efficient training algorithms and large datasets which enables them to outperform other existing ML techniques. The power of such ML tools has become a problem itself that may compromise the privacy of photos once they are shared on social media and a challenging problem that needs to be addressed. The privacy of sensitive data, photos and videos become more crucial in IoT networks, as users might not even be aware of their information such as pictures and videos being recorded. For instance, areas controlled under surveillance cameras can severely compromise user privacy as people lose control of how their photos and videos are being captured and managed. It is likely that the surveillance system applies techniques such as face recognition and detection to identify the users without their permission. Pew Internet survey in 2014 reported that over 91 percent of participants \"strongly agree\" or \"agree\" that \"they have lost their control over how their personal information is being collected and used by companies\" [17] . Major ML attack models include re-identification attacks and inference attacks, as shown in Fig. 9 . These attack models are different from those described in Section 4 in the sense that ML is used as an attack tool here. ‚Ä¢ The re-identification attack can be launched by face recognition techniques. The recent advance in DNN makes it more harmful from two aspects. First, the process becomes automatic with high accuracy [67, 151, 168] . Second, traditional protection schemes such as obfuscation no longer work effectively [96, 109] . An illustration of the re-identification attack can be found in Fig. 9 (a). ‚Ä¢ Inference attack has also become more powerful when equipped with ML. ML classifiers can be used to infer a target user's private information (e.g., location, occupation, hobby, political view) from its public data (e.g., twitters, movie rating scores) [20, 22] . Moreover, a series of research work have demonstrated how the advanced artificial neural networks can be used as an adversarial tool to detect sensitive information in images, including people's age [63] , relationship [150] and vehicle license plates [181] from ordinary or even obfuscated images. An illustration of the inference attack can be found in Fig. 9(b ). Therefore, it is quite urgent to accelerate the research on privacy protection schemes against ML aided attacks. Protection Schemes Against ML-based Attacks There has been some preliminary research in this area. For privacy protection against traditional ML attack, Liu et al. [81] designed community-based information sharing scheme that changes the overall spatial and temporal features so that the clustering-based privacy attack [83] no longer works. The problem becomes more challenging when deep learning is involved. The solutions may come from a better understanding of deep learning itself. Some researchers recently found that there are limitations to deep learning. Specifically, \"it is proved to be vulnerable to some well-designed inputs termed adversarial examples\" [36, 138] . Szegedy et al. [152] first discovered that the superposition of \"imperceptible noise onto the original image\" would mislead DNNs to the wrong classification. Then, Goodfellow et al. [45] proposed the \"fast gradient sign method (FGSM) that can be used to generate this type of adversarial examples\". Other algorithms to generate such noise can be found in [72, 103, 131] . According to [119] , the primary reason for why neural networks are vulnerable to adversarial examples is the linear nature of the neural networks. The authors formalize the space of adversaries against DNNs, which are mostly originated from ML techniques itself. In simple words, ML is used as a tool to breach the ML classifiers. Kurakin et al. [72] focused on adversarial training and how they can be scaled to large datasets. Sharif et al. [138] proposed an algorithm for manufacturing adversarial examples based on ML to disable DNN detection systems from finding objects in shared photos. Additionally, a significant point about adversarial examples is its transferability property [45] . It means that if they are able to fool one model, they are often likely to mislead another model with a different set of parameters and architecture [152] . This is even true if the other model is trained on a different training set or model [117] . This leads to the idea of universal perturbation [102, 128] . It is even possible to \"generate adversarial examples that fool both human and computer alike\". Elsayed et al. [35] exploited ML to construct adversarial examples that transfer from models created based on computer vision to the human visual system. The authors generated adversarial examples without utilizing the parameters of the model's architecture, and then mimic the visual processing of humans using ML. Enlighted by the idea of adversarial examples, researchers started to focus on the generation of adversarial examples based on ML to improve the privacy of users against attacks mostly based on DNNs. Liu et al. [88] proposed an algorithm that is against automatic detection using adversarial examples based on the \"Faster RCNN framework\". Jia et al. [65] proposed a two-phase framework called AttriGuard to defend against attribute inference attacks launched by a classifier. Liu et al. [80] investigated schemes for using adversarial examples in ML systems so that they cannot identify the sensitive information from images. Oh et al. [110] set up a game-theoretical framework and studied the effectiveness of adversarial image perturbations for privacy protection. Li et al. [78] proposed to use adversarial perturbation for face de-identification. Friedrich et al. [41] proposed a privacy-preserving shareable representation of medical texts for a de-identification classifier. Summary on Privacy Protection against ML Previously, the common understanding of privacy protection is to prevent human adversaries from knowing some sensitive information about people. For example, obfuscating faces in images is a well-researched topic. However, the situation has dramatically changed recently. First, the growth of data volume has reached a point where it is physically impossible for anyone to browse everything with their eyes. Second, as a result, people increasingly rely on machines with advanced algorithms to extract relevant information of interest. Third, the booming of ML open source community makes ML tools easy to be obtained by anyone. This brings up a new problem, that is, it is now possible to automatically process data to infer sensitive user information, such as personal identity, social relationships, location, and context. Indeed, ML has recently been used by malicious parties as an efficient tool to launch new types of privacy attacks, especially for social media data. Therefore, we would expect that privacy protection against machines is as important as privacy protection against humans. ML-based privacy attacks are more challenging to defend against, due to three main reasons. First, the average user is not aware of the capability of state-of-the-art ML methods in extracting personal information. Second, privacy in some contexts such as multimedia data is not obvious. Third, privacy threats also arise from organizations and government sectors that collect and analyze data on a large scale. Therefore, we need to prevent ML algorithms from automatically mining private information, either intentionally or unintentionally. In summary, privacy protection against the fast-evolving ML techniques is the most challenging task among all three categories we discussed in the paper. The methodology is to exploit the weakness and limitations of ML methods. Although there have been some initial solutions to this problem using adversarial machine learning, there are still many research problems that require further investigations. OUTLOOK AND FUTURE DIRECTIONS Significant previous work focuses on making ML algorithms differentially private to preserve the privacy of training sets. However, we should be aware that machine learning, as a whole, also provide potent tools for privacy research (not just for the training datasets), both from attack and defense perspectives. Perturbation in Deep Learning The goal of perturbation in deep learning is to train a model while ensuring DP concerning information about individual training examples. Theoretically, the noise can be added to either the input data, the model parameters (through gradient updates), or the model output. In practical, the majority of work proposed to inject noise into gradients. The main disadvantage of this group of methods is that amount of injected noise is dependent on the number of training epochs, and it potentially can accumulate too much noise due to the significant number of parameters. Directly adding noise to input data is an option, but it is similar to a typical big data privacy problem and does not closely related to deep learning. Output perturbation and objective perturbation seem to be reasonable directions in the future. Output perturbation adds noise to the output of the ML system, e.g., the logits at the prediction stage. This method is fast and easy to implement. However, it can suffer from degradation from an attack of repeated querying by an adversarial. Therefore, it is important to restrict the number of queries [129] . One potential solution is to use output perturbation in certain intermediate outputs, such as the teacher voting output in PATE frame work [116] . Objective perturbation is one of the most effective methods for differential privacy ML. This technique adds a random linear term to the objective function. Objective perturbation has been extensively studied in convex optimization. Recently, Iyengar et al. [61] has provided a practical algorithm for differentially private convex optimization, which is a big step towards practical deployment of this technique. Moreover, Neel et al. [106] has extended this approach to non-convex optimization problems. Despite the success in traditional ML, applying objective perturbation to deep neural network is still challenges due to several obstacles: 1) the sensitivity calculation is difficult because the objective functions of deep learning models are mostly non-convex and do not have closed-form expressions; 2) the privacy guarantee is implicitly based on the rank-one assumption on the Hessian of the loss, which is difficult to verify; 3) the privacy guarantee holds only at the exact minima (at least the approximate minima as proposed in [61] ) of the optimization problem, which is hard to be guaranteed in practical deep learning systems. One possible solution is to use a convex approximation of the loss function [124] . However, the approximation error might outweigh the reduced perturbation due to smaller sensitivity. It is expected to see more effective methods following this path. Moreover, instead of perturbing the final output, it is also possible to add noise to the middle layers of the neural networks. Lecuyer et al. [74] proposed the PixelDP framework that includes a DP noise layer in the DNN. Although the purpose of PixelDP is \"to increase robustness to adversarial examples\", the idea can be further investigated to serve for privacy preservation. For example, PixelDP scheme enforces that the output prediction function is DP provided the input changes on a small number of pixels (when the input is an image). Potential extensions to PixelDP include: 1) enforcing DP for given different input samples so that it can provide privacy preservation for the training set against membership inference attacks; 2) adding DP noise to the hidden layer of an autoencoder. With the post-processing property of DP, the output of the autoencoder remains to be DP as well. This idea is briefly mentioned in [74] . But we can further explore it in different applications. For instance, we can protect a social network image by generating a perturbed version using this autoencoder with a DP guarantee. Defending ML-based Privacy Attack: Adversarial Examples As we have discussed in Section 5, when ML is used as a privacy attack method, adversarial examples become a powerful way of privacy protection. Despite the preliminaries work on this topic, there are several issues that need to be solved: ‚Ä¢ Adversarial example generation methods fall into two categories of attack scenarios: whitebox and black-box. The research of using an adversarial example for privacy protection usually assumes that the deep learning model is known, using the white-box setting. In practice, the black-box scenario seems to be a more realistic assumption, e.g., the latest black-box adversarial generation methods such as ZOO [19] , N attack [79] and AdvFlow [28] , could be potentially used for privacy protection. ‚Ä¢ It is still hard to evaluate the effectiveness of this mechanism with respect to privacy and utility. The existing works use the change of ML outputs (labels) to evaluate the privacy protection methods. We need to prompt more concise and better evaluation metrics. ‚Ä¢ There have been some recent research works that connect the DP framework and adversarial example [74] . The PixelDP algorithm [74] proposed to add a DP-noise to the input or any middle layer to the network's architecture to provide guaranteed robustness against adversarial examples., In more details, if we consider \"a DNN's input (e.g., images) as databases in DP parlance, and individual features (e.g., pixels) as rows in DP\", randomizing the output prediction function to enforce DP can guarantee the robustness of predictions against adversarial examples. PixelDP cannot effectively preserve privacy in the training set as the input changes are restricted to \"a small number of pixels\" [74] . Phan et al. [125] proposed a heterogeneous Gaussian Mechanism (HGM) that can preserve DP in training data and provide provable robustness against adversarial examples at the same time. They further proposed the stochastic batch mechanism in [123] that can retain higher model utility and is more scalable to large DNNs and datasets, compared with HGM. Overall, the interplay among DP, adversarial example and certified robustness would be a very interesting future topic. ML-aided Privacy Protection: GAN and VAE Excessive amounts of unstructured data including images, videos, audios and texts are being generated constantly and are being used by the government and a wide range of industries. According to the projections of the international data corporation, unstructured data will constitute approximately 80 percent of worldwide data by 2025. Unstructured data, especially image and videos, often containing rich personal information, play a key role in the future privacy preservation ecosystem. And the problem of private data release for unstructured data will be a hot topic in the future. We expect GAN to play an important role in this area, as it has demonstrated the capability to preserve high utility for ML algorithms while protecting sensitive information in the dataset. Moreover, GAN, as part of VAE, might also be used for privacy protection for a signal data entry (i.e., an image). In this case, we can encode an original data entry and then decode it with some additional privacy protection. CONCLUSION This study surveys the literature on privacy in the context of machine learning. By classifying the existing research into three groups: (i) private machine learning, (ii)machine learning aided privacy protection, and (iii) privacy protection against machine learning attack, we comprehensively review the state-of-art techniques on this topic and draw several conclusions as follows. ‚Ä¢ The private machine learning problem has drawn the most attention recently. In this category of research works, many try to use the differential privacy criterion during the analysis. However, DP notation cannot provide comprehensive privacy evaluation due to the complexity of the data and privacy protection target. Therefore, how to define new privacy metrics and notations is still an open question. ‚Ä¢ The research on machine learning aided privacy protection is gaining momentum these days. For example, using GNN to generate synthetic datasets opens the new direction for privacy protection research, especially for unstructured data such as image and video. ‚Ä¢ Research on protection schemes against ML-based privacy attack is in its infancy. But it is expected to fly in the future due to the proliferation of AI techniques in every corner of the future networks. Currently, mainstream technology in this category is the adversarial example/perturbation technique. We believe our timely study will shed valuable light on the research problems associated with privacy and machine learning. With the increasing attention paid to this topic, we would expect to see increasing research activities in this area. Fig. 1 . 1 Fig. 1. Centralized and distributed ML systems: (a) centralized learning; (b) distributed learning. Fig. 2 . 2 Fig. 2. Three different categories of research problems in privacy and ML: (a) Privacy of ML model and data; (b) ML enhanced privacy protection; (c) ML-based privacy attack. Fig. 3 . 3 Fig. 3. The proposed taxonomy of privacy and ML. ( 2 ) 2 Model privacy: There are also privacy concerns about the ML model including the model parameters, and training algorithms. For example, a financial institution may hold a sensitive (a) Privacy of the ML model. (b) Privacy of the underlying data. Fig. 4 . 4 Fig. 4. Two different types of privacy attack targets in ML: (a) Model privacy; (b) Training data privacy. Fig. 5 . 5 Fig. 5. Different attack models targeting ML. 3. 1 . 2 12 Feature Estimation Attack. A feature estimation attack aims to estimate certain features ùë• ‚òÖ ùëñ ‚àà √¨ ùë• ‚òÖ or statistical properties such as ùëéùë£ùëî( √¨ ùë• ‚òÖ ) of the training dataset Fig. 6 . 6 Fig. 6. Different privacy attack and threat models. Fig. 7 . 7 Fig. 7. ML-aided privacy protection schemes. Fig. 8 . 8 Fig. 8. Privacy preserving framework based on generative model approach. Fig. 9 . 9 Fig. 9. Different privacy attack and threat models when ML is used as the attack tool. Table 1 . 1 Summary of acronyms used in the paper. CNN convolutional neural network DNN deep neural network DP  * differential privacy ERM empirical risk minimization FGSM fast gradient sign method FHE fully homomorphic encryption GAN generative adversarial network GNN generative neural network IoT Internet of things ML machine learning SGD stochastic gradient descent SMC secure multi-party computation SVM support vector machine VAE variational autoencoder * DP in this survey is used as the abbreviation for Differential Privacy, not deep learning. Table 2 . 2 Three categories of privacy protection problems in the context of ML. Category Role of ML in Privacy Protection Private ML Protection target ML enhanced Privacy Protection Protection tool ML-based Privacy Attack Attack tool Table 3 . 3 Summary of Attack Models. Adversary features Model Extraction Feature Estimation Membership Inference Model Memorization Knowledge Black-box White-box ‚úì ‚úì ‚úì ‚úì ‚úì Model ‚úì Target Data features Exact data values ‚úì ‚úì Membership ‚úì Model inversion ‚úì Scheme Shadow training Encoding ‚úì ‚úì ‚úì ‚úì Table 4 . 4 Comparisons of Attack Methods. Attack and Threat ME FE MI MM Adversary's Knowledge Attack Method System Settings [155] ‚úì Black-box Shadow training ML-as-a-service [111] ‚úì Black-box Metamodel Centralised [161] ‚úì Black-box Hyperparameter-stealing Centralised [56] ‚úì Black-box Reverse-engineering Centralised [39] ‚úì White-box Model inversion Centralised [38] ‚úì Black-box Model inversion Centralised [8] ‚úì White-box Shadow training Centralised [55] ‚úì White-box GAN Distributed [164] ‚úì Black-box Power side-channel attack Centralised [141] ‚úì Black-box Shadow training Centralised [84] ‚úì White-box Shadow training Centralised [134] ‚úì Black-box Unsupervised binary classification ML-as-a-service [52] ‚úì White/Black-box GAN Centralised [98] ‚úì White-box Gradient-based Distributed [144] ‚úì White/Black-box Encoding Centralised ME: Model Extraction; FE: Feature Estimation; MI: Membership Inference; MM: Model Memorization. Table 5 . 5 Comparisons of Private ML Schemes. Private ML Schemes ME FE MI MM Categories Methods System Settings [13, 15, 29, 47, 54, 77] ‚úì ‚úì Encryption Homomorphic encryption (training data) Centralised [126] ‚úì ‚úì Encryption Homomorphic encryption (model) Distributed [7, 16, 30, 66, 101, 157, 170] ‚úì ‚úì Encryption SMC Distributed [132] ‚úì ‚úì Obfuscation DP SVM Centralised [18] ‚úì ‚úì Obfuscation DP ERM Centralised [146] ‚úì ‚úì Obfuscation DP-SGD for convex objectives Centralised [140] ‚úì ‚úì Obfuscation DPSGD Distributed [2] ‚úì ‚úì Obfuscation DPSGD Centralised [100] ‚úì ‚úì Obfuscation multi-round DP Centralised [95] ‚úì ‚úì Obfuscation DP-FedAvg Distributed [179] ‚úì Obfuscation Training data obfuscation Centralised [121] ‚úì ‚úì ‚úì Aggregation/Obfuscation DP+Aggregation Distributed [69, 94] ‚úì Aggregation Federated learning Distributed [12] ‚úì ‚úì Aggregation/Encryption Federated learning + SMC Distributed [118] ‚úì ‚úì Aggregation PATE Centralised [34] ‚úì Aggregation/Obfuscation Output aggregation + DP Centralised ME: Model Extraction; FE: Feature Estimation; MI: Membership Inference; MM: Model Memorization. ACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: November 2020. ACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: November 2020.When Machine Learning Meets Privacy: A Survey and Outlook"
}