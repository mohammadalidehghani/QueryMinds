{
  "title": "The Tribes of Machine Learning and the Realm of Computer Architecture",
  "abstract": "Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture.",
  "introduction": "INTRODUCTION Machine learning (ML) refers to the process in which computers learn to make decisions based on the given data set without being explicitly programmed to do so [8] . There are various classifications of the ML algorithms. One of the more insightful classifications has been done by Pedro Domingos in his book The Master Algorithm [39] . Domingos presents five fundamental tribes of ML: the symbolists, the connectionists, the evolutionaries, the bayesians and the analogizers. Each of these believe in a different strategy to go through the learning process. These tribes or schools of thought of ML along-with their primary algorithms and origins are shown in Table 1 . There are existing proofs that given the enough amount of data, each of these algorithms can fundamentally learn anything. Most of the well known ML techniques/algorithms foot_0 belong to one of these tribes of ML. In this paper, we look at these five school of thoughts of ML and identify how each of them can be fundamentally used to solve different research problems related to computer architecture. ML techniques have already influenced many domains of computer architecture. Figure 1 shows the number of research works using ML each year since 1995. It is observed that most of the work employing ML techniques (approximately 65% of the studied work) is done in last 5-6 years indicating increasing popularity of ML models in computer architecture research. Findings also indicate that Neural Networks are the most used ML technique in computer architecture research as shown in Figure 2 . Decision Trees Genetic Algo. KNN K-Means Linear Regress. Logistic Regress. Naive Bayes Neural Nets. Random Forest Reinforc. Learn. SVM Others 0 10 20 30 40 Number of Works Fig. 2. Number of works for each ML Algorithm We also present an extensive survey of the research works employing these techniques in computer architecture. Most of the insights discussed in this paper are based on a review of more than a hundred papers 2 which use ML for computer architecture. The specific questions we answer in this paper include: • What are the fundamental features of different ML algorithms which make them suitable for particular architecture problems compared to the others? 2 Detailed summaries of most of these papers are available in: https://github.com/ayaz91/Literature-Review/blob/main/ML_   In_CompArch/ML_CompArch.md The Tribes of Machine Learning and the Realm of Computer Architecture 0:3 • How has ML impacted computer architecture research so far? • What are the most important challenges that need to be addressed to fully utilize ML potential in computer architecture research? Microarchitecture [45, 105, 130, 144] , Performance Estimation [43, 79] , Scheduling [56] , Energy [122] , instruction scheduling [112, 124] Connectionists Credit Assignment",
  "body": "INTRODUCTION Machine learning (ML) refers to the process in which computers learn to make decisions based on the given data set without being explicitly programmed to do so [8] . There are various classifications of the ML algorithms. One of the more insightful classifications has been done by Pedro Domingos in his book The Master Algorithm [39] . Domingos presents five fundamental tribes of ML: the symbolists, the connectionists, the evolutionaries, the bayesians and the analogizers. Each of these believe in a different strategy to go through the learning process. These tribes or schools of thought of ML along-with their primary algorithms and origins are shown in Table 1 . There are existing proofs that given the enough amount of data, each of these algorithms can fundamentally learn anything. Most of the well known ML techniques/algorithms foot_0 belong to one of these tribes of ML. In this paper, we look at these five school of thoughts of ML and identify how each of them can be fundamentally used to solve different research problems related to computer architecture. ML techniques have already influenced many domains of computer architecture. Figure 1 shows the number of research works using ML each year since 1995. It is observed that most of the work employing ML techniques (approximately 65% of the studied work) is done in last 5-6 years indicating increasing popularity of ML models in computer architecture research. Findings also indicate that Neural Networks are the most used ML technique in computer architecture research as shown in Figure 2 . Decision Trees Genetic Algo. KNN K-Means Linear Regress. Logistic Regress. Naive Bayes Neural Nets. Random Forest Reinforc. Learn. SVM Others 0 10 20 30 40 Number of Works Fig. 2. Number of works for each ML Algorithm We also present an extensive survey of the research works employing these techniques in computer architecture. Most of the insights discussed in this paper are based on a review of more than a hundred papers 2 which use ML for computer architecture. The specific questions we answer in this paper include: • What are the fundamental features of different ML algorithms which make them suitable for particular architecture problems compared to the others? 2 Detailed summaries of most of these papers are available in: https://github.com/ayaz91/Literature-Review/blob/main/ML_   In_CompArch/ML_CompArch.md The Tribes of Machine Learning and the Realm of Computer Architecture 0:3 • How has ML impacted computer architecture research so far? • What are the most important challenges that need to be addressed to fully utilize ML potential in computer architecture research? Microarchitecture [45, 105, 130, 144] , Performance Estimation [43, 79] , Scheduling [56] , Energy [122] , instruction scheduling [112, 124] Connectionists Credit Assignment SURVEY OF USE OF MACHINE LEARNING IN COMPUTER ARCHITECTURE Deep-Neural Networks, Perceptrons Microarchitecture [27, 37, 69, 76, 84, 113, 158, 173, 185, 193] , Performance Estimation [43, 189] , scheduling [17, 56, 102, 131, 186, 187] , DSE [60, 85] , Energy [188] , Security [31, 86, 88, 135, 137] Evolutionaries Structure Discovery Genetic Algorithm Performance estimation [59] , Design Space Exploration [44, 145, 167] , instruction scheduling [34, 95] Bayesians Uncertainty Reduction Naive-Bayes, LDA Microarchitecture [13] , DSE [145] Analogizers Similarity Discovery SVM, KNN Microarchitecture [36, 105] , Performance [12, 43, 59] , Scheduling [56, 186, 187] , Security [137] In this section, we will discuss how each of the previously mentioned paradigms of ML can be (or have been) applied to the field of computer architecture. Table 2 enlists the five tribes of ML along-with their targeted problem and some examples of the relevant research works. Of course, many architectural problems can be solved by more than one of these families of ML algorithms. The Symbolists: This tribe of ML relies on symbol manipulation to produce intelligent algorithms. The fundamental problem that Symbolists are trying to solve is knowledge composition. Their insight is to use initial knowledge to learn quicker than learning from scratch. More specific examples of this group of ML algorithms include inverse deduction and decision trees. Algorithms belonging to Symbolists seem to be ideal to be used for cases where a cause and effect relationship needs to be established between events. For example, the architecture problems where we need this type of learning include: finding reasons for hardware security flaws and consistency bugs. The more interpretable nature of these algorithms make them a good candidate to understand the impact of certain design features (input events) on the performance metric of the entire system. As a result, certain parameters can be fine tuned to produce desirable results. For example, this kind of learning can be applied to understand which pipeline structures consume the most amount of energy in a given configuration. However, it might be hard to map these algorithms directly to the hardware (if desired to be used at run-time) due to their symbolic nature. There exist some examples of usage of these algorithms in computer architecture research. Fern et al. [45] introduced decision tree based branch predictors. Decision trees allowed the proposed branch predictor to be controlled by different processor state features. The relevant features could change at run-time without increasing linearly in size with the addition of new features (compared to table based predictors), providing significant benefits over conventional table based predictors. Decision tree based models have been used in many cases to understand the impact of different architectural events on systems' performance. Yount et al. [43] compared various machine learning (ML) algorithms with respect to their ability to analyze architectural performance of different workloads and found tree-based ML models to be the most interpretable and almost as accurate as Artificial Neural Networks (ANNs). Jundt et al. [79] used a ML model named Cubist [1] , which uses a tree of linear regression models to observe the importance of architectural blocks that have an effect on performance and power of high performance computing applications on Intel's Sandy Bridge and ARMv8 XGene processors. Mariani et al. [115] used Random Forest to estimate HPC (High Performance Computing) applications' performance on cloud systems using hardware independent profiling of applications. Rahman et al. [144] used decision trees and logistic regression to build framework to identify the best prefetcher configuration for given multithreaded code (in contrast to focus on serial code as in [105] ). Hardware prefetcher configuration guided by the presented machine learning framework achieved close to 96% speed-up of optimum configuration speed-up. Moeng and Melhem [122] used decision trees (implemented in hardware) to propose a DVFS (dynamic voltage and frequency scaling) policy that will be able to predict clock frequency resulting into the least amount of energy consumption in a multicore processor. They used simple measurement metrics like cycles, user instructions, total instructions, L1 accesses and misses, L2 accesses, misses and stalls for a DVFS interval during execution as inputs to the decision trees. The Connectionists: The connectionists rely on brain structure to develop their learning algorithms and try to learn connections between different building blocks (neurons) of the brain. The main problem they are trying to solve is of credit assignment i.e. figure out which connections are responsible for errors and what should be the actual strength of those connections. The most common example of this class of ML algorithms is (deep) neural networks. The algorithms belonging to the Connectionists are good at learning complex patterns specially at run time (as evident by many microarchitecture design examples). This type of learning can be useful in many microarchitectural predictive structures (like branch predictors, cache prefetchers and value predictors) which try to forecast an event based on similar events of the past, where different past events might have different weights in determining if a certain event will happen in the future (the problem of credit assignment). In contrast to symbolists, the algorithms belonging to this group might not need any initial knowledge, however they are not very interpretable. So, they might not be very useful to understand the importance of different architectural events/components in overall performance/output for static analysis. This kind of algorithms have found uses in the architecture research, specially simple neural networks like perceptrons [19] (owing to their simple structure and being more amenable to be implemented in the hardware). Calder et al. [27] introduced the use of neural networks for static branch prediction in the late 1990's. One of the earliest works to use ML for dynamic branch prediction was done by Vinton and Iridon [179] . They used neural networks with Learning Vector Quantization (LVQ) [94] as a learning algorithm for neural networks and were able to achieve around 3% improvement in misprediction rate compared to conventional table based branch predictors. Later, Egan et al. [42] and Wang and Chen [185] also used LVQ for branch prediction. The complex hardware implementation of LVQ due to computations involving floating point numbers, could significantly increase the latency of the predictor [76] . Jimenez et al. [76] , working independently, also used neural network based components, perceptrons [19] , to perform dynamic branch prediction. In their work, each single branch is allocated a perceptron. The inputs to the perceptron are the weighted bits of the \"global branch history shift register\", and the output is the decision about branch direction. One big advantage of perceptron predictor is the fact that the size of perceptron grows linearly with the branch history size (input to the perceptron) in contrast the size of pattern history table (PHT) in PHT based branch predictors which grows exponentially with the size of the branch history. Therefore, within same hardware budget, perceptron based predictor is able to get a benefit from longer branch history register. One of the big problems with the use of perceptrons is their inability to learn linear separability 3 . This was later resolved by Jimenez [71] using a set of piecewise linear functions to predict the outcomes for a single branch. These linear functions refer to a distinct historical path that lead to the particular branch instruction. Graphically, all these functions, when combined together, form a surface. Since the introduction of neural network based branch predictors, there has been a lot of research done to optimize the design of these predictors. Different analog versions have been proposed to improve speed and power consumption of the neural network based branch predictors [9, 73, 92, 151, 166] . Perceptron based predictors have also been combined with other perceptron or non-neural network based predictors to achieve better accuracy overall: [42, 67, 72, 98, 123, 149, 165, 170] . Different optimizations/modifications to neural network branch predictors including use of different types of neural networks have been explored: [26, 50, 52, 54, 58, 69, 70, 90, 132, 156, 157, 160, 174] . These optimizations improve performance of the predictor and save power and hardware cost. Similar concepts are also used in development of other branch predictor related structures like confidence estimators and indirect branch predictors: [6, 38, 68, 75, 82] . The statistical corrector predictor used in TAGE-SC_L [158] branch predictor 4 is also a perceptron based predictor. This statistical corrector predictor regresses TAGE's prediction if it statistically mispredicts in similar situations. Mao et al. [113] , recently, applied deep learning (easy to train Deep Belief Networks (DBN) [57] to the problem of branch prediction. The popularity of perceptrons in branch prediction has also affected the design of other microarchitecture structures. For example, Wang and Luo [182] proposed perceptron based data cache prefetching. The proposed prefetcher is a two level prefetcher which uses conventional table-based prefetcher at the first level. At the second level a perceptron is used to reduce unnecessary prefetches by relying on memory access patterns. Peled et al. [139] used neural networks to capture semantic locality of programs. Memory access streams alongwith a machine state is used to train neural network at run-time which predicts the future memory accesses. Evaluation of the proposed neural prefetcher using SPEC2006 [163] , Graph500 [127] benchmarks and other hand-written kernels indicated an average speed-up of 22% on SPEC2006 benchmarks and 5x on other kernels. However, importantly, Peled et al. [139] also performed a feasibility analysis of the proposed prefetcher which shows that the benefits of neural prefetcher are outweighed by other factors like learning overhead, power and area efficiency. Teran et al. [173] applied perceptron learning algorithm (not actual perceptrons) to predict reuse of cache bkocks using features like addresses of recent memory instructions and portions of address of current block. Verma [177] extended the work of Teran et al. [173] and proposed CARP (coherence-aware reuse prediction). CARP uses cache coherence information as an additional feature in the alogrithm of Teran et al. [173] . Seng and Hamerly 3 A boolean function is \"linearly separable\" if all false instances of the function can be separated from its all true instances using a hyperplane [121] . As an example XOR is linearly inseparable and AND is linearly separable. 4 won the 2014 branch predictor championship and combines TAGE branch predictor with a loop predictor and a statistical corrector predictor [155] did one of the earliest works to present perceptron based register value predictor, where each perceptron is fed with the global history of recently committed instructions. Later, Black and Franklin [18] proposed perceptron based confidence estimator for a value predictor. Nemirovsky et al. [131] proposed a neural network based scheduler design for heterogeneous processors. These algorithms have also been used to build performance/power models. For example, Khan et al. [85] proposed feed-forward neural network based predictive modeling technique to do design space exploration for chip multiprocessors. Wu et al. [189] proposed a neural network based GPU performance and power prediction model to solve the problem of slow simulation speed of simulators to study performance/power of GPUs. The proposed model estimates performance and power consumption of applications with changes in GPU configurations. Dai et al. [37] exploited deep learning techniques to propose Block2Vec (which is inspired by Word2Vec [118] used in word embeding), which can find out correlations among blocks in storage systems. Such information can be used to predict the next block accesses and used to make prefetching decisions. Khakhaeng and Chantrapornchai [84] used perceptron based neural network to build model to predict ideal cache block size. Neural network is trained using features from address traces of benchmarks. The particular features used for training include: cache misses and size and frequency adjoining addresses which reflects temporal and spatial locality of the program. Neural networks have also been used (both offline and online) to discover the optimal schedule of workloads or manage other aspects of shared hardware resources. For example, Bitirgen et al.. [17] implemented Artificial Neural Network (ANN) in the hardware for the management of shared resources in multiprocessor systems. Each ANN (edge weights are multiplexed at the run time to achieve virtual ANNs using one hardware ANN) acts as a performance model and takes available resources and other factors describing the program behavior (e.g. read/write hits and misses in data cache and the portion of dirty cache ways allocated to the applications) as inputs and outputs the information which helps to decide which resource distiribution would result in the best overall performance. Li et al. [102] used ANNs to predict the performance of parallel tasks at run-time to do an optimal scheduling. Nemirovsky et al. [131] proposed an ANN based approach to perform scheduling on heterogeneous processors, which increases throughput by 25-31% compared to a RoundRobin scheduler on an ARM big.Little system [55] . The proposed methodology relies on ANNs to predict thread's performance for a particular scheduling interval on different hardware core types and the scheduler picks the schedule of threads that would maximize the system performance. Other examples of the use of neural networks include the work of Ipek et al. [60] for design space exloration and the work of Chiappetta et al. [31] to detect cache based side channel attacks (e.g. Flush+Reload [192] ) relying on the hardware performance counter values. The Evolutionaries: This class of algorithms are based on the evolutionary process of nature and rely on learning structures instead of learning parameters. These algorithms keep on evolving and adapting to unspecified surroundings. The most obvious example are genetic algorithms, which potentially can be used for searching the best design in a large design space. Another problem these algorithms are a natural fit to solve is of an attacker which evolves to different versions once a preventive technique is deployed (e.g. different ML based side channel attack detection and mitigation solutions). These algorithms can help in building preventive techniques which can evolve with the evolution of the attacker. General purpose computer systems might be used for a diverse set of applications after being deployed. It seems reasonable to make these systems able to evolve their configuration at runtime depending on their use. Genetic algorithms seem to be the most suitable algorithms to learn how to adapt to the best possible system configuration depending on the workloads being executed. There exist only a limited number of examples of the utilization of these algorithms by computer architects. For example, Joel et al. [44] used genetic algorithms to design better branch predictors and Jimenez et al. [74] took help of genetic algorithms to introduce a pseudo-LRU (least recently used) insertion and promotion mechanism for cache blocks in last level caches, which could result in 5% speedup compared to traditional LRU (least recently used) algorithm using much less overhead. The performance of the proposed mechanism matched other contemporary techniques like DRRIP (dynamic rereference interval prediction [63] ) and PDP (protecting distance policy [41] ) with less overhead. Mukundan and Martinez [125] used genetic algorithms to propose MORSE (Multi-objective Reconfigurable Self-optimizing Memory Scheduler) extending Ipek et al's work [61] which used reinforcement learning for memory scheduling. MORSE can target optimization of different metrics like performance, energy and throughput. The Bayesians: The algorithms belonging to the Bayesians are usually concerned with incorporating new evidence into previous beliefs. The main problem they try to solve is of reducing uncertainty. They are specially good at predicting events when there is a known probability of occurrence of particular events. Naive Bayes and Linear Discriminant Analysis (LDA) are a couple of examples of this group of algorithms. These algorithms have a high potential to be used by the architects due to their inherent characterstics, as a lot of architecture problem solutions rely on the knowledge of existence of certain events. For instance, a bayesian based model to detect hardware side channel attacks can rely on the fact that the probability of system under no attack would be very high normally. Similarly, a model to predict memory events can depend on the average probability of an instruction being a memory operation. Beckman and Sanchez [13] recognized that the problem of design of cache replacement policies have to deal with uncertainty as the time when the candidate cache blocks will be accessed is not known. Thus, bayesians have the potential to learn in such environments and develop smart cache replacement policies. Overall, Bayesians also have not found a lot of use in the computer architecture community as our survey suggests. Jung and Pedram [80] used Bayesian classification to tune voltage and frequency settings to reduce system's energy consumption in a multicore processor. Reagen et al. [145] used Bayesian optimization [141] to explore design space for a Deep Neural Network hardware accelerator. Different design parameters studied in this work, like neurons/layer for DNN, L2 regularization for DNN, learning rate of DNN, loop parallelism in hardware, and hardware pipelining, have \"complex interactions\" among them which the bayesian optimization tries to learn. The Analogizers: Analogizers believe in learning things from other examples. The primary problem they target is finding similarities between different situations. Few examples of algorithms belonging to this domain include: support vector machines (SVM), k-means clustering and k-nearest neighbor (KNN). Some of these algorithms (like k-means clustering and KNN) are used in un-supervised learning. Algorithms from this group can be good candidates to be applied in cutting down the search space for design space exploration especially when configuration space is large and there exists multiple designs with marginal differences. These algorithms can help to focus on designs which are sufficiently different from a baseline. Similarly, they can help in detecting similarities at the granularity of programs or at the granularity of instructions to guide better predictions at the run-time by different microarchitectural structures. Analogizers have been pretty successful in the domain of computer architecture. For example, Culpepper and Gondree [36] used support vector machines (SVM) to improve accuracy of branch prediction. This SVM based branch predictor performed better compared to fast-path based predictor and gshare predictor at high hardware budgets [36] . Sherwood et al. [159] used K-means clustering to form groups/clusters of similar basic block vectors (BBV 5 ). These groups then act as a few representative portions of the entire program and are known as Simpoints, which can be used to approximate the performance/power for the entire program. Hoste et al. [59] detected similarity among programs using data from various microarchitecture-independent statistics (like instruction classes, instruction-level parallelism and register traffic, branch predictability and working set sizes) to predict performance of programs similar to reference benchmarks with known performance on specific microarchitecture. They used principal component analysis for this purpose alongwith other ML techniques. Walker et al. [181] relied on heirarchical cluster analysis [25] to form groups of various performance monitoring counters which are then used to estimate power consumption for mobile and embedded devices and showed that their technique leads to lower percentage error compared to many other power estimation techniques [142, 148, 150, 180] . Baldini et al. [12] trained two binary classifiers Nearest Neighbor with Generalized exemplars (NNGE) and Support Vector Machine (SVMs) to predict possible GPU speed-up given a run of parallel code on CPU (using OpenMP implementation). Wang and Ipek [184] proposed an online data clustering based technique to reduce energy of data transfers in memory by reducing the number of ones in the data. Reinforcement Learning: The Sixth Tribe 6 Reinforcement learning refers to the process in which an agent learns to take actions in a given environment through independent exploration of different possible actions and choosing the ones that increase the overall reward. Reinforcement learning does not require any prior data to learn to take the right action, rather it learns to do this on the fly. This makes it a good fit to be used for computer architecture problems where significant former knowledge or data is not available and the right action can only be learned dynamically. For example, hardware optimizations or hardware based resource management techniques which are totally transparent to the software can rely on reinforcement learning to decide the best action to take for the overall efficiency of the system. One of the earliest examples of the use of reinforcement learning in computer architecture is the work of Ipek et al [61] . Ipek et al [61] introduced reinforcement learning based DRAM scheduling for an increased utilization of memory bandwidth. The DRAM scheduler which acts as a reinforcement learning agent utilizes system state defined by different factors like number of read/write requests residing in the transaction queue. The actions that the agent can take include all normal commands of DRAM controller like read, write, pre-charge and activate. Ipek et al [61] implemented a five stage hardware pipeline to calculate q-values [169] associated with the credit assignment which determine the eventual benefit for an action given the current state. Peled et al [138] used reinforcement learning to approximate program semantic locality, which was later used to anticipate data access patterns to improve prefetching decisions. Accesses are considered to have semantic locality if there exists a relationship between them via a series of actions. A subset of different attributes (e.g. program counter, accesses history, branch history, status of registers, types and offsets of objects in program data structures and types of reference operations) is used to represent the current context a program. Juan and Marculescu [78] proposed a reinforcement learning based DVFS technique which divides the central agent in many \"distributed\" agents and uses a supervisor for coordination 5 BBV contains the frequency of occurrence of all basic blocks during an interval of execution 6 We refer to reinforcement learning as the sixth tribe of ML as it is one of the most important algorithms belonging to the class of self learning algorithms. Reinforcement learning does not belong to the original ML tribes taxonomy of Pedro Domingos [39] . among other agents to maximize performance under given power budget. Similarly, Ye and Xu [194] proposed a reinforcement learning based dynamic power management (DPM) technique for multi-core processors. Regression Techniques -Statistics Meets Machine Learning 7 Regression techniques from applied statistics have been largely borrowed by machine learning. A detailed discussion on different regression techniques used in machine learning is provided in the Appendix section. In this section, we take a look at how these regression techniques have influenced the field of computer architecture. Performance and power estimation of different workloads on a particular architecture is a critical step to design new architectures or to have a better understanding of the already existing architectures. There are numerous examples of research works where regression techniques are used to estimate performance or power consumption. Performance/power estimation of applications on a particular architecture is mostly done by simulation tools and analytical models. Regression techniques have also been used to build new and accurate models using empirical data. Sometimes they are also used to increase the accuracy of simulation techniques. For example, Lee et al. [99] proposed regression analysis based calibration methodology called PowerTrain to improve the accuracy of McPAT [103] . McPAT [103] is a well-known power estimation simulator, but it is shown to have various inaccuracies due to different sources like un-modeled architectural components, discrepancy between the modeled and actual hardware components and vague configurational parameters [190] . The proposed calibration methodology called PowerTrain uses power measurements on real hardware to train McPAT using regression analysis. Reddy et al. [146] studied correlation among gem5 [16, 110] statistics and hardware performance monitoring counters to build a model in gem5 to estimate power consumption of ARM Cortex-A15 processor. This work uses the same model built by Walker et al. [181] but does not include all performance monitoring events as some gem5 equivalents would not be available. The used events in ML model are cycle counts, speculative instructions, L2 cache accesses, L1 instruction cache accesses and memory bus reads. Reddy et al. [146] show that the differences between statistics of the simulator (gem5) and those of the real hardware only affect the estimated power consumption by approximately 10%. There are also examples [97, 142] of use of regression techniques to estimate performance of multi-core processors from single core processors. Lee et al. [97] used spline-based regression to build model for multiprocessor performance estimation from uniprocessor models to mitigate the problems associated with cycle accurate simulation of multiprocessors. Pricopi et al. [142] used regression analysis to propose an analytical model to estimate performance and power for heterogeneous multi-core processors (HMPs). During an application run, a cpi (cycles per instruction) stack is built for the application using different micro-architectural events that can impact the execution time of the application. Relying on some compiler analysis results alongwith the cpi stack, performance and power can be estimated for other cores in an HMP system. Experiments performed with an ARM big.Little system indicate an intra-core prediction error of below 15% and an inter-core prediction error of below 17%. Regression techniques are also used in heterogeneous systems for cross platform performance estimation. Examples include [199] , [20] , [133] , and [10] . Zheng et al. [199] used Lasso Linear Regression and Constrained Locally Sparse Linear Regression (CLSLR) to explore correlation among performance of same programs on different platforms to perform cross-platform performance prediction. The test case used in their work is prediction of an ARM ISA platform based on performance on Intel and AMD x86 real host systems. The authors extended this work in [198] by proposing LACross framework which applies similar methodology to predict performance and power at fine granularity of phases. LACross is shown to have an average error less than 2% in entire program's performance estimation, in contrast to more than 5% error in [198] for SD-VBS benchmark suite [176] . Boran et al. [20] followed Pricopi et al.'s [142] work and used regression techniques to estimate execution cycle count of a particular ISA core based on the performance statistics of another ISA core. This model is used to dynamically schedule programs in a heterogeneous-ISA multi-core system. ARMv8 and x86-64 based multi-core system is used to validate the model. Although this model shows above 98% accuracy to estimate performance on any particular ISA core, the inter-core performance estimation has high error (23% for estimation from ARM to x86 and 54% for estimation from x86 to ARM). O'Neal et al. [133] used various linear/non-linear regression algorithms to propose GPU performance predicting model focusing on \"pre-silicon design\" of GPUs using DirectX 3D workloads. Ardalani et al. [10] proposed XAPP (Cross Architecture Performance Prediction) technique to estimate GPU performance from CPU code using regression and bootstrap aggregating (discussed in the Appendix section). Different features associated with the program behavior are used to train machine learning models. These features include some basic features related to ILP (instruction level parallelism), floating point operations and memory operations and also some advanced features like shared memory bank utilization, memory coalescing and branch divergence. Regression techniques have also been used in design space exploration (e.g. [66, 96] ). Lee et al. [96] used regression modeling for fast design space exploration to avoid expensive cycle accurate simulations. Jia et al. [66] proposed StarGazer, an automated framework which uses a stepwise regression algorithm to explore GPU design space by using only few samples out of all possible design points and estimates the performance of design points with only 1.1% error on average. Some regression techniques have also been used in the domain of hardware security, specifically to detect the malware or micro-architectural side channel attacks. For instance, Ozosoy et al. [135] used neural network and logistic regression for detection of malware using hardware architectural features (like memory accesses and instruction mixes). They evaluated the FPGA based implementations of both ML models (logistic regression and neural networks). Khasawneh et al. [86] used neural networks and logistic regression (LR) based hardware malware detectors to prove the possibility of evading malware detection. Availability of malware detector training data makes it possible for attackers to reverse engineer detectors and potentially modify malware to evade their detection. Khasawneh et al. [86] also proposed randomization based technique to attain resilient malware detection and avoid reverse engineering. Many other works which utilize regression or other ML techniques for micro-architectural side channel attack detection are referred in [7] . SUMMARY TABLE Table 3 provides a summary of the previously discussed literature survey at a finer granularity of individual ML techniques. Each column in this table refer to a broader category (or field) of computer architecture that has relied on ML. OPPORTUNITIES AND CHALLENGES: This section presents some opportunities to use ML for architecture that have not been explored in their full potential if explored at all. Moreover, we also enlist a number of challenges that need to be tackled to be able to fully exploit ML for architecture research. Opportunities: • There exists an opportunity to borrow ideas from ML instead of using ML algorithms in their intact form. Since, implementing ML algorithms in the hardware can be costly, computer architects can rely on ML algorithms to optimize the classical learning techniques used in computer systems. For instance, the work done by Teran et al. [173] to apply perceptron learning algorithm to predict reuse of cache blocks is a good example of this. • ML based system components can be used in synergy with traditional components. A good example of this collaboration is the work of Wang and Luo [182] to perform data cache prefetching discussed in the previous section. • ML techniques can potentially be used to build customizable computer systems which will learn to change their behavior at different granularities. Take an example of an 'A' cache eviction policy which works better for workload type 'X' and another policy 'B' which works better for workload type 'Y'. Since, ML makes learning easier, same processor can learn to adapt to using policy 'A' or 'B' depending on the workloads run on it. • Computer architects have relied on heuristics to design different policies. One possiblity is to start using ML in place of these heuristics. Machine Learning can also help us to come up with new and better heuristics that are hard to ascertain otherwise. • For all the decisions taken inside a computer system, multiple policies are used. If single (or a small number of) ML algorithm(s) can learn all of these policies, the design of systems can be significantly simplified. • As we observed earlier, performance and power estimation of existing or new architectures is an area which can be highly influenced by ML. Architects have historically used simulation tools or mathematical models for this purpose. Future architectural innovations are expected to come from optimizations across the entire software/hardware computing stack [100] . Current simulation techniques are not designed with this kind of use case in mind. Therefore, ML techniques can come to rescue and enable building simulation tools of the future as traditional cycle level modeling might be too slow for cross-stack studies. Not only that ML techniques can enable building new tools, they can help in improving the accuracy of current simulation tools (or performance/power models) which are the primary enabler of computer architecture research at the moment (for example the work by Renda et al. [147] ). • ML can specially prove useful to build systems/accelerators for ML. • Design space explorations (which are extensively done in computer architecture research) should use more of ML techniques (especially the ones belonging to evolutionaries and analogizers). Techniques like genetic algorithms can help to evolve the design space itself as well, thus enabling the evaluation of significant configurations which might remain hidden otherwise. Since, all of this exploration is done statically (is part of pre-silicon design), there will be no issues of the cost of the algorithm here. • Computer architecture research often relies on combination of multiple (independently functioning) structures to optimize the system. For example, tournament branch predictors use multiple branch predictors and pick one of them for a given prediction. Similar tasks can be relegated to ML to learn the best of different independent techniques. • Computer architects often run a number of benchmarks/workloads on a multitude of hardware configurations (and generate multiple statistics) when studying new or existing ideas. This leads to huge amounts of data which researchers often study in a limited fashion on ad hoc basis. In contrast, if ML based analysis is performed on this data, we might be able to see the true potential of that data and its ability to help us discover new findings. Challenges: • The resources needed by these ML techniques when they are implemented in hardware can be significant. Specially, if these techniques need to be used for microarchitectural decisions at a finer granularity (at the program phase level), the hardware implementations become more critical. There is a need to develop more optimized hardware implementations of these algorithms to make them more feasible to be applied towards architectural problems/design. • ML algorithms primarily rely on huge data sets for learning. In comparison to other fields, there does not exist standardized data sets in architecture domain to be used by ML algorithms. • Although there exist a number of research works which take help of ML algorithms for computer architecture problems, the use of a particular algorithm is not justified by the authors mostly. As we observed in this paper, there are ML algorithms which naturally fit certain problems. We think it will be sensible to emphasize the use of appropriate algorithms for a given problem, which can help in ensuring the robustness of the proposed ideas. • ML methods have found more use in branch prediction like problems where the decision to be made is a binary decision (taken/not-taken) as opposed to the problems like memory prefetching where the output has to be a complete memory address (which increases the complexity of the ML model). • Data selection, cleaning, and pre-processing and then interpreting results is a challenge. • Formulation of architecture problems as machine learning problem is a general challenge. • As we pointed out in the opportunities sub-section that ML can help improving the accuracy of simulation tools, that opportunity comes with a challenge of building new tools to embed ML algorithms into simulation techniques. Specifically, the question of how to make the simulation tools talk to ML algorithms needs to be addressed. • There exist a big disparity between the time that ML algorithms take to make a prediction and the time that often microarchitectural structures take for an estimation (known as time scale gap). This needs to be solved before practical ML based microarchitectural solutions can be designed. An example where this problem has been addressed is the design of branch predictors i.e. the proposal of analog versions of neural network based branch predictors (for example: [9, 73] ) to improve latency of prediction. • It is also not clear how (and if) the ML based architecture design will scale. Traditionally, in computer architecture, many microarchitectural/architectural techniques work the same way irrespective of the workloads used or size of the processor. For heuristics based designs, it is easier for humans to argue about their scalability. However, this might not be true for ML based design (specially if it is not very interpretable). Thus, the question arises if the optimal machine learning based design for a specific problem at hand will work if the workloads change significantly or the hardware resources scale to a bigger budget. • As ML is finding more and more uses in the real world, many security challenges have been raised. Before computer architecture design can be fully influenced by ML, the attacks (like adversarial ML attacks) possible on ML techniques need to be addressed in the context of hardware design. Moreover, these machine learning implementations might lead to new sidechannels and computer architects will need to be cognizant of this to avoid major security vulnerabilities in the future. A AN OVERVIEW OF ML TECHNIQUES Machine Learning (ML) refers to the process in which computers learn to make decisions based on the given data set without being explicitly programmed to do so [8] . There are numerous ML algorithms that can be grouped into three categories: supervised learning algorithms, unsupervised learning algorithms and other types of algorithms. Learning algorithms are usually trained on a set of samples called a \"training set\". A different set of data is usually used to test the accuracy of decisions made by the learning algorithm known as \"test set\". This section briefly discusses different ML techniques that are used by the research surveyed in this paper. Readers who wish to have detailed explanation of the discussed methodologies in this section can refer to [64] . A common textbook classification of ML algorithms is shown in Figure 3 . A.1 Supervised Learning The process of training machine learning models given a set of samples of input vectors with the corresponding output/target vectors is called Supervised learning [32] . The trained model can then be used to predict ouput vectors given a new set of input vectors. The output/target vectors are also known as labels and input data as labelled data. Machine learning models can operate on different types of data. If the target labels in a data set can be a set of discrete classes/categories, the modeling task is referred as classification task. On the other hand if the target labels in a data set are continous variables, the modeling task is called a K=8 ? X X X X X X Y Y Y Y Y Y Y X X X Fig. 4. KNN X X X X X X Y Y Y Y Y Y Y Y X X X X Y Y Y Fig. 5. SVM make new predictions for test data. KNN has also been used for regression problems [32] . More information on KNN can be found in [35] . A.1.8 Support Vector Machines. Support Vector Machine (SVM) is a non-probabilistic ML model that has been used for both classification and regression problems, but mostly for classification. Assuming that we have n number of features or input variables, SVM classifier plots given data points in an n-dimensional space. The algorithm finds hyper-planes in this n-dimensional space which can distinguish given classes with the largest margin. This hyperplane can be a linear/nonlinear function of input variables resulting into linear/non-linear SVM. SVM based classifiers use \"a few training points\" (also known as support vectors) while classifying new data points. Theoretically, support vectors are the most difficult points to classify since they are the nearest to the \"decision surface\". Maximum margin between these support vectors ensures that the distance between classes is maximized. Figure 5 shows a linear SVM classifier, distinguishing between two classes of data. SVMs are effective for high number of features and are memory-efficient. However, these models are not very interpretable [89] . More information on SVMs can be found in [21, 64] A.1.9 Decision Trees. Decision trees summarize the relationship between different values of given features (represented by branches) and the conlusions/results about the output variables's value (represnted by leaves) [2] . Decision trees can be used for both classification and regression problems. Figure 6 shows an example of a decision tree used for a classification problem; is a person ready to run a marathon? Decision trees are easy to interpret because of their pictorial nature. Generally decision trees are considered to be less accuarate compared to other advanced ML techniques. Interested readers can obtain more information about decision trees in [24, 64] . A.1.10 Random Forests. Random forests form a large number of decision trees based on the training data and then uses the combined behavior of those trees to make any predictions [3] . Only a subset of the entire training data is used to train individual trees. The subset of the training data and input features are selected randomly. Random forests can be used for both classification and regression. Random forests can solve the decision trees' problem of overfitting as they use an ensemble of decision trees. Random forests are known for their higher accuracy, however they are not very interpretable [28] . More information on random forests can be found in [23, 64] . network connect to each other using weighted edges. Neural networks are composed of three different layers: input layer (neurons in this layer get input from outer world), output layer (neurons in this layer generate predicted output response) and hidden layer(this layer performs useful transformations on input data and feed it to the output layer). There can be any number of hidden layers in a neural network. Figure 7 shows an example of a basic neural network, with only one hidden layer. This type of neural networks in which connections between layers move in forward direction only are known as feed-forward neural networks. Neural networks can have variable number of nodes and hidden layers. Different architectures of neural networks like perceptron, multilayer perceptron, recurrent neural network and boltzman machine network have been proposed. Neural networks learn by configuring the weights of edges and thresholds of activation functions iteratively to achieve the required results. There are two main learning algorithms to train neural networks: Gradient Descent: In this learning algorithm the weights of the NN are modified to minimize the difference between the actual outputs and the predicted outputs. Back propagation: In this learning algorithm the dissimilarity of the predicted and actual outputs is computed at the output layer and transferred to the input layer using hidden layer. Neural networks find their use in both regression and classification problems. Neural networks are known for their good accuracy. More information on neural networks can be found in [49, 108] A.2 Un-Supervised Learning The process of training machine learning models given only sets of input vectors and no output/target vectors is called Un-supervised learning [32] . In other words, the training data is not labelled in case of Unsupervised learning. This type of learning is used to attain a better understanding of the given data by identifying existing patterns in the data. A.2.1 Prinicpal Component Analysis. Principal component analysis (PCA) is largely used to reduce the number of dimensions (dimensionality reduction). It allows to understand exisintg patterns in given data by using small number of \"representative\" variables (called principal components) from a larger number of given \"correlated\" variables. Basically, PCA transforms the given data to another space such that there is maximum variance among variables in new space. The components which have least variance are discarded as they do not contain much information. More information on PCA can be found in [77] . A.2.2 Clustering. Another popular un-supervised learning technique is clustering i.e. finding groups of data in given unlabelled data-set. Following are two main types of clustering algorithms: K-Means Clustering: This algorithm starts with specification of required clusters K. Random data points are chosen as centoids of K clusters. The algorithm then identifies the points nearest to the centroid by using some distance measure, calculates mean of all points and assign a new centre to the cluster. The algorithm keeps on identifying closest points and calculate new centres untill a convergence condition is met. More information on K-means clustering can be found in [49, 64] . Heirarchical clustering: Heirarchical clustering does not require the specification of total number of clusters in advance. The algorithm builds a data binary tree that repeatedly combines similar data points. The mostly used form of heirachical clustering known as agglomerative clustering is performed as follows: Each single data point forms its own group. Two closest groups are combined iteratively, untill the time when all data points are contained in a single group/cluster. More information on heirarchical clustering can be found in [49, 64] . A.2.3 Anomaly Detection. Anamoly detection algorithms is a class of algorithms that identify data points with abnormal behavior in given data set. Many of the anamoly detection algorithms are unsupervised but they can be supervised and semisupervised as well [30] . A.3 Other Types of ML Techniques A.3.1 Reinforcement Learning. Reinforcement learning is based on an \"agent\" attached/associated to an \"environement\" [81] . Agent decides to take certain actions depending on the state of the environment. The changes in the state because of agent's actions are fed back to the agent using reinforcement signal. Depending on the consequences of earlier decision agent receives a reward or a penalty. Agent should always take actions that try to increase the overall reward [81] , which it tries to learn by trial and error using different algorithms. Q-learning algorithm is one of the largely used Reinforcement Learning (RL) algorithms. Interested readers can read more about reinforcement learning in [169] . A.3.2 Heuristic Algorithms. These type of ML algorithms use rules or heuristics while making decisions. They work well if the solution to a problem is expensive [93] . One of their famous types is Genetic Alorithms. These algorithms take their inspiration from nature. They use a process similar to evolution to find the best working solution to a problem. More information on genetic algorithms can be found in [164] . A.4 Techniques Related to Model Assessment There are various techniques used to assess a machine learning model. Following is a description of few of them: A.4.1 Validation Set Approach. This method involves partitioning the given data into two halves: training and validation/test sets. The training data set us used to train the Machine learning model. The trained model is then used for prediction of outputs using the available test set data. The error in test data is usually estimated through MSE (mean squared error). A potential problem with this approach is that the test error can have high variations [64] . A.4.2 Leave-One-Out Cross-Validation. Consider that there are n total observations, then in LOOCV one observation is excluded and the model is trained on the n-1 observations. The excluded observation is used to caluclate test error. All the observations are included in the test set one by one and as a result n test errors are calculated by using training on other observations. LOOCV has less bias and randomness compared to validation set approach [64] . This method can take long time to process if the number of observations is large. Fig. 1 .Fig. 2 . 12 Fig. 1. Number of works done in chronological order A Fig. 6. Decision Tree Fig. 7. Neural Network Table 1 . 1 Five Tribes of ML (taken from [39] ) Tribe Origins Master Algorithms Symbolists Logic, philosophy Inverse deduction Connectionists Neuroscience Backpropagation Evolutionaries Evolutionary biology Genetic programming Bayesians Statistics Probabilistic infernce Analogizers Psychology Kernel machines Table 2 . 2 ML Tribes and Examples of Their Use in Computer Architecture Tribe Targeted Problem Example rithms Algo- Example Works Inverse Knowledge Symbolists Composi- deduction, Decision tion trees Table 3 . 3 Summary of the Literature Survey on Machine Learning in Computer Architecture Technique Microarchitecture Power/Perform. Estimation Thread Scheduling Design Space Exploration Energy Improvement Instruction Scheduling Hardware Security Decision Trees [45, 105, 130, 144] [29, 43, 79] [56, 143] [122, 154, 200] [112, 124] [128, 129] Genetic Algo. [44] [59] [145, 167] [34, 95] K-Nearest [105] Neighbor [12, 29] [56] K-Means [79, 159, 189] [111, 184] Linear/Non-[5, 10, 20, 43, 99, Linear 133, 134, 136, 142, [120, 143] [66, 96] [191] [128, 129] Regression 146, 181, 198, 199] Logistic [105, 144] Regression [29] [33, 109] [86-88, 129, 135, 137], Bayes Theory [130] [85, 145] [80] [137] [14, 15, 27, 37, 40, 51, 69, 76, 84, 107, [22, 60, 83, 85, [31, 86, 88, Neural Network 113, 114, 140, 158, [29, 43, 48, 65, 117, 134, 147, 189] [17, 53, 56, 102, 131, 186, 187] 104, 106, 109, [188] [62] 119, 129, 135, 161, 162, 171-173, 154] 137] 185, 193, 195, 196] PCA [59] Random [11, 115, 134, 136] Forest [143] [154] Reinforcment [61, 91, 138, 152, [47] Learning 183, 197] [46, 78, 194] [116] SVM [36, 105] [12, 43] [56, 186, 187] [154] [128, 129, 137] Others [71, 130, 178] [43, 181] [120] [145] [101] [4] [137] Thread Scheduling: management of shared hardware resources, thread scheduling in heterogeneous systems. Design Space Exploration: design space exploration of single or multi-cores, gpus, accelerators, network-on-chips. Energy Improvements: energy aware thread assignment or micro-architecture design, dynamic voltage and frequency scaling. Hardware Security: micro-architectural Instruction Scheduling: static/dynamic instruction scheduling. side-channel attack detection and evasion. Note: The examples of sub-problems for each column in this table are following: Microarchitecture: branch prediction, cache replacement, cache reuse prediction, value prediction, memory scheduling, prefetching, network-on-chip design. Power/Perform. Estimation: power/performance estimation for single or multi-cores, gpus, cross-platform (e.g. one ISA to other or cpu to gpu), simulation augmentation. See Appendix A for a detailed summary of many ML techniques referred in this paper.Authors' addresses: Ayaz Akram, University of California, Davis, yazakram@ucdavis.edu; Jason Lowe-Power, University of California, Davis, jlowepower@ucdavis.edu. Regression techniques are not a part of the original ML tribes taxonomy of Pedro Domingos [39]"
}