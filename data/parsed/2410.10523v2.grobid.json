{
  "title": "Machine Learning for Inverse Problems and Data Assimilation",
  "abstract": "Data Assumption 5.9. We are able to sample from the likelihood l(•|𝑢) for any 𝑢 ∈ R 𝑑 . We are given data in the form of independent samples {(𝑦 (𝑛) , 𝑢 (𝑛) )} 𝑁 𝑛=1 from the joint distribution 𝛾.",
  "introduction": "Introduction",
  "body": "Preface Aim and Overview The aim of these notes is to demonstrate the potential for ideas in machine learning to impact on the fields of inverse problems and data assimilation. The perspective is one that is primarily aimed at researchers from inverse problems and/or data assimilation who wish to see a mathematical presentation of machine learning as it pertains to their fields. As a by-product, we include a succinct mathematical treatment of various fundamental underpinning topics in machine learning, and adjacent areas of (computational) mathematics. The fundamental underpinning material is summarized in Part III. Part I of the notes is concerned with inverse problems, employing material from Part III; Part II of the notes is concerned with data assimilation, employing material from Parts I and III. There are different ways to learn from these notes, and consequently there are competing ways in which the material can naturally be organized, from a pedagogical perspective. In particular, as presented here, the material allows application-oriented readers to proceed directly to the use of machine learning methodology, in either (or both) of inverse problems (Part I) and data assimilation (Part II), dipping into the fundamental material in Part III only as it arises and is used. Theory-oriented readers may instead prefer to work through the fundamentals in Part III first and, having absorbed this material, study the applications of machine learning from Parts I and II. Collectively we have taught from these notes in both ways and both have their merits. Overarching Concepts and Underpinning Fundamental Ideas Parts I and II, concerning inverse problems and data assimilation respectively, are organized to parallel one another. Both start with the introductory Chapters 1 and 6, framing the fields of inverse problems and data assimilation respectively. We adopt a Bayesian perspective, deriving optimization approaches from it. There are then four overarching concepts that define the parallel organization of material in Parts I and II. In Chapters 2 and 7 we discuss the use of ideas from variational Bayes to approximate probability measures from within tractable, and ideally expressive for the task at hand, subclasses of probability measures. Chapters 3 and 8 study the machine learning of prior models, from data. In Chapters 4 and 9 we focus on transport as a unifying concept for the learning of algorithms in inverse problems and data assimilation respectively. Chapters 5 and 10 discuss amortization, demonstrating how the machine learning ideas developed previously, and in particular transport based methods, can be generalized to learn dependence on the observations that define the inverse problems or data assimilation task. The key underpinning fundamental ideas used in these notes are contained in Part III. In Chapter 11 we study metrics, divergences and scoring rules. The first two provide ways of quantifying closeness of two probability measures; scoring rules assign a notion of closeness to a probability measure and to a point and, through averaging, can also be used to construct divergences. These notions of closeness are used in two primary ways: to measure robustness of probability measures to perturbations in problem specification; and to define objective functions used in machine learning. Chapter 12 is devoted to supervised learning, focusing on the use of neural networks, random features and Gaussian processes to approximate functions from data. Chapter 13 is devoted to the topics of unsupervised learning and generative modeling; in particular we introduce the use of transport as an important way of framing, and actioning, many tasks in unsupervised learning and generative modeling. In Chapter 14 we discuss time-series forecasting. Much of this material is fundamental to new emerging methods for purely data-driven data assimilation. The ideas in this chapter are important for understanding the current landscape of data assimilation research, but not our focus when studying machine learning for data assimilation in Part II; this is because purely data-driven methods are rapidly evolving and not yet as established as methods that combine modeland data-driven approaches. Chapter 15 describes optimization and sampling techniques that are broadly useful for defining algorithms in the fields of inverse problems and data assimilation. Optimization also plays a fundamental role in machine learning methods for inverse problems and data assimilation, since it is used for training machine learning models. Existing Review Articles This is a rapidly evolving research area and there are already several articles that review aspects of the material we will cover. See, for example, [25] for uses of ML in inverse problems, and [61, 106] and Chapter 10 of [96] for uses of ML in data assimilation. For background on neural networks and deep learning see [179] ; for background on Gaussian processes see [446] . For an overview of metrics and divergences, and their inter-relations, see [171] ; for a recent overview of scoring rules see [434] . Prerequisites Even though the subjects of inverse problems and data assimilation are succinctly reviewed in these notes, there remains an assumption of previous knowledge of these topics. The book [378] (also available in closely related form on arXiv) provides a good background on these topics, and we have tried to maintain similar notation. These notes also assume familiarity with linear algebra, probability, statistics, multivariable calculus, and matrix calculus; we establish notation in these areas in the following subsection. Useful graduate-level references on the topics of linear algebra, probability and statistics include [416, 186, 439] . We also refer the reader to books on inverse problems [139, 234, 429] and data assimilation [27, 362, 270] , that provide additional expositions on these topics from the mathematical and computational perspectives. And lastly we refer to the texts [2, 236, 289, 413] which cover inverse problems and data assimilation from more applications-oriented perspectives. Notation Throughout the notes we adopt the following notational conventions: • Sets. N denotes the set of positive integers {1, 2, 3, . . .}, and Z + := N ∪ {0} = {0, 1, 2, 3, . . .} denotes the set of non-negative integers. R 𝑑 denotes the set of 𝑑-dimensional real vectors, and R + denotes the non-negative reals. The set 𝐵(𝑢, 𝛿) ⊂ R 𝑑 denotes the open ball of radius 𝛿 at 𝑢 in R 𝑑 , in the Euclidean norm. The symbol ∅ denotes the empty set. • Vector Spaces Given 𝑢 ∈ R 𝑑 we define 𝑢 -𝑖 ∈ R 𝑑-1 by 𝑢 -𝑖 := (𝑢 1 , . . . , 𝑢 𝑖-1 , 𝑢 𝑖+1 , . . . , 𝑢 𝑑 ) ∈ R 𝑑-1 ; we make the natural modification if 𝑖 ∈ {1, 𝑑}. The symbol 𝐼 𝑑 ∈ R 𝑑×𝑑 denotes the identity matrix on R 𝑑 , and Id denotes the identity mapping. We denote the Euclidean norm on R 𝑑 by | • |, noting that it is induced by the inner-product ⟨𝑎, 𝑏⟩ = 𝑎 ⊤ 𝑏. We also use | • | to denote the induced norm on matrices and | • | 𝐹 to denote the Frobenius norm. We say that symmetric matrix 𝐴 is positive definite (resp. positive semi-definite) if ⟨𝑢, 𝐴𝑢⟩ is positive (resp. non-negative) for all 𝑢 ̸ = 0, sometimes denoting this by 𝐴 > 0 (resp. 𝐴 ≥ 0). We let R 𝑑×𝑑 sym denote the subset of symmetric matrices in R 𝑑×𝑑 and R 𝑑×𝑑 sym,> the subset of positive definite symmetric matrices and R 𝑑×𝑑 and R 𝑑×𝑑 sym,≥ the subset of positive semidefinite symmetric matrices. We use ≺ (resp. ⪯) to denote ordering between matrices in the cone of positive definite (resp. semidefinite) symmetric matrices. Likewise we use ≻ and ⪰ . We often wish to use covariance-weighted inner-product and norm and, to this end, for covariance matrix 𝐴 > 0, we define |𝑣| 2 𝐴 = 𝑣 ⊤ 𝐴 -1 𝑣. This norm | • | 𝐴 is induced by the weighted Euclidean inner-product ⟨• , •⟩ 𝐴 := ⟨• , 𝐴 -1 •⟩. The outer product ⊗ between two vectors 𝑎, 𝑏 ∈ R 𝑑 is defined by the following identity, assumed to hold for all vectors 𝑐 ∈ R 𝑑 : (𝑎 ⊗ 𝑏)𝑐 = ⟨𝑏, 𝑐⟩𝑎. We also use ⊗ to denote the Kronecker product between matrices. We use det and Tr to denote the determinant and trace functions on matrices. We use vec(•) to denote the vectorization operation on matrices. The Kronecker delta is 𝛿 𝑖𝑗 = 1 for 𝑖 = 𝑗 and 0 otherwise. • Probability Unless otherwise stated, throughout these notes random variables take values in Euclidean space. Any such random variable has probability distribution defined by a probability measure, say 𝜇, on Euclidean space. We will mainly consider probability measures that are characterized by a probability density function, say 𝜌, with respect to Lebesgue measure. For this reason we will sometimes blur the distinction between probability measures and probability density functions. In particular, when a random variable 𝑢 has probability density function 𝜌 we will write 𝑢 ∼ 𝜌 rather than 𝑢 ∼ 𝜇. We write i.i.d. for independent and identically distributed. Occasionally we will need to employ Dirac measures. In this context we will use the notational convention that the Dirac measure at point 𝑣 has \"density\" 𝛿(• -𝑣), also denoted by 𝛿 𝑣 (•). A particular use of Dirac measures arises in the construction of an empirical measure, approximating another measure 𝜇: if {𝑣 (𝑛) } 𝑁 𝑛=1 ∼ 𝜇 are i.i.d. then we may form an empirical approximation of 𝜇 as 1 𝑁 ∑︀ 𝑁 𝑛=1 𝛿 𝑣 (𝑛) . A centered random variable is a random variable with mean zero. Similarly, a probability density function 𝜌 is called centered if 𝑢 ∼ 𝜌 is a centered random variable. Given probability density function 𝜌, defined on R 𝑑𝑧 , and function 𝑔 : R 𝑑𝑧 → R 𝑑 , 𝜌 𝑔 denotes the probability density function of the random variable 𝑔(𝑧) where 𝑧 ∼ 𝜌. We refer to 𝜌 𝑔 as the pushforward of 𝜌 under 𝑔 and write 𝜌 𝑔 as 𝑔 ♯ 𝜌. Formally the pushforward notation 𝜌 𝑔 = 𝑔 ♯ 𝜌 means that P 𝜌𝑔 (𝐴) = P 𝜌 (𝑔 -1 (𝐴)) for any Borel set 𝐴. We note that, in some other texts, the pushforward is denoted by 𝑔#𝜌 = 𝜌 𝑔 or 𝑔 ⋆ 𝜌 = 𝜌 𝑔 . We denote by P(•), P(• | •) the probability density function of a random variable and its conditional probability density function, respectively. For jointly varying random variables (𝑢, 𝑣) we let 𝑢|𝑣 denote the random variable found by conditioning 𝑢 on a specific realization of random variable 𝑣. Thus 𝑢|𝑣 has probability density function P(𝑢|𝑣); and we write P(𝑢) and P(𝑣) for the marginal probability densities on 𝑢 and 𝑣, respectively. If two random variables are independent then we write 𝑢 ⊥ ⊥ 𝑣. We let 𝒫(R 𝑑 ) denote the space of all probability measures over R 𝑑 ; we simply write 𝒫 if the Euclidean space R 𝑑 is clear. Given 𝑓 : R 𝑑 ↦ → R we denote by E 𝜌 [𝑓 ] = ∫︁ R 𝑑 𝑓 (𝑢)𝜌(𝑢) 𝑑𝑢 the expectation of 𝑓 with respect to probability density function 𝜌 on R 𝑑 . On occasion, we also denote this expectation by 𝜌(𝑓 ). We write 𝒩 (︀ 𝑚, Σ )︀ for the distribution of a Gaussian random variable on R 𝑑 with mean 𝑚 and covariance Σ. If we want to denote the probability density function of this random variable, expressed in terms of dummy variable 𝑢, we write 𝒩 (︀ 𝑢; 𝑚, Σ )︀ . • Calculus Given function 𝑓 : R 𝑑 → R we denote by 𝐷𝑓 : R 𝑑 → R 𝑑 the gradient and by 𝐷 2 𝑓 : R 𝑑 → R 𝑑×𝑑 the Hessian. For more general functions 𝑓 : 𝒱 → R acting on elements 𝑣 of vector space 𝒱 we write 𝐷𝑓 to denote the derivative. We also directly consider functions 𝑓 : R 𝑑 → R 𝑑 with Jacobian matrix 𝐷𝑓 : R 𝑑 → R 𝑑×𝑑 . When there are two potentially varying arguments and we wish to indicate differentiation with respect to only one of them, say 𝑣, we will indicate this with a subscript: 𝐷 𝑣 (and similarly for second derivatives). When we contract a derivative to obtain divergence we write div : for example, for 𝑓 : R 𝑑 → R, Tr 𝐷𝑓 = div 𝑓. We use 𝑓 ∘ 𝑔 to indicate the composition of 𝑓 and 𝑔, assuming input dimensions (of 𝑓 ) and output dimensions (of 𝑔) are compatible, allowing composition. Finally if 𝐴 ⊂ R 𝑑 then we define the indicator function 1 𝐴 : R 𝑑 → R + by 1 𝐴 (𝑥) = 1 for 𝑥 ∈ 𝐴 and 1 𝐴 (𝑥) = 0 otherwise. support of DOE and NSF (US) and the BBVA Foundation (Spain). AMS would like to acknowledge the various US Government Agencies that have generously supported research in areas related to these notes: AFOSR, ARO, DARPA, DoD, NSF and ONR; in particular he would like to acknowledge support as a Vannevar Bush Faculty Fellow. All the authors thank the students who took ACM270 at Caltech in Spring 2024 and the TA Yixuan Wang. The lecture notes for this course constituted an early draft of these notes. The students' comments and questions have helped to improve them considerably. The authors are also grateful to Bohan Chen and Jochen Bröcker for helpful input. Warning and Request This is an early draft of these notes. As such the notes are likely to contain mathematical errors, incomplete bibliographical commentary and missing citations to the rapidly growing literature, inconsistencies in notation, and typographical errors. The authors would be grateful for all feedback that might help eliminate any of these issues. The authors may be contacted at: Eviatar Bach (eviatarbach@protonmail.com), Ricardo Baptista (r.baptista@utoronto.ca), Daniel Sanz-Alonso (sanzalonso@uchicago.edu), Andrew Stuart (astuart@caltech.edu) Contents I Inverse Problems 1 Bayesian Inversion 1.1 Bayesian Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Finite Dimensional Summaries . . . . . . . . . . . . . . . . . . . . . . . 1.2.1 Maximum A Posteriori Estimator . . . . . . . . . . . . . . . . . . 1.2.2 Posterior Expectations . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Well-Posedness Of Bayesian Inverse Problems . . . . . . . . . . . . . . . 1.4 Model Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4.1 Representing Error In Data Space . . . . . . . . . . . . . . . . . 1.4.2 Representing Error In Parameter Space . . . . . . . . . . . . . . 1.4.3 Parameterizing The Forward Model . . . . . . . . . . . . . . . . 1.5 Surrogate Modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.5.1 Accelerating Bayesian Inversion . . . . . . . . . . . . . . . . . . . 1.5.2 Posterior Approximation Theorem . . . . . . . . . . . . . . . . . 1.5.3 Accelerating Bayesian Inversion In The Presence Of Model Error 1.6 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Variational Inference 2.1 Variational Formulation Of Bayes Theorem . . . . . . . . . . . . . . . . 2.2 Canonical Approaches To Variational Inference . . . . . . . . . . . . . . 2.2.1 Mean-Field Family . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.2 Gaussian Distributions . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Properties Of Variational Inference . . . . . . . . . . . . . . . . . . . . . 2.3.1 Mode-Seeking Versus Mean-Seeking Variational Inference . . . . 2.3.2 Evidence Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Learning The Prior 3.1 Bayesian Hierarchical Modelling . . . . . . . . . . . . . . . . . . . . . . . 3.2 Learning The Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Representing The Prior Via A Pushforward . . . . . . . . . . . . . . . . 3.4 Perturbations To The Prior . . . . . . . . . . . . . . . . . . . . . . . . . 15.2.2 Gauss-Newton . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.3 Ensemble Kalman Inversion . . . . . . . . . . . . . . . . . . . . . . . . . 15.4 Expectation Maximization . . . . . . . . . . . . . . . . . . . . . . . . . . 15.5 Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . 15.5.1 The Metropolis-Hastings Algorithm . . . . . . . . . . . . . . . . 15.5.2 The Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . . . . . 15.6 Automatic Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . 15.6.1 Forward Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.6.2 Reverse Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15.7 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Bibliography Alphabetical Index Part I Inverse Problems Chapter 1 Bayesian Inversion A forward model 𝐺 : R 𝑑 → R 𝑘 specifies output 𝑦 ∈ R 𝑘 from input 𝑢 ∈ R 𝑑 by the relationship 𝑦 = 𝐺(𝑢). The related inverse problem is to recover unknown parameter 𝑢 ∈ R 𝑑 from data, or observation, 𝑦 ∈ R 𝑘 defined by 𝑦 = 𝐺(𝑢) + 𝜂; (1.1) here 𝜂 ∈ R 𝑘 denotes observation noise. In the Bayesian approach to the inverse problem we view (𝑢, 𝑦) as jointly varying random variables and the solution of the inverse problem is the posterior distribution on parameter 𝑢, given a specific instance of the data 𝑦, found by conditioning the joint distribution. Implicit in this framing of the inverse problem is that, although 𝜂 is unknown, its distribution is known. In Section 1.1 we formulate Bayesian inverse problems and state Bayes Theorem, giving an explicit expression for the posterior probability density function. In Section 1.2 we discuss finite dimensional summaries of the posterior and we connect Bayesian inversion to the classical optimization-based approach to inverse problems. Section 1.3 describes the well-posedness of the Bayesian formulation, using the Hellinger distance from Chapter 11. In Section 1.4 we discuss various perspectives on model error. Section 1.5 studies the use of machine-learned surrogate models. We conclude the chapter in Section 1.6, containing bibliographical remarks. Bayesian Inversion We view (𝑢, 𝑦) ∈ R 𝑑 × R 𝑘 as a random variable, whose joint distribution is specified by means of the identity (1.1) and the following assumption on the distribution of (𝑢, 𝜂) ∈ R 𝑑 × R 𝑘 : Assumption 1.1. The distribution of the random variable (𝑢, 𝜂) ∈ R 𝑑 × R 𝑘 is defined by assuming that 𝑢 ⊥ ⊥ 𝜂, that 𝑢 ∼ 𝜌 and that 𝜂 ∼ 𝜈, where 𝜌 and 𝜈 are given probability density functions. We make the following assumption about the provenance of the data: Data Assumption 1.2. Data 𝑦 ∈ R 𝑘 is given and is assumed to have come from a realization of the random variable (𝑢, 𝜂), under Assumption 1.1, and identity (1.1) . Combining these assumptions we can define the Bayesian inverse problem, a specific and important subclass of general Bayesian inference. We refer to 𝜌, the probability density function of 𝑢, as the prior probability density function. Given identity (1.1), then for fixed 𝑢 ∈ R 𝑑 , the distribution of 𝑦 given 𝑢 defines the likelihood: 𝑦|𝑢 ∼ l(𝑦|𝑢) := 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ . (1. 2) The posterior probability density function is the conditional distribution of 𝑢 given 𝑦, that is the distribution of random variable 𝑢|𝑦. The posterior density is the solution to the Bayesian formulation of the inverse problem. The primary computational challenge associated with Bayesian inversion is that it is an infinite dimensional problem. In particular it is important to appreciate that, although Bayes Theorem (which follows) delivers a formula for the probability density function of 𝑢|𝑦, the task of obtaining information from this probability density function, for example by drawing many samples from it, is, in general, a substantial challenge. 𝑦 -𝐺(𝑢) )︀ 𝜌(𝑢) 𝑑𝑢 > 0. Then 𝑢|𝑦 ∼ 𝜋 𝑦 , where 1 𝜋 𝑦 (𝑢) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝜌(𝑢). ( Proof. Recall from the Preface that we denote by P(•) the probability density function of a random variable; and we denote by P(•|•) the probability density function conditional on the second argument. The standard laws of conditional probability give the two identities P(𝑢, 𝑦) = P(𝑢|𝑦) P(𝑦), if P(𝑦) > 0, P(𝑢, 𝑦) = P(𝑦|𝑢) P(𝑢), if P(𝑢) > 0. The marginal probability density function on 𝑦 is given by P(𝑦) = 𝑦 -𝐺(𝑢) )︀ 𝜌(𝑢). Remark 1.4. We write the density of the joint distribution of (𝑦, 𝑢) ∈ R 𝑘 × R 𝑑 as 𝛾(𝑦, 𝑢). We write the marginal density on 𝑦 ∈ R 𝑘 as 𝜅(𝑦). Note that 𝜅(𝑦) = 𝑍. ♢ Remark 1.5. If 𝐺 is linear and (𝑢, 𝜂) is Gaussian then the posterior on 𝑢|𝑦 is Gaussian. This Gaussian setting provides an explicitly solvable inverse problem in which the posterior is characterized by its mean and covariance; it reduces the infinite dimensional problem to a finite dimensional problem for a vector (mean) in R 𝑑 and a positive definite matrix (covariance) in R 𝑑×𝑑 . To see this explicitly, let 𝐺(𝑢) = 𝐴𝑢 for matrix 𝐴 ∈ R 𝑘×𝑑 , and assume that 𝑢 ∼ 𝒩 (0, ̂︀ 𝐶) and 𝜂 ∼ 𝒩 (0, Γ) for positive definite covariance matrices ̂︀ 𝐶 and Γ. Using independence of 𝑢 and 𝜂 to define the likelihood and completing the square, we see that setting 𝐶 -1 = ̂︀ 𝐶 -1 + 𝐴 ⊤ Γ -1 𝐴, (1.4a) 𝐶 -1 𝑚 = 𝐴 ⊤ Γ -1 𝑦, ( 1.4b) we have that 𝜋 𝑦 = 𝒩 (𝑚, 𝐶). ♢ Remark 1.6. We have concentrated on formulating Bayes Theorem around equation (1.1) in which the noise appears additively and is independent of the unknown 𝑢. However Bayes Theorem is not restricted to this setting. Section 13.1 provides an explicit example, going beyond this additive noise setting, to formulate density estimation in a Bayesian fashion. ♢ We conclude by discussing the following modification of Data Assumption 1.2: Data Assumption 1.7. Assume that 𝑢 true ∈ R 𝑑 is a deterministic true unknown parameter and that 𝑦 ∈ R 𝑘 is given by 𝑦 = 𝐺(𝑢 true ) + 𝜂, (1.5) where 𝜂 is an unknown realization drawn from given centred noise probability density function 𝜈. Remark 1.8. This assumption is useful for two primary reasons. The first is as the basis for analysis of Bayesian methods from the frequentist perspective; for instance, it can be important to understand whether the posterior distribution concentrates around true parameter 𝑢 true if the variance of the noise 𝜂 is small or if the volume of data is high, and then to quantify the concentration rate. A second use of the assumption in the Bayesian context is when studying model misspecification; in particular it can be useful when the data 𝑦 is derived from a true unknown 𝑢 true ∈ R 𝑑 that could not have been drawn from the prior 𝜌. ♢ Finite Dimensional Summaries The posterior 𝜋 𝑦 contains all knowledge about the parameter 𝑢, given Assumption 1.1 and Data Assumption 1.2. It is often useful, however, to extract finite dimensional information from the posterior distribution, in order to summarize aspects of the distribution. Remark 1.5 identifies a specific situation where a finite dimensional summary, comprising the posterior mean and covariance, fully characterizes the posterior, since the posterior is Gaussian. In general, however, we must seek finite dimensional summaries that will not fully characterize the posterior. These summaries are often computed using optimization and sampling algorithms described in Chapter 15. Maximum A Posteriori Estimator A natural summary is the posterior mode or MAP estimator: Definition 1.9. A maximum a posteriori (MAP) estimator of 𝑢 given data 𝑦 is defined as any point 𝑢 MAP satisfying 𝑢 MAP ∈ arg max 𝑢∈R 𝑑 𝜋 𝑦 (𝑢). ♢ Optimization algorithms are often required to compute the MAP estimator. We now show how MAP estimation connects with classical optimization-based approaches to inversion. Recall that the posterior probability density function 𝜋 𝑦 on 𝑢|𝑦 from Theorem 1.3 has the form 𝜋 𝑦 (𝑢) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝜌(𝑢). Recalling (1.2) we define a loss function L(𝑢) = -log 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ = -log l(𝑦|𝑢), (1.6) and a regularizer R(𝑢) = -log 𝜌(𝑢). (1.7) In fact we will refer to any function which differs from L(𝑢) (resp. R(𝑢)) by a constant independent of 𝑢 as the loss (resp. regularizer). Adding the loss function and regularizer we obtain an objective function of the form J(𝑢) = L(𝑢) + R(𝑢). (1.8) Furthermore 𝜋 𝑦 (𝑢) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝜌(𝑢) ∝ 𝑒 -J(𝑢) . Thus the MAP estimator can be rewritten as a minimizer of J as follows: Remark 1.10. Setting the regularizer to zero, known as choosing a flat prior, we obtain the maximum likelihood estimation (MLE) problem. ♢ 𝑢 MAP ∈ arg max Remark 1.11. The formulation involving minimization of J(•) is simplest when the argument of the logarithm in (1.7), the prior, is strictly positive on all of R 𝑑 . However where the prior 𝜌 is zero we may interpret the regularizer R to take value +∞. The effect of this is to confine the minimization problem to the support of the prior. ♢ Example 1.12 (Gaussian Observation Noise and Gaussian Prior). If 𝜈 = 𝒩 (0, Γ), for positive definite covariance Γ, then 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ ∝ exp(- 1 2 |𝑦 -𝐺(𝑢)| 2 Γ ). Thus in this case the loss is L(𝑢) = 1 2 |𝑦 -𝐺(𝑢)| 2 Γ , a Γ-weighted ℓ 2 loss. Now assume that the prior is a centered Gaussian 𝜌 = 𝒩 (0, ̂︀ 𝐶), where ̂︀  𝐶 is positive definite. Then the corresponding regularizer is given by R(𝑢) = 1 2 |𝑢| 2 ︀ 𝐶 . Combining these assumptions we obtain a canonical objective function J(𝑢) = 1 2 |𝑦 -𝐺(𝑢)| 2 Γ + 1 2 |𝑢| 2 ︀ 𝐶 . (1.10) We refer to minimization of (1.10) as the Tikhonov-Phillips regularized inverse problem. If Γ = 𝐼 𝑘 and ̂︀ 𝐶 = 𝜆 -1 𝐼 𝑑 then this reduces to the classical Tikhonov regularized inverse problem with objective function J(𝑢) = 1 2 |𝑦 -𝐺(𝑢)| 2 + 𝜆 2 |𝑢| 2 . (1.11) Finally, consider the setting of Remark 1.5, where 𝐺(𝑢) = 𝐴𝑢 and the observation noise and prior are Gaussian. Then the MAP estimator related to the Tikhonov-Phillips regularized inverse problem is unique and given by point 𝑚 defined in (1.4) . In this case the posterior is Gaussian and the posterior mean is equal to the (in this case unique) MAP estimator. For general non-Gaussian posteriors, however, the MAP estimator and the posterior mean do not coincide. ♢ Example 1. 13 (ℓ 1 Regularizer -Laplace Prior). Now consider the setting where 𝜌(𝑢) ∝ exp (︂ -𝜆 𝑑 ∑︁ 𝑖=1 |𝑢 𝑖 | )︂ = exp(-𝜆|𝑢| 1 ). This is known as a Laplace distribution. Then R(𝑢) = 𝜆|𝑢| 1 , an ℓ 1 regularizer. Combining this prior with the weighted ℓ 2 loss above, we obtain the objective function J(𝑢) = 1 2 |𝑦 -𝐺(𝑢)| 2 Γ + 𝜆|𝑢| 1 . It is well known that regularizers of this type promote sparse solutions when J(•) is minimized. However it is important to appreciate that samples from the underlying posterior distribution are typically not sparse. ♢ Posterior Expectations In the previous Subsection 1.2.1 we summarized the posterior distribution 𝜋 𝑦 using its mode, known as the MAP estimator. Many other finite dimensional summaries of the posterior can be obtained through posterior expectations of the form E 𝜋 𝑦 [𝑓 (𝑢)] = ∫︁ R 𝑑 𝑓 (𝑢) 𝜋 𝑦 (𝑢) 𝑑𝑢, (1.12) where 𝑓 is a suitable choice of test function, which may take real, vector or matrix values. We next discuss three important choices of test function and the corresponding posterior summaries. Example 1.14 (Posterior Mean Estimator). Letting 𝑓 : R 𝑑 → R 𝑑 be the identity map in R 𝑑 , so that 𝑓 (𝑢) = 𝑢 for all 𝑢 ∈ R 𝑑 , we obtain the posterior mean estimator 𝑢 PM := E 𝜋 𝑦 [𝑢] of 𝑢 given data 𝑦. In applications such as imaging, the MAP estimator often provides sharper reconstructions than the posterior mean estimator; however, the latter has the advantage of being more stable to perturbations in the model, as will be explained in Section 1.3 below. ♢ Letting 𝑓 : R 𝑑 → R be the indicator function of a set 𝐴 ⊂ R 𝑑 , so that 𝑓 (𝑢) = 1 𝐴 (𝑢), we obtain the posterior probability of 𝐴 E 𝜋 𝑦 [1 𝐴 (𝑢)] = P 𝜋 𝑦 (𝐴). In some applications, it is important to understand whether the unknown 𝑢 lies on given set 𝐴 ⊂ R 𝑑 of the parameter space. In others, it is important to find set 𝐴 ⊂ R 𝑑 with a given prescribed probability, in order to build credible intervals. Credible intervals play a similar role in Bayesian statistics to confidence intervals in frequentist statistics. ♢ Finding the MAP estimator involves solving an optimization problem; in contrast, finding posterior expectations of the form (1.12) involves computing an integral over R 𝑑 . This task can be challenging in high dimension -constants entering errors resulting from standard quadrature rules typically depend badly on dimension. Monte Carlo methods are often preferred over standard quadrature rules because they have favorable dependence on dimension. The idea of Monte Carlo methods is to obtain 𝑁 posterior samples {𝑢 (𝑛) } 𝑁 𝑛=1 and approximate E 𝜋 𝑦 [𝑓 (𝑢)] ≈ 1 𝑁 𝑁 ∑︁ 𝑛=1 𝑓 (𝑢 (𝑛) ), 𝑢 (𝑛) ∼ 𝜋 𝑦 . In practice obtaining independent samples that are exactly distributed according to 𝜋 𝑦 can still be challenging. However Markov chain Monte Carlo (MCMC), discussed in Section 15.5, provides a widely applicable approach to obtain correlated samples {𝑢 (𝑛) } 𝑁 𝑛=1 from a Markov chain so that 𝑢 (𝑛) is approximately distributed according to the posterior for large 𝑛. MCMC sampling algorithms are commonly used to compute posterior expectations in Bayesian inverse problems. Well-Posedness Of Bayesian Inverse Problems The MAP estimator is an unstable quantity in that a small change in the problem specification, such as the forward model 𝐺 or the data 𝑦, can lead to a large change in the MAP estimator. On the other hand, the Bayesian formulation of the inverse problem leads to stability in broad generality: small changes in the problem specification lead to small changes in the posterior density and, consequently, to small changes in posterior expectations of suitable test functions. Here we prove a representative result of this type. We consider two different likelihoods l(𝑦|𝑢) = 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ and l 𝛿 (𝑦|𝑢) = 𝜈 (︀ 𝑦 -𝐺 𝛿 (𝑢) )︀ associated with two different forward models 𝐺(𝑢) and 𝐺 𝛿 (𝑢). Assuming the associated Bayesian inverse problems both adopt the same prior 𝜌 we obtain two different posteriors of the form 𝜋 𝑦 (𝑢) = 1 𝑍 l(𝑦|𝑢)𝜌(𝑢) and 𝜋 𝑦 𝛿 (𝑢) = 1 𝑍 𝛿 l 𝛿 (𝑦|𝑢)𝜌(𝑢), where 𝑍, 𝑍 𝛿 are the corresponding normalizing constants. Our aim is to show that if l(𝑦|𝑢) and l 𝛿 (𝑦|𝑢) are close, then so are the associated posteriors. To this end we make the following assumptions about the likelihoods, in which we view the data 𝑦 as fixed. Assumption 1.17. The data defined by Assumption 1.1 and Data Assumption 1.2 has positive probability under the resulting joint distribution on (𝑢, 𝑦), so that 𝑍 > 0. There exist 𝛿 + > 0 and 𝐾 1 , 𝐾 2 < ∞ such that, for all 𝛿 ∈ (0, 𝛿 + ), Recall the Hellinger distance D H (•, •) from Chapter 11. The main result of this section is: Theorem 1.18. Under Assumption 1.17, there exist Δ, 𝑐 > 0 such that, for all 𝛿 ∈ (0, Δ), D H (𝜋 𝑦 , 𝜋 𝑦 𝛿 ) ≤ 𝑐𝛿. Remark 1. 19 . Before proving it, we make two observations about the implications of Theorem 1.18: • The theorem ensures that expectations of test functions of 𝑢, with 𝑢 distributed according to 𝜋 𝑦 and 𝜋 𝑦 𝛿 respectively, are order 𝛿 apart, provided that sufficient moments of those test functions are available; this may be shown by use of Lemma 11.3. • The theorem is prototypical of a variety of stability results for Bayesian inversion. Here we have viewed data 𝑦 as fixed and considered error in the likelihood arising from approximation of the forward model. In Section 1.5 we establish a similar result in the context of machine learning approximations of the Bayesian inverse problem. The forward model is replaced by a surrogate found, for example, from a neural network approximation; this then changes the likelihood. It is also possible to estimate changes in the posterior, in the Hellinger metric, with respect to changes in the data; this, of course, also changes the likelihood. ♢ Proof of Theorem 1.18. To prove Theorem 1.18, we first characterize the stability of the normalization constants. We show that, under Assumption 1.17, there exist Δ, 𝑐 1 , 𝑐 2 > 0 such that, for all 𝛿 ∈ (0, Δ), |𝑍 -𝑍 𝛿 | ≤ 𝑐 1 𝛿 and 𝑍, 𝑍 𝛿 > 𝑐 2 . To see this, noting that 𝑍 = ∫︀ l(𝑦|𝑢)𝜌(𝑢) 𝑑𝑢 and 𝑍 𝛿 = ∫︀ l 𝛿 (𝑦|𝑢)𝜌(𝑢) 𝑑𝑢, we have |𝑍 -𝑍 𝛿 | = ⃒ ⃒ ⃒ ⃒ ∫︁ (︀ l(𝑦|𝑢) -l 𝛿 (𝑦|𝑢) )︀ 𝜌(𝑢) 𝑑𝑢 ⃒ ⃒ ⃒ ⃒ ≤ (︁ ∫︁ ⃒ ⃒ ⃒ √︁ l(𝑦|𝑢) - √︁ l 𝛿 (𝑦|𝑢) ⃒ ⃒ ⃒ 2 𝜌(𝑢) 𝑑𝑢 )︁ 1/2 (︁ ∫︁ ⃒ ⃒ ⃒ √︁ l(𝑦|𝑢) + √︁ l 𝛿 (𝑦|𝑢) ⃒ ⃒ ⃒ 2 𝜌(𝑢) 𝑑𝑢 )︁ 1/2 ≤ (︁ ∫︁ 𝛿 2 𝜙(𝑢) 2 𝜌(𝑢) 𝑑𝑢 )︁ 1/2 (︁ ∫︁ 𝐾 2  2 𝜌(𝑢) 𝑑𝑢 )︁ 1/2 ≤ 𝐾 1 𝐾 2 𝛿, 𝛿 ∈ (0, 𝛿 + ). The Lipschitz stability result follows by taking 𝑐 1 = 𝐾 1 𝐾 2 . Therefore, for 𝛿 ≤ Δ := min{ 𝑍 2𝐾 1 𝐾 2 , 𝛿 + }, we have 𝑍 𝛿 ≥ 𝑍 -|𝑍 -𝑍 𝛿 | ≥ 1 2 𝑍. Since 𝑍 is assumed positive we deduce the lower bound on 𝑍 𝛿 and take 𝑐 2 = 1 2 𝑍. We now study the Hellinger distance between the two posteriors, using the preceding estimate on the Lipschitz stability of the normalization constant. To this end we break the total error into two contributions, one reflecting the difference between 𝑍 and 𝑍 𝛿 , and the other the difference between l and l 𝛿 : D H (𝜋 𝑦 , 𝜋 𝑦 𝛿 ) = 1 √ 2 ⃦ ⃦ ⃦ √ 𝜋 𝑦 - √︁ 𝜋 𝑦 𝛿 ⃦ ⃦ ⃦ 𝐿 2 = 1 √ 2 ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 - √︃ l𝜌 𝑍 𝛿 + √︃ l𝜌 𝑍 𝛿 - √︃ l 𝛿 𝜌 𝑍 𝛿 ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 ≤ 1 √ 2 ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 - √︃ l𝜌 𝑍 𝛿 ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 + 1 √ 2 ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 𝛿 - √︃ l 𝛿 𝜌 𝑍 𝛿 ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 . From the stability estimate on the normalization constants we have, for 𝛿 ∈ (0, Δ), ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 - √︃ l𝜌 𝑍 𝛿 ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 = ⃒ ⃒ ⃒ ⃒ ⃒ 1 √ 𝑍 - 1 √ 𝑍 𝛿 ⃒ ⃒ ⃒ ⃒ ⃒ (︃ ∫︁ l(𝑦|𝑢)𝜌(𝑢) 𝑑𝑢 )︃ 1/2 = |𝑍 -𝑍 𝛿 | ( √ 𝑍 + √ 𝑍 𝛿 ) √ 𝑍 𝛿 ≤ 𝑐 1 2𝑐 2 𝛿. From Assumption 1.17 we have ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 𝛿 - √︃ l 𝛿 𝜌 𝑍 𝛿 ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 = 1 √ 𝑍 𝛿 (︃ ∫︁ ⃒ ⃒ ⃒ √︁ l(𝑦|𝑢) - √︁ l 𝛿 (𝑦|𝑢) ⃒ ⃒ ⃒ 2 𝜌(𝑢) 𝑑𝑢 )︃ 1/2 ≤ √︃ 𝐾 2 1 𝑐 2 𝛿. Therefore D H (𝜋 𝑦 , 𝜋 𝑦 𝛿 ) ≤ 1 √ 2 𝑐 1 2𝑐 2 𝛿 + 1 √ 2 √︃ 𝐾 2 1 𝑐 2 𝛿 = 𝑐𝛿, with 𝑐 = 1 √ 2 𝑐 1 2𝑐 2 + 𝐾 1 √ 2𝑐 2 , which is independent of 𝛿. The following corollary of Theorem 1.18 is a straightforward consequence of Lemma 11.2: Corollary 1.20 (Well-Posedness of Posterior). Under Assumption 1.17 there exist Δ, 𝑐 > 0 such that, for all 𝛿 ∈ (0, Δ), D TV (𝜋 𝑦 , 𝜋 𝑦 𝛿 ) ≤ 𝑐𝛿. Model Error The underlying concept behind this entire section is that the data 𝑦, which we use to determine unknown parameter 𝑢, is not actually generated by the identity (1.1). In short the computer code defining forward model 𝐺(•) does not exactly represent the physical reality that gave rise to the data, a situation referred to as model error; it is also another form of model misspecification. We describe three approaches to addressing this issue. The starting point for all of them is that the data 𝑦 arises from noisy observation of a physical process 𝐺 p : R 𝑑 → R 𝑘 so that 𝑦 = 𝐺 p (𝑢) + 𝜂. (1.13) The assumption is that 𝐺 p is not available to us, but that we have a (family of) computational model(s) 𝐺 c which we can use in place of 𝐺 p to determine 𝑢 from 𝑦. The reader should note that it is also possible to combine each of the three approaches to devise more general approaches to model error. Representing Error In Data Space This first approach makes the assumption that 𝐺 p : R 𝑑 → R 𝑘 is related to computer code 𝐺 c : R 𝑑 → R 𝑘 in the sense that 𝐺 p (𝑢) = 𝐺 c (𝑢) + 𝑏 (1.14) for some unknown vector 𝑏 ∈ R 𝑘 . Furthermore, random variable 𝜂 is assumed to be drawn from a centered distribution 𝜈 𝜎 (•), known up to a parameter 𝜎 ∈ R 𝑝 . Thus model error is present both because of the unknown shift between 𝐺 p and 𝐺 c and because of the unknown parameter in the distribution of the additive noise. Rather than just putting a prior 𝜌 on 𝑢, as in Section 1.1, we put a prior 𝜌 on (𝑢, 𝑏, 𝜎). The inverse problem is now reformulated as determining the distribution 𝜋 𝑦 of (𝑢, 𝑏, 𝜎)|𝑦. Combining (1.13) and (1.14) we obtain 𝑦 = 𝐺 c (𝑢) + 𝑏 + 𝜂, (1.15) where 𝜂 ∼ 𝜈 𝜎 . By applying Bayes Theorem 1.3 we obtain 𝜋 𝑦 (𝑢, 𝑏, 𝜎) = 1 𝑍 𝜈 𝜎 (︀ 𝑦 -𝐺 c (𝑢) -𝑏 )︀ 𝜌(𝑢, 𝑏, 𝜎), ( where 𝑍 = 𝑍(𝑦) := ∫︁ R 𝑑 ×R 𝑘 ×R 𝑝 𝜈 𝜎 (︀ 𝑦 -𝐺 c (𝑢) -𝑏 )︀ 𝜌(𝑢, 𝑏, 𝜎) 𝑑𝑢 𝑑𝑏 𝑑𝜎 > 0. Remark 1. 21 . In this setting the prior 𝜌(𝑢, 𝑏, 𝜎) is typically factored as an independent product of priors on 𝑢 and on each of the hyper-parameters 𝑏 and 𝜎. ♢ Representing Error In Parameter Space A different approach is to account for model error in parameter space. To achieve this, parameter 𝑢 is assumed to be perturbed by a draw 𝜗 from a random variable with distribution 𝑟 𝛽 known up to parameter 𝛽 ∈ R 𝑝 . Thus we write 𝑦 = 𝐺(𝑢 + 𝜗) + 𝜂, (1.17) assuming in this setting that 𝜂 ∼ 𝜈 and that 𝜈 is completely known. In principle the parameters (𝑢, 𝛽) can be found by putting a prior 𝜌 on (𝑢, 𝛽) and determining the likelihood l(𝑦|𝑢, 𝛽) from (1.17). Applying Bayes Theorem 1.3 we obtain 𝜋 𝑦 (𝑢, 𝛽) = 1 𝑍 l(𝑦|𝑢, 𝛽)𝜌(𝑢, 𝛽), (1.18) where 𝑍 = 𝑍(𝑦) := ∫︁ R 𝑑 ×R 𝑝 l(𝑦|𝑢, 𝛽)𝜌(𝑢, 𝛽) 𝑑𝑢 𝑑𝛽 > 0. As in Remark 1.21 the prior is usually factored as an independent product, here of 𝑢 and of 𝛽. A substantial challenge facing this approach is to define a tractable likelihood l(𝑦|𝑢, 𝛽). Rather than restricting to settings where the likelihood is tractable, an alternative approach is to use likelihood-free methods such as approximate Bayesian computation. We will discuss likelihood-free inference in Section 5.5. Here, we illustrate the representation of error in parameter space in the setting of a linear forward model. This can be interpreted as a linear forward model with the additive error 𝐴𝜗 + 𝜂. If 𝜗 and 𝜂 are independent zero-mean Gaussian random variables, the embedded model error corresponds to a likelihood model with inflated variance where the additional variance is related to the forward model. Then l(𝑦|𝑢, 𝛽) is determined by P(𝑦|𝑢, 𝛽) = 𝒩 (︀ 𝐴𝑢, 𝐴𝐶(𝛽)𝐴 ⊤ + Σ )︀ , where 𝐶(𝛽) = Cov(𝜗), Σ = Cov(𝜂). ♢ Parameterizing The Forward Model A third approach to model error postulates that the physically realizable forward map 𝐺 p is related to a class of computational models 𝐺 c : R 𝑑 × R 𝑝 → R 𝑘 in the sense that, for all 𝑢 ∈ R 𝑑 , and for some 𝛼 ∈ R 𝑝 , 𝐺 p (𝑢) = 𝐺 c (𝑢, 𝛼). ( It is assumed that 𝛼 is not known to us. Thus the problem becomes one of determining the pair (𝑢, 𝛼) from the observation 𝑦. Combining (1.13) and (1.19) suggests consideration of the Bayesian inverse problem of determining the distribution of (𝑢, 𝛼)|𝑦 given that 𝑦 = 𝐺 c (𝑢, 𝛼) + 𝜂. (1.20) Here 𝜂 again describes additive noise with known distribution 𝜈. Applying Bayes Theorem 1.3 we obtain 𝜋 𝑦 (𝑢, 𝛼) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺 c (𝑢, 𝛼) )︀ 𝜌(𝑢, 𝛼), (1.21) where 𝑍 = 𝑍(𝑦) := ∫︁ R 𝑑 ×R 𝑝 𝜈 (︀ 𝑦 -𝐺 c (𝑢, 𝛼) )︀ 𝜌(𝑢, 𝛼) 𝑑𝑢 𝑑𝛼 > 0. Similarly to Remark 1.21 the prior is usually factored as an independent product of priors on 𝑢 and on 𝛼. Surrogate Modelling In this section we focus on solving the inverse problem for 𝑢 given 𝑦, defined by (1.1) . The key idea is to approximate 𝐺 : R 𝑑 → R 𝑘 with a cheap-to-evaluate surrogate, 𝐺 𝛿 , using supervised learning from Chapter 12. Thus, to learn this approximation, we make the following assumption. Data Assumption 1.23. Data is available in the form {︁ 𝑢 (𝑛) , 𝑦 (𝑛)   }︁ 𝑁 𝑛=1 , (1.22) where the {𝑢 (𝑛) } 𝑁 𝑛=1 are generated i.i.d. from probability density function ϒ ∈ 𝒫(𝐷), for some 𝐷 ⊆ R 𝑑 , and where 𝑦 (𝑛) = 𝐺(𝑢 (𝑛) ). Remark 1.24. A key point to appreciate is that, since we will use approximation of 𝐺 to solve the Bayesian inverse problem for 𝜋 𝑦 given by Bayes Theorem 1.3, an ideal choice for ϒ is that it is close to 𝜋 𝑦 . This, of course, leads to a chicken-egg issue. In practice this can be addressed by choosing ϒ to have generous support, aiming to subsume that of 𝜋 𝑦 ; or by generating supervised learning data in tandem with solving the inverse problem, in an iterative fashion. ♢ In Subsection 1.5.1 we discuss the use of surrogate forward models to speed-up Bayesian inversion. Subsection 1.5.2 studies the effect of approximating the forward model on the posterior. In Subsection 1.5.3 we comment on surrogate modelling in the context of Bayesian inversion in the presence of model error; this requires a generalization of Data Assumption 1.23. Accelerating Bayesian Inversion Recall the inverse problem (1.1) of finding 𝑢 from 𝑦 where 𝑦 = 𝐺(𝑢) + 𝜂, under the setting of Assumption 1.1 and Data Assumption 1.2. If the Bayesian approach is adopted and MCMC (see Section 15.5 ) is used to sample the posterior, generating each new sample typically requires evaluating the likelihood, and hence the forward model 𝐺. For instance, in the Metropolis-Hastings Algorithm 15.2 the likelihood needs to be evaluated to compute the acceptance probability (15.19) . When 𝐺 is computationally expensive to evaluate, the multiple evaluations of 𝐺 may be prohibitively expensive. We address this issue by using a cheap computational surrogate 𝐺 𝛿 . While optimization algorithms for MAP estimation often require much fewer forward model evaluations for convergence than MCMC sampling algorithms, surrogate models can also be helpful in classical optimization-based approaches to inverse problems. For instance, surrogate models can facilitate the use of gradient-based optimization algorithms (see Chapter 15) in applications where gradients of 𝐺 are not available but gradients of the surrogate model 𝐺 𝛿 can easily be computed. The methods described in Chapter 12 can be used to approximate the (scalar-valued) likelihood l resulting from 𝐺, by an approximation l 𝛿 with small error 𝛿, uniformly over bounded open 𝐷 ⊂ R 𝑑 . In Chapter 12 we focus on learning functions taking values in R; however all such methods can be generalized to approximate vector-valued 𝐺. This leads to uniform approximation over 𝐷 ⊂ R 𝑑 of 𝐺 by 𝐺 𝛿 , which in turn results in a uniform approximation l 𝛿 of the likelihood l. Such an approximation 𝐺 𝛿 can be learned under the setting of Data Assumption 1.23. If carefully designed, the machine learning approximation of the likelihood will be much faster to evaluate than the true likelihood and, because of the approximation properties, will result in accurate posterior inference, with errors of size 𝛿. Such results rely on error bounds of the form given in Section 12.5 which (with high probability) can be obtained with a number 𝑁 of evaluations of the forward model which is often orders of magnitude smaller than the number of evaluations required within MCMC, or even MAP estimation in the context of learning model error (see Subsection 1.5.3 below). The resulting method can thus be very efficient. The next subsection describes underpinning theory to transfer approximation of the forward model to approximation of the posterior. Posterior Approximation Theorem We now establish that the approximate posterior resulting from approximating the likelihood is indeed close to the true posterior. We use error bounds of the form (12.21) within a modification of the well-posedness theory from Section 1.3. For notational convenience we drop the dependence of the likelihood on 𝑦 in the remainder of this chapter. Specifically we let l(𝑢) = 𝜈 here 𝑍, 𝑍 𝛿 are the corresponding normalizing constants. To prove the closeness in Hellinger distance between the true posterior and the posterior with machine-learned likelihood, we will rely on the following assumption. Assumption 1.25. The prior distribution on 𝑢 with density 𝜌 is supported on bounded open set 𝐷 ⊂ R 𝑑 and the data defined by Assumption 1.1 and Data Assumption 1.2 has positive probability under the resulting joint distribution on (𝑢, 𝑦), so that 𝑍 > 0. Furthermore, there exist 𝛿 + > 0 and 𝐾 1 , 𝐾 2 ∈ (0, ∞) such that, for all 𝛿 ∈ (0, 𝛿 + ), Using Assumption 1.25 we may state the following approximation result for the machine-learned posterior. When combined with Lemma 11.3, the following theorem guarantees that expectations computed with respect to the machine-learned posterior commit an error of order 𝛿, the magnitude of the error in the approximate likelihood. Proof. Throughout this proof all integrals are over the set 𝐷 defined in Assumption 1. 25 . As in the proof of the well-posedness Theorem 1.18, it can be shown that there exist Δ, 𝑐 1 , 𝑐 2 > 0 such that, for all 𝛿 ∈ (0, Δ), |𝑍 -𝑍 𝛿 | ≤ 𝑐 1 𝛿 and 𝑍, 𝑍 𝛿 ≥ 𝑐 2 . (1.24) Next, following proof of the well-posedness Theorem 1.18, we decompose the total error into two contributions, reflecting respectively the difference between 𝑍 and 𝑍 𝛿 , and the difference between the likelihoods l and l 𝛿 : D H (𝜋 𝑦 , 𝜋 𝑦 𝛿 ) ≤ 1 √ 2 ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 - √︃ l𝜌 𝑍 𝛿 ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 + 1 √ 2 ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 𝛿 - √︃ l 𝛿 𝜌 𝑍 𝛿 ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 . It then follows from (1.24) that, for 𝛿 ∈ (0, Δ), ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 - √︃ l𝜌 𝑍 𝛿 ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 = ⃒ ⃒ ⃒ ⃒ ⃒ 1 √ 𝑍 - 1 √ 𝑍 𝛿 ⃒ ⃒ ⃒ ⃒ ⃒ (︃ ∫︁ l(𝑢)𝜌(𝑢) 𝑑𝑢 )︃ 1/2 = |𝑍 -𝑍 𝛿 | ( √ 𝑍 + √ 𝑍 𝛿 ) √ 𝑍 𝛿 ≤ 𝑐 1 2𝑐 2 𝛿. And from Assumption 1.25 we have that ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 𝛿 - √︃ l 𝛿 𝜌 𝑍 𝛿 ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 = 1 √ 𝑍 𝛿 (︃ ∫︁ ⃒ ⃒ ⃒ √︁ l(𝑢) - √︁ l 𝛿 (𝑢) ⃒ ⃒ ⃒ 2 𝜌(𝑢) 𝑑𝑢 )︃ 1/2 ≤ √︃ 𝐾 2 1 𝑐 2 𝛿. Therefore D H (𝜋 𝑦 , 𝜋 𝑦 𝛿 ) ≤ 1 √ 2 𝑐 1 2𝑐 2 𝛿 + 1 √ 2 √︃ 𝐾 2 1 𝑐 2 𝛿 = 𝑐𝛿, with 𝑐 = 1 √ 2 𝑐 1 2𝑐 2 + 1 √ 2 √︂ 𝐾 2 1 𝑐 2 independent of 𝛿. Accelerating Bayesian Inversion In The Presence Of Model Error We argued in Subsection 1.5.1 for the potential use of surrogate modelling within MCMC, where multiple forward evaluations are typically required. Even when MAP estimation is used, multiple evaluations of 𝐺 may be needed to optimize the objective function. Furthermore, if model error is being jointly learned, in the setting of Subsection 1.4.3, then multiple MAP estimators may be required leading to many more evaluations. This subsection is centered on surrogate modelling for MAP estimation when model error is also being learned. Recall the Bayesian inverse problem to jointly estimate unknown parameter 𝑢 and unknown model error parameter 𝛼 defined in (1.21) . Thus we have access to a family of computational models 𝐺 c (• ; 𝛼) : R 𝑑 → R 𝑘 parameterized by 𝛼 ∈ R 𝑝 . Our data assumption is as follows: Data Assumption 1.28. Data is available in the form {︁ 𝑢 (𝑛) , 𝛼 (𝑛) , 𝑦 (𝑛)   }︁ 𝑁 𝑛=1 , (1.25) where the {𝑢 (𝑛) , 𝛼 (𝑛) } 𝑁 𝑛=1 are generated i.i.d. from probability density function ϒ ⊗ 𝑞 ∈ 𝒫(𝐷 × R 𝑝 ), for some 𝐷 ⊆ R 𝑑 , and where 𝑦 (𝑛) = 𝐺 c (𝑢 (𝑛) , 𝛼 (𝑛) ). From this data we may learn a cheap surrogate for 𝐺 c , using the techniques of Chapter 12. The MAP estimation problem, in the context of Subsection 1.4.3, requires minimization, over the pair (𝑢, 𝛼), of J(𝑢, 𝛼) = -log 𝜈 (︀ 𝑦 -𝐺 c (𝑢, 𝛼) )︀ -log 𝜌(𝑢, 𝛼). Some methods for this problem work by alternating minimization over 𝑢 and over 𝛼; a prototypical such method is to iterate, for ℓ until convergence, starting from initial 𝛼 0 ∈ R 𝑝 : 𝑢 ℓ+1 ∈ arg min 𝑢∈R 𝑑 J (︀ 𝑢, 𝛼 ℓ )︀ , (1.26a) 𝛼 ℓ+1 ∈ arg min 𝛼∈R 𝑝 J (︀ 𝑢 ℓ+1 , 𝛼 )︀ . (1.26b) Generalizations of this alternating strategy are possible; for example a finite number of steps (possibly ℓ-dependent) of gradient descent may be substituted for the two minimizations. For each ℓ, multiple evaluations of 𝐺 c (•, •) may be required and use of a surrogate may make the method more efficient. Finally, we point out that similar ideas to accelerate Bayesian inversion in the presence of model error apply when computing posterior summaries via sampling rather than a MAP estimator via optimization. For sampling the joint posterior distribution over the pair (𝑢, 𝛼), a natural approach is given by the Gibbs sampler from Subsection 15.5.2. As in (1.26) , the Gibbs sampler would then alternate between sampling the conditional of 𝑢 given 𝛼 and the conditional of 𝛼 given 𝑢. Such a Gibbs sampler would also directly benefit from use of cheap surrogate 𝐺 c . Bibliography For the Bayesian approach to inverse problems see [234, 413, 403, 119, 264, 265, 322] . In [250] the use of Data Assumption 1.7 underlies the study of posterior consistency for linear inverse problems. In the infinite dimensional setting considered there, the two motivations for Data Assumption 1.7 given in Remark 1.8 overlap: the regularity of 𝑢 true is such that it is not a draw from the prior, with probability one, and the effect of this on rates of contraction is studied. For classical optimization-based approaches to inverse problems, we refer to the books and lecture notes [414, 139, 429, 37, 307] . The concept of MAP estimation, which links probability to optimization, is discussed in the books [234, 413, 120] . For generalizations see the papers [202, 4, 261] . Monte Carlo methods are a broad class of approaches for characterizing posterior distributions using a sum of random Dirac measures; see [281, 365, 373] for textbooks on the theoretical and computational aspects of these methods. Quasi Monte Carlo (QMC) methods blend ideas from deterministic quadrature with the high dimensional benefits of Monte Carlo; see [79, 328] . While direct Monte Carlo sampling is possible in some settings, Markov chain Monte Carlo (MCMC) is a variant that is used in practice when it is not possible to sample exactly from the posterior. A general introduction to MCMC may be found in [159] . Sequential Monte Carlo (SMC) [124, 108] is a methodology which proceeds by introducing an interpolation from prior to posterior and incrementally steps through a sequence of inverse problems to connect them [53, 54, 239] . We provide a brief overview of MCMC in Section 15.5 and discuss the use of learned transport maps to accelerate MCMC in Section 4.5. Additional bibliographical remarks on MCMC can be found in Sections 4.6 and 15.7. The stability and well-posedness of Bayesian inverse problems was first studied in [294] , using the Kullback-Leibler divergence (see Definition 11.34 ) and focusing on perturbations in the data. The articles [403, 119] study similar stability and wellposedness results in the Hellinger metric, as we do here; application to perturbations arising from numerical approximation of the forward model 𝐺 may be found in [111] . Related results on stability and well-posedness, but using other distances and divergences, may be found in [266] . The papers [218, 217] discuss generalizations of the well-posedness theory to various classes of specific non-Gaussian priors. The approach to model error outlined in Subsection 1.4.1 was introduced in [242] . The embedded model error framework of Subsection 1.4.2 was introduced in [379] ; for an introduction to approximate Bayesian computation see [389] . An example of the approach to model error described in Subsection 1.4.3 may be found in [112] . A parameterized forward model may be a coarse-scale model where unresolved small-scale physical processes are described by parameters 𝛼. (For example, 𝛼 may describe cloud cover in a climate model 𝐺 c .) More examples will be provided in Chapter 8. Methodologies to identify parameters of structural error models (e.g., within computational models for dynamical processes), often using indirect data, are described in [449, 276] . Using emulators to speed up forward model evaluations, for example in the context of likelihood evaluation in Bayesian inversion, was first introduced as a systematic methodology in [369] , and taken further in the realm of Bayesian model error estimation in [242] . The paper [402] studies the use of Gaussian processes for emulation, and derives errors bounds quantifying the effect of emulation error on the posterior. The methodology is developed for a range of applications in the geosciences in [109] ; in particular that paper addresses the issue of how to determine ϒ in tandem with solution of the inverse problem as discussed in Remark 1.24. A specific application of the idea in climate science may be found in [133] . Data-driven discretizations of forward models for Bayesian inversion are studied in [58] . A recent approach to directly learning parameter to solution (forward) and solution to parameter (inverse) surrogate maps may be found in [424] . Surrogate models are also useful in optimization approaches to inverse problems. In this regard, Bayesian optimization techniques [153] replace an expensive-to-evaluate objective function with a surrogate model, and gradually improve this surrogate model along the optimization process by judiciously acquiring new training data. We refer to [244, 245] for a discussion of Bayesian optimization for MAP estimation in Bayesian inverse problems. An important aspect of learning forward models is to choose the pair of supervised training data over which we would like to find an accurate surrogate model. While we would ideally like to be accurate over the support of the posterior, finding such a surrogate model requires being able to sample the posterior. Instead, it is common to seek the approximate surrogate model to be accurate over the support of the prior. To address this, several recent methods use approximate posteriors to iteratively refine the approximation of the surrogate model [109, 203] . Chapter 2 Variational Inference A problem has a variational formulation if its solution can be written as the minimizer of an objective function. The variational formulation of any problem in mathematics is useful because it opens the door to computational methods. Such computational methods seek to minimize the objective function over a strict subset of the whole space over which the original variational problem is posed. If the strict subset is chosen judiciously (this is a problem-dependent choice) then the resulting computational methodology is both tractable and yields a useful, and interpretable, approximation of the solution of the original variational problem posed on the whole space. This chapter is devoted to formulating Bayes Theorem 1.3 variationally and using this formulation as the basis for approximate inference. We continue to work under Data Assumption 1.2. Under this data assumption, and given a prior distribution on 𝑢, the posterior distribution on 𝑢|𝑦 is defined by Bayes Theorem 1.3. In Section 2.1 we show how this theorem can be formulated variationally as a minimization problem over the set of all probability density functions. This leads to the core methodology of variational inference, and in Section 2.2 we present two canonical choices of subsets of all probability density functions over which to approximate the posterior: the mean-field and Gaussian subsets. In Section 2.3 we discuss some key properties of variational inference, studying other choices of optimization problem which yield the posterior as a minimizer, and linking to the evidence lower bound. We conclude the chapter, in Section 2.4, with bibliographical remarks. Variational Formulation Of Bayes Theorem Our starting point in this section is Bayes Theorem 1.3. Because 𝑦 dependence is not central to our discussions here we write the posterior as 𝜋 := 𝜋 𝑦 . A useful formulation of Bayes Theorem arises from seeking the posterior as the solution of an optimization problem over the space of probability density functions. In what follows we recall from the Preface that 𝒫 := 𝒫(R 𝑑 ) denotes the space of all probability density functions over R 𝑑 . The objective F : 𝒫 → R of the optimization problem measures the discrepancy between the posterior density 𝜋 and a candidate density 𝑞; the objective is constructed to be minimized at 𝑞 = 𝜋. By using Bayes Theorem 1.3 the objective function can be rewritten in terms of the prior, the likelihood and the normalization constant. One natural class of objective functions F is defined by the class of f-divergences D f (𝑞‖𝜋) : F(𝑞) := D f (𝑞‖𝜋) = ∫︁ f (︂ 𝑞(𝑢) 𝜋(𝑢) )︂ 𝜋(𝑢) 𝑑𝑢. See Chapter 11 for the definition and properties of this family of distance-like objects defined on the space of probability densities. By the properties of all divergences, detailed at the start of Section 11.2, we deduce that F(𝑞) is minimized at 𝑞 = 𝜋 and that this is the unique minimizer. A particularly useful choice arises from setting f(𝑡) = 𝑡 log(𝑡), leading to definition of the objective F(𝑞) := D KL (𝑞‖𝜋) = ∫︁ log (︂ 𝑞(𝑢) 𝜋(𝑢) )︂ 𝑞(𝑢) 𝑑𝑢, ( 2 .1) the Kullback-Leibler (KL) divergence. This choice of objective is convenient because F(•) can be minimized without knowledge of the normalization constant of 𝜋, thus sharing a key property with Metropolis-Hastings MCMC methods -see Remark 15.21. For proof of this property of the minimization of F(•) see the proof of Theorem 2.1 and Remark 11.36. Because of this desirable property, we work almost exclusively with KL divergence (2.1) to define variational formulations of Bayes Theorem 1.3. However, in Subsection 2.3.1, we discuss reversing the order of 𝑞 and 𝜋 in the arguments of the KL divergence. Using the KL divergence (2.1) to formulate variational inference delivers the following theorem: Theorem 2.1. Consider the Bayesian inverse problem defined by prior 𝜌 and likelihood l given by (1.2), under Data Assumption 1.2. Define J : 𝒫 → R by J(𝑞) = D KL (𝑞‖𝜌) -E 𝑞 [log l(𝑦|•)], (2.2a) 𝑞 opt ∈ arg min 𝑞∈𝒫 J(𝑞). ( 2 .2b) Then 𝑞 opt = 𝜋, the posterior distribution. Proof. By the properties of all divergences, detailed at the start of Section 11.2, it follows that F(•) = D KL (• ‖𝜋) is non-negative for all inputs from the set of probability density functions and has a unique minimizer over 𝒫 at 𝑞 = 𝜋. Using the form of the posterior density, we see that F(𝑞) = E 𝑞 [log 𝑞 -log 𝜌 -log l(𝑦|•) + log 𝑍] (2.3a) = D KL (𝑞‖𝜌) -E 𝑞 [log l(𝑦|•)] + log 𝑍 (2.3b) = J(𝑞) + log 𝑍. (2.3c) Because 𝑍 is a constant with respect to 𝑞, the minimizer of F(•), which is unique, is equivalent to the minimizer of J(•) and the proof is complete. Remark 2.2. Note that, using the loss function (1.6), we may write J(𝑞) = E 𝑞 [L] + D KL (𝑞‖𝜌). (2.4) Using this formulation of J(•) we comment on the structure on the minimization problem (2.2) in relation to the MAP estimation problem presented in Section 1.2. The MAP problem requires minimization of (1.8). Comparison with (2.4) shows that in both cases the objective involves two terms that balance the fit to the data (the first term) with properties of the prior distribution (the second term). Furthermore the first term in (2.4) is the expectation of the first term in (1.8) . Note that minimization of (2.4) is more general than minimization of (1.8): the former provides a probability density function rather than the point estimator provided by the latter. ♢ Canonical Approaches To Variational Inference In practice, characterizing the posterior distribution is difficult in general. One approach to doing so is to approximate the posterior using the variational formulation. We replace the set 𝒫 in (2.2) by a tractable class of probability measures 𝒬 ⊂ 𝒫 for the purpose of computations and for the purpose of revealing an explicit, interpretable form. The variational inference problem is then given by solving the following optimization problem: 𝑞 ⋆ ∈ arg min 𝑞∈𝒬 J(𝑞). (2.5) In the next two subsections we outline two canonical classes of tractable distributions 𝒬: in Subsection 2.2.1 the product of independent components for each marginal; and in Subsection 2.2.2 multivariate Gaussians. The former case is known as mean-field variational inference and the latter as Gaussian variational inference. Mean-Field Family Mean-field inference works with a family 𝒬 of distributions which factorize into a product of independent components. While this choice does not represent dependencies among its variables, it is often chosen because of the efficiency of methods for finding 𝑞 ⋆ in this case, and because of the ease of computing marginal properties once 𝑞 ⋆ is determined. Definition 2.3. The probability density function 𝑞 of a random variable 𝑢 ∈ R 𝑑 is in the mean-field family 𝒬 if the coordinates of 𝑢 are independent. That is, 𝑞 can be written in the form 𝑞(𝑢) = 𝑑 ∏︁ 𝑖=1 𝑞 𝑖 (𝑢 𝑖 ), where 𝑞 𝑖 : R → R + denotes the probability density function of the 𝑖th coordinate 𝑢 𝑖 ∈ R of 𝑢 ∈ R 𝑑 . ♢ With this choice of 𝒬 the solution of the optimization problem in (2.5) may be approached by using the following consistency equations, relating the marginal densities {𝑞 𝑖 } 𝑑 𝑖=1 to one another. All integrals appearing in the statement and proof of the following proposition are over the whole of Euclidean space of the relevant dimension. We employ the notation 𝑢 -𝑖 ∈ R 𝑑-1 described in the Preface. Proposition 2.4. Let 𝒬 be defined as in Definition 2.3. Then the optimal 𝑞 ⋆ = ∏︀ 𝑑 𝑖=1 𝑞 ⋆ 𝑖 (𝑢 𝑖 ) solving optimization problem (2.5) satisfies 𝑞 ⋆ 𝑖 (𝑢 𝑖 ) ∝ exp ⎛ ⎝ ∫︁ log 𝜋(𝑢) ∏︁ 𝑗̸ =𝑖 𝑞 ⋆ 𝑗 (𝑢 𝑗 ) 𝑑𝑢 -𝑖 ⎞ ⎠ for 𝑖 = 1, . . . , 𝑑. Proof. Recall that minimizing J(•) is equivalent to minimizing D KL (•‖𝜋). The KL divergence D KL (𝑞‖𝜋) for a density 𝑞 in the mean-field family is given by D KL (𝑞‖𝜋) = ∫︁ log (︃ 𝑑 ∏︁ 𝑖=1 𝑞 𝑖 (𝑢 𝑖 ) )︃ 𝑑 ∏︁ 𝑖=1 𝑞 𝑖 (𝑢 𝑖 ) 𝑑𝑢 - ∫︁ log 𝜋(𝑢) 𝑑 ∏︁ 𝑖=1 𝑞 𝑖 (𝑢 𝑖 ) 𝑑𝑢 = 𝑑 ∑︁ 𝑖=1 ∫︁ log 𝑞 𝑖 (𝑢 𝑖 )𝑞 𝑖 (𝑢 𝑖 ) 𝑑𝑢 𝑖 - ∫︁ log 𝜋(𝑢) 𝑑 ∏︁ 𝑖=1 𝑞 𝑖 (𝑢 𝑖 ) 𝑑𝑢, where in the last line we used that each integrand only depends on 𝑢 𝑖 and each 𝑞 𝑖 is a probability density function that integrates to 1. Taking the first variation of the KL divergence with respect to each 𝑞 𝑖 , we have 𝛿 𝛿𝑞 𝑖 D KL (𝑞‖𝜋) = 1 + log 𝑞 𝑖 (𝑢 𝑖 ) - ∫︁ log 𝜋(𝑢) ∏︁ 𝑗̸ =𝑖 𝑞 𝑗 (𝑢 𝑗 ) 𝑑𝑢 -𝑖 . Setting the first variation equal to zero and re-arranging the terms gives us the unnormalized form for each marginal density 𝑞 ⋆ 𝑖 at any critical point of J(•) over 𝒬. Remark 2.5. We note that although the preceding proposition is formulated in terms of the posterior density, its statement remains unchanged if 𝜋(𝑢) is replaced by 𝜌(𝑢)l(𝑦|𝑢); this simply changes the constant of proportionality. Thus, to use it, we do not need to know the normalization constant 𝑍 in Bayes Theorem 1. ♢ The proposition defines a set of coupled equations that must be satisfied by the optimal marginal densities in terms of the other marginal densities. Moreover, it prescribes how to set 𝑞 𝑖 when keeping all other coordinates fixed. Thus, a natural approach to find the minimizer is to perform coordinate updates on each marginal. This algorithm is known as coordinate-ascent variational inference (CAVI). Ascent because the method is traditionally formulated in terms of maximizing the ELBO functional (see Definition 15.15) rather than minimizing the KL divergence; we prefer, for consistency with other optimization problems in the notes, to formulate it in terms of minimizing J, and hence descent. The updates are thus, sequentially for iteration index ℓ ∈ Z + until convergence, and for 𝑖 = 1, . . . , 𝑑 , 𝑞 ℓ+1 𝑖 (𝑢 𝑖 ) ∝ exp ⎛ ⎝ ∫︁ log 𝜋(𝑢) ∏︁ 𝑗<𝑖 𝑞 ℓ+1 𝑗 (𝑢 𝑗 ) ∏︁ 𝑗>𝑖 𝑞 ℓ 𝑗 (𝑢 𝑗 ) 𝑑𝑢 -𝑖 ⎞ ⎠ . ( 2 .6) Algorithm 2.1 Coordinate-Ascent Variational Inference 1: Input: Density 𝜋 known up to normalizing constant. Initialization 𝑞 0 . Number 𝐿 of iterations. 2: For ℓ = 0, 1, 2, . . . , 𝐿 -1, compute 𝑞 ℓ+1 from 𝑞 ℓ : 3: Update 𝑞 ℓ+1 𝑖 for 𝑖 = 1, . . . , 𝑑 using (2.6). 4: Output: Approximation 𝑞 𝐿 of density 𝜋. Remark 2.6. The mean-field methodology has the computational advantage of reducing the inference problem in (potentially high) dimension 𝑑 to one of 𝑑 independent onedimensional inference problems. As such it can potentially accurately learn marginal information on specific coordinates; but it cannot learn correlations. In the next subsection we work in a different subset 𝒬, the set of Gaussian measures, in which it is possible to approximate correlation information. ♢ Gaussian Distributions Another tractable variational approach is to seek an approximate distribution 𝑞 ⋆ from within a parametric family 𝒬 of probability density functions. In many applications, interest is focused on ultimately estimating the first two moments of 𝜋; in this context a natural family to consider is Gaussian distributions. In this subsection we derive optimality conditions for the solution to the resulting optimization problem. To find a Gaussian approximation, we seek to minimize D KL (•‖𝜋) over the set of distributions 𝒬 := {︀ 𝑞 ∈ 𝒫 : 𝑞 = 𝒩 (𝑚, Σ), (𝑚, Σ) ∈ 𝒞 }︀ , ( 2.7a ) 𝒞 := {︀ 𝑚 ∈ R 𝑑 , Σ ∈ R 𝑑×𝑑 sym,≥ }︀ . (2.7b) Proposition 2.7. Let 𝒬 be defined as in (2.7) . Then the optimal 𝑞 ⋆ = 𝒩 (𝑚 ⋆ , Σ ⋆ ) solving (2.5) satisfies (𝑚 ⋆ , Σ ⋆ ) ∈ arg min (𝑚,Σ)∈𝒞 (︁ E 𝑞 [-log 𝜋(𝑢)] - 1 2 log det(Σ) )︁ , (2.8) where the expectation is with respect to 𝑞 = 𝒩 (𝑚, Σ). Proof. Recall that 𝑞 ⋆ minimizes D KL (•‖𝜋) over 𝒬. The result follows from using the fact that, for 𝑞(𝑢) being the density of 𝒩 (𝑚, Σ), we have 𝑞(𝑢) = 1 (2𝜋) 𝑑/2 det(Σ) 1/2 exp (︁ - 1 2 |𝑢 -𝑚| 2 Σ )︁ . (2.9) We may rewrite any expectation under 𝑞 in terms of a standard Gaussian random variable 𝜉 ∼ 𝒩 (0, 𝐼 𝑑 ). Indeed, using the relation 𝑢 = 𝑚 + Σ foot_1 / foot_2 𝜉, it may be shown that ∫︁ log 𝑞(𝑢) 𝑞(𝑢) 𝑑𝑢 = -𝑑 2 -𝑑 2 log(2𝜋) -1 2 log det(Σ). Since D KL (𝑞‖𝜋) = ∫︁ log 𝑞(𝑢) 𝑞(𝑢) 𝑑𝑢 -∫︁ log 𝜋(𝑢) 𝑞(𝑢) 𝑑𝑢, the desired result follows. Remark 2.8. The two terms in the objective function (2.8) can be interpreted as having a competitive behavior and as regularizing the MAP perspective. The first term is maximized by a Gaussian distribution with zero variance, centered at the MAP estimator of 𝜋, the point of highest posterior density: a Dirac measure at the posterior mode. The second term, however, approaches positive infinity if the variance of any marginal approaches zero. Hence, the second term in the objective regularizes the optimization problem to ensure the approximating Gaussian is not degenerate in any direction. ♢ Proposition 2.9. Let (𝑚 ⋆ , Σ ⋆ ) ∈ 𝒞 solve the Gaussian variational inference problem defined by Proposition 2.7. The solution satisfies the first-order optimality conditions 1 E 𝜉 [𝐷 log 𝜋(𝑚 ⋆ + (Σ ⋆ ) 1/2 𝜉)] = 0, E 𝜉 [𝐷 2 log 𝜋(𝑚 ⋆ + (Σ ⋆ ) 1/2 𝜉)] = -(Σ ⋆ ) -1 , where 𝜉 ∼ 𝒩 (0, 𝐼 𝑑 ). Proof. Throughout the proof, use of 𝐷 or 𝐷 2 without a subscript is to be taken to denote derivative(s) with respect to variable 𝑢; in addition, 𝑞 denotes the Gaussian density 𝒩 (𝑚, Σ) given in (2.9). The first-order optimality condition for the optimization problem in (2.8) is given by 𝐷 (𝑚,Σ) [︂ E 𝑞 [-log 𝜋(𝑢)] - 1 2 log det(Σ) ]︂⃒ ⃒ ⃒ ⃒ (𝑚 ⋆ ,Σ ⋆ ) = 0. The gradients of the objective with respect to 𝑚 and Σ are given by 𝐷 𝑚 [︂∫︁ -log 𝜋(𝑢)𝑞(𝑢) 𝑑𝑢 - 1 2 log det(Σ) ]︂ = ∫︁ -log 𝜋(𝑢)𝐷 𝑚 𝑞(𝑢) 𝑑𝑢, 𝐷 Σ [︂∫︁ -log 𝜋(𝑢)𝑞(𝑢) 𝑑𝑢 - 1 2 log det(Σ) ]︂ = ∫︁ -log 𝜋(𝑢)𝐷 Σ 𝑞(𝑢) 𝑑𝑢 - 1 2 Σ -1 . From the symmetry of the Gaussian density with respect to 𝑢 and 𝑚, we have 𝐷 𝑚 𝑞(𝑢) = 𝐷𝑞(𝑢), 𝐷 Σ 𝑞(𝑢) = 𝑞(𝑢)𝐷 Σ log 𝑞(𝑢) = -𝑞(𝑢) 1 2 [Σ -1 -Σ -1 (𝑢 -𝑚)(𝑢 -𝑚) ⊤ Σ -1 ] = 1 2 𝐷 2 𝑞(𝑢). Substituting these expressions above and applying integration by parts gives us ∫︁ -log 𝜋(𝑢)𝐷 𝑚 𝑞(𝑢) 𝑑𝑢 = ∫︁ -log 𝜋(𝑢)𝐷𝑞(𝑢) 𝑑𝑢 = ∫︁ 𝐷 log 𝜋(𝑢)𝑞(𝑢) 𝑑𝑢, ∫︁ -log 𝜋(𝑢)𝐷 Σ 𝑞(𝑢) 𝑑𝑢 = - 1 2 ∫︁ log 𝜋(𝑢)𝐷 2 𝑞(𝑢) 𝑑𝑢 = - 1 2 ∫︁ 𝐷 2 log 𝜋(𝑢)𝑞(𝑢) 𝑑𝑢. Setting the two gradients equal to zero and re-writing the expectations in terms of a standard Gaussian random variable 𝜉 ∼ 𝒩 (0, 𝐼 𝑑 ) using the relation 𝑢 = 𝑚 + Σ 1/2 𝜉 for a positive definite covariance Σ gives us the result. Remark 2.10. If 𝑐 -𝐼 𝑑 ⪯ 𝐷 2 log 𝜋(𝑢) ⪯ 𝑐 + 𝐼 𝑑 for all 𝑢 ∈ R 𝑑 , then the optimal solution satisfies 1 𝑐 + 𝐼 𝑑 ⪯ Σ ⋆ ⪯ 1 𝑐 -𝐼 𝑑 . ♢ 2.3 Properties Of Variational Inference 2.3.1 Mode-Seeking Versus Mean-Seeking Variational Inference In the preceding section we seek an approximation to the posterior distribution 𝜋 by minimizing, over 𝒬 ⊂ 𝒫, the functional 𝑞 ↦ → D KL (𝑞‖𝜋) = E 𝑞 [log(𝑞/𝜋)]; this is sometimes referred to as the reverse KL divergence. Using the reverse KL divergence as an objective is computationally convenient in the setting where we can evaluate the likelihood and the prior, but we do not necessarily know the normalization constant, nor do we necessarily have the ability to easily sample from the posterior. In comparison, the forward KL divergence 𝑞 ↦ → D KL (𝜋‖𝑞) = E 𝜋 [log(𝜋/𝑞)] is more convenient for optimization over 𝒬 when we can sample from 𝜋, but do not necessarily have access to an analytical expression for the target density. In addition to computational considerations, the two choices of KL divergence lead to different behavior when minimizing over a subset of probability measures. The reverse KL favors approximate distributions where log(𝑞/𝜋) is small in regions of high probability of 𝑞, that is when 𝑞 ≈ 𝜋 where 𝑞 is large, or in regions where 𝑞 is much smaller than 𝜋. As a result, the minimizers for 𝑞 tend to fit one mode of multi-modal distributions 𝜋, but miss the other modes. Hence, minimizing the reverse KL is also known as mode-seeking. In contrast, the minimizers of the forward KL favor approximate distributions 𝑞 where log(𝜋/𝑞) is small in regions of high probability of 𝜋. This occurs by having 𝑞 be non-zero everywhere in the support of 𝜋 so the denominator in the log does not approach zero. Placing mass everywhere in the support of 𝜋 leads to a mean-seeking behavior, where the minimizer for 𝑞 corresponds to matching the moments of the target density 𝜋. The following remark shows that mean-seeking behavior is apparent when the set 𝒬 comprises Gaussians. The remark also introduces a useful definition for a form of Gaussian projection from the space of all probability measures on R 𝑑 onto the space of Gaussians on R 𝑑 . Remark 2.11. We write 𝒢 = 𝒢(R 𝑑 ) for the set of all probability density functions over R 𝑑 , including degenerate Gaussians with non-invertible covariance; in particular all Dirac measures are contained in 𝒢. Define G : 𝒫 → 𝒢 by G𝜋 ∈ arg min 𝜇∈𝒢 D KL (𝜋‖𝜇). It then follows that, in fact, G𝜋 = 𝒩 (𝑚 𝜋 , Σ 𝜋 ), where 𝑚 𝜋 and Σ 𝜋 are the mean and covariance of random variable 𝑢 ∼ 𝜋. ♢ Further discussion on mode-seeking versus mean-seeking variational inference and references to the literature may be found in the bibliography in Section 2.4. Evidence Lower Bound We now relate J(𝑞), defined by (2.2a), to an important concept in variational inference:, namely the evidence lower bound (ELBO) given in Definition 15.15: ELBO(̃︀ 𝜋, 𝑞) = E 𝑞 [log ̃︀ 𝜋(𝑢)] -E 𝑞 [log 𝑞(𝑢)]. The reason for the terminology evidence lower bound will be made clear through Remarks 2.13, 2.14. The evidence lower bound is sometimes also known as the negative variational free energy. The definition is often used in the situation where ̃︀ 𝜋(𝑢) is found from the joint distribution of random variable (𝑢, 𝑦), with 𝑦 frozen. This is the setting of the following proposition, which sheds first light on the significance of the ELBO functional. Recall prior measure 𝜌(𝑢), likelihood function l(𝑦|𝑢) given by equation (1.2) and posterior measure 𝜋 := 𝜋 𝑦 given by Theorem 1.3. Proof. This follows from noting that ELBO(̃︀ 𝜋, 𝑞) = E 𝑞 [log 𝜋(𝑢)] -E 𝑞 [log 𝑞(𝑢)] + E 𝑞 [log ̃︀ 𝜋(𝑢)] -E 𝑞 [log 𝜋(𝑢)] = E 𝑞 [log 𝜋(𝑢)] -E 𝑞 [log 𝑞(𝑢)] + E 𝑞 [log(𝑍)] = -D KL (𝑞‖𝜋) + log(𝑍). Since 𝑍 is independent of 𝑞, optimality requires that D KL (𝑞 opt ‖𝜋) = 0, and hence that 𝑞 opt = 𝜋. As a consequence ELBO(̃︀ 𝜋, 𝑞 opt ) = log(𝑍). Remark 2.13. In the preceding proposition ̃︀ 𝜋(𝑢) = P(𝑢, 𝑦) is the joint distribution and the normalizing constant 𝑍 = P(𝑦) is called the evidence. Thus the preceding proposition demonstrates the significance of the quantity ELBO(𝜋, 𝑞 opt ): it delivers the logarithm of the evidence, namely the log-probability that the observed data 𝑦 was produced by the statistical model for the joint random variable (𝑢, 𝑦) defined by equation ( 1.1) and Assumption 1.1. It is natural to ask what we can learn about the evidence if we perform variational inference over subset 𝒬 of 𝒫; this is the subject of the next remark. ♢ Remark 2.14. Variational inference is performed by solving the following optimization problem over 𝒬 ⊂ 𝒫 : 𝑞 ⋆ ∈ arg min 𝑞∈𝒬 J(𝑞). (2.10) We continue with the setting of Proposition 2.12 in which 𝜋 = 𝑍 -1 ̃︀ 𝜋. Recall the definitions of F(•) from (2.1) and note that it may be extended to unnormalized second argument so that we may write F(𝑞) = D KL (𝑞‖𝜋) = ∫︁ log (︂ 𝑞(𝑢) 𝜋(𝑢) )︂ 𝑞(𝑢) 𝑑𝑢 = ∫︁ log (︂ 𝑞(𝑢) ︀ 𝜋(𝑢) )︂ 𝑞(𝑢) 𝑑𝑢 + log(𝑍) = -ELBO(̃︀ 𝜋, 𝑞) + log(𝑍). But by (2.3) we have, for J(•) defined by (2.2), F(𝑞) = J(𝑞) + log(𝑍). Hence J(𝑞) = -ELBO(̃︀ 𝜋, 𝑞) and it follows that log(𝑍) = ELBO(̃︀ 𝜋, 𝑞 opt ) ≥ ELBO(̃︀ 𝜋, 𝑞 ⋆ ) = -J(𝑞 ⋆ ). Hence, after variational inference over 𝒬 is performed, we can estimate the normalizing constant, or the evidence, by 𝑍 ≥ exp (︀ -J(𝑞 ⋆ ) )︀ . The terminology evidence lower bound is now made clear. ♢ Bibliography The paper [451] provided impetus for variational Bayes, highlighting the fact that the posterior is found by minimizing J(•) given in (2.5) . Our presentation has focused on finding the posterior as the minimizer of the KL divergence, but there are other functionals that may be considered. For instance, paper [163] shows that the posterior is found by minimizing a functional of the same structural form as (2.3) based on the 𝜒 2 divergence (see Definition 11.34) , rather than KL divergence. Papers [279, 204] show how to perform variational inference using the family of Renyi-alpha divergences, which include the KL and 𝜒 2 divergences as well as the Hellinger metric. Whilst there are many divergences that could be used to define Bayes Theorem via optimization over the space of measures, the specific choice of forward KL divergence has a special place: amongst a wide class of divergences it is the unique choice for which the objective function does not require knowledge of the normalization constant 𝑍 [103] . Refer to [213] for a modern approach to variational inference, with applications to large collections of documents and topic models. For applications in graphical models see [436] . An extension to the mean-field model is to include interactions [232] . Natural extensions of Gaussians include the Gaussian mixtures considered in [258] .The derivation of the mean-field equations for Gaussians was first shown in [327] . Variational inference for sparsity-promoting Bayesian models [81] is considered in [7] ; see also [268] . Discussion of mean-seeking versus mode-seeking may be found in [378] ; see Sections 4.2 and 4.3 in particular. The property of G presented in Remark 2.11 is highlighted in [59] and proved in [378, Section 4.3] . In this chapter we have seen that the problem of minimizing KL divergence over its first argument can be reformulated in terms of maximizing the ELBO functional. From a conceptual viewpoint, it is often insightful to derive algorithms and estimation procedures using as guiding principle the minimization of KL divergence; however in practice it is common to work with the ELBO, whose definition only requires density known up to normalizing constant. The idea of minimizing KL divergence/maximizing ELBO is ubiquitous in machine learning and statistics, and it underpins many methods studied in these notes, including learning the prior to posterior map in Section 4.1, the variational formulations of data assimilation in Sections 7.1 and 7.2, variational autoencoders in Section 13.6, and the expectation-maximization algorithm in Section 15.4, used to learn priors for data assimilation in Section 8.2. The ELBO is a non-convex functional even for simple potentials; see the example in Appendix G of [259] . Due to this non-convexity, it is often challenging to develop guarantees for variational inference outside restricted settings on the distribution. However, there have been several recent papers studying statistical and algorithmic guarantees for variational inference. We refer to [258] for analysis using gradient flows and reference to the literature. Chapter 3 Learning The Prior In Bayesian inference the prior acts as a form of regularization. This can be seen explicitly for both the optimization and probabilistic approaches to inverse problems. First, consider the optimization approach and MAP estimation as described in Subsection 1.2.1. In that setting the objective function is a sum of two terms: the loss function, which is a data mismatch term; and a regularizer, given by the negative logarithm of the prior 𝜌. Secondly, consider the probabilistic approach, and the variational form of Bayes Theorem as described in Subsection 2.1, and Remark 2.2, in particular. The posterior is shown to be the minimizer of an objective defined over probability densities. This objective is the sum of two terms: the data mismatch term, which is the expectation, with respect to the putative minimizer 𝑞, of the loss function; and the regularizer, which is the KL divergence between the putative minimizer 𝑞 and the prior 𝜌. Defining a prior, or regularizer, can be challenging. Often modellers make simple parametric choices, based on domain specific knowledge or intuition, within some class of priors or regularizers. For example if a Gaussian prior is adopted then the mean and covariance is selected using domain knowledge. However the abundance of data that is available in many fields suggests the possibility of employing purely data-driven priors for inversion, a major focus of this chapter. We start, in Section 3.1, by discussing Bayesian Hierarchical modelling, learning parameters of the prior simply employing the Data Assumption 1.2 used to define the inverse problem. Then, in Section 3.2, we introduce Data Assumption 3.2, when multiple samples from the prior are available. We discuss learning the prior measure from data {𝑢 (𝑛) } 𝑁 𝑛=1 drawn from the prior, relating this problem to the unsupervised learning task studied in Chapter 13. In Section 3.3 we consider this setting, representing the prior via a pushforward; such an approach arises when the prior is learned from data as a transport or using autoencoders. Section 3.4 contains a theoretical analysis of the effect of errors in the prior on errors in the posterior. Section 3.5 contains concluding bibliographic remarks. When learning parameterized priors and regularizers, the optimal parameter typically depends on the observed data 𝑦. In Chapter 5 we will study how to learn this dependence from data {(𝑢 (𝑛) , 𝑦 (𝑛) } 𝑁 𝑛=1 drawn from the joint distribution of (𝑢, 𝑦) ∈ R 𝑑 × R 𝑘 determined by the product of prior and likelihood. Bayesian Hierarchical Modelling Recall the Bayesian inverse problem of finding 𝑢 ∈ R 𝑑 from 𝑦 ∈ R 𝑘 when related by (1.1), so that 𝑦 = 𝐺(𝑢) + 𝜂. The Bayesian hierarchical methodology postulates a parameterized family of priors, 𝜌(𝑢; 𝜃) and proposes to learn the pair (𝑢, 𝜃) from the single piece of data 𝑦 given by Data Assumption 1.2. Thus, we learn both the unknown and parameters that define the prior, from 𝑦. To formulate this problem we view the parameterized family of priors 𝜌(𝑢; 𝜃) as conditional probability densities 𝜌(𝑢|𝜃), put a prior 𝜌(𝜃) on parameter 𝜃 and construct the prior 𝜌(𝑢, 𝜃) = 𝜌(𝑢|𝜃)𝜌(𝜃) on the pair (𝑢, 𝜃). Under appropriate generalization of the assumptions of Theorem 1.3 to the jointly varying pair (𝑢, 𝜃), the posterior distribution in this case is given by 𝜋(𝑢, 𝜃) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝜌(𝑢, 𝜃); (3.1) we have dropped the explicit dependence of the posterior on 𝑦 for notational convenience. and 𝜂 ∼ 𝒩 (0, 1). We place a prior on 𝑢|𝜃 with form 𝒩 (𝜃, 1); if we then assume 𝜃 ∼ 𝒩 (0, 1) then this specifies a jointly Gaussian prior 𝜌(𝑢, 𝜃) on the pair (𝑢, 𝜃). Under the specified prior 𝜌(𝑢, 𝜃), 𝑢 = 𝜃 + 𝜉 𝑢 and 𝜃 = 𝜉 𝜃 where 𝜉 𝑢 and 𝜉 𝜃 are independent unit Gaussians. Thus, under this prior, 𝑢 = 𝜉 𝑢 + 𝜉 𝜃 and hence has distribution given by 𝜌(𝑢) := 𝒩 (𝑢; 0, 2). Using these formulae we can identify the mean and covariance of the joint random variable: the mean is 0 and covariance is (︃ 2 1 1 1 )︃ . By (3.1) we obtain 𝜋(𝑢, 𝜃) ∝ 𝜈 (︀ 𝑦 -𝑢 )︀ 𝜌(𝑢, 𝜃), where 𝜈 is the density associated with the standard unit Gaussian on R. By integrating out 𝜃 we arrive at the posterior for the original non-hierarchical problem 𝜋(𝑢) ∝ 𝜈 (︀ 𝑦 -𝑢 )︀ 𝜌(𝑢). In general it is not possible to integrate out the parameter 𝜃; the specific structure on 𝜌(𝑢, 𝜃) allows it in this case. ♢ Hierarchical Bayesian learning works with the same Data Assumption 1.2 that we employ to define the standard Bayesian inverse problem defined by (1.1) . Each different data instance for 𝑦 would give rise to a different choice of the parameter 𝜃, or distribution over the parameter 𝜃, entering the prior for 𝑢. Hence we obtain a different prior on 𝑢 for each data instance 𝑦. In the next subsection, and indeed in the remainder of this chapter, we work in a different scenario where the form of the prior is learned from samples of that prior; we hence have a different data assumption. Learning The Prior In this section we describe the idea of learning a prior on 𝑢 from data derived by sampling 𝑢, from the prior, many times. We make the following data assumption to enable this: Data Assumption 3.2. We are given samples {𝑢 (𝑛) } 𝑁 𝑛=1 assumed to be drawn i.i.d. from prior measure 𝜌 on 𝑢 which is unknown. This immediately yields an empirical approximation for 𝜌 : 𝜌 𝑁 (𝑢) = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿(𝑢 -𝑢 (𝑛) ). (3.2) Once a smooth prior is learned from this empirical approximation, it can be re-used for different inverse problems for 𝑢, or for different instances of the realization 𝑦 used in that inverse problem. To clearly understand the methodology of this section it is important to distinguish between the piece of data, 𝑦, for which we wish to solve the inverse problem defined by (1.1), and the training data {𝑢 (𝑛) } 𝑁 𝑛=1 which we assume are available to us, and which we use to learn about the prior. Now compare Data Assumption 3.2 to Data Assumption 13.1 in Chapter 13. The latter assumption arises in unsupervised learning; notice that, taking ϒ := 𝜌 shows that the two assumptions are identical. Thus, when the prior is only given to us through Data Assumption 3.2, we may seek to approximate it using the generative modelling techniques of Chapter 13. Representing The Prior Via A Pushforward Under the assumptions of Theorem 1.3, the posterior distribution is given by (1.3): 𝜋(𝑢) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝜌(𝑢); (3.3) we have again dropped the explicit dependence of the posterior on 𝑦 for notational convenience. We assume that the prior 𝜌 is only known through an empirical approximation 𝜌 𝑁 ≈ 𝜌, derived from Data Assumption 3.2. It is then possible to use ideas from Chapter 13, such as autoencoders and transport maps, to find map 𝑔 that (approximately) pushes forward a given density 𝜁 on latent space R 𝑑𝑧 into the prior. That is, 𝜌 ≈ 𝑔 ♯ 𝜁. In the remainder of this section we assume that 𝜌 = 𝑔 ♯ 𝜁, noting that the effects of approximating this identity will be discussed in the next section. Then we may define 𝜋 𝑔 (𝑧) = 1 𝑍 𝜈 (︁ 𝑦 -𝐺 (︀ 𝑔(𝑧) )︀ )︁ 𝜁(𝑧). (3.4) If map 𝑔 is given by an autoencoder, as in Section 13.5, then typically the latent space 𝑑 𝑧 is smaller than R 𝑑 ; then (3.4) is the Bayesian formulation of the inverse problem of finding 𝑧 from 𝑦 where 𝑦 = 𝐺 (︀ 𝑔(𝑧) )︀ + 𝜂. In the case of transport, as described in Section 13.2, 𝑑 𝑧 = 𝑑 and we have the following: Proposition 3.3. Let 𝑑 𝑧 = 𝑑 and assume that 𝜌 = 𝑔 ♯ 𝜁 and that 𝑔 is invertible. Then 𝜋 𝑔 given by (3.4) and the desired posterior 𝜋 are related by the identity 𝜋 = 𝑔 ♯ 𝜋 𝑔 . Remark 3.4. The significance of Proposition 3.3 is that it implies that we can solve the Bayesian inverse problem (3.4), posed in the latent space, and push forward under 𝑔 to find solutions of the original Bayesian inverse problem defined by (3.3). For example if we generate samples under 𝜋 𝑔 in the latent space, then application of 𝑔 to those samples will generate samples under 𝜋 in the original space. Hence, if pair (𝑔, 𝜁) are known, then the original Bayesian inverse problem for 𝜋 on R 𝑑 may be converted to one for 𝜋 𝑔 on R 𝑑𝑧 ; pushforward of 𝜋 𝑔 under 𝑔 yields solution to the original problem. A similar expression to (3.4) may be established even if 𝑑 𝑧 < 𝑑, but it no longer follows that 𝜋 = 𝑔 ♯ 𝜋 𝑔 because 𝑔 is not invertible. ♢ Proof of Proposition 3.3. From Lemma 11.40 with 𝑢 = 𝑔(𝑧), and (11.20) in particular, 𝑔 ♯ 𝜋 𝑔 (𝑢) = (︀ 𝜋 𝑔 ∘ 𝑔 -1 )︀ (𝑢)det𝐷 (︀ 𝑔 -1 )︀ (𝑢) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀(︀ 𝜁 ∘ 𝑔 -1 )︀ (𝑢)det𝐷 (︀ 𝑔 -1 )︀ (𝑢) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝑔 ♯ 𝜁(𝑢) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝜌(𝑢), = 𝜋(𝑢) as required. Perturbations To The Prior Remark 3.4 shows that, if we learn an exact transport map 𝑔 satisfying 𝜌 = 𝑔 ♯ 𝜁, then we have access to the true posterior with prior 𝜌. In practice, however, we learn 𝑔 from 𝜌 𝑁 ≈ 𝜌 and so we cannot hope to recover an exact transport map; the issue of lack of exactness is further compounded by only learning the transport map over a parametric family of densities and by not iterating the optimization solver to convergence. In Subsections 3.4.1 and 3.4.2 we prove two theorems which address this effect. In Theorem 3.6 we assume that the approximation of the exact transport map leads to an approximate smooth prior, in the space of 𝑢, and look at the effect on the posterior of the approximation error between this prior and the true prior. In Theorem 3.7 we study a related problem, namely the effect on the posterior of replacing the prior by an empirical approximation. Smooth Approximation Of The Prior Consider the inverse problem arising from use of a smooth approximate prior 𝜌 ′ ≈ 𝜌. This gives rise to an approximate posterior, 𝜋 ′ : 𝜋 ′ (𝑢) = 1 𝑍 ′ 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝜌 ′ (𝑢). (3.5) It is thus important to know that a small change in the prior leads to a small change in the posterior. We prove this in the following theorem which exhibits conditions under which the prior to posterior map is Lipschitz in the Hellinger metric. In the proof of the theorem, and conditions that precede it, we let l(𝑢) = 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ , suppressing dependence on 𝑦. D H (𝜋, 𝜋 ′ ) ≤ (︃ 2𝐾 ( √ 𝑍 + √ 𝑍 ′ ) √ 𝑍 ′ + √ 𝐾 √ 𝑍 ′ )︃ D H (𝜌, 𝜌 ′ ). (3.6) Proof. In the proof all 𝐿 2 norms are over domain 𝐷 and all integrals are restricted to domain 𝐷. With this notation we have that D H (𝜋, 𝜋 ′ ) = 1 √ 2 ⃦ ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 - √︃ l𝜌 ′ 𝑍 ′ ⃦ ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 ≤ 1 √ 2 ⃦ ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 - √︃ l𝜌 𝑍 ′ ⃦ ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 + 1 √ 2 ⃦ ⃦ ⃦ ⃦ ⃦ ⃦ √︃ l𝜌 𝑍 ′ - √︃ l𝜌 ′ 𝑍 ′ ⃦ ⃦ ⃦ ⃦ ⃦ ⃦ 𝐿 2 = 1 √ 2 ⃒ ⃒ ⃒ ⃒ 1 √ 𝑍 - 1 √ 𝑍 ′ ⃒ ⃒ ⃒ ⃒ √ 𝑍 + 1 √ 2 √ 𝑍 ′ ‖ √︀ l𝜌 - √︀ l𝜌 ′ ‖ 𝐿 2 . The first term can be written as ⃒ ⃒ ⃒ ⃒ 1 √ 𝑍 - 1 √ 𝑍 ′ ⃒ ⃒ ⃒ ⃒ √ 𝑍 = |𝑍 -𝑍 ′ | ( √ 𝑍 + √ 𝑍 ′ ) √ 𝑍 ′ . The difference between the normalization constants can be bounded, using Lemma 11.2, by |𝑍 -𝑍 ′ | ≤ ∫︁ |l(𝑢)𝜌(𝑢) -l(𝑢)𝜌 ′ (𝑢)| 𝑑𝑢 ≤ 𝐾 ∫︁ |𝜌(𝑢) -𝜌 ′ (𝑢)| 𝑑𝑢 = 2𝐾D TV (𝜌, 𝜌 ′ ) ≤ 2𝐾 √ 2D H (𝜌, 𝜌 ′ ). For the second term, we have ‖ √︀ l𝜌 - √︀ l𝜌 ′ ‖ 𝐿 2 ≤ √ 𝐾‖ √ 𝜌 - √︀ 𝜌 ′ ‖ 𝐿 2 = √ 2𝐾D H (𝜌, 𝜌 ′ ). Collecting the bounds for the two terms, we obtain the result. Empirical Approximation Of The Prior Theorem 3.6 applies with priors 𝜌, 𝜌 ′ that have a probability density function. Here we consider the setting where the approximate prior is specified empirically by a collection random samples 𝑢 (𝑛) for 𝑛 = 1, . . . , 𝑁 from the true prior 𝜌. Then we have the random probability measure 𝜌 𝑁 (𝑢) = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿(𝑢 -𝑢 (𝑛) ). We sometimes refer to 𝜌 as the population limit of 𝜌 𝑁 . With this finite sample approximation of the population limit of the prior, we obtain the approximate posterior 𝜋 𝑁 (𝑢) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝜌 𝑁 (𝑢). (3.7) We wish to compare the population limit 𝜋 with 𝜋 𝑁 . In this setting, it is natural to use the following metric from Subsection 11.1.6: 𝑑(𝜋, 𝜋 𝑁 ) = sup |𝑓 |∞≤1 ⃒ ⃒ ⃒E [︁ (︀ 𝜋(𝑓 ) -𝜋 ′ (𝑓 ) )︀ 2 ]︁⃒ ⃒ ⃒ 1/2 , ( 3.8) where 𝜋(𝑓 ) := E 𝑢∼𝜋 [𝑓 (𝑢)] and 𝜋 𝑁 (𝑓 ) := E 𝑢∼𝜋 𝑁 [𝑓 (𝑢)]. We note, because it will be useful in what follows, that sup |𝑓 |∞≤𝐹 ⃒ ⃒ ⃒ ⃒ E [︂ (︁ 𝜋(𝑓 ) -𝜋 𝑁 (𝑓 ) )︁ 2 ]︂⃒ ⃒ ⃒ ⃒ 1/2 ≤ 𝐹 𝑑(𝜋, 𝜋 𝑁 ). (3.9) Recall that this reduces to the total variation metric for non-random measures as discussed in Remark 11.29; and that, furthermore, the square root of the total variation metric upper bounds the Hellinger metric, by Lemma 11.2. The following proposition uses this metric on random probability measures to quantify the distance between the true and approximate posterior when one prior is specified using samples: Theorem 3.7. Let Assumption 3.5 hold and let 𝜋, 𝜋 𝑁 be the posteriors given by (3.3) and (3.7) respectively. Then, 𝑑(𝜋, 𝜋 𝑁 ) ≤ 2𝐾 𝑍 𝑑(𝜌, 𝜌 𝑁 ). (3.10) Proof. For the true and approximate posterior, we can write the integrals required to estimate 𝑑(𝜋, 𝜋 𝑁 ), with distance given in (3.8), by 𝜋(𝑓 ) = 𝜌(l𝑓 ) 𝜌(l) , 𝜋 𝑁 (𝑓 ) = 𝜌 𝑁 (l𝑓 ) 𝜌 𝑁 (l) . Then we have 𝜋(𝑓 ) -𝜋 𝑁 (𝑓 ) = 𝜌(l𝑓 ) 𝜌(l) - 𝜌 𝑁 (l𝑓 ) 𝜌 𝑁 (𝑓 ) = 𝜌(l𝑓 ) -𝜌 𝑁 (l𝑓 ) 𝜌(l) - 𝜌 𝑁 (l𝑓 ) (︀ 𝜌(l) -𝜌 𝑁 (l) )︀ 𝜌(l)𝜌 𝑁 (l) = 𝜌(l𝑓 ) -𝜌 𝑁 (l𝑓 ) 𝜌(l) - 𝜋 𝑁 (𝑓 ) (︀ 𝜌(l) -𝜌 𝑁 (l) )︀ 𝜌(l) . Using the basic inequality (𝑎 - 𝑏) 2 ≤ 2(𝑎 2 + 𝑏 2 ), (3.9), that |𝜋 𝑁 (𝑓 )| 2 ≤ 1 for all |𝑓 | ∞ ≤ 1 and that |l| ∞ |, |l𝑓 | ∞ ≤ 𝐾 ⃒ ⃒ ⃒E [︁ (︀ 𝜋(𝑓 ) -𝜋 ′ (𝑓 ) )︀ 2 ]︁⃒ ⃒ ⃒ ≤ 2 𝜌(l) 2 (︃ E [︁ (︀ 𝜌(l𝑓 ) -𝜌 𝑁 (l𝑓 ) )︀ 2 ]︁ + E [︁ (︀ 𝜋 𝑁 (𝑓 ) )︀ 2 (︀ 𝜌(l) -𝜌 𝑁 (l) )︀ 2 ]︁ )︃ ≤ 4𝐾 2 𝜌(l) 2 𝑑(𝜌, 𝜌 𝑁 ) 2 . Taking the supremum over test functions on the left-hand side and using that 𝜌(l) = 𝑍 gives us the desired result. We note that so far we did not specify how the samples 𝑢 (𝑛) defining the perturbed prior 𝜌 𝑁 were generated. In fact, the result above holds for any empirical measure. If 𝑢 (𝑛) ∼ 𝜋 are sampled i.i.d., however, 𝜌 𝑁 is a Monte Carlo approximation of 𝜌. Moreover, we can appeal to convergence results for Monte Carlo to show the convergence rate of 𝜋 𝑁 to the true posterior 𝜋. Corollary 3.8. Let 𝜌 𝑁 be a Monte Carlo estimator of 𝜌 in Theorem 3.7. Then, we have 𝑑(𝜋, 𝜋 𝑁 ) ≤ 2𝐾 𝑍 1 √ 𝑁 . Proof. By Theorem 11.30, 𝑑(𝜌, 𝜌 𝑁 ) 2 ≤ 1 𝑁 . Using this in the right-hand side of (3.10) gives the desired result. Bibliography The idea of learning prior probabilistic models from data is overviewed in [28] . An application is described in [333] where a generative adversarial network (GAN) is used to determine a mapping from a Gaussian to the space of prior data samples. The paper [160] also implicitly learns a prior, through simultaneous consideration of multiple inverse problems; but it also learns the posterior distribution for each of these inverse problems at the same time, linking to the two subsequent Chapters 4 and 5. Many methods of this type reduce the dimension of the unknown parameter space, determining a latent space of low effective dimension. A different approach to learning this mapping is to use invertible maps [246] and the work on normalizing flows [409, 393] ; see [28] for application of invertible maps to prior construction for inversion. This idea can be combined with variational inference to solve sampling problems in general, and inverse problems in particular [363, 405, 156, 326] . The use of triangular transport maps for Bayesian inference was introduced in [137] . The idea of learning regularizers from data, to define objective functions for the optimization approach to inversion, is overviewed in [25] and [50] . The paper [392] provides a framework for the subject which employs structured factorizations of data matrices to learn semidefinite regularizers. The paper [287] develops an adversarial approach to the problem. Inspired by the Neumann series, the work [172] proposes an end-to-end learning approach that utilizes data-driven nonlinear regularizers. A collection of stability results relating the posterior error to perturbations in the prior measure with respect to various metrics and divergences (e.g., KL divergence, 𝜒 2 divergence and Wasserstein-1 metric) can be found in [161, 395] . We refer to Chapter 11 for background on these and other distances and divergences. While the stability results presented in this chapter are with respect to distance between two priors, the results can also be translated to stability with respect to the distance between the transport maps that define the prior. A set of these stability results for various metrics and divergences can be found in [40] . Chapter 4 Transport To The Posterior In this chapter we formulate various transport approaches to determining the posterior distribution, building on the material relating to transport in Chapter 13. In Section 4.1 we discuss mapping the prior to the posterior, setting it up as an optimization problem over a parametrized class of transports. In Section 4.2 we obtain insights on this problem by considering a non-parametric formulation. Section 4.3 sets this work in the context of variational inference. In Section 4.4 we generalize the transport approach: we consider maps from general latent spaces, not restricted to the prior, to the posterior. Section 4.5 is devoted to using transport to enhance MCMC. Bibliographic remarks are contained in Section 4.6. Learning The Prior To Posterior Map Consider Bayes Theorem 1.3 written as map relating prior 𝜌 to posterior 𝜋 via the relation 𝜋(𝑢) = 1 𝑍 l(𝑢)𝜌(𝑢), (4.1a ) 𝑍 = E 𝑢∼𝜌 [︀ l(𝑢) ]︀ . ( 4.1b) We drop explicit dependence on 𝑦 since we do not exploit it in this chapter. Equation (4.1) defines a nonlinear map 𝜌 ↦ → 𝜋 on the space of probability density functions. 1 We now ask whether we can realize this map via an invertible transport map 𝑇 on R 𝑑 with the property 𝜋 = 𝑇 ♯ 𝜌; equivalently, since 𝑇 is invertible, 𝜌 = (𝑇 -1 ) ♯ 𝜋. Specifically, following the approach in Section 13.2 as outlined in Remark 13.7, we seek to find 𝜃 ∈ Θ ⊆ R 𝑝 to minimize F(𝜃) = D KL (︀ 𝜌‖𝑇 -1 (•; 𝜃) ♯ 𝜋 )︀ . ( 4 .2) From (13.5), we deduce that minimizing F(•) over a class of diffeomorphisms (continuously differentiable, invertible maps) 𝑇 is equivalent to minimizing -E 𝑢∼𝜌 [︁ log 𝜋 ∘ 𝑇 (𝑢; 𝜃) + log det𝐷 𝑢 𝑇 (𝑢; 𝜃) ]︁ , Using the expression (4.1) for the posterior in terms of the prior, and noting that log 𝑍 is an additive constant with respect to 𝜃, we may instead define the objective function J(𝜃) = -E 𝑢∼𝜌 [︁ log 𝜌 ∘ 𝑇 (𝑢; 𝜃) + log l ∘ 𝑇 (𝑢; 𝜃) + log det𝐷 𝑢 𝑇 (𝑢; 𝜃) ]︁ , (4.3) Finally, we set 𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃), and employ transport map 𝑇 ⋆ = 𝑇 (•; 𝜃 ⋆ ). We observe that it is not necessary to know the normalization constant for the posterior in order to apply the methodology of this section. In practice the expectation over the prior must be evaluated empirically and we work under Data Assumption 3.2. Remark 4.1. This methodology is not specific to inverse problems; rather it applies to the sampling of any measure 𝜋 which is defined via its density l(𝑢) with respect to a measure 𝜌, and is known up to a normalization constant. If, however, the density l(𝑢) is defined by a Bayesian inverse problem, then Data Assumption 1.2 is needed to provide data point 𝑦 entering the likelihood. ♢ Remark 4.2. We have sought the posterior as transport from the prior. This is natural in applications where the prior distribution is easy to sample from. A key advantage of seeking transports, rather than directly learning an approximation to 𝜋 as in Chapter 2, is that it provides an easy approach to sample the posterior after identifying 𝑇 ⋆ . In particular, if 𝑧 (𝑛) ∼ 𝜌 are i.i.d. reference samples, then 𝑇 ⋆ (︀ 𝑧 (𝑛) )︀ ∼ 𝜋 are i.i.d. posterior samples. In this setting where 𝜌 corresponds to the prior density, we say that 𝑇 is a prior-to-posterior transport map. ♢ Insight From Nonparametric Formulation The terms in objective (4.3) are competitive. To understand this, first consider minimizing an objective function defined by the first two terms alone. This is achieved by setting 𝑇 (𝑢; 𝜃) = 𝑢 MAP , given by (1.9), for all 𝑢 ∈ R 𝑑 : thus the resulting posterior approximation, found by pushing forward the prior under 𝑇 (𝑢; 𝜃), is simply a Dirac measure at the MAP point of the posterior density. On the other hand, including the third and last term ensures that the determinant of the Jacobian of the map is strictly positive: det 𝐷 𝑢 𝑇 (𝑢; 𝜃) > 0 for all 𝑢 ∈ R 𝑑 . For a continuously differentiable map with positive Jacobian determinant at each input 𝑢 ∈ R 𝑑 , the inverse function theorem then establishes that the map is locally invertible at 𝑢. This rules out a map, such as 𝑇 (𝑢; 𝜃) = 𝑢 MAP , which is not invertible. The optimal solution determined by (4.3) thus defines an invertible map whose pushforward takes the prior to the posterior; it may be thought of as a regularization of the map that simply takes the prior to a Dirac measure centered at the MAP point. The following gives insight into the learning problem for the transport map 𝑇 if the posterior density is log-concave, by considering minimization of a functional over an infinite class of transports. First we define convex optimization problems. To understand the following definition, recall the definitions for a convex function (Definition 15.1) and a convex set (Definition 15.2). Definition 4.3. Let J : 𝒯 → R be a functional defined on a subset 𝒯 of a vector space. The optimization problem min 𝑇 ∈𝒯 J(𝑇 ) is convex if 𝒯 is a convex set and if J is convex: J(𝜆𝑇 1 +(1-𝜆)𝑇 2 ) ≤ 𝜆J(𝑇 1 )+(1-𝜆)J(𝑇 2 ) for all 𝑇 1 , 𝑇 2 ∈ 𝒯 and 𝜆 ∈ [0, 1]. ♢ Theorem 4.4. Assume that the posterior density 𝜋(𝑢) ∝ 𝜌(𝑢)l(𝑢) is (strictly) log-concave. Define the space of diffeomorphic maps 𝒯 = {︁ 𝑇 ∈ 𝐶 1 (R 𝑑 ; R 𝑑 ), det 𝐷 𝑢 𝑇 (𝑢) > 0 for all 𝑢 ∈ R 𝑑 }︁ . Then the optimization problem arg min 𝑇 ∈𝒯 -E 𝑢∼𝜌 [︁ log 𝜌 ∘ 𝑇 (𝑢) + log l ∘ 𝑇 (𝑢) + log det 𝐷 𝑢 𝑇 (𝑢) ]︁ is (strictly) convex. Proof. If 𝜋(𝑢) is log-concave, then -log (︀ 𝜌(𝑢)l(𝑢) )︀ = -log 𝜌(𝑢) -log l(𝑢) is convex. By the convexity of 𝑢 ↦ → -log 𝜌(𝑢) -log l(𝑢) and 𝑢 ↦ → -log det(𝑢) we have that 𝑇 ↦ → E 𝑢∼𝜌 [︀ -log 𝜌 (︀ 𝑇 (𝑢) )︀ -log l (︀ 𝑇 (𝑢) )︀ -log det 𝐷 𝑢 𝑇 (𝑢) ]︀ is convex. Strict convexity follows analogously from the strictly log-concavity of 𝜋(𝑢). Given that 𝑡𝑇 1 + (1 -𝑡)𝑇 2 ∈ 𝒯 for all 𝑇 1 , 𝑇 2 ∈ 𝒯 and 𝑡 ∈ [0, 1], then 𝒯 is a convex set. Then minimizing a convex functional over a convex set yields the result. Remark 4.5. This theorem demonstrates that, if the posterior is log-concave, then parameterization of 𝜃 ↦ → 𝑇 (𝑢; 𝜃) restricted to 𝒯 will lead to a desirable objective function for the purpose of optimization. In particular, the minimizer will be unique for a strictly convex problem, provided it exists. However, we note that if the map parameterization 𝜃 ↦ → 𝑇 (𝑢; 𝜃) is not convex for all 𝑢 ∈ R 𝑑 , then the optimization problem of minimizing (4.3) over the parameters 𝜃 is in general non-convex even if 𝜋 is log-concave. See the related discussion in Remark 12.7 concerning supervised learning. ♢ Remark 4.6. In general, it is not the case that the nonparametric optimization of J(•) over 𝒯 will have a minimizer; if 𝒯 is an open set the minimizing sequences may not have a limit in 𝒯 . In practice, compactness can be enforced on the optimization by adding a penalty, such as an RKHS norm, or by imposing a constraint on the parameters of the map to compactify the optimization problem. ♢ Connection To Variational Inference In this section we return to the parameterized optimization problem (4.2) and consider it from the perspective of variational inference. Motivated by the analysis in the previous section we consider the parametric class of maps 𝒯 := {︁ 𝑇 (•; 𝜃) ∈ 𝐶 1 (R 𝑑 ; R 𝑑 ), 𝜃 ∈ Θ ⃒ ⃒ ⃒det 𝐷 𝑢 𝑇 (𝑢; 𝜃) > 0 for all (𝑢, 𝜃) ∈ R 𝑑 × Θ }︁ . Theorem 4.7. Assume that 𝑇 (• ; 𝜃) ∈ 𝒯 for all 𝜃 ∈ Θ. Let 𝜃 ⋆ be the optimal parameter solving (4.4), with J(•) defined by (4.3). Furthermore, let 𝒬 := {𝑞 : 𝑞 = 𝑇 ♯ 𝜌, 𝑇 ∈ 𝒯 }. If 𝑞 ⋆ := 𝑇 (•; 𝜃 ⋆ ) ♯ 𝜌 then this density solves the variational inference problem 𝑞 ⋆ ∈ arg min 𝑞∈𝒬 D KL (𝑞‖𝜋). Proof. We first note that the class 𝒯 necessarily comprises only invertible maps. We showed above that solving the optimization problem (4.4), with J(•) defined by (4.3), is equivalent to minimizing F(•) from (4.2). But from Theorem 11.41 we have that, for invertible and differentiable transformations, D KL (𝜌‖𝑇 -1 (•; 𝜃) ♯ 𝜋) = D KL (𝑇 (•; 𝜃) ♯ 𝜌‖𝜋). Thus, arg min 𝜃∈Θ J(𝜃) = arg min 𝜃∈Θ D KL (𝑇 (•; 𝜃) ♯ 𝜌‖𝜋). Lastly, 𝑇 (•; 𝜃 ⋆ ) : R 𝑑 → R 𝑑 is a differentiable map, and its derivative has positive determinant; it is therefore an invertible map. Thus 𝑇 (𝑢; 𝜃 ⋆ ) ♯ 𝜌 ∈ 𝒬. Example 4.8. Consider the setting where the prior is a standard Gaussian 𝒩 (0, 𝐼). Now assume that 𝑇 is affine: 𝑇 (𝑢) = 𝐴𝑢 + 𝑚, for an invertible matrix 𝐷 𝑢 𝑇 (𝑢) = 𝐴 ∈ R 𝑑×𝑑 and vector 𝑚 ∈ R 𝑑 . An affine transformation of a standard Gaussian yields a Gaussian random variable. In fact, the class of approximate posterior distributions then consists of multivariate Gaussians of the form 𝒬 = {︁ 𝑞 = 𝒩 (𝑚, 𝐴𝐴 ⊤ ), 𝑚 ∈ R 𝑑 , 𝐴 ∈ R 𝑑×𝑑 invertible }︁ . Hence, solving the optimization problem for J(•) is a subclass of the general Gaussian variational inference problem considered in Subsection 2.2.2. Here we are parameterizing the Gaussian covariance through a square root; for example it is possible to parameterize the covariance using its Cholesky factorization by considering triangular matrices 𝐴 ∈ R 𝑑×𝑑 in 𝒬 with positive diagonal entries. ♢ Example 4.9. Let 𝜌 be a probability density function in the mean-field family in Definition 2.3. Furthermore let 𝑇 be a diagonal map of the form 𝑇 (𝑢) = (︀ 𝑇 1 (𝑢 1 ), . . . , 𝑇 𝑑 (𝑢 𝑑 ) )︀ where 𝑇 𝑖 : R → R are univariate differentiable and invertible transformations: that is 𝐷 𝑢 𝑖 𝑇 𝑖 (𝑢 𝑖 ) > 0 for all 𝑢 𝑖 ∈ R. Then, the class of approximate posterior distributions corresponds to 𝒬 = {︃ 𝑞(𝑢) = ∏︁ 𝑖 𝑞 𝑖 (𝑢 𝑖 ), 𝑞 𝑖 = (𝑇 𝑖 ) ♯ 𝜌 𝑖 }︃ . ♢ Learning Other Maps To The Posterior So far we looked for a map pushing forward the prior to the posterior. This is advantageous when we would like the posterior to inherit certain properties of the prior. For example, if the prior and posterior have the same tail behavior, then the transport map 𝑇 only needs to depart from an identity map in the bulk of the distribution. Similarly, if the posterior and prior only differ on a low-dimensional subspace of the parameters 𝑢 ∈ R 𝑑 , then the map 𝑇 can be represented using a ridge function that is constant for inputs orthogonal to the subspace of interest. Pushing forward from the prior can be advantageous because priors that are easily sampled are often deployed in practice. However in some settings it may be computationally expedient to seek 𝜋 which is the pushforward of a different, easy-to-sample, reference density 𝜚(𝑢) on R 𝑑 , (such as a multivariate Gaussian) rather than the prior 𝜌(𝑢) (when it is not Gaussian). Choosing a different reference measure may thus be convenient when it is challenging to sample from the prior density or when the posterior is very different from the prior -for example having different tail behavior. To consider this setting we replace (4.2) by F(𝜃) = D KL (︀ 𝜚‖𝑇 -1 (•; 𝜃) ♯ 𝜋 )︀ . ( 4.5) Similar analysis to that leading to (4.3) in the case of mapping the prior to the posterior shows that we may learn the parameters 𝜃 ∈ Θ solving (4.5) by minimizing Letting 𝜃 ⋆ denote the optimal parameters, the resulting posterior approximation is given by 𝜋 ≈ 𝑇 (•; 𝜃 ⋆ ) ♯ 𝜚. Again, as in Subsection 4.1, the expectation over the prior must be evaluated empirically and, for this, we generalize Data Assumption 3.2 from 𝜌 to 𝜚. J(𝜃) = -E 𝑢∼𝜚 Remark 4.10. Unlike the variational inference problem in Chapter 2, we now have two degrees of freedom: the reference measure and the map. In principle, one may even parameterize the reference measure 𝜚(•, 𝜗) and learn its parameters 𝜗 simultaneously to learning the map 𝑇 (•; 𝜃). For example, we might parameterize 𝜚 using a Gaussian mixture with unknown means, covariances, and mixture weights. ♢ Transport-Assisted MCMC Given a reference distribution 𝜚 on R 𝑑 and parameter space Θ ⊆ R 𝑝 , the approach considered in the previous Section 4.4 yields the posterior approximation 𝜋 ≈ 𝑇 (• ; 𝜃 ⋆ ) ♯ 𝜚. This approximation can naturally be used to produce 𝑁 independent approximate posterior samples as follows: 1. Draw 𝑧 (𝑛) i.i.d. ∼ 𝜚 for 1 ≤ 𝑛 ≤ 𝑁. 2. Set 𝑢 (𝑛) := 𝑇 (𝑧 (𝑛) ; 𝜃 ⋆ ) for 1 ≤ 𝑛 ≤ 𝑁. The extent to which the samples {𝑢 (𝑛) } 𝑁 𝑛=1 are approximately distributed like the posterior 𝜋 is fully determined by the quality of the approximation 𝜋 ≈ 𝑇 (• ; 𝜃 ⋆ ) ♯ 𝜚. This section describes a MCMC transport-assisted approach for asymptotically-exact posterior sampling; we refer to Section 15.5 for background on MCMC, a general methodology to obtain approximate and correlated samples from a desired target distribution. To introduce the idea of transport-assisted MCMC recall first that since the map 𝑇 (• , 𝜃 ⋆ ) from Section 4.4 is invertible, we also have the approximation 𝜚 ≈ 𝑇 -1 (• ; 𝜃 ⋆ ) ♯ 𝜋. The idea behind transport-assisted MCMC is that, since 𝜚 is straightforward to sample from, then its approximation 𝑇 -1 (• ; 𝜃 ⋆ ) ♯ 𝜋 will be also be straightforward to sample from using MCMC. For example, if 𝜚 is a standard multivariate Gaussian, and if 𝑇 -1 (• ; 𝜃 ⋆ ) ♯ 𝜋 approximates this measure well, then sampling it using MCMC will be straightforward. Moreover, the reference distribution 𝜚 can be used to design efficient proposal distributions for the MCMC algorithm. Applying this approach leads to the methodology in Algorithm 4.1 to generate approximate and correlated posterior samples. -1 (• ; 𝜃 ⋆ ) ♯ 𝜋 3: Run MCMC to obtain 𝑁 approximate samples {𝑧 (𝑛) } 𝑁 𝑛=1 from 𝑇 -1 (• ; 𝜃 ⋆ ) ♯ 𝜋. 4: Set 𝑢 (𝑛) := 𝑇 (𝑧 (𝑛) ; 𝜃 ⋆ ) for 1 ≤ 𝑛 ≤ 𝑁. As compared to directly running MCMC with the target 𝜋, transport-assisted MCMC has the advantage that MCMC algorithms with target 𝑇 -1 (• ; 𝜃 ⋆ ) ♯ 𝜋 may converge faster. In other words, the transport map is used to precondition the posterior for MCMC sampling; for instance map 𝑇 -1 (• , 𝜃 ⋆ ) may pushforward multimodal posterior 𝜋 into an approximately Gaussian distribution that is easy to sample from. Remark 4.11. It is important to notice that the extent to which transport-assisted MCMC samples {𝑢 (𝑛) } 𝑁 𝑛=1 are approximately distributed like the posterior is determined by the convergence of the Markov chain {𝑢 (𝑛) } 𝑁 𝑛=1 to its limit distribution 𝑇 -1 (• ; 𝜃 ⋆ ) ♯ 𝜋, rather than by the quality of the approximation 𝜋 ≈ 𝑇 (• ; 𝜃 ⋆ ) ♯ 𝜚. In other words, provided that we run MCMC with target 𝑇 -1 (• ; 𝜃 ⋆ ) ♯ 𝜋 for long enough, transport-assisted MCMC will yield samples whose distribution is arbitrarily close to the posterior, even if the approximation 𝜋 ≈ 𝑇 (• ; 𝜃 ⋆ ) ♯ 𝜚 is poor. ♢ Bibliography The approach to seek prior-to-posterior maps was proposed in [137] . In particular that paper parameterized 𝑇 as a monotone-triangular map known as the Knothe-Rosenblatt rearrangement [426] ; the advantage of this choice of map is that the determinant Jacobian, required to define the minimization, is easy-to-evaluate. The approach was extended to compositions of triangular maps, commonly referred to as normalizing flows, in [363] . An approach to seek prior to posterior maps that are optimal with respect to minimizing a transportation distance (see Subsection 11.1.3 for a discussion on optimal transport) was proposed in [410] . An alternative to seeking the transport map directly is to construct the map incrementally by learning a map that is a small perturbation of the identity and that minimizes the distance between the current approximation and the actual posterior distribution. Stein-Variational Gradient Descent is one algorithm for finding such maps; it proceeds by seeking the map in a reproducing kernel Hilbert space in which the distance is measured by the KL divergence [282] . Another approach learns the dynamics of a (possibly stochastic) differential equation to define the transport from its flow map as in [243, 92, 12] . To handle certain classes of target distributions, it is sometimes desirable to consider objectives other than the KL divergence; examples include use of 𝛼-divergences [204] , Wasserstein distances (see Subsection 11.1.3) [14] or the development of tailored transport map approximations for heavy-tailed posteriors [280] . The development of transport-assisted MCMC sampling algorithms is an active research area, see for example [212, 337, 448, 297, 190, 22, 182, 157, 332, 157] . We refer to [70] for an overview of transport-based techniques for enhanced MCMC sampling. While some methods first learn the transport map and then apply MCMC in the reference space as described in Section 4.5, other approaches learn the transport map on the fly using the output of a Markov chain. These techniques are closely related to adaptive MCMC algorithms where the proposal kernel is suitably parameterized and learned online [29, 366] . Transports learned from samples of a target posterior distribution have also been used to learn proposal distributions for importance sampling [314] . Chapter 5 Data Dependence Of The Posterior This chapter presents methodologies for learning observation dependence in the regularizers, MAP estimators and transports that characterize the posterior distribution for Bayesian inverse problems. It is helpful to recall the notation, from Remark 1.4, for the probability density function 𝛾(𝑦, 𝑢) characterizing the joint distribution of random variable (𝑦, 𝑢) on observations and state, and the marginal 𝜅(𝑦) = ∫︀ 𝛾(𝑦, 𝑢) 𝑑𝑢 on the observations. The density may be factorized as 𝛾(𝑦, 𝑢) = l(𝑦|𝑢)𝜌(𝑢), formed from the product of the likelihood l(𝑦|𝑢), for the distribution on 𝑦|𝑢, and the prior density 𝜌(𝑢) on 𝑢. Our focus is on learning the dependence of the posterior distribution 𝜋 𝑦 (𝑢) on 𝑦; recall the posterior is given by 𝜋 𝑦 (𝑢) = 1 𝑍 l(𝑦|𝑢)𝜌(𝑢), (5.1a ) 𝑍 = E 𝑢∼𝜌 [︀ l(𝑦|𝑢) ]︀ . (5.1b) In this chapter, unlike the previous one, the dependence of the target measure on 𝑦 is important, and so we retain it explicitly in the notation 𝜋 𝑦 . Our goal here is to learn a map that explicitly depends on the observation 𝑦 and can be used to sample from, or otherwise characterize, the posterior distribution corresponding to any realization of 𝑦. The resulting model can then be repeatedly used for different realizations, rather than having to re-learn a different map for each 𝑦. This reduces the cost of multiple inference procedures to training a single model. Hence\" the approach is known as amortized inference or simply amortization. We start with two subsections devoted to the learning of regularizers. We have already discussed learning the prior in Chapter 3, but there it was simply assumed that data samples from the prior were available. Here we work with different assumptions, involving both samples from the prior and from the marginal on observation 𝑦. We do this first in the context of MAP estimation, in Section 5.1; we then study the general Bayesian inversion setting in Section 5.2. It is natural that regularizers are learned to work successfully for multiple data realizations, to avoid over-fitting to a specific data realization. It is for this reason that learning regularizers is included in this chapter. In Section 5.1 we assume that we have access to samples from the joint distribution 𝛾. In Section 5.2 we assume that we have independent samples from the two marginals of 𝛾, 𝜅 and 𝜌, and that the likelihood l(𝑦|𝑢) can be evaluated during training. In Section 5.3 we return to brief discussion of learning observation data dependence of MAP estimators, but do not specifically focus on learning a regularizer; furthermore we allow for randomness in the mapping from data to map estimator. In Section 5.4 we introduce transport approaches to amortized inference, returning to the setting were marginal samples from 𝛾 are available and where the likelihood l(𝑦|𝑢) can be evaluated during training. We introduce likelihood-free approaches in Section 5.5 -we assume that the likelihood cannot be evaluated, but it can be sampled. We again use transport maps and in that section we also discuss the consequence of having block-triangular pushforwards; and we discuss the learning of block-triangular transport maps, encoding data dependence in a natural way. Section 5.6 discusses the learning of likelihoods which is a natural extension of the ideas in the preceding subsection. We end with Section 5.7 containing bibliographic remarks. Learning Regularizers For MAP Estimation Again recall the inverse problem of finding 𝑢 ∈ R 𝑑 from 𝑦 ∈ R 𝑘 when related by (1.1): 𝑦 = 𝐺(𝑢) + 𝜂. We describe the idea of bilevel optimization to determine parameter 𝜃 ∈ Θ ⊆ R 𝑝 defining a regularizer in MAP estimation. The data assumption here differs from that employed in Section 3.2. We now assume (see Remark 1.4): Data Assumption 5.1. We are given samples {𝑦 (𝑛) , 𝑢 (𝑛) } 𝑁 𝑛=1 assumed to be drawn i.i.d. from the joint probability measure 𝛾 on (𝑦, 𝑢) ∈ R 𝑘 × R 𝑑 defined by equation (1.1) and Assumption 1.1. Such data is exactly what is required for the ideas of supervised learning from Chapter 12, where we use it to learn map from 𝑢 ∈ R 𝑑 to 𝑦 ∈ R; this is readily generalized to vector-valued output 𝑦. Note that, to solve the inverse problem, we would like to learn the inverse map from the data 𝑦 ∈ R 𝑘 to the state 𝑢 ∈ R 𝑑 . We could try and do this directly using supervised learning. However, for inverse problems in which 𝑘 < 𝑑, learning such a map may be difficult because of a lack of uniqueness; furthermore there is noise present in the 𝑦 (𝑛) which needs to be carefully accounted for. For these reasons it is arguably more informative to learn, from the supervised data defined by Data Assumption 5.1, how to regularize the inverse problem. This is the viewpoint we take here. To understand the proposed methodology clearly it is important to distinguish between the one piece of data 𝑦 for which we wish to solve the inverse problem defined by (1.1) and the training data pairs {𝑢 (𝑛) , 𝑦 (𝑛) } 𝑁 𝑛=1 (supervised data) which we assume are available to us, and which we use to learn the regularizer. This is different from Chapter 3 where we discussed learning the prior (and hence a regularizer for MAP estimation); in that chapter the training data from which we learn or define the prior was unsupervised data {𝑢 (𝑛) } 𝑁 𝑛=1 . We recall the posterior probability density function 𝜋 𝑦 (𝑢) on 𝑢|𝑦 from Theorem 1.3, but assume that prior 𝜌 depends on an unknown parameter 𝜃 ∈ Θ ⊆ R 𝑝 so that the posterior has form 𝜋 𝑦 (𝑢; 𝜃) = 1 𝑍(𝜃) 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝜌(𝑢; 𝜃). (5.2) We will determine 𝜃 by choosing it so that MAP estimators for the inverse problem defined by data 𝑦 (𝑛) best match the paired data point 𝑢 (𝑛) . To this end recall the loss function L and the now 𝜃-dependent regularizer R defined by L(𝑢; 𝑦) = -log 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ = -log l(𝑦|𝑢), R(𝑢; 𝜃) = -log 𝜌(𝑢; 𝜃), (5.3) leading to an objective function of the form J(𝑢; 𝑦, 𝜃) = L(𝑢; 𝑦) + R(𝑢; 𝜃). (5.4) We are explicitly including the 𝑦-dependence in the likelihood, since our goal here includes learning of this dependence. Note that we have also emphasized the parametric dependence of the loss and the objective function on 𝑦 ∈ R 𝑘 , something we did not do in Section 1.2. The MAP estimator can be viewed as a function, for each 𝜃, 𝑢 MAP (•; 𝜃) : R 𝑘 → R 𝑑 : 𝑢 MAP (𝑦; 𝜃) ∈ arg min 𝑢∈R 𝑑 J(𝑢; 𝑦, 𝜃). (For simplicity we assume that a unique point is determined by the minimization). We now introduce a loss function defined through a distance-like deterministic scoring rule D : R 𝑑 × R 𝑑 → R + , as defined in Definition 11.65; the canonical example is the squared Euclidean norm. We then define the choice of 𝜃 through the optimization problem 𝜃 ⋆ ∈ arg min 𝜃∈Θ E (𝑦,𝑢)∼𝛾 D (︀ 𝑢, 𝑢 MAP (𝑦; 𝜃) )︀ . This is referred to as bilevel optimization because of the optimization to find 𝑢 MAP which is used within the optimization to find 𝜃 ⋆ . This procedure corresponds to hyperparameter tuning of 𝜃 on the validation set {𝑦 (𝑛) , 𝑢 (𝑛) } 𝑁 𝑛=1 . In practice the determination of 𝜃 ⋆ is implemented using 𝛾 𝑁 , the approximation of 𝛾 found from empirical samples: 𝜃 ⋆ ∈ arg min 𝜃∈Θ 1 𝑁 𝑁 ∑︁ 𝑛=1 D (︀ 𝑢 (𝑛) , 𝑢 MAP (𝑦 (𝑛) ; 𝜃) )︀ . Learning Priors For Posterior Approximation In this section we generalize the setting from the previous one to the more general problem of learning parameters in the prior to best approximate the posterior. We work under the following data assumption, which differs from that in the previous section: we assume only marginal samples from 𝛾, but this is compensated for by employing evaluation of the likelihood. Data Assumption 5.2. We are able to evaluate the likelihood l(𝑦|𝑢) for any pair (𝑦, 𝑢) ∈ R 𝑘 × R 𝑑 . We are given data in the form of independent samples {𝑦 (𝑛) } 𝑁 𝑛=1 from the marginal distribution 𝜅 and independent samples {𝑢 (𝑛) } 𝑁 𝑛=1 from the prior distribution 𝜌. Remark 5.3. In the preceding Data Assumption 5.2 the set of independent samples {𝑦 (𝑛) } 𝑁 𝑛=1 and the set of independent samples {𝑢 (𝑛) } 𝑁 𝑛=1 do not need to be independent of one another, and the same number of each is not required. However, in practice they are often found by marginalizing the data set of samples {(𝑦 (𝑛) , 𝑢 (𝑛) )} 𝑁 𝑛=1 from the joint distribution 𝛾 and then it is natural to have the same number of samples of each. ♢ It is again helpful to recall the notation from Remark 1.4. The goal is to choose parameters 𝜃 in the prior so that the resulting posterior 𝜋 𝑦 (𝑢; 𝜃) in (5.2) is as close as possible to the true posterior 𝜋 𝑦 (𝑢) corresponding to the joint distribution 𝛾(𝑦, 𝑢) = P(𝑦|𝑢)𝜌(𝑢) underlying the data given in Data Assumption 5.2. We identify the parameters by minimizing the posterior error with respect to the KL divergence, in expectation over the data marginal on 𝑦 which, recall, has probability density function 𝜅: J(𝜃) := E 𝑦 [︁ D KL (︀ 𝜋 𝑦 ‖𝜋 𝑦 (•; 𝜃) )︀ ]︁ = ∫︁ [︂∫︁ log (︂ 𝜋 𝑦 (𝑢) 𝜋 𝑦 (𝑢; 𝜃) )︂ 𝜋 𝑦 (𝑢)𝑑𝑢 ]︂ 𝜅(𝑦) 𝑑𝑦. (5.5) Let 𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃). (5.6) The following result shows that the optimal parameters can be computed without needing to evaluate the true posterior density. Theorem 5.4. The optimal parameter 𝜃 ⋆ in (5.6) corresponds to the solution of the optimization problem 𝜃 ⋆ ∈ arg min 𝜃∈Θ (︂ E 𝑢∼𝜌 [-log 𝜌(𝑢; 𝜃)] + E 𝑦∼𝜅 [︂ log ∫︁ 𝜈 (︀ 𝑦 -𝐺(𝑢 ′ ) )︀ 𝜌(𝑢 ′ ; 𝜃) 𝑑𝑢 ′ ]︂)︂ . Proof. First note that 𝛾(𝑦, 𝑢) = 𝜋 𝑦 (𝑢)𝜅(𝑦). The expected KL divergence in (5.5) can be decomposed into two terms as E 𝑦 [︁ D KL (︀ 𝜋 𝑦 ‖𝜋 𝑦 𝜃 (•; 𝜃) )︀ ]︁ = ∫︁ 𝛾(𝑦, 𝑢) [︂ log (︂ l(𝑦|𝑢)𝜌(𝑢) 𝑍 𝑦 )︂ -log (︂ l(𝑦|𝑢)𝜌(𝑢; 𝜃) 𝑍 𝑦 (𝜃) )︂]︂ 𝑑𝑢𝑑𝑦 = ∫︁ 𝛾(𝑦, 𝑢) [︁ (︀ log 𝜌(𝑢) -log 𝑍 𝑦 )︀ - (︀ log 𝜌(𝑢; 𝜃) -log 𝑍 𝑦 (𝜃) )︀ ]︁ 𝑑𝑢𝑑𝑦. (5.7) Noticing that the first two terms are constant with respect to 𝜃, we only need to minimize with respect to the second term. That is, 𝜃 ⋆ ∈ arg min 𝜃∈Θ ∫︁ 𝛾(𝑦, 𝑢)[-log 𝜌(𝑢; 𝜃) + log 𝑍 𝑦 (𝜃)] 𝑑𝑢𝑑𝑦. (5.8) Using the form of the normalizing constant 𝑍 𝑦 (𝜃) = ∫︀ 𝜈 (︀ 𝑦 -𝐺(𝑢 ′ ) )︀ 𝜌(𝑢 ′ ; 𝜃) 𝑑𝑢 ′ , we arrive at the objective above, after noticing that in the first term integration over 𝑦 is redundant, and in the second integration over 𝑢 is redundant. Note that the objective function in Theorem 5.4 can be approximated empirically using Data Assumption 5.2. In particular this requires only marginal samples from 𝛾, and does not require knowledge of the posterior -evaluation of the likelihood is sufficient. Remark 5.5. The expected KL divergence in (5.7) can also be written as E 𝑦 [︁ D KL (︀ 𝜋 𝑦 ‖𝜋 𝑦 𝜃 (•; 𝜃) )︀ ]︁ = ∫︁ 𝜌(𝑢)[log 𝜌(𝑢) -log 𝜌(𝑢; 𝜃)] 𝑑𝑢 + ∫︁ 𝜅(𝑦) log (︃ 𝑍 𝑦 (𝜃) 𝑍 𝑦 )︃ 𝑑𝑦. = D KL (︀ 𝜌‖𝜌(•; 𝜃) )︀ -D KL (︀ 𝜅‖𝜅(•; 𝜃) )︀ , where in the last line we recognize that the normalizing constants 𝑍 𝑦 , 𝑍 𝑦 (𝜃) are equivalent to the marginal distributions of the observations 𝜅(𝑦) = ∫︀ 𝜈 (︀ 𝑦 -𝐺(𝑢 ′ ) )︀ 𝜌(𝑢 ′ ) 𝑑𝑢 ′ and 𝜅(𝑦; 𝜃) = ∫︀ 𝜈 (︀ 𝑦 -𝐺(𝑢 ′ ) )︀ 𝜌(𝑢 ′ ; 𝜃) 𝑑𝑢 ′ , respectively. Thus, the objective in (5.5) involves two competing terms, the first relating only to the prior and the second only to the likelihood. Note that when this objective is to be approximated by means of empirical samples from 𝜌 and 𝜅, undefined terms arise in both divergences; these terms are 𝜃-independent and can be removed as we did to arrive at (5.8). See Remark 13.4 for discussion of this point in the canonical context of maximum likelihood density estimation. ♢ Amortized MAP Estimation Here we return to the topic of MAP estimation, discussed in Section 5.1; however we no longer focus on learning prior dependence. We simply learn data dependence. However we allow for the possibility that the MAP estimator is a random function of the data, reflecting the potential for randomixed optimization algorithms to find the MAP estimator, or for the use of measures supported on multiple minimizers of the objective function defining the MAP estimator. Recall that Theorem 1.3 delivers the posterior 𝜋 𝑦 (𝑢) = 1 𝑍 𝜈 (︀ 𝑦 -𝐺(𝑢) )︀ 𝜌(𝑢). Recall that the MAP estimator of 𝑢 given data 𝑦 is defined as any point solving the maximization problem 𝑢 MAP ∈ arg max 𝑢∈R 𝑑 𝜋 𝑦 (𝑢). We make the following assumption: Data Assumption 5.6. We are given multiple pairs of observations and numerically computed MAP estimators {︀(︀ 𝑦 (𝑛) , 𝑢 (𝑛) MAP )︀}︀ 𝑁 𝑛=1 , drawn i.i.d. from probability measure 𝜇 on R 𝑘 × R 𝑑 . Remark 5.7. The measure 𝜇 may be constructed as the product of a measure on R 𝑘 and its pushforward under a deterministic algorithm to find the MAP estimator in R 𝑑 from data in R 𝑘 ; however, as the MAP estimation algorithm may be random (for example stochastic gradient descent, or a weighted empirical measure formed from multiple minimizers) we work with a general measure 𝜇. ♢ Given measure 𝜇 we may try and learn the dependence of the MAP estimator on observations. To this end we seek a parameterized function 𝑢 : R 𝑘 × Θ → R 𝑑 for Θ ⊆ R 𝑝 . We aim to find 𝜃 ⋆ ∈ Θ for which 𝑢(•; 𝜃 ⋆ ) : R 𝑘 → R 𝑑 approximates the mapping from observed data to the MAP estimator. Recall Definition 11.65 from Subsection 11.3.7 on distance-like deterministic scoring rules d. We may use this to compare the point estimator 𝑢(𝑦; 𝜃) at point 𝑦 to 𝑢 MAP obtained from that same 𝑦. We then define 𝜃 ⋆ by J(𝜃) = E (𝑦,𝑢 MAP )∼𝜇 [︀ d (︀ 𝑢(𝑦; 𝜃), 𝑢 MAP )︀]︀ , 𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃). (5.9) In practice we replace the expectation over 𝜇 in (5.9) with expectation with respect to 𝜇 𝑁 , the empirical measure defined by {︀(︀ 𝑦 (𝑛) , 𝑢 (𝑛) MAP )︀}︀ 𝑁 𝑛=1 . Likelihood-Based Inference In this section we initiate the development of transport maps for amortizing data dependence, in the setting of generalized variational inference. We again employ Data Assumption 5.2. With the goal of generalizing variational inference to learn the dependence of the optimal approximation on the data, we define the optimization problem 𝑞 ⋆ ∈ arg min 𝑞∈𝒬 E 𝑦∼𝜅 [︀ D KL (𝑞(•; 𝑦)‖𝜋 𝑦 ) ]︀ . As in standard variational inference from Chapter 2, 𝒬 may for instance be the meanfield family or a parameterized family of probability density functions on R 𝑑 . However, noting that now we wish to account for the dependence on data, we emphasize that the parameters arising in variational inference may themselves be parameterized to reflect dependence on the observations 𝑦 ∈ R 𝑘 . Example 5.8. This generalized variational inference problem can be implemented by seeking a mean-field approximation or a Gaussian approximation where the parameters are functions of the observations. For instance, 𝒬 may contain multivariate Gaussian densities 𝑞(•; 𝑦) = 𝒩 (︀ 𝑚(𝑦), Σ(𝑦) )︀ where the mean 𝑚(𝑦) and covariance Σ(𝑦) depend on the observation. This dependence on 𝑦 will itself need to be parameterized, for example as a linear function, or using neural networks, random features or Gaussian processes (see Chapter 12) constrained to ensure the covariance is a positive semi-definite matrix for all 𝑦 (see Example 4.8) . ♢ Generalizing the transport approach to variational inference described in Chapter 4, we can define the family of approximating distributions by using a transport map that pushes forward a simple reference distribution 𝜚 (for instance the prior 𝜌) and is also parameterized by 𝑦. In this case, we seek a transport map 𝑇 : R 𝑑 × R 𝑘 × R 𝑝 → R 𝑑 that depends on both input parameters and observations so that 𝑢 ↦ → 𝑇 (𝑢; 𝑦, 𝜃) defines an invertible transport map approximating the pushforward of 𝜚 to the posterior 𝜋 𝑦 (𝑢), for each choice of the observations 𝑦. We seek this transport as the minimizer of the following objective F(𝜃) = E 𝑦∼𝜅 [︀ D KL (︀ 𝑇 (•; 𝑦, 𝜃) ♯ 𝜚‖𝜋 𝑦 )︀]︀ (5.11a) = E 𝑦∼𝜅 [︀ D KL (︀ 𝜚‖𝑇 -1 (•; 𝑦, 𝜃) ♯ 𝜋 𝑦 )︀]︀ , ( 5.11b ) 𝜃 ⋆ ∈ arg min 𝜃∈Θ F(𝜃), (5.11c) where Θ ⊆ R 𝑝 . (In going from (5.11a) to (5.11b) we have used the invariance of the KL divergence under invertible transformations, Theorem 11.41). Following the approach in Chapter 4, we can rewrite this objective for the parameters via minimization of a loss that may be approximated empirically: 1 J(𝜃) = -E (𝑦, 𝑢)∼𝜅⊗𝜚 [︁ log 𝜚 (︀ 𝑇 (𝑢; 𝑦, 𝜃) )︀ + log l (︀ 𝑦|𝑇 (𝑢; 𝑦, 𝜃) )︀ + log det 𝐷 𝑢 𝑇 (𝑢; 𝑦, 𝜃) ]︁ . Likelihood-Free Inference A common setting arising in many inverse problems is one in which the likelihood l(𝑦|𝑢) is not analytically available or tractable to evaluate, but it is possible to sample from it. We work in this section under the following assumption, which should be compared with Assumption 5.2. Thus here we show how to construct posterior approximations without requiring evaluations of l(•|•), unlike in Section 5.4. Instead, we rely on sampling the joint distribution 𝛾(𝑦, 𝑢) to learn the dependence between states and observations. Sampling 𝛾 is feasible by sampling 𝑢 from the prior 𝜌(𝑢) and then sampling a synthetic observation 𝑦 from the likelihood model conditioned on 𝑢. Hence, this is known as likelihood-free or simulation-based inference. Note, in particular, that we are assuming that sampling from the likelihood is straightforward, even though evaluation of it is not. (5.12) When 𝑑 𝑧 ≫ 1, it will be difficult to evaluate the integration over 𝑧. But we can simulate from the joint distribution P(𝑦, 𝑢) = l(𝑦|𝑢)P(𝑢), with likelihood as in (5.12), as follows. First sample 𝑢 from the prior 𝜌; then sample latent state 𝑧 from P(𝑧|𝑢); and finally sample 𝑦 from P(𝑦|𝑢, 𝑧). The collection (𝑦, 𝑧, 𝑢) is a sample from the distribution P(𝑦, 𝑧, 𝑢). The subset of pairs (𝑦, 𝑢) are then samples from the distribution l(𝑦|𝑢)𝜌(𝑢). ♢ Example 5.11. A likelihood with high-dimensional latent variables arises when performing parameter inference in a hidden Markov model; noisy observations are given of the hidden state in a dynamical system. Hidden Markov models are the common framework for data assimilation, which is explored in Chapters 6-10. In data assimilation, the states 𝑣 𝑗 ∈ R 𝑑𝑣 and observations 𝑦 𝑗 ∈ R 𝑘 follow the dynamics and observation models: 𝑣 𝑗+1 = Ψ(𝑣 𝑗 ; 𝑢) + 𝜉 𝑗 , 𝜉 𝑗 ∼ 𝒩 (0, Σ), 𝑦 𝑗+1 = ℎ(𝑣 𝑗+1 ) + 𝜂 𝑗+1 , 𝜂 𝑗+1 ∼ 𝒩 (0, Γ), for 𝑗 = 0, . . . , 𝐽 -1 starting from a known initial condition 𝑣 0 ∈ R 𝑑𝑧 . Here, Ψ : R 𝑑𝑣 ×R 𝑑 → R 𝑑𝑣 is a forward model depending on the parameter 𝑢 and ℎ : R 𝑑𝑣 → R 𝑘 is an observation operator. The noise sequences {𝜉 𝑗 } and {𝜂 𝑗 } are assumed to be independent and i.i.d. Let 𝑣 = (𝑣 1 , . . . , 𝑣 𝐽 ) and 𝑦 = (𝑦 1 , . . . , 𝑦 𝐽 ) denote the collection of states and observations, respectively. The conditional probabilities in (5.12) have the closed forms P(𝑦|𝑢, 𝑣) = 𝐽 ∏︁ 𝑗=1 𝒩 (︀ 𝑦 𝑗 ; ℎ(𝑣 𝑗 ), Γ )︀ , P(𝑣|𝑢) = 𝐽-1 ∏︁ 𝑗=0 𝒩 (︀ 𝑣 𝑗+1 ; Ψ(𝑣 𝑗 ; 𝑢), Σ )︀ . Moreover, the likelihood for the marginal variables is given by: l(𝑦|𝑢) = ∫︁ R 𝑑𝑣 𝐽 P(𝑦|𝑢, 𝑣)P(𝑣|𝑢) 𝑑𝑣. The likelihood is typically not available in explicit form due to the non-linearity in Ψ and ℎ and hence requires numerical approximations that are computationally expensive for large 𝐽. This motivates the use of likelihood-free inference methods that do not require (approximate) evaluations of l. A different approach to learn parameters in hidden Markov models based on expectation-maximization is presented in Chapter 8. ♢ Consequences Of Block-Triangular Pushforward In this setting the approach we introduce is to construct a map to approximate the joint distribution 𝛾(𝑦, 𝑢) in such a way that we can extract the 𝑦-parameterized family of conditionals 𝜋 𝑦 (𝑢). To this end, we make the following assumption: Assumption 5.12. Let 𝜚(𝑦, 𝑢) = 𝜚 1 (𝑦)𝜚 2 (𝑢) be a product reference density on R 𝑘 × R 𝑑 and 𝑇 : R 𝑘 × R 𝑑 → R 𝑘 × R 𝑑 be a transport map with the property that the joint density 𝛾(𝑦, 𝑢) = 𝜅(𝑦)𝜋 𝑦 (𝑢) is a pushforward under 𝑇 , i.e., 𝛾 = 𝑇 ♯ 𝜚. Assume also that 𝑇 is block-triangular: 2 𝑇 (𝑦, 𝑢) = [︃ 𝑇 1 (𝑦) 𝑇 2 (𝑦, 𝑢) ]︃ , (5.13) where 𝑇 1 : R 𝑘 → R 𝑘 is invertible and 𝑇 2 (𝑦, •) : R 𝑑 → R 𝑑 is invertible for every 𝑦 ∈ R 𝑘 . Remark 5.13. We discuss training based on the pushforward of a block-triangular map in Subsection 5.5.2, a context in which 𝑇 -1 arises. To avoid inverting the map during training, it is often convenient to directly work with the inverse map 𝑆 := 𝑇 -1 . For a block-triangular 𝑇 , the inverse 𝑆 is also a block-triangular map of the form 𝑆(𝑦, 𝑢) = [︃ 𝑆 1 (𝑦) 𝑆 2 (𝑦, 𝑢) ]︃ , (5.14) where 𝑆 1 : R 𝑘 → R 𝑘 is the inverse of 𝑇 1 and 𝑆 2 (𝑦, •) : R 𝑑 → R 𝑑 is the inverse of 𝑇 2 (𝑇 -1 1 (𝑦), •) for each 𝑦 ∈ R 𝑘 . By writing 𝑇 ♯ 𝜚 = (𝑆 -1 ) ♯ 𝜚, the push-forward density can be more easily evaluated in terms of the map 𝑆, rather than the inverse of 𝑇 . As well as being useful computationally, these definitions of 𝑆 1 , 𝑆 2 simplify various expressions in the proof of the following theorem. ♢ The next theorem motivates the choice of tensor-product reference measure: such a choice ensures that the block-triangular form of 𝑇 provides a map that can be used to readily characterize the posterior density. Theorem 5.14. Let Assumption 5.12 hold. Then (𝑇 1 ) ♯ 𝜚 1 = 𝜅 and 𝑇 2 (𝑇 -1 1 (𝑦), •) ♯ 𝜚 2 = 𝜋 𝑦 . ( 5 .15) Remark 5.15. The theorem shows that, if 𝑤 ∼ 𝜚 2 then 𝑇 2 (︀ 𝑇 -1 1 (𝑦), 𝑤 )︀ ∼ 𝜋 𝑦 . Choosing 𝜚 2 to be an easy distribution to sample, and learning 𝑇 from data, then leads to a method for sampling from the posterior. In other words, imposing block-triangular structure on the map and the product form for the reference measure yields a map that meets our goal of characterizing the posterior distribution. ♢ Proof of Theorem 5.14. We first prove that (𝑇 1 ) ♯ 𝜚 1 = 𝜅. Let 𝑆 = 𝑇 -1 be the inverse map of the form in (5.14) . By Lemma 11.40 we have that 𝑇 ♯ 𝜚(𝑦, 𝑢) = 𝜚 1 (︀ 𝑆 1 (𝑦) )︀ det 𝐷𝑆 1 (𝑦)𝜚 2 (︀ 𝑆 2 (𝑦, 𝑢) )︀ det 𝐷 𝑢 𝑆 2 (𝑦, 𝑢), (5.16a) (𝑇 1 ) ♯ 𝜚 1 (𝑦) = 𝜚 1 (︀ 𝑆 1 (𝑦) )︀ det 𝐷𝑆 1 (𝑦), (5.16b) (︀ 𝑇 2 (𝑇 -1 1 (𝑦), •) )︀ ♯ 𝜚 2 (𝑢) = 𝜚 2 (︀ 𝑆 2 (𝑦, 𝑢) )︀ det 𝐷 𝑢 𝑆 2 (𝑦, 𝑢). (5.16c) Integrating the last identity, using that the left-hand side is a probability density function, gives for all 𝑦 ∈ R 𝑘 , 1 = ∫︁ R 𝑑 𝜚 2 (︀ 𝑆 2 (𝑦, 𝑢) )︀ det 𝐷 𝑢 𝑆 2 (𝑦, 𝑢) 𝑑𝑢. Hence, multiplying identity (5.16a) by an arbitrary test function 𝜓 : R 𝑘 → R and integrating over 𝑦 and 𝑢 we obtain ∫︁ R 𝑘 ∫︁ R 𝑑 𝜓(𝑦)𝑇 ♯ 𝜚(𝑦, 𝑢) 𝑑𝑦𝑑𝑢 = ∫︁ R 𝑘 𝜓(𝑦)𝜚 1 (︀ 𝑆 1 (𝑦) )︀ det 𝐷𝑆 1 (𝑦) 𝑑𝑦. (5.17) But 𝑇 ♯ 𝜚(𝑦, 𝑢) = 𝛾(𝑦, 𝑢) = 𝜅(𝑦)𝜋 𝑦 (𝑢). Thus, using the fact that, for all 𝑦 ∈ R 𝑘 , 1 = ∫︁ R 𝑑 𝜋 𝑦 (𝑢) 𝑑𝑢 we see that (5.17) simplifies to give ∫︁ R 𝑘 𝜓(𝑦)𝜅(𝑦) 𝑑𝑦 = ∫︁ R 𝑘 𝜓(𝑦)𝜚 1 (︀ 𝑆 1 (𝑦) )︀ det 𝐷𝑆 1 (𝑦) 𝑑𝑦. (5.18) Since this is true for all 𝜓 we deduce that 𝜅(𝑦) = 𝜚 1 (︀ 𝑆 1 (𝑦) )︀ det 𝐷𝑆 1 (𝑦) and this, by Lemma 11.40, is the desired result. We now prove identity (5.15) . Recall from Remark 5.13 that 𝑆 2 (𝑦, •) = 𝑇 -1 2 (𝑇 -1 1 (𝑦), •) for each fixed 𝑦. Multiplying and dividing the right-hand side of (5.16c) by the right-hand side of (5.16b), we have 𝑇 2 (𝑇 -1 1 (𝑦), •) ♯ 𝜚 2 (𝑢) = 𝜚 2 (︀ 𝑆 2 (𝑦, 𝑢) )︀ det 𝐷 𝑢 𝑆 2 (𝑦, 𝑢) = 𝜚 1 (︀ 𝑆 1 (𝑦) )︀ 𝜚 2 (︀ 𝑆 2 (𝑦, 𝑢) )︀ 𝜚 1 (︀ 𝑆 1 (𝑦) )︀ det 𝐷𝑆 1 (𝑦) det 𝐷𝑆 1 (𝑦) det 𝐷 𝑢 𝑆 2 (𝑦, 𝑢) = 𝜚 (︀ 𝑆(𝑦, 𝑢) )︀ det 𝐷 (𝑦,𝑢) 𝑆(𝑦, 𝑢) 𝜚 1 (︀ 𝑆 1 (𝑦) )︀ det 𝐷𝑆 1 (𝑦) . (5. 19) In the last equality we used the product form of the reference probability density function and we used the fact that the determinant of a block-triangular matrix can be written as a product of the determinant of its diagonal blocks. Now, note that the numerator of (5.19) is the joint density 𝑇 ♯ 𝜚(𝑦, 𝑢) = 𝛾(𝑦, 𝑢) and the denominator is the marginal 𝜅(𝑦) on 𝑦 under 𝛾, as shown in (5.18) . Thus, by conditioning the joint density 𝛾(𝑦, 𝑢) on 𝑦 we find that 𝜚 (︀ 𝑆(𝑦, 𝑢) )︀ det 𝐷 (𝑦,𝑢) 𝑆(𝑦, 𝑢) 𝜚 1 (︀ 𝑆 1 (𝑦) )︀ det 𝐷𝑆 1 (𝑦) = 𝛾(𝑦, 𝑢) 𝜅(𝑦) = 𝜋 𝑦 (𝑢). Hence we have shown the desired result that 𝑇 2 (𝑇 -1 1 (𝑦), •) ♯ 𝜚 2 (𝑢) = 𝛾(𝑦, 𝑢) 𝜅(𝑦) = 𝜋 𝑦 (𝑢). Remark 5.16. While a block-triangular map can sample from conditional distributions, it does not uniquely determine a particular map with the desired property. Indeed, when such a map exists, there may exist an infinite number of maps with the structure in Assumption 5.12. We refer to Section 13.2 for discussion of computational methods to find transports from data. ♢ Remark 5.17. The reference measure 𝜚 is a degree of freedom in the measure transport framework. A useful choice is 𝜚(𝑦, 𝑢) = 𝜅(𝑦)𝜚 2 (𝑢); that is, to choose 𝜚 1 (𝑦) = 𝜅(𝑦). It then follows that the identity map 𝑇 1 (𝑦) = 𝑦 is a valid transport, trivially pushing forward 𝜚 1 (𝑦) = 𝜅(𝑦) to 𝜅(𝑦). This choice avoids the inversion of 𝑇 1 in (5.15) and makes sampling from 𝜋 1 straightforward: once we have determined 𝑇 2 compatible with this choice of 𝑇 1 then for all 𝑦 ∼ 𝜅 we have 𝑇 2 (𝑦, 𝑢) ∼ 𝜋 𝑦 , for 𝑢 ∼ 𝜚 2 . ♢ Example 5.18. Let 𝜚(𝑦, 𝑢) be the standard Gaussian distribution on R 𝑘 × R 𝑑 and let 𝛾(𝑦, 𝑢) be the Gaussian distribution 𝒩 (𝑚, Σ) on R 𝑘 × R 𝑑 with mean and covariance 𝑚 = [︃ 𝑚 𝑦 𝑚 𝑢 ]︃ , Σ = [︃ Σ 𝑦𝑦 Σ 𝑦𝑢 Σ 𝑢𝑦 Σ 𝑢𝑢 ]︃ , where Σ 𝑢𝑦 = Σ ⊤ 𝑦𝑢 . Let 𝑇 : R 𝑘 × R 𝑑 → R 𝑘 × R 𝑑 be a block-triangular transport map of the form 𝑇 (𝑦, 𝑢) = [︃ 𝑇 1 (𝑦) 𝑇 2 (𝑦, 𝑢) ]︃ = ⎡ ⎣ 𝑚 𝑦 + Σ 1/2 𝑦𝑦 𝑦 𝑚 𝑢|𝑦 (𝑚 𝑦 + Σ 1/2 𝑦𝑦 𝑦) + Σ 1/2 𝑢|𝑦 𝑢 ⎤ ⎦ , where 𝑚 𝑢|𝑦 (𝑦) := 𝑚 𝑢 + Σ 𝑢𝑦 Σ -1 𝑦𝑦 (𝑦 -𝑚 𝑦 ) and Σ 𝑢|𝑦 := Σ 𝑢𝑢 -Σ 𝑢𝑦 Σ -1 𝑦𝑦 Σ 𝑦𝑢 denote the conditional mean and covariance matrix of 𝑢 given 𝑦. Note that 𝑇 (𝑦, 𝑢) = ⎡ ⎣ 𝑚 𝑦 + Σ 1/2 𝑦𝑦 𝑦 𝑚 𝑢 + Σ 𝑢𝑦 Σ -1/2 𝑦𝑦 𝑦 + Σ 1/2 𝑢|𝑦 𝑢 ⎤ ⎦ . For (𝑦, 𝑢) distributed according to the standard Gaussian distribution on R 𝑘 × R 𝑑 straightforward calculation shows that 𝑇 (𝑦, 𝑢) has distribution 𝒩 (𝑚, Σ) on R 𝑘 × R 𝑑 . Alternatively we may show that 𝑇 ♯ 𝜚 = 𝛾 by working with densities. To this end we will compute the pushforward density 𝑇 ♯ 𝜚(𝑦, 𝑢) = 𝜚 (︀ 𝑇 -1 (𝑦, 𝑢) )︀ det 𝐷𝑇 -1 (𝑦, 𝑢). First, the inverse map has the form 𝑇 -1 (𝑦, 𝑢) = [︃ 𝑇 -1 1 (𝑦) 𝑇 -1 2 (𝑦, 𝑢) ]︃ = ⎡ ⎣ Σ -1/2 𝑦𝑦 (𝑦 -𝑚 𝑦 ) Σ -1/2 𝑢|𝑦 (︀ 𝑢 -𝑚 𝑢|𝑦 (𝑦) )︀ ⎤ ⎦ . Then, for the standard Gaussian reference 𝜚 we have 𝜚 (︀ 𝑇 -1 (𝑦, 𝑢) )︀ = 1 √︁ (2𝜋) 𝑘+𝑑 exp (︂ - 1 2 |Σ -1/2 𝑦𝑦 (𝑦 -𝑚 𝑦 )| 2 - 1 2 |Σ -1/2 𝑢|𝑦 (︀ 𝑢 -𝑚 𝑢|𝑦 (𝑦) )︀ | 2 )︂ = 1 √︁ (2𝜋) 𝑘+𝑑 exp ⎛ ⎜ ⎝- 1 2 ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ ⎡ ⎣ Σ -1/2 𝑦𝑦 0 -Σ -1/2 𝑢|𝑦 Σ 𝑢𝑦 Σ -1 𝑦𝑦 Σ -1/2 𝑢|𝑦 ⎤ ⎦ [︃ 𝑦 -𝑚 𝑦 𝑢 -𝑚 𝑢 ]︃ ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ 2 ⎞ ⎟ ⎠ = 1 √︁ (2𝜋) 𝑘+𝑑 exp ⎛ ⎝ - 1 2 ⃒ ⃒ ⃒ ⃒ ⃒ Σ -1/2 [︃ 𝑦 -𝑚 𝑦 𝑢 -𝑚 𝑢 ]︃⃒ ⃒ ⃒ ⃒ ⃒ 2 ⎞ ⎠ , where Σ -1/2 denotes the inverse of the block Cholesky factor of Σ. We note that the block-Cholesky factor Σ 1/2 satisfies Σ 1/2 (Σ 1/2 ) ⊤ = ⎡ ⎣ Σ 1/2 𝑦𝑦 0 Σ 𝑢𝑦 Σ -1/2 𝑦𝑦 Σ 1/2 𝑢|𝑦 ⎤ ⎦ ⎡ ⎣ Σ 1/2 𝑦𝑦 Σ -1/2 𝑦𝑦 Σ 𝑦𝑢 0 Σ 1/2 𝑢|𝑦 ⎤ ⎦ = [︃ Σ 𝑦𝑦 Σ 𝑦𝑢 Σ 𝑢𝑦 Σ 𝑢𝑢 ]︃ = Σ. Moreover, the determinant of the block-triangular Jacobian of 𝑇 -1 is given by the product of the determinant of the Jacobian's diagonal elements. That is, det 𝐷𝑇 -1 (𝑦, 𝑢) = det Σ -1/2 𝑦𝑦 det Σ -1/2 𝑢|𝑦 = (det Σ 𝑦𝑦 ) -1/2 (det Σ 𝑢|𝑦 ) -1/2 = (det Σ) -1/2 , where in the last equality we used the formula for the determinant of a block matrix with invertible diagonal elements. Thus, the pushforward density is given by 𝜚 (︀ 𝑇 -1 (𝑦, 𝑢) )︀ det 𝐷𝑇 -1 (𝑦, 𝑢) = 1 √︁ (2𝜋) 𝑘+𝑑 det Σ exp ⎛ ⎝ - 1 2 [︃ 𝑦 -𝑚 𝑦 𝑢 -𝑚 𝑢 ]︃ ⊤ Σ -1 [︃ 𝑦 -𝑚 𝑦 𝑢 -𝑚 𝑢 ]︃ ⎞ ⎠ . That is, the pushforward is a multivariate Gaussian with mean 𝑚 and covariance Σ, which matches the density for 𝛾. Moreover, 𝑇 1 (𝑦) = 𝑚 𝑦 + Σ 1/2 𝑦𝑦 𝑦 pushes forward 𝜚 1 = 𝒩 (0, 𝐼 𝑘 ) to 𝜅 = 𝒩 (𝑚 𝑦 , Σ 𝑦𝑦 ), and so by Theorem 5.14 we have that 𝑇 2 (𝑇 -1 1 (𝑦), 𝑢) = 𝑚 𝑢|𝑦 (𝑦) + Σ 1/2 𝑢|𝑦 𝑢 pushes forward 𝒩 (0, 𝐼 𝑑 ) to the posterior 𝜋 𝑦 for each 𝑦. ♢ Learning Block-Triangular Pushforward Maps Once again, recall the notation of Remark 1.4. In this subsection we discuss the learning of block-triangular transport maps, given data pairs {(𝑦 (𝑛) , 𝑢 (𝑛) )} 𝑁 𝑛=1 from the joint distribution 𝛾(𝑦, 𝑢), as in Assumption 5.2. Let 𝜃 ∈ Θ ⊆ R 𝑝 denote the parameters of the block-triangular map 𝑇 (𝑢, 𝑦; 𝜃) given by (5.13). Our goal is to find the optimal parameters by minimizing the KL divergence between the pushforward distribution 𝑇 ♯ 𝜚 and the joint distribution 𝛾. That is, 𝜃 ⋆ ∈ arg min 𝜃∈Θ D KL (𝛾‖𝑇 (•; 𝜃) ♯ 𝜚). (5.20) Remark 5.19. Compare with the minimization problems (4.2) and (5.11) for which the reference measure appears in the left-hand argument of D KL (•‖•). This choice is dictated by the form of the data, drawn from the joint distribution; and specifically drawn by sampling from prior on 𝑢 and then from likelihood on 𝑦|𝑢. The following theorem shows that the optimal parameters can then be identified from an optimization objective that only depends on the joint distribution 𝛾(𝑦, 𝑢) through an expectation, and hence can be solved using an empirical approximation of the objective. ♢ Remark 5.20. In implementation, the distribution 𝛾 in the optimization problem (5.20) is replaced by an empirical approximation. Using the form of the KL divergence, the optimization problem takes the form 𝜃 * = arg min 𝜃 1 𝑁 𝑁 ∑︁ 𝑖=1 -log 𝑇 (•; 𝜃) ♯ 𝜚(𝑦 𝑖 , 𝑢 𝑖 ). Thus, the optimal solution 𝜃 * computed in practice corresponds to solving the maximum likelihood problem for the parameters under the model 𝑇 (•; 𝜃) ♯ 𝜚. See Section 13.1 for more discussion on maximum likelihood estimation. ♢ Theorem 5.21. Let 𝑇 be a block-triangular transport map of the form in Assumption 5.12, let 𝜚(𝑦, 𝑢) = 𝜅(𝑦)𝜚 2 (𝑢) be a reference distribution as in Remark 5.17 with 𝑇 1 (𝑦) = 𝑦 and let 𝜃 ⋆ be defined by (5.20) . Then, the optimal parameters for the map 𝑇 2 are given by 𝜃 ⋆ ∈ arg min 𝜃∈Θ -E (𝑦,𝑢)∼𝛾 [︁ log 𝜚 2 ∘ 𝑇 -1 2 (𝑦, 𝑢; 𝜃) + log det 𝐷 𝑢 𝑇 -1 2 (𝑦, 𝑢; 𝜃) ]︁ , where 𝑇 -1 2 (𝑦, 𝑢; 𝜃) denotes the inverse of the function 𝑢 ↦ → 𝑇 2 (𝑦, 𝑢; 𝜃) for each given pair (𝑦, 𝜃). Proof. From the chain rule for the KL divergence, Lemma 11.37, we have D KL (𝛾‖𝑇 (•; 𝜃) ♯ 𝜚) = D KL (𝜅‖(𝑇 1 ) ♯ 𝜅) + E 𝑦∼𝜅 [D KL (𝜋 𝑦 (𝑢)‖𝑇 2 (𝑦, •; 𝜃) ♯ 𝜚 2 )]. ( Setting 𝑇 1 (𝑦) = 𝑦, the first term in (5.21) is zero, and the second term is given by E 𝑦∼𝜅 [D KL (𝜋 𝑦 ‖𝑇 2 (𝑦, •; 𝜃) ♯ 𝜚 2 )] = ∫︁ ∫︁ 𝜅(𝑦)𝜋 𝑦 (𝑢) [︁ log 𝜋 𝑦 (𝑢) -log (︀ 𝑇 2 (𝑦, •; 𝜃) ♯ 𝜚 2 (𝑢) )︀ ]︁ 𝑑𝑦𝑑𝑢 = 𝑐 - ∫︁ ∫︁ 𝛾(𝑦, 𝑢) log (︀ 𝑇 2 (𝑦, •; 𝜃) ♯ 𝜚 2 (𝑢) )︀ 𝑑𝑢𝑑𝑦, where 𝑐 is a constant that is independent of the parameters 𝜃. Therefore, minimizing the second term achieves the minimum of the objective as stated in the theorem, by use of Lemma 11.40. Remark 5.22. As in Remark 5.13, it is common to parameterize the inverse map 𝑇 -1 2 (𝑦, •; 𝜃) directly by letting 𝑆 2 (𝑦, •; 𝜃) := 𝑇 -1 2 (𝑦, •; 𝜃). This choice avoids inversion during learning, however it requires inverting the map 𝑆 2 to sample from the posterior 𝜋 𝑦 after learning. ♢ The following result shows that the optimization problem in Theorem 5.21 over the space of inverse transports is convex for certain reference distributions. 𝑆 ⋆ 2 ∈ arg min 𝑆 2 ∈𝒮 -E (𝑦,𝑢)∼𝛾 [︁ log 𝜚 2 ∘ 𝑆 2 (𝑦, 𝑢) + log det 𝐷 𝑢 𝑆 2 (𝑦, 𝑢) ]︁ , (5.22) over the space of invertible and diffeomorphic maps 𝒮 = {︁ 𝑆 2 ∈ 𝐶 1 (R 𝑘+𝑑 ; R 𝑑 ), 𝐷 𝑢 𝑆 2 (𝑦, 𝑢) ≻ 0 for all 𝑢 ∈ R 𝑑 , 𝑦 ∈ R 𝑘 }︁ is convex. Proof. The proof follows the steps of Theorem 4.4 by replacing the posterior 𝜋 𝑦 with the reference density 𝜚 2 . Remark 5.24. Theorem 5.23 does not imply that the optimization problem in Theorem 5.21 is convex in 𝜃. In particular when 𝑆 has a non-linear parameterization based on neural networks, the problem will be non-convex. ♢ Example 5.25. Let 𝜚 2 (𝑢) = (2𝜋) -𝑑/2 exp(-1 2 |𝑢| 2 ) be the standard Gaussian reference density. Then, log 𝜚 2 (𝑢) = -𝑑 2 log(2𝜋) -1 2 |𝑢| 2 . Ignoring the first term that is a constant, Theorem 5.23 shows that optimal map 𝑆 2 is found as solution of the optimization problem arg min 𝑆 2 E (𝑦,𝑢)∼𝛾 [︂ 1 2 |𝑆 2 (𝑦, 𝑢)| 2 -log det 𝐷 𝑢 𝑆 2 (𝑦, 𝑢) ]︂ . By Theorem 5.23, the optimization problem for 𝑆 2 is convex. The first term in the objective minimizes the squared norm of 𝑆 2 (𝑦, 𝑢), which encourages the map's output to be at zero, the MAP point of the reference distribution 𝜚 2 . The second term prevents the map from concentrating the output at a single point. Moreover, the second term acts a log-barrier for the space of invertible maps by adding a large penalty as the derivative of 𝑆 2 approaches zero. ♢ The convexity of the objective yields uniqueness of the solution to (5.22) when a minimizer exists. The following result provides a concrete example for the closed-form solution when 𝒮 is restricted to the space of affine maps. Theorem 5.26. Let 𝜚 2 (𝑢) = (2𝜋) -𝑑/2 exp(-1 2 |𝑢| 2 ) be Gaussian and let (𝑦, 𝑢) ∼ 𝛾. We consider 𝒮 be the space of affine maps 𝒮 = {︁ 𝑆 2 (𝑦, 𝑢) = 𝐴(𝑢 + 𝐵𝑦 + 𝑐), 𝐴 ∈ R 𝑑×𝑑 , 𝐴 ≻ 0, 𝐵 ∈ R 𝑑×𝑘 , 𝑐 ∈ R 𝑑 }︁ , where 𝐴 is also constrained to be a triangular matrix. Then, the optimal map in the sense of solving (5.22) has the form 𝑆 2 (𝑦, 𝑢) = Σ -1/2 𝑢|𝑦 (︁ 𝑢 -E[𝑢] + Σ 𝑢𝑦 Σ -1 𝑦𝑦 (𝑦 -E[𝑦]) )︁ , where Σ 𝑢|𝑦 := Σ 𝑢𝑢 -Σ 𝑢𝑦 Σ -1 𝑦𝑦 Σ 𝑦𝑢 . The matrices Σ 𝑢𝑢 , Σ 𝑦𝑦 and Σ 𝑢𝑦 denote the covariance of 𝑢, covariance of 𝑦, and cross-covariance of (𝑢, 𝑦) under 𝛾, respectively. Proof. The optimization problem has the form (𝐴 ⋆ , 𝐵 ⋆ , 𝑐 ⋆ ) ∈ arg min 𝐴,𝐵,𝑐 E (𝑦,𝑢)∼𝛾 [︂ 1 2 (𝑢 + 𝐵𝑦 + 𝑐) ⊤ 𝐴 ⊤ 𝐴(𝑢 + 𝐵𝑦 + 𝑐) -log det 𝐴 ]︂ . For fixed 𝐴 and 𝐵, taking the gradient of the objective with respect to 𝑐 and setting it equal to zero, we have 𝑐 ⋆ = -E[𝑢] -𝐵E[𝑦]. Substituting the optimal 𝑐 ⋆ in the objective, we then define the loss for 𝐵 given a fixed 𝐴 to be L(𝐵; 𝐴) := E (𝑦,𝑢)∼𝛾 [︂ 1 2 (︀ 𝑢 -E[𝑢] + 𝐵(𝑦 -E[𝑦] )︀ ⊤ 𝐴 ⊤ 𝐴 (︀ 𝑢E[𝑢] + 𝐵(𝑦 -E[𝑦] )︀ -log det 𝐴 ]︂ . Taking the gradient with respect to 𝐵 and setting it equal to zero we have 𝐷 𝐵 L(𝐵; 𝐴) = 2𝐴 ⊤ 𝐴(𝐵Σ 𝑦𝑦 + Σ 𝑢𝑦 ) = 0, where Σ 𝑦𝑦 and Σ 𝑢𝑦 denote the covariance and cross-covariance of 𝑦 and (𝑢, 𝑦). Re-arranging for 𝐵 gives us 𝐵 ⋆ = -Σ 𝑢𝑦 Σ -1 𝑦𝑦 . Substituting the optimal 𝐵 ⋆ in the loss L, we notice that the optimization problem for 𝐴 ⊤ 𝐴 corresponds to the Gaussian variational inference problem in (2.8). The optimal solution for 𝐴 ⋆ is given by the inverse Cholesky factor of the covariance matrix (𝐴 ⋆ ) -1 (𝐴 ⋆ ) -⊤ = (︁ E [︁ (︀ 𝑢 -E[𝑢] -Σ 𝑢𝑦 Σ -1 𝑦𝑦 (𝑦 -E[𝑦]) )︀(︀ 𝑢 -E[𝑢] -Σ 𝑢𝑦 Σ -1 𝑦𝑦 (𝑦 -E[𝑦]) )︀ ⊤ ]︁)︁ -1 . Expanding the squared terms yields (𝐴 ⋆ ) -1 (𝐴 ⋆ ) -⊤ = Σ 𝑢𝑢 -Σ 𝑢𝑦 Σ -1 𝑦𝑦 Σ 𝑦𝑢 -Σ 𝑢𝑦 Σ -1 𝑦𝑦 Σ 𝑦𝑢 + Σ 𝑢𝑦 Σ -1 𝑦𝑦 Σ 𝑦𝑦 Σ -1 𝑦𝑦 Σ 𝑦𝑢 = Σ 𝑢𝑢 -Σ 𝑢𝑦 Σ -1 𝑦𝑦 Σ 𝑦𝑢 = Σ 𝑢|𝑦 , which gives us the final result, (𝐴 ⋆ ) -1 = Σ 1/2 𝑢|𝑦 . Learning Likelihood Models The framework in Section 5.5 can also be used to learn the likelihood function in an inverse problem on the basis of data. We again work in the setting of Assumption 5.9. Learning the likelihood is particularly useful in inverse problems where the likelihood is unknown analytically or intractable to evaluate as in Example 5.11. Given an approximation to the likelihood function and a known prior density, we have access to the approximate posterior density up to a normalizing constant. The resulting density can be used in algorithms such as MCMC to sample from the posterior distribution. For this purpose we define a block-triangular map 𝑇 , identical to the form in (5.13), but with a reversed ordering for the variables (𝑢, 𝑦); that is 𝑇 (𝑢, 𝑦) = [︃ 𝑇 1 (𝑢) 𝑇 2 (𝑢, 𝑦) ]︃ . (5.23) We use a map of this form, assumed for now to exactly pushforward the product reference density 𝜚(𝑦, 𝑢) := 𝜚 1 (𝑢)𝜚 2 (𝑦) to 𝛾(𝑦, 𝑢); later we approximate this pushforward. In this section we factor 𝛾(𝑦, 𝑢) = 𝜌(𝑢)l(𝑦|𝑢). By Theorem 5.14, with the roles of 𝑢 and 𝑦 reversed, the map 𝑦 ↦ → 𝑇 2 (𝑇 -1 1 (𝑢), 𝑦) pushes forward 𝜚 2 (𝑦) to l(𝑦|𝑢) for each 𝑢. We choose the first marginal of the reference density to be the prior, 𝜚 1 (𝑢) = 𝜌(𝑢). Then, letting 𝑇 1 (𝑢) = 𝑢, the likelihood function is given by l(𝑦|𝑢) = 𝜚 2 (︀ 𝑇 -1 2 (𝑢, 𝑦) )︀ det 𝐷 𝑦 𝑇 -1 2 (𝑢, 𝑦), ( 5.24) where 𝑇 -1 2 (𝑢, •) denotes the inverse of the function 𝑦 ↦ → 𝑇 2 (𝑢, 𝑦). Example 5.27. Let 𝜚 2 (𝑦) be the standard Gaussian on R 𝑘 . Choosing 𝑇 2 (𝑢, 𝑦) = 𝐺(𝑢) -Γ 1/2 𝑦 for some map 𝐺 : R 𝑑 → R 𝑑 and a positive definite matrix Γ ∈ R 𝑘 × R 𝑘 , we have the Gaussian likelihood function l(𝑦|𝑢) = 𝜚 2 (︁ Γ -1/2 (︀ 𝑦 -𝐺(𝑢) )︀ )︁ det Γ -1/2 = 1 √︁ (2𝜋) 𝑘 det Γ exp (︂ - 1 2 |𝑦 -𝐺(𝑢)| 2 Γ )︂ = 𝒩 (𝐺(𝑢), Γ). ♢ Following the approach in Subsection 5.5.2, we learn the parameters 𝜃 to define a map 𝑇 (𝑢, 𝑦; 𝜃) which approximates the exact pushforward. Thus we solve the optimization problem 𝜃 ⋆ ∈ arg min 𝜃∈Θ D KL (𝛾‖𝑇 (•; 𝜃) ♯ 𝜚). ( 5.25) The next theorem shows that we can find the parameters 𝜃 ⋆ by minimizing a loss function that only depends on the joint distribution 𝛾 via an expectation, and thus is amenable to learning from paired data. The proof is analogous to that of Theorem 5.21 but with the roles of 𝑦 and 𝑢 reversed. Theorem 5.28. Let 𝑇 be a block-triangular transport map of the form in (5.23) . Let the reference density be 𝜚(𝑢, 𝑦) = 𝜌(𝑢)𝜚 2 (𝑦) and 𝑇 1 (𝑢) = 𝑢. The optimal parameters 𝜃 ⋆ from (5.25) also solve the optimization problem 𝜃 ⋆ ∈ arg min 𝜃∈Θ -E (𝑦,𝑢)∼𝛾 [︁ log 𝜚 2 ∘ 𝑇 -1 2 (𝑢, 𝑦; 𝜃) + log det 𝐷 𝑦 𝑇 -1 2 (𝑢, 𝑦; 𝜃) ]︁ . (5.26) Proof. From the chain rule for the KL divergence, Lemma 11.37, we have D KL (𝛾‖𝑇 (•; 𝜃) ♯ 𝜚) = D KL (𝜌‖(𝑇 1 ) ♯ 𝜌) + E 𝑢∼𝜌 [︀ D KL (l(•|𝑢)‖𝑇 2 (𝑢, •; 𝜃) ♯ 𝜚 2 ) ]︀ . (5.27) Setting 𝑇 1 (𝑢) = 𝑢, the first term in (5.27) is zero, and the second term is given by E 𝑢∼𝜌 [D KL (l(•|𝑢)‖𝑇 2 (𝑢, •; 𝜃) ♯ 𝜚 2 )] = ∫︁ 𝜌(𝑢)l(𝑦|𝑢) [︁ log l(𝑦|𝑢) -log (︀ 𝑇 2 (𝑢, •; 𝜃) ♯ 𝜚 2 (𝑦) )︀ ]︁ 𝑑𝑦𝑑𝑢 = 𝑐 - ∫︁ 𝛾(𝑦, 𝑢) log (︀ 𝑇 2 (𝑢, •; 𝜃) ♯ 𝜚 2 (𝑦) )︀ 𝑑𝑢𝑑𝑦, where 𝑐 is a constant that is independent of 𝜃. Therefore, minimizing the second term achieves the minimum of the objective as stated in the theorem, by use of Lemma 11.40. Given a collection of 𝑁 paired samples {(𝑦 (𝑛) , 𝑢 (𝑛) )} 𝑁 𝑛=1 from the joint distribution 𝛾(𝑦, 𝑢) we may approximate the expectation in (5.26) . This leads to the problem of minimizing the following empirical loss function to find the approximate likelihood arg min ) , 𝑦 (𝑛) ; 𝜃) + log det 𝐷 𝑦 𝑇 -1 2 (𝑢 (𝑛) , 𝑦 (𝑛) ; 𝜃). 𝜃∈Θ - 1 𝑁 𝑁 ∑︁ 𝑛=1 log 𝜚 2 ∘ 𝑇 -1 2 (𝑢 (𝑛 (5.28) Example 5.29. Let 𝜚 2 be a standard Gaussian density of dimension R 𝑘 and let 𝑇 2 be a parameterized transport map of the form 𝑇 2 (𝑢, 𝑦; 𝜃) = 𝐺(𝑢; 𝜃)-Γ 1/2 𝑦 as in Example 5.27 where Γ ≻ 0 is known. Then, minimizing the loss function in (5.28) corresponds to solving the problem arg min 𝜃∈Θ {︃ 1 𝑁 𝑁 ∑︁ 𝑛=1 1 2 ⃒ ⃒ ⃒𝑇 -1 2 (𝑢 (𝑛) , 𝑦 (𝑛) ; 𝜃) ⃒ ⃒ ⃒ 2 + log det 𝐷 𝑦 𝑇 -1 2 (𝑢 (𝑛) , 𝑦 (𝑛) ; 𝜃) }︃ = arg min 𝜃∈Θ {︃ 1 𝑁 𝑁 ∑︁ 𝑛=1 1 2 ⃒ ⃒ ⃒Γ -1/2 (︀ 𝑦 (𝑛) -𝐺(𝑢 (𝑛) ; 𝜃) )︀ ⃒ ⃒ ⃒ 2 Γ + log det Γ -1/2 }︃ = arg min 𝜃∈Θ 1 𝑁 𝑁 ∑︁ 𝑛=1 1 2 ⃒ ⃒ ⃒𝑦 (𝑛) -𝐺(𝑢 (𝑛) ; 𝜃) ⃒ ⃒ ⃒ 2 Γ . Hence, finding the transport map is equivalent to seeking an approximate forward model as the solution of a mean-squared regression problem. This generalizes the approach of learning forward surrogate models in Section 1.5. ♢ Bibliography The variable 𝑧 over which we marginalize is often referred to as a nuisance or auxiliary random variable; this is often used in the setting where 𝑧 is not the primary parameter of interest in the inverse problem. An alternative approach to those described here seeks the joint posterior of (𝑢, 𝑧), and then marginalizes after the computation. While the corresponding likelihood for the joint posterior doesn't involve a marginal or integrated likelihood, it can lead to a challenging problem for high or even infinite-dimensional latent variables. We refer to [42] for more details on integrated likelihoods. In MCMC algorithms, these integrated likelihoods are commonly addressed using pseudo-marginal methods that work with unbiased estimators of the likelihood [20] . Approximate Bayesian computation (ABC) is a classic inference method for performing likelihood-free inference with latent variables or other intractable likelihoods; see [389] for a comprehensive overview on ABC. These approaches define a distance function that compare simulated observations to the true observation and reject parameter samples that are not consistent with the true observation based on a small tolerance. ABC methods can be shown to be consistent in the limit of the tolerance approaching zero, but typically require large sample sizes with high-dimensional observations. Learning approaches have appeared as alternatives to ABC; machine learning-based approaches for simulation-based inference are outlined in [113] . Methodologies for both posterior and likelihood approximation in this setting are implemented using various unsupervised learning architectures (see Chapter 13) including: conditional normalizing flows [447, 331] , conditional generative adversarial networks [359] , conditional diffusion models [43] . Triangular transport maps are a core element of these architectures for solving inverse problems. They are related to the well known Knothe-Rosenblatttransport, which has been used for conditional density estimation in [295, 39] . The design and analysis of block-triangular maps on function space is investigated in [41, 219] . Amortized inference is overviewed in [452] . Variational autoencoders, which were proposed concurrently in [247] and [364] , introduced the concept of amortized inference in the specific context of autoencoding. The idea of iterating amortized inference was introduced in [293] ; this approach is designed to close the amortization gap caused by failing to reach optimality when training. Part II Data Assimilation Chapter 6 Filtering and Smoothing Problems This chapter provides an introduction to data assimilation. We study both the filtering and the smoothing problems. Consider the stochastic dynamics model given by 𝑣 † 𝑗+1 = Ψ(𝑣 † 𝑗 ) + 𝜉 † 𝑗 , 𝑗 ∈ Z + , (6.1a) 𝑣 † 0 ∼ 𝒩 (𝑚 0 , 𝐶 0 ), 𝜉 † 𝑗 ∼ 𝒩 (0, Σ) i.i.d. , (6.1b) combined with a data model given by 𝑦 † 𝑗+1 = ℎ(𝑣 † 𝑗+1 ) + 𝜂 † 𝑗+1 , 𝑗 ∈ Z + , ( 6.2a ) 𝜂 † 𝑗+1 ∼ 𝒩 (0, Γ) i.i.d. (6.2b) Broadly speaking, data assimilation seeks to find the state {𝑣 † 𝑗 } over some set of time indices 𝑗 ∈ {0, 1, . . . , 𝐽} based on realized observations {𝑦 † 𝑗 } from (6.2). Various different inverse problems, with this aim, can and will be defined. We make the following assumptions throughout this chapter and in all the remaining chapters devoted to data assimilation. Recall, from the preface, that ⊥ ⊥ denotes independence between random variables. The first assumption in this chapter concerns the independence structure of our model: Assumption 6.1. {𝜉 † 𝑗 } 𝑗∈Z + ⊥ ⊥ {𝜂 † 𝑗 } 𝑗∈N ⊥ ⊥ 𝑣 † 0 . The second assumption concerns the matrices and maps defining the model: Assumption 6.2. The matrices 𝐶 0 , Σ and Γ are positive definite. Furthermore the nonlinear maps Ψ and ℎ are continuously differentiable: Ψ ∈ 𝐶 1 (R 𝑑 , R 𝑑 ) and ℎ ∈ 𝐶 1 (R 𝑑 , R 𝑘 ). Section 6.1 contains introductory remarks that will orient the reader in the remainder of the chapter: Subsection 6.1.1 is devoted to notation and, in particular, explains the use of the † on the state-data models (6.1), (6.2); Subsection 6.1.2 contrasts the filtering and smoothing approaches to data assimilation; and Subsection 6.1.3 contrasts the probabilistic and state estimation approaches to data assimilation. In Section 6.2 we formulate the filtering (Subsection 6.2.1) and smoothing (Subsection 6.2.2) problems in detail. In Sections 6.3 and 6.4, respectively, we describe algorithms for the filtering and smoothing approaches to data assimilation. Subsection 6.3.1 recalls the Kalman filter, applicable in the linear and Gaussian setting. Subsections 6.3.2, 6.3.3, 6.3.4 and 6.3.5 recall 3DVar, the extended Kalman filter (ExKF), the unscented Kalman filter (UKF) and the ensemble Kalman filter (EnKF); Subsections 6.3.6 and 6.3.7 concern the bootstrap particle filter and the optimal particle filter approaches to filtering. Subsection 6.4.1 is devoted to the 4DVar approach to smoothing, Subsection 6.4.2 discusses the strong constraint variant, and the subject of reanalysis is overviewed in Subsection 6.4.3. In Section 6.5 we discuss model error in the context of filtering and smoothing problems and Section 6.6 is devoted to the study of surrogate modeling, for both filtering and smoothing. In Section 6.7 we describe various generalizations of the setting in this chapter and Section 6.8 contains bibliographic notes. Introduction Notation To state the inverse problems of interest precisely, we make the following definitions: for a given and fixed integer 𝐽, and for 1 ≤ 𝑗 ≤ 𝐽, 𝑉 † :={𝑣 † 0 , . . . , 𝑣 † 𝐽 }, 𝑉 † 𝑗 := {𝑣 † 0 , . . . , 𝑣 † 𝑗 }, (6.3a ) 𝑌 † :={𝑦 † 1 , . . . , 𝑦 † 𝐽 }, 𝑌 † 𝑗 := {𝑦 † 1 , . . . , 𝑦 † 𝑗 }. (6.3b) The sequence 𝑉 † is often termed the signal and the sequence 𝑌 † the data. It is sometimes helpful to define 𝑌 † 0 as the empty set ∅. We use the † notation throughout Part II of the notes, devoted to data assimilation, to denote all quantities associated with the underlying model for signal and data. We adopt this convention because many of the algorithms that we develop for data assimilation will use the signal and/or data models (6.1), (6.2) as part of their backbone. We will use symbols 𝑣 𝑗 , or for ensemble algorithms 𝑣 (𝑛) 𝑗 with 𝑛 indexing particles in the ensemble, for the outputs of algorithms to find the state {𝑣 † 𝑗 } over some set of time indices 𝑗 ∈ {0, 1, . . . , 𝐽} based on realized observations 𝑌 † . We will also use the notation ̂︀ 𝑣 𝑗+1 , or for ensemble algorithms (̂︀ 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑦 (𝑛) 𝑗+1 ), for predictions made as part of algorithms; in particular for predictions of the state 𝑣 † 𝑗+1 at time 𝑗 + 1 using only data up to time 𝑗. These predictions are then corrected, using the observed data 𝑦 † 𝑗+1 , to yield the output of algorithms at time 𝑗 + 1. By adopting this notational convention we distinguish between the output of the underlying signal and/or data models (6.1), (6.2) (𝑣 and 𝑦 symbols with †) and algorithms to estimate the signal (𝑣 and 𝑦 symbols without †); this serves to avoid confusion between the source of data, and the algorithms for data assimilation, when the latter also use the underlying signal and/or data models as part of their definition. We do not use the † convention in Part I of the notes, devoted to inverse problems, because the algorithms we study do not have the same potential for confusion with output of the underlying inverse problem model (1.1). Filtering versus Smoothing There are two core problems in data assimilation, one known as filtering, and the other as smoothing. Both use: Data Assumption 6.3. Data 𝑌 † ∈ R 𝐽𝑘 is given and is assumed to have come from the dynamics model (6.1) and the data model (6.2). The filtering distribution at time 𝑗 is P(𝑣 † 𝑗 |𝑌 † 𝑗 ): the distribution of the state 𝑣 † 𝑗 ∈ R 𝑑 at time 𝑗, conditioned on data 𝑌 † 𝑗 ∈ R 𝑘𝑗 . This distribution at time 𝑗 is defined without knowledge of future observations at times 𝑗 ′ > 𝑗. We say that this distribution is timecausal. The objective of filtering is to find P(𝑣 † 𝑗 |𝑌 † 𝑗 ) for each 𝑗 ∈ {1, . . . , 𝐽}, sequentially. The smoothing distribution is P(𝑉 † |𝑌 † ), a probability density function on an entire sequence of states 𝑉 † given the entire dataset 𝑌 † ; note that 𝑉 † ∈ R 𝑑(𝐽+1) and 𝑌 † ∈ R 𝑘𝐽 represent sequences over a window of length 𝐽 + 1 and 𝐽, respectively. The objective of smoothing is to find P(𝑉 † |𝑌 † ). For 0 ≤ 𝑗 ≤ 𝐽 -1, the marginal distribution of the smoothing distribution corresponding to time 𝑗, P(𝑣 † 𝑗 |𝑌 † ), depends on observations at times 𝑗 ′ > 𝑗, highlighting an essential difference between filtering and smoothing. The smoothing problem is an example of a single Bayesian inverse problem of the type defined in Chapter 1. The filtering problem comprises 𝐽 Bayesian inverse problems, interleaved with predictions of the model. We now describe the Bayesian perspective on filtering and on smoothing. In smoothing the prior on 𝑉 † is a probability density function defined by (6.1); the posterior is found by conditioning this prior on data 𝑌 † defined by (6.2). In filtering, at time 𝑗 + 1, the prior is given by P(𝑣 † 𝑗+1 |𝑌 † 𝑗 ) which itself is defined by combining P(𝑣 † 𝑗 |𝑌 † 𝑗 ) with the dynamics model (6.1); thus we see that the prior for the filtering distribution at time 𝑗 + 1 is defined by combining the filtering distribution at time 𝑗 with the dynamics model (6.1). The posterior P(𝑣 † 𝑗+1 |𝑌 † 𝑗+1 ) is then found by conditioning the prior on observation 𝑦 † 𝑗+1 using the data model (6.2). Probabilistic versus State Estimation When trying to solve these Bayesian formulations of the filtering and smoothing problems, we will refer to probabilistic estimation. The underlying inverse problems are solved in a Bayesian fashion. On the other hand many optimization-based filtering and smoothing algorithms exist with the aim being simply to estimate the state itself, conditioned on data. We refer to this as state estimation. State estimation may be thought of as being analogous to MAP estimation, from Subsection 1.2.1, in the general Bayesian inverse problem setting. Indeed, for the smoothing problem, the MAP estimator associated with the posterior distribution on 𝑉 † is frequently employed as a point estimator of the state. The situation is more complicated for filtering; this is because the sequential nature of the problem means that we only have access to approximate priors, often in the form of point masses, or a Gaussian, at best, and iterative computation of the MAP estimator is not always natural. Nonetheless, useful algorithms for state estimation do exist in filtering and they will play an important role in what follows. Formulation Of Data Assimilation Formulation Of The Filtering Problem Definition 6.4. The filtering problem is to find, and update sequentially in 𝑗, the probability densities 𝜋 𝑗 (𝑣 † 𝑗 ) := P(𝑣 † 𝑗 |𝑌 † 𝑗 ) on R 𝑑 for 𝑗 = 1, . . . , 𝐽. We refer to 𝜋 𝑗 as the filtering distribution at time 𝑗. ♢ Filtering may be understood as the sequential interleaving of prediction, using the stochastic dynamics model (6.1), with inversion, using the data model (6.2). To explain this perspective it is helpful to introduce 1 ̂︀ 𝜋 𝑗+1 = P(𝑣 † 𝑗+1 |𝑌 † 𝑗 ). We also define the Gaussian likelihood l(𝑦|𝑢) = 𝒩 (𝑦; ℎ(𝑢), Γ), dictated by the data model (6.2a). Combining ̂︀ 𝜋 𝑗+1 as prior with this likelihood, evaluated at time 𝑗 + 1, Bayes theorem delivers the formula 𝜋 𝑗+1 (𝑣 † 𝑗+1 ) ∝ l(𝑦 † 𝑗+1 |𝑣 † 𝑗+1 )̂︀ 𝜋 𝑗+1 (𝑣 † 𝑗+1 ). It is useful to decompose the sequential updating 𝜋 𝑗 ↦ → 𝜋 𝑗+1 into the following two steps, expressed abstractly using maps on probability measures: Prediction Step: ̂︀ 𝜋 𝑗+1 = P𝜋 𝑗 . Analysis Step: 𝜋 𝑗+1 = A 𝑗 (̂︀ 𝜋 𝑗+1 ) := A(̂︀ 𝜋 𝑗+1 ; 𝑦 † 𝑗+1 ). (6.4) The combination of the prediction and analysis steps is shown schematically in Figure 6 .2 and leads to the update 𝜋 𝑗+1 = A 𝑗 (P𝜋 𝑗 ). (6.5) Here P is a linear map, 2 defining the Markov process underlying the stochastic dynamics model, and A 𝑗 = A(•; 𝑦 † 𝑗+1 ) is a nonlinear likelihood map defined by application of Bayes Theorem 1.3 to solve the inverse problem, defined by the likelihood l(𝑦 † 𝑗+1 |𝑣 † 𝑗+1 ), with prior ̂︀ 𝜋 𝑗+1 . The linear prediction operator P is defined as follows: P𝜋(𝑢) = 1 √︁ (2𝜋) 𝑑 det Σ ∫︁ exp (︂ - 1 2 |𝑢 -Ψ(𝑣)| 2 Σ )︂ 𝜋(𝑣) 𝑑𝑣 (6.6a) = ∫︁ P(𝑢|𝑣)𝜋(𝑣) 𝑑𝑣, (6.6b) 1 Notice that ̂︀ 𝜋1 := P(𝑣 † 1 ) with the convention that 𝑌 † 0 = ∅. 2 Strictly speaking P is linear when viewed as acting on the vector space 𝐿 1 (R 𝑑 ; R); the space of probability density functions is a subset of this vector space, but is not a linear subspace. where the expression for the Gaussian conditional probability density function P(𝑢|𝑣) = 𝒩 (𝑢; Ψ(𝑣), Σ) is dictated by the given stochastic dynamics model (6.1a). If the state is distributed according to 𝜋 at time 𝑗, then P𝜋 represents the distribution of the state at time 𝑗 + 1 when that state is defined by the Markovian dynamics (6.1a). To define the analysis operator, notice that any probability density function 𝜋 on R 𝑑 can be extended to a joint probability density 𝛾 𝜋 on state-data space R 𝑑 × R 𝑘 as follows: 𝛾 𝜋 (𝑢, 𝑦) = 1 √︁ (2𝜋) 𝑘 det Γ exp (︂ - 1 2 ⃒ ⃒ 𝑦 -ℎ(𝑢) ⃒ ⃒ 2 Γ )︂ 𝜋(𝑢) (6.7a) = l(𝑦|𝑢)𝜋(𝑢) ; (6.7b) here, recall, l(𝑦|𝑢) is dictated by the data model (6.2a). Then we define the nonlinear operator A(•; 𝑦) by A(𝜋; 𝑦)(𝑢) = 𝛾 𝜋 (𝑢, 𝑦) ∫︀ R 𝑑 𝛾 𝜋 (𝑢, 𝑦) 𝑑𝑢 . ( 6.8) Given a prior 𝜋 on the state at time 𝑗 + 1, A 𝑗 (𝜋) = A(𝜋; 𝑦 † 𝑗+1 ) represents the posterior distribution of the state at time 𝑗 + 1 determined by data model (6.2a) and observed data 𝑦 † 𝑗+1 . Formulation Of The Smoothing Problem Recall definitions of 𝑉 † , 𝑌 † and 𝑌 † 𝑗 from (6.3). We note that the stochastic dynamics model (6.1) defines a probability density function on 𝑉 † , with density 𝜌(𝑉 † ), defined through the Markovian structure as 𝜌(𝑉 † ) ∝ exp (︁ - 1 2 |𝑣 † 0 -𝑚 0 | 2 𝐶 0 - 1 2 𝐽-1 ∑︁ 𝑗=0 |𝑣 † 𝑗+1 -Ψ (︀ 𝑣 † 𝑗 )︀ | 2 Σ )︁ . (6.9) Definition 6.5. The smoothing problem is to find the probability density Π 𝑌 † (𝑉 † ) := P(𝑉 † |𝑌 † ) on R 𝑑(𝐽+1) for some fixed integer 𝐽. We refer to Π 𝑌 † as the smoothing distribution. ♢ To make connection with the Bayesian inverse problems of Chapter 1 we define 𝜂 † := {𝜂 † 1 , . . . , 𝜂 † 𝐽 } ∈ R 𝑘𝐽 , noting that 𝜂 † ∼ 𝒩 (0, Γ), where Γ is block diagonal with Γ in each diagonal block. If we then define h : R 𝑑(𝐽+1) → R 𝑘𝐽 by h(𝑉 † ) := {︀ ℎ(𝑣 † 1 ), . . . , ℎ(𝑣 † 𝐽 ) }︀ then the data model may be written as 𝑌 † = h(𝑉 † ) + 𝜂 † . We are interested in finding P(𝑉 † |𝑌 † ) = Π 𝑌 † (𝑉 † ). We may apply Bayes Theorem 1.3, with prior 𝜌 noting that, under this prior, 𝑉 † ⊥ ⊥ 𝜂 † , the standard setting for Bayesian inversion from Chapter 1. We obtain Π 𝑌 † (𝑉 † ) ∝ exp (︁ - 1 2 |𝑌 † -h(𝑉 † )| 2 Γ )︁ 𝜌(𝑉 † ), (6.10) also a probability density function in 𝒫(R 𝑑(𝐽+1) ). Recall from Chapter 1 the likelihood which here has the form l(𝑌 † |𝑉 † ) ∝ exp (︁ - 1 2 |𝑌 † -h(𝑉 † )| 2 Γ )︁ , ( 6.11) enabling us to write (6.10) in the form Π 𝑌 † (𝑉 † ) ∝ l(𝑌 † |𝑉 † )𝜌(𝑉 † ). (6.12) Remark 6.6. Because filtering is defined by conditioning on data arriving sequentially, such algorithms may be used online, the probability distribution over states being updated every time a new data point 𝑦 † 𝑗 arrives. Smoothing gives rise to methodologies that are most naturally used, in their most basic form, in an offline fashion. However, smoothing algorithms may also be used sequentially, in block form with respect to discrete time index 𝑗, as we now explain. In the above we assumed data 𝑦 † 𝑗 given on the index set 𝑗 ∈ {1, . . . , 𝐽} and solved the smoothing problem on 𝑗 ∈ {0, . . . , 𝐽}. However this can be shifted to solve the inverse problem on any interval, for example containing 𝐾 + 1 points for some integer 𝐾, using data on an interval excluding the first point, but provided with an initial Gaussian distribution at the first point. Let 𝐾 ≪ 𝐽 be such an integer. We first solve the smoothing problem defined on the index set 𝑗 ∈ {0, . . . , 𝐾}. Taking the solution at time 𝐾 as starting point we may then solve a smoothing problem on index set 𝑗 ∈ {𝐾, . . . , 2𝐾}. This idea may be iterated, working on index set 𝑗 ∈ {ℓ𝐾, . . . , (ℓ+1)𝐾}, then on index set 𝑗 ∈ {(ℓ + 1)𝐾, . . . , (ℓ + 2)𝐾} and so on. If ℓ = 0, . . . , 𝐿 -1 then this will deliver a solution to the data assimilation problem for 𝑣 † 𝑗 defined on 𝑗 ∈ {0, . . . , 𝐽} where 𝐽 = 𝐿𝐾. Such methods are known as fixed-interval smoothers and may be thought of as blending aspects of smoothing and filtering, when iterated over ℓ. At overlap points, which are a multiple of 𝐾, some form of Gaussian projection (see Remark 2.11) will be needed to restart the process on the next time-interval as the assumption made above is that the initial condition is Gaussian; however, this assumption on the initial distribution can be relaxed. The case 𝐾 = 1 delivers the filtering distribution if the Gaussian projection is omitted. ♢ Filtering Algorithms We formulate various filtering algorithms. They all take the form of nonautonomous dynamical systems, defined on state spaces of varying dimension. They are nonautonomous because they are driven by the observation sequence {𝑦 † 𝑗 } 𝑗∈N ; the dimension varies depending on whether a simple state estimate is evolved (3DVar), a mean and covariance are evolved (Kalman filter or the extended Kalman filter), or an ensemble is evolved (ensemble Kalman filter, bootstrap particle filter, optimal particle filter). We conclude the section with discussion of a commonly used heuristic methodology for evaluating filtering algorithms. In general the filtering distribution is an infinite dimensional object. In order to approximate it and facilitate probabilistic estimation some form of finite-dimensionalization is needed. Most approximation methods use either empirical particle or Gaussian approximations, or a combination of both ideas. Kalman Filter Suppose that at time 𝑗, Ψ(•) = 𝐴 𝑗 • and ℎ(•) = 𝐻 𝑗 •: the model and observation operator are linear, but can change in time. (This is a slight generalization of the filtering setting we focus on here, because Ψ(•) and ℎ(•) were assumed independent of time 𝑗.) Then the solution to the filtering problem is Gaussian and 𝜋 𝑗 = 𝒩 (𝑣 𝑗 , 𝐶 𝑗 ), ̂︀ 𝜋 𝑗+1 = 𝒩 (̂︀ 𝑣 𝑗+1 , ̂︀ 𝐶 𝑗+1 ). The update rules for the mean 𝑣 𝑗 and covariance 𝐶 𝑗 are given by the Kalman filter: the mean is initialized at 𝑣 0 = 𝑚 0 and then updated according to ︀ 𝑣 𝑗+1 = 𝐴 𝑗 𝑣 𝑗 , ( 6.13a ) 𝑣 𝑗+1 = ̂︀ 𝑣 𝑗+1 + 𝐾 𝑗+1 (︀ 𝑦 † 𝑗+1 -𝐻 𝑗+1 ̂︀ 𝑣 𝑗+1 )︀ ; (6.13b) here the Kalman gain 𝐾 𝑗+1 is determined by the update rule for the covariances ︀ 𝐶 𝑗+1 = 𝐴 𝑗 𝐶 𝑗 𝐴 ⊤ 𝑗 + Σ, (6.14a ) 𝐾 𝑗+1 = ̂︀ 𝐶 𝑗+1 𝐻 ⊤ 𝑗+1 (︀ 𝐻 𝑗+1 ̂︀ 𝐶 𝑗+1 𝐻 ⊤ 𝑗+1 + Γ )︀ -1 , (6.14b ) 𝐶 𝑗+1 = (𝐼 -𝐾 𝑗+1 𝐻 𝑗+1 ) ̂︀ 𝐶 𝑗+1 . (6.14c) Note that the update for the covariance evolves independently of the update for the mean. Furthermore, the covariance update is independent of the data. Given the covariance, 𝑣 𝑗 provides an state estimator for 𝑣 † 𝑗 , given the observations 𝑌 † 𝑗 ; the Kalman filter updates this estimator sequentially. Furthermore, the Gaussian 𝒩 (𝑣 𝑗 , 𝐶 𝑗 ) is actually equal to the filtering distribution 𝜋 𝑗 and thus constitutes exact probabilistic estimation. 3DVar To motivate 3DVar we consider the form of the Kalman filter when the covariance is in steady state. We note that if 𝐴 𝑗 = 𝐴 and 𝐻 𝑗 = 𝐻 are constant in time, and the covariance is in steady state, then 𝐶 𝑗+1 = 𝐶 𝑗 = 𝐶 ∞ and ̂︀ 𝐶 𝑗+1 = ̂︀ 𝐶 𝑗 = ̂︀ 𝐶 ∞ . Under appropriate controllability and observability assumptions, the steady-state covariance and gain can be obtained by finding the unique solution ( ̂︀ 𝐶 ∞ , 𝐾 ∞ ) to the equations ︀ 𝐶 ∞ = 𝐴(𝐼 -𝐾 ∞ 𝐻) ̂︀ 𝐶 ∞ 𝐴 ⊤ + Σ, (6.15a ) 𝐾 ∞ = ̂︀ 𝐶 ∞ 𝐻 ⊤ (𝐻 ̂︀ 𝐶 ∞ 𝐻 ⊤ + Γ) -1 , (6.15b) and setting 𝐶 ∞ = (𝐼 -𝐾 ∞ 𝐻) ̂︀ 𝐶 ∞ . (6.16) Using the steady-state Kalman gain from (6.15b), the state updates become ︀ 𝑣 𝑗+1 = 𝐴𝑣 𝑗 , ( 6.17a ) 𝑣 𝑗+1 = ̂︀ 𝑣 𝑗+1 + 𝐾 ∞ (︀ 𝑦 † 𝑗+1 -𝐻 ̂︀ 𝑣 𝑗+1 )︀ . (6.17b) We note the form of this update of the mean 𝑣 𝑗 ↦ → 𝑣 𝑗+1 , viewed as a state estimator: it comprises a prediction step 𝑣 𝑗 ↦ → ̂︀ 𝑣 𝑗+1 , in the form of the stochastic dynamics model (6.1), and an analysis step ̂︀ 𝑣 𝑗+1 ↦ → 𝑣 𝑗+1 which incorporates data given by the data model (6.2). Combining the prediction and data incorporation steps we obtain 𝑣 𝑗+1 = (𝐼 -𝐾 ∞ 𝐻)𝐴𝑣 𝑗 + 𝐾 ∞ 𝑦 † 𝑗+1 . ( 6.18) Motivated by the form (6.17) of the mean update equations for the Kalman filter, when the gain is in steady state, we propose the following generalization to the setting of nonlinear (Ψ, ℎ) : ̂︀ 𝑣 𝑗+1 = Ψ(𝑣 𝑗 ) + 𝑠𝜉 𝑗 , ( 6.19a ) 𝑣 𝑗+1 = ̂︀ 𝑣 𝑗+1 + 𝐾 (︀ 𝑦 † 𝑗+1 -ℎ(̂︀ 𝑣 𝑗+1 ) )︀ . (6.19b) The algorithm is initialized as 𝑣 0 , typically chosen from Gaussian 𝒩 (𝑚 0 , 𝐶 0 ), the known distribution of 𝑣 † 0 . In the setting where 𝐻 is linear the algorithm can be written in the form 𝑣 𝑗+1 = (𝐼 -𝐾𝐻)Ψ(𝑣 𝑗 ) + 𝐾𝑦 † 𝑗+1 + 𝑠𝜉 𝑗 . (6.20) The most basic form of (6.19) will use 𝑠 = 0; the algorithm should be viewed as a form of state estimator. When 𝑠 = 0 and Ψ(•) = 𝐴• is linear (6.20) recovers the steady state Kalman update (6.18). Remark 6.7. We have included the case 𝑠 = 1 primarily so that we can make a connection to the optimal particle filter introduced in Subsection 6.3.7. ♢ Remark 6.8. We make some comments on the terminology 3DVar that we employ here. The approach is in fact more properly termed cycled 3DVar. In this context the 3DVar component of the nomenclature refers to the analysis step (6.19b), which is cycled with the prediction step (6.19a) . For linear observation operator we show below that the analysis step has an optimization formulation, the source of the \"Var\" terminology. The use of \"3D\" refers to the fact that, in weather forecasting where the methodology was introduced, the optimization problem is for a field in three physical space dimensions. ♢ We conclude this discussion of 3DVar by formulating the analysis step via an optimization problem. To appreciate this connection to optimization we consider the case where Ψ is allowed to be nonlinear, but ℎ(•) = 𝐻• is linear. The predict-thenoptimize viewpoint then leads to 3DVar above if 𝑣 𝑗+1 is computed from (̂︀ 𝑣 𝑗+1 , 𝑦 † 𝑗+1 ) by solving the optimization problem J 𝑗 (𝑣) = 1 2 |𝑣 -̂︀ 𝑣 𝑗+1 | 2 ︀ 𝐶 + 1 2 |𝑦 † 𝑗+1 -𝐻𝑣| 2 Γ , (6.21a) 𝑣 𝑗+1 ∈ arg min 𝑣∈R 𝑑 J 𝑗 (𝑣). (6.21b) This minimization is equivalent to (6.19b ), in the case ℎ(•) = 𝐻•, provided that 𝐾 = ̂︀ 𝐶𝐻 ⊤ (𝐻 ̂︀ 𝐶𝐻 ⊤ + Γ) -1 ; (6.22) This should be compared with formula (6.15b) for the steady-state Kalman gain. In particular the formula suggests a methodology for choosing 𝐾: instead choose an estimate of the uncertainty in the prediction, ̂︀ 𝐶, and use this to define 𝐾 from (6.22) . This approach to modeling ̂︀ 𝐶, and then deducing 𝐾, is natural because the uncertainty in the prediction is an interpretable quantity. Extended Kalman Filter (ExKF) The extended Kalman filter (ExKF) generalizes the Kalman filter to vector fields defining nonlinear dynamics (Ψ) and nonlinear observation operators (ℎ) by linearizing these functions in order to compute the gain and propagate the covariance. The output 𝑣 𝑗 from ExKF provides state estimation for 𝑣 † 𝑗 , given 𝑌 † 𝑗 . The Gaussian 𝒩 (𝑣 𝑗 , 𝐶 𝑗 ) enables probabilistic estimation to be undertaken. For this reason we will refer, for this algorithm, to 𝑣 𝑗 as the mean, as for the Kalman filter. We define the Jacobians of the functions and evaluate them at the outputs 𝑣 𝑗 and ̂︀ 𝑣 𝑗 of a putative filtering algorithm: 𝐴 𝑗 := 𝐷Ψ(𝑣 𝑗 ), (6.23a ) 𝐻 𝑗 := 𝐷ℎ(̂︀ 𝑣 𝑗 ). (6.23b) These Jacobians may be known analytically; otherwise, auto-differentiation (Section 15.6) can be used to obtain them. The mean is initialized at 𝑣 0 = 𝑚 0 and the update is then performed using the nonlinear Ψ and ℎ, similarly to 3DVar (6. 19 ) with 𝑠 = 0, but with an evolving gain: ︀ 𝑣 𝑗+1 = Ψ(𝑣 𝑗 ), (6.24a ) 𝑣 𝑗+1 = ̂︀ 𝑣 𝑗+1 + 𝐾 𝑗+1 (︀ 𝑦 † 𝑗+1 -ℎ(̂︀ 𝑣 𝑗+1 ) )︀ . (6.24b) The computation of 𝐾 𝑗+1 , and the covariances involved in its definition, uses (6.14a), but with evolving (𝐴 𝑗 , 𝐻 𝑗 ) defined by (6.23): ︀ 𝐶 𝑗+1 = 𝐴 𝑗 𝐶 𝑗 𝐴 ⊤ 𝑗 + Σ, (6.25a ) 𝐾 𝑗+1 = ̂︀ 𝐶 𝑗+1 𝐻 ⊤ 𝑗+1 (︀ 𝐻 𝑗+1 ̂︀ 𝐶 𝑗+1 𝐻 ⊤ 𝑗+1 + Γ )︀ -1 , (6.25b ) 𝐶 𝑗+1 = (𝐼 -𝐾 𝑗+1 𝐻 𝑗+1 ) ̂︀ 𝐶 𝑗+1 . (6.25c) Remark 6.9. The ExKF recovers the true filtering distribution in the case of linear Ψ and ℎ, where it reduces to the standard Kalman filter. It is often used beyond the linear Gaussian setting: for example when Ψ and ℎ are close to linear; or when small covariances are assumed in both the dynamics and data models. In both these settings linearization may be used to justify use of the ExKF. ♢ Unscented Kalman Filter The Kalman filter solves the filtering problem when the exact distribution on state given observations is Gaussian; thus only a mean and covariance needs to be computed. The ExKF approximates the general filtering distribution by a Gaussian, using linearization to derive the joint evolution of mean and covariance. The unscented Kalman filter (UKF) also makes a Gaussian ansatz, but does not require linearization of the vector fields defining the filter; instead it uses the projection G onto Gaussian measures, defined in Remark 2.11. The UKF algorithm may be written in the following form, which approximates the exact prediction-analysis cycle from (6.4): Prediction Step: ̂︀ 𝜋 𝑗+1 = P𝜋 𝑗 . Analysis Step: 𝜋 𝑗+1 = U(̂︀ 𝜋 𝑗+1 ; 𝑦 † 𝑗+1 ). (6.26) Here, for 𝛾 𝜋 (𝑢, 𝑦) defined by (6.7), we define the unscented approximation of the analysis operator by U(𝜋; 𝑦 † )(𝑢) = G𝛾 𝜋 (𝑢, 𝑦 † ) ∫︀ R 𝑑 G𝛾 𝜋 (𝑢, 𝑦 † ) 𝑑𝑢 . (6.27) The preceding gives an iteration which remains in the manifold of Gaussians; it can hence be represented in terms of propagation of means and covariances. As for the ExKF it reduces to the Kalman filter if the vector fields defining the filtering problem are linear. In practice the integrations required to define the Gaussian projection are approximated by quadratures; it is the resulting method, after quadrature, that is typically referred to as the unscented Kalman filter. Ensemble Kalman Filter (EnKF) Despite the simplicity of the ExKF, computing covariances when 𝑑 or 𝑘 is large can be prohibitively expensive. This motivates the ensemble methods we describe next. In this subsection our first, and primary, goal is to introduce the ensemble Kalman filter (EnKF) algorithm. We then also discuss inflation and localization, two important practical modifications of the methodology. The EnKF Algorithm The content of Remark 6.8, concerning 3DVar, is to shift the problem of choosing 𝐾 to one of choosing ̂︀ 𝐶, an estimate of the uncertainty in the predictions. In the ExKF uncertainty in predictions is achieved via a linearization. The EnKF builds on this idea by running an ensemble of 𝑁 copies of 3DVar-like algorithms, with a time-dependent gain 𝐾 𝑗+1 ; this gain is estimated empirically by computing the empirical covariances of the predicted states {̂︀ 𝑣 (𝑛) 𝑗+1 } 𝑁 𝑛=1 and {ℎ (︀ ̂︀ 𝑣 (𝑛) 𝑗+1 )︀ } 𝑁 𝑛=1 , their mappings under ℎ. The ensemble {𝑣 (𝑛) 𝑗 } 𝑁 𝑛=1 is mapped to {𝑣 (𝑛) 𝑗+1 } 𝑁 𝑛=1 according to the following algorithm: ︀ 𝑣 (𝑛) 𝑗+1 = Ψ(𝑣 (𝑛) 𝑗 ) + 𝜉 (𝑛) 𝑗 , 𝑛 = 1, . . . , 𝑁, (6.28a) 𝑣 (𝑛) 𝑗+1 = ̂︀ 𝑣 (𝑛) 𝑗+1 + 𝐾 𝑗+1 (︀ 𝑦 † 𝑗+1 -𝜂 (𝑛) 𝑗+1 -ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) )︀ , 𝑛 = 1, . . . , 𝑁. (6.28b) Here 𝜉 (𝑛) 𝑗 ∼ 𝒩 (0, Σ), 𝜂 (𝑛) 𝑗+1 ∼ 𝒩 (0, Γ) are independent sequences of i.i.d. random vectors with respect to both 𝑗 and 𝑛, and the two sequences themselves are independent of one another. The gain matrix 𝐾 𝑗+1 is calculated according to ︀ 𝑚 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 ̂︀ 𝑣 (𝑛) 𝑗+1 , (6.29a) ︀ ℎ 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ), (6.29b) ︀ 𝐶 𝑣ℎ 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 (︀ ̂︀ 𝑣 (𝑛) 𝑗+1 -̂︀ 𝑚 𝑗+1 )︀ ⊗ (︀ ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) -̂︀ ℎ 𝑗+1 )︀ , ( 6.29c) ︀ 𝐶 ℎℎ 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 (︀ ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) -̂︀ ℎ 𝑗+1 )︀ ⊗ (︀ ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) -̂︀ ℎ 𝑗+1 )︀ , ( 6.29d) ︀ 𝐶 𝑦𝑦 𝑗+1 = ̂︀ 𝐶 ℎℎ 𝑗+1 + Γ, (6.29e ) 𝐾 𝑗+1 = ̂︀ 𝐶 𝑣ℎ 𝑗+1 (︀ ̂︀ 𝐶 𝑦𝑦 𝑗+1 )︀ -1 . (6.29f) The ensemble mean 𝑚 𝑗 given by 𝑚 𝑗 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝑣 (𝑛) 𝑗 , may be viewed as providing state estimation. The ensemble itself may be used to furnish a form of probabilistic estimation; this is achieved by simply forming an empirical measure from the ensemble members: we approximate 𝜋 𝑗 ≈ 𝜋 EnKF 𝑗 where 𝜋 EnKF 𝑗 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑣 (𝑛) 𝑗 . (6.30) It is also possible, and indeed natural, to approximate 𝜋 𝑗 with a Gaussian computed using empirical moments. Recall the notation for G, the projection onto Gaussian measures, defined in Remark 2.11. To define a Gaussian approximation of 𝜋 𝑗 from the EnKF we set 𝜋 EnKF 𝑗 = G (︁ 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑣 (𝑛) 𝑗 )︁ . ( 6 .31) Remark 6.10. Conditions under which (6.30) provides a good approximation of the true filtering distribution are discussed in the bibliography Section 6.8, and revolve around Gaussian approximations. When the true filtering distribution is far from Gaussian the method will not provide a good approximation. ♢ Remark 6.11. We note that in the case where ℎ(•) = 𝐻• (and is hence linear) we may calculate the gain through estimation of a single covariance, as follows: ︀ 𝐶 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 (︀ ̂︀ 𝑣 (𝑛) 𝑗+1 -̂︀ 𝑚 𝑗+1 )︀ ⊗ (︀ ̂︀ 𝑣 (𝑛) 𝑗+1 -̂︀ 𝑚 𝑗+1 )︀ , ( 6.32a ) 𝐾 𝑗+1 = ̂︀ 𝐶 𝑗+1 𝐻 ⊤ (︀ 𝐻 ̂︀ 𝐶 𝑗+1 𝐻 ⊤ + Γ )︀ -1 . (6.32b) This should be compared with the gain (6.22), used in 3DVar. The analysis mean and covariance can be estimated as 𝑚 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝑣 (𝑛) 𝑗+1 , (6.33a) 𝐶 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 (︀ 𝑣 (𝑛) 𝑗+1 -𝑚 𝑗+1 )︀ ⊗ (︀ 𝑣 (𝑛) 𝑗+1 -𝑚 𝑗+1 )︀ . ( 6.33b) ♢ Remark 6.12. The particle system (6.28) is sometimes referred to as the perturbed observation form of the EnKF; this is because of the presence of the noise 𝜂 (𝑛) 𝑗+1 which is sometimes viewed by practitioners as perturbing the observation 𝑦 † 𝑗+1 . We present an alternative way of thinking about the algorithm, in terms of simulated observations. Equations (6.28) can be written in the equivalent form ︀ 𝑣 (𝑛) 𝑗+1 = Ψ(𝑣 (𝑛) 𝑗 ) + 𝜉 (𝑛) 𝑗 , 𝑛 = 1, . . . , 𝑁, (6.34a) ︀ 𝑦 (𝑛) 𝑗+1 = ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) + 𝜂 (𝑛) 𝑗 , 𝑛 = 1, . . . , 𝑁, (6.34b) 𝑣 (𝑛) 𝑗+1 = ̂︀ 𝑣 (𝑛) 𝑗+1 + 𝐾 𝑗+1 (︀ 𝑦 † 𝑗+1 -̂︀ 𝑦 (𝑛) 𝑗+1 )︀ , 𝑛 = 1, . . . , 𝑁. (6.34c) This has the desirable interpretation of predicting both the state ̂︀ 𝑣 (𝑛) 𝑗+1 and the observation ︀ 𝑦 (𝑛) 𝑗+1 and then correcting the predicted state by a quantity linearly related to the discrepancy between the observation 𝑦 † 𝑗+1 and the predicted observation. The difference between the actual and predicted observation 𝑖 (𝑛) 𝑗+1 = 𝑦 † 𝑗+1 -̂︀ 𝑦 (𝑛) 𝑗+1 (6.35) is known as the innovation. Without the addition of these perturbations to the observations, the covariance in (6.32b) does not converge to that of the Kalman filter in the large ensemble limit 𝑁 = ∞. ♢ EnKFs are widely used in high-dimensional problems because they do not suffer from the weight collapse that plagues the particle filters of Subsections 6.3.6 and 6.3.7. However, they often need inflation and localization to perform well for high-dimensional systems, for chaotic dynamical systems, and for small ensemble sizes. Inflation and localization are discussed in the following two paragraphs. Inflation For simplicity we confine the discussion to the case of linear observation operator ℎ(•) = 𝐻 • . The EnKF gain is then determined by ̂︀  𝐶 𝑗+1 (𝑁 ), as explained in Remark 6.11. We assume that the covariance at infinite sample size 𝑁 = ∞ , ̂︀ 𝐶 𝑗+1 , leads to the optimal gain; this is provably true for linear Ψ, as discussed in the bibliography. From this perspective, EnKFs suffer from sampling error due to finite ensemble size 𝑁 . When sampling error in the forecast covariance matrix ︀ 𝐶 𝑗+1 (𝑁 ) leads to underestimation of the analysis covariance 𝐶 𝑗+1 , this can lead to filter divergence, whereby repeated underestimation of the forecast covariance causes the filter to put increasingly more weight on the forecasts than observations, eventually becoming unresponsive to observations. We provide two examples to illustrate the underestimation of the analysis covariance. Example 6.13. Suppose that we assimilate a scalar observation of the 𝑏 th variable into the 𝑎 th variable. Thus 𝐻 is a 1 × 𝑑 vector with entry 1 in the 𝑏 th position and zeros elsewhere. Since the phenomenon we wish to illustrate occurs at any fixed time, we simply let 𝐶 and ̂︀ 𝐶 represent the desired infinite sample size covariances, and 𝐶(𝑁 ) and ̂︀ 𝐶(𝑁 ) the finite sample size estimators coming from the EnKF. Now, we claim that E[ (︀ 𝐶(𝑁 ) )︀ 𝑎𝑎 ] - (︀ ̂︀ 𝐶(𝑁 ) )︀ 𝑎𝑎 = -( ̂︀ 𝐶(𝑁 )) 2 𝑎𝑏 (︀ ̂︀ 𝐶(𝑁 ) )︀ 𝑏𝑏 + (Γ) 𝑏𝑏 , ( 6.36) where the expectation is taken with respect to the observation perturbations 𝜂 (𝑛) 𝑗 . References that substantiate this claim can be found in the bibliography Section 6.8. Next, suppose that variables with indices 𝑎 and 𝑏 are in fact uncorrelated, so that ( ̂︀ 𝐶) 𝑎𝑏 = 0. When estimated with a finite ensemble, in an implementation of the EnKF, the estimated ( ̂︀ 𝐶) 𝑎𝑏 ̸ = 0, due to sampling error resulting from a finite ensemble. This leads to a spurious reduction in the analysis covariance, with respect to its value in the infinite-ensemble case. ♢ Example 6.14. Inflation is also used to compensate for an unknown, or ignored, Σ, a form of model error. If the 𝜉 (𝑛) 𝑗 are excluded in (6.28a), then the covariance ̂︀ 𝐶 𝑗+1 will be underestimated in (6.32a) . ♢ The underestimation of the analysis covariance can be mitigated by inflating the covariance of the forecast ensemble, although it can also be applied to the analysis ensemble instead. The most common form of inflation is multiplicative inflation, whereby the forecast ensemble is modified as ︀ 𝑣 (𝑛) 𝑗+1 → ̂︀ 𝑚 𝑗+1 + 𝛼(̂︀ 𝑣 (𝑛) 𝑗+1 -̂︀ 𝑚 𝑗+1 ), (6.37) with 𝛼 ≥ 1 being the inflation parameter. This corresponds to scaling the covariance ︀ 𝐶 𝑗+1 by 𝛼 2 . Theoretical results on the impact of sampling error on EnKFs, including results on the optimal value of the inflation parameter under assumptions on the model dynamics, are referenced in the bibliography Section 6.8. Localization In addition to the undersampling effects from the previous paragraph, further negative impacts of sampling errors on EnKFs can arise in spatially extended systems. In such physical systems there is typically decay of correlations with distance; but sampling error can induce spurious long-range correlations. In the empirical covariance matrices computed by the EnKFs, nearby points in space are likely to be truly correlated, while ones that are farther apart may only have spurious correlations dominated by sampling error. These spurious correlations not only lead to an underestimation of the analysis covariance, as discussed in Example 6.13, but also lead to degraded state estimation due to spurious information transfer between uncorrelated variables. Localization is introduced to damp covariances, within the empirical covariance matrices computed by the EnKF, according to physical distance (for discretization of PDE problems) or index distance (for large ODEs with notion of index locality). A common way of implementing localization is by taking the Hadamard product foot_6 of the empirical covariance with a localization matrix 𝐿: ︀ 𝐶 𝑗+1 → 𝐿 ∘ ̂︀ 𝐶 𝑗+1 . (6.38) Typically, 𝐿 is constructed such that the covariance between between variables indexed by 𝑎 and by 𝑏 is exponentially damped according to a measure of distance foot_7 d(𝑎, 𝑏) between them, scaled by characteristic length scale ℓ: (𝐿) 𝑎𝑏 = 𝑒 -d(𝑎,𝑏) 2 /ℓ 2 . (6.39) The choice of ℓ will depend on the ensemble size, the correlation scales in the system, and the time between analysis steps (since this will control how far information has had time to propagate). Localization can also be implemented in other ways that scale better to high-dimensional problems, such as domain localization and observation localization, discussed in the bibliography Section 6.8; we will restrict our discussion to covariance localization (6.38). Remark 6.15. If ℎ(•) is linear, the localized ̂︀ 𝐶 𝑗+1 is used to compute the gain according to (6.32b). If ℎ(•) is nonlinear, then localization needs to be applied to both ̂︀ 𝐶 𝑣ℎ 𝑗+1 and ︀ 𝐶 ℎℎ 𝑗+1 . If function ℎ(•) is such that each observation has a corresponding spatial location, then localization can be applied as discussed above. If some observations correspond to, for example, spatially integrated quantities, then localization is not straightforward to apply; see the bibliography Section 6.8. ♢ Remark 6.16. Besides the sampling error considerations discussed above, localization is also important in increasing the rank of the forecast covariance matrix. When the ensemble size 𝑁 is less than the system dimension 𝑑, the forecast covariance will be rank deficient. This implies that the analysis ensemble will lie in the span of the forecast ensemble, and thus that analysis increments are restricted to an 𝑁 -dimensional subspace. Localization typically increases the rank of the forecast covariance, bypassing this subspace property. ♢ Bootstrap Particle Filter The starting point for the bootstrap particle filter (BPF) is the factorization (6.4) of the filtering update into two steps: prediction composed with analysis. The method starts with an empirical approximation to the filtering distribution 𝜋 𝑗 ≈ 𝜋 BPF 𝑗 . Here 𝜋 BPF 0 = 𝜋 0 = 𝒩 (𝑚 0 , 𝐶 0 ) and, for 𝑗 ∈ N, 𝜋 BPF 𝑗 = 𝑁 ∑︁ 𝑛=1 𝑤 (𝑛) 𝑗 𝛿 ̂︀ 𝑣 (𝑛) 𝑗 . (6.40) To define this approximation we evolve the particles ̂︀ 𝑣 (𝑛) 𝑗 (the prediction step) and the weights 𝑤 (𝑛) 𝑗 (the analysis step). This is done according to ︀ 𝑣 (𝑛) 𝑗+1 = Ψ (︀ 𝑣 (𝑛) 𝑗 )︀ + 𝜉 (𝑛) 𝑗 , 𝑣 (𝑛) 𝑗 i.i.d. ∼ 𝜋 BPF 𝑗 , (6.41a) ℓ (𝑛) 𝑗+1 = exp (︂ - 1 2 ⃒ ⃒ 𝑦 † 𝑗+1 -ℎ (︀ ̂︀ 𝑣 (𝑛) 𝑗+1 )︀⃒ ⃒ 2 Γ )︂ , ( 6.41b ) 𝑤 (𝑛) 𝑗+1 = ℓ (𝑛) 𝑗+1 ⧸︁(︁ 𝑁 ∑︁ 𝑚=1 ℓ (𝑚) 𝑗+1 )︁ . (6.41c) Here 𝜉 (𝑛) 𝑗 ∼ 𝒩 (0, Σ) are Gaussian random variables, i.i.d. with respect to both 𝑛 and 𝑗, generalizing the Assumption 6.1 concerning the underlying signal-observation model. At each step, the BPF alternates sampling from the Markovian dynamics, the prediction P, implemented in (6.41a), followed by an application of Bayes Theorem, the analysis A(•; 𝑦 † 𝑗+1 ), which is implemented using importance sampling with weights as in (6.41c). With this we obtain 𝜋 BPF 𝑗+1 = 𝑁 ∑︁ 𝑛=1 𝑤 (𝑛) 𝑗+1 𝛿 ̂︀ 𝑣 (𝑛) 𝑗+1 . ( 6 .42) The map from (6.40) to (6.42) approximates the true filter update (6.5). The resulting algorithm is known as the BPF. It performs probabilistic estimation: (6.40) delivers an approximation 𝜋 BPF 𝑗 to the true filtering distribution 𝜋 𝑗 . Remark 6.17. The BPF is provably convergent to the true filtering distribution as 𝑁 → ∞, under quite general conditions, including filtering distributions that are far from Gaussian; this should be contrasted with ensemble methods as discussed in Remark 6.10. On the other hand the BPF can perform poorly in high dimensions, suffering from weight collapse, a phenomenon whereby, for 𝑗 large, all weights {𝑤 (𝑛) 𝑗 } 𝑁 𝑛=1 but one are close to zero; thus the ensemble apporoximation 𝜋 BPF 𝑗 is dominated by one particle. Ensemble methods have been designed to perform robustly in high dimensions, and have equal weights on all particles; again see Remark 6.10 for further discussion of methods used to impart robustness on the EnKF in high dimensions. ♢ Optimal Particle Filter The optimal particle filter (OPF) is also a probabilistic estimation methodology and like the BPF it is based on a particle approximation of the filter 𝜋 𝑗 . The OPF differs from the BPF by reversing the order of the Bayesian inference step and the sampling from a Markovian kernel based on the dynamics. The starting point to understand the OPF is the following identity, derivable from a general hidden Markov model for (𝑣 † 𝑗 , 𝑦 † 𝑗+1 ) 𝑗∈Z + , and from (6.1), (6.2) in particular: P(𝑣 † 𝑗+1 |𝑌 † 𝑗+1 ) = ∫︁ R 𝑑 P(𝑣 † 𝑗+1 |𝑣 † 𝑗 , 𝑦 † 𝑗+1 ) P(𝑦 † 𝑗+1 |𝑣 † 𝑗 ) P(𝑦 † 𝑗+1 |𝑌 † 𝑗 ) P(𝑣 † 𝑗 |𝑌 † 𝑗 ) 𝑑𝑣 † 𝑗 . Because it simplifies notation, we drop explicit reference to the conditioning on 𝑌 † 𝑗 in what follows, and because it is not needed to describe the methodology, we drop the normalization, writing the preceding identity as P(𝑣 † 𝑗+1 |𝑦 † 𝑗+1 ) ∝ ∫︁ R 𝑑 P(𝑣 † 𝑗+1 |𝑣 † 𝑗 , 𝑦 † 𝑗+1 ) P(𝑦 † 𝑗+1 |𝑣 † 𝑗 ) P(𝑣 † 𝑗 ) 𝑑𝑣 † 𝑗 . ( 6.43) Assume that we have a particle approximation of the true filter at time 𝑗, in the form 𝜋 OPF 𝑗 = 𝑁 ∑︁ 𝑛=1 𝑤 (𝑛) 𝑗 𝛿 ̂︀ 𝑣 (𝑛) 𝑗 . (6.44) We wish to update the particles and weights in (6.44) to obtain 𝜋 OPF 𝑗+1 = 𝑁 ∑︁ 𝑛=1 𝑤 (𝑛) 𝑗+1 𝛿 ̂︀ 𝑣 (𝑛) 𝑗+1 . ( 6 .45) Assume now that we draw 𝑁 independent samples from the measure (6.44) to obtain particles {𝑣 (𝑛) 𝑗 } 𝑁 𝑛=1 . Now consider any candidate particle 𝑣 𝑗 from {𝑣 (𝑛) 𝑗 } 𝑁 𝑛=1 . Following the roadmap dictated by (6.43), the OPF first incorporates the observation 𝑦 † 𝑗+1 by evaluating the likelihood weights P(𝑦 † 𝑗+1 |𝑣 𝑗 ). The algorithm then samples from the probability density function p 𝑗+1 := P(̂︀ 𝑣 𝑗+1 |𝑣 𝑗 , 𝑦 † 𝑗+1 ), weighting the samples by using the weights P(𝑦 † 𝑗+1 |𝑣 𝑗 ). Carrying this out for every candidate particle 𝑣 𝑗 = 𝑣 (𝑛) 𝑗 gives the sampled particles and weights (̂︀ 𝑣 (𝑛) 𝑗+1 , 𝑤 (𝑛) 𝑗+1 ), and hence defines (6.45). In general nonlinear settings, it may not be possible to implement the OPF exactly. This arises for two reasons. First, the likelihood weights must integrate the dependence on the latent variable 𝑣 𝑗+1 . That is, the integral P(𝑦 † 𝑗+1 |𝑣 † 𝑗 ) = ∫︁ P(𝑦 † 𝑗+1 |𝑣 † 𝑗+1 ) P(𝑣 † 𝑗+1 |𝑣 † 𝑗 ) 𝑑𝑣 † 𝑗+1 , may not have a closed form. Second, sampling exactly from p 𝑗+1 may not be possible. However, one setting where it is tractable to evaluate the likelihood weights and sample from p 𝑗+1 is when the observation model is linear: ℎ(•) = 𝐻• for some 𝐻 ∈ R 𝑘×𝑑 . The state and observation models (6.1), (6.2) then reduce to the form 𝑣 † 𝑗+1 = Ψ(𝑣 † 𝑗 ) + 𝜉 † 𝑗 , (6.46a) 𝑦 † 𝑗+1 = 𝐻𝑣 † 𝑗+1 + 𝜂 † 𝑗+1 . (6.46b) We make the same Assumptions 6.1 and 6.2 as made for equations (6.1), (6.2). The models for propagation of the state and observation can be combined to yield the likelihood function P(𝑦 † 𝑗+1 |𝑣 † 𝑗 ) = 𝒩 (︀ 𝐻Ψ(𝑣 † 𝑗 ), 𝐻Σ𝐻 ⊤ + Γ )︀ . (6.47) We note also that P(𝑣 † 𝑗+1 |𝑣 † 𝑗 ) = 𝒩 (︀ Ψ(𝑣 † 𝑗 ), Σ ). The Markovian kernel for the dynamics is given by P(𝑣 † 𝑗+1 |𝑣 † 𝑗 , 𝑦 † 𝑗+1 ) ∝ P(𝑦 † 𝑗+1 |𝑣 † 𝑗+1 , 𝑣 † 𝑗 ) P(𝑣 † 𝑗+1 |𝑣 † 𝑗 ) = P(𝑦 † 𝑗+1 |𝑣 † 𝑗+1 ) P(𝑣 † 𝑗+1 |𝑣 † 𝑗 ). The resulting log density is a quadratic function of 𝑣 † 𝑗+1 and so is Gaussian: P(𝑣 † 𝑗+1 |𝑣 † 𝑗 , 𝑦 † 𝑗+1 ) = 𝒩 (𝑚 𝑗+1 , 𝐶). Completing the square, as in derivation of the Kalman filter, shows that the mean 𝑚 𝑗+1 is given by 𝑚 𝑗+1 = (𝐼 -𝐾𝐻)Ψ(𝑣 † 𝑗 ) + 𝐾𝑦 † 𝑗+1 (6.48) and the covariance 𝐶 satisfies 𝐶 = (𝐼 -𝐾𝐻)Σ, (6.49a) 𝐾 = Σ𝐻 ⊤ 𝑆 -1 , (6.49b) 𝑆 = 𝐻Σ𝐻 ⊤ + Γ. (6.49c) We can sample from p 𝑗+1 by sampling from the Gaussian kernel 𝒩 (𝑚 𝑗+1 , 𝐶), and we can then reweight using likelihood (6.47) to build a sample approximation for 𝜋 𝑗+1 . This leads to the following OPF algorithm. First set 𝜋 OPF 0 = 𝜋 0 = 𝒩 (𝑚 0 , 𝐶 0 ). Then recall, for 𝑗 ∈ N, the desired approximate filtering distribution 𝜋 OPF 𝑗 given by (6.44). We have shown that the particles ̂︀ 𝑣 (𝑛) 𝑗 and weights 𝑤 (𝑛) 𝑗 evolve according to, for 𝜁 (𝑛) 𝑛+1 i.i.d. 𝒩 (0, 𝐶), ︀ 𝑣 (𝑛) 𝑗+1 = (𝐼 -𝐾𝐻)Ψ(𝑣 (𝑛) 𝑗 ) + 𝐾𝑦 † 𝑗+1 + 𝜁 (𝑛) 𝑗+1 , 𝑣 (𝑛) 𝑗 i.i.d. ∼ 𝜋 OPF 𝑗 , (6.50a) ℓ (𝑛) 𝑗+1 = exp (︂ - 1 2 |𝑦 † 𝑗+1 -𝐻Ψ(𝑣 (𝑛) 𝑗 )| 2 𝑆 )︂ , (6.50b) 𝑤 (𝑛) 𝑗+1 = ℓ (𝑛) 𝑗+1 / 𝑁 ∑︁ 𝑚=1 ℓ (𝑚) 𝑗+1 . (6.50c) Now define 𝜋 OPF 𝑗+1 according to (6.45), set 𝑗 ↦ → 𝑗 + 1, and repeat the above steps. The map from (6.44) to (6.45) approximates the true filter update (6.5). We again emphasize that the ensemble of predictions (6.50a) are 3DVar-like; see Remark 6.7. Thus the complete algorithm is a form of ensemble 3DVar. Remark 6.18. Optimality here refers to minimizing the variance of the weights, in one step of the filter, over a wide class of possible particle-based methods. Once cycled through multiple steps 𝑗, this optimality property is lost. Nonetheless the approach of using the data to predict is natural. Indeed the OPF has a clear potential advantage over the BPF: the predictive kernel in the OPF, p 𝑗+1 , incorporates knowledge of the current observation 𝑦 † 𝑗+1 . In contrast, the predictive kernel in the BPF simply predicts with the unconditioned forecast kernel P, drawing samples from P(𝑣 𝑗+1 |𝑣 𝑗 ) without knowledge of 𝑦 † 𝑗+1 , and then reweights them to reflect the observation. ♢ Evaluating Probabilistic Estimation In this subsection we describe a widely used methodology for evaluating probabilistic estimation of the filtering distribution. The following theorem shows that the filtering distribution has an interesting property: its variance, averaged over the marginal distribution on observations, equals the squared bias in its mean, averaged over the joint distribution of the state and observations. Theorem 6.19. In the following, the pair (𝑣 † 𝑗 , 𝑌 † 𝑗 ) is distributed according to the joint distribution under the dynamics/data model (6.1), (6.2), with expectation denoted E (𝑣 † 𝑗 ,𝑌 † 𝑗 ) , and the expectation under the marginal distribution for 𝑌 † 𝑗 is denoted by E 𝑌 † 𝑗 . Furthermore, 𝑣 𝑗 is distributed according to the the filtering distribution for 𝑣 † 𝑗 |𝑌 † 𝑗 , with expectation denoted by E. Then E 𝑌 † 𝑗 E ⃒ ⃒ ⃒𝑣 𝑗 -E𝑣 𝑗 ⃒ ⃒ ⃒ 2 E (𝑣 † 𝑗 ,𝑌 † 𝑗 ) ⃒ ⃒ ⃒𝑣 † 𝑗 -E𝑣 𝑗 ⃒ ⃒ ⃒ 2 = 1. Proof. This is a consequence of the properties of conditional probability, using that P(𝑣 † 𝑗 , 𝑌 † 𝑗 ) can be factored as P(𝑣 † 𝑗 |𝑌 † 𝑗 )P(𝑌 † 𝑗 ). Thus E (𝑣 † 𝑗 ,𝑌 † 𝑗 ) ⃒ ⃒ ⃒𝑣 † 𝑗 -E𝑣 𝑗 ⃒ ⃒ ⃒ 2 = E 𝑌 † 𝑗 (︁ E 𝑣 † 𝑗 |𝑌 † 𝑗 ⃒ ⃒ ⃒𝑣 † 𝑗 -E𝑣 𝑗 ⃒ ⃒ ⃒ 2 )︁ . But, since 𝑣 𝑗 is distributed according to the conditional 𝑣 † 𝑗 |𝑌 † 𝑗 , for frozen 𝑌 † 𝑗 we have E 𝑣 † 𝑗 |𝑌 † 𝑗 ⃒ ⃒ ⃒𝑣 † 𝑗 -E𝑣 𝑗 ⃒ ⃒ ⃒ 2 = E ⃒ ⃒ ⃒𝑣 𝑗 -E𝑣 𝑗 ⃒ ⃒ ⃒ 2 . Thus the result follows. Remark 6.20. The preceding statement in Theorem 6.19, and method of proof, can be generalized to the smoothing distribution P(𝑣 † 𝑗 |𝑌 † ) if 𝑌 † 𝑗 is replaced by 𝑌 † . ♢ The result above motivates the use of the spread-error ratio, which is defined next, to study probabilistic predictions. Definition 6.21. Consider a sequence of probabilistic forecasts {𝜋 alg 𝑗 } 𝐽 𝑗=1 . The spread-error ratio of this sequence with respect to the true trajectory {𝑣 † 𝑗 } 𝐽 𝑗=1 is the ratio 𝑟 := 1 𝐽 ∑︀ 𝐽 𝑗=1 E 𝑣 𝑗 ∼𝜋 alg 𝑗 [︀ |𝑣 𝑗 -E 𝑣 𝑗 ∼𝜋 alg 𝑗 [𝑣 𝑗 ]| 2 ]︀ 1 𝐽 ∑︀ 𝐽 𝑗=1 ⃒ ⃒ 𝑣 † 𝑗 -E 𝑣 𝑗 ∼𝜋 alg 𝑗 [𝑣 𝑗 ] ⃒ ⃒ 2 . ♢ Remark 6.22. This terminology for the spread-error ratio is used because the denominator is the square error of the mean under the filtering distribution, with respect to the true signal underlying the data; the numerator is the variance under the filtering distribution (the spread). The ratio in Theorem 6.19 averages over the marginal on the data and over the joint distribution of the state and observation. The spread-error ratio, however, replaces these expectation by averaging over the observations along a single trajectory; this reflects the fact that that only a single trajectory is available in practice. Nonetheless, the ratio serves as a heuristic for evaluating an algorithm. If 𝑟 = 1, we say that the approximate filtering distributions {𝜋 alg 𝑗 } 𝐽 𝑗=1 have a perfect spread-error relationship. If 𝑟 < 1, we say that they are underdispersed. If 𝑟 > 1, we say that they are overdispersed. Ratios other than 1 are also used, reflecting the heuristic derivation of 𝑟 as an approximation of the exact value 1 that is obtained in Theorem 6. 19 . ♢ Smoothing Algorithms 4DVar We introduce the 4DVar methodology, a nomenclature which abbreviates the specific weak-constraint 4DVar formulation that we focus on in this subsection. In short this method computes a MAP estimator (see Definition 1.9) for the smoothing problem from Subsection 6.2.2. It is a state estimator. To describe the methodology we define 𝑉 † := {𝑣 † 0 , . . . , 𝑣 † 𝐽 }, the dummy variable used in defining our optimization problem to estimate the state. We then set R(𝑉 † ) := 1 2 |𝑣 † 0 -𝑚 0 | 2 𝐶 0 + 1 2 𝐽-1 ∑︁ 𝑗=0 |𝑣 † 𝑗+1 -Ψ(𝑣 † 𝑗 )| 2 Σ . (6.51) Note that exp (︀ -R(𝑉 † ) )︀ is proportional to the prior density given in (6.9). We also define L(𝑉 † ; 𝑌 † ) := 1 2 𝐽-1 ∑︁ 𝑗=0 |𝑦 † 𝑗+1 -ℎ(𝑣 † 𝑗+1 )| 2 Γ . (6.52) Note that 𝑉 † ∈ R 𝑑(𝐽+1) and 𝑌 † ∈ R 𝑘𝐽 and that exp (︀ -L(𝑉 † ; 𝑌 † ) )︀ is proportional to the likelihood given in (6.11) . We add the expressions in (6.51) and (6.52) to obtain J(𝑉 † ; 𝑌 † ) = R(𝑉 † ) + L(𝑉 † ; 𝑌 † ), (6.53) noting that exp (︀ -J(𝑉 † ; 𝑌 † ) )︀ is proportional to the posterior density given in (6.12). The MAP estimator for this posterior is given by 𝑉 ∈ arg min 𝑉 † ∈R 𝑑(𝐽+1) J(𝑉 † ; 𝑌 † ). (6.54) We then take 𝑉 as our estimate of 𝑉 † . Remark 6.23. Minimization (6.54) is often implemented using Newton or Gauss-Newton methodologies (see Section 15.2). Ensemble approximations of Gauss-Newton are also used -see Remark 15.14. The problem (6.54) is the weak constraint 4DVar method. The use of \"4D\" refers to the fact that, in weather forecasting, the optimization problem is for a field in three physical space dimensions and one time dimension; this should be contrasted with the reasoning for the nomenclature described in Remark 6.8 for 3DVar. As in 3DVar the \"Var\" refers to the optimization framing of the methodology. Weak constraint can be understood by comparing with strong-constraint 4DVar in the next subsection. As with all MAP estimators, the result of applying 4DVar is simply a point estimator. Obtaining additional information about the posterior with density proportional to exp (︀ -J(𝑉 † ; 𝑌 † ) )︀ requires the use of methods such as Markov chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC), or variational approximations; for the latter, see Section 7.4. Hybrid ensemble-4DVar methods are also used to give uncertainty estimates. ♢ Strong-Constraint 4DVar Strong-constraint 4DVar is found by letting Σ → 0 in (6.54) . This results in a minimization problem defined with respect to variable 𝑣 † 0 ∈ R 𝑑 , the initial condition of the deterministic dynamics model arising from (6.1) by setting Σ = 0. We then obtain point estimator 𝑣 0 ∈ arg min 𝑣 † 0 ∈R 𝑑 J 0 (𝑣 † 0 ; 𝑌 † ). (6.55) Here J 0 (𝑣 † 0 ; 𝑌 † ) := R 0 (𝑣 † 0 ) + L 0 (𝑣 † 0 ; 𝑌 † ), (6.56) where R 0 (𝑣 † 0 ) := 1 2 |𝑣 † 0 -𝑚 0 | 2 𝐶 0 (6.57) and L 0 (𝑣 † 0 ; 𝑌 † ) := 1 2 𝐽-1 ∑︁ 𝑗=0 ⃒ ⃒ 𝑦 † 𝑗+1 -ℎ (︀ Ψ (𝑗+1) (𝑣 † 0 ) )︀⃒ ⃒ 2 Γ . (6.58) In the preceding definition Ψ (𝑗) is the 𝑗-fold composition of Ψ with itself. Strongconstraint 4DVar takes 𝑣 0 as point estimate of 𝑣 † 0 ; it is a state estimator. Other techniques, such as MCMC or SMC sampling algorithms, can be used for probabilistic estimation of the posterior on 𝑣 † 0 |𝑌 † , which has density proportional to exp (︀ -J 0 (𝑣 † 0 ; 𝑌 † ) )︀ . Remark 6.24. The nomenclature strong constraint, as opposed to weak constraint, comes from the fact that the solution of (6.55) can then be used to initialize the deterministic model, found from (6.1) with Σ = 0, leading to sequence 𝑣 𝑗 = Ψ (𝑗) (𝑣 0 ). Then sequence 𝑣 𝑗 is said to be \"strongly constrained\" by the deterministic model. In contrast the trajectory in (6.54) is not constrained to satisfy the deterministic model. Strong constraint 4DVAR is appealing when the deterministic model conserves certain quantities, or has qualitative attributes, that the stochastic model does not share. ♢ Reanalysis Reanalysis, also known as retrospective analysis, is concerned with retrospectively producing an estimate of the trajectory of a system, or the conditional probability density function of the states given the observations. An important feature of reanalysis is that it consolidates possibly sparse, irregular observations to give, at every time step, a complete state estimate of the system on the model state space, one which also incorporates information from the model dynamics. The extent to which the information from observed parts of the system can transfer to unobserved parts is formalized under the concept of observability; see the bibliography Section 6.8 for references. Reanalysis datasets, due to the impact of the observations, are often assumed to exhibit lower effects from model error (see Section 6.5), in contrast to the raw forecasts produced by the model. This is discussed in Subsection 8.2.3. Both because the output of reanalysis is produced on a regular grid, and because it is thought to exhibit lower model error, reanalyses are often used to train ML forecast models. Model Error We now consider the data assimilation problem in the setting where parts of the model are unknown, building on discussions in Section 1.4 for the general inverse problem. To describe forecast model error, we introduce parameter 𝜗 that captures unknown aspects of the systematic part of the dynamical model. The stochastic dynamics model becomes 𝑣 † 𝑗+1 = Ψ 𝜗 (𝑣 † 𝑗 ) + 𝜉 † 𝑗 , 𝑗 ∈ Z + , (6.59a) 𝑣 † 0 ∼ 𝒩 (︀ 𝑚 0 , 𝐶 0 )︀ , 𝜉 † 𝑗 ∼ 𝒩 (︀ 0, Σ )︀ i.i.d. (6.59b) To describe observation model error we introduce parameter 𝜙 which captures unknown aspects of the observation operator. The data model becomes 𝑦 † 𝑗+1 = ℎ 𝜙 (𝑣 † 𝑗+1 ) + 𝜂 † 𝑗+1 , 𝑗 ∈ Z + , (6.60a) 𝜂 † 𝑗 ∼ 𝒩 (︀ 0, Γ )︀ i.i.d. (6.60b) In the preceding we again invoke Asssumption 6.1. Remark 6.25. In the following we work with unknown parameter 𝜃 = (𝜗, Σ, 𝜙, Γ), or choose 𝜃 to be a subset of these four parameters as we recognize that this will often be natural in applications. Furthermore, whilst we have general parameterization of the vector fields, the covariance matrices Σ and Γ may themselves be parameterized; for example we might write a covariance matrix as an unknown scalar non-negative real parameter multiplying the identity, or we might parameterize it in terms of a Cholesky factor. ♢ In the following two subsections we formulate model error in the filtering and smoothing contexts. Chapter 8 is devoted to detailed algorithms concerned with learning model error. Model Error: Filtering It is possible to learn about model error using filtering. To illustrate this idea we consider the setting in which Σ and Γ are known and the unknown 𝜃 comprise only 𝜗 and 𝜙 appearing in Ψ 𝜗 and ℎ 𝜙 , respectively. Consider the following dynamical system, holding for all 𝑗 ∈ Z + : 𝑣 † 𝑗+1 = Ψ 𝜗 𝑗 (𝑣 † 𝑗 ) + 𝜉 † 𝑗 , (6.61a) 𝜗 † 𝑗+1 = 𝜗 † 𝑗 , (6.61b) 𝜙 † 𝑗+1 = 𝜙 † 𝑗 , (6.61c ) 𝑦 † 𝑗+1 = ℎ 𝜙 𝑗 (𝑣 † 𝑗+1 ) + 𝜂 † 𝑗+1 . ( 6.61d) This can now be viewed as a filtering problem for {(𝑣 † 𝑗 , 𝜗 † 𝑗 , 𝜙 † 𝑗 )} given the data generated by {𝑦 † 𝑗 }. As it stands it is slightly out of the scope of problems studied in Section 6.3, because no noise appears in the evolution of the unknown parameters. However, as discussed in more detail in Section 6.7, filtering can be extended to this setting. Alternatively it is possible to replace equations (6.61b, 6.61c) by stochastic processes and to use an average over index 𝑗 of the filtering solution to provide a parameter estimate; in this context linear autoregressive models, as developed in Section 14.3, are natural replacements for (6.61b, 6.61c). Model Error: Smoothing It is also possible to consider model error in the context of smoothing. To formulate learning of model error in the context of a smoothing problem we put a prior on (𝑉 † , 𝜃), defined by the stochastic dynamics model (6.59) for 𝑉 † |𝜃, and a prior on 𝜃; we then condition on a likelihood defined by the data model. Building on Section 6.4 we define prior 𝜌(𝑉 † , 𝜃) ∝ exp (︁ - 1 2 |𝑣 † 0 -𝑚 0 | 2 𝐶 0 - 1 2 𝐽-1 ∑︁ 𝑗=0 |𝑣 † 𝑗+1 -Ψ 𝜗 (︀ 𝑣 † 𝑗 )︀ | 2 Σ )︁ 𝜌 𝜃 (𝜃), (6.62) where we have written the prior on 𝑉 † |𝜃 and then multiplied by prior 𝜌 𝜃 (𝜃) on 𝜃. As in Section 6.4 we define 𝜂 † := {𝜂 † 1 , . . . , 𝜂 † 𝐽 }, noting that 𝜂 † ∼ 𝒩 (0, Γ), where Γ is block diagonal with Γ in each diagonal block. If we then define h 𝜙 (𝑉 † ) := {︀ ℎ 𝜙 (𝑣 † 1 ), . . . , ℎ 𝜙 (𝑣 † 𝐽 ) }︀ then the data model may be written as 𝑌 † = h 𝜙 (𝑉 † ) + 𝜂 † . We are interested in finding P(𝑉 † , 𝜃, |𝑌 † ) = Π 𝑌 † (𝑉 † , 𝜃). We may apply Bayes Theorem 1.3, with prior 𝜌 noting that, under this prior, 𝑉 † ⊥ ⊥ 𝜂 † , the standard setting for Bayesian inversion from Chapter 1. We obtain Π 𝑌 † (𝑉 † , 𝜃) ∝ exp (︁ - 1 2 |𝑌 † -h 𝜙 (𝑉 † )| 2 Γ )︁ 𝜌(𝑉 † , 𝜃). (6.63) Surrogate Modelling Filtering With Surrogate Dynamics Consider the filtering problem of estimating signal {𝑣 † 𝑗 } 𝑗≥0 given observations {𝑦 † 𝑗 } 𝑗≥1 in the setting where the dynamics-observation models (6.1), (6.2) reduce to 𝑣 † 𝑗+1 = Ψ(𝑣 † 𝑗 ), (6.64a ) 𝑦 † 𝑗+1 = 𝐻𝑣 † 𝑗+1 + 𝜂 † 𝑗+1 . (6.64b) That is, we assume the dynamics are noiseless (Σ = 0) and that the observation map ℎ(•) = 𝐻• is linear. We make the same assumptions on the noise and initial condition as detailed at the start of Chapter 6. Consider the 3DVar method (6. 19) , with 𝑠 = 0, applied in the setting (6.64); we obtain 𝑣 𝑗+1 = (𝐼 -𝐾𝐻)Ψ(𝑣 𝑗 ) + 𝐾𝑦 † 𝑗+1 . We are interested in applications where evaluating Ψ is computationally expensive, but where we have a surrogate model Ψ ′ that approximates Ψ and can be cheaply evaluated. The following theorem proves long-time accuracy for a 3DVar filtering algorithm that uses the surrogate dynamics Ψ ′ rather than the true dynamics Ψ. Thus we consider deploying the algorithm 𝑣 𝑗+1 = (𝐼 -𝐾𝐻)Ψ ′ (𝑣 𝑗 ) + 𝐾𝑦 † 𝑗+1 . ( 6 .65) Theorem 6.26. Consider the 3DVar method (6.65), using approximate forecast model Ψ ′ , and driven by observations 𝑦 † 𝑗+1 from (6.64). Assume that Ψ ′ is close to Ψ in the sense that, for some 𝛿 ∈ [0, ∞), sup 𝑣∈R 𝑑 ⃒ ⃒ (𝐼 -𝐾𝐻) (︀ Ψ(𝑣) -Ψ ′ (𝑣) )︀⃒ ⃒ = 𝛿. Assume, further, the observability condition that there exists constant 𝜆 ∈ (0, 1) so that sup 𝑣∈R 𝑑 |(𝐼 -𝐾𝐻)𝐷Ψ(𝑣)| ≤ 𝜆. (6.66) Denote 𝜖 := E|𝐾𝜂 † 𝑗 |. Then, the 3DVar estimate 𝑣 𝑗 based on (6.65) satisfies lim sup 𝑗→∞ E|𝑣 𝑗 -𝑣 † 𝑗 | ≤ 𝜖 + 𝛿 1 -𝜆 , where 𝑣 † 𝑗 is given by (6.64). Proof. The 3DVar update with the approximate forecast model from (6.65) can be written as 𝑣 𝑗+1 = (𝐼 -𝐾𝐻)(Ψ(𝑣 𝑗 ) + 𝑏 𝑗 ) + 𝐾𝑦 † 𝑗+1 . where 𝑏 𝑗 := Ψ ′ (𝑣 𝑗 ) -Ψ(𝑣 𝑗 ) represents bias in the forecast model at time 𝑗. From (6.64) we obtain 𝑦 † 𝑗+1 = 𝐻Ψ(𝑣 † 𝑗 ) + 𝜂 † 𝑗+1 so that 𝑣 𝑗+1 -𝑣 † 𝑗+1 = (𝐼 -𝐾𝐻) (︀ Ψ(𝑣 𝑗 ) + 𝑏 𝑗 )︀ + 𝐾 (︀ 𝐻Ψ(𝑣 † 𝑗 ) + 𝜂 † 𝑗+1 )︀ -Ψ(𝑣 † 𝑗 ) = (𝐼 -𝐾𝐻)(Ψ(𝑣 𝑗 ) -Ψ(𝑣 † 𝑗 )) + 𝐾𝜂 † 𝑗+1 + (𝐼 -𝐾𝐻)𝑏 𝑗 = ∫︁ 1 0 (𝐼 -𝐾𝐻)𝐷Ψ(𝑠𝑣 𝑗 + (1 -𝑠)𝑣 † 𝑗 )(𝑣 𝑗 -𝑣 † 𝑗 ) 𝑑𝑠 + 𝐾𝜂 † 𝑗+1 + (𝐼 -𝐾𝐻)𝑏 𝑗 , where in the last line we used the mean value theorem for Ψ on the line segment 𝑠𝑣 𝑗 + (1 -𝑠)𝑣 † 𝑗 for 𝑠 ∈ [0, 1] . By the triangle inequality, the error is bounded by |𝑣 𝑗+1 -𝑣 † 𝑗+1 | ≤ (︁ ∫︁ 1 0 |(𝐼 -𝐾𝐻)𝐷Ψ(𝑠𝑣 𝑗 + (1 -𝑠)𝑣 † 𝑗 )| 𝑑𝑠 )︁ |𝑣 𝑗 -𝑣 † 𝑗 | + |𝐾𝜂 † 𝑗+1 | + 𝛿 ≤ (︁ ∫︁ 1 0 𝜆 𝑑𝑠 )︁ |𝑣 𝑗 -𝑣 † 𝑗 | + |𝐾𝜂 † 𝑗+1 | + 𝛿. Taking an expectation over the measurement errors, we have E|𝑣 𝑗+1 -𝑣 † 𝑗+1 | ≤ 𝜆E|𝑣 𝑗 -𝑣 † 𝑗 | + 𝜖 + 𝛿. Letting 𝑒 𝑗 := E|𝑣 𝑗+1 -𝑣 † 𝑗+1 | and applying the discrete Gronwall inequality we have 𝑒 𝑗 ≤ 𝜆 𝑗 𝑒 0 + (𝜖 + 𝛿) 1 -𝜆 𝑗 1 -𝜆 . Noting that 𝜆 ∈ (0, 1) it follows that 𝜆 𝑗 → 0 as 𝑗 → ∞ and so the result follows. Remark 6.27. Theorem 6.26 highlights that running the 3DVar algorithm for long times can prevent small model error from accumulating. Moreover, this model error only needs to be controlled in the unobserved directions. That is, the overall error of the recovered state will be small if (𝐼 -𝐾𝐻)𝑏 𝑗 is small for all 𝑗, rather than the overall model error 𝑏 𝑗 being small. Ideally, the gain 𝐾 in 3DVar is chosen such that the conditions in the assumption of Theorem 6.26 hold. See the bibliography Section 6.8 for one such guarantee under an observability condition. ♢ Remark 6.28. As discussed in the bibliography Section 6.8, similar accuracy results can be established for other filtering algorithms. These results rely on observability conditions on the true dynamics and observation model (Ψ, 𝐻) and on accuracy of the surrogate model Ψ ′ in the unobserved part of the state-space. Notice that in Theorem 6.26 we do not assume long-time accuracy or stability of the surrogate dynamics defined by Ψ ′ , or of the true dynamics defined by Ψ, but we can nevertheless obtain long-time accuracy of filtering estimates by leveraging the stability properties of the map (𝐼 -𝐾𝐻)Ψ; this is a form of observability. ♢ Smoothing With Surrogate Dynamics Recall definitions of 𝑉 † , 𝑌 † and 𝑌 † 𝑗 from (6.3). From (6.9) we obtain the prior on 𝑉 † from the dynamics model, a probability density function in 𝒫(R 𝑑(𝐽+1) ) given by 𝜌(𝑉 † ) ∝ exp (︁ - 1 2 |𝑣 † 0 -𝑚 0 | 2 𝐶 0 - 1 2 𝐽-1 ∑︁ 𝑗=0 |𝑣 † 𝑗+1 -Ψ (︀ 𝑣 † 𝑗 )︀ | 2 Σ )︁ . (6.67) Immediately after (6.9) we define 𝜂 † = {𝜂 † 1 , . . . , 𝜂 † 𝐽 } ∈ R 𝑘𝐽 , h(𝑉 † ) = {︀ ℎ(𝑣 † 1 ), . . . , ℎ(𝑣 † 𝐽 ) }︀ , where 𝜂 † ∼ 𝒩 (0, Γ), and Γ is block diagonal with Γ in each diagonal block. We then obtain the posterior Π(𝑉 † ) ∝ l(𝑌 † |𝑉 † )𝜌(𝑉 † ), (6.68) also a probability density function in 𝒫(R 𝑑(𝐽+1) ), where l(𝑌 † |𝑉 † ) ∝ exp (︁ - 1 2 |𝑌 † -h(𝑉 † )| 2 Γ )︁ . ( 6.69) (We drop the superscript 𝑌 † on the posterior as it is not central to the following discussion.) If we use a surrogate model Ψ ′ to accelerate computations we will change the prior dynamics model to have the form 𝜌 ′ (𝑉 † ) ∝ exp (︁ - 1 2 |𝑣 † 0 -𝑚 0 | 2 𝐶 0 - 1 2 𝐽-1 ∑︁ 𝑗=0 |𝑣 † 𝑗+1 -Ψ ′ (︀ 𝑣 † 𝑗 )︀ | 2 Σ )︁ . (6.70) The resulting posterior is Π ′ (𝑉 † ) ∝ l(𝑌 † |𝑉 † )𝜌 ′ (𝑉 † ). (6.71) We are interested in what effect the use of a surrogate model has on the posterior. As a first step we simply assume that 𝜌 ′ is close to 𝜌 and ask what can be said about the closeness of Π ′ and Π. In the following theorem and proof, all integrals are over R 𝑑(𝐽+1) . Theorem 6.29. Assume that 𝑍 := ∫︀ l(𝑌 † |𝑉 † )𝜌(𝑉 † ) 𝑑𝑉 † > 0. Then, there is a constant 𝐶 > 0 such that, for all D TV (𝜌, 𝜌 ′ ) sufficiently small, D TV (Π, Π ′ ) ≤ 𝐶D TV (𝜌, 𝜌 ′ ). Proof. In this proof 𝑍 ′ := ∫︀ l(𝑌 † |𝑉 † )𝜌 ′ (𝑉 † ) 𝑑𝑉 † > 0 and l(𝑉 † ) = l(𝑌 † |𝑉 † ). We have 2D TV (Π, Π ′ ) = ∫︁ ⃒ ⃒ ⃒ 1 𝑍 l(𝑉 † )𝜌(𝑉 † ) - 1 𝑍 ′ l(𝑉 † )𝜌 ′ (𝑉 † ) ⃒ ⃒ ⃒ 𝑑𝑉 † ≤ 𝐼 1 + 𝐼 2 , 𝐼 1 := ∫︁ ⃒ ⃒ ⃒ 1 𝑍 l(𝑉 † )𝜌(𝑉 † ) - 1 𝑍 l(𝑉 † )𝜌 ′ (𝑉 † ) ⃒ ⃒ ⃒ 𝑑𝑉 † , 𝐼 2 := ∫︁ ⃒ ⃒ ⃒ 1 𝑍 l(𝑉 † )𝜌 ′ (𝑉 † ) - 1 𝑍 ′ l(𝑉 † )𝜌 ′ (𝑉 † ) ⃒ ⃒ ⃒ 𝑑𝑉 † . Since 𝑍 > 0 and l(𝑉 † ) is uniformly bounded with respect to 𝑉 † we deduce that, for some 𝐶 1 > 0, 𝐼 1 ≤ 𝐶 1 D TV (𝜌, 𝜌 ′ ). It also follows that, for some 𝐶 2 > 0, |𝑍 -𝑍 ′ | ≤ ∫︁ l(𝑉 † )|𝜌(𝑉 † ) -𝜌 ′ (𝑉 † )| 𝑑𝑉 † ≤ 𝐶 2 D TV (𝜌, 𝜌 ′ ). Hence, for all D TV (𝜌, 𝜌 ′ ) sufficiently small, 𝑍 ′ > 1 2 𝑍 > 0. Finally we deduce that, for some 𝐶 1 > 0, for some 𝐶 3 > 0, 𝐼 2 = ⃒ ⃒ ⃒ ⃒ 1 𝑍 - 1 𝑍 ′ ⃒ ⃒ ⃒ ⃒ 𝑍 ≤ 𝐶 3 |𝑍 -𝑍 ′ |. The result follows. Remark 6.30. The sense in which 𝜌 ′ is close to 𝜌 depends on details of surrogate models that we do not get into in these notes. In particular the approximation theorems we allude to in Section 12.5 are valid, in their simplest form, over compact sets 𝐷; in contrast, the prior distribution here is supported on the whole Euclidean space. However, by making assumptions about the behaviour of Ψ and Ψ ′ at infinity, which controls errors outside 𝐷, and by choosing 𝐷 large enough, it is possible to deduce that 𝜌 ′ is close to 𝜌. ♢ Generalizations Here we conclude with some remarks on generalizations of the setting we adopt in this chapter. First we observe that the stochastic dynamics model (6.1) and data model (6.2) are readily generalized to settings in which the maps Ψ(•) and ℎ(•), as well as the covariances Σ and Γ, are dependent on the time-index 𝑗. It is also possible to allow degenerate noises (positive semi-definite), and the case where no noise is present in the dynamics (Σ ≡ 0) arises frequently. Furthermore, all the algorithms in this chapter, except for the Kalman filter which leverages Gaussian structure, admit generalizations to settings in which the noises {𝜉 † 𝑗 } 𝑗∈Z + , {𝜂 † 𝑗 } 𝑗∈N are independent i.i.d. centered but non-Gaussian sequences. Adding correlation to the noises is also possible. For example it is possible to consider correlation across the discrete time index 𝑗. This significantly complicates filtering, but it is possible to accommodate it if the noise itself is generated by a Markov process. Adding correlation to the noises across the discrete time index 𝑗 also complicates smoothing, but can also be handled, typically at additional computational cost. In addition, allowing for correlation between the dynamics and observational noise is also possible. Finally we note that assuming differentiability, or even continuity, of the maps Ψ(•) and ℎ(•) is also not necessary; however assuming differentiability of Ψ(•) and ℎ(•) facilitates specific filtering methods such as the ExKF and gradient-based algorithms for smoothing methodologies such as 4DVar. Bibliography For overviews of the subjects of data assimilation and filtering/smoothing, see the text books [230, 270, 362, 27, 380, 36, 115, 141] and the review paper [361] . Furthermore the books [236, 289] and the review paper [84] comprise pedagogical introductions to data assimilation in the context of weather forecasting, turbulence modeling, and geophysical sciences, respectively. The importance of data assimilation for numerical weather forecasting was articulated in [330] , then known as objective analysis, and connected to sequential estimation theory in [169] . The terminology of prediction and analysis to define filtering was introduced in the weather forecasting community [330] . The Kalman filter was introduced in [235] . Discussion of the steady state covariance, and results concerning convergence to the steady state, may be found in [262] . The ideas of 3DVar for the solution of the analysis step was introduced in the context of weather forecasting in [284] . Using the formula (6.22) directly, rather than carrying out the minimization (6.21) numerically, is known as optimal interpolation in the weather forecasting literature. Optimal interpolation was proposed earlier by Eliassen (1954) and Gandin (1963) ; see [49] for bibliographical notes. For definition of the (cycled) 3DVar algorithm as employed here, see [270] . For analysis of the (cycled) 3DVar algorithm see [269, 309] . The EnKF was introduced in [140] . See [27, 141, 270, 362] for more recent discussion of the methodology. The methodology based around optimization has a probabilistic interpretation in the Gaussian setting, and this can be used to justify the empirical approximation (6.30). In the Gaussian setting see [272, 291] ; in the near Gaussian setting see [86] . The paper [376] establishes long-time accuracy of EnKF for a wide class of partially observed chaotic dynamical systems; moreover, [376] also shows that long-time accuracy still holds when the true dynamics are replaced with a sufficiently accurate surrogate model. The impact of sampling error on EnKFs in the case that the signal is a Gaussian process is addressed in [368] . Localization and inflation are reviewed in [141] . Insights into localization are given in [155, 8, 57, 10, 11, 9, 428] . Localization in the context of using the EnKF to solve inverse problems is discussed in [1] . For further background on the derivation of (6.36), see [27, Page 158] and also [155, 8] . There are many variants of the EnKF, including ensemble square-root filters that are more computationally suited for high-dimensional problems than the formulation presented in this chapter [415, 222] . Since these variants avoid the construction of the covariance matrices, localization is often implemented in these filters through domain localization, where the spatial domain is divided into multiple areas, and analyses done locally in each. A popular EnKF variant that uses domain localization is the local ensemble transform Kalman filter (LETKF) [222] . We note that besides the considerations about spurious correlations and forecast covariance rank discussed in the chapter, localization can also be interpreted in a dynamical systems context. For an EnKF, one generally needs enough ensemble members to span the unstable-neutral subspace, corresponding to the number of nonnegative Lyapunov exponents, in order to prevent filter divergence. This observation is supported by the fact that, in the case of linear dynamics, the forecast covariance matrix will collapse onto the unstable-neutral subspace; see the early work of [417] and the review of data assimilation for chaotic dynamics [85] for references to these results. It has been observed, however, that dynamical systems such as the atmosphere are locally low dimensional [335, 324] ; that is, the dynamics within a spatial region may have a significantly lower dimension (as quantified by the dimension of the subspace spanned by the fastest growing modes) than that of the entire system, implying that filtering may be successful with a small ensemble when localization is applied. For an introduction to the particle filter see [129] . For the OPF see [128] . Particle filters typically suffer from weight collapse in high dimensional problems: all weights become zero, except one [56, 391] . For a discussion of 4DVar, in the context of weather forecasting, see [151] . As with all MAP estimators, the result of applying 4DVar is simply a point estimator. To obtain information about the posterior with density proportional to exp (︀ -J(𝑉 ) )︀ requires the use of methods such as MCMC [76] , SMC [124] or variational methods [232] . Hybrid methods that combine 4DVar with ensembles are also popular ways to estimate uncertainty for 4DVar [84] . The concept of reanalysis originates in atmospheric science [423] . Although operational weather forecasting has been done for decades, there have been considerable changes in forecast models and data assimilation methods, necessitating reanalysis to have a consistent trajectory estimate over decadal timescales. One of the first reanalysis datasets for the atmosphere was produced by the National Centers for Environmental Prediction (NCEP) and the National Center for Atmospheric Research (NCAR) [237] . The subject of observability is reviewed in [298] . Model error in data assimilation is often modelled by Gaussian noise, and encapsulated in the model noises 𝜉 † . Deterministic formulations of model error are reviewed in [82] . The special case of model bias is considered in [122] . Time-correlated model error is considered in [16] . Methods for correcting model error are discussed in Chapter 8, and a bibliography provided there. While the effect of model bias on data assimilation is an open research topic, [122] analyzed Kalman filtering in the presence of model bias. Under observability conditions, [298] showed that the Kalman gain 𝐾 can be designed to satisfy the condition in (6.66) for stability of a data assimilation scheme with linear dynamics; this is also related to the design of Luenberger observers in control theory. Machine learning models for numerical weather forecasting have recently received significant attention; see, e.g., [334, 55, 241, 95, 94, 257] . Data assimilation using learned forecast models has been considered in [193, 277, 97, 90, 338, 450, 3, 251] . The importance of these forecast models correctly reproducing the Lyapunov spectrum and forecast error covariance was explored in [338] . Including the Lyapunov spectrum and attractor dimension into the training process was explored in [343] . Instead of learning a full forward model, learning an adjoint model for use in 4DVar was considered in [201] . Hybrid methods combining a numerical and learned forecast model in data assimilation and ensemble forecasting have been considered in [32, 33, 91] . The latter used a large ensemble of a learned forecast model, along with a smaller ensemble of a more expensive numerical model solving the equations of motion, in an EnKF, mitigating the need for localization. Combining a small high-fidelity ensemble with a large ensemble of reduced-order models in an EnKF using control variates was considered in [345, 387] . A multilevel EnKF was introduced in [211] , and a multi-model EnKF in [31] . Performing data assimilation in the latent space of an autoencoder (Section 13.5) or variational autoencoder (Section 13.6) has been considered in [340, 301, 173] . The generation of ensembles from an ML forecast model was considered in [382, 278, 346] . Chapter 7 Variational Inference For Data Assimilation In this chapter we use the variational formulation of Bayes Theorem, introduced in Chapter 2, to frame data assimilation problems as optimization problems over probability measures; both smoothing and filtering problems are considered. In Section 7.1 we introduce the variational formulation of the smoothing problem, in analogy to the procedures in Chapter 2 for inverse problems; Section 7.2 extends the variational formulation to filtering. In both of these first two sections we also introduce algorithmic frameworks, stemming from the variational formulations. In Section 7.3 we show how filtering may be linked to smoothing by imposing a filtering structure on a variational formulation of smoothing. Sections 7.4 and 7.5 introduce examples of variational algorithms, based on the variational formulations in Sections 7.1 and 7.2 respectively. We conclude the chapter with bibliographic remarks in Section 7.6. Remark 7.1. We are focused on the problem of finding certain distributions on the state of the stochastic dynamical system (6.1) conditioned on data from (6.2). Throughout we employ the notational conventions established in Subsection 6.1.1. We define 𝒫 := 𝒫(R 𝑑 ) the space of probability measures on R 𝑑 ; then, the probability distribution of the state of the system at any fixed time belongs to 𝒫. It is also convenient to denote by 𝒫 𝑗 := 𝒫(R 𝑑(𝑗+1) ) the space of probability measures on R 𝑑(𝑗+1) ; then, the probability distribution of 𝑉 † 𝑗 belongs to 𝒫 𝑗 , and, in particular, the distribution of 𝑉 † belongs to 𝒫 𝐽 . Consider the smoothing problem defined in Subsection 6.2.2; we extend the notation from Remark 1.4 to allow definition of marginal 𝜅 on 𝑌 † determined by the joint 𝛾 on (𝑌 † , 𝑉 † ). Likewise, for the filtering problem defined in Subsection 6.2.1, it will be useful in later chapters to define the joint distribution 𝛾 𝑗+1 (𝑣 † 𝑗+1 , 𝑦 † 𝑗+1 ) := l(𝑦 † 𝑗+1 |𝑣 † 𝑗+1 )̂︀ 𝜋 𝑗+1 (𝑣 † 𝑗+1 ), on state and observation at time 𝑗 + 1, and its marginal 𝜅 𝑗+1 (𝑦 † 𝑗+1 ) on the observation coordinate. ♢ Throughout the chapter we make the following assumption about data available to define the filtering and smoothing problems variationally, and for learning the parameters of models for filtering and smoothing through these variational formulations. Data Assumption 7.2. The data available is 𝑌 † := {𝑦 † 1 , . . . , 𝑦 † 𝐽 } , a single draw from the marginal 𝜅 on the observed data. Variational Formulation Of Smoothing From (6.12), the density for the smoothing distribution P(𝑉 † |𝑌 † ) is given by Π 𝑌 † (𝑉 † ) ∝ l(𝑌 † |𝑉 † )𝜌(𝑉 † ), where the likelihood and the prior are defined, respectively, as l(𝑌 † |𝑉 † ) = 𝐽 ∏︁ 𝑗=1 l 𝑗 (𝑣 † 𝑗 ), 𝜌(𝑉 † ) = (︁ 𝐽 ∏︁ 𝑗=1 t(𝑣 † 𝑗 , 𝑣 † 𝑗-1 ) )︁ 𝜌 0 (𝑣 † 0 ), (7.1a) l 𝑗 (𝑣 † ) = 𝑍 -1 Γ exp (︁ - 1 2 |𝑦 † 𝑗 -ℎ(𝑣 † )| 2 Γ )︁ , 𝑍 Γ = (2𝜋) 𝑘/2 det(Γ) 1/2 , (7.1b) t(𝑣 † , 𝑤 † ) = 𝑍 -1 Σ exp (︁ - 1 2 |𝑣 † -Ψ(𝑤 † )| 2 Σ )︁ , 𝑍 Σ = (2𝜋) 𝑑/2 det(Σ) 1/2 , (7.1c) 𝜌 0 (𝑣 † ) = 𝑍 -1 𝐶 0 exp (︁ - 1 2 |𝑣 † -𝑚 0 | 2 𝐶 0 )︁ , 𝑍 𝐶 0 = (2𝜋) 𝑑/2 det(𝐶 0 ) 1/2 . (7.1d) The following theorem shows that the smoothing density arises as the solution of a variational problem. This result is analogous to the variational formulation for the posterior in an inverse problem that was presented in Theorem 2.1. Theorem 7.3. Consider the likelihood l(𝑌 † |𝑉 † ) and prior 𝜌(𝑉 † ) given in (7.1), and let J(𝑞) = D KL (𝑞‖𝜌) -E 𝑞 [︀ log l(𝑌 † |•) ]︀ , ( 7.2a ) 𝑞 opt ∈ arg min 𝑞∈𝒫 𝐽 J(𝑞). (7.2b) Then, the minimizer of J over 𝒫 𝐽 is the smoothing density: 𝑞 opt = Π 𝑌 † . Proof. This follows directly from the variational formulation of Bayes Theorem 2.1. This formulation of the smoothing problem can be used as the basis for algorithms to learn approximations for the smoothing distribution from data: we replace 𝒫 𝐽 in (7.2) by some 𝒬 𝐽 ⊂ 𝒫 𝐽 and solve J(𝑞) = D KL (𝑞‖𝜌) -E 𝑞 [︀ log l(𝑌 † |•) ]︀ , (7.3a) 𝑞 ⋆ ∈ arg min 𝑞∈𝒬 𝐽 J(𝑞). (7.3b) In Section 7.4 we give an explicit example of a choice of 𝒬 𝐽 that leads to actionable algorithms. Remark 7.4. In this section we have focused on the idea of solving the smoothing problem for a given realization 𝑌 † . However it is of interest to learn the dependence of the solution on the data instance 𝑌 † . This is known as amortization and is typically tackled by considering learning problems defined over multiple realizations of the data 𝑌 † ; see Chapters 5 and 10. ♢ Variational Formulation Of Filtering Recall the prediction (P) and analysis (A(•; 𝑦 † 𝑗+1 )) maps that define the evolution of the filtering distribution in (6.4), (6.5): a mapping from the filter at time 𝑗, 𝜋 𝑗 , to the filter at time 𝑗 + 1, 𝜋 𝑗+1 , made from two steps: the forecast step ̂︀ 𝜋 𝑗+1 = P𝜋 𝑗 defined by (6.6); and the analysis step as detailed in (6.8), an application of Bayes Theorem 1.3 with likelihood P(𝑦 † 𝑗+1 |𝑣 † 𝑗+1 ) and prior ̂︀ 𝜋 𝑗+1 . The analysis density 𝜋 𝑗+1 can be written as the solution to an optimization problem using the variational formulation of Bayes Theorem 2.1: J 𝑗+1 (𝑞) = D KL (𝑞‖̂︀ 𝜋 𝑗+1 ) -E 𝑞 [︀ log P(𝑦 † 𝑗+1 |•) ]︀ , ( 7.4a ) 𝜋 𝑗+1 ∈ arg min 𝑞∈𝒫 J 𝑗+1 (𝑞). (7.4b) The preceding formulation assumes that we have access to the true prior ̂︀ 𝜋 𝑗+1 . We now consider densities 𝑞 ′ 𝑗 recursively defined by the prediction-analysis cycle 𝑞 ′ 0 = 𝜋 0 , (7.5a) J 𝑗+1 (𝑞) = D KL (𝑞‖P𝑞 ′ 𝑗 ) -E 𝑞 [︀ log P(𝑦 † 𝑗+1 |•) ]︀ , ( 7.5b ) 𝑞 ′ 𝑗+1 ∈ arg min 𝑞∈𝒫 J 𝑗+1 (𝑞). (7.5c) Applying this for 𝑗 = 0, . . . , 𝐽 -1 yields 𝑞 ′ 𝑗 = 𝜋 𝑗 for 𝑗 = 1, . . . , 𝐽: indeed 𝑞 ′ 𝑗+1 = A(P𝑞 ′ 𝑗 ; 𝑦 † 𝑗+1 ), 𝑞 ′ 0 = 𝜋 0 . (7.6) Performing the minimization (7.5c) over a restricted class 𝒬 𝑗 of probability densities, potentially defined differently at each step 𝑗, leads to a sequence of approximate filtering densities {𝑞 ′ 𝑗 } 𝑗≥0 . We now parameterize the set of candidate approximate filters 𝑞 to develop the basis of actionable algorithms, implicitly defining 𝒬 𝑗 . Consider a family of algorithms in which the analysis step is approximated by 𝜃-parameterized model A 𝜃 (•; 𝑦 † 𝑗+1 ), leading to the following recursion: 𝑞 𝑗+1 (𝜃) = A 𝜃 (P𝑞 𝑗 (𝜃); 𝑦 † 𝑗+1 ), 𝑞 0 = 𝜋 0 . (7.7) Then, using the variational formulation of filtering in (7.4) and substituting in (7.7), we define the cost function J 𝑗+1 (𝜃) = D KL (︀ 𝑞 𝑗+1 (𝜃)‖P𝑞 𝑗 (𝜃) )︀ -E 𝑞 𝑗+1 (𝜃) [log P(𝑦 † 𝑗+1 |•)]. (7.8) We may then consider the following optimization problem to define the best choice of 𝜃: J(𝜃) = 𝐽 ∑︁ 𝑗=1 J 𝑗 (𝜃), (7.9a ) 𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃). (7.9b) Alternatively we may choose to learn 𝜃 sequentially and average over time to find a best estimator: 𝜃 ⋆ 𝑗 ∈ arg min 𝜃∈Θ J 𝑗 (𝜃), (7.10a ) 𝜃 ⋆ = 1 𝐽 𝐽 ∑︁ 𝑗=1 𝜃 ⋆ 𝑗 . (7.10b) Remark 7.5. This section is focused on the idea of solving the filtering problem for a given realization 𝑌 † . However, as for smoothing in the preceding section, there is a possibility to exploit amortization and learn dependence on the data, at each step of a filter. Doing so requires two ingredients: (i) choosing 𝒬 so that explicit dependence of the learned probability density function on the data 𝑦 † 𝑗+1 is present, as well as dependence on the state 𝑣 † 𝑗+1 over which the probability density function 𝑞 ′ 𝑗+1 is defined; (ii) having large 𝐽 so that, by invoking an ergodic argument, it is possible to argue that sufficient variability is seen in the data to be able to reuse the learned model for different data instances. Convergence of 𝜃 ⋆ = 𝜃 ⋆ (𝐽), as a function of 𝐽, can be used to determine a value of 𝐽 sufficient for amortization. Alternatively, amortization may be tackled by considering learning problems defined over multiple realizations of the data 𝑌 † ; see Chapters 5 and 10. ♢ Remark 7.6. To be able to employ algorithms based on the objective function (7.8) it is necessary that the KL divergence between 𝑞 𝑗+1 (𝜃), given by (7.7), and P𝑞 𝑗 (𝜃), is well-defined and that we are able to compute it. In Section 7.5 we give an example of how this may be done for a class of algorithms that maintains 𝑞 𝑗 (𝜃), and P𝑞 𝑗 (𝜃), in Gaussian form. In Subsection 9.4.1 we show how the ideas may be used for ensemble methods where the KL divergence is not defined; we extract Gaussian approximations from the empirical distributions defined by the ensemble method. ♢ Imposing Filtering Structure On Smoothing An alternative way to use variational Bayes to derive filtering algorithms is to start from the variational formulation of smoothing and then impose a temporal structure on the resulting density, to enforce filtering; specifically to impose that the probability density function for state 𝑣 † 𝑗 at time 𝑗 only depends on past observations 𝑌 † 𝑗 , as defined in (6.3), and not future observations. In order to approximate a filtering distribution it is natural to seek minimizers of J(•) defined by (7.2a) which factorize in the form 𝑞(𝑉 † ) = 𝐽 ∏︁ 𝑖=1 q 𝑖 (𝑣 † 𝑖 |𝑉 † 𝑖-1 ; 𝑌 † 𝑖 )q 0 (𝑣 † 0 ); (7.11) thus 𝑞 ∈ 𝒫 𝐽 . For later use we also define 𝑞 𝑗 ∈ 𝒫 𝑗 by 𝑞 𝑗 (𝑉 † 𝑗 ) = 𝑗 ∏︁ 𝑖=1 q 𝑖 (𝑣 † 𝑖 |𝑉 † 𝑖-1 ; 𝑌 † 𝑖 )q 0 (𝑣 † 0 ). (7.12) We let 𝒞 𝐽 ⊂ 𝒫 𝐽 denote probability density functions of the form (7.11) . The use of symbol 𝒞 is to invoke the conditional structure in terms of the dependence of q 𝑖 on only 𝑌 † 𝑖 (recalling the convention, from Subsection 6.1.1, that 𝑌 † 0 = ∅). The smoothing distribution is, in general, contained in 𝒫 𝐽 ∖𝒞 𝐽 . This is since the conditional distribution of 𝑣 † 𝑖 |𝑉 𝑖-1 will, for smoothing, depend on 𝑌 † and not just on 𝑌 † 𝑖 . Remark 7.7. Ideally we would like to minimize the loss function J(•) defined in (7.2a) over probability measures of the form (7.11) . This, however, will not lead to a minimizer which is an element of 𝒞 𝐽 ; even if we prescribe the functional form of each q 𝑖 (𝑣 † 𝑖 |𝑉 † 𝑖-1 ; •), that is the dependence on 𝑌 † 𝑖 , the learned parameters in q 𝑖 will depend on the entire sequence 𝑌 † and so the resulting learned model will not have the desired conditional form. In short, there is no mechanism to ensure dependence of q 𝑖 on only 𝑌 † 𝑖 , and not on the entirety of 𝑌 † , since we have access only to one realization of the data, under Data Assumption 7.2. Overcoming this may be achieved by using amortization, assuming access to multiple realizations of the observation data -see Subsection 10.2.1. ♢ We now describe a way of circumventing the issue highlighted in Remark 7.7. The resulting methodology involves minimization of a sequence of objective functions J 𝑗 (•) each of which only sees 𝑌 † 𝑗 . The approach is based on the following theorem, which employs the notation (7.11), (7.12) and in which it is important to note the distinction between q 𝑗 and 𝑞 𝑗 : Theorem 7.8. Consider 𝑞 ∈ 𝒫 𝐽 factorized in the form (7.11), (7.12) so that 𝑞 ∈ 𝒞 𝐽 . Define the objective function J ′ : 𝒫 𝐽 → R by J 𝑗 (q 𝑗 ) = D KL (︀ q 𝑗 (•|𝑉 † 𝑗-1 ; 𝑌 † 𝑗 )‖𝜌 𝑗 )︀ -E q 𝑗 [︀ log l 𝑗 ]︀ , (7.13a) J ′ (𝑞) = 𝐽 ∑︁ 𝑗=1 E 𝑉 † 𝑗-1 ∼𝑞 𝑗-1 [︀ J 𝑗 (q 𝑗 ) ]︀ , ( 7.13b) where l 𝑗 (𝑣 † 𝑗 ) := P(𝑦 † 𝑗 |𝑣 † 𝑗 ). Then the minimizers of J given by (7.2) and of J ′ agree. Proof. First we note that minimizing J(•) is the same as minimizing D KL (•‖Π 𝑌 † ), as shown in the proof of Theorem 2.1. We will use the ELBO introduced in Definition 15.15 and the relationship between ELBO, KL divergence, and likelihood established in Theorem 15.18. Secondly we note that D KL (𝑞‖Π 𝑌 † ) = log P(𝑌 † ) -ELBO(P(𝑉 † , 𝑌 † ), 𝑞). Since the first term does not depend on 𝑞, we focus on the second term, given by ELBO(P(𝑉 † , 𝑌 † ), 𝑞) = E 𝑉 † ∼𝑞 [︃ log P(𝑉 † , 𝑌 † ) 𝑞(𝑉 † ) ]︃ . The joint density P(𝑉 † , 𝑌 † ) can be factorized autoregressively as P(𝑉 † , 𝑌 † ) = 𝐽 ∏︁ 𝑗=1 P(𝑦 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗 )P(𝑣 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗-1 ). Using this, as well as the factorized structure (7.11) of 𝑞, we obtain ELBO(P(𝑉 † , 𝑌 † ), 𝑞) = E 𝑞 ⎡ ⎣ log ⎛ ⎝ 𝐽 ∏︁ 𝑗=1 P(𝑦 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗 )P(𝑣 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗-1 ) q 𝑗 (𝑣 † 𝑗 |𝑉 † 𝑗-1 ; 𝑌 † 𝑗 ) ⎞ ⎠ ⎤ ⎦ = 𝐽 ∑︁ 𝑗=1 E 𝑞 ⎡ ⎣ log ⎛ ⎝ P(𝑦 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗 )P(𝑣 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗-1 ) q 𝑗 (𝑣 † 𝑗 |𝑉 † 𝑗-1 ; 𝑌 † 𝑗 ) ⎞ ⎠ ⎤ ⎦ . Since each term is only conditioned on variables up to time 𝑗, we can simplify to ELBO(P(𝑉 † , 𝑌 † ), 𝑞) = 𝐽 ∑︁ 𝑗=1 E 𝑞 𝑗 ⎡ ⎣ log ⎛ ⎝ P(𝑦 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗 )P(𝑣 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗-1 ) q 𝑗 (𝑣 † 𝑗 |𝑉 † 𝑗-1 ; 𝑌 † 𝑗 ) ⎞ ⎠ ⎤ ⎦ = 𝐽 ∑︁ 𝑗=1 E 𝑞 𝑗-1 E q 𝑗 ⎡ ⎣ log ⎛ ⎝ P(𝑦 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗 )P(𝑣 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗-1 ) q 𝑗 (𝑣 † 𝑗 |𝑉 † 𝑗-1 ; 𝑌 † 𝑗 ) ⎞ ⎠ ⎤ ⎦ = - 𝐽 ∑︁ 𝑗=1 E 𝑉 † 𝑗-1 ∼𝑞 𝑗-1 [︂ D KL (︁ q 𝑗 (𝑣 † 𝑗 |𝑉 † 𝑗-1 ; 𝑌 † 𝑗 )‖𝜌 𝑗 (𝑣 † 𝑗 ) )︁ -E 𝑣 † 𝑗 ∼q 𝑗 [︀ log l 𝑗 (𝑣 † 𝑗 ) ]︀ ]︂ , where the last line follows from noticing that P(𝑦 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗 ) = P(𝑦 † 𝑗 |𝑣 † 𝑗 ) = l 𝑗 (𝑣 † 𝑗 ) is the observation likelihood at time 𝑗, and P(𝑣 † 𝑗 |𝑌 † 𝑗-1 , 𝑉 † 𝑗-1 ) = P(𝑣 † 𝑗 |𝑣 𝑗-1 ) = 𝜌 𝑗 (𝑣 † 𝑗 ) is the prior for time 𝑗. Then, minimizing D KL (𝑞‖Π 𝑌 † ) is equivalent to minimizing J ′ (𝑞) given in (7.13) . Using this theorem, an approach to enforcing the auto-regressive filtering structure may be developed. Given a single realization of the data 𝑌 † it is reasonable to proceed as follows to enforce this dependence: instead of minimizing J(•) from (7.13b), we may sequentially minimize J 𝑗 (•) from (7.13a), over index 𝑗 = 1, . . . , 𝐽. Thus q ⋆ 𝑗 ∈ arg min q∈𝒫(R 𝑑 ) J 𝑗 (q), (7.14a) q ⋆ (𝑉 † ) = 𝐽 ∏︁ 𝑗=1 q ⋆ 𝑗 (𝑣 † 𝑗 |𝑉 † 𝑗-1 ; 𝑌 † 𝑗 )q ⋆ 0 (𝑣 † 0 ). (7.14b) Note that each J 𝑗 is found using data that only depends on 𝑌 † 𝑗 ; hence it is possible to enforce the desired dependence on each conditional q ⋆ 𝑗 (𝑣 † 𝑗 |𝑉 † 𝑗-1 ; 𝑌 † 𝑗 ). Variational Algorithm For Smoothing: Example We may apply the ideas of Chapter 2 to the smoothing problem, working under Data Assumption 7.2, to develop algorithms. The purpose of this section is to introduce an example of such an approach to smoothing, using variational inference. To this end we recall (7.3) and introduce a specific choice of 𝒬 𝐽 , the class of probability distributions within which we approximate the smoothing distribution. Recall the weak constraint 4DVar solution (6.54) which is the MAP estimator for the smoothing problem, as introduced in Subsection 6.4.1. Precompute this solution of an optimization problem, and call it 𝑉 ⋆ . Now let 𝒬 𝐽 = {︁ 𝑞 ∈ 𝒫 𝐽 : 𝑞 = 𝒩 (𝑉 ⋆ , Σ), Σ ∈ R 𝑑(𝐽+1)×𝑑(𝐽+1) sym,> }︁ . Thus in solving minimization problem (7.3) with this choice of variational space we seek a Gaussian approximation of the smoothing problem, centred on pre-computed weak constraint 4DVar solution. In this formulation we thus seek to optimize over parameter Σ. In practice we may wish to (Cholesky) square-root factorize Σ, as in Example 4.8, as a simple way to impose symmetry, and instead minimize over the space of triangular Cholesky factors. An amortized version of the methodology from this section is described in Example 10.7. Variational Algorithm For Filtering: Example Here we give an example of the methodology introduced in Section 7.2, centered around optimizing parameter 𝜃 appearing in (7.7). We formulate the algorithm class around the idea of learning a gain matrix, 𝐾, that defines a family of updates for the approximate filtering distributions 𝑞 𝑗 (𝜃); thus, in the notation of Section 7.2, 𝜃 = 𝐾. Although we will ultimately work with a Gaussian approximation of the true filtering distribution, we start by considering a more complicated approximate model for the evolution of the filtering distribution; we then derive the Gaussian approximation from it. Let 𝑣 𝑗 denote an, in general, non-Gaussian random vector whose law defines an approximate filtering distribution. We assume that 𝑣 𝑗 is defined through the following 3DVar-like recursion: ︀ 𝑣 𝑗+1 = Ψ(𝑣 𝑗 ) + 𝜉 𝑗 , 𝜉 𝑗 ∼ 𝒩 (0, Σ) i.i.d., (7.15a ) 𝑣 𝑗+1 = ̂︀ 𝑣 𝑗+1 + 𝐾 (︀ 𝑦 † 𝑗+1 + 𝜂 𝑗+1 -ℎ(̂︀ 𝑣 𝑗+1 ) )︀ , 𝜂 𝑗+1 ∼ 𝒩 (0, Γ) i.i.d. ( 7 .15b) 𝑣 0 ∼𝒩 (𝑚 0 , 𝐶 0 ). (7.15c) If we then postulate that the distribution of 𝑣 𝑗 is approximately Gaussian we can derive the Gaussian approximation by a linearization procedure. (Indeed the following Gaussian approximation is exact if Ψ and ℎ are linear.) To that end we seek the filtering distribution in form 𝑞 𝑗 (𝐾) = 𝒩 (𝑚 𝑗 , 𝐶 𝑗 ), and define the predictive distribution ̂︀ 𝑞 𝑗+1 (𝐾) by the Gaussian 𝒩 ( ̂︀ 𝑚 𝑗+1 , ̂︀ 𝐶 𝑗+1 ). A linearization procedure applied to the law of 𝑣 𝑗 evolving according to (7.15) then yields the evolution 𝑞 𝑗 (𝐾) ↦ → 𝑞 𝑗+1 (𝐾) as follows: ︀ 𝑚 𝑗+1 = Ψ(𝑚 𝑗 ), ̂︀ 𝐶 𝑗+1 = 𝐴 𝑗 𝐶 𝑗 𝐴 ⊤ 𝑗 + Σ, (7.16a ) 𝑚 𝑗+1 = ̂︀ 𝑚 𝑗+1 + 𝐾(𝑦 † 𝑗+1 -ℎ( ̂︀ 𝑚 𝑗+1 )), (7.16b ) 𝐶 𝑗+1 = (𝐼 -𝐾𝐻 𝑗+1 ) ̂︀ 𝐶 𝑗+1 (𝐼 -𝐾𝐻 𝑗+1 ) ⊤ + 𝐾Γ𝐾 ⊤ , ( 7.16c ) 𝐴 𝑗 = 𝐷Ψ(𝑚 𝑗 ), 𝐻 𝑗 = 𝐷ℎ( ̂︀ 𝑚 𝑗 ). (7.16d) Note the resemblance to the extended Kalman filter from Subsection 6.3.3, with two differences: here we learn the gain 𝐾 rather than compute it with (6.25b); and we employ a different formula for the update of ̂︀ 𝐶 𝑗+1 ↦ → 𝐶 𝑗+1 due to the use of a non-Kalman gain. We now generalize the optimization problems from Subsection 7.2 to this Gaussian approximation. Map 𝑞 𝑗+1 (𝐾) → ̂︀ 𝑞 𝑗+1 (𝐾) maps the Gaussian manifold into itself and is defined by (7.16a), (7.16d ). This mapping is not equal to P, but rather is a Gaussian approximation of it, which we denote by P G . Map P G is only defined on the Gaussian manifold; on that manifold it has the property that P G = P if Ψ and ℎ are linear. Map ̂︀ 𝑞 𝑗+1 (𝐾) ↦ → 𝑞 𝑗+1 (𝐾) also maps the Gaussian manifold into itself and is defined by (7.16b)-(7.16d); this is the map A 𝐾 (•; 𝑦 † 𝑗+1 ). We may generalize (7.8) to this approximate Gaussian setting to obtain J 𝑗+1 (𝐾) = D KL (︀ 𝑞 𝑗+1 (𝐾)‖P G 𝑞 𝑗 (𝐾) )︀ -E 𝑞 𝑗+1 (𝐾) [log P(𝑦 † 𝑗+1 |•)]. This gives the optimization problem J 𝑗+1 (𝐾) =D KL (︀ 𝒩 (𝑚 𝑗+1 , 𝐶 𝑗+1 )‖𝒩 ( ̂︀ 𝑚 𝑗+1 , ̂︀ 𝐶 𝑗+1 ) )︀ + 1 2 E 𝒩 (𝑚 𝑗+1 ,𝐶 𝑗+1 ) |𝑦 † 𝑗+1 -ℎ(•)| 2 Γ . (7.17) Recalling the formula (11.18a) for the KL divergence between two 𝑑-dimensional Gaussians we see the first term of (7.17) can be evaluated in closed form. Evaluation of the second term, for nonlinear ℎ, will require quadrature or sampling. We can then consider the online learning problem (7.10), in which we learn a time-dependent gain 𝐾 ⋆ 𝑗 at each analysis time: 𝐾 ⋆ 𝑗 ∈ arg min 𝐾∈R 𝑑×𝑘 J 𝑗 (𝐾); or the offline problem (7.9) in which a single fixed gain 𝐾 ⋆ is learned for all analysis times: 𝐾 ⋆ ∈ arg min 𝐾∈R 𝑑×𝑘 𝐽 ∑︁ 𝑗=1 J 𝑗 (𝐾). Bibliography Several works have studied variational formulations of smoothing and filtering problems. The use of Gaussian approximations in filtering, including via variational Bayes, is studied in the papers [116, 432, 23] . Combinations with ensemble methods are discussed in [433] . The paper [158] employs mean-field models. Variational filtering is discussed in [292] , and the proof of Theorem 7.8 is given there. Learning parameterized filters using the variational objective introduced in Section 7.2 is considered in [35] ; in particular that work includes introduction and deployment of the learnable Gaussian filter described in Section 7.5. An alternative variational objective for jointly learning a parameterized filter and the system dynamics is given in [71] . A variational formulation of the filtering and smoothing problems in continuous time, where the posterior is restricted to be in the exponential family, is provided in [406] . The variational formulation of the optimal continuous-time filter (the Kushner-Stratonovich equation) with no dynamics is derived in [267] . The latter is formally combined with a variational formulation of the Fokker-Planck equation for gradient systems in [259] , and it is shown that the solution coincides with the Kalman-Bucy filter in the linear case. The connection between 4DVar and variational inference is discussed in [148] , and the idea of using variational inference and Gaussian approximations to extend 4DVar to posterior estimation is explored in [149] . Chapter 8 Learning The Prior For Data Assimilation This chapter is devoted to learning priors for smoothing and filtering problems. In the smoothing problem, the prior is determined by the stochastic dynamics model; thus, in this chapter we will consider learning the dynamics model, along with the state, from data. In the filtering problem, there is a prior at each time point, found by predicting with the dynamics model, starting from an approximation of the filtering distribution; this prior is then combined with a likelihood to incorporate the next observation. In this chapter, we will consider learning priors for the filtering step using Gaussian and particle-based approximations. Learning priors for data assimilation is intimately related to the important problem of jointly estimating the state and parameters of a stochastic dynamics model; this problem is ubiquitous in data assimilation, and it arises whenever the available dynamics models are inaccurate or expensive to evaluate. When they are inaccurate it may be desirable to learn a correction which enhances their accuracy; when they are expensive it may be desirable to learn a cheap-to-evaluate surrogate. Such a correction or surrogate may be represented by a neural network, random features, or Gaussian process model, for example. We addressed the topic of learning the prior for general inverse problems in Chapter 3 and that material may all be applied in the context of smoothing, viewed as an inverse problem. In particular Section 3.1 considers Bayesian hierarchical modeling in which the unknown parameter, and the parameters of the prior, are jointly learned from the single observation available. In this chapter we consider this Bayesian hierarchical problem in the context of the smoothing problem, learning the state of the system and parameters of the dynamics model simultaneously. To solve this joint estimation problem, we will leverage filtering algorithms to estimate the state. We introduce the problem of learning priors for smoothing in Section 8.1. The next two sections discuss general methodologies for solving this problem. Section 8.2 describes the expectation-maximization (EM) framework for joint parameter and state estimation. In particular, we will present practical algorithmic instances of this framework to estimate parameters defining the dynamics map Ψ and/or the error covariance Σ. Section 8.3 discusses auto-differentiable Kalman filters, an approach to maximum likelihood estimation of the parameters that relies on Kalman-type algorithms to approximate the likelihood function, and on auto-differentiation to approximate its gradient. In Section 8.4 we discuss the learning of prior distributions for each step of the filtering problem, considering the use of empirical priors and Gaussian priors constructed from the outcome of the preceding step of the filtering process. Section 8.5 closes with extensions and bibliographical remarks. Learning Priors For Smoothing: Problem Setting Consider the stochastic dynamics and observation models given by 𝑣 † 𝑗+1 = Ψ 𝜗 (𝑣 † 𝑗 ) + 𝜉 † 𝑗 , 𝜉 † 𝑗 ∼ 𝒩 (0, Σ) i.i.d., (8.1a ) 𝑦 † 𝑗+1 = ℎ(𝑣 † 𝑗+1 ) + 𝜂 † 𝑗+1 , 𝜂 † 𝑗 ∼ 𝒩 (0, Γ) i.i.d., (8.1b) with the independence structure summarized in Assumption 6.1. Here, 𝜗 are unknown parameters entering the definition of the dynamics map Ψ 𝜗 and we assume the error covariance Σ to be also, in general, unknown. We denote by 𝜃 := {𝜗, Σ} the collection of unknown parameters; it is possible to extend to parameterizations of the covariance Σ, rather than to learn the entire covariance matrix, as discussed in Remark 6.25, but we do not pursue this here. For a given and fixed integer 𝐽, we recall the following notation from Subsection 6.1.1: 𝑉 † := {𝑣 † 0 , . . . , 𝑣 † 𝐽 }, 𝑌 † := {𝑦 † 1 , . . . , 𝑦 † 𝐽 }. Note that 𝑉 † ∈ R (𝐽+1)𝑑 and 𝑌 † ∈ R 𝐽𝑘 . We make the following: Data Assumption 8.1. Data 𝑌 † is given and assumed to come from the model (8.1) . Given data 𝑌 † we wish to recover the parameter 𝜃 defining the dynamics model. In this process some algorithms will estimate the state 𝑉 † as well. Furthermore, once 𝜃 is learned and the dynamics model is known, the filtering and smoothing algorithms from Chapter 6 can be used to estimate the state, using the same given data 𝑌 † or using new realizations of the data from the same system. The aim of determining parameters 𝜗 in the dynamics map Ψ 𝜗 from data resembles the task of supervised learning in Chapter 12. However, here we do not assume to have direct data {︀ 𝑣 (𝑛) , Ψ 𝜗 (𝑣 (𝑛) ) }︀ 𝑁 𝑛=1 to learn the parameter 𝜗. Instead, we only have access to data 𝑌 † obtained from indirect and noisy measurement of a trajectory 𝑉 † . It is then natural to seek to recover 𝑉 † as a step towards recovering 𝜃; this idea underlies the EM and auto-differentiable Kalman filtering algorithms introduced in the next two sections. Before presenting these algorithms, we consider three motivating examples for wishing to learn the parameter 𝜗 entering the definition of the dynamics map Ψ 𝜗 in (8.1a). Example 8.2 (Parameterized Dynamics). The state dynamics may be governed by the flow of a parameterized system of differential equations, and we may be interested in estimating a parameter 𝜗 entering the definition of the vector field in the differential equations from data 𝑌 † . ♢ Example 8.3 (Fully Unknown Dynamics). The state dynamics may be fully unknown and 𝜗 may represent the parameters of, for example, a neural network, random features, or Gaussian process surrogate model Ψ 𝜗 ; see Chapter 12 for background on these parameterizations of functions. The goal is to find an accurate surrogate model Ψ 𝜗 . Even if the true dynamics were known, a surrogate model, or emulator, can be cheaper to evaluate, enabling the use of large sample sizes for particle filters or ensemble Kalman methods; see Section 6.6. Similar computational considerations motivate learning the forward map for inverse problems in Section 1.5. ♢ Example 8.4 (Model Correction). The dynamics may be unknown, but we may still have access to an inaccurate model Ψ approx . Here 𝜗 can represent, for example, the parameters of a neural network, random features, or Gaussian process Ψ correction 𝜗 used to correct the inaccurate model Ψ approx . The goal is to learn 𝜗 so that Ψ 𝜗 := Ψ approx + Ψ correction 𝜗 approximates the true dynamics of the state. Learning model corrections is important in applications where available models have moderate predictive accuracy. For instance, fine scales of the state may not be resolved accurately due to computational constraints, and we may be interested in learning a surrogate model from data which accounts for the unresolved scales of the system. ♢ Expectation Maximization In this section we work under Data Assumption 8.1, and so have access only to data 𝑌 † obtained from indirect and noisy measurement of a signal trajectory. We view the problem of estimating 𝜃 := {𝜗, Σ} from data 𝑌 † as a missing data problem: ideally we estimate 𝜃 from both 𝑉 † and 𝑌 † ; but the data 𝑉 † is missing and we must first estimate 𝑉 † from 𝑌 † . We adopt a maximum likelihood approach to the problem of estimating 𝜃 from 𝑌 † : we aim to maximize P(𝑌 † |𝜃). The premise of the methodology we now introduce is that evaluation of the likelihood function P(𝑌 † |𝜃) is challenging due to the missing, or latent, variable 𝑉 † ; but that the joint distribution P(𝑌 † , 𝑉 † |𝜃) is readily available. With this in mind we seek to address the problem of missing data by employing the identity P(𝑌 † |𝜃) = ∫︁ P(𝑌 † , 𝑉 † |𝜃) 𝑑𝑉 = ∫︁ P(𝑌 † |𝑉 † , 𝜃) P(𝑉 † |𝜃) 𝑑𝑉 † . (8.2) Thus we write the desired likelihood by marginalizing over a distribution P(𝑉 † |𝜃) for the unknown states 𝑉 † ; here, and throughout this chapter, integrals are over R (𝐽+1)𝑑 . Remark 8.5. Likelihood functions that are computationally expensive to evaluate, such as the integral in (8.2), motivated the introduction of likelihood-free methods for Bayesian inference in Chapter 5. In this chapter, we will focus on optimization algorithms, rather than Bayesian methods, to find point estimators for the parameter 𝜃. ♢ The likelihood P(𝑌 † |𝜃) in (8.2) is typically intractable to compute, but expressible by marginalization of the tractable extended likelihood P(𝑌 † , 𝑉 † |𝜃). Consequently, the maximum likelihood estimator of 𝜃 can be naturally computed by employing the expectation-maximization (EM) algorithm described in Section 15.4. Specifically, in the notation of that section, we consider parameter 𝑢 := 𝜃, missing variable 𝑧 := 𝑉 † and data 𝑦 := 𝑌 † . The identity (15.16) shows that, in order to implement the EM algorithm, we need to identify the extended likelihood P(𝑌 † , 𝑉 † |𝜃). This can be readily computed from the dynamics and observation models in (8.1): log P(𝑌 † , 𝑉 † |𝜃) = log P(𝑌 † |𝑉 † , 𝜃) + log P(𝑉 † |𝜃) = - 1 2 𝐽-1 ∑︁ 𝑗=0 ⃒ ⃒ 𝑦 † 𝑗+1 -ℎ(𝑣 † 𝑗+1 ) ⃒ ⃒ 2 Γ - 1 2 |𝑣 † 0 -𝑚 0 | 2 𝐶 0 - 1 2 log det(Σ) - 1 2 𝐽-1 ∑︁ 𝑗=0 ⃒ ⃒ 𝑣 † 𝑗+1 -Ψ 𝜗 (𝑣 † 𝑗 ) ⃒ ⃒ 2 Σ + 𝑐, (8.3) where 𝑐 is a constant independent of 𝑉 † , 𝑌 † and 𝜃. We recall that the E-step in the EM algorithm -see Algorithm 15.1 -integrates the log-likelihood log P(𝑌 † , 𝑉 † |𝜃) in (8.3) with respect to 𝑞 ℓ (𝑉 † ) = P(𝑉 † |𝑌 † , 𝜃 ℓ ). Then 𝜃 ℓ+1 is computed in the M-step by maximizing the expectation resulting from this integration, with respect to 𝜃. The resulting maximization problem, dropping from the objective terms that do not depend on 𝜃, is as follows: L(𝜃; 𝑞 ℓ ) = - 1 2 log det(Σ) - 1 2 ∫︁ 𝐽-1 ∑︁ 𝑗=0 ⃒ ⃒ 𝑣 † 𝑗+1 -Ψ 𝜗 (𝑣 † 𝑗 ) ⃒ ⃒ 2 Σ 𝑞 ℓ (𝑉 † ) 𝑑𝑉 † , ( 8.4 ) 𝜃 ℓ+1 ∈ arg max 𝜃 L(𝜃; 𝑞 ℓ ), emphasizing that 𝑞 ℓ (𝑉 † ) depends on 𝜃 ℓ . To illustrate the application of the EM algorithm, in the next Subsection 8.2.1 we discuss a concrete simplified setting: learning only the model error covariance Σ, assuming a known dynamics map Ψ. Then, in Subsection 8.2.2 we return to the problem of learning 𝜃 = {𝜗, Σ}. Finally, in Subsection 8.2.3 we consider the application of the EM algorithm in a model correction setting with known error covariance Σ, and make a connection to supervised learning. Learning Model Error Covariance In this subsection we consider the setting where the dynamics map Ψ is known, but the dynamics error covariance Σ is unknown. That is, we have 𝜃 = Σ. In this setting, we notice that the loss function (8.4 ) in the M-step is separable as the sum of two loss functions that each depend on only one parameter Σ. Furthermore, the loss function depending on Σ has the same form as the objective in Proposition 2.7 for an unknown covariance in a Gaussian variational inference problem. Thus, the loss can be explicitly maximized over Σ. Setting 𝜃 ℓ = Σ ℓ , and using the explicit optimizer, we find the covariance matrix updates Σ ℓ+1 = ∫︁ 𝐽-1 ∑︁ 𝑗=0 (︁ 𝑣 † 𝑗+1 -Ψ(𝑣 † 𝑗 ) )︁ ⊗ (︁ 𝑣 † 𝑗+1 -Ψ(𝑣 † 𝑗 ) )︁ 𝑞 ℓ (𝑉 † ) 𝑑𝑉 † , ( 8.5) where we recall that 𝑞 ℓ (𝑉 † ) = P(𝑉 † |𝑌 † , 𝜃 ℓ ). The following result shows that this update for the parameter Σ leads to a monotonic increase of the log-likelihood for the observed data 𝑌 † . Theorem 8.6. The iterates (8.5) of the EM algorithm for 𝜃 = Σ satisfy log P(𝑌 † |𝜃 ℓ+1 ) ≥ log P(𝑌 † |𝜃 ℓ ). Proof. The function L(𝜃; 𝑞 ℓ ) in the M-step is concave in 𝜃 and so 𝜃 ℓ+1 = Σ ℓ+1 is the unique global optimum, i.e., 𝜃 ℓ+1 ∈ arg max 𝜃 L(𝜃; 𝑞 ℓ ). From Theorem 15.19, the iterates of the EM algorithm with the exact optimal parameters in the M-step satisfy the desired monotonic increase in the log-likelihood. Remark 8.7. The monotonic increase of the likelihood in the population setting does not guarantee that the EM iterations converge. Additional assumptions are required to show that there is a strict improvement in the likelihood at each iteration and to provide a condition where the parameters reach a local critical point. Furthermore, in practice we will only have samples from an empirical approximation of 𝑞 ℓ (𝑉 † ) = P(𝑉 † |𝑌 † , 𝜃 ℓ ) and so we may update the covariances using Monte Carlo approximations of the covariance matrices in (8.5) . With these Monte Carlo approximations the monotonic increase in the likelihood is no longer guaranteed. ♢ Monte Carlo EM Now we return to the general setting of finding parameter 𝜃 = {𝜗, Σ}. Given (approximate) samples {(𝑉 † ) (𝑛) } 𝑁 𝑛=1 from 𝑞 ℓ (𝑉 † ) = P(𝑉 † |𝑌 † , 𝜃 ℓ ), we may use Monte Carlo to approximate the expectation in the loss function to obtain the empirical approximation L(𝜃; 𝑞 ℓ ) ≈ 𝑐 - 1 2 log det(Σ) - 1 2𝑁 𝑁 ∑︁ 𝑛=1 𝐽-1 ∑︁ 𝑗=0 ⃒ ⃒ (𝑣 † 𝑗+1 ) (𝑛) -Ψ 𝜗 (︀ (𝑣 † 𝑗 ) (𝑛) )︀⃒ ⃒ 2 Σ , ( 8.6) where 𝑐 accounts for terms that are constant with respect to the unknown parameter 𝜃. As in the last subsection, we notice that, given 𝜗, the expression (8.6) can be explicitly maximized over Σ. Indeed, the optimal solution is given by the covariance matrix Σ ℓ+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝐽-1 ∑︁ 𝑗=0 (︁ (𝑣 † 𝑗+1 ) (𝑛) -Ψ 𝜗 (︀ (𝑣 † 𝑗 ) (𝑛) )︀ )︁ ⊗ (︁ (𝑣 † 𝑗+1 ) (𝑛) -Ψ 𝜗 (︀ (𝑣 † 𝑗 ) (𝑛) )︀ )︁ . We note that this covariance depends on the parameters 𝜗 via the model Ψ 𝜗 . To maximize the loss L(𝜃; 𝑞 ℓ ) over 𝜃 = {𝜗, Σ}, we can employ iterative optimization methods: alternate optimization over Σ, for fixed 𝜗, as given in the preceding subsection, with optimization over 𝜗. Algorithm 8.1 summarizes a resulting EM type algorithm, for learning both states and parameters of a dynamical system, using Monte Carlo and gradient ascent updates. We notice that this implementation of the EM framework treats differently the parameters 𝜗 and Σ in the M-step to account for the closed-form update for Σ ℓ+1 . Remark 8.8. The distribution P(𝑉 † |𝑌 † , 𝜃 ℓ ) for the signal {𝑣 † 𝑗 } can be approximated by performing data assimilation using the stochastic dynamics and data models in (8.1) Algorithm 8.1 Expectation-Maximization with Monte Carlo and Gradient Ascent 1: Input: Initialization 𝜃 0 = {𝜗 0 , Σ 0 }. Rule for gradient ascent step-sizes {𝛼 𝑖 } ℐ 𝑖=1 . 2: For ℓ = 0, 1, . . . , 𝐿 -1 do the following expectation and maximization steps: 3: E-Step: Obtain (approximate) samples {(𝑉 † ) (𝑛) } 𝑁 𝑛=1 from P(𝑉 † |𝑌 † , 𝜃 ℓ ) and use these samples to approximate L(𝜃; 𝑞 ℓ ) as in (8.6) . 4: M-Step: 5: Set Σ ℓ+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝐽-1 ∑︁ 𝑗=0 (︁ (𝑣 † 𝑗+1 ) (𝑛) -Ψ 𝜗 ℓ (︀ (𝑣 † 𝑗 ) (𝑛) )︀ )︁ ⊗ (︁ (𝑣 † 𝑗+1 ) (𝑛) -Ψ 𝜗 ℓ (︀ (𝑣 † 𝑗 ) (𝑛) )︀ )︁ . 6: Set Σ ℓ+1 (𝜗) = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝐽-1 ∑︁ 𝑗=0 (︁ (𝑣 † 𝑗+1 ) (𝑛) -Ψ 𝜗 (︀ (𝑣 † 𝑗 ) (𝑛) )︀ )︁ ⊗ (︁ (𝑣 † 𝑗+1 ) (𝑛) -Ψ 𝜗 (︀ (𝑣 † 𝑗 ) (𝑛) )︀ )︁ and define J (︀ 𝜗, {(𝑉 † ) (𝑛) } 𝑁 𝑛=1 )︀ := - 1 2𝑁 𝑁 ∑︁ 𝑛=1 𝐽-1 ∑︁ 𝑗=0 ⃒ ⃒ ⃒(𝑣 † 𝑗+1 ) (𝑛) -Ψ 𝜗 (︀ (𝑣 † 𝑗 ) (𝑛) )︀ ⃒ ⃒ ⃒ 2 Σ ℓ+1 (𝜗) . 7: Initialize 𝜗 ℓ,0 = 𝜗 ℓ . 8: for 𝑖 = 0, . . . , ℐ -1 do 9: 𝜗 ℓ,𝑖+1 = 𝜗 ℓ,𝑖 + 𝛼 𝑖 𝐷 𝜗 J(𝜗 ℓ,𝑖 , {(𝑉 † ) (𝑛) } 𝑁 𝑛=1 ). 10: end for 11: Set 𝜗 ℓ+1 = 𝜗 ℓ,ℐ . 12: Output: Approximation 𝜃 𝐿 to the maximum likelihood estimate for 𝜃. with learned parameter 𝜗 ℓ and covariance model Σ ℓ . These samples may be obtained, for instance, using Markov chain Monte Carlo methods. Alternatively, the ensemble Kalman methods described in Chapter 6 may be employed, with the proviso that they can yield accurate posterior samples only in approximately linear-Gaussian settings. ♢ Remark 8.9. In analogy to the supervised learning task studied in Chapter 12, equation (8.4) may be conceptually interpreted as defining a risk for the parameter 𝜃, where the distribution P(𝑉 † |𝑌 † , 𝜃 ℓ ) represents our available knowledge of the latent variable 𝑉 † given data 𝑌 † and parameter estimate 𝜃 ℓ . Then, (8.6) can be interpreted as an empirical risk, defined via approximate samples (𝑉 † ) (𝑛) ∼ P(𝑉 † |𝑌 † , 𝜃 ℓ ). ♢ Remark 8.10. While we present Algorithm 8.1 using gradient ascent, in practice we may employ other optimization methods such as accelerated first-order methods or second-order methods, like Gauss-Newton to optimize the parameter 𝜗; we refer to Chapter 15 for more details. The choice of algorithm may depend on computational constraints and properties of the loss function, such as its smoothness. ♢ Correcting Model Error Using Analysis Increments Here we assume the model error covariance Σ to be known, and consider the problem of learning the parameter 𝜃 := 𝜗 in the dynamics map model correction setting introduced in Example 8.4. That is, we consider a parameterization of the dynamics map of the form Ψ 𝜗 = Ψ approx + Ψ correction 𝜗 , ( 8.7) where Ψ approx is a known approximation to the dynamics map and we seek to find a correction Ψ correction 𝜗 . In this setting, there is a natural connection between the EM framework and supervised learning. Specifically, approximating 𝑞 ℓ (𝑉 † ) in the E-step using trajectories {(𝑉 † ) (𝑛) } 𝑁 𝑛=1 computed with EnKF (or another filtering or smoothing algorithm) we obtain an empirical approximation of the loss function in (8.4) of the form L 𝑁 (𝜃; 𝑞 ℓ ) = 𝑐 - 1 2𝑁 𝑁 ∑︁ 𝑛=1 𝐽-1 ∑︁ 𝑗=0 ⃒ ⃒ ⃒(𝑣 † 𝑗+1 ) (𝑛) -Ψ approx (︀ (𝑣 † 𝑗 ) (𝑛) )︀ -Ψ correction 𝜗 (︀ (𝑣 † 𝑗 ) (𝑛) )︀ ⃒ ⃒ ⃒ 2 Σ ; (8.8) here 𝑐 is a constant that is independent of the parameter 𝜃 = 𝜗 to be estimated. We recognize that maximizing (8.8) over 𝜃 corresponds to the supervised learning problem of estimating the mapping from 𝑣 † 𝑗 to the increments 𝑣 † 𝑗+1 -Ψ approx (𝑣 † 𝑗 ) given (approximate) analysis samples (𝑉 † ) (𝑛) . Remark 8.11. The Monte Carlo samples {(𝑉 † ) (𝑛) } 𝑁 𝑛=1 of the time-series may have been created without the model error correction; this amounts to assuming Ψ approx is the true dynamics model, in which case the loss in (8.8) is simply L(𝜃). Alternatively they may have been created from the corrected model for a setting 𝜗 ℓ that was found in an earlier iteration of the EM algorithm. In the latter case, the maximization of L(•; 𝑞 ℓ ) yields 𝜃 ℓ+1 . ♢ Auto-Differentiable Kalman Filters Consider again the problem of estimating the unknown parameters 𝜃 = {𝜗, Σ}. Here we make the additional assumption that the observation operator is linear: ℎ(𝑣) = 𝐻𝑣 for some matrix 𝐻 ∈ R 𝑘×𝑑 . We again work under Data Assumption 8.1, but consider a different algorithmic framework, based on two ideas: firstly approximating the loglikelihood log P(𝑌 † |𝜃) using Kalman-based filters; and secondly computing its derivatives via automatic differentiation. The goal is to perform (approximate) gradient ascent on the log-likelihood function; we refer to Section 15.1 for background on gradient descent (in this section ascent as we seek to maximize the likelihood) and to Section 15.6 for background on automatic differentiation. We will illustrate the idea of approximating the log-likelihood function using Kalman-based filters by focusing on the extended Kalman filter (ExKF), but the methodology directly generalizes to the ensemble Kalman filter (EnKF) and other Kalman-based filtering algorithms. Recall from Subsection 6.3.3 that the ExKF is based on a Gaussian approximation to the prediction and analysis distributions: P(𝑣 † 𝑗+1 |𝑌 † 𝑗 , 𝜃) ≈ 𝒩 (︀ ̂︀ 𝑚 𝑗+1 (𝜃), ̂︀ 𝐶 𝑗+1 (𝜃) )︀ (8.9) for the predictive distribution; and P(𝑣 † 𝑗+1 |𝑌 † 𝑗+1 , 𝜃) ≈ 𝒩 (︀ 𝑚 𝑗+1 (𝜃), 𝐶 𝑗+1 (𝜃) )︀ (8.10) for the analysis distribution. The linearity of ℎ(•) is used in the derivation of these Gaussian approximations. Note that here we make explicit the dependence of computed means and covariances on the parameter 𝜃. In this section we show how the Gaussian ansatz (8.9) can be leveraged to produce a ExKF-based approximation of the log-likelihood function log P(𝑌 † |𝜃) and thereby an algorithm for maximum likelihood estimation of parameters 𝜃 in the stochastic dynamics model. It is intrinsic to the methodology presented that the observation operator is linear. The derivation rests on the following characterization of the log-likelihood function under a Gaussian ansatz. Theorem 8.12. Assume that the observation operator is linear: ℎ(•) = 𝐻 • . Suppose that, for each 0 ≤ 𝑗 ≤ 𝐽 -1, the predictive distribution P(𝑣 † 𝑗+1 |𝑌 † 𝑗 , 𝜃) of the stochastic dynamics and data models (8.1) is Gaussian with mean ̂︀ 𝑚 𝑗+1 (𝜃) and covariance ̂︀ 𝐶 𝑗+1 (𝜃). Then the log-likelihood function admits the following characterization log P(𝑌 † |𝜃) = - 1 2 𝐽-1 ∑︁ 𝑗=0 ⃒ ⃒ 𝑦 † 𝑗+1 -𝐻 ̂︀ 𝑚 𝑗+1 (𝜃) ⃒ ⃒ 2 𝑆 𝑗+1 (𝜃) - 1 2 𝐽-1 ∑︁ 𝑗=0 log det (︀ 𝑆 𝑗+1 (𝜃) )︀ , ( 8.11) where 𝑆 𝑗+1 (𝜃) = 𝐻 ̂︀ 𝐶 𝑗+1 (𝜃)𝐻 ⊤ + Γ. Proof. We have log P(𝑌 † |𝜃) = 𝐽-1 ∑︁ 𝑗=0 log P(𝑦 † 𝑗+1 |𝑌 † 𝑗 , 𝜃), where we use the convention that 𝑌 † 0 := 0 so that conditioning to 𝑌 † 0 does not provide any information. Now conditioning in the data model 𝑦 † 𝑗+1 = 𝐻𝑣 † 𝑗+1 + 𝜂 † 𝑗+1 , we see that E [︀ 𝑦 † 𝑗+1 |𝑌 † 𝑗 , 𝜃 ]︀ = E [︀ 𝐻𝑣 † 𝑗+1 + 𝜂 † 𝑗+1 |𝑌 † 𝑗 , 𝜃 ]︀ = 𝐻 ̂︀ 𝑚 𝑗+1 (𝜃), Cov [︀ 𝑦 † 𝑗+1 |𝑌 † 𝑗 , 𝜃 ]︀ = Cov [︀ 𝐻𝑣 † 𝑗+1 + 𝜂 † 𝑗+1 |𝑌 † 𝑗 , 𝜃 ]︀ = 𝐻 ̂︀ 𝐶 𝑗+1 (𝜃)𝐻 ⊤ + Γ. Moreover, P(𝑦 † 𝑗+1 |𝑌 † 𝑗 , 𝜃) is Gaussian. The result follows. Thus, for any value of 𝜃, we may obtain an approximation of the log-likelihood log P(𝑌 † |𝜃) by running a Kalman filtering algorithm, obtaining predictive means and covariances (︀ ̂︀ 𝑚 𝑗+1 (𝜃), ̂︀ 𝐶 𝑗+1 (𝜃) )︀ , for 0 ≤ 𝑗 ≤ 𝐽 -1, and using (8.11) . Moreover, an approximation of the log-likelihood gradient at 𝜃 can be obtained auto-differentiating through our Kalman-based likelihood estimate. Auto-differentiable Kalman filters use these estimates of the log-likelihood gradients to conduct gradient ascent. The procedure is summarized in Algorithm 8.2, for which we define J(𝜃) := - 1 2 𝐽-1 ∑︁ 𝑗=0 ⃒ ⃒ 𝑦 † 𝑗+1 -𝐻 ̂︀ 𝑚 𝑗+1 (𝜃) ⃒ ⃒ 2 𝑆 𝑗+1 (𝜃) - 1 2 𝐽-1 ∑︁ 𝑗=0 log det (︀ 𝑆 𝑗+1 (𝜃) )︀ . ( 8.12) Algorithm 8.2 Auto-Differentiable Kalman Filter 1: Input: Initialization 𝜃 0 , rule to choose step-sizes {𝛼 ℓ } 𝐿-1 ℓ=0 . 2: For ℓ = 0, 1, . . . , 𝐿 -1 do the following Kalman filtering and gradient ascent steps: 3: Kalman Filtering: Run an extended (or ensemble) Kalman filtering algorithm to obtain predictive means and covariances ̂︀ 𝑚 𝑗+1 (𝜃 ℓ ), ̂︀ 𝐶 𝑗+1 (𝜃 ℓ ), for 0 ≤ 𝑗 ≤ 𝐽 -1. 4: Gradient Ascent: Auto-differentiate the map 𝜃 ↦ → J(𝜃) defined in (8.12) to obtain a gradient estimate 𝐷J(𝜃 ℓ ). Set 𝜃 ℓ+1 = 𝜃 ℓ + 𝛼 ℓ 𝐷J(𝜃 ℓ ). ( 8 .13) 5: Output: Approximation 𝜃 𝐿 to the maximum likelihood estimate for 𝜃. Remark 8.13. If desired, the signal {𝑣 † 𝑗 } can be estimated by performing data assimilation with the stochastic dynamics and data models (8.1) with learned parameter 𝜃 𝐿 . ♢ Remark 8.14. Notice that in the EM Algorithm 8.1 each posterior sample {(𝑉 † ) (𝑛) } 𝑁 𝑛=1 is used to perform ℐ gradient descent steps in the M-Step, with the goal of maximizing the lower bound ℒ(𝑞 ℓ , 𝜃). In contrast, in the auto-differentiable Kalman filter Algorithm 8.2, each run of a Kalman filtering algorithm is used to produce an estimate of the log-likelihood gradient, and a gradient ascent step is taken. This is possible because we can analytically integrate out the state variable, given a linear observation operator and Gaussian predictive distribution. ♢ Learning Priors For The Filtering Step From Samples The preceding sections have focused on learning the prior for the smoothing problem, in the setting described in Section 8.1. In this section we study learning the prior, and generalizations, for the filtering problem. We start by describing how particle-based filtering methods can be seen as learning a prior for the analysis step in the filtering cycle, in both Subsections 8.4.1 and 8.4.2. These methods are all based on learning, from samples, either a non-parametric or parametric distribution for the prior used in the analysis step; the resulting prior is either an empirical measure (non-parametric) or a Gaussian (parametric) measure and in either case is found from a form of density estimation (see Subsection 13.1). We also show, in subsection 8.4.2, that certain algorithms are based not on learning the prior, but on learning the joint distribution on the state-observation pair. Recall that the filtering distribution cycles between predicting with the (stochastic) dynamics model (6.1) and solving an inverse problem defined by the data model (6.2). The prediction and analysis steps of the filtering cycle are summarized in (6.4) . The analysis step itself corresponds to solving the Bayesian inverse problem for 𝑣 † 𝑗+1 , given 𝑦 † 𝑗+1 , where 𝑦 † 𝑗+1 = ℎ(𝑣 † 𝑗+1 ) + 𝜂 † 𝑗+1 . ( 8.14) The prior on 𝑣 † 𝑗+1 is ̂︀ 𝜋 𝑗+1 and the noise is 𝜂 † 𝑗+1 ∼ 𝒩 (0, Γ). Filtering is intrinsically harder than a standard Bayesian inverse problem, however, in the sense that the the prior ̂︀ 𝜋 𝑗+1 is typically not available exactly. For this reason, many algorithms used in practice employ either learned priors on the state to facilitate the analysis step, or learned joint distributions on the state and observation, also to facilitate the analysis step. We describe two algorithms from this point of view. Bootstrap Particle Filter For particle-based methods the prior, or the joint distribution on state and observation, must be reconstructed from the predicted ensemble of particles {̂︀ 𝑣 (𝑛) 𝑗+1 } 𝑁 𝑛=1 . For the bootstrap particle filter we reconstruct a particle approximation to ̂︀ 𝜋 𝑗+1 , namely ︀ 𝜋 app 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 ̂︀ 𝑣 (𝑛) 𝑗+1 . (8.15) This is an elementary form of learning the prior from data, as explained in Chapter 13, and equation (13.2) in particular. The Bayesian inverse problem defined by (8.14) may be solved exactly with this prior, to obtain the posterior 𝜋 app 𝑗+1 = 𝑁 ∑︁ 𝑛=1 𝑤 (𝑛) 𝑗+1 𝛿 ̂︀ 𝑣 (𝑛) 𝑗+1 . (8.16) The weights, defined by the likelihood from (8.14) , are given in (6.41). Thus we have shown that the bootstrap particle filter has at its core a form of learning the prior, in this case construction of an empirical measure from the predicted samples. Ensemble Kalman Filter Now consider the setting where the particles {̂︀ 𝑣 (𝑛) 𝑗+1 } 𝑁 𝑛=1 are derived from the prediction step of the EnKF. Initially we consider the setting in which ℎ(•) = 𝐻• is linear, then generalize to nonlinear ℎ. In the setting of linear observation map, the inverse problem defining the analysis step is to find 𝑣 † 𝑗+1 given 𝑦 † 𝑗+1 where 𝑦 † 𝑗+1 = 𝐻𝑣 † 𝑗+1 + 𝜉 † 𝑗+1 . (8.17) Recall the notation for G, the projection onto Gaussian measures, defined in Remark 2.11. To define the EnKF we learn the Gaussian prior ︀ 𝜋 app 𝑗+1 = G (︁ 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 ̂︀ 𝑣 (𝑛) 𝑗+1 )︁ . (8.18) We obtain ̂︀ 𝜋 app 𝑗+1 = 𝒩 (︀ ̂︀ 𝑚 𝑗+1 , ̂︀ 𝐶 𝑗+1 )︀ , where ︀ 𝑚 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 ̂︀ 𝑣 (𝑛) 𝑗+1 , (8.19a) ︀ 𝐶 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 (︀ ̂︀ 𝑣 (𝑛) 𝑗+1 -̂︀ 𝑚 𝑗+1 )︀ ⊗ (︀ ̂︀ 𝑣 (𝑛) 𝑗+1 -̂︀ 𝑚 𝑗+1 )︀ . ( 8.19b) Thus we have adopted a different form of learning the prior, in comparison to the bootstrap particle filter, since we have formed a Gaussian. If we solve the linear inverse problem defined by (8.17), for 𝑣 † 𝑗+1 given 𝑦 † 𝑗+1 , using this Gaussian prior, we obtain a Gaussian posterior. Samples from this Gaussian posterior are given by 𝑣 (𝑛) 𝑗+1 = ̂︀ 𝑣 (𝑛) 𝑗+1 + 𝐾 𝑗+1 (︀ 𝑦 † 𝑗+1 -𝜂 (𝑛) 𝑗+1 -𝐻 ̂︀ 𝑣 (𝑛) 𝑗+1 )︀ , 𝑛 = 1, . . . , 𝑁, ( ) 𝐾 𝑗+1 = ̂︀ 𝐶 𝑗+1 𝐻 ⊤ (︀ 𝐻 ̂︀ 𝐶 𝑗+1 𝐻 ⊤ + Γ )︀ -1 , (8.20b) where 𝜂 (𝑛) 𝑗+1 ∼ 𝒩 (0, Γ) are drawn i.i.d. with respect to particle-index 𝑛, as well time-index 𝑗. This simply defines the EnKF analysis step from Subsection 6.3.5, in the linear case described in Remark 6.11. Thus, in the case of linear 𝐻, the EnKF can be thought of as being derived from using a learned Gaussian prior in every analysis step, and then sampling the posterior, before cycling back to the prediction step. Similar ideas may be applied for nonlinear observation operator ℎ, but instead of being based on learning the prior in the Gaussian class, they are instead based on learning the joint distribution of the predicted state and data in the Gaussian class. To this end we use the particles {̂︀ 𝑣 (𝑛) 𝑗+1 } 𝑁 𝑛=1 in (8.14) to create simulated observations ︀ 𝑦 (𝑛+1) 𝑗+1 = ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) + 𝜂 (𝑛) 𝑗+1 , (8.21) where 𝜂 (𝑛) 𝑗+1 ∼ 𝒩 (0, Γ) are drawn i.i.d. with respect to particle-index 𝑛, as well as time-index 𝑗, as above. From this we may learn the Gaussian approximation of the joint distribution 𝛾 𝑗+1 on the predicted state-observation pair (̂︀ 𝑣 𝑗+1 , ̂︀ 𝑦 𝑗+1 ) : 𝛾 app 𝑗+1 = G (︁ 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 (̂︀ 𝑣 (𝑛) 𝑗+1 ,̂︀ 𝑦 (𝑛) 𝑗+1 ) )︁ . (8.22) We have again used the notation G, the projection onto Gaussian measures, defined in Remark 2.11. If we condition this prior on observation ̂︀ 𝑦 𝑗+1 = 𝑦 † 𝑗+1 then we obtain another Gaussian. Samples from this Gaussian are given by 𝑣 (𝑛) 𝑗+1 = ̂︀ 𝑣 (𝑛) 𝑗+1 + 𝐾 𝑗+1 (︀ 𝑦 † 𝑗+1 -̂︀ 𝑦 (𝑛) 𝑗+1 )︀ , 𝑛 = 1, . . . , 𝑁, ( where the gain matrix 𝐾 𝑗+1 is calculated according to ︀ 𝑚 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 ̂︀ 𝑣 (𝑛) 𝑗+1 , (8.24a) ︀ ℎ 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ), (8.24b) ︀ 𝐶 𝑣ℎ 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 (︀ ̂︀ 𝑣 (𝑛) 𝑗+1 -̂︀ 𝑚 𝑗+1 )︀ ⊗ (︀ ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) -̂︀ ℎ 𝑗+1 )︀ , ( 8.24c) ︀ 𝐶 ℎℎ 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 (︀ ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) -̂︀ ℎ 𝑗+1 )︀ ⊗ (︀ ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) -̂︀ ℎ 𝑗+1 )︀ , (8.24d) ︀ 𝐶 𝑦𝑦 𝑗+1 = ̂︀ 𝐶 ℎℎ 𝑗+1 + Γ (𝑛) , 𝐾 𝑗+1 = ̂︀ 𝐶 𝑣ℎ 𝑗+1 (︀ ̂︀ 𝐶 𝑦𝑦 𝑗+1 )︀ -1 . (8.24e) Here Γ (𝑛) = 1 𝑁 𝑛 ∑︁ 𝑛=1 𝜂 (𝑛) 𝑗+1 ⊗ 𝜂 (𝑛) 𝑗+1 . Noting that Γ (𝑛) ≈ Γ, by the law of large numbers, and replacing Γ (𝑛) with Γ in the formula (8.24e) above, we obtain the EnKF analysis step from Subsection 6.3.5, as given by equations (6.28b) and (6.29). Thus, in the case of nonlinear ℎ, the EnKF can be thought of as being derived from using a learned Gaussian distribution on the joint state-observation variable, conditioning the Gaussian on the observation to obtain an approximation of the posterior defined by the analysis step, and then sampling the approximate posterior, before cycling back to the prediction step. Bibliography In this chapter we have introduced two computational frameworks for joint state and parameter estimation: the EM algorithm and auto-differentiable filters. Both frameworks find the parameters in the dynamical system by approximating the maximum likelihood estimator, and then use a filtering or smoothing algorithm to recover the state. We have focused on learning the dynamics model, but both frameworks directly generalize to learning the observation model; thus, the EM algorithm and auto-differentiable filters can be used not only to learn the prior distribution for the smoothing problem, but also the likelihood function. This final section provides bibliographical context for the algorithms considered in this chapter, and briefly discusses other computational methods that do not stem from a maximum likelihood formulation. Embedding of the EnKF and the ensemble Kalman smoother (EnKS) into the EM algorithm was proposed in [411, 421, 130, 349] , with a focus on estimation of error covariance matrices. The E-step is approximated with an EnKS under the Monte Carlo EM framework [441] . In addition, [73, 321, 444] incorporate machine learning techniques in the M-step to train neural network surrogate models or corrections. The paper [62] proposes Bayesian estimation of model error statistics, together with a neural network emulator for the dynamical system. On the other hand, [422, 412, 110] consider online EM methods for error covariance estimation with EnKF. Online methods aim to reduce computation by not reprocessing the smoothing distribution for each new observation. Although gradient information is used during the M-step to train the surrogate models in [73, 321, 62] , these methods do not auto-differentiate through the EnKF. Other methods to learn observation and model error covariances are considered in [300, 52, 437, 302] . Auto-differentiable Kalman filters were introduced in [102] , which proposes and analyzes an approach for joint state and parameter estimation that leverages gradient information of an EnKF estimate of the likelihood. The paper [104] considers learning a latent low-dimensional surrogate model for the dynamics and a decoder that maps from the latent space to the state space using auto-differentiable Kalman filters. EnKFs for derivative-free maximum likelihood estimation are studied in [400, 349] . An empirical comparison of the likelihood computed using the EnKF and other filtering algorithms is made in [83] ; see also [195, 305] . The paper [131] uses EnKF likelihood estimates to design a pseudo-marginal MCMC method for Bayesian inference of model parameters. The works [399, 401] propose online Bayesian parameter estimation using the likelihood computed from the EnKF under a certain family of conjugate distributions. While our discussion has focused on ensemble Kalman methods, particle filters can also be employed for joint state and parameter estimation. Particle filters give an unbiased estimate of the data likelihood [123, 21] . Based upon this likelihood estimate, a particle MCMC Bayesian parameter estimation method is designed in [21] . Although particle filter likelihood estimates are unbiased, they suffer from two potential drawbacks that limit their applicability to some problems. First, their variance can be large, as they inherit the weight degeneracy of importance sampling in high dimensions [391, 5, 372, 375] -see Chapter 6 for further background on this subject. Second, while the forecast and analysis steps of particle filters can be auto-differentiated, the resampling steps involve discrete distributions that cannot be handled by the reparameterization trick, as discussed in [102] . For this reason, previous differentiable particle filters omit auto-differentiation of the resampling step [317, 288, 271] , introducing a bias. An alternative to maximum likelihood estimation is to optimize a lower bound of the data log-likelihood with variational inference [59, 247, 355] . The posterior distribution over the latent states is approximated with a parametric distribution and is jointly optimized with model parameters defining the stochastic dynamics model. In this direction, variational sequential Monte Carlo methods [317, 288, 271] construct the lower bound using a particle filter. Moreover, the proposal distribution of the particle filter is parameterized and jointly optimized with model parameters. Although variational sequential Monte Carlo methods provide consistent data log-likelihood estimates, they suffer from the same two potential drawbacks as likelihood-based particle filter methods. A recent work [228] proposes blending variational sequential Monte Carlo and EnKF with an importance sampling-type lower bound estimate, which is effective if the state dimension is small. Other works that build on the variational inference framework include [255, 356, 152, 292] . An important challenge is to obtain suitable parameterizations of the posterior, especially when the state dimension is high. For this reason, a restrictive Gaussian parameterization with a diagonal covariance matrix is often used in practice [255, 152] . The topic of variational approximations to filtering and smoothing problems is discussed in Chapter 7. Another alternative approach to maximum likelihood estimation is to concatenate state and parameters into an augmented state-space and employ the data assimilation methods, from other chapters in this Part II, in the augmented state-space. This approach requires one to design a pseudo-dynamic for the parameters, which can be challenging when certain types of parameters (e.g., error covariance matrices) are involved [399, 125] or if the dimension of the parameters is high. The use of the EnKF for jointly learning the state and model parameters by state augmentation was introduced in [18] . Beyond the problem of joint parameter and state estimation, the development of data-driven frameworks for learning dynamical systems is a very active research area. We refer to [276] for a framework and to [78, 180, 181, 198, 353] for recent methods that do not rely on the EM algorithm, auto-differentiation of filtering methods, or variational inference. In addition, data-driven machine learning frameworks have gained popularity for combining physics-based models with model error corrections. These hybrid models may avoid the over-smoothing or unphysical behavior of predictive models that replace the entire forecast dynamics [66] . Two common representations for model error include additive corrections [143, 67] and data-driven subgrid scale parameterizations [357, 65] ; we refer to Section 1.4 for other forms of model error. We note that the approach discussed in this chapter learns the corrections offline, as online methods typically require the adjoint operator of the physics-based model, which may be challenging to obtain for some physics-based models, e.g., in numerical weather prediction and for climate modeling [283] . Model error correction using analysis increments is considered in [118, 143, 99] . Alternatively, machine learning can be used to account for model error in data assimilation by learning a mapping between the attractor defined by the model and the attractor of the true system; see [6] for one approach. Chapter 9 Transport Perspective On Filters This chapter is devoted to the transport perspective on filtering and other related state estimation problems. The methodology is linked, therefore, to Chapter 4 concerning the solution of inverse problems using transport. In this chapter the term transport refers primarily to a map designed to learn about the analysis step defining the filtering distribution; both state and probabilistic estimation are considered in this generalized transport framework. We start with the dynamics/data model (6.1) and (6.2), repeated here for convenience: 𝑣 † 𝑗+1 = Ψ(𝑣 † 𝑗 ) + 𝜉 † 𝑗 , 𝑗 ∈ Z + , ( 9.1a ) 𝑦 † 𝑗+1 = ℎ(𝑣 † 𝑗+1 ) + 𝜂 † 𝑗+1 , 𝑗 ∈ Z + . (9.1b) The assumptions on the noise are as detailed Assumption 6.1, and we also deploy the notation from Subsection 6.1.1. We are interested in designing algorithms to learn about 𝑣 † 𝑗 , given all the data observed up to time 𝑗, 𝑌 † 𝑗 . Recall that there are two natural approaches to this problem, probabilistic estimation and state estimation, as discussed in Subsection 6.1.3. In this chapter we will study both approaches, concentrating on estimating the state {𝑣 † 𝑗 }, or on estimating the filtering distribution on the state {𝜋 𝑗 }. In both cases we consider sequential algorithms with respect to 𝑗, and refer to these collectively as filters. The basic idea underlying this chapter is to learn transport algorithms designed to solve these problems. The class of algorithms from which we learn takes the form of mappings from R 𝑑 (or multiple copies thereof) into itself. They are based around the prediction-analysis cycle (6.4); we also briefly consider the modified prediction-analysis structure underlying the optimal particle filter from Subsection 6.3.7. We start by formulating a class of state estimation algorithms of the form ︀ 𝑣 𝑗+1 = Ψ(𝑣 𝑗 ) + 𝑠𝜉 𝑗 , ( 9.2a ) 𝑣 𝑗+1 = 𝑇 (︀ ̂︀ 𝑣 𝑗+1 , 𝑦 † 𝑗+1 ; 𝜃 )︀ . (9.2b) We allow for both 𝑠 = 0 and 𝑠 = 1, as in our formulation of 3DVar in (6. 19 ). We will primarily use this form of state estimation algorithm with 𝑠 = 0, but use of 𝑠 = 1 with 𝜉 𝑗 ∼ 𝒩 (0, Σ) as an i.i.d. sequence may also be considered. In the context of state estimation, the objective is to choose 𝜃 ∈ Θ ⊆ R 𝑝 so that the resulting dynamics for {𝑣 𝑗 } 𝐽 𝑗=0 determined by (9.2) results in a sequence approximating 𝑉 † , the state underlying the data 𝑌 † ; the pair (𝑉 † , 𝑌 † ) are linked through the dynamics-observation model (9.1) . Motivated by the EnKF in form (6.34), we may generalize (9.2) and consider a particle based map of the form ︀ 𝑣 (𝑛) 𝑗+1 = Ψ(𝑣 (𝑛) 𝑗 ) + 𝜉 (𝑛) 𝑗 , 𝑛 = 1, . . . , 𝑁, (9.3a) ︀ 𝑦 (𝑛) 𝑗+1 = ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) + 𝜂 (𝑛) 𝑗 , 𝑛 = 1, . . . , 𝑁, (9.3b) 𝑣 (𝑛) 𝑗+1 = 𝑇 (︁ ̂︀ 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑦 (𝑛) 𝑗+1 , 𝑦 † 𝑗+1 , {̂︀ 𝑣 (ℓ) 𝑗+1 , ̂︀ 𝑦 (ℓ) 𝑗+1 } 𝑁 ℓ=1 ; 𝜃 )︁ , 𝑛 = 1, . . . , 𝑁. (9.3c) Here 𝜉 (𝑛) 𝑗 ∼ 𝒩 (0, Σ), 𝜂 (𝑛) 𝑗+1 ∼ 𝒩 (0, Γ) are independent sequences of i.i.d. random vectors with respect to both 𝑗 and 𝑛, distributed as in (9.1); recall that the two sequences themselves are independent of one another. The collection {𝑣 (ℓ) 𝑗 } 𝑁 ℓ=1 defines the empirical measure 𝜋 EnKF 𝑗 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑣 (𝑛) 𝑗 . ( 9.4) The step (9.3a) defines a Markov operator P, identical to the one defining the prediction step in (6.4). Furthermore P𝜋 EnKF 𝑗 is a Gaussian mixture and the points {̂︀ 𝑣 This measure is invariant under permutations of the ensemble from which it is formed. Motivated by this fact, and extending the same reasoning to the samples on observation space, we highlight that dependence of the map 𝑇 (•) on {̂︀ 𝑣 (ℓ) 𝑗+1 , ̂︀ 𝑦 (ℓ) 𝑗+1 } 𝑁 ℓ=1 is assumed to be invariant under permutations in index ℓ. To be specific, the map in (9.3c) is assumed to take the form (abusing notation) 𝑇 (︁ ̂︀ 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑦 (𝑛) 𝑗+1 , 𝑦 † 𝑗+1 , {̂︀ 𝑣 (ℓ) 𝑗+1 , ̂︀ 𝑦 (ℓ) 𝑗+1 } 𝑁 ℓ=1 ; 𝜃 )︁ = 𝑇 (︁ ̂︀ 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑦 (𝑛) 𝑗+1 , 𝑦 † 𝑗+1 , 1 𝑁 𝑁 ∑︁ ℓ=1 𝛿 (̂︀ 𝑣 (ℓ) 𝑗+1 ,̂︀ 𝑦 (ℓ) 𝑗+1 ) ; 𝜃 )︁ ; thus, the map depends on the joint empirical measure of the states and observations. We now discuss two different ways in which we might learn the parameters 𝜃 for a model of the form (9.3c). For the first approach, in the context of state estimation, we define the mean of the particles 𝑣 𝑗 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝑣 (𝑛) 𝑗 = E 𝑣∼𝜋 EnKF 𝑗 𝑣. (9.6) The objective is to choose 𝜃 so that the resulting time series for 𝑣 𝑗 recovers an approximation of the time series for 𝑣 † 𝑗 , the state underlying the data 𝑦 † 𝑗 . In the second approach, where we choose parameters to learn the filtering distribution rather than just the state, the objective is to choose 𝜃 so that the resulting empirical measure 𝜋 EnKF 𝑗 recovers an approximation of 𝜋 𝑗 . We may think of the map 𝑇 as a transport map, as introduced in Section 13.2. This nomenclature is most commonly used in the mathematics literature to describe maps that, through the pushforward operation, map one probability measure into another. We will use this terminology, however, in the context of both state estimation and filtering. We think of the map 𝑇 as transporting states that are predicted by the underlying stochastic dynamics so that they match the data. In Section 9.1 we describe a variety of learning objectives that may be used so that this transport performs well at state estimation; Section 9.2 then deploys these objectives to learn optimal parameters in specific problems classes of the form (9.2) or (9.3), and their generalizations; the ideas are based on using data to learn variants on 3DVar, EnKF and the optimal particle filter. In Section 9.3 we describe various objectives that may be used so that this transport performs well at matching the filtering distribution; Section 9.4 describes algorithm classes that can be learned using these objectives, concentrating on ensemble-based methods. State Estimation: Learning Objectives This section defines a variety of learning objectives that are relevant for state estimation. We work primarily under the following data assumption concerning a data stream in R 𝑑 × R 𝑘 . Data Assumption 9.1. We have access to data set {𝑣 † 𝑗+1 , 𝑦 † 𝑗+1 } 𝑗∈{0,...,𝐽-1} , generated as a realization of (9.1). The filtering algorithms we are studying in this chapter provide a sequential estimate of the state, or the distribution on the state, given a sequence of observations. The preceding assumption, in which we are given data pairs of observations and states, enables us, potentially, to learn good choices of the parameters defining the filters. If the filter is viewed as an architecture that maps 𝑌 † to 𝑉 † , while respecting the casual structure of these sequences in time, then utilizing Data Assumption 9.1 could be viewed as solving a supervised learning problem with only one data pair, a data poor learning environment. However, embedded in the solution to this problem is the learning of a single transport map that uses the observation at each step to perform filtering sequentially. If 𝐽 is large then we may view this as a data rich learning environment for this single transport map. For the filters learned using Data Assumption 9.1 to be successful, we assume the model (9.1) generating the data to be a faithful representation of the process that creates the observed data which is utilized after learning. When model error is significant, however, it is natural to use a more restrictive data assumption, namely that we simply have access to an observation sequence. Here, we assume that the observation operator ℎ : R 𝑑 → R 𝑘 that maps states into observations is accurate; however, we do not assume that the stochastic dynamics model is accurate. Data Assumption 9.2. We have access to data set {𝑦 † 𝑗+1 } 𝑗∈{0,...,𝐽-1} , generated as a realization of (9.1). In this setting we will use the observation operator ℎ : R 𝑑 → R 𝑘 to map the states into the observation space, to define a loss from which to learn parameters of the filtering algorithm. The remainder of this section is organized as follows. Subsection 9.1.1 focuses on state estimation. Subsection 9.1.2 goes beyond this setting by working with scoring rules that compare the distribution defined by an ensemble algorithm with the true state. In both Subsection 9.1.1 and Subsection 9.1.2 we employ Data Assumption 9.1. In Subsection 9.1.3 we discuss objective functions defined in observation space rather than state space; this is useful when model error is significant and we work under Data Assumption 9.2. In Subsection 9.1.4 we assume that our algorithm comes with a covariance estimator, alongside the mean used to estimate the state; we focus on minimizing this covariance, again working with Data Assumption 9.1. Matching The State: Deterministic Scoring Rule Consider an algorithm, such as (9.2) or (9.3), which gives rise to an estimator 𝑣 𝑗 = 𝑣 𝑗 (𝜃) for 𝑣 † 𝑗 ; for example 𝑣 𝑗 (𝜃) may be 𝑣 𝑗 itself for algorithm (9.2) or the particle average 𝑣 𝑗 in (9.6), for algorithm (9.3). Then, apply a deterministic scoring rule d(•, •), satisfying Definition 11.65, to measure the distance between 𝑣 † 𝑗 and 𝑣 𝑗 (𝜃). We may identify an optimal 𝜃 ⋆ that minimizes the score averaged over the time series by solving: J 𝐽 (𝜃) = 1 𝐽 𝐽 ∑︁ 𝑗=1 d (︀ 𝑣 𝑗 (𝜃), 𝑣 † 𝑗 )︀ , (9.7a) 𝜃 ⋆ ∈ arg min 𝜃 J 𝐽 (𝜃). (9.7b) Note that 𝑣 𝑗 (𝜃) depends on the observed data {𝑦 † 𝑖 } 𝑖∈{1,...,𝑗} and so we are indeed using the entirety of the data specified in Data Assumption 9.1. The optimization problem defined by (9.7) may be performed using auto-differentiation with respect to 𝜃; see Section 15.6 for details on this methodology. Matching The State: Probabilistic Scoring Rule We now go beyond simply matching a state estimator (from the algorithm) with the true state of the system (from (9.1a)). Instead, we look at the distribution implied by the algorithm, and use a scoring rule to measure its distance from the true state of the system. Assume that the algorithm class of interest produces an estimator of the filtering distribution 𝜋 alg 𝑗 (𝜃) ≈ 𝜋 𝑗 . To this end, we employ a probabilistic scoring rule S(•, •), introduced in Section 11.3, to obtain J 𝐽 (𝜃) = 1 𝐽 𝐽 ∑︁ 𝑗=1 S(𝜋 alg 𝑗 (𝜃), 𝑣 † 𝑗 ), (9.8a ) 𝜃 ⋆ ∈ arg min 𝜃 J 𝐽 (𝜃). (9.8b) We emphasize that it is important to use scoring rules which can be evaluated when 𝜋 alg 𝑗 (𝜃) is an empirical measure. These include the energy score, from Definition 11.44, and the Dawid-Sebastiani score, from Definition 11.61. Thus, the optimization problem in (9.8) can be implemented with ensemble-based methods if the scoring rule is chosen appropriately. Remark 9.4. In Section 10.4 we show how working with an amortized version of (9.8), averaged over multiple realizations of the state and data, leads to objective functions that are minimized at the true filtering distribution. ♢ Matching The Data We now consider the setting in which model (9.1a) is imperfect, i.e., there is model error in the stochastic dynamics model; we assume that the observation model (9.1b) is perfect. In this setting, learning data assimilation algorithms using the methodology from Subsection 9.1.1 may introduce bias because the model generating the data in Data Assumption 9.1 is not the same as the model giving rise to the available observations. Here, instead of (9.7), we consider the following objective function defined in the observation space: J 𝐽 (𝜃) = 1 𝐽 𝐽 ∑︁ 𝑗=1 d (︁ ℎ (︀ 𝑣 𝑗 (𝜃) )︀ , 𝑦 † 𝑗 )︁ . ( 9.9) Once again we typically choose 𝑣 𝑗 (𝜃) to be 𝑣 𝑗 itself, given by (9.2), or the particle average 𝑣 𝑗 , given by (9.3), (9.6). We note that J 𝐽 (𝜃) is then a proxy for the loss with respect to the true trajectory in the observation space: 1 .10) The difference between the two losses is that (9.9) contains observational noise in the observation, which may be substantial. The following remark addresses this issue. 𝐽 𝐽 ∑︁ 𝑗=1 d (︁ ℎ (︀ 𝑣 𝑗 (𝜃) )︀ , ℎ(𝑣 † 𝑗 ) )︁ . ( 9 Remark 9.5. Optimizing the cost function J 𝐽 (𝜃) in (9.9) does not always lead to good performance. The root cause is that the objective function can overfit to the specific observations, which contain unknown observational errors. For example, if ℎ is uniquely invertible then a perfect score can be achieved by simply setting 𝑣 𝑗 (𝜃) = ℎ -1 (𝑦 † 𝑗 ). When the signal-to-noise ratio is small, this will lead to a very noisy estimate 𝑣 𝑗 (𝜃) of the signal. If only a single realization of the observed process is available, then this is a fundamental obstacle. However, if it is possible to obtain multiple independent realizations, then the overfitting issue can be mitigated. In this setting, we describe a method based on this observation to avoid overfitting. We take the squared Euclidean distance d(𝑢 1 , 𝑢 2 ) = |𝑢 1 -𝑢 2 | 2 as the scoring rule and analyze a single time 𝑗. Similar analyses could be performed for other scoring rules and multiple times. Recall that 𝑦 † 𝑗 = ℎ(𝑣 † 𝑗 ) + 𝜂 † 𝑗 . Note that the data assimilation algorithm produces an estimator 𝑣 𝑗 (𝜃) for 𝑣 † 𝑗 which will depend on 𝑦 † 𝑗 and hence on 𝜂 † 𝑗 . We consider the contribution to the objective function in (9.9) at 𝑗 and take expectation E over independent realizations of the observation noise 𝜂 † 𝑗 ∼ 𝒩 (0, Γ). This yields E ⃒ ⃒ ℎ (︀ 𝑣 𝑗 (𝜃) )︀ -𝑦 † 𝑗 ⃒ ⃒ 2 = E ⃒ ⃒ ℎ(𝑣 𝑗 (𝜃)) -ℎ(𝑣 † 𝑗 ) ⃒ ⃒ 2 + Tr(Γ) -2E [︁ ℎ (︀ 𝑣 𝑗 (𝜃) )︀ ⊤ 𝜂 † 𝑗 ]︁ . Rearranging we obtain E ⃒ ⃒ ℎ(𝑣 𝑗 (𝜃)) -ℎ(𝑣 † 𝑗 ) ⃒ ⃒ 2 + Tr(Γ) = E ⃒ ⃒ ℎ (︀ 𝑣 𝑗 (𝜃) )︀ -𝑦 † 𝑗 ⃒ ⃒ 2 + 2E [︁ ℎ (︀ 𝑣 𝑗 (𝜃) )︀ ⊤ 𝜂 † 𝑗 ]︁ . (9.11) Minimizing the left-hand side is desirable as it matches the output of the analysis, 𝑣 𝑗 (𝜃), with the true signal, 𝑣 † 𝑗 ; note that it does not depend on the observational noise explicitly, only through the dependence of 𝑣 𝑗 (𝜃) on the data. The right-hand side can be approximated through sampling the observational noise 𝜂 † 𝑗 , and then optimized over 𝜃 to approximately minimize the left-hand side. The natural setting in which the idea explained here may be used is when Data Assumption 9.2 is generalized to multiple realizations of an observation stream defined over 𝑗 ∈ {1, . . . , 𝐽}. 1  ♢ Minimizing The Variance We now use the uncertainty in our state estimator to define a loss. We assume that our transport algorithm for state estimation produces, in addition to a state estimator 𝑣 𝑗 (𝜃), a covariance 𝐶 𝑗 (𝜃). For example, for an algorithm of the form (9.3), this may be computed by regularizing the empirical covariance of the ensemble around 𝑣 𝑗 (𝜃) = 𝑣 𝑗 given by (9.6). We may then consider learning 𝜃 by minimizing J 𝐽 (𝜃) = 1 𝐽 + 1 𝐽 ∑︁ 𝑗=0 Tr (︀ 𝐶 𝑗 (𝜃) )︀ , (9.12a) 𝜃 ⋆ ∈ arg min 𝜃 J 𝐽 (𝜃). (9.12b) State Estimation: Algorithms Our interest in this section is in learning filtering algorithms. We study learning within the context of specific forms of algorithms (9.2) and (9.3), or their generalizations. In particular, we consider algorithms designed by modifying three specific algorithms from Chapter 6: 3DVar, the EnKF and the optimal particle filter. We work under 1 The term E [︁ ℎ(𝑣𝑗(𝜃)) ⊤ 𝜂 † 𝑗 ]︁ is sometimes referred to as the optimism. Data Assumption 9.1. Subsection 9.2.1 is devoted to learning the gain in 3DVar. In Subsection 9.2.2 we study the same question, but in the context of generalizations of the EnKF. Subsection 9.2.3 also studies generalizations of the EnKF, but focuses on learning localization and inflation parameter. In Subsection 9.2.4 we go beyond algorithm classes of the form in (9.3), showing how the optimal particle filter may be used as the basis to learn new algorithms for equally weighted ensemble methods. Learning The Gain In 3DVar Recall the 3DVar algorithm (6.19), repeated here for convenience: ︀ 𝑣 𝑗+1 = Ψ(𝑣 𝑗 ), (9.13a ) 𝑣 𝑗+1 = ̂︀ 𝑣 𝑗+1 + 𝐾 (︀ 𝑦 † 𝑗+1 -ℎ(̂︀ 𝑣 𝑗+1 ) )︀ . (9.13b) This is a specific subclass of the algorithm class defined in (9.2). To fully specify the 3DVar algorithm we need to choose the gain matrix 𝐾. We investigate learning 𝜃 := 𝐾 on the basis of data. (We note that, alternatively, 𝐾 could be parameterized as 𝐾 = 𝐾(𝜃), and the parameter 𝜃 could be learned, but we do not pursue this here.) For the dynamics/data model underlying Data Assumption 9.1 and for the 3DVar algorithm (9.13), we make the same Gaussian and independence assumptions on the initialization and noise as made in equations (6.1), (6.2) and Assumption 6.2. In particular, we assume that 𝑣 † 0 and 𝑣 0 are drawn from the same distribution 𝒩 (𝑚 0 , 𝐶 0 ), but independently. The Original Approach Consider the linear setting where ℎ(𝑣) = 𝐻𝑣 for some matrix 𝐻 ∈ R 𝑘×𝑑 . Motivated by the form of the Kalman gain itself, and equations (6.14a) in particular, but invoking a steady-state hypothesis on the dynamics, we seek 𝐾 in the form 𝐾 = ̂︀ 𝐶𝐻 ⊤ (︀ 𝐻 ̂︀ 𝐶𝐻 ⊤ + Γ )︀ -1 , (9.14) where ̂︀ 𝐶 is a time-independent state covariance. In the original derivation of 3DVar, ̂︀ 𝐶 itself is estimated and 𝐾 is formed using (9.14) . Estimation of ̂︀ 𝐶 is achieved using data relating to the underlying dynamics model (9.1a), and estimates of the covariance in forecasts made by this model. However, in the remainder of this subsection we adopt a different approach, aiming to find 𝐾 directly from data. General Ψ And ℎ We now move away from the assumption that 𝐻 is linear and we consider general Ψ and ℎ. We note that 𝑣 𝑗 arising from algorithm class (9.13) depends on the gain matrix: 𝑣 𝑗 = 𝑣 𝑗 (𝜃). We use the loss function in (9.7) to determine the optimal choice of 𝜃: J 𝐽 (𝜃) = 1 𝐽 𝐽 ∑︁ 𝑗=1 d (︀ 𝑣 𝑗 (𝜃), 𝑣 † 𝑗 )︀ , 𝜃 ⋆ ∈ arg min 𝜃 J 𝐽 (𝜃). Remark 9.6. In the learning frameworks of Chapter 4 we introduced the use of transport maps to solve inverse problems. There we started from the population loss, defined as an expectation over a measure with Lebesgue density, and then noted that in practice we approximate this empirically. Here, because the population loss is more complicated to write down, we work the other way around: we have started with empirical loss (9.7) and now proceed to derive a population loss. To this end, let us view (9.1), (9.13) as a coupled stochastic dynamical system for (𝑣 † 𝑗 , 𝑦 † 𝑗 , 𝑣 𝑗 ) and assume it is ergodic with invariant measure 𝜇(𝑑𝑣 † , 𝑑𝑦 † , 𝑑𝑣; 𝜃); this measure depends on 𝜃. Furthermore, note that the measure factorizes naturally as 𝜇(𝑑𝑣 † , 𝑑𝑦 † , 𝑑𝑣; 𝜃) = 𝜇 alg (𝑑𝑣|𝑣 † , 𝑦 † ; 𝜃)l(𝑑𝑦 † |𝑣 † )𝜇 0 (𝑑𝑣 † ). Here 𝜇 0 (•) is the invariant measure for the state variable governed by (9.1a); l(•|𝑣 † ) defines the probability of the data given the state 𝑣 † , defined by (9.1b); and 𝜇 alg (•|𝑣 † , 𝑦 † ; 𝜃) defines the invariant measure for the algorithm, given the state-data pair (𝑣 † , 𝑦 † ). We may then view J 𝐽 (•) as approximation of the population-level loss J(•) found, by ergodicity, in the limit 𝐽 → ∞ : J(𝜃) = ∫︁ d (︀ 𝑣(𝜃), 𝑣 † )︀ 𝜇 alg (𝑑𝑣|𝑣 † , 𝑦 † ; 𝜃)l(𝑑𝑦 † |𝑣 † ) 𝜇 0 (𝑑𝑣 † ). This ergodicity observation also suggests that, rather than using one long trajectory, Data Assumption 9.1 could be modified to deploy a set of independently generated dynamics-data pairs from (9.1), rather than a single one. ♢ Linear Ψ And ℎ We now consider using the covariance-based loss function (9.12) to determine the gain 𝐾. We show that, in the setting of linear dynamics and observations, the learned gain 𝐾 in the 3DVar algorithm (9.13) converges to the steady-state Kalman gain as 𝐽 → ∞. To this end, consider the 3DVar algorithm (9.13) with Ψ(•) = 𝐴• and ℎ(•) = 𝐻•. Then 𝑣 𝑗 obeys the recursion ̂︀ 𝑣 𝑗+1 = 𝐴𝑣 𝑗 , ( 9.15a ) 𝑣 𝑗+1 = (𝐼 -𝐾𝐻)̂︀ 𝑣 𝑗+1 + 𝐾𝑦 † 𝑗+1 . (9.15b) Define the error covariances ̂︀ 𝐶 𝑗 = E [︀ (𝑣 † 𝑗 -̂︀ 𝑣 𝑗 )⊗(𝑣 † 𝑗 -̂︀ 𝑣 𝑗 ) ]︀ and 𝐶 𝑗 = E [︀ (𝑣 † 𝑗 -𝑣 𝑗 )⊗(𝑣 † 𝑗 -𝑣 𝑗 ) ]︀ , which we note are not the covariances of the forecast and filtering distributions. These covariances obey the recursions ︀ 𝐶 𝑗+1 = 𝐴𝐶 𝑗 𝐴 ⊤ + Σ, (9.16a ) 𝐶 𝑗+1 = (𝐼 -𝐾𝐻) ̂︀ 𝐶 𝑗+1 (𝐼 -𝐾𝐻) ⊤ + 𝐾Γ𝐾 ⊤ . (9.16b) The recursion for ̂︀ 𝐶 𝑗+1 holds because ︀ 𝐶 𝑗+1 = E [︁ (︀ 𝑣 † 𝑗+1 -̂︀ 𝑣 𝑗+1 )︀ ⊗ (︀ 𝑣 † 𝑗+1 -̂︀ 𝑣 𝑗+1 )︀ ]︁ = E [︁ (︀ 𝐴𝑣 † 𝑗 + 𝜉 † 𝑗 -𝐴𝑣 𝑗 )︀ ⊗ (︀ 𝐴𝑣 † 𝑗 + 𝜉 † 𝑗 -𝐴𝑣 𝑗 )︀ ]︁ = E [︁ (︀ 𝐴(𝑣 † 𝑗 -𝑣 𝑗 ) )︀ ⊗ (︀ 𝐴(𝑣 † 𝑗 -𝑣 𝑗 ) )︀ ]︁ + Σ = 𝐴𝐶 𝑗 𝐴 ⊤ + Σ. The recursion for 𝐶 𝑗+1 holds because 𝐶 𝑗+1 = E [︁ (︀ 𝑣 † 𝑗+1 -𝑣 𝑗+1 )︀ ⊗ (︀ 𝑣 † 𝑗+1 -𝑣 𝑗+1 )︀ ]︁ = E [︁ (︀ 𝑣 † 𝑗+1 -(𝐼 -𝐾𝐻)̂︀ 𝑣 𝑗+1 -𝐾𝐻𝑣 † 𝑗+1 -𝐾𝜂 † 𝑗+1 )︀ ⊗ (︀ 𝑣 † 𝑗+1 -(𝐼 -𝐾𝐻)̂︀ 𝑣 𝑗+1 -𝐾𝐻𝑣 † 𝑗+1 -𝐾𝜂 † 𝑗+1 )︀ ]︁ = E [︁ (𝐼 -𝐾𝐻)(𝑣 † 𝑗+1 -̂︀ 𝑣 𝑗+1 ) ⊗ (𝐼 -𝐾𝐻)(𝑣 † 𝑗+1 -̂︀ 𝑣 𝑗+1 ) ]︁ + 𝐾Γ𝐾 ⊤ = (𝐼 -𝐾𝐻) ̂︀ 𝐶 𝑗+1 (𝐼 -𝐾𝐻) ⊤ + 𝐾Γ𝐾 ⊤ . In the following theorem, we show that the minimizer of J 𝐽 (𝐾) defined by (9.12) is the steady-state Kalman gain found from the Kalman filter in the limit 𝐽 → ∞. We will use the preceding covariances and also use the definitions of the covariance and gain arising from the Kalman filter itself, given in (6.14), with a constant-in-time observation operator 𝐻. Here we recall the update formulae for the Kalman covariance (i.e., the actual forecast and filtering covariances) and the Kalman gain for notational convenience, introducing a new notation to distinguish from (9.15): ︀ 𝐶 kalman 𝑗+1 = 𝐴𝐶 kalman 𝑗 𝐴 ⊤ + Σ, (9.17 ) Assume that the analysis covariance and gain of the Kalman filter converge to their steady states in (6.16): 𝐾 kalman 𝑗+1 = ̂︀ 𝐶 kalman 𝑗+1 𝐻 ⊤ (︀ 𝐻 ̂︀ 𝐶 kalman 𝑗+1 𝐻 ⊤ + Γ )︀ -1 , ( 9 𝐶 kalman 𝑗 → 𝐶 ∞ , 𝐾 kalman → 𝐾 ∞ , 𝑗 → ∞. For J 𝐽 (𝐾) given by (9.12), consider the limiting objective J(𝐾) = lim 𝐽→∞ J 𝐽 (𝐾). Then, the minimizer of J is the steady-state Kalman gain in (6.15b), i.e., 𝐾 ∞ ∈ arg min 𝐾 J(𝐾). Proof. We begin by noting that the update for the state in (9.15) and the covariance in (9.16) can be written using a time-varying gain 𝐾 𝑗+1 as: 𝑣 𝑗+1 = (𝐼 -𝐾 𝑗+1 𝐻)𝐴𝑣 𝑗 + 𝐾 𝑗+1 𝑦 † 𝑗+1 , (9.20a) ︀ 𝐶 𝑗+1 = 𝐴𝐶 𝑗 𝐴 ⊤ + Σ, (9.20b ) 𝐶 𝑗+1 = (𝐼 -𝐾 𝑗+1 𝐻) ̂︀ 𝐶 𝑗+1 (𝐼 -𝐾 𝑗+1 𝐻) ⊤ + 𝐾 𝑗+1 Γ𝐾 ⊤ 𝑗 . (9.20c) Then, the gain 𝐾 𝑗+1 minimizing 𝐶 𝑗+1 given ̂︀ 𝐶 𝑗+1 (with respect to the order of positive definite matrices) is the Kalman gain in (9.15): 𝐾 𝑗+1 = 𝐾 kalman 𝑗+1 . This can be seen by considering an arbitrary perturbation Δ𝐾 to the Kalman gain and showing that the analysis covariance in (9.20c) as a function of the gain satisfies the lower bound 𝐶 𝑗+1 (𝐾 kalman 𝑗+1 + Δ𝐾) ⪰ 𝐶 𝑗+1 (𝐾 kalman 𝑗+1 ). By the same argument, it can be shown that for a parameter gain 𝐾, which is independent of time, we have 𝐶 𝑗+1 (𝐾) ⪰ 𝐶 𝑗+1 (𝐾 kalman 𝑗+1 ). Since Tr(•) is monotonically increasing with respect to the order of positive definite matrices, we have that Tr (︀ 𝐶 𝑗+1 (𝐾) )︀ ≥ Tr(𝐶 𝑗+1 (𝐾 kalman 𝑗+1 )) (9.21) for all 𝑗. From the assumption that the Kalman filter covariance and gain converge to their steady states 𝐶 ∞ and 𝐾 ∞ , respectively, and the continuity of the trace operator, we have Tr(𝐶 𝑗+1 (𝐾 kalman 𝑗+1 )) → Tr(𝐶 ∞ ), 𝑗 → ∞. Therefore, by the Cesàro-mean theorem (see bibliography), the average of the trace of the analysis covariances over the sequence also converges. That is, lim 𝑗→∞ 1 𝐽 + 1 𝐽 ∑︁ 𝑗=0 Tr(𝐶 𝑗+1 (𝐾 kalman 𝑗+1 )) → Tr(𝐶 ∞ ). (9.22) Lastly, averaging (9.21) over 𝐽 steps and taking the limit 𝐽 → ∞ we have lim inf 𝐽→∞ 1 𝐽 + 1 𝐽 ∑︁ 𝑗=0 Tr(𝐶 𝑗 (𝐾)) ≥ lim 𝐽→∞ 1 𝐽 + 1 𝐽 ∑︁ 𝑗=0 Tr(𝐶 𝑗 (𝐾 kalman 𝑗 )) = Tr(𝐶 ∞ ), (9.23) where we have used the convergence of the limit in (9.22) . The inequality in (9.23) holds for all 𝐾 and the lower bound is attained by 𝐾 = 𝐾 ∞ . From (9.23), we have J(𝐾) ≥ J(𝐾 ∞ ), and hence the minimizer of the objective is 𝐾 ∞ . Remark 9.8. The lower bound in (9.21) also follows from optimality of the Kalman filter. In particular, the filter provides the minimum mean-squared error estimate of the state over given at each step. That is, let 𝑣 𝑗 be the sequence given by the Kalman filter mean in (6.13). Then, for any measurable function 𝑧 𝑗 of the observations 𝑌 † 𝑗 we have E [︁ |𝑣 † 𝑗 -𝑣 𝑗 | 2 | 𝑌 † 𝑗 ]︁ ≤ E [︁ |𝑣 † 𝑗 -𝑧 𝑗 | 2 | 𝑌 † 𝑗 ]︁ . The left-hand side corresponds to the trace of the analysis covariance 𝐶 𝑗 , thereby showing that the Kalman filter minimizes the trace of the analysis covariance, analogously to the result in (9.21). ♢ Learning The Gain In EnKF Although the ensemble Kalman method was designed as a Monte Carlo method, it is often used as a state estimator and the ensemble is used to estimate uncertainty in the state estimates. This is the perspective we adopt here. We continue to work under Data Assumption 9.1 and recall the EnKF algorithm (6.34) . Rather than calculating 𝐾 𝑗+1 from empirical covariances as in (6.29), we instead try and learn dependence on the ensemble. To this end we modify (6.34) to read ︀ 𝑣 (𝑛) 𝑗+1 = Ψ(𝑣 (𝑛) 𝑗 ) + 𝜉 (𝑛) 𝑗 , 𝑛 = 1, . . . , 𝑁, (9.24a) ︀ 𝑦 (𝑛) 𝑗+1 = ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) + 𝜂 (𝑛) 𝑗 , 𝑛 = 1, . . . , 𝑁, (9.24b) 𝑣 (𝑛) 𝑗+1 = ̂︀ 𝑣 (𝑛) 𝑗+1 + 𝐾 𝑗+1 (︀ 𝑦 † 𝑗+1 -̂︀ 𝑦 (𝑛) 𝑗+1 )︀ , 𝑛 = 1, . . . , 𝑁, (9.24c) 𝐾 𝑗+1 = k (︀ {̂︀ 𝑣 (ℓ) 𝑗+1 , ̂︀ 𝑦 (ℓ) 𝑗+1 } 𝑁 ℓ=1 ; 𝜃 )︀ , ( 9.24d) where 𝜉 (𝑛) 𝑗 ∼ 𝒩 (0, Σ), 𝜂 (𝑛) 𝑗+1 ∼ 𝒩 (0, Γ) are independent sequences of i.i.d. random vectors with respect to both 𝑗 and 𝑛, and the two sequences themselves are independent of one another. Here, in this setting, it is desirable that k(•; 𝜃) be invariant with respect to permutation of the ensemble. This is then a specific subclass of the algorithm class defined in (9.3) . It is useful to define a state estimator by taking the mean of the ensemble at time 𝑗 as defined in (9.6). We may then use the loss function in (9.7) to determine 𝜃, with 𝑣 𝑗 (𝜃) given by the ensemble mean in (9.6). Remark 9.9. It may be of interest to replace (9.24d) by 𝐾 𝑗+1 = k (︀ ̂︀ 𝐶 𝑣ℎ 𝑗+1 , ̂︀ 𝐶 𝑦𝑦 𝑗+1 ; 𝜃 )︀ , where the covariance matrices are as defined in (6.29) and K(•; 𝜃) is a parameterized family of gain functions to be learned by optimizing over 𝜃; this automatically builds in the desired invariance. Function K(•; 𝜃) may be parameterized, for example, as a neural network. Recall from Remark 6.11 that if ℎ is linear then the two covariances ̂︀ 𝐶 𝑣ℎ 𝑗+1 , ̂︀ 𝐶 𝑦𝑦 𝑗+1 can be expressed in terms of ̂︀ 𝐶 𝑣𝑣 𝑗+1 and 𝐻; thus, in this case we might seek to learn the gain in the form K (︀ ̂︀ 𝐶 𝑣𝑣 𝑗+1 , 𝐻; 𝜃 )︀ . Recalling definition (6.35) of the innovation, it may also be of interest to replace (9.24c,9.24d) by an update of the form 𝑣 (𝑛) 𝑗+1 = k (︀ ̂︀ 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑤 𝑗+1 , 𝑖 (𝑛) 𝑗+1 , ̂︀ 𝐶 𝑣ℎ 𝑗+1 , ̂︀ 𝐶 𝑦𝑦 𝑗+1 , ̂︀ 𝐶 𝑣𝑣 𝑗+1 ; 𝜃 )︀ , 𝑛 = 1, . . . , 𝑁, 𝑖 (𝑛) 𝑗+1 = 𝑦 † 𝑗+1 -̂︀ 𝑦 (𝑛) 𝑗+1 , 𝑛 = 1, . . . , 𝑁, where ̂︀ 𝑤 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 ̂︀ 𝑣 (𝑛) 𝑗+1 is the mean of the predicted forecast ensemble. The reader will be able to suggest many variants on the preceding formulations of a learning problem for an EnKF-like data assimilation algorithm. ♢ Learning Localization And Inflation In EnKF Inflation and localization, discussed in Subsection 6.3.5, are essential for the performance of the EnKF, as discussed in Remark 6.10. Learning can be used to determine appropriate parameters for these features of the EnKF. Again we continue to work under Data Assumption 9.1. Recall multiplicative inflation given by (6.37), has the form ︀ 𝑣 (𝑛) 𝑗+1 → ̂︀ 𝑚 𝑗+1 + 𝛼(̂︀ 𝑣 (𝑛) 𝑗+1 -̂︀ 𝑚 𝑗+1 ), where ̂︀ 𝑣 (𝑛) 𝑗+1 are predicted states in the ensemble and ̂︀ 𝑚 𝑗+1 is their mean; and Schur product covariance localization given by (6.38), (6.39) has the form ︀ 𝐶 𝑗+1 → 𝐿 ∘ ̂︀ 𝐶 𝑗+1 , (𝐿) 𝑎𝑏 = 𝑒 -d(𝑎,𝑏) 2 /ℓ 2 where ∘ denotes the Hadamard product and 𝑎, 𝑏 ∈ {1, . . . , 𝑑} represent variable indices. The inflation parameter 𝛼 and localization radius ℓ can be considered parameters 𝜃 = (𝛼, ℓ), and then optimized using one of the learning objections from Subsection 9.1. Remark 9.10. Other ways of parameterizing localization could also be considered, such as taking 𝜃 = 𝐿 and learning the entire matrix, or parameterizing 𝐿(𝜃) with a given structure different from the one given above. Alternatives to Schur product localization can also be considered, such as ones that learn a nonlinear map that localizes the gain, 𝐾 𝑗 → L(𝐾 𝑗 ; 𝜃). ♢ Optimal Particle Filter Here we work in the setting of the linear observation operator (6.46), repeated here for convenience: 𝑣 † 𝑗=1 = Ψ(𝑣 † 𝑗 ) + 𝜉 † 𝑗 , 𝑦 † 𝑗+1 = 𝐻𝑣 † 𝑗+1 + 𝜂 † 𝑗+1 . We again make the same Gaussian and independence assumptions on the initialization and noise as are made in equations (6.1), (6.2) and Assumption 6.2. Recall that the optimal particle filter, from Subsection 6.3.7, works by proposing particles through an ensemble of equally weighted noisy 3DVar estimators and then reweighting them: see equation (6.50). Here, recognizing that reweighting often leads to particle collapse (see citations in Section 6.8), we instead seek to learn a desirable combination of the ensemble of 3DVars: ︀ 𝑣 (𝑛) 𝑗+1 = (𝐼 -𝐾𝐻)Ψ(𝑣 (𝑛) 𝑗 ) + 𝐾𝑦 † 𝑗+1 + 𝜁 (𝑛) 𝑗+1 , 𝑛 = 1, . . . , 𝑁, (9.25a) 𝑣 (𝑛) 𝑗+1 = ̂︀ 𝑣 (𝑛) 𝑗+1 + k (︁ {̂︀ 𝑣 (ℓ) 𝑗+1 } 𝑁 ℓ=1 , ̂︀ 𝑣 (𝑛) 𝑗+1 , 𝑦 † 𝑗+1 , 𝑖 (𝑛) 𝑗+1 ; 𝜃 )︁ , 𝑛 = 1, . . . , 𝑁, (9.25b) 𝑖 (𝑛) 𝑗+1 = 𝑦 † 𝑗+1 -𝐻 ̂︀ 𝑣 (𝑛) 𝑗+1 , 𝑛 = 1, . . . , 𝑁. (9.25c) Here, the innovation is defined slightly differently from (6.35). Random variables 𝜁 (𝑛) 𝑗+1 are i.i.d. in 𝑗 and 𝑛 and distributed according to 𝒩 (0, 𝐶), where, as in (6.49): 𝐶 = (𝐼 -𝐾𝐻)Σ, 𝐾 = Σ𝐻 ⊤ 𝑆 -1 , 𝑆 = 𝐻Σ𝐻 ⊤ + Γ. (9.26) Again, it is desirable that k(•, ̂︀ 𝑣 (𝑛) 𝑗+1 , 𝑦 † 𝑗+1 , 𝑖 (𝑛) 𝑗+1 ; 𝜃) is invariant with respect to permutation of the ensemble. The proposed form of the algorithm is a generalization of (9.3). We continue to work under Data Assumption 9.1. To emphasize dependence of the algorithm on the parameters 𝜃 to be learned, we again write 𝑣 (𝑛) 𝑗 (𝜃) and consider the state estimator in (9.6). We may then determine 𝜃 from (9.7). Remark 9.11. As in the context of the EnKF, other forms of learning problems may be postulated; see Remark 9.9. One generalization specific to this system relates to the fact that it may be desirable to learn the fixed gain 𝐾 appearing in (9.25a), rather than fix it according to (9.26), alongside 𝜃. ♢ Probabilistic Estimation: Learning Objectives In this section we discuss learning objectives that may be used to optimize a specified class of algorithms with respect to their ability to approximate the filtering distribution. We concentrate on particle-based methods. Recall that the time evolution of the filtering distribution can be defined by interweaving prediction by the underling stochastic dynamics with Bayes Theorem to incorporate the observations via the analysis step-see Section 6.3. In particular, (6.4) defines the evolution via the (i) prediction 𝜋 𝑗 ↦ → ̂︀ 𝜋 𝑗+1 = P𝜋 𝑗 and the (ii) analysis ̂︀ 𝜋 𝑗+1 ↦ → 𝜋 𝑗+1 = A 𝑗 (̂︀ 𝜋 𝑗+1 ) steps; it is in step (ii) that the data 𝑦 † 𝑗+1 is incorporated. The bootstrap particle filter of Subsection 6.3.6 uses this factorization of the filter evolution. This can be summarized, similarly to Section 6.3, as 𝜋 𝑗+1 = A 𝑗 ∘ P𝜋 𝑗 . The optimal particle filter (OPF) of Subsection 6.3.7 uses a different factorization into (i) 𝜋 𝑗 ↦ → p 𝑗+1 and (ii) p 𝑗+1 ↦ → 𝜋 𝑗+1 , in which both steps depends on the data 𝑦 † 𝑗+1 . At each time step 𝑗, the OPF first applies an analysis step to sample the conditional distribution p 𝑗+1 := P(𝑣 𝑗 |𝑌 𝑗 , 𝑦 † 𝑗+1 ) starting from P(𝑣 𝑗 |𝑌 𝑗 ); and second it applies a prediction step to sample from P(𝑣 𝑗+1 |𝑌 𝑗 , 𝑦 † 𝑗+1 ). These two steps may be summarized using the relationship 𝜋 𝑗+1 = P OPF 𝑗 ∘ A OPF 𝑗 𝜋 𝑗 . While the OPF performs the analysis step using importance sampling, we will show in this section how to achieve the first step using transports. In particular, our goal is to find a map 𝑇 depending on some parameters 𝜃 so that 𝑇 (𝑣 𝑗 , 𝑦 † 𝑗+1 ; 𝜃) ∼ A OPF 𝑗 𝜋 𝑗 , 𝑣 𝑗 ∼ 𝜋 𝑗 . (9.27) In what follows we use distance measures, such as those derived from the scoring rules in Section 11.3. (We exclude the deterministic scoring rules from Subsection 11.3.7, given in Definition 11.65, that are employed solely in the context of state estimation.) The key attribute we seek for the distance is that it is implementable given only samples. Indeed it is instructive to think of scoring rules as being introduced for this purpose: metrics and divergences may not be amenable to measuring distance between two probability measures when both are given only through samples, and are hence a sum of Dirac measures. Subsection 9.3.3 focuses on learning objectives appropriate for ensemble algorithms of the form (9.3): prediction using P and learning an approximation of the analysis map A 𝑗 , using a transport. Subsection 9.3.4 presents a generalized setting that includes the preceding subsection, but also allows for OPF-based algorithms, using transports that learn to apply the map A OPF 𝑗 . Matching The Filtering Distribution A desirable objective function for matching the approximate filtering distribution 𝜋 alg 𝑗 (𝜃) to the true filtering distribution 𝜋 𝑗 , over 𝑗 ∈ {1, . . . , 𝐽}, is J 𝐽 (𝜃) = 1 𝐽 𝐽 ∑︁ 𝑗=1 D(𝜋 alg 𝑗 (𝜃), 𝜋 𝑗 ), (9.28a ) 𝜃 ⋆ ∈ arg min 𝜃 J 𝐽 (𝜃). (9.28b) Here D : 𝒫(R 𝑑 )×𝒫(R 𝑑 ) → R is a metric, as defined in Section 11.1, although a divergence could also be used. However, implementation of such an objective is typically very difficult because 𝜋 𝑗 is only known through a complicated iteration that interweaves forecast and analysis steps, as detailed in Subsection 6.2.1. For this reason a variety of related approaches are used as the basis of algorithms. We detail these in the next three subsections. Matching The Filtering Distribution: Variational Bayes In Sections 7.2 and 7.3 we demonstrate how variational Bayes may be used to design objective functions from which to learn approximate filters that are defined implicitly by the action of transports. The reader is encouraged to study the resulting learning objectives implied by these approaches. We include an example, based on the ideas in Section 7.2, in Subsection 9.4.1. Matching The Filtering Distribution: Transport The focus of this section is the development of measure transport approaches for approximation of the filtering problem. We consider finding a transport map 𝑇 (•; 𝑦 † 𝑗+1 , 𝜃) : R 𝑑 → R 𝑑 with property that, approximately, ︀ 𝑣 𝑗+1 ∼ ̂︀ 𝜋 𝑗+1 ⇒ 𝑇 (̂︀ 𝑣 𝑗+1 ; 𝑦 † 𝑗+1 , 𝜃) ∼ 𝜋 𝑗+1 , where ̂︀ 𝜋 𝑗+1 and 𝜋 𝑗+1 are, respectively, the outputs of the prediction and analysis steps (6.4). Thus, in particular, 𝑇 will depend on ̂︀ 𝜋 𝑗+1 . In many practical settings we only have access to ̂︀ 𝜋 𝑗+1 through samples. We hence work under the following data assumption: Data Assumption 9.12. We are given i.i.d. samples {̂︀ 𝑣 (𝑛) 𝑗+1 } 𝑁 𝑛=1 from the distribution ̂︀ 𝜋 𝑗+1 at time 𝑗 + 1 arising from the prediction step in (6.4); furthermore we are able to evaluate l(𝑦 † 𝑗+1 |̂︀ 𝑣 (𝑛) 𝑗+1 ). Remark 9.13. In practice, when using methods based on 𝑇 within multiple iterations of the predict-analysis cycle, we will not have access to exact samples from the forecast distribution ̂︀ 𝜋 𝑗+1 at time 𝑗 + 1. However the proposed methodologies that follow in the remainder of this chapter may be implemented both when the given data forms exact samples and when it forms approximate samples. ♢ Noting the previous remark, we consider an algorithm of the form (9.3): 𝑣 (𝑛) 𝑗+1 = 𝑇 (︁ ̂︀ 𝑣 (𝑛) 𝑗+1 ; 𝑦 † 𝑗+1 , 𝜃 )︁ , 𝑛 = 1, . . . , 𝑁. (9.29) where ̂︀ 𝑣 (𝑛) 𝑗+1 are the predictive samples given by Data Assumption 9.12. Remark 9.14. For the specific transport map in (9.29) we have removed the dependence on the predicted data ̂︀ 𝑦 (ℓ) 𝑗+1 ; as such we are seeking to transport from the prior ̂︀ 𝜋 𝑗+1 to the posterior 𝜋 𝑗+1 . In subsequent transport methods, under different data assumptions, we will seek mappings from the joint predicted distribution to the posterior, hence approximating conditioning. ♢ of the predictive distribution. Furthermore the same data may be used to encode information about the observation operator ℎ, through evaluation of likelihoods which is assumed possible under Data Assumption 9.12. ♢ The empirical approximation (9.30) may itself be reweighted by likelihood information in order to obtain an approximation 𝜋 alg 𝑗+1 of the analysis distribution: 𝜋 alg 𝑗+1 = 𝑁 ∑︁ 𝑛=1 𝑤 (𝑛) 𝑗+1 𝛿 ̂︀ 𝑣 (𝑛) 𝑗+1 , ℓ (𝑛) 𝑗+1 = l(𝑦 † 𝑗+1 |̂︀ 𝑣 (𝑛) 𝑗+1 ), 𝑤 (𝑛) 𝑗+1 = ℓ (𝑛) 𝑗+1 ∑︀ 𝑁 𝑖=1 ℓ (𝑖) 𝑗+1 , ( 9.31) where the likelihood weights are normalized to sum to one so that 𝜋 alg 𝑗+1 is a probability measure. Recall the definition of a metric D : 𝒫(R 𝑑 ) × 𝒫(R 𝑑 ) → R from Section 11.1. Using construction (9.31) we may now estimate an optimal 𝜃 ⋆ that minimizes the distance at each analysis step: J 𝑁 (𝜃) = D (︁ 𝑇 (•; 𝑦 † 𝑗+1 , 𝜃) ♯ ̂︀ 𝜋 alg 𝑗+1 , 𝜋 alg 𝑗+1 )︁ , (9.32a) 𝜃 ⋆ ∈ arg min 𝜃 J 𝑁 (𝜃). (9.32b) In principle we might use a divergence, rather than a metric. In practice, the use of particles approximations ̂︀ 𝜋 alg , 𝜋 alg means that specific choices of the metric or divergence need to be made for the loss function J 𝐽 to be well-defined; this issue is addressed in Subsections 9.4.2, 9.4.3 and 9.4.4. After finding 𝜃 ⋆ , and hence 𝑇 , we can evaluate the map at samples from the prediction to generate analysis samples. By resampling 𝜋 alg 𝑗+1 we may obtain approximate i.i.d. samples {𝑣 (𝑚) 𝑗+1 } 𝑀 𝑚=1 from the analysis distribution 𝜋 𝑗+1 at time 𝑗 + 1 arising from the analysis step in (6.4). We then have the alternate approximation of the analysis distribution given by 𝜋 ralg 𝑗+1 = 1 𝑀 𝑀 ∑︁ 𝑚=1 𝛿 𝑣 (𝑚) 𝑗+1 . (9.33) Using 𝜋 ralg 𝑗+1 in (9.33) instead of 𝜋 alg 𝑗+1 in (9.32), we may estimate an optimal 𝜃 ⋆ by solving the following problem: J 𝑁,𝑀 (𝜃) = D (︁ 𝑇 (•; 𝑦 † 𝑗+1 , 𝜃) ♯ ̂︀ 𝜋 alg 𝑗+1 , 𝜋 ralg 𝑗+1 )︁ , (9.34a) 𝜃 ⋆ ∈ arg min 𝜃 J 𝑁,𝑀 (𝜃). (9.34b) Remark 9.16. We have considered the use of data from the predictive distribution at one fixed time 𝑗 + 1. However the ideas in this subsection may be generalized to allow averaging over a time-series with 𝑗 ∈ {0, . . . , 𝐽 -1}. This idea is developed explicitly in the next subsection. ♢ Matching The Filtering Distribution: Generalized Transport The methods of the previous section can be generalized to settings that go beyond the algorithm class (9.3) and allow use of, for example, the optimal particle filter, employed for state estimation in Subsection 9.2.4. To allow for this setting we now work under the following data assumption: Data Assumption 9.17. We are given i.i.d. samples {𝑣 (𝑚) 𝑗+1 } 𝑀 𝑚=1 from the filtering distribution 𝜋 𝑗+1 at time 𝑗 + 1. In practice we typically will not have access to exact samples from the filtering distribution 𝜋 𝑗+1 at time 𝑗 + 1, but to samples which may be empiricalized to find an approximate estimate 𝜋 est 𝑗+1 = 1 𝑀 𝑀 ∑︁ 𝑚=1 𝛿 𝑣 (𝑚) 𝑗+1 . (9.35) For instance, 𝜋 est 𝑗+1 may arise from an application of the OPF in (6.50) to provide an approximate estimate via (6.44). We note that the OPF is only exact in the limit 𝑀 → ∞. In what follows, the proposed methodologies we describe are blind as to whether the given data forms exact or approximate samples, similarly to Remark 9.13. Consider an algorithm which gives rise to an estimator 𝜋 alg 𝑗+1 (𝜃) for the true filter 𝜋 𝑗+1 ; for example 𝜋 alg 𝑗+1 (𝜃) may be the empirical distribution of the particles arising from an algorithm in class (9.3). Again, recalling the definition of a metric from Section 11.1, we estimate 𝜃 ⋆ that minimizes the distance between the true and approximate filters via the optimization problem J 𝑀 (𝜃) = D( 𝜋 alg 𝑗+1 (𝜃), 𝜋 est 𝑗+1 ), (9.36a) 𝜃 ⋆ ∈ arg min 𝜃 J 𝑀 (𝜃). (9.36b) In practice we may wish to deploy the ideas with data at multiple times 𝑗: Data Assumption 9.18. We are given i.i.d. samples {𝑣 (𝑚) 𝑗+1 } 𝑀 𝑚=1 from the filtering distribution 𝜋 𝑗+1 at times 𝑗 = 1, . . . , 𝐽. We then deploy the objective function: J 𝐽,𝑀 (𝜃) = 1 𝐽 𝐽-1 ∑︁ 𝑗=0 D(𝜋 alg 𝑗+1 (𝜃), 𝜋 est 𝑗+1 ), (9.37a ) 𝜃 ⋆ ∈ arg min 𝜃 J 𝐽,𝑀 (𝜃). (9.37b) Probabilistic Estimation: Algorithms In Subsection 9.4.1 we discuss the learning of analysis maps, in the ensemble setting, using variational inference. In Subsection 9.4.2 we discuss the learning of a general transport map, using Data Assumption 9.12, showcasing use of the energy distance to define the learning objective. For particle filters, an issue arising in application to high-dimensional problems is weight collapse-one of the weights approaches one and all others are close to zero. We provide a methodology to learn new ensemble methods that lead to equal-weight ensemble filters, trained to be close to the bootstrap or optimal particle filters (OPFs). Subsection 9.4.3 is devoted to a specific form of ensemble transport map, and may be viewed in this context as an instance of the setting of Subsection 9.4.2 with Data Assumption 9.17 arising from the bootstrap particle filter. Subsection 9.4.4 uses the same perspective to build an ensemble transport map with Data Assumption 9.17 arising from the OPF. Variational Bayes In this section we present a formulation for learning parameters based on the variational inference objectives introduced in Section 7.2. To do so, we consider a class of variational approximations defined by 𝑞 𝑗+1 (𝜃) = 𝑇 (•; 𝑦 † 𝑗+1 , 𝜃) ♯ P𝑞 𝑗 (𝜃), 𝑞 0 = 𝜋 0 . If 𝑞 𝑗 is an empirical measure 𝑞 𝑗 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑣 (𝑛) 𝑗 , then 𝑞 𝑗+1 is also an empirical measure whose samples are defined by first sampling from the forecast model, followed by evaluating the transport map. That is, 𝑞 𝑗+1 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑣 (𝑛) 𝑗+1 = Ψ(𝑣 (𝑛) 𝑗 ) + 𝜉 (𝑛) 𝑗 , 𝑣 (𝑛) 𝑗+1 = 𝑇 (̂︀ 𝑣 (𝑛) 𝑗+1 ; 𝑦 † 𝑗+1 , 𝜃), 𝑣 (𝑛) 0 ∼ 𝜋 0 . Our goal is to identify an element of this class of measures 𝑞 𝑗 , by selecting 𝜃 that approximates the filtering distribution 𝜋 𝑗 at step 𝑗 of filtering using the variational Bayes objective in (7.8) . Recalling the likelihood P(𝑦 † 𝑗+1 |𝑣 † 𝑗+1 ) and the objective function (7.8): J 𝑗+1 (𝜃) = D KL (𝑞 𝑗+1 (𝜃)‖P𝑞 𝑗 (𝜃)) -E 𝑞 𝑗+1 (𝜃) [log P(𝑦 † 𝑗+1 |•)]. Since we have access to the likelihood in the filtering setting, the second term in the preceding objective can be directly evaluated for the empirical measure to obtain -E 𝑣 † 𝑗+1 ∼𝑞 𝑗+1 (𝜃) [log P(𝑦 † 𝑗+1 |𝑣 † 𝑗+1 )] = - 1 𝑁 𝑁 ∑︁ 𝑛=1 log P(𝑦 † 𝑗+1 |𝑣 (𝑛) 𝑗+1 ). (9.38) However, the first term is problematic since one cannot evaluate the KL divergence between two empirical measures. To approximate this term we can first project into the space of Gaussian measures. Using the Gaussian projection G introduced in Remark 2.11, we make the replacement D KL (𝑞 𝑗+1 (𝜃)‖P𝑞 𝑗 (𝜃)) ↦ → D KL (︁ G(𝑞 𝑗+1 (𝜃))‖G (︀ P𝑞 𝑗 (𝜃) )︀ )︁ , ( 9.39) and then use the formula for the KL divergence between two Gaussian measures, (11.18) . One can then either minimize the sum over 𝑗 of the objectives J 𝑗 , as in (7.9), or minimize each J 𝑗 individually, as in (7.10). Remark 9.19. The substitution (9.39) does not lead to a viable method if 𝑁 < 𝑑, since the covariance matrix of the resulting Gaussian will be rank-deficient, leading to a degenerate distribution for which the KL divergence cannot be computed. The covariance matrix can be modified to be full rank by a technique such as localization; see Subsection 6.3.5. ♢ Remark 9.20. Using the invariance of the KL divergence under invertible transformations, it can be shown that an alternative to (9.39) can be formulated that uses the Gaussian projection G only once rather than twice; see the bibliography. ♢ General Transport Map We start by discussing the learning of a transport map using the methodology outlined in Subsection 9.3.3. Using algorithm class (9.29) and noting Remark 9.15, forms a natural setting for which we wish to learn parameter 𝜃 under Data Assumption 9.12. A major challenge for learning within ensemble filtering algorithm classes is that both the reference (prediction) and target (analysis) distributions for the map do not have analytical density functions; everything is defined through approximate ensembles, and hence combinations of Dirac measures. For this reason, the transport approaches derived in Chapter 4 that rely on the explicit form of the reference density to define the loss function do not apply; in particular the KL divergence in (4.2) cannot be used. Remark 11.26 highlights the fact that both MMD and the energy distance can be implemented when both probability measures are sums of Dirac measures. In this subsection we use the energy distance for ease of notation, while the following subsection will be implemented with the (more general) MMD. Recall from Definition 11.24 the squared energy distance, here expressed in terms of matching measures ̂︀ 𝜋, 𝜋 through a pushforward 𝑇 : D 2 E (𝑇 ♯ ̂︀ 𝜋, 𝜋) = 2E (𝑣,𝑣 ′ )∼𝑇 ♯ ̂︀ 𝜋⊗𝜋 |𝑣 -𝑣 ′ | -E (𝑣,𝑣 ′ )∼𝑇 ♯ ̂︀ 𝜋⊗𝑇 ♯ ̂︀ 𝜋 |𝑣 -𝑣 ′ | -E (𝑣,𝑣 ′ )∼𝜋⊗𝜋 |𝑣 -𝑣 ′ |. (9.40) Note that, in fact, D 2 E (𝑇 ♯ ̂︀ 𝜋, 𝜋) = L(𝑇 ) + 𝑐, L(𝑇 ) := 2E (𝑣,𝑣 ′ )∼𝑇 ♯ ̂︀ 𝜋⊗𝜋 |𝑣 -𝑣 ′ | -E (𝑣,𝑣 ′ )∼𝑇 ♯ ̂︀ 𝜋⊗𝑇 ♯ ̂︀ 𝜋 |𝑣 -𝑣 ′ |, := 2E (𝑢,𝑣 ′ )∼̂︀ 𝜋⊗𝜋 |𝑇 (𝑢) -𝑣 ′ | -E (𝑢,𝑢 ′ )∼̂︀ 𝜋⊗̂︀ 𝜋 |𝑇 (𝑢) -𝑇 (𝑢 ′ )|. (9.41) where 𝑐 is a constant that is independent of the transport 𝑇 . Given that we will optimize over the (𝜃-parameterized) transport 𝑇 , we emphasize that this constant is, for our purposes, irrelevant. Now consider optimization problem (9.32) with D = D E . Choosing 𝑇 = 𝑇 (•; 𝑦 † 𝑗+1 , 𝜃) with the L in (9.41), we set 𝜃 ⋆ ∈ arg min 𝜃∈Θ L(𝜃). (9.42) We now consider two settings for the empirical loss. By choosing ̂︀ 𝜋 = ̂︀ 𝜋 alg 𝑗+1 and 𝜋 = 𝜋 alg 𝑗+1 in (9.41) the loss is given by L 𝑁 (𝜃) = 2 𝑁 2 𝑁 ∑︁ 𝑛,𝑚=1 𝑛̸ =𝑚 |𝑇 (̂︀ 𝑣 (𝑛) 𝑗+1 ; 𝜃) -̂︀ 𝑣 (𝑚) 𝑗+1 |𝑤 (𝑚) 𝑗 -1 𝑁 2 𝑁 ∑︁ 𝑛,𝑚=1 𝑛̸ =𝑚 |𝑇 (̂︀ 𝑣 (𝑛) 𝑗+1 ; 𝜃) -𝑇 (̂︀ 𝑣 (𝑚) 𝑗+1 ; 𝜃)|, (9.43) where we only keep independent pairs of samples in the empirical loss. If, instead of using (9.32), we use (9.34) by resampling 𝑀 particles from the weighted ensemble, then we again obtain the loss L 𝑁,𝑀 (𝜃) = 2 𝑁 𝑀 𝑁 ∑︁ 𝑛=1 𝑀 ∑︁ 𝑚=1 |𝑇 (̂︀ 𝑣 (𝑛) 𝑗+1 ; 𝜃) -𝑣 (𝑚) 𝑗+1 | - 1 𝑁 𝑀 𝑁 ∑︁ 𝑛=1 𝑀 ∑︁ 𝑚=1 |𝑇 (̂︀ 𝑣 (𝑛) 𝑗+1 ; 𝜃) -𝑇 (̂︀ 𝑣 (𝑚) 𝑗+1 ; 𝜃)|. (9.44) Remark 9.21. While evaluating the loss function, and hence the learned transport map, relies on importance sampling, the forecast ensemble will be updated using the transport alone. The transport will push-forward an equally weighted ensemble of forecast to analysis samples. This may partially alleviate the degeneracy faced by particle filters where the ensemble eventually has an effective sample size of 1. Particle degeneracy arises from keeping the position of each particle fixed during the analysis step. This constraint forces particles to possibly remain in regions of low probability under the filtering distribution, after conditioning on an observation. Instead, the approach outlined in this section uses transport to move the particle positions in each analysis step. ♢ Bootstrap Particle Filter We continue to work under Data Assumption 9.12. We show how we may learn parameters in a class of ensemble Kalman filters in order to match the true filter based on the methodology from the previous subsection. The methodology is essentially the same as that from the previous subsection, with the exceptions that we employ MMD rather than energy distance and we learn within a subclass of transport maps given by the ensemble Kalman filter. To represent the true filter we use the particle filter, since its ensemble is known to converge to the true filtering distribution in the large particle limit. Recall, from (6.40), (6.41), the particle filter approximation of the filtering distribution 𝜋 𝑗+1 : 𝜋 BPF 𝑗+1 = 𝑀 ∑︁ 𝑚=1 𝑤 (𝑚) 𝑗+1 𝛿 ̂︀ 𝑣 (𝑚) 𝑗+1 . Here the particles ̂︀ 𝑣 (𝑚) 𝑗 and weights 𝑤 (𝑚) 𝑗 evolve according to ︀ 𝑣 (𝑚) 𝑗+1 = Ψ (︀ 𝑣 (𝑚) 𝑗 )︀ + 𝜉 (𝑚) 𝑗 , ℓ (𝑚) 𝑗+1 = exp (︂ - 1 2 ⃒ ⃒ 𝑦 † 𝑗+1 -ℎ (︀ ̂︀ 𝑣 (𝑚) 𝑗+1 )︀⃒ ⃒ 2 Γ )︂ , 𝑤 (𝑚) 𝑗+1 = ℓ (𝑚) 𝑗+1 ⧸︁(︁ 𝑀 ∑︁ 𝑖=1 ℓ (𝑖) 𝑗+1 )︁ . We have chosen integer 𝑀 here for the number of particles to emphasize that it may be different from integer 𝑁 used in the ensuing ensemble Kalman-like method. From 𝜋 BPF 𝑗+1 , we may construct an equally weighted approximate distribution by resampling to obtain 𝜋 est 𝑗+1 = 1 𝑀 𝑀 ∑︁ 𝑚=1 𝛿 𝑣 (𝑚) 𝑗+1 . Here the particles 𝑣 (𝑚) 𝑗+1 are drawn i.i.d. from 𝜋 BPF 𝑗+1 . In this process some of the {̂︀ 𝑣 (𝑖) 𝑗+1 } 𝑀 𝑖=1 may be dropped and others repeated. With 𝜋 est in hand, we may also view ourselves as working under Data Assumption 9.17. We want to learn parameters 𝜃 in a modified ensemble Kalman filter, defined by equation (9.24) , so that the following equally weighted approximation of the filtering distribution is close to the true filtering distribution: 𝜋 EnKF 𝑗+1 (𝜃) = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑣 (𝑛) 𝑗+1 (𝜃) , where we recall (9.24) to define the particles as ︀ 𝑣 (𝑛) 𝑗+1 = Ψ(𝑣 (𝑛) 𝑗 ) + 𝜉 (𝑛) 𝑗 , 𝑛 = 1, . . . , 𝑁, 𝑣 (𝑛) 𝑗+1 (𝜃) = ̂︀ 𝑣 (𝑛) 𝑗+1 + 𝐾 𝑗+1 (︀ 𝑦 † 𝑗+1 -𝜂 (𝑛) 𝑗+1 -ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) )︀ , 𝑛 = 1, . . . , 𝑁, 𝐾 𝑗+1 = k (︀ ̂︀ 𝑣 (1) 𝑗+1 , . . . , ̂︀ 𝑣 (𝑁 ) 𝑗+1 ; 𝜃 )︀ . Note that the {𝑣 𝑗+1 } 𝑀 ℓ=1 used to define 𝜋 est . Recall the integral probability metric MMD from Definition 11.18, noting that it may be implemented for empirical (equally weighted ensemble) measures as in (11.12) . We then define 𝜃 ⋆ by J 𝑀,𝑁 (𝜃) = D MMD (︀ 𝜋 EnKF 𝑗+1 (𝜃), 𝜋 est 𝑗+1 )︀ , ( 9 .45a) 𝜃 ⋆ ∈ arg min 𝜃 J 𝑀,𝑁 (𝜃). (9.45b) This is simply optimization problem (9.36) with a specific choice of MMD for the metric, and with 𝜋 alg 𝑗+1 = 𝜋 EnKF 𝑗+1 . Because 𝜋 EnKF 𝑗+1 depends on the ensemble size 𝑁 , we indicate this in the loss function. 𝑗 (𝜃)} 𝑁 𝑛=1 for the equally weighted ensemble filter. They should be chosen from the same distribution 𝜋 𝑗 , but they do not need to be identical as points. For instance, it is possible to choose 𝜋 𝑗 = 𝜋 EnKF 𝑗 and to average J(•) over time indices 𝑗 ∈ {0, . . . , 𝐽 -1}. The preceding methodology could then be implemented under Data Assumption 9.18, leading to the objective function in (9.37) with 𝜋 alg 𝑗+1 = 𝜋 EnKF 𝑗+1 . Recall also that 𝑀 may not equal 𝑁 ; indeed it may be advantageous to choose 𝑀 ≫ 𝑁 to ensure that the resampling process provides a good representation of the true filter 𝜋 𝑗+1 . ♢ Optimal Particle Filter We now describe a similar methodology to that in the preceding subsection, but based on the optimal particle filter rather than the bootstrap particle filter. Recall, from (6.44), (6.50), the optimal particle filter approximation of the filtering distribution 𝜋 𝑗+1 : 𝜋 OPF 𝑗 = 𝑀 ∑︁ 𝑚=1 𝑤 (𝑚) 𝑗 𝛿 ̂︀ 𝑣 (𝑚) 𝑗 , where the particles ̂︀ 𝑣 (𝑚) 𝑗 and weights 𝑤 (𝑚) 𝑗 evolve according to the following steps, with noise samples 𝜁 (𝑚) 𝑛+1 drawn i.i.d. from 𝒩 (0, 𝐶): ︀ 𝑣 (𝑚) 𝑗+1 = (𝐼 -𝐾𝐻)Ψ(𝑣 (𝑚) 𝑗 ) + 𝐾𝑦 † 𝑗+1 + 𝜁 (𝑚) 𝑗+1 , 𝑣 (𝑚) 𝑗 i.i.d. ∼ 𝜋 OPF 𝑗 , ℓ (𝑚) 𝑗+1 = exp (︂ - 1 2 |𝑦 † 𝑗+1 -𝐻Ψ(𝑣 (𝑚) 𝑗 )| 2 𝑆 )︂ , 𝑤 (𝑚) 𝑗+1 = ℓ (𝑚) 𝑗+1 ⧸︁(︁ 𝑁 ∑︁ 𝑖=1 ℓ (𝑖) 𝑗+1 )︁ . From 𝜋 OPF 𝑗+1 , we may construct an equally weighted approximate distribution by resampling to obtain 𝜋 est 𝑗+1 = 1 𝑀 𝑀 ∑︁ 𝑚=1 𝛿 𝑣 (𝑚) 𝑗+1 . Here the partciles 𝑣 (𝑚) 𝑗+1 are drawn i.i.d. from 𝜋 OPF 𝑗+1 . Recall Subsection 9.2.4, and (9.25) repeated here for convenience, recalling the slightly different definition of innovation in comparison with (6.35): ︀ 𝑣 (𝑛) 𝑗+1 = (𝐼 -𝐾𝐻)Ψ(𝑣 (𝑛) 𝑗 ) + 𝐾𝑦 † 𝑗+1 + 𝜉 (𝑛) 𝑗+1 , 𝑣 (𝑛) 𝑗+1 = ̂︀ 𝑣 (𝑛) 𝑗+1 + k (︁ {̂︀ 𝑣 (ℓ) 𝑗+1 } 𝑁 ℓ=1 , ̂︀ 𝑣 (𝑛) 𝑗+1 , 𝑦 † 𝑗+1 , 𝑖 (𝑛) 𝑗+1 ; 𝜃 )︁ , 𝑛 = 1, . . . , 𝑁, 𝑖 (𝑛) 𝑗+1 = 𝑦 † 𝑗+1 -𝐻 ̂︀ 𝑣 (𝑛) 𝑗+1 . We want to learn parameters 𝜃 so that the following equally weighted approximation of the filtering distribution is close to the true filtering distribution: 𝜋 EOPF 𝑗+1 (𝜃) = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑣 (𝑛) 𝑗+1 (𝜃) . Note that the {𝑣 The same comments as in Remark 9.22 apply here too. Bibliography This chapter presents various methodologies for learning filters and smoothers using a common framework based on transport between random variables. We show how several learning problems for classical algorithms, such as learning the parameters in an EnKF can be viewed as parameterizing a transport map, and generalize the classical algorithm based on a constant gain function. This perspective permits generalizations of the methodology to new architectures and loss functions to identify optimal parameters in these methods from data, such as the use of probabilistic losses that aim to match the true filtering distribution. Whilst some algorithms have already been implemented and tested, demonstrating favourable behaviour, it is of interest to examine the performance of variants proposed here, such as the OPF with learned parameters, which have not yet been tested. In this section, we provide some relevant references to existing numerical methods and relevant techniques. Learning approaches for ensemble filtering and smoothing based on transportation of measure have shown recent promise in generalizing classic Kalman approaches and reducing the error. The paper [394] introduces the idea of learning transports to build prior-to-posterior maps, while [11] seeks optimal transport maps by solving adversarial learning problems. A related framework known as Gaussian anamorphosis seeks invertible transformations of the state and observations where Kalman algorithms apply. See [187] for an approach based on nonlinear diagonal transformations and a generalization that uses invertible neural networks in [107] . Other nonlinear generalizations of ensemble Kalman filtering methods include [210, 19, 274] . A nonlinear filter that is derived using variational inference using transports that lie in reproducing kernel Hilbert spaces (see Example 11.16 ) is [348] . An approach based on diffusion models is provided in [38] . Approaches to parameterize and learn the gain to minimize state estimation error has been studied in various settings. 3DVar gain learning was considered in [208, 290] , and a lower-dimensional parameterization was considered in [209] ; geophysical applications motivated this work. The form of 3DVar, based on estimation of ̂︀ 𝐶 and use of (6.14a) to form the gain, originated in numerical weather prediction [284, 285] . It was proven in [208, 290] that the fixed gain which minimizes the expected error with respect to observations, in the asymptotic limit, is the steady-state Kalman gain. However, the cost functions considered were different and [208] required assumptions on the rank of 𝐻. Learning the gain was also considered in [276] with nonlinear dynamics. The learning of a neural network analysis step by minimizing a state estimation loss was considered in [299] . The analysis step there is EnKF-like in the sense that the ensemble members interact during the analysis step only through the ensemble mean and covariance. The learning of an analysis step was also considered in [63] . Permutationinvariant neural networks were used for postprocessing of an ensemble in [215] , and for learning improved ensemble filters in [34] . The paper [34] considers a general framework for learning, based on minimizing state estimation error, and generalizing the ensemble Kalman filter; the set transformer is used to encode ensembles of arbitrary size with a signle set of learned parameters, and fine-tuning is used for localization and inflation. The issue of estimating optimism and out-of-sample performance of DA algorithms is discussed in [74, 290] . Various approaches to learning inflation are discussed in [17, 308] . Learning localization by minimizing the squared Euclidean distance between the analysis and the true trajectory is considered in [438] , while a similar approach with a different scoring rule is considered in [310] . Other approaches to learning localization are discussed in [344, 105, 238, 347, 428] . Emulating the analysis step of a Kalman filter using recurrent neural networks is considered in [200] . An ensemble estimator for the energy score was introduced in [177] . For the statement and proof of the Stolz-Cesàro theorem, see Theorem 1.23 in [315] . Several works have proposed to learn the filtering distribution directly without using knowledge of the model dynamics [75, 71, 367] . This is sometimes referred to as \"endto-end\" learning. This has also been considered for state estimation in [418, 132, 425] . The methodology presented in Subsection 9.4.1 for learning ensemble filters using variational inference was introduced in [35] . The reformulation of the objective function that avoids one of the Gaussian projections, mentioned in 9.20, is discussed therein. Chapter 10 Data Dependence Of Filters And Smoothers In this chapter we use learning to solve the filtering and smoothing problems introduced in Chapter 6. We consider both state estimation, i.e., finding a representative sequence of states from the observations and knowledge of the model, and probabilistic estimation, i.e., approximating the posterior distributions for sequences of states given observations. Doing this, of course, was also the subject of Chapters 7 and 9. The key distinguishing concept in this chapter is amortization: the idea that, if we learn dependence of an algorithm on data it may be re-used multiple times and hence the investment cost in the original training of the method is amortized; the success of such an approach depends on the ability of the proposed method to learn data dependence that generalizes well. Recall that the idea of learning based on observing multiple instances of data was introduced in Sections 5.1-5.2 in the context of learning regularizers for general inverse problems and in Sections 5.3-5.5 in the context of approximations to the posterior distribution arising from Bayesian inversion based on transport. Furthermore, we note that in Chapter 9 we have also learned models that take different observation sequences as input; there we use long time-trajectories and ergodicity, coupled with inductive bias in the form of the learned filters, to address the issue of learning dependence on different observation sequences. In this chapter our approach is more akin to Chapter 5, using multiple data sets, rather than one long trajectory and the appeal to ergodicity. We focus on learning data dependence for filtering and smoothing algorithms. Section 10.1 is devoted to state estimation, and to learning data-dependence in 3DVar and 4DVar-type algorithms, respectively. In Section 10.2 we study variational approaches from Chapter 7, showing how to introduce amortization in both filtering and smoothing approaches. Section 10.3 is devoted to transport-based methods, from Chapter 9, generalizing them to the amortized setting; a transport methodology for learning ensemble filters is introduced that employs objective functions based on the energy distance or maximum likelihood estimation. We include a concrete example of learning transports for smoothing, again employing the energy distance. In Section 10.4 we introduce a new learning paradigm for amortized data assimilation problems, based on strictly proper scoring rules. The chapter concludes with bibliographic remarks in Section 10.5. Throughout, we use the notational conventions summarized in Remark 7.1. We recommend that the reader review this notation. State Estimation In the following two subsections we show how the 3DVar and 4DVar algorithms may be amortized. We employ distance-like deterministic scoring rules d; see Definition 11.65 from Subsection 11.3.7. We also recall the notation from Remark 7.1. Amortized 3DVar Recall the 3DVar algorithm defined in Section 6.3.2, which has the gain matrix 𝐾 as a tunable parameter. The dependence of the 3DVar algorithm on data 𝑦 † 𝑗 is thus prescribed to be linear, representing a form of inductive bias inherited from the linear Gaussian setting and from the steady state of the Kalman filter. In order to allow application to a wider class of problems, here we generalize the 3DVar algorithm by introducing a learnable nonlinear gain function, which takes the innovation as input: ︀ 𝑣 𝑗+1 = Ψ(𝑣 𝑗 ), (10.1a ) 𝑣 𝑗+1 = ̂︀ 𝑣 𝑗+1 + k (︀ 𝑦 † 𝑗+1 -ℎ(̂︀ 𝑣 𝑗+1 ); 𝜃 )︀ . (10.1b) Thus, k(• ; 𝜃) : R 𝑘 → R 𝑑 is a (possibly nonlinear) map for each fixed 𝜃 ∈ Θ ⊆ R 𝑝 that applies the state correction at each assimilation time based on the observed data. This is a specific subclass of the algorithm class defined in (9.2). Moreover, it reduces to the classical ansatz made for 3DVar in (6.19) when the map k is a linear function of the innovation. We aim to learn 𝜃 given the following assumption about available data. Data Assumption 10.1. The data available is {(𝑉 † ) (𝑛) , (𝑌 † ) (𝑛) } 𝑁 𝑛=1 , comprising multiple independent realizations from the joint distribution 𝛾 on 𝑉 † := {𝑣 † 0 , . . . , 𝑣 † 𝐽 }, 𝑌 † := {𝑦 † 1 , . . . , 𝑦 † 𝐽 } defined by (6.1), (6.2). Recall the loss function in (9.7) that measures the distance between the estimated state 𝑣 𝑗 (𝜃) and true state 𝑣 † 𝑗 , both depending on specific realizations of data. Here we make the choice 𝑣 𝑗 (𝜃) = 𝑣 𝑗 from (10.1) and define {𝑣 (𝑛) 𝑗 } 𝑁 𝑛=1 to be the ensemble of state sequences from (10.1) when driven by the ensemble of observation sequences {(𝑦 † 𝑗 ) (𝑛) } 𝑁 𝑛=1 for each 𝑗 ∈ {1, . . . , 𝐽}; this ensemble of state sequences also depends on the gain parameter 𝜃. We average the loss function in (9.7) over both the ensembles of sequences indexed by 𝑛 and the time-index 𝑗 within each sequence; minimization defines the optimal 𝜃 ⋆ . Thus, we obtain the optimization problem J 𝐽,𝑁 (𝜃) = 1 𝐽𝑁 𝑁 ∑︁ 𝑛=1 𝐽 ∑︁ 𝑗=1 d (︀ 𝑣 (𝑛) 𝑗 , (𝑣 † 𝑗 ) (𝑛) )︀ , ( 10 .2) 𝜃 ⋆ ∈ arg min 𝜃 J 𝐽,𝑁 (𝜃). (10.3) Remark 10.2. Here we are solving the supervised learning problem of recovering the state 𝑉 † from the observations 𝑌 † . We have imposed a sequential filtering structure, via the architecture (10.1), on the learned solution. It is natural to ask whether a smoothing-type architecture could also be learned. The next subsection considers such approaches. ♢ Amortized 4DVar Recall the 4DVar approach to the smoothing problem defined in Section 6.4.1. The methodology defines an estimate of the state 𝑉 ⋆ := {𝑣 ⋆ 0 , . . . , 𝑣 ⋆ 𝐽 }, from the observed data 𝑌 † := {𝑦 † 1 , . . . , 𝑦 † 𝐽 }, through the solution of an optimization problem. Emphasizing the dependence of the objective function J and the solution 𝑉 ⋆ on the available data 𝑌 † we have J(𝑉 ; 𝑌 † ) = R(𝑉 ) + L(𝑉 ; 𝑌 † ), (10.4a) 𝑉 ⋆ (𝑌 † ) ∈ arg min 𝑉 ∈R 𝑑(𝐽+1) J(𝑉 ; 𝑌 † ), (10.4b) where R(𝑉 ) and L(𝑉 ; 𝑌 † ) are defined in (6.51) and (6.52), respectively. In the following we assume that we have multiple solutions of the 4DVar MAP estimator for multiple realizations of the data 𝑌 † . Our goal is to learn the dependence of 𝑉 ⋆ (•) on 𝑌 † . Data Assumption 10.3. The data available is {(𝑉 ⋆ ) (𝑛) , (𝑌 † ) (𝑛) } 𝑁 𝑛=1 , comprising multiple independent realizations of the data from the marginal distribution 𝜅 on 𝑌 † := {𝑦 † 1 , . . . , 𝑦 † 𝐽 } defined by (6.1), (6.2), together with solutions (𝑉 ⋆ ) (𝑛) = 𝑉 ⋆ (︀ (𝑌 † ) (𝑛) )︀ defined by (10.4a ). Given the data {︀ (𝑌 † ) (𝑛) , (𝑉 ⋆ ) (𝑛) }︀ 𝑁 𝑛=1 from Data Assumption 10.3 we may apply the methodology of Section 5.3 to learn the mapping from 𝑌 † to 𝑉 ⋆ . Indeed we find 𝑉 ⋆ (𝑌 † ) ≈ 𝑉 (𝑌 † ; 𝜃 ⋆ ) where 𝜃 ⋆ solves an empirical approximation of the minimization problem Assume now that direct simulation of the joint model for state and observations, 𝛾 defined by (6.1), (6.2), is available, as in Data Assumption 10.1. Then it possible to circumvent the use of 4DVar altogether and instead solve an empirical approximation of the supervised learning problem that maps a sequence of data to a sequence of states. J(𝜃) = E 𝑌 † ∼𝜅 [︁ d (︀ 𝑉 (𝑌 † ; 𝜃), 𝑉 ⋆ (𝑌 † ) )︀ ]︁ , ( 10 J(𝜃) = E (𝑌 † ,𝑉 † )∼𝛾 [︁ d (︀ 𝑉 (𝑌 † ; 𝜃), 𝑉 † )︀ ]︁ , ( 10 .6a) 𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃). (10.6b) Indeed, (10.2) is an empirical approximation of this minimization problem in the setting where a sequential filtering structure is imposed on the supervised learning architecture. However, this filtering structure is not necessary and a full smoothing architecture can also be used. The more general model appearing in (10.6) makes use of all available data (past and future) to predict the state at a given time. ♢ Variational Approaches We study amortized versions of variational approaches to both filtering in Subsection 10.2.1, and smoothing in Subsection 10.2.2. Throughout this section we use the following assumption concerning data foot_8 : Data Assumption 10.5. The data available is {(𝑌 † ) (𝑛) } 𝑁 𝑛=1 which are multiple independent realizations from the marginal distribution 𝜅 on the observed data 𝑌 † := {𝑦 † 1 , . . . , 𝑦 † 𝐽 }. Amortized Variational Filtering Here we build on the variational formulation of filtering from Section 7.2. Recall the definition for the subset of probability density functions 𝒞 𝐽 that are formed by the temporal autoregressive factorization in (7.11) , and Remark 7.7. Assuming that we have access to the distribution 𝜅 on 𝑌 † implied by the stochastic dynamics model (6.1), (6.2), (6.3) , and recalling that 𝜌 is the prior over the state sequence and l is the likelihood, both defined in (7.1a), we may consider the following optimization problem, constrained to the autoregressive factorization in (7.11) : J(𝑞) = E 𝑌 † ∼𝜅 [︁ D KL (𝑞‖𝜌) -E 𝑞 [︀ log l(𝑌 † |•) ]︀ ]︁ , ( 10.7 ) 𝑞 opt ∈ arg min 𝑞∈𝒞 𝐽 J(𝑞). (10.8) Here, the auto-regressive structure of 𝑞 makes it possible to encode filtering rather than smoothing, on the learned solution. To show how this approach works in practice we first parameterize (7.11 ) by 𝜃 = (𝜗 0 , 𝜗 1 ) to obtain 𝑞(𝑉 ; 𝜃) = 𝐽 ∏︁ 𝑗=1 q 𝑗 (𝑣 𝑗 |𝑉 𝑗-1 ; 𝑌 † 𝑗 , 𝜗 1 )q 0 (𝑣 0 ; 𝜗 0 ). (10.9) Recall 𝒞 𝐽 defined in Subsection 7.3. This defines a map ℱ 𝜃 : R 𝑘𝐽 → 𝒞 𝐽 ⊂ 𝒫 𝐽 that, for a given parameter vector 𝜃 ∈ Θ ⊆ R 𝑝 , maps observation sequence 𝑌 † into probability measures on R 𝑑(𝐽+1) with filtering structure. The parameterization in (10.9) leads to the following optimization problem: 𝜃 ⋆ ∈ arg min 𝜃∈Θ J (︀ 𝑞(•; 𝜃) )︀ , ( 10.10a) with the objective J defined in (10.7). In practice, the expectation over 𝑌 † is approximated empirically using Data Assumption 10.5. Having solved the optimization problem, and with 𝜃 ⋆ determined, for a new previously unseen data set 𝑌 † , we choose our approximate filtering distribution to be 𝑞 ⋆ = ℱ 𝜃 ⋆ (𝑌 † ). Example 10.6. We might choose q 0 to be the Gaussian 𝒩 (𝑚 0 , 𝐶 0 ) distribution for the initial state 𝑣 0 in (6.1) and q 𝑗 (𝑣 𝑗 |𝑉 𝑗-1 ; 𝑌 † 𝑗 , 𝜗 1 ) to be a Gaussian distribution on 𝑣 𝑗 with mean and covariance given as 𝜃-parameterized functions of (𝑣 𝑗-1 , 𝑦 † 𝑗 ) : q 𝑗 (𝑣 𝑗 |𝑉 𝑗-1 ; 𝑌 † 𝑗 , 𝜗 1 ) = 𝒩 (︁ m(𝑣 𝑗-1 , 𝑦 † 𝑗 ; 𝜃), C(𝑣 𝑗-1 , 𝑦 † 𝑗 ; 𝜃) )︁ . ♢ Amortized Variational Smoothing We may also be interested in learning a parametric model to capture dependence of an approximate smoothing distribution on the data 𝑌 † . To this end, in this subsection we continue to work under Data Assumption 10.5. We generalize the previous subsection and choose to minimize over the map 𝒮 𝜃 : R 𝐽𝑘 → 𝒬 𝐽 ⊂ 𝒫 𝐽 , parameterized by 𝜃 ∈ Θ ⊆ R 𝑝 : J(𝑞) = E 𝑌 † ∼𝜅 [︁ D KL (𝑞‖𝜌) -E 𝑞 [︀ log l(𝑌 † |•) ]︀ ]︁ , ( 10.11a ) 𝜃 ⋆ ∈ arg min 𝜃∈Θ J (︀ 𝒮 𝜃 (𝑌 † ) )︀ . (10.11b) Then, for given and unseen data 𝑌 † , we choose our approximate smoothing distribution to be 𝑞 ⋆ = 𝒮 𝜃 ⋆ (𝑌 † ). We emphasize that the resulting distribution in smoothing does not enforce the temporal dependence in filtering where each state marginal on depends on past observations. In particular, we typically expect that 𝑞 ⋆ ∈ 𝒫 𝐽 ∖𝒞 𝐽 . We consider an example of amortized variational smoothing which is similar in approach to that outlined for Bayesian inversion in Example 5.8. However, we propose a different two-stage approach to the optimization problem; this two-stage approach could have been adopted in Example 5.8 and likewise the one-stage approach of Example 5.8 could be adopted here. Example 10.7. In Subsection 7.4 we discuss the use of the variational smoothing approach to find a Gaussian approximation, centered on the map estimator. We now generalize this to the amortized setting. We use a two-stage process, first determining a MAP estimator 𝑉 (𝑌 † ; 𝜃 ⋆ ), capturing parametric dependence on 𝑌 † , using the ideas of Subsection 10.1.2. We then use amortized variational smoothing to identify the covariance Σ = Σ(𝑌 † ; 𝜗) with parametric dependence on 𝑌 † . Specifically, we define 𝒬 ⊂ 𝒫 𝐽 to be the set 𝒬 = {︁ 𝑣 ∈ 𝒫 𝐽 : 𝑣 = 𝒩 (︀ 𝑉 (𝑌 † ; 𝜃 ⋆ ), Σ(𝑌 † ; 𝜗) )︀ , 𝜗 ∈ Θ ′ }︁ . By considering the map into this set of parametrized distributions 𝒮 𝜗 : R 𝐽𝑘 → 𝒬, parametrized by 𝜗 ∈ Θ ′ , we identify the variational approximation by solving the minimization problem J(𝑞) = E 𝑌 † ∼𝜅 [︁ D KL (𝑞‖𝜌) -E 𝑞 [︀ log l(𝑌 † |•) ]︀ ]︁ , (10.12a) 𝜗 * ∈ arg min 𝜗∈Θ ′ J(𝒮 𝜗 (𝑌 † )). (10.12b) Then, we define the optimal variational distribution as 𝑞 ⋆ = 𝒮 𝜗 * (𝑌 † ). ♢ Transport Approaches to Filtering and Smoothing In this section we first seek transports that map to each filtering distribution 𝜋 𝑗+1 , as in Subsection 9.4.2. But in this case, where we seek to amortize over multiple data instances, we let the map 𝑇 explicitly depend on both the predicted state ̂︀ 𝑣 𝑗+1 ∈ R 𝑑 and the true observation 𝑦 † 𝑗+1 ∈ R 𝑘 . We aim to learn the dependence under a data assumption, which assumes only access to an ensemble from the joint distribution of state and observations; the ability to make likelihood evaluations is not assumed, thereby making the approach likelihood-free. In this section, we will develop approaches to perform inference at one fixed time 𝑗 + 1, but the ideas may also be generalized by averaging over a time-series indexed by 𝑗, as it is developed in Section 9.3.4 without amortization over the observations. Data Assumption 10.8. We are given pairs of i.i.d. samples {̂︀ 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑦 (𝑛) 𝑗+1 } 𝑁 𝑛=1 from the joint distribution 𝛾 𝑗+1 (𝑣, 𝑦) := ̂︀ 𝜋 𝑗+1 (𝑣)l(𝑦|𝑣). Noting Remark 9.14, we consider a generalization of (9.29) to algorithms of the form ̂︀ 𝑣 (𝑛) 𝑗+1 = Ψ(𝑣 (𝑛) 𝑗 ) + 𝜉 (𝑛) 𝑗 , 𝑛 = 1, . . . , 𝑁, (10.13a) ︀ 𝑦 (𝑛) 𝑗+1 = ℎ(̂︀ 𝑣 (𝑛) 𝑗+1 ) + 𝜂 (𝑛) 𝑗 , 𝑛 = 1, . . . , 𝑁, (10.13b) 𝑣 (𝑛) 𝑗+1 = 𝑇 (︁ ̂︀ 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑦 (𝑛) 𝑗+1 , 𝑦 † 𝑗+1 ; 𝜃 )︁ , 𝑛 = 1, . . . , 𝑁. (10.13c) Our goal is to find the parameters 𝜃 ∈ Θ ⊆ R 𝑝 of the map 𝑇 so that, at least approximately, (̂︀ 𝑣 𝑗+1 , ̂︀ 𝑦 𝑗+1 ) ∼ 𝛾 𝑗+1 ⇒ 𝑇 (̂︀ 𝑣 𝑗+1 , ̂︀ 𝑦 𝑗+1 , 𝑦 † 𝑗+1 ; 𝜃) ∼ 𝜋 𝑗+1 . (10.14) Similarly to Remark 9.15, optimal choice of the parameter 𝜃 encapsulates dependence on the given data for training, here being samples from the joint distribution of predicted state and observations. This data dependence reflects that the transport map depends on the underlying probabilistic structure of the filtering distribution and its updates. In the following two subsections we will present two formulations to find these maps and approximately enforce (10.14) . In Subsection 10.3.1 we minimize an objective based on the energy distance, as in Subsection 9.4.2, which concerns the setting where dependence on the observed data is not learned; in Subsection 10.3.2, on the other hand, we leverage the idea of transports learned via the ideas in Chapter 4 based on maximum likelihood. Lastly, we extend this framework of learning transports to smoothing in Subsection 10.3.3. Minimizing The Energy Distance The filtering distribution 𝜋 𝑗+1 is found from Bayes Theorem 1.3 with the prior coming from the forecast ̂︀ 𝜋 𝑗+1 and the likelihood model l(𝑦 † 𝑗+1 |•). Using the push-forward notation, our goal is to choose 𝜃 to get as close as possible to achieving 𝑇 (•, 𝑦 † 𝑗+1 ; 𝜃) ♯ ̂︀ 𝜋 𝑗+1 = 𝜋 𝑗+1 . (10.15) Generalizing the approach in Subsection 9.4.2, we consider learning a transport map to approximately enforce (10.15) by defining an objective using the energy distance. We compute the energy distance between the true and approximate filtering distributions and then take expectation over observations 𝑦 † 𝑗+1 drawn from their marginal distribution foot_9 𝜅 𝑗+1 . Recalling the definition for the energy distance in (11.24) we have E 𝑦 † 𝑗+1 ∼𝜅 𝑗+1 [︁ D 2 E (︀ 𝑇 (•, 𝑦 † 𝑗+1 ; 𝜃) ♯ ̂︀ 𝜋 𝑗+1 , 𝜋 𝑗+1 )︀ ]︁ = 𝑐 + 2E 𝑦 † 𝑗+1 ∼𝜅 𝑗+1 E (𝑣,𝑣 ′ )∼̂︀ 𝜋 𝑗+1 ⊗𝜋 𝑗+1 |𝑇 (𝑣, 𝑦 † 𝑗+1 ; 𝜃) -𝑣 ′ | -E 𝑦 † 𝑗+1 ∼𝜅 𝑗+1 E (𝑣,𝑣 ′ )∼̂︀ 𝜋 𝑗+1 ⊗̂︀ 𝜋 𝑗+1 |𝑇 (𝑣, 𝑦 † 𝑗+1 ; 𝜃) -𝑇 (𝑣 ′ , 𝑦 † 𝑗+1 ; 𝜃)|, where 𝑐 is a constant that is independent of 𝑇 , and hence 𝜃, as shown similarly in Subsection 9.4.2. By using the factorization of the joint distribution 𝛾 𝑗+1 (𝑣, 𝑦) = 𝜋 𝑗+1 (𝑣)𝜅 𝑗+1 (𝑦), and parameterizing the transport to be determined by 𝜃, we can write the loss function defined by the expected energy distance based on the terms that depend on 𝜃 as L(𝜃) = 2E (︀ 𝑣,(𝑣 ′ ,𝑦 † 𝑗+1 ) )︀ ∼̂︀ 𝜋 𝑗+1 ⊗𝛾 𝑗+1 |𝑇 (𝑣, 𝑦 † 𝑗+1 ; 𝜃) -𝑣 ′ | -E (𝑣,𝑣 ′ ,𝑦 † 𝑗+1 )∼̂︀ 𝜋 𝑗+1 ⊗̂︀ 𝜋 𝑗+1 ⊗𝜅 𝑗+1 |𝑇 (𝑣, 𝑦 † 𝑗+1 ; 𝜃) -𝑇 (𝑣 ′ , 𝑦 † 𝑗+1 ; 𝜃)|. This allows us to use the joint ensemble for 𝛾 𝑗+1 arising from Data Assumption 10.8 to define the following empirical loss function for the map parameters: L 𝑁 (𝜃) = 2 𝑁 (𝑁 -1) 𝑁 ∑︁ 𝑚,𝑛=1 𝑚̸ =𝑛 |𝑇 (̂︀ 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑦 (𝑚) 𝑗+1 ; 𝜃) -̂︀ 𝑣 (𝑚) 𝑗+1 | - 1 𝑁 (𝑁 -1) 𝑁 ∑︁ 𝑙,𝑚,𝑛=1 𝑙̸ =𝑚̸ =𝑛 |𝑇 (̂︀ 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑦 (𝑙) 𝑗+1 ; 𝜃) -𝑇 (̂︀ 𝑣 (𝑚) 𝑗+1 , ̂︀ 𝑦 (𝑙) 𝑗+1 ; 𝜃)|, where we note that we use a single ensemble to approximate the expectations; see Remark 10.10. We then define the optimal parameters for the map as 𝜃 ⋆ ∈ arg min 𝜃∈Θ L 𝑁 (𝜃). 𝑗+1 , the second term encourages diversity in the map evaluations. That is, the second term is maximized by increasing the difference between all pairwise map evaluations for each observation. ♢ Remark 10.10. The expectations in each term of the loss function involves a tensor product of distributions for the states and observations. These expectation can be estimated from a single ensemble of paired states and observations by permuting the pairings. For instance, (̂︀ 𝑣 (𝑛) 𝑗+1 , ̂︀ 𝑦 (𝑙) 𝑗+1 ) ∼ ̂︀ 𝜋 𝑗+1 ⊗ 𝜅 𝑗+1 for 𝑛 ̸ = 𝑙. ♢ Maximum Likelihood Estimation In this subsection we consider an alternative approach to construct an observationdependent transport. To do so, we first identify a transport map that pushes forward the joint distribution 𝛾 𝑗+1 (𝑦, 𝑣) to the filtering distribution 𝜋 𝑗+1 (𝑣) by composing transport maps that can be estimated using the maximum likelihood approach introduced in Chapter 5; see Remark 5.20 for the connection between KL divergence minimization and maximum-likelihood. Recall from Remark 7.1 that 𝜅 𝑗+1 (𝑦) denotes the marginal of joint distribution 𝛾 𝑗+1 (𝑦, 𝑣). Let 𝜚(𝑣) be a reference density on R 𝑑 , such as a standard Gaussian. The following theorem shows how to construct a transport that pushes forward the joint distribution on the state and observation, 𝛾 𝑗+1 , to the filtering distribution 𝜋 𝑗+1 based on a map that pushes forward any conditional distribution P(𝑣|𝑦) of 𝛾 𝑗+1 to the same reference distribution. Theorem 10.11. Let 𝑆 : R 𝑑 × R 𝑘 → R 𝑑 be a transport map such that 𝑆(𝑦, •) : R 𝑑 → R 𝑑 is invertible for each 𝑦 ∈ R 𝑘 . We denote this inverse by 𝑆(𝑦, •) -1 . If 𝑆(𝑦, •) pushes forward the conditional P(𝑣|𝑦) of 𝛾 𝑗+1 to 𝜚(𝑣) for any 𝑦 ∼ 𝜅 𝑗+1 , then 𝑇 (𝑣, 𝑦, 𝑦 † 𝑗+1 ) := 𝑆(𝑦 † 𝑗+1 , •) -1 ∘ 𝑆(𝑦, 𝑣) (10.16 ) pushes forward 𝛾 𝑗+1 (𝑦, 𝑣) to 𝜋 𝑗+1 (𝑣) for any 𝑦 † 𝑗+1 ∈ R 𝑘 . Proof. The map 𝑆(𝑦, 𝑣) acts on conditional samples of 𝑣|𝑦 for any 𝑦 or equivalently samples from the joint 𝛾 𝑗+1 (𝑦, 𝑣). Given that 𝑆(𝑦, •) pushes forward the conditional P(𝑣|𝑦) of 𝛾 𝑗+1 (𝑦, 𝑣) to 𝜚(𝑣) for any 𝑦 ∈ R 𝑘 , from Theorem 5.14 we have that the inverse map 𝑆(𝑦 † 𝑗+1 , •) -1 pushes forward the conditional P(𝑣|𝑦 † 𝑗+1 ) given by the filtering density 𝜋 𝑗+1 to 𝜚(𝑣). The composed maps gives us a transport pushing forward 𝛾 𝑗+1 to 𝜋 𝑗+1 . Remark 10.12. The map 𝑆 can also be interpreted as a single component of a blocktriangular transport 𝑆 𝐵 (𝑦, 𝑣) = [︃ 𝑦 𝑆(𝑦, 𝑣) ]︃ , (10.17) that pushes forward 𝛾 𝑗+1 (𝑦, 𝑣) = 𝜅 𝑗+1 (𝑣)𝜋 𝑗+1 (𝑣) to a product reference with the form 𝜅 𝑗+1 (𝑦)𝜚(𝑣), i.e., (𝑆 𝐵 ) ♯ 𝛾 𝑗+1 = 𝜅 𝑗+1 𝜚. We refer the reader to the construction and properties of block-triangular maps in Section 5.5.1. ♢ Given the joint ensemble of states and observations, as defined in Data Assumption 10.8, we can define our desired transport map as follows. Using the procedure in Subsection 5.5.2, and noting Remark 5.22 in particular, we identify the map 𝑆(•, •; 𝜃), parameterized by 𝜃 ∈ Θ ⊆ R 𝑝 , and find an optimal parameter 𝜃 ⋆ from joint samples of 𝛾 𝑗+1 by solving the maximum likelihood problem: 𝜃 * = arg max 𝜃 1 𝑁 𝑁 ∑︁ 𝑖=1 log 𝑆(𝑦 𝑖 , •; 𝜃) ♯ 𝜚(𝑣 𝑖 ). (10.18) Using Theorem 10.11 we then define 𝑇 (𝑦, 𝑣, 𝑦 † 𝑗+1 ; 𝜃 ⋆ ) := 𝑆(𝑦 † 𝑗+1 , •; 𝜃 ⋆ ) -1 ∘ 𝑆(𝑦, 𝑣; 𝜃 ⋆ ). We may then consider the following explicit choice for the sample update in (9.3c): 𝑣 (𝑛) 𝑗+1 = 𝑆(𝑦 † 𝑗+1 , •; 𝜃 ⋆ ) -1 ∘ 𝑆(̂︀ 𝑦 (𝑛) 𝑗+1 , ̂︀ 𝑣 (𝑛) 𝑗+1 ; 𝜃 ⋆ ), 𝑛 = 1, . . . , 𝑁. We note that the dependence of the 𝑗 𝑡ℎ particle update on the entire ensemble is through the parameter 𝜃 ⋆ . Remark 10.13. The joint-to-posterior map depends on both the true observation 𝑦 † 𝑗+1 and synthetic observations ̂︀ 𝑦 𝑗+1 at time 𝑗 + 1. For a fixed 𝑦 † 𝑗+1 , the transformation from the forecast to analysis state can be seen as a stochastic transport due to the randomness in the synthetic observations ̂︀ 𝑦 𝑗+1 . In contrast, the transport in Subsection 10.3.1, for given fixed observation 𝑦 † 𝑗+1 , are deterministic maps that push forward ̂︀ 𝜋 𝑗+1 to 𝜋 𝑗+1 . ♢ While the framework above is quite general, in practice we may want to seek transports within a parameterized family of functions with a particular structure. The following result shows that these filters based on composed maps are related to classic ensemble Kalman filters. parameterized by 𝜃 := (𝐴, 𝐵, 𝑐); assume that the optimal value of this parameter is found by solving (10.18) over the space Θ = {𝐴 ≻ 0, 𝐵 ∈ R 𝑑×𝑘 , 𝑐 ∈ R 𝑑 }. Then, the composed map in (10.16 ) has the form of the ensemble Kalman update 𝑇 (𝑦, 𝑣, 𝑦 † 𝑗+1 ) = 𝑣 + Σ ̂︀ 𝑣 𝑗+1 ,̂︀ 𝑦 𝑗+1 Σ -1 ︀ 𝑦 𝑗+1 ,̂︀ 𝑦 𝑗+1 (𝑦 † 𝑗+1 -𝑦), where Σ ̂︀ 𝑣 𝑗+1 ,̂︀ 𝑦 𝑗+1 and Σ ̂︀ 𝑦 𝑗+1 ,̂︀ 𝑦 𝑗+1 denote the cross-covariance of (̂︀ 𝑣 𝑗+1 , ̂︀ 𝑦 𝑗+1 ) and the covariance of ̂︀ 𝑦 𝑗+1 under 𝛾 𝑗+1 , respectively. Proof. From Theorem 5.26, solving the optimization problem over the space of affine maps 𝒮 yields the transformation 𝑆(𝑦, 𝑣; 𝜃 ⋆ ) = Σ -1/2 𝑣|𝑦 (︁ 𝑣 -E[𝑣] -Σ 𝑣𝑦 Σ -1 𝑦𝑦 (𝑦 -E[𝑦]) )︁ , where we have suppressed the dependence of the state and observation in the covariance on time for conciseness. The inverse of the map evaluated at 𝑦 † 𝑗+1 is given by 𝑆(𝑦 † 𝑗+1 , •; 𝜃 ⋆ ) -1 | 𝑣 = Σ 1/2 𝑣|𝑦 𝑣 + E[𝑣] + Σ 𝑣𝑦 Σ -1 𝑦𝑦 (𝑦 † 𝑗+1 -E[𝑦]). Composing these maps cancels out Σ -1/2 𝑣|𝑦 and the constant terms, thus yielding the desired result. Amortized Smoothing In this subsection, we generalize the approach in Subsection 9. 4 .4 by seeking the transport map 𝑇 that explicitly depends on the observation and pushes forward a distribution 𝜋 𝑗 (𝑣 𝑗 ) = P(𝑣 𝑗 |𝑌 † 𝑗 ) for the state at time 𝑗 to the smoothing distribution P(𝑣 𝑗 |𝑌 † 𝑗 , 𝑦 † 𝑗+1 ) given a new observation 𝑦 † 𝑗+1 at time 𝑗 + 1. That is, 𝑇 (𝑣 𝑗 , 𝑦 † 𝑗+1 ; 𝜃) ∼ P(𝑣 𝑗 |𝑌 𝑗 , 𝑦 † 𝑗+1 ) 𝑣 𝑗 ∼ 𝜋 𝑗 . (10.19) We will describe a similar methodology to subsection 10.3.1 that identifies the map by minimizing the energy distance. The main difference is that we will identify the transport that matches the smoothing distribution for the state 𝜋 OPF 𝑗 at time 𝑗 rather than the filtering distribution 𝜋 𝑗+1 . To identify the transport 𝑇 parameterized by 𝜃 ∈ R 𝑝 , we introduce the objective function: J(𝜃) = E 𝑦 † 𝑗+1 ∼𝜅 𝑗+1 [︁ D E (𝑇 (•; 𝑦 † 𝑗+1 , 𝜃) ♯ 𝜋 𝑗 , P(•|𝑌 𝑗 , 𝑦 † 𝑗+1 )) ]︁ . As in Subsection 10.3.1, we can implement this loss as long as we can sample from the joint distribution of (𝑣 𝑗 , ̂︀ 𝑦 𝑗+1 ) for the state at time 𝑗 and the observation at time 𝑗 + 1. Given a collection of analysis samples {𝑣 Then, we decompose the objective function J for the map as follows: J(𝜃) = 𝑐 + 2E 𝑦 † 𝑗+1 ∼𝜅 𝑗+1 E (𝑣,𝑣 ′ )∼𝜋 𝑗 ⊗P(•|𝑌 † 𝑗 ,𝑦 † 𝑗+1 ) |𝑇 (𝑣, 𝑦 † 𝑗+1 ; 𝜃) -𝑣 ′ | -E 𝑦 † 𝑗+1 ∼𝜅 𝑗+1 E (𝑣,𝑣 ′ )∼𝜋 𝑗 ⊗𝜋 𝑗 |𝑇 (𝑣, 𝑦 † 𝑗+1 ; 𝜃) -𝑇 (𝑣 ′ , 𝑦 † 𝑗+1 ; 𝜃)|, where 𝑐 is a constant that is independent of 𝑇 . Hence, we can identify the optimal map by minimizing the last two terms in the objective. The empirical loss minimization problem for the map parameters given 𝑁 samples {(𝑣 (𝑛) 𝑗 , ̂︀ 𝑦 (𝑛) 𝑗+1 )} 𝑁 𝑛=1 then has the form sum 𝑁 = 2 𝑁 ∑︁ 𝑚,𝑛=1 𝑚̸ =𝑛 |𝑇 (𝑣 (𝑛) 𝑗 , ̂︀ 𝑦 (𝑚) 𝑗+1 ; 𝜃) -𝑣 (𝑚) 𝑗 | - 𝑁 ∑︁ 𝑙,𝑚,𝑛=1 𝑙̸ =𝑚̸ =𝑛 |𝑇 (𝑣 (𝑛) 𝑗 , ̂︀ 𝑦 (𝑙) 𝑗+1 ; 𝜃) -𝑇 (𝑣 (𝑚) 𝑗 , ̂︀ 𝑦 (𝑙) 𝑗+1 ; 𝜃)|, L 𝑁 (𝜃) = 1 𝑁 (𝑁 -1) sum 𝑁 , 𝜃 ⋆ ∈ arg min 𝜃∈Θ L 𝑁 (𝜃). After identifying the map parameters, we generate the ensemble from the lag-1 smoothing distribution by evaluating the map 𝑇 (𝑣 (𝑛) 𝑗 , 𝑦 † 𝑗+1 ; 𝜃 ⋆ ), 𝑣 (𝑛) 𝑗 ∼ 𝜋 𝑗 with the true observation at time 𝑗 + 1. Remark 10.15. The procedure in Subsection 9.2.4 for sampling from the lag-1 smoothing distribution requires a closed-form expression for the likelihood weights, because it uses the optimal particle filter (6.3.7) to approximately sample P(𝑣 𝑗 |𝑌 † 𝑗+1 ); thus, it is restricted to linear observation operators. On the other hand, the approach in this subsection can be implemented for general non-linear observation operators, as long as we can sample from the dynamics and observation model in (10.20) . ♢ Strictly Proper Scoring Rules In this section we consider minimizing a scoring rule between the true and filtering distributions in expectation over the data. This is a special case of minimizing an expected scoring rule between the state and the filtering distribution, without amortization over the data, that is introduced in Section 9.1.2. We again recall the notation established in Remark 7.1. Employing this notation, we make the following assumption about available data. Data Assumption 10.16. For some given 𝑗 ∈ {1, . . . , 𝐽} we have access to data set {(𝑉 † 𝑗 ) (𝑛) , (𝑌 † 𝑗 ) (𝑛) } 𝑛∈{1,...,𝑁 } generated as a realization of (9.1). The following theorem provides the basis for learning filters using strictly proper scoring rules from Definition 11.42. Theorem 10.17. Consider a strictly proper scoring rule S(•, •). Let 𝜋 𝑗 (𝑣 † 𝑗 ) = P(𝑣 † 𝑗 |𝑌 † 𝑗 ) be the filtering distribution on R 𝑑𝑣 and let 𝑞(•; 𝑌 † 𝑗 ) denote any distribution on R 𝑑𝑣 parameterized by 𝑌 † 𝑗 . Then, with E (𝑣 † 𝑗 ,𝑌 † 𝑗 ) denoting expectation under the dynamics/observation model (6.1), (6.2), we have .22) Proof. From the definition of a strictly proper scoring rule, we have that E (𝑣 † 𝑗 ,𝑌 † 𝑗 ) [︁ S(𝑞(•; 𝑌 † 𝑗 ), 𝑣 † 𝑗 ) ]︁ ≥ E (𝑣 † 𝑗 ,𝑌 † 𝑗 ) [︁ S(𝜋 𝑗 , 𝑣 † 𝑗 ) ]︁ . ( 10 E 𝑣 𝑗 ∼𝜋 𝑗 [︁ S(𝑞(•; 𝑌 † 𝑗 ), 𝑣 𝑗 ) ]︁ ≥ E 𝑣 𝑗 ∼𝜋 𝑗 [︁ S(𝜋 𝑗 , 𝑣 𝑗 ) ]︁ . Taking the expectation with respect to 𝑌 † 𝑗 , of both sides of this identity, we obtain the desired result. Thus, having only the information 𝑌 † 𝑗 , the filtering distribution minimizes the expected score. Hence this expected score may be used as the basis for learning algorithms designed to match the true filter, using only samples from the joint distribution of 𝑣 † 𝑗 and 𝑌 † 𝑗 , denoted by 𝛾 𝑗 (𝑣, 𝑦). Certain scoring rules are amenable to sample-based implementation, which will be convenient in what follows. We now define a learning problem for the filtering distributions. As in (7.7), we parameterize a set of approximate filters 𝑞 of the form: 𝑞 𝑗+1 (𝜃) = A 𝜃 (P𝑞 𝑗 (𝜃); 𝑦 † 𝑗+1 ), 𝑞 0 = 𝜋 0 , where A 𝜃 denotes an analysis map, parameterized by 𝜃 ∈ Θ; this map depends on the approximate forecast distribution and the observation 𝑦 † 𝑗+1 and computes the filtering distribution. The following result shows that if set of parameters Θ is large enough to represent the true analysis map in (6.8), then the learning problem that minimizes the scoring rule will recover the true filtering distribution. Corollary 10.18. Consider the distribution 𝑞 𝑗 evolving under the 𝜃-parameterized algorithm (7.7) and define the optimization problem J(𝜃) = E (𝑣 † 𝑗 ,𝑌 † 𝑗 ) [︁ S(A 𝜃 (P𝑞 𝑗-1 (𝜃); 𝑦 † 𝑗 ), 𝑣 † 𝑗 ) ]︁ , 𝜃 opt ∈ arg min 𝜃 J(𝜃). Then, if there is some 𝜃 ∈ Θ for which A 𝜃 = A and 𝑗 > 0, we have that J(𝜃 opt ) = E (𝑣 † 𝑗 ,𝑌 † 𝑗 ) [︁ S(P(𝑣 † 𝑗 |𝑌 † 𝑗 ), 𝑣 † 𝑗 ) ]︁ . Proof. This follows from Theorem 10.17 by substituting (7.7) for 𝑞(𝑌 † 𝑗 ). In practice, the parameterized models we learn will not be rich enough to recover the exact analysis map, and good approximations are sought. This requires empirical minimization, as we now discuss. Remark 10.19. Empirical minimization algorithms based on J(𝜃) can be identified and used in practice. In particular, we may approximate the expectation E (𝑣 † 𝑗 ,𝑌 † 𝑗 ) by obtaining pairs {(𝑉 † 𝑗 ) (𝑛) , (𝑌 † 𝑗 ) (𝑛) } 𝑁 𝑛=1 from the dynamical system (9.1) as per Data Assumption 10. 16 . ♢ Remark 10.20. A modification of the preceding methodology can be used to learn a parameterized prediction step P 𝜗 rather than to learn a parameterized analysis step. This can be useful, for example, to determine model error, as in Subsection 6.5, or to determine the parameters of a time-series forecasting model, as in Subsection 14.2.1. In particular in the case of time-series forecasting models, the approach we now describe rewards probabilistic performance, in contrast to the deterministic cost functions described in Subsection 14.2.1. Consider an approximate filtering distribution ̂︀ 𝑞(•; 𝑌 † 𝑗 ) parameterized by 𝑌 † 𝑗 . Then, by minor modification of Theorem 10.17, we have a similar result for prediction: E (𝑣 † 𝑗+1 ,𝑌 † 𝑗 ) [︁ S(̂︀ 𝑞(•; 𝑌 † 𝑗 ), 𝑣 † 𝑗+1 ) ]︁ ≥ E (𝑣 † 𝑗+1 ,𝑌 † 𝑗 ) [︁ S(̂︀ 𝜋 𝑗+1 , 𝑣 † 𝑗+1 ) ]︁ , where ̂︀ 𝜋 𝑗+1 = P(𝑣 † 𝑗+1 |𝑌 † 𝑗 ). Using this result, we can define a learning problem for the prediction step by considering a set of approximate distributions of the form: ︀ 𝑞 𝑗+1 (𝜗) = P 𝜗 A(̂︀ 𝑞 𝑗 (𝜗); 𝑦 † 𝑗 ), 𝑞 0 = 𝜋 0 . Part III Fundamentals Chapter 11 Metrics, Divergences And Scoring Rules In this chapter we define various \"distance-like\" ways to quantify closeness between probability measures. We also introduce scoring rules which quantify closeness of a probability measure to a point, and deterministic scoring rules, which quantify the distance between two points. We work with probability measures on R 𝑑 , denoted 𝒫(R 𝑑 ). To be consistent with the rest of the notes, the presentation focuses mainly on probability density functions; but the definitions can all be extended to general probability measures. Indeed we will employ Dirac measures in several instances. We start with metrics in Section 11.1; we continue with divergences in Section 11.2; and finally we consider scoring rules in Section 11.3. Throughout the chapter we will make connections between metrics, divergences and scoring rules; these connections are summarized in Figure 11 .1. Our motivation for studying this topic is twofold. First, some metrics and divergences provide a language in which to state theoretical results about inverse problems and data assimilation. Second, others are used to define loss functions for probabilistic machine learning models based on data, and to design and assess the convergence of machine learning algorithms. As an example of the first motivation, the Hellinger distance is useful for stating stability results for measures; see, for instance, Theorems 1.18 and 1.27. As an example of the second, the energy distance is a metric that is used as a loss function for tasks such as learning probabilistic filters; see Section 9.3. The Kullback-Leibler divergence is useful both for stating theoretical results for inverse problems and data assimilation (see discussion in bibliography Section 1.6) and for defining loss functions in machine learning (see Chapter 4). Scoring rules have been used traditionally for evaluating probabilistic forecasts against samples and hold potential to define objectives for machine learning tasks. When comparing two probability density functions in this chapter we will generically denote them by 𝜌 and 𝜚. When considering scalar-valued random variables we denote the cumulative density functions associated with these two probability density functions by 𝐹 𝜌 and 𝐹 𝜚 . Metrics Divergences Expected scoring rules Metrics Metrics On The Space Of Probability Measures A metric between probability measures is a function D : 𝒫(R 𝑑 )×𝒫(R 𝑑 ) → R that satisfies the following four properties for all 𝜌, 𝜚 ∈ 𝒫(R 𝑑 ): 1. Non-negative: D(𝜌, 𝜚) ≥ 0. A metric is also sometimes referred to as a distance. Total Variation And Hellinger Metrics Definition 11.1. The total variation distance between two probability density functions 𝜌 and 𝜚 is defined by foot_10 D TV (𝜌, 𝜚) := 1 2 ∫︁ |𝜌(𝑢) -𝜚(𝑢)| 𝑑𝑢 = 1 2 ‖𝜌 -𝜚‖ 𝐿 1 . The Hellinger distance between two probability density functions 𝜌 and 𝜚 is defined by D H (𝜌, 𝜚) := (︂ 1 2 ∫︁ ⃒ ⃒ ⃒ √︁ 𝜌(𝑢) - √︁ 𝜚(𝑢) ⃒ ⃒ ⃒ 2 𝑑𝑢 )︂ 1/2 = 1 √ 2 ‖ √ 𝜌 - √ 𝜚‖ 𝐿 2 . ♢ Note also that, because all densities have norm one in 𝐿 1 , D H (𝜌, 𝜚) 2 = 1 - ⟨︀√ 𝜌, √ 𝜚 ⟩︀ 𝐿 2 . (11.1) We now establish bounds between the Hellinger and total variation distance. Lemma 11.2. For any probability density functions 𝜌 and 𝜚, the following holds. • (i) The total variation and Hellinger metrics are uniformly bounded; indeed 0 ≤ D TV (𝜌, 𝜚) ≤ 1, 0 ≤ D H (𝜌, 𝜚) ≤ 1; • (ii) The total variation and Hellinger metrics bound one another; indeed 1 √ 2 D TV (𝜌, 𝜚) ≤ D H (𝜌, 𝜚) ≤ √︁ D TV (𝜌, 𝜚). Proof. For part (i) we note that the lower bounds follow immediately from the definitions, so we only need to prove the upper bounds. For total variation distance D TV (𝜌, 𝜚) = 1 2 ∫︁ |𝜌(𝑢) -𝜚(𝑢)| 𝑑𝑢 ≤ 1 2 ∫︁ 𝜌(𝑢) 𝑑𝑢 + 1 2 ∫︁ 𝜚(𝑢) 𝑑𝑢 = 1. For the Hellinger distance the upper bound follows from (11.1) . For the lower bound in part (ii) we use the Cauchy-Schwarz inequality D TV (𝜌, 𝜚) = 1 2 ∫︁ ⃒ ⃒ ⃒ √︁ 𝜌(𝑢) - √︁ 𝜚(𝑢) ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ √︁ 𝜌(𝑢) + √︁ 𝜚(𝑢) ⃒ ⃒ ⃒ 𝑑𝑢 ≤ (︂ 1 2 ∫︁ ⃒ ⃒ ⃒ √︁ 𝜌(𝑢) - √︁ 𝜚(𝑢) ⃒ ⃒ ⃒ 2 𝑑𝑢 )︂ 1/2 (︂ 1 2 ∫︁ ⃒ ⃒ ⃒ √︁ 𝜌(𝑢) + √︁ 𝜚(𝑢) ⃒ ⃒ ⃒ 2 𝑑𝑢 )︂ 1/2 ≤ D H (𝜌, 𝜚) (︂ 1 2 ∫︁ (︀ 2𝜌(𝑢) + 2𝜚(𝑢) )︀ 𝑑𝑢 )︂ 1/2 = √ 2D H (𝜌, 𝜚). For the upper bound in (ii) first notice that | √︁ 𝜌(𝑢) - √︁ 𝜚(𝑢)| ≤ | √︁ 𝜌(𝑢) + √︁ 𝜚(𝑢)| since √︀ 𝜌(𝑢), √︀ 𝜚(𝑢) ≥ 0. Thus we have D H (𝜌, 𝜚) = (︂ 1 2 ∫︁ ⃒ ⃒ ⃒ √︁ 𝜌(𝑢) - √︁ 𝜚(𝑢) ⃒ ⃒ ⃒ 2 𝑑𝑢 )︂ 1/2 ≤ (︂ 1 2 ∫︁ ⃒ ⃒ ⃒ √︁ 𝜌(𝑢) - √︁ 𝜚(𝑢) ⃒ ⃒ ⃒ ⃒ ⃒ ⃒ √︁ 𝜌(𝑢) + √︁ 𝜚(𝑢) ⃒ ⃒ ⃒ 𝑑𝑢 )︂ 1/2 ≤ (︂ 1 2 ∫︁ ⃒ ⃒ 𝜌(𝑢) -𝜚(𝑢) ⃒ ⃒ 𝑑𝑢 )︂ 1/2 = √︁ D TV (𝜌, 𝜚). In addition to relating closeness of densities to closeness of expectations with respect to different densities, the following lemma also provides a useful characterization of the total variation distance. Lemma 11.3. Let 𝑓 : R 𝑑 → R. If two densities are close in total variation or in Hellinger distance, expectations computed with respect to both densities are also close. • (i) Let 𝑓 be a function such that |𝑓 | ∞ := sup 𝑢∈R 𝑑 |𝑓 (𝑢)| < ∞. It holds that ⃒ ⃒ E 𝜌 [𝑓 ] -E 𝜚 [𝑓 ] ⃒ ⃒ ≤ 2|𝑓 | ∞ D TV (𝜌, 𝜚). In fact, the following variational characterization of the total variation distance holds: D TV (𝜌, 𝜚) = 1 2 sup |𝑓 |∞≤1 ⃒ ⃒ E 𝜌 [𝑓 ] -E 𝜚 [𝑓 ] ⃒ ⃒ . ( 11.2) • (ii) Let 𝑓 be a function such that 𝑓 2 := (︀ E 𝜌 [|𝑓 | 2 ] + E 𝜚 [|𝑓 | 2 ] )︀ 1/2 < ∞. It holds that ⃒ ⃒ E 𝜌 [𝑓 ] -E 𝜚 [𝑓 ] ⃒ ⃒ ≤ 2𝑓 2 D H (𝜌, 𝜚). Proof. For part (i) we start by noting that ⃒ ⃒ E 𝜌 [𝑓 ] -E 𝜚 [𝑓 ] ⃒ ⃒ = ⃒ ⃒ ⃒ ∫︁ 𝑓 (𝑢) (︀ 𝜌(𝑢) -𝜚(𝑢) )︀ 𝑑𝑢 ⃒ ⃒ ⃒ ≤ 2|𝑓 | ∞ • 1 2 ∫︁ |𝜌(𝑢) -𝜚(𝑢)| 𝑑𝑢 = 2|𝑓 | ∞ D TV (𝜌, 𝜚). For any 𝑓 with |𝑓 | ∞ = 1 we obtain D TV (𝜌, 𝜚) ≥ 1 2 ⃒ ⃒ E 𝜌 [𝑓 ] -E 𝜚 [𝑓 ] ⃒ ⃒ . We complete the proof of part (i) by exhibiting a choice of 𝑓 with |𝑓 | ∞ = 1 that achieves equality. To this end we choose 𝑓 (𝑢) := sign (︁ 𝜌(𝑢) -𝜚(𝑢) )︁ , so that 𝑓 (𝑢) (︁ 𝜌(𝑢) -𝜚(𝑢) )︁ = |𝜌(𝑢) -𝜚(𝑢)|. Then |𝑓 | ∞ = 1, and D TV (𝜌, 𝜚) = 1 2 ∫︁ |𝜌(𝑢) -𝜚(𝑢)| 𝑑𝑢 = 1 2 ∫︁ 𝑓 (𝑢) (︁ 𝜌(𝑢) -𝜚(𝑢) )︁ 𝑑𝑢 = 1 2 ⃒ ⃒ E 𝜌 [𝑓 ] -E 𝜚 [𝑓 ] ⃒ ⃒ . For part (ii) we use the Cauchy-Schwarz inequality to show that ⃒ ⃒ E 𝜌 [𝑓 ] -E 𝜚 [𝑓 ] ⃒ ⃒ = ⃒ ⃒ ⃒ ⃒ ∫︁ 𝑓 (𝑢) (︁√︁ 𝜌(𝑢) - √︁ 𝜚(𝑢) )︁(︁√︁ 𝜌(𝑢) + √︁ 𝜚(𝑢) )︁ 𝑑𝑢 ⃒ ⃒ ⃒ ⃒ ≤ (︂ 1 2 ∫︁ ⃒ ⃒ ⃒ √︁ 𝜌(𝑢) - √︁ 𝜚(𝑢) ⃒ ⃒ ⃒ 2 𝑑𝑢 )︂ 1/2 (︂ 2 ∫︁ |𝑓 (𝑢)| 2 ⃒ ⃒ ⃒ √︁ 𝜌(𝑢) + √︁ 𝜚(𝑢) ⃒ ⃒ ⃒ 2 𝑑𝑢 )︂ 1/2 ≤ D H (𝜌, 𝜚) (︁ 4 ∫︁ |𝑓 (𝑢)| 2 (︀ 𝜌(𝑢) + 𝜚(𝑢) )︀ 𝑑𝑢 )︁ 1/2 = 2𝑓 2 D H (𝜌, 𝜚). Example 11.4. Consider the two Gaussians 𝜌 = 𝒩 (𝑚 𝜌 , Σ 𝜌 ) and 𝜚 = 𝒩 (𝑚 𝜚 , Σ 𝜚 ). If Σ 𝜚 = 0, so that 𝜚 is a Dirac measure at 𝑚 𝜚 , and if Σ 𝜌 ̸ = 0, then D TV (𝜌, 𝜚) = 1. This fact follows from the characterization (11.2) and the choice of test functions 𝑓 𝜖 = 1 -21 𝐵(𝑚𝜚,𝜖) . Then E 𝜚 [𝑓 𝜖 ] = -1 and E 𝜌 [𝑓 𝜖 ] → 1 as 𝜖 → 0. Taking the limit 𝜖 → 0 delivers the desired supremum. If Σ 𝜚 = Σ 𝜌 = 0 then D TV (𝜌, 𝜚) = 0 if 𝑚 𝜌 = 𝑚 𝜌 ; however if 𝑚 𝜌 ̸ = 𝑚 𝜌 then D TV (𝜌, 𝜚) = 1 a fact which can be seen by again using characterization (11.2) and now making choice of test function equal to the difference of two indicator functions with disjoint support, and supported on each of 𝑚 𝜌 and 𝑚 𝜚 . ♢ Remark 11.5. Consider the setting where 𝜌 and 𝜚 are probability density functions. Then the Hellinger distance between 𝜌 and 𝜚 is equal to one if and only if the measures have disjoint supports. This follows from (11.1): the Hellinger distance is 1 if and only if √ 𝜌 √ 𝜚 = 0 in a Lebesgue a.e. sense, since the square-roots of the densities are non-negative functions; this implies that their supports must be disjoint. A similar result holds for total variation distance. Care is needed in stating such results between measures that include atoms, as this makes arguments which apply Lebesgue a.e. redundant; the Example 11.4 demonstrates this. ♢ 11.1.3 Transportation Metrics We start by defining the notions of coupling and transport map. Definition 11.6. Let 𝜌 and 𝜚 be two probability densities on R 𝑑 . A coupling of 𝜌 and 𝜚 is a probability density function 𝜋 on R 𝑑 × R 𝑑 with the property that ∫︁ R 𝑑 𝜋(𝑧, 𝑢) 𝑑𝑢 = 𝜌(𝑧), ∫︁ R 𝑑 𝜋(𝑧, 𝑢) 𝑑𝑧 = 𝜚(𝑢); thus the 𝑧 and 𝑢 marginals of 𝜋 deliver 𝜌 and 𝜚 respectively. We denote the set of all such couplings by Π 𝜌,𝜚 . ♢ Definition 11.7. Let 𝜌 and 𝜚 be two probability densities on R 𝑑 . A transport map 𝑔 : R 𝑑 → R 𝑑 between 𝜌 and 𝜚 is a map with the property that 𝜚 = 𝑔 ♯ 𝜌. ♢ We now link these two concepts through two distinct formulations of optimal transport. Kantorovich Formulation Given a cost function c : R 𝑑 × R 𝑑 → R + we define the Kantorovich formulation of the optimal transport problem as follows: 𝜋 ⋆ = arginf 𝜋∈Π𝜌,𝜚 ∫︁ R 𝑑 ×R 𝑑 c(𝑧, 𝑢)𝜋(𝑧, 𝑢) 𝑑𝑧𝑑𝑢. ( 11.3) The reason for the \"optimal\" terminology is clear from the infimization. The connection of optimal transport to transport maps on R 𝑑 is made clear in the Monge formulation of optimal transport which we now introduce. Monge Formulation In this formulation we explicitly link 𝜚 with a pushforward of 𝜌 through a transport map 𝑔 on R 𝑑 . We do this by identifying the map which solves the following minimization problem: 𝑔 ⋆ = arginf 𝑔:𝑔 ♯ 𝜌=𝜚 ∫︁ R 𝑑 c (︀ 𝑧, 𝑔(𝑧) )︀ 𝜌(𝑧) 𝑑𝑧. (11.4) Under certain smoothness assumptions, the Kantorovich formulation has solution within the Monge class. That is, the optimal coupling is constructed using the pushforward of a transport map: 𝜋 ⋆ (𝑧, 𝑢) = 𝛿 (︀ 𝑢 -𝑔 ⋆ (𝑧) )︀ 𝜌(𝑧). ( 11.5) See the bibliography Section 11.4 for detailed discussion of this topic. Metrics From Optimal Transport The notion of optimal transport may be used to define families of probability metrics by choosing the cost function in the Kantorovich formulation of optimal transport to be a power of a metric on R 𝑑 . Definition 11.8. Given a metric d(•, •) on R 𝑑 and 𝑝 ≥ 1, the Wasserstein-𝑝 distance between two probability density functions 𝜌 and 𝜚 is defined by 𝑊 𝑝 (𝜌, 𝜚) := (︂ inf 𝜋∈Π𝜌,𝜚 ∫︁ R 𝑑 ×R 𝑑 d(𝑧, 𝑢) 𝑝 𝜋(𝑧, 𝑢) 𝑑𝑧𝑑𝑢 )︂ 1/𝑝 . ( 11.6) ♢ This probability metric has two key aspects: first, it is a metric on 𝒫(R 𝑑 ) that relates to an underlying metric on R 𝑑 ; and second, it allows for a meaningful distance to be calculated between measures which are mutually singular. 𝑊 2 (𝜌, 𝜚) 2 = |𝑚 𝜌 -𝑚 𝜚 | 2 + Tr (︁ Σ 𝜌 + Σ 𝜚 -2 (︀ Σ 1/2 𝜌 Σ 𝜚 Σ 1/2 𝜌 )︀ 1/2 )︁ . In particular, if 𝜚 is a Dirac measure at 𝑚 𝜚 then 𝑊 2 (𝜌, 𝜚) 2 = |𝑚 𝜌 -𝑚 𝜚 | 2 + Tr (︀ Σ 𝜌 )︀ . Thus, in the Wasserstein-2 distance, Gaussian 𝜌 is close to a Dirac at 𝑚 𝜚 if the mean of the Gaussian 𝜌 is close to 𝑚 𝜚 and if its covariance is small. In contrast, Example 11.4 shows that the total variation distance between Gaussian 𝜌 and a Dirac at 𝑚 𝜚 is maximal, and equal to 1, unless the two measures coincide (𝑚 𝜌 = 𝑚 𝜚 , Σ 𝜌 = 0) in which case it is 0. Notice also that if 𝜌 and 𝜚 are Diracs at 𝑚 𝜌 and 𝑚 𝜚 , then 𝑊 2 (𝜌, 𝜚) = |𝑚 𝜌 -𝑚 𝜚 |; thus closeness of the mass locations 𝑚 𝜌 and 𝑚 𝜚 in Euclidean space translates into closeness of 𝜌 and 𝜚 in Wasserstein distance. Again, Example 11.4 shows that two Diracs are maximally far apart in the total variation distance unless they coincide. ♢ One-Dimensional Setting Optimal transport metrics have as an advantage that they directly link a metric on R 𝑑 to the metric on 𝒫(R 𝑑 ). However it is an implicit definition, via an optimization. Notwithstanding the explicit Gaussian formula in Example 11.9 and the Monge formulation via a map on R 𝑑 , it can be hard to develop intuition about the metric in general. In one dimension, however, there are a number of explicit formulae for the optimal transport metric which are insightful and build intuition; we describe them here. 𝑊 𝑝 (𝜌, 𝜚) 𝑝 = ∫︁ 1 0 |𝐹 -1 𝜌 (𝑞) -𝐹 -1 𝜚 (𝑞)| 𝑝 𝑑𝑞. Proof. The map 𝑔(𝑧) = 𝐹 -1 𝜚 ∘ 𝐹 𝜌 (𝑧) pushes forward 𝜌 to 𝜚, and it can be shown to lead to the optimal Kantorovich coupling in the one-dimensional setting; this follows from the Monge formulation as detailed in the bibliography Section 11.4. By substituting this map in the objective and performing the change of variables 𝑞 = 𝐹 𝜌 (𝑧), we have 𝑊 𝑝 (𝜌, 𝜚) 𝑝 = inf 𝑔 ♯ 𝜌=𝜚 ∫︁ R |𝑧 -𝑔(𝑧)| 𝑝 𝜌(𝑧) 𝑑𝑧 = ∫︁ R |𝑧 -𝐹 -1 𝜚 ∘ 𝐹 𝜌 (𝑧)| 𝑝 𝜌(𝑧) 𝑑𝑧 = ∫︁ 1 0 |𝐹 -1 𝜌 (𝑞) -𝐹 -1 𝜚 (𝑞)| 𝑝 𝑑𝑞. The Wasserstein-𝑝 distance for 𝑝 = 1 has some additional computational advantages. It is an IPM, as discussed in Example 11.15. Furthermore, the closed-form expression in one dimension can be rewritten without requiring the inverse of cumulative distribution functions: Lemma 11.11. In the setting of Lemma 11.10, the Wasserstein-1 distance can be expressed as 𝑊 1 (𝜌, 𝜚) = ∫︁ 1 0 |𝐹 -1 𝜌 (𝑞) -𝐹 -1 𝜚 (𝑞)| 𝑑𝑞 = ∫︁ R |𝐹 𝜌 (𝑧) -𝐹 𝜚 (𝑧)| 𝑑𝑧. Proof. The proof of this result is shown using Figure 11 .2. It illustrates that integrating the difference between the cumulative distribution functions vertically (left) is equivalent to the horizontal integration (right). Integral Probability Metrics We now build on the variational characterization (11.2) of the total variation distance to describe a wide class of probability metrics. Rather than taking the supremum over functions with a bound on their maximum value, we consider taking supremum over other classes of functions. To this end, let ℱ be a set of real-valued functions 𝑓 : R 𝑑 → R known as discriminators. These are used to define a metric as follows: Definition 11.12. Let ℱ be a set of discriminator functions. An integral probability metric (IPM) is defined by setting, for measures 𝜌, 𝜚, D ℱ (𝜌, 𝜚) := sup 𝑓 ∈ℱ ⃒ ⃒ E 𝜌 [𝑓 ] -E 𝜚 [𝑓 ] ⃒ ⃒ . ( 11.7) ♢ It is natural to ask, then, what conditions on ℱ lead to a metric? Citation to proof of the following may be found in the bibliography Section 11.4. Lemma 11.13. Function D ℱ : 𝒫(R 𝑑 ) × 𝒫(R 𝑑 ) → R given by (11.7) defines a metric if and only if ℱ separates points on the space of probability measures: for any two different probability measures 𝜌, 𝜚 there is an element in ℱ with different expectations under 𝜌 and 𝜚. Example 11.14. The total variation distance, given in (11.2), corresponds to choosing ℱ to be the scaled unit ball of functions bounded by 1  2 in 𝐿 ∞ . ♢ Example 11.15. A second important example of an IPM is the Wasserstein-1 distance. This metric is reproduced as an IPM by choosing ℱ to be the space of Lipschitz continuous functions with Lipschitz constant less than or equal to one; see the bibliography Section 11.4. ♢ Example 11.16. A third example of an IPM arises when ℱ is the unit ball of an RKHS, a concept that we introduce in the following subsection. The resulting metric, known as the maximum mean discrepancy, will also be described in the following subsection. ♢ Maximum Mean Discrepancy And Energy Distance Recall first the definition of a kernel: Definition 11.17. Let 𝐷 ⊆ R 𝑑 . A kernel is a function 𝑐 : 𝐷 × 𝐷 → R. It is called non- negative if 𝑐 : 𝐷 × 𝐷 → R + . It is symmetric if 𝑐(𝑢, 𝑢 ′ ) = 𝑐(𝑢 ′ , 𝑢) for all (𝑢, 𝑢 ′ ) ∈ 𝐷 × 𝐷. It is non-negative definite if, for all 𝑁 ∈ N, all {𝑢 (𝑖) } 𝑁 𝑖=1 ⊂ 𝐷 and all 𝑒 ∈ R 𝑁 , 𝑁 ∑︁ 𝑖=1 𝑁 ∑︁ 𝑗=1 𝑐 (︀ 𝑢 (𝑖) , 𝑢 (𝑗) )︀ 𝑒 𝑖 𝑒 𝑗 ≥ 0. If equality implies that 𝑒 = 0, the kernel is called positive definite. ♢ Definition 11.18. Let 𝑐 : R 𝑑 × R 𝑑 → R + be a symmetric and non-negative kernel. The maximum mean discrepancy (MMD) with kernel 𝑐 between two probability density functions 𝜌 and 𝜚 is defined by D 2 MMD (𝜌, 𝜚) := E (𝑢,𝑢 ′ )∼𝜌⊗𝜌 [︀ 𝑐(𝑢, 𝑢 ′ ) ]︀ + E (𝑣,𝑣 ′ )∼𝜚⊗𝜚 [︀ 𝑐(𝑣, 𝑣 ′ ) ]︀ -2E (𝑢,𝑣)∼𝜌⊗𝜚 [︀ 𝑐(𝑢, 𝑣) ]︀ . ♢ We will now show that MMD is an example of an IPM. To that end, we recall first the notion of an RKHS: Definition 11.19. A Hilbert space ℋ of functions 𝑓 : 𝐷 ⊆ R 𝑑 → R is called a reproducing kernel Hilbert space (RKHS) if there is a function 𝑐 : 𝐷 × 𝐷 → R, with 𝑐(𝑢, •) ∈ ℋ for all 𝑢 ∈ 𝐷, and with the property that, for all 𝑓 ∈ ℋ and 𝑢 ∈ 𝐷, 𝑓 (𝑢) = ⟨𝑓, 𝑐(𝑢, •)⟩ ℋ . (11.8) This is known as the reproducing property and the function 𝑐 is called the reproducing kernel. ♢ An important classical result, the Moore-Aronszajn theorem, ensures that given a symmetric and positive definite kernel 𝑐 : 𝐷 × 𝐷 → R, there is a unique Hilbert space of functions on 𝐷 for which 𝑐 is a reproducing kernel. Citation of this result can be found in the bibliography Section 11.4. Lemma 11.20. Let 𝑐 : R 𝑑 × R 𝑑 → R + be a symmetric, non-negative, and positive definite kernel. Let ℋ be the RKHS with reproducing kernel 𝑐. Define ℱ = {𝑓 ∈ ℋ : ‖𝑓 ‖ ℋ ≤ 1}. Then the MMD with kernel 𝑐 is an IPM with set of discriminator functions ℱ. Proof. Let 𝑚 𝜌 (•) = E 𝑢∼𝜌 [𝑐(𝑢, •)] denote the mean embedding of a distribution 𝜌. By the reproducing property (11.8) for functions 𝑓 in RKHS ℋ we have E 𝜌 [𝑓 ] = ∫︁ 𝑓 (𝑢)𝜌(𝑢) 𝑑𝑢 = ∫︁ ⟨𝑓, 𝑐(𝑢, •)⟩ ℋ 𝜌(𝑢) 𝑑𝑢 = ⟨ 𝑓, ∫︁ 𝑐(𝑢, •)𝜌(𝑢) 𝑑𝑢 ⟩ ℋ = ⟨𝑓, 𝑚 𝜌 ⟩ ℋ . Using the definition of ℱ, and definition of the Hilbert space norm through duality, ‖𝑔‖ ℋ = sup 𝑓 ∈ℱ ⟨𝑓, 𝑔⟩ ℋ , we have sup 𝑓 ∈ℱ ⃒ ⃒ E 𝜌 [𝑓 ] -E 𝜚 [𝑓 ] ⃒ ⃒ = sup 𝑓 ∈ℱ ⃒ ⃒ ⟨𝑓, 𝑚 𝜌 -𝑚 𝜚 ⟩ ℋ ⃒ ⃒ = ‖𝑚 𝜌 -𝑚 𝜚 ‖ ℋ = (︁ ⟨𝑚 𝜌 , 𝑚 𝜌 ⟩ ℋ -2⟨𝑚 𝜌 , 𝑚 𝜚 ⟩ ℋ + ⟨𝑚 𝜚 , 𝑚 𝜚 ⟩ ℋ )︁ 1/2 . The right-hand side is identical to D MMD (𝜌, 𝜚). To see this, we can apply for each term the reproducing property to compute the inner product of the mean embedding. In particular we have ⟨𝑚 𝜌 , 𝑚 𝜚 ⟩ ℋ = ⟨ E 𝑢∼𝜌 [𝑐(𝑢, •)], E 𝑢 ′ ∼𝜚 [𝑐(𝑢 ′ , •)] ⟩ ℋ = E (𝑢,𝑢 ′ )∼𝜌⊗𝜚 [︁ ⟨︀ 𝑐(𝑢, •), 𝑐(𝑢 ′ , •) ⟩︀ ℋ ]︁ = E (𝑢,𝑢 ′ )∼𝜌⊗𝜚 [︀ 𝑐(𝑢, 𝑢 ′ ) ]︀ . ( 11.9) and, similarly, ⟨𝑚 𝜌 , 𝑚 𝜌 ⟩ ℋ = E (𝑢,𝑢 ′ )∼𝜌⊗𝜌 [︀ 𝑐(𝑢, 𝑢 ′ ) ]︀ . Remark 11.21. The reproducing property in (11.9) permits the computation of inner products with respect to the kernel (or an associated infinite-dimensional feature map) using evaluations of the kernel alone. In many machine learning tasks, this property is referred to as the kernel trick. ♢ Remark 11.22. The MMD is a metric under certain conditions on the kernel. In particular, we say that the kernel is characteristic if The energy distance D E between two probability density functions 𝜌 and 𝜚 is defined by E 𝜌 [𝑓 ] = E 𝜚 [𝑓 ] D 2 E (𝜌, 𝜚) := 2E (𝑢,𝑣)∼𝜌⊗𝜚 |𝑢 -𝑣| -E (𝑢,𝑢 ′ )∼𝜌⊗𝜌 |𝑢 -𝑢 ′ | -E (𝑣,𝑣 ′ )∼𝜚⊗𝜚 |𝑣 -𝑣 ′ |. (11.10) ♢ Proof of the following lemma is postponed to the end of Subsection 11.3.1 where we discuss energy scores: Lemma 11.25. The energy distance D E is a metric on 𝒫(R 𝑑 ). Remark 11.26. A key property of the MMD and the energy distance is that they are amenable to ensemble approximation; they can be implemented using only samples from 𝜌 or 𝜚. Given independent samples {𝑢 𝑖 } 𝑁 𝑖=1 ∼ 𝜌 and, independently, given independent samples {𝑣 𝑖 } 𝑀 𝑖=1 ∼ 𝜚, we define the empirical measures 𝜌 𝑁 (𝑢) = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑢 (𝑛) (𝑢), 𝜚 𝑀 (𝑣) = 1 𝑀 𝑀 ∑︁ 𝑚=1 𝛿 𝑣 (𝑚) (𝑢). (11.11) We can estimate the MMD distance between 𝜌 and 𝜚 by D 2 MMD (𝜌, 𝜚) ≈ D 2 MMD (𝜌 𝑁 , 𝜚 𝑀 ), (11.12a) D 2 MMD (𝜌 𝑁 , 𝜚 𝑀 ) = 1 𝑁 2 ∑︁ 𝑖,𝑗 𝑐(𝑢 𝑖 , 𝑢 𝑗 ) + 1 𝑀 2 ∑︁ 𝑖,𝑗 𝑐(𝑣 𝑖 , 𝑣 𝑗 ) - 2 𝑁 𝑀 ∑︁ 𝑖,𝑗 𝑐(𝑢 𝑖 , 𝑣 𝑗 ). (11.12b) It is standard in practice to use a slightly different empirical approximation of MMD. This arises from dropping the diagonal term in the first two sums appearing in (11.12b), and renormalizing, resulting in the approximation D 2 MMD (𝜌, 𝜚) ≈ 1 𝑁 (𝑁 -1) ∑︁ 𝑖̸ =𝑗 𝑐(𝑢 𝑖 , 𝑢 𝑗 ) + 1 𝑀 (𝑀 -1) ∑︁ 𝑖̸ =𝑗 𝑐(𝑣 𝑖 , 𝑣 𝑗 ) - 2 𝑁 𝑀 ∑︁ 𝑖,𝑗 𝑐(𝑢 𝑖 , 𝑣 𝑗 ). We present the approximation (11.12) including the diagonal term because it is simple conceptually. Dropping the diagonal term, however, leads to improved constants in error estimates relating the empirical and population distances, because the samples on the diagonal are correlated. Choosing the negative kernel 𝑐(𝑢, 𝑢 ′ ) = -|𝑢-𝑢 ′ |, equations (11.12) give the following estimate for the energy distance between 𝜌 and 𝜚: D 2 E (𝜌 𝑁 , 𝜚 𝑀 ) = 2 𝑁 𝑀 ∑︁ 𝑖,𝑗 |𝑢 𝑖 -𝑣 𝑗 | - 1 𝑁 2 ∑︁ 𝑖,𝑗 |𝑢 𝑖 -𝑢 𝑗 | - 1 𝑀 2 ∑︁ 𝑖,𝑗 |𝑣 𝑖 -𝑣 𝑗 |. (11.13) This too can be slightly improved, as an estimate of the population limit, by dropping the diagonal terms and renormalizing. ♢ Remark 11.27. Another important property of both the MMD with isotropic kernel and the energy distance is that they are rotation invariant. That is, the metrics do not change under the transformations 𝑢 ← 𝑅𝑢 and 𝑣 ← 𝑅𝑣, for any unitary matrix 𝑅. ♢ Metrics On The Space Of Random Probability Measures In these notes, when we consider random variables, they primarily take values in finite dimensional Euclidean space. However it is sometimes useful to consider random variables which take values in the space of probability measures on finite dimensional Euclidean space. This leads to consideration of the space of random probability measures. This space arises naturally when building methodologies from empirical sampling or from Monte Carlo methods. We consider functions 𝜋 : Ω → 𝒫(R 𝑑 ), for some abstract probability space (Ω, ℬ, P); expectation under P is denoted by E. For any fixed 𝜔 ∈ Ω let 𝜋 = 𝜋(𝜔) and define E 𝜋 [︀ 𝑓 ]︀ := ∫︀ 𝑓 (𝑢)𝜋(𝑑𝑢). Definition 11.28. We define the following metric on the space of random probability measures: 𝑑(𝜋, 𝜋 ′ ) := sup |𝑓 |∞≤1 ⃒ ⃒ ⃒ ⃒ E [︂ (︁ E 𝜋 [︀ 𝑓 ]︀ -E 𝜋 ′ [︀ 𝑓 ]︀ )︁ 2 ]︂⃒ ⃒ ⃒ ⃒ 1/2 . ♢ Remark 11.29. Note that 𝑑 reduces to twice the total variation distance when 𝜋, 𝜋 ′ are not random. ♢ Given a fixed deterministic probability density function 𝜋, we now define a random Monte Carlo approximation, from independent samples, using Diracs: 𝜋 𝑁 MC = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑢 (𝑛) , 𝑢 (𝑛) ∼ 𝜋 i.i.d. (11.14) Note that 𝜋 𝑁 MC is a random probability measure, over space Ω underlying the random samples {𝑢 (𝑛) }, approximating 𝜋. From this random probability measure we can compute random approximations of expectation under 𝜋 : E 𝜋 𝑁 MC [︀ 𝑓 ]︀ = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝑓 (𝑢 (𝑛) ), 𝑢 (𝑛) ∼ 𝜋 i.i.d. (11.15) The metric 𝑑 introduced in Definition 11.28 is well-adapted to quantify the error in approximating the population limit 𝜋 by 𝜋 𝑁 MC . Let E denote expectation with respect to the 𝑁 i.i.d. samples from 𝜋 defining the Monte Carlo approximation of 𝜋, a product measure; likewise Var denotes variance under the same product measure. The following theorem captures the properties of this random approximation: Theorem 11.30. Given 𝑓 : R 𝑑 -→ R define |𝑓 | ∞ := sup 𝑢∈R 𝑑 |𝑓 (𝑢)|. Then sup |𝑓 |∞≤1 ⃒ ⃒ ⃒E [︁ E 𝜋 𝑁 MC [︀ 𝑓 ]︀ -E 𝜋 [︀ 𝑓 ]︀ ]︁⃒ ⃒ ⃒ = 0, 𝑑(𝜋 𝑁 MC , 𝜋) 2 ≤ 1 𝑁 . Proof. To prove that the estimator is unbiased, use linearity of the expected value and that 𝑢 (𝑛) ∼ 𝜋 to obtain the following identity, from which the desired result follows: E [︁ E 𝜋 𝑁 MC [︀ 𝑓 ]︀ ]︁ = E [︃ 1 𝑁 𝑁 ∑︁ 𝑛=1 𝑓 (︀ 𝑢 (𝑛) )︀ ]︃ = 1 𝑁 𝑁 E 𝜋 [︀ 𝑓 ]︀ = E 𝜋 [︀ 𝑓 ]︀ = E [︀ E 𝜋 [︀ 𝑓 ]︀]︀ . For the error estimate in 𝑑(•, •) we note that, since E 𝜋 𝑁 MC [︀ 𝑓 ]︀ is unbiased, its variance coincides with its mean squared error. Using the fact that the 𝑢 (𝑛) ∼ 𝜋 are independent we deduce that Var [︁ E 𝜋 𝑁 MC [︀ 𝑓 ]︀ ]︁ = Var [︃ 1 𝑁 𝑁 ∑︁ 𝑛=1 𝑓 (︀ 𝑢 (𝑛) )︀ ]︃ = 1 𝑁 2 𝑁 Var 𝜋 [𝑓 ] = 1 𝑁 Var 𝜋 [𝑓 ]. Assuming |𝑓 | ∞ ≤ 1, we have Var 𝜋 [𝑓 ] = E 𝜋 [︀ 𝑓 2 ]︀ -E 𝜋 [︀ 𝑓 ]︀ 2 ≤ E 𝜋 [︀ 𝑓 2 ]︀ ≤ 1, and therefore sup |𝑓 |∞≤1 ⃒ ⃒ ⃒ ⃒ E [︂ (︁ E 𝜋 𝑁 MC [︀ 𝑓 ]︀ -E 𝜋 [︀ 𝑓 ]︀ )︁ 2 ]︂⃒ ⃒ ⃒ ⃒ = sup |𝑓 |∞≤1 ⃒ ⃒ ⃒ ⃒ 1 𝑁 Var 𝜋 [𝑓 ] ⃒ ⃒ ⃒ ⃒ ≤ 1 𝑁 . Divergences Recall the conditions for a metric given in Section 11.1. If we remove the conditions of symmetry and the triangle inequality then we obtain a statistical divergence: a function D : 𝒫(R 𝑑 ) × 𝒫(R 𝑑 ) → R that satisfies the following two properties for all 𝜌, 𝜚 ∈ 𝒫(R 𝑑 ): 1. Non-negative: D(𝜌, 𝜚) ≥ 0. 2. Positive: D(𝜌, 𝜚) = 0 if and only if 𝜌 = 𝜚. To emphasize the asymmetry in their arguments, divergences are often written with double bars D(𝜌‖𝜚) or a colon D(𝜌 : 𝜚). We adopt the former convention. The following is a straightforward consequence of the fact that divergence is obtained by removing two of the four defining characteristics of a metric: Lemma 11.31. Every metric is a statistical divergence. f-Divergences An important feature of f-divergences, which we now define, is that they depend on a ratio of two probability density functions. To simplify the presentation, throughout this section we restrict our attention to positive densities. Definition 11.32. Let f : [0, ∞) → R be a strictly convex function that satisfies f(0) = lim 𝑡→0 + f(𝑡) and f(1) = 0. The f-divergence between distributions with positive densities 𝜌 and 𝜚 is defined by D f (𝜌‖𝜚) := ∫︁ f (︂ 𝜌(𝑢) 𝜚(𝑢) )︂ 𝜚(𝑢) 𝑑𝑢. ♢ Remark 11.33. Some f-divergences define metrics. For example, the choice f(𝑡) = 1 2 |𝑡 -1| delivers D f (𝜌‖𝜚) = D TV (𝜌, 𝜚) and the choice f(𝑡) = (1 - √ 𝑡) 2 delivers D f (𝜌‖𝜚) = 2D H (𝜌, 𝜚) 2 . It can be shown that the only f-divergence that is also an IPM is the total variation distance. The next definition introduces two important f-divergences that do not define metrics: the Kullback-Leibler divergence and the 𝜒 foot_11 divergence. They correspond to choosing f(𝑡) = 𝑡 log 𝑡 and f(𝑡) = (𝑡 -1) 2 , respectively. ♢ Definition 11.34. The Kullback-Leibler (KL) divergence between two positive probability density functions 𝜌 and 𝜚 is defined by D KL (𝜌‖𝜚) := ∫︁ log (︂ 𝜌(𝑢) 𝜚(𝑢) )︂ 𝜌(𝑢) 𝑑𝑢. (11.16) The 𝜒 2 divergence between two positive probability density functions 𝜌 and 𝜚 is defined by D 𝜒 2 (𝜌‖𝜚) := ∫︁ (︂ 𝜌(𝑢) 𝜚(𝑢) -1 )︂ 2 𝜚(𝑢) 𝑑𝑢. ( 11.17) ♢ Example 11.35. The KL and 𝜒 2 divergences can be explicitly computed for some parametric distributions. For example, if 𝜌 = 𝒩 (𝑚 𝜌 , Σ 𝜌 ) and 𝜚 = 𝒩 (𝑚 𝜚 , Σ 𝜚 ) are both 𝑑-dimensional multivariate Gaussians, the KL and 𝜒 2 divergences are given by D KL (𝜌‖𝜚) = 1 2 (︃ Tr(Σ -1 𝜚 Σ 𝜌 ) -𝑑 + (𝑚 𝜌 -𝑚 𝜚 ) ⊤ Σ -1 𝜚 (𝑚 𝜌 -𝑚 𝜚 ) + log (︃ det Σ 𝜚 det Σ 𝜌 )︃)︃ , ( 11.18a) D 𝜒 2 (𝜌‖𝜚) = det(Σ 𝜚 Σ -1 𝜌 ) det(2Σ 𝜚 Σ -1 𝜌 -𝐼) exp (︂ (𝑚 𝜌 -𝑚 𝜚 ) ⊤ Σ -1 𝜌 (2Σ 𝜚 Σ -1 𝜌 -𝐼) -1 (𝑚 𝜌 -𝑚 𝜚 ) -1 )︂ . ( 11.18b) We give references for these formulae in the bibliography Section 11.4. ♢ Note that neither the KL divergence nor the 𝜒 2 divergence are symmetric in their arguments, illustrating that the converse of Lemma 11.31 does not hold in general. The following remark shows that the KL divergence can be minimized over dependence in its first argument without knowledge of the normalizing constant of the second; this fact is useful in variational inference (see Chapter 2). Remark 11.36. Here we extend the notion of f-divergence in Definition 11.32 to allow the density in the second argument to be unnormalized. For any probability density function 𝜋 and f-divergence it holds that 𝜋 ∈ arg min 𝑞∈𝒫 D f (𝑞‖𝜋) and the minimizer is unique. The KL divergence is the only f-divergence with the property that, for any 𝑐 > 0, D f (𝑞‖𝑐𝜋) -D f (𝑞‖𝜋) is independent of 𝑞. 2 Consequently, for any 𝑐 > 0, 𝜋 ∈ arg min 𝑞∈𝒫 D KL (𝑞‖𝑐𝜋). That is, the minimizer is independent of scaling factors on the target density, such as a normalization constant. ♢ The following result, which we use repeatedly in Chapter 5, is known as the chain rule of KL divergence. Note the convention that the KL divergence between two conditionals is defined by averaging over the marginal distribution of the variable used to condition. 𝜌(𝑢, 𝑣)‖𝜚(𝑢, 𝑣) )︀ = D KL (︀ 𝜌(𝑢)‖𝜚(𝑢) )︀ + D KL (︀ 𝜌(𝑣|𝑢)‖𝜚(𝑣|𝑢) )︀ , where D KL (︀ 𝜌(𝑣|𝑢)‖𝜚(𝑣|𝑢) )︀ := ∫︁ [︂∫︁ log (︂ 𝜌(𝑣|𝑢) 𝜚(𝑣|𝑢) )︂ 𝜌(𝑣|𝑢) 𝑑𝑣 ]︂ 𝜌(𝑢) 𝑑𝑢. Proof. By direct calculation, D KL (︀ 𝜌(𝑢, 𝑣)‖𝜚(𝑢, 𝑣) )︀ = ∫︁ ∫︁ log (︂ 𝜌(𝑢, 𝑣) 𝜚(𝑢, 𝑣) )︂ 𝜌(𝑢, 𝑣) 𝑑𝑢𝑑𝑣 = ∫︁ ∫︁ (︂ log (︂ 𝜌(𝑢) 𝜚(𝑢) )︂ + log (︂ 𝜌(𝑣|𝑢) 𝜚(𝑣|𝑢) )︂)︂ 𝜌(𝑢)𝜌(𝑣|𝑢) 𝑑𝑢𝑑𝑣 = ∫︁ log (︂ 𝜌(𝑢) 𝜚(𝑢) )︂ 𝜌(𝑢) 𝑑𝑢 + ∫︁ [︂∫︁ log (︂ 𝜌(𝑣|𝑢) 𝜚(𝑣|𝑢) )︂ 𝜌(𝑣|𝑢) 𝑑𝑣 ]︂ 𝜌(𝑢) 𝑑𝑢 = D KL (︀ 𝜌(𝑢)‖𝜚(𝑢) )︀ + D KL (︀ 𝜌(𝑣|𝑢)‖𝜚(𝑣|𝑢) )︀ , as desired. Relationships Between f-Divergences And Metrics Here we establish bounds between some metrics and f-divergences. We continue to work under the simplifying assumption that 𝜌, 𝜚 are positive densities. Lemma 11.38. The Hellinger and total variation distances are upper bounded by the KL divergence as follows: D H (𝜌, 𝜚) 2 ≤ 1 2 D KL (𝜌‖𝜚), D TV (𝜌, 𝜚) 2 ≤ D KL (𝜌‖𝜚). Then 𝜙 ′ (𝑥) = 1 - 1 𝑥 , 𝜙 ′′ (𝑥) = 1 𝑥 2 , 𝜙(∞) = 𝜙(0) = ∞, and so the function is convex on its domain. The minimum of 𝜙 is attained at 𝑥 = 1, and 𝜙(1) = 0; hence 𝜙(𝑥) ≥ 0 for all 𝑥 ∈ (0, ∞). It follows that, for all 𝑥 ≥ 0, 𝑥 -1 ≥ log 𝑥, √ 𝑥 -1 ≥ 1 2 log 𝑥. Using this last inequality we can bound the Hellinger distance as follows: D H (𝜌, 𝜚) 2 = 1 2 ∫︁ (︃ 1 - √︃ 𝜚(𝑢) 𝜌(𝑢) )︃ 2 𝜌(𝑢) 𝑑𝑢 = 1 2 ∫︁ (︃ 1 + 𝜚(𝑢) 𝜌(𝑢) -2 √︃ 𝜚(𝑢) 𝜌(𝑢) )︃ 𝜌(𝑢) 𝑑𝑢 = ∫︁ (︃ 1 - √︃ 𝜚(𝑢) 𝜌(𝑢) )︃ 𝜌(𝑢) 𝑑𝑢 ≤ - 1 2 ∫︁ log (︂ 𝜚(𝑢) 𝜌(𝑢) )︂ 𝜌(𝑢) 𝑑𝑢 = 1 2 D KL (𝜌‖𝜚). The following lemma shows that the 𝜒 2 divergence upper bounds the KL divergence. By Lemma 11.38 this implies that it also bounds the total variation and Hellinger distances. Lemma 11.39. The 𝜒 2 divergence upper bounds the KL divergence as follows: D KL (𝜌‖𝜚) ≤ log (︁ D 𝜒 2 (𝜌‖𝜚) + 1 )︁ , D KL (𝜌‖𝜚) ≤ D 𝜒 2 (𝜌‖𝜚). Proof. The second inequality is a direct consequence of the first one, noting that, for 𝑥 ≥ 0, log(𝑥 + 1) ≤ 𝑥. To prove the first inequality note that by Jensen inequality D KL (𝜌‖𝜚) = ∫︁ log (︂ 𝜌(𝑢) 𝜚(𝑢) )︂ 𝜌(𝑢) 𝑑𝑢 ≤ log (︂∫︁ 𝜌(𝑢) 𝜚(𝑢) 𝜌(𝑢) 𝜚(𝑢) 𝜚(𝑢) 𝑑𝑢 )︂ = log (︁ D 𝜒 2 (𝜌‖𝜚) + 1 )︁ , where for the last equality we used that D 𝜒 2 (𝜌‖𝜚) = ∫︁ (︂ 𝜌(𝑢) 𝜚(𝑢) -1 )︂ 2 𝜚(𝑢) 𝑑𝑢 = ∫︁ (︂ 𝜌(𝑢) 𝜚(𝑢) )︂ 2 𝜚(𝑢) 𝑑𝑢 -2 ∫︁ (︂ 𝜌(𝑢) 𝜚(𝑢) )︂ 𝜚(𝑢) 𝑑𝑢 + ∫︁ 𝜚(𝑢) 𝑑𝑢 = ∫︁ (︂ 𝜌(𝑢) 𝜚(𝑢) )︂ 2 𝜚(𝑢) 𝑑𝑢 -1. Invariance Of f-Divergences Under Invertible Tranformations This subsection discusses an important property satisfied by all f-divergences: they are invariant under invertible transformations of the underlying variables. Recall the notation for pushforward from the preface. We start with the following lemma relating to pushforwards. The proof is a straightforward consequence of change of variables. Lemma 11.40. Let 𝜋 be a probability density on R 𝑑 and let 𝑞 ∈ 𝐶 1 (R 𝑑 , R 𝑑 ) be invertible everywhere on R 𝑑 . Assume further that the determinant of the Jacobian of the inverse, det𝐷(𝑞 -1 )(𝑢) = (︁ det𝐷𝑞 (︀ 𝑞 -1 (𝑢) )︀ )︁ -1 , ( 11.19) is positive everywhere on R 𝑑 . Then, for 𝑢 = 𝑞(𝑧), ∫︁ 𝜙(𝑧)𝜋(𝑧) 𝑑𝑧 = ∫︁ (︀ 𝜙 ∘ 𝑞 -1 )︀ (𝑢) (︀ 𝜋 ∘ 𝑞 -1 )︀ (𝑢)det𝐷(𝑞 -1 )(𝑢) 𝑑𝑢. Thus 𝑞 ♯ 𝜋(𝑢) = (︀ 𝜋 ∘ 𝑞 -1 )︀ (𝑢)det𝐷(𝑞 -1 )(𝑢), (11.20a) log 𝑞 ♯ 𝜋(𝑢) = log (︀ 𝜋 ∘ 𝑞 -1 )︀ (𝑢) + log det𝐷 (︀ 𝑞 -1 (𝑢) )︀ . ( 11.20b) The following theorem, concerning the invariance of the f-divergence D f under invertible transformations, is a consequence of the preceding lemma. Theorem 11.41. Let 𝑇 : R 𝑑 → R 𝑑 be an invertible and differentiable transformation. Then, for two probability density functions 𝜌, 𝜚 on R 𝑑 we have D f (𝜌‖𝜚) = D f (𝑇 ♯ 𝜌‖𝑇 ♯ 𝜚). Proof. Let 𝑣 = 𝑇 (𝑢) denote the transformed variable. For an invertible transformation, 𝑢 = 𝑇 -1 (𝑣) and 𝑑𝑢 = det (︀ 𝐷𝑇 -1 (𝑣) )︀ 𝑑𝑣. Performing a change of variables in the divergence we have D f (𝜌‖𝜚) = ∫︁ f (︂ 𝜌(𝑢) 𝜚(𝑢) )︂ 𝜚(𝑢) 𝑑𝑢 = ∫︁ f (︃ 𝜌 (︀ 𝑇 -1 (𝑣) )︀ 𝜚 (︀ 𝑇 -1 (𝑣) )︀ )︃ 𝜚 (︀ 𝑇 -1 (𝑣) )︀ det (︀ 𝐷𝑇 -1 (𝑣) )︀ 𝑑𝑣 = ∫︁ f (︃ 𝜌 (︀ 𝑇 -1 (𝑣) )︀ det (︀ 𝐷𝑇 -1 (𝑣) )︀ 𝜚 (︀ 𝑇 -1 (𝑣) )︀ det (︀ 𝐷𝑇 -1 (𝑣) )︀ )︃ 𝜚 (︀ 𝑇 -1 (𝑣) )︀ det (︀ 𝐷𝑇 -1 (𝑣) )︀ 𝑑𝑣. Using Lemma 11.40 we see that 𝜌 (︀ 𝑇 -1 (𝑣) )︀ det (︀ 𝐷𝑇 -1 (𝑣) )︀ = 𝑇 ♯ 𝜌(𝑣) denotes the density of the pushforward random variable 𝑣; thus we have D f (𝜌‖𝜚) = ∫︁ f (︃ 𝑇 ♯ 𝜌(𝑣) 𝑇 ♯ 𝜚(𝑣) )︃ 𝑇 ♯ 𝜚(𝑣) 𝑑𝑣 = D f (𝑇 ♯ 𝜌‖𝑇 ♯ 𝜚). Scoring Rules Probabilistic scoring rules quantify the accuracy of a given distribution, which we will refer to as the forecast, with respect to a true distribution known only through samples; the latter is sometimes termed the verification. Let 𝜌 and 𝜚 denote the densities of the forecast and verification distributions, respectively. A scoring rule is a function S : 𝒫(R 𝑑 ) × R 𝑑 → R that assigns a score S(𝜌, 𝑣) to a sample 𝑣 ∼ 𝜚, from the verification, with respect to the forecast distribution 𝜌. We follow the convention here that scoring rules are negatively oriented meaning that a lower score indicates a better forecast. To compare the forecast and verification distributions, we define the expected score S : 𝒫(R 𝑑 ) × 𝒫(R 𝑑 ) → R by S(𝜌, 𝜚) := E 𝑣∼𝜚 [S(𝜌, 𝑣)] = ∫︁ S(𝜌, 𝑣)𝜚(𝑣) 𝑑𝑣. Definition 11.42. A scoring rule S is called proper if, for all 𝜌, 𝜚 ∈ 𝒫(R 𝑑 ), S(𝜚, 𝜚) ≤ S(𝜌, 𝜚). A scoring rule is called strictly proper when equality holds if and only if 𝜌 = 𝜚. ♢ Thus, for a proper scoring rule, forecasting with the true distribution results in the lowest expected score. For a strictly proper scoring rule, the lowest expected score can only be attained when forecasting with the true distribution. Using this property, scoring rules can be used to define a divergence if strict propriety is imposed: Lemma 11.43. Let S be a strictly proper scoring rule. Then the resulting expected score can be used to define a divergence between two probability density functions given by D S (𝜌‖𝜚) := S(𝜌, 𝜚) -S(𝜚, 𝜚). (11.21) Proof. For a proper scoring rule, the distance is non-negative: D S (𝜌, 𝜚) ≥ 0. If S is strictly proper then D S (𝜌, 𝜚) = 0 if and only if 𝜌 = 𝜚. In the following subsections we present five probabilistic scoring rules: the energy score, the continuous ranked probability score, the quantile score, the logarithmic score and the Dawid-Sebastiani score. Energy Score We first consider the energy score, which is closely related to the energy distance studied in Subsection 11.1.5. Definition 11.44. For 𝛽 ∈ (0, 2), the energy score ES 𝛽 : 𝒫(R 𝑑 ) × R 𝑑 → R is defined by ES 𝛽 (𝜌, 𝑣) := E 𝑢∼𝜌 |𝑢 -𝑣| 𝛽 - 1 2 E (𝑢,𝑢 ′ )∼𝜌⊗𝜌 |𝑢 -𝑢 ′ | 𝛽 . ( 11.22) The expected energy score ES 𝛽 : 𝒫(R 𝑑 ) × 𝒫(R 𝑑 ) → R is then defined by ES 𝛽 (𝜌, 𝜚) := E 𝑣∼𝜚 [ES 𝛽 (𝜌, 𝑣)] = ∫︁ ES 𝛽 (𝜌, 𝑣)𝜚(𝑣) 𝑑𝑣. ( 11.23) ♢ The following lemma makes an explicit connection between the energy score and the energy distance. Lemma 11.45. Recall the squared energy distance in Definition 11.24, given by D 2 E (𝜌, 𝜚) := 2E (𝑢,𝑣)∼𝜌⊗𝜚 |𝑢 -𝑣| -E (𝑢,𝑢 ′ )∼𝜌⊗𝜌 |𝑢 -𝑢 ′ | -E (𝑣,𝑣 ′ )∼𝜚⊗𝜚 |𝑣 -𝑣 ′ |. (11.24) The energy score ES 𝛽 (𝜌, 𝑣) is a strictly proper scoring rule for 𝛽 ∈ (0, 2). Using the distance function in (11.21) with the expected score and with 𝛽 = 1, yields 1 2 D 2 E (𝜌, 𝜚) = ES 1 (𝜌, 𝜚) -ES 1 (𝜚, 𝜚). Proof. It can be shown that the expected energy score (11.23) can be written, for 𝛽 ∈ (0, 2), as ES 𝛽 (𝜌, 𝜚) = 𝛽2 𝛽-2 Γ( 𝑑 2 + 𝛽 2 ) 𝜋 𝑑/2 Γ(1 -𝛽 2 ) ∫︁ |𝜙 𝜌 (𝑢) -𝜙 𝜚 (𝑢)| 2 |𝑢| 𝑑+𝛽 𝑑𝑢 + ES 𝛽 (𝜚, 𝜚), (11.25) where 𝜙 𝜌 and 𝜙 𝜚 are the characteristic functions of 𝜌 and 𝜚, respectively. 3 It follows that the first term is minimized if and only if 𝜙 𝜌 = 𝜙 𝜚 and hence if and only if 𝜌 = 𝜚. Since the second term does not depend on 𝜚, it follows that the energy score is strictly proper for 𝛽 ∈ (0, 2). We show the relationship between the energy score and the energy distance as follows: ES 𝛽 (𝜌, 𝜚) = E (𝑢,𝑣)∼𝜌⊗𝜚 |𝑢 -𝑣| 𝛽 - 1 2 E (𝑢,𝑢 ′ )∼𝜌⊗𝜌 |𝑢 -𝑢 ′ | 𝛽 , ES 𝛽 (𝜚, 𝜚) = 1 2 E (𝑢,𝑢 ′ )∼𝜚⊗𝜚 |𝑢 -𝑢 ′ | 𝛽 . Then, for 𝛽 = 1, ES 1 (𝜌, 𝜚) -ES 1 (𝜚, 𝜚) = E (𝑢,𝑣)∼𝜌⊗𝜚 |𝑢 -𝑣| - 1 2 E (𝑢,𝑢 ′ )∼𝜌⊗𝜌 |𝑢 -𝑢 ′ | - 1 2 E (𝑢,𝑢 ′ )∼𝜚⊗𝜚 |𝑢 -𝑢 ′ |, which is indeed half of the squared energy distance in Definition 11.24. Lemma 11.46. The energy score ES 2 (𝜌, 𝑣) is proper, but not strictly proper. Proof. Notice that ES 2 (𝜌, 𝑣) = E 𝑢∼𝜌 |𝑢| 2 -2E 𝑢∼𝜌 [𝑢] ⊤ 𝑣 + |𝑣| 2 - 1 2 (E 𝑢∼𝜌 |𝑢| 2 -2E 𝑢∼𝜌 [𝑢] ⊤ E 𝑢∼𝜌 [𝑢] + E 𝑢∼𝜌 |𝑢| 2 ) = E 𝑢∼𝜌 [𝑢] ⊤ E 𝑢∼𝜌 [𝑢] -2E 𝑢∼𝜌 [𝑢] ⊤ 𝑣 + |𝑣| 2 = ⃒ ⃒ E 𝑢∼𝜌 [𝑢] -𝑣 ⃒ ⃒ 2 . The minimizer of J(𝑏) := E 𝑣∼𝜚 |𝑏 -𝑣| 2 is 𝑏 ⋆ = E 𝑢∼𝜚 [𝑢], which shows that ES 2 is proper. It is not strictly proper, since for any other distribution ρ with E 𝑢∼𝜚 [𝑢] = E 𝑢∼ρ [𝑢] it holds that ES 2 (𝜚, 𝜚) = ES 2 (ρ, 𝜚). Remark 11.47. When 𝛽 = 2 the expected energy score is the mean-square error of E 𝑢∼𝜌 [𝑢]. Its square root, the root-mean-square error (RMSE) of E 𝑢∼𝜌 [𝑢], is often used for evaluating probabilistic forecasts, despite lacking strict propriety. ♢ We are now in a position to prove Lemma 11.25: Proof of Lemma 11.25. The proof follows from noting that, from Lemma 11.45 and equation (11.25) D 2 E (𝜌, 𝜚) = ES 1 (𝜌, 𝜚) -ES 1 (𝜚, 𝜚) = Γ( 𝑑 2 + 1 2 ) 2𝜋 𝑑/2 Γ( 1 2 ) ∫︁ |𝜙 𝜌 (𝑥) -𝜙 𝜚 (𝑢)| 2 |𝑢| 𝑑+1 𝑑𝑢. Using the fact that D E (𝜌, 𝜚) is defined as a weighted 𝐿 2 -norm of the difference between the characteristic functions of the pair (𝜌, 𝜚) leads to the desired metric structure. Continuous Ranked Probability Score This score is only defined for probability measures on R and real-valued random variables. Similar to the Wasserstein-1 distance in the one-dimensional setting studied in Subsection 11.1.3, the continuous ranked probability score (CRPS) is based on comparing cumulative distribution functions. To introduce the definition, we let 1 𝑢≥𝑣 denote the indicator function of the set {𝑢 ∈ R : 𝑢 ≥ 𝑣}; this is also known as a Heaviside step function. We will also employ natural generalizations of this notation in which ≥ is replaced by >, ≤ and <. Definition 11.48. The continuous ranked probability score CRPS : 𝒫(R) × R → R is defined by CRPS(𝜌, 𝑣) := ∫︁ (︀ 𝐹 𝜌 (𝑢) -1 𝑢≥𝑣 (𝑢) )︀ 2 𝑑𝑢. ( 11.26) The expected continuous ranked probability score CRPS : 𝒫(R) × 𝒫(R) → R is then defined by CRPS(𝜌, 𝜚) := E 𝑣∼𝜚 [CRPS(𝜌, 𝑣)] = ∫︁ CRPS(𝜌, 𝑣)𝜚(𝑣) 𝑑𝑣. ♢ Note that the indicator function 1 𝑢≥𝑣 is the cumulative density function of a Dirac mass at 𝑣. Thus the CRPS compares two cumulative density functions, one for the forecast and one for the verification; in particular it is the square of the 𝐿 2 distance between the two cumulative density functions. The following lemma shows that the CRPS is also equivalent to the energy score in Definition 11.24 with 𝛽 = 1 in dimension 𝑑 = 1; thus the energy score with 𝛽 = 1 provides a natural generalization of CRPS to more than one dimension. Lemma 11.49. For 𝜌 ∈ 𝒫(R) and 𝑣 ∈ R, it holds that CRPS(𝜌, 𝑣) = ES 1 (𝜌, 𝑣) = E 𝑢∼𝜌 |𝑢 -𝑣| - 1 2 E (𝑢,𝑢 ′ )∼𝜌⊗𝜌 |𝑢 -𝑢 ′ |. (11.27) Proof. First, let us recall that the absolute value of a difference can be written as |𝑢 -𝑣| = ∫︁ ∞ 𝑣 1 𝑧≤𝑢 (𝑧) 𝑑𝑧 + ∫︁ 𝑣 -∞ 1 𝑧>𝑢 (𝑧) 𝑑𝑧, by considering the two cases 𝑣 ≤ 𝑢 and 𝑣 > 𝑢. Taking an expectation with respect to 𝑢 ∼ 𝜌 and applying Fubini's theorem we have E 𝑢∼𝜌 |𝑢 -𝑣| = ∫︁ ∞ 𝑣 ∫︁ R 1 𝑧≤𝑢 (𝑧)𝜌(𝑢) 𝑑𝑢𝑑𝑧 + ∫︁ 𝑣 -∞ ∫︁ R 1 𝑧>𝑢 (𝑧)𝜌(𝑢) 𝑑𝑢𝑑𝑧 = ∫︁ ∞ 𝑣 (︀ 1 -𝐹 𝜌 (𝑧) )︀ 𝑑𝑧 + ∫︁ 𝑣 -∞ 𝐹 𝜌 (𝑧) 𝑑𝑧 = ∫︁ R (︀ 1 -𝐹 𝜌 (𝑧) )︀ 1 𝑧≥𝑣 (𝑧) 𝑑𝑧 + ∫︁ R 𝐹 𝜌 (𝑧)1 𝑧<𝑣 (𝑧) 𝑑𝑧. ( 11.28) Following similar steps for the second term in (11.27) and taking an expectation over 𝑢 ′ ∼ 𝜌, we have 1 2 E (𝑢,𝑢 ′ )∼𝜌⊗𝜌 |𝑢 -𝑢 ′ | = ∫︁ R 𝐹 𝜌 (𝑧) (︀ 1 -𝐹 𝜌 (𝑧) )︀ 𝑑𝑧. ( 11.29) Lastly, subtracting (11.28) and (11.29) gives E 𝑢∼𝜌 |𝑢 -𝑣| - 1 2 E (𝑢,𝑢 ′ )∼𝜌⊗𝜌 |𝑢 -𝑢 ′ | = ∫︁ R (︁ 1 𝑧≥𝑣 (𝑧) 2 -2𝐹 𝜌 (𝑧)1 𝑧≥𝑣 (𝑧) + 𝐹 𝜌 (𝑧) 2 )︁ 𝑑𝑧 = ∫︁ (︀ 𝐹 𝜌 (𝑧) -1 𝑧≥𝑣 (𝑧) )︀ 2 𝑑𝑧 = CRPS(𝜌, 𝑣). The characterization of the CRPS in (11.27) shows that it is composed of two parts: a calibration term that quantifies closeness of the forecast to 𝑣 and a sharpness term that is related to the spread of the distribution. Example 11.50. Consider a forecast distribution which is deterministic: 𝜌 = 𝛿 𝑧 and 𝐹 𝜌 (𝑢) = 1 𝑢≥𝑧 (𝑢). Then either (11.26) or (11.27) show that the CRPS reduces to the absolute error between the point 𝑧 and sample 𝑣. ♢ Lemma 11.51. The expected CRPS for forecast and verification distributions with probability density functions 𝜌 and 𝜚, respectively, is given by CRPS(𝜌, 𝜚) = ∫︁ (︀ 𝐹 𝜌 (𝑢) -𝐹 𝜚 (𝑢) )︀ 2 𝑑𝑢 + ∫︁ 𝐹 𝜚 (𝑢) (︀ 1 -𝐹 𝜚 (𝑢) )︀ 𝑑𝑢. (11.30) Proof. Start with form (11.26) of the CRPS. Then, E 𝑣∼𝜚 [︀ CRPS(𝜌, 𝑣) ]︀ = ∫︁ E 𝑣∼𝜚 [︁ (︀ 𝐹 𝜌 (𝑢) -1 𝑢≥𝑣 (𝑢) )︀ 2 ]︁ 𝑑𝑢 = ∫︁ E 𝑣∼𝜚 [︁ 𝐹 𝜌 (𝑢) 2 -2𝐹 𝜌 (𝑢)1 𝑢≥𝑣 (𝑢) + 1 𝑢≥𝑣 (𝑢) 2 ]︁ 𝑑𝑢. Noting that 1 𝑢≥𝑣 (𝑢) 2 = 1 𝑢≥𝑣 (𝑢) and that E 𝑣∼𝜚 [1 𝑢≥𝑣 (𝑢)] = 𝐹 𝜚 (𝑢), we obtain the result after rearranging. Remark 11.52. A direct consequence of Lemma 11.51 is that the divergence (11.21) defined by the CRPS takes the form D CRPS (𝜌‖𝜚) = CRPS(𝜌, 𝜚) -CRPS(𝜚, 𝜚) = ∫︁ (︀ 𝐹 𝜌 (𝑢) -𝐹 𝜚 (𝑢) )︀ 2 𝑑𝑢, which is known as Cramér's distance. Thus, we have shown that in one dimension CRPS agrees with the energy score ES 1 (Lemma 11.49), the divergence induced by CRPS agrees with Cramér's distance (Lemma 11.51), and the squared energy distance is exactly twice Cramér's distance (Lemma 11.45). ♢ Example 11.53. For a univariate Gaussian density 𝜌 = 𝒩 (𝑚, 𝜎 2 ), the CRPS with respect to a single observation 𝑣 has the form CRPS(𝜌, 𝑣) = 𝜎 (︂ 2𝜙 (︂ 𝑣 -𝑚 𝜎 )︂ + 𝑣 -𝑚 𝜎 (︂ 2Φ (︂ 𝑣 -𝑚 𝜎 )︂ -1 )︂ - 1 √ 𝜋 )︂ , where 𝜙 and Φ denote the probability density function and cumulative density function of a standard Gaussian distribution, respectively. We give a reference for this formula in the bibliography Section 11.4. One can observe that the CRPS is minimized for the forecast chosen to have the observation 𝑣 near the mean 𝑚 with a small-enough variance to maximize precision. ♢ Quantile Score Like the CRPS and Wasserstein-1 distance in the one-dimensional setting, the quantile score is based on comparing cumulative distribution functions; and like CRPS it is only defined for probability measures on R and for real-valued random variables. The quantile score introduces additional flexibility, in comparison to the CRPS, through a parameter 𝛼 ∈ [0, 1] that allows to penalize deviations at a specific quantile level. To introduce the quantile score we first define the quantile function and then demonstrate how that function can be derived through a minimization problem. And finally we link the quantile score to the CRPS. Definition 11.54. Given a probability density function 𝜚 corresponding to a random variable on R with cumulative density function 𝐹 𝜚 , the corresponding quantile function 4 is 𝑞 𝜚 (𝛼) := 𝐹 -1 𝜚 (𝛼). ♢ Note that we have already used the quantile function, in Lemma 11.10, to define the Wasserstein distance in one dimension. The quantile function is solution to the 𝛼-parameterized family of equations 𝐹 𝜚 (︀ 𝑞 𝜚 (𝛼) )︀ = 𝛼. We now prove a lemma which characterizes the quantile function in a different way: showing that the quantile function (of 𝛼) is also the minimizer of a scalar-valued function 𝐿 𝛼,𝜚 (viewed as a function of 𝛼 for given 𝜚). To this end let 𝛼 ∈ [0, 1]. Start by defining the 𝛼-parameterized family of functions ℎ 𝛼 : R → R by ℎ 𝛼 (𝑢) = {︃ -(1 -𝛼)𝑢, 𝑢 < 0, 𝛼𝑢, 𝑢 ≥ 0. (11.31) Function ℎ 𝛼 is sometimes termed the hinge loss. The hinge loss may also be written ℎ 𝛼 (𝑢) = {︃ (1 -𝛼)|𝑢|, 𝑢 < 0, 𝛼|𝑢|, 𝑢 ≥ 0, (11.32) or as ℎ 𝛼 (𝑢) = -(1 𝑢≤0 -𝛼)𝑢. (11.33) Using this function, together with a probability density function 𝜚, we define another 𝛼-parameterized family of functions 𝐿 𝛼,𝜚 : R → R by 𝐿 𝛼,𝜚 (𝜃) = E 𝑣∼𝜚 [︀ ℎ 𝛼 (𝑣 -𝜃) ]︀ . ( 11.34) We may now prove the following lemma, providing the alternative characterization of the quantile function. Lemma 11.55. For 𝛼 ∈ [0, 1], it holds that argmin 𝜃∈R 𝐿 𝛼,𝜚 (𝜃) = 𝑞 𝜚 (𝛼). Proof. First note that 𝐿 𝛼,𝜚 (𝜃) = (𝛼 -1) ∫︁ 𝜃 -∞ (𝑣 -𝜃)𝜚(𝑣) 𝑑𝑣 + 𝛼 ∫︁ ∞ 𝜃 (𝑣 -𝜃)𝜚(𝑣) 𝑑𝑣. Differentiating with respect to 𝜃 yields 𝑑 𝑑𝜃 𝐿 𝛼,𝜚 (𝜃) = (1 -𝛼) ∫︁ 𝜃 -∞ 𝜚(𝑣) 𝑑𝑣 -𝛼 ∫︁ ∞ 𝜃 𝜚(𝑣) 𝑑𝑣 = ∫︁ 𝜃 -∞ 𝜚(𝑣) 𝑑𝑣 -𝛼 = 𝐹 𝜚 (𝜃) -𝛼. Setting the derivative to zero yields the desired result. We now define the quantile score. Furthermore, using the preceding lemma as a building block, we link the CRPS and the quantile score. Definition 11.56. For 𝛼 ∈ [0, 1], the quantile score at level 𝛼, QS 𝛼 : 𝒫(R) × R → R, is defined by QS 𝛼 (𝜌, 𝑣) := (︁ 1 𝑣≤𝑞𝜌(𝛼) -𝛼 )︁ (𝑞 𝜌 (𝛼) -𝑣). ( 11.35) The expected quantile score QS 𝛼 : 𝒫(R) × 𝒫(R) → R is then defined by QS 𝛼 (𝜌, 𝜚) := E 𝑣∼𝜚 [QS 𝛼 (𝜌, 𝑣)] = ∫︁ QS 𝛼 (𝜌, 𝑣)𝜚(𝑣) 𝑑𝑣. ♢ The quantile score arises from evaluating the hinge loss in (11.33) at 𝑢 = 𝑣 -𝑞 𝜌 (𝛼) so that the following two identities are obtained: QS 𝛼 (𝜌, 𝑣) = ℎ 𝛼 (︀ 𝑣 -𝑞 𝜌 (𝛼) )︀ , QS 𝛼 (𝜌, 𝑣) = ℎ 𝛼 (︀ 𝑣 -𝐹 -1 𝜌 (𝛼) )︀ . Note that constant function taking value 𝑣 in (0, 1) is the quantile function associated with the Dirac mass at 𝑣: for 𝛼 ∈ (0, 1), we have that 𝐹 -1 𝛿𝑣 (𝛼) = inf{𝑢 : 𝐹 𝛿𝑣 (𝑢) ≥ 𝛼} = 𝑣, where here we have used the generalized inverse distribution. Thus equation (11.35 ) is comparing the value of two quantile functions evaluated at 𝛼. The following is a direct consequence of Lemma 11.55: Proposition 11.57. Let 𝜌 ⋆ ∈ argmin 𝜌 QS 𝛼 (𝜌, 𝜚). Then, for all 𝛼 ∈ [0, 1], 𝜌 ⋆ = 𝜚 is a solution of the minimization problem and hence QS 𝛼 is a proper scoring rule. Proof. Note that E 𝑣∼𝜚 [QS 𝛼 (𝜌, 𝑣)] = E 𝑣∼𝜚 [︁ ℎ 𝛼 (︀ 𝑣 -𝐹 -1 𝜌 (𝛼) )︀ ]︁ = 𝐿 𝛼,𝜚 (︀ 𝑞 𝜌 (𝛼) )︀ . By Lemma 11.55 the optimal 𝜌 will be one for which 𝑞 𝜌 (𝛼) = 𝑞 𝜚 (𝛼). This can be achieved by setting 𝜌 = 𝜚. Our final result on quantile scores shows that the CRPS can also be computed as twice the integral of the quantile score over all quantiles. Lemma 11.58. Assume that 𝜌 has finite first moment. CRPS(𝜌, 𝑣) = 2 ∫︁ 1 0 QS 𝛼 (𝜌, 𝑣) 𝑑𝛼. Hence CRPS(𝜌, 𝜚) = 2 ∫︁ 1 0 QS 𝛼 (𝜌, 𝜚) 𝑑𝛼. Proof. We proceed by applying the change of variables 𝑢 = 𝐹 -1 𝜌 (𝛼) to the integrated quantile score. Note that this change of variables implies that 𝑑𝛼 = 𝜌(𝑢) 𝑑𝑢. Thus 2 ∫︁ 1 0 QS 𝛼 (𝜌, 𝑣) 𝑑𝛼 = ∫︁ 1 0 2 (︁ 1 𝑣≤𝐹 -1 𝜌 (𝛼) -𝛼 )︁(︁ 𝐹 -1 𝜌 (𝛼) -𝑣 )︁ 𝑑𝛼 = 2 ∫︁ R (︀ 1 𝑣≤𝑢 -𝐹 𝜌 (𝑢) )︀ (𝑢 -𝑣)𝜌(𝑢) 𝑑𝑢. We recognize that, for 𝑢 ̸ = 𝑣, 𝑑 𝑑𝑢 (︀ 1 𝑣≤𝑢 -𝐹 𝜌 (𝑢) )︀ 2 = -2 (︀ 1 𝑣≤𝑢 -𝐹 𝜌 (𝑢) )︀ 𝜌(𝑢). Applying integration by parts gives us 2 ∫︁ 1 0 QS 𝛼 (𝜌, 𝑣) 𝑑𝛼 = - ∫︁ R 𝑑 𝑑𝑢 (1 𝑣≤𝑢 -𝐹 𝜌 (𝑢)) 2 (𝑢 -𝑣) 𝑑𝑢 = (︀ 1 𝑣≤𝑢 -𝐹 𝜌 (𝑢) )︀ 2 (𝑢 -𝑣) ⃒ ⃒ ⃒ 𝑢=∞ 𝑢=-∞ + ∫︁ R (︀ 1 𝑣≤𝑢 -𝐹 𝜌 (𝑢) )︀ 2 𝑑𝑢. Now we claim that the boundary terms in the integration by parts vanish. Indeed, by Markov's inequality it holds that 1 -𝐹 𝜌 (𝑢) ≤ E 𝑤∼𝜌 |𝑤| 𝑢 and so (︀ 1 -𝐹 𝜌 (𝑢) )︀ 2 (𝑢 -𝑣) → 0 as 𝑢 → ∞. The limit as 𝑢 → -∞ is similar. Hence, we have shown that 2 ∫︁ 1 0 QS 𝛼 (𝜌, 𝑣) 𝑑𝛼 = ∫︁ R (︀ 1 𝑣≤𝑢 -𝐹 𝜌 (𝑢) )︀ 2 𝑑𝑢 = CRPS(𝜌, 𝑣), as desired. Logarithmic Score The logarithmic score is based on the intuitive idea that 𝜌 ∈ 𝒫(R 𝑑 ) provides an accurate forecast for verification sample 𝑣 ∼ 𝜚 if 𝜌(𝑣) is large. Noting that the logarithm function is monotonically increasing and that scoring rules are negatively oriented by convention, the logarithmic score evaluates the negative logarithm of the forecast probability density function 𝜌 ∈ 𝒫(R 𝑑 ) at a sample 𝑣 ∼ 𝜚 : ♢ The logarithmic score is also sometimes referred to as the ignorance when the logarithm is in base 2. Notice that the logarithmic score penalizes heavily forecasts 𝜌 that place low probability in outcomes that materialize: for small 𝜌(𝑣), LS(𝜌, 𝑣) := -log 𝜌(𝑣) is very large. Recall the KL divergence given in Definition 11.34. Lemma 11.60. The logarithmic score is a strictly proper scoring rule. The divergence resulting from the expected logarithmic score is the KL divergence: D KL (𝜚‖𝜌) = LS(𝜌, 𝜚) -LS(𝜚, 𝜚). (11.37) Proof. We have that LS(𝜌, 𝜚) = E 𝑣∼𝜚 [LS(𝜌, 𝑣)] = - ∫︁ log 𝜌(𝑣)𝜚(𝑣) 𝑑𝑣, LS(𝜚, 𝜚) = E 𝑣∼𝜚 [LS(𝜚, 𝑣)] = - ∫︁ log 𝜚(𝑣)𝜚(𝑣) 𝑑𝑣. Recalling that D KL (𝜚‖𝜌) = ∫︁ log (︂ 𝜚(𝑣) 𝜌(𝑣) )︂ 𝜚(𝑣) 𝑑𝑣 delivers the desired result. Dawid-Sebastiani Score The Dawid-Sebasatiani score controls the first and second moments of the forecast: The Dawid-Sebastiani score is proper, but not strictly proper. This is similar to the ES 2 score, which reduces to controlling the first moment and is not strictly proper -see Lemma 11.46 ; the Dawid-Sebastiani score controls first and second moments and hence it is not strictly proper. Noise In The Verification In the presence of noise in the verification, the scoring rule will generally favor more dispersed forecasts than if the noise were not present. To account for this we consider scores calculated using noisy data ̃︀ 𝑣 drawn from ̃︀ 𝑣|𝑣 ∼ 𝑟(• |𝑣), given true predicted value 𝑣 ∼ 𝜚. We can then modify the Definition 11.42 of propriety as follows: 𝑟(̃︀ 𝑣|𝑣)𝜚(𝑣) 𝑑𝑣. A scoring rule S is called proper with respect to the noise distribution 𝑟 if, for all 𝜌, 𝜚, E ̃︀ 𝑣∼̃︀ 𝜚 [︀ S(𝜚, ̃︀ 𝑣) ]︀ ≤ E ̃︀ 𝑣∼̃︀ 𝜚 [︀ S(𝜌, ̃︀ 𝑣) ]︀ . It is called strictly proper when equality holds if and only if 𝜌 = 𝜚. ♢ Proposition 11.64. Given noise distribution 𝑟(• |•) and probability 𝜌 define ︀ 𝜚(̃︀ 𝑣) = ∫︁ 𝑟(̃︀ 𝑣|𝑣)𝜚(𝑣) 𝑑𝑣. If S is proper, then ̃︀ S(𝜌, ̃︀ 𝑣) := S(̃︀ 𝜌, ̃︀ 𝑣) is proper with respect to 𝑟. Proof. We have E ̃︀ 𝑣∼̃︀ 𝜚 [︀ ̃︀ S(𝜌, ̃︀ 𝑣) ]︀ = E ̃︀ 𝑣∼̃︀ 𝜚 [︀ S(̃︀ 𝜌, ̃︀ 𝑣) ]︀ ≥ E ̃︀ 𝑣∼̃︀ 𝜚 [︀ S(̃︀ 𝜚, ̃︀ 𝑣) ]︀ = E ̃︀ 𝑣∼̃︀ 𝜚 [︀ ̃︀ S(𝜚, ̃︀ 𝑣) ]︀ , where the second line follows from the propriety of S. The energy distance is overviewed in [408] . Relationships between energy distance and MMD may be found in [386] . An efficient algorithm to compute gradients of the energy distance is provided in [205] . Relating the characteristic property (and universal property of positive definite kernels) to mean embedding of measures is the subject of [397] . An influential paper in the use of of probabilistic scoring rules is [174] and the recent review article [434] overviews the current state of the art and reflects the growing use of these ideas. An important consideration for sample-based metrics is their statistical consistency with empirical measures and the associated rates of convergence to their population-level limits. The paper [440] shows that the sample-averaged Wasserstein-𝑝 metric, for 𝑝 ≥ 1, scales poorly with respect to dimension: E[𝑊 𝑝 (𝜈, 𝜈 𝑛 )] = 𝒪(𝑛 -1/𝑑 ). On the other hand, the MMD with appropriate choices of the kernel only depends on the mean embedding of the measures in the RKHS and thus converges at the rate 𝒪(𝑛 -1/2 ), independent of the dimension of the data; see [184, 396] for more details on MMD convergence. The CRPS was first introduced in [77] and [296] . The relationship between the quantile score and the CRPS stated in Lemma 11.58 can be found in [256] ; see also [142, 175] . The discussion of bias in the ensemble CRPS is given in [154] . Closed-form expressions for the CRPS for various distributions are given in [231] , and references to some additional known expressions are given in Table 9 .7 of [445] . The explicit formula for a univariate Gaussian in Example 11.53 was established in [176] . Although not employed here, we mention the Fréchet inception distance from [206] which is widely used to evaluate image generation. RKHSs have become a useful framework to understand and develop theoretical results for machine learning methods. The background and properties of RKHSs can be found in [51, 390] ; see also [312] for a broad survey on kernel mean embeddings. The Moore-Aronszajn theorem first appeared in Aronszajn's article [24] , where he attributes it to Moore. Some applications of RKHSs in machine learning include comparing distributions based on two-sample tests from estimators for the maximum-mean discrepancy [184] and measuring (conditional) dependence between random variables [183] . The statistical properties of MMD have also made it a popular metric for generative modeling [168] . The proof of Lemma 11.13, containing a precise characterization of when IPMs define a metric on the space of probability measures, can be found in [313, 396] . The paper [398] shows that the total variation distance is the only f-divergence that is also an IPM. A framework for scoring rules in the presence of observation error in the verification is given in [145] . An extensive discussion of scoring rules, with focus on meteorological applications, is given in Chapter 9 of [445] . The relationship between the CRPS and the spread-error relationship is discussed in [275] . The identity (11.25 ) is proved by using Proposition 2 in [408] . Another quantity that is widely used in the verification of spatial fields, particularly in weather forecasting, is the anomaly correlation coefficient [316, 445] . Chapter 12 Supervised Learning This chapter is concerned with the supervised learning task of approximating, from data, a function 𝜓 † : 𝐷 → 𝑅, 𝐷 the domain and 𝑅 the range. Supervised learning refers specifically to learning 𝜓 † from data given in the form of input-output pairs of 𝜓 † . In this chapter we focus on the regression problem in which the domain 𝐷 ⊆ R 𝑑 and range 𝑅 ⊆ R are uncountable subsets of R 𝑑 and R, respectively; the methods we introduce are readily generalized to the case 𝑅 ⊆ R 𝑚 for any integer 𝑚. When the range of 𝜓 † has finite cardinality we refer to the function approximation problem as classification. Classification played a central role in the historical development of supervised learning, and is still one of its major uses. We, however, focus on regression: this is a task that arises naturally in the context of learning forward maps for inverse problems and learning dynamical systems for data assimilation; in particular learning emulators. Our data assumption is as follows: Data Assumption 12.1. Let 𝐷 ⊆ R 𝑑 , 𝑅 ⊆ R and 𝜓 † : 𝐷 → 𝑅. Let {𝜉 (𝑛) } 𝑁 𝑛=1 be i.i.d. drawn from a mean zero noise process taking values in R and let 𝜆 ≥ 0. Data is available in the form {︁ 𝑢 (𝑛) , 𝑦 (𝑛)   }︁ 𝑁 𝑛=1 , 𝑦 (𝑛) = 𝜓 † (𝑢 (𝑛) ) + √ 𝜆𝜉 (𝑛) , (12.1) where the {𝑢 (𝑛) } 𝑁 𝑛=1 are generated i.i.d. from probability density function ϒ(𝑢), supported on 𝐷, and independently of the draws from the noise process. The parameter 𝜆 ≥ 0 allows us to consider noiseless data (𝜆 = 0) and noisy data (𝜆 > 0). Since the goal is to determine 𝜓 † we are concerned with function approximation, and to that end we will study three approaches to parameterize and learn functions: neural networks (Section 12.2), random features (Section 12.3), and Gaussian processes (Section 12.4). We will concentrate on the noiseless case 𝜆 = 0, but will briefly mention the noisy case 𝜆 > 0 in the context of random features and Gaussian processes. Before considering these methods of function approximation we introduce some notational conventions that will be useful in this and the following chapter. 12.1 Notational Conventions Definition 12.2. We let 𝐻 ϒ denote the Hilbert space of real-valued functions on 𝐷 with inner-product and induced norm ⟨𝜓, 𝜙⟩ 𝐻 ϒ = ∫︁ 𝐷 𝜓(𝑢)𝜙(𝑢)ϒ(𝑢) 𝑑𝑢, |𝜓| 2 𝐻 ϒ = ⟨𝜓, 𝜓⟩ 𝐻 ϒ . ♢ Thus the induced norm on 𝐻 ϒ is given more explicitly by |𝜓| 𝐻 ϒ = (︂∫︁ 𝐷 𝜓(𝑢) 2 ϒ(𝑢) 𝑑𝑢 )︂ 1/2 . (12.2) Definition 12.3. Given the data from Data Assumption 12.1 we define the empirical density ϒ 𝑁 (𝑢) = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿(𝑢 -𝑢 (𝑛) ). ♢ The probability density function ϒ is thus approximated by the empirical density ϒ 𝑁 . We may then define an empirical approximation of |𝜓| 𝐻 ϒ by replacing ϒ by ϒ 𝑁 in (12.2) to obtain |𝜓| 𝐻 ϒ N = (︃ 1 𝑁 𝑁 ∑︁ 𝑛=1 𝜓(𝑢 (𝑛) ) 2 )︃ 1/2 . Neural Networks We define neural networks acting as real-valued maps on finite dimensional Euclidean space. For simplicity we work, throughout this section, under Data Assumption 12.1, considering only the noise-free setting 𝜆 = 0. To define a neural network we first define: The Exponential Linear Unit (ELU) activation function is obtained from the SELU by setting 𝜅 = 1. ♢ A neural network is a parametric family of functions found by composing activation functions with affine functions. We introduce Θ ⊆ R 𝑑 𝜃 and then 𝜓 : R 𝑑 × Θ → R via the iteration 𝜓 0 (𝑢; 𝜃) = 𝑢, (12.4 ) 𝜓 (ℓ+1) (𝑢; 𝜃) = 𝜎 (︀ 𝑊 ℓ 𝜓 (ℓ) (𝑢; 𝜃) + 𝑏 ℓ )︀ , ℓ = 0, . . . , 𝐿 -1, (12.5) 𝜓(𝑢; 𝜃) = 𝛽 ⊤ 𝜓 (𝐿) (𝑢; 𝜃), (12.6) where, for ℓ ∈ {0, . . . , 𝐿 -1}, 𝑊 ℓ ∈ R 𝑑 ℓ+1 ×𝑑 ℓ and 𝑏 ℓ ∈ R 𝑑 ℓ+1 , with 𝑑 0 = 𝑑 and 𝛽 ∈ R 𝑑 𝐿 ; together the matrices {𝑊 ℓ } 𝐿- foot_14 ℓ=0 and vectors {𝑏 ℓ } 𝐿-1 ℓ=0 define 𝜃 (and 𝑑 𝜃 ). Indeed here we define 𝜗 = {𝑊 ℓ , 𝑏 ℓ } 𝐿-1 ℓ=0 and 𝜃 = (𝜗, 𝛽). Note that 𝜓 (ℓ) : R 𝑑 × Θ → R 𝑑 ℓ . Resulting function 𝜓 : R 𝑑 × Θ → R is known as a (deep 1 ) neural network. The reader will note that the concept is readily generalized to 𝜓 : R 𝑑 × Θ → R 𝑞 for any integer 𝑞; we concentrate on 𝑞 = 1 for simplicity of exposition only. By use of the data, a value of the parameter defining the affine functions may be chosen so as to determine an approximation of 𝜓 † . An idealized approach to determining 𝜃 is to minimize the risk J(𝜃) := |𝜓 † -𝜓(•; 𝜃)| 2 𝐻 ϒ (12.7) by setting 𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃). (12.8) However, this requires knowing the probability density function ϒ and the function 𝜓 † , neither of which are known. What we do have access to is the data set in Data Assumption 12.1 which contains implicit information about both ϒ and 𝜓 † . Thus the optimal parameter 𝜃 ⋆ is chosen to minimize the empirical risk J 𝑁 (𝜃) := |𝜓 † -𝜓(•; 𝜃)| 2 𝐻 ϒ N . Notice that, since we have assumed that 𝜆 = 0, J 𝑁 (𝜃) = 1 𝑁 𝑁 ∑︁ 𝑛=1 |𝑦 (𝑛) -𝜓(𝑢 (𝑛) ; 𝜃)| 2 . (12.9) Remark 12.6. We have derived this loss function J 𝑁 (•) under Data Assumption 12.1 with 𝜆 = 0. However it may also be derived under the more general assumption that data set (𝑢 (𝑛) , 𝑦 (𝑛) ) is drawn i.i.d. from a probability measure 𝜋 on 𝒫(R 𝑑 × R 𝑘 ). Examples of such measures may be found from Data Assumption 12.1 for any 𝜆 ≥ 0, but the setting is much more general. The risk may then be written as J(𝜃) := E (𝑢,𝑦)∼𝜋 |𝑦 -𝜓(𝑢; 𝜃)| 2 and approximated empirically to again obtain (12.9). ♢ Minimizing (12.9) leads to an implementable strategy for function approximation. The empirical risk is typically non-convex as a function of 𝜃, with multiple saddle points and local minima. Robust software to perform the minimization via the use of stochastic gradient descent (Chapter 15) is readily available. Once 𝜃 ⋆ is obtained (typically an approximation of the global minimizer), we write 𝜓 ⋆ (𝑢) := 𝜓(𝑢; 𝜃 ⋆ ). Remark 12.7. We derived (12.9) by approximating (12.7) empirically. If the objective functional J(•) in (12.7) is viewed as a function of 𝜓, rather than 𝜃, then we say that the optimization problem is non-parametric. It is interesting to note that this problem is convex in 𝜓. This fact is often used to motivate the empirical observation that the parametric optimization problem (12.9) becomes simpler when over-parameterized. ♢ Random Features In this section we introduce the idea of approximation by random features, and in so doing we make a link between neural networks and Gaussian processes, the objects of study (respectively) in the preceding and following sections. We continue to work under Data Assumption 12.1 in the setting 𝜆 = 0. To motivate our eventual formulation of random features, we consider a thought experiment in which we again use the class of functions defined by the neural networks (12.4) . Now, rather than optimizing over all of Θ, the parameters 𝜗 = {𝑊 ℓ , 𝑏 ℓ } 𝐿-1 ℓ=0 are fixed and optimization is performed only over 𝛽 ∈ R 𝑑 𝐿 . Note that 𝜓 (𝐿) depends only on 𝜗 and not 𝛽. Thus we modify notation and write 𝜓 (𝐿) (•; 𝜗) : R 𝑑 → R 𝑑 𝐿 . We then define 𝜙 𝑖 (•; 𝜗) : R 𝑑 → R to be the 𝑖 𝑡ℎ component, for 𝑖 = 1, . . . , 𝑑 𝐿 , of the vector-valued output function 𝜓 (𝐿) (•; 𝜗). We can then write 𝜓(𝑢; 𝜗, 𝛽) := 𝑑 𝐿 ∑︁ 𝑖=1 𝛽 𝑖 𝜙 𝑖 (𝑢; 𝜗), (12.10) viewing 𝜓(•; 𝜗, 𝛽) as parameterized by 𝛽, since the remaining elements 𝜗 of 𝜃 have been fixed. From 𝜓 † we may define 𝛽 ⋆ ∈ arg min 𝛽∈R 𝑑 𝐿 |𝜓 † -𝜓(•; 𝜗, 𝛽)| 2 𝐻 ϒ N = arg min 𝛽∈R 𝑑 𝐿 𝑁 ∑︁ 𝑛=1 |𝑦 (𝑛) -𝜓(𝑢 (𝑛) ; 𝜗, 𝛽)| 2 and 𝜓 ⋆ (𝑢) := 𝜓(𝑢; 𝜗, 𝛽 ⋆ ). In contrast to full optimization over 𝜃, this leads to a convex, indeed quadratic, optimization problem that can be readily solved. So far we have simply fixed 𝜗; we have not discussed how to choose it. One natural approach is to simply pick 𝜗 at random from some probability measure. The idea behind this is that, if 𝑑 𝐿 is large, the collection of random functions {𝜙 𝑖 (•; 𝜗)} 𝑑 𝐿 𝑖=1 may be sufficiently expressive for the function approximation task at hand; whether or not it is sufficiently expressive will depend on the parameterization of 𝜙(•; 𝜗) and on the support and distribution of the probability measure from which 𝜗 is drawn. With our current construction, the resulting collection of functions {𝜙 𝑖 } 𝑑 𝐿 𝑖 are then random but not, in general, i.i.d. It is intuitive that, given 𝑑 𝐿 , working with an i.i.d. set of random functions will be more efficient and so we discuss a variant on the preceding construction that enforces the i.i.d. property on the {𝜙 𝑖 } 𝑑 𝐿 𝑖 . To realize this construction we consider 𝑞 ∈ 𝒫(R 𝑑 𝜗 ) and define 𝜙 : R 𝑑 × R 𝑑 𝜗 → R. We then set 𝜓(𝑢; 𝛽) := 𝑑 𝐿 ∑︁ 𝑖=1 𝛽 𝑖 𝜙(𝑢; 𝜗 𝑖 ), 𝜗 𝑖 ∼ 𝑞, i.i.d. (12.11) We may again employ empirical risk minimization to determine vector 𝛽 in (12.11) . To this end we introduce a regularized quadratic loss function and then 𝛽 ⋆ is defined via 𝛽 ⋆ ∈ arg min 𝛽∈R 𝑑 𝐿 J 𝑁,reg (𝛽), (12.12a) J 𝑁,reg (𝛽) := 1 2 𝑁 ∑︁ 𝑛=1 |𝑦 (𝑛) -𝜓(𝑢 (𝑛) ; 𝛽)| 2 + 𝜆 reg 2 |𝛽| 2 . (12.12b) Here 𝜆 reg > 0 is the regularization parameter. The regularization is quadratic and hence does not change the simplicity of the optimization problem for 𝛽 : it still results in solution of a linear system. Furthermore the specific regularization arises naturally in the context of noisy data 𝜉 (𝑛) ∼ 𝒩 (0, 𝐼) in (12.1) -see Remark 12.13. Finally we observe that, since 𝜆 reg > 0, the system of linear equations is guaranteed to have a unique solution. This we now prove. Theorem 12.8. Assume that 𝜆 reg > 0. Then the linear system for 𝛽 defined by the optimization problem (12.12) takes the form 𝐵𝛽 = 𝑟 where 𝐵 is strictly positive-definite. Proof. Recall empirical meausre ϒ 𝑁 from Definition 12.3. The (𝑖, 𝑗) entry of matrix 𝐵 has the form 𝐵 𝑖𝑗 = 𝑁 E 𝑢∼ϒ 𝑁 𝜙(𝑢; 𝜗 𝑖 )𝜙(𝑢; 𝜗 𝑗 ) + 𝜆 reg 𝛿 𝑖𝑗 , where 𝛿 𝑖𝑗 is the Kronecker delta. For each 𝑢 ∈ R 𝑑 define 𝜙(𝑢) ∈ R 𝑑 𝐿 to be the vector with 𝑖 𝑡ℎ entry 𝜙(𝑢; 𝜗 𝑖 ). From this it follows that ⟨𝑧, 𝐵𝑧⟩ R 𝑑 𝐿 = 𝑁 E 𝑢∼ϒ 𝑁 |⟨𝜙(𝑢), 𝑧⟩ R 𝑑 𝐿 | 2 + 𝜆 reg |𝑧| 2 R 𝑑 𝐿 ≥ 𝜆 reg |𝑧| 2 R 𝑑 𝐿 . Thus symmetric matrix 𝐵 is strictly positive definite since we have shown that all eigenvalues are bounded below by 𝜆 reg > 0. We refer to the functions 𝜙(•; 𝜗), with 𝜗 ∼ 𝑞 i.i.d., as random features. Similarly to before we may write 𝜓 ⋆ (𝑢) := 𝜓(𝑢; 𝛽 ⋆ ). where 𝜗 = (𝜔, 𝑏) is chosen at random. It is natural to take 𝑏 ∼ 𝑈 [0, 2𝜋], and we assume 𝜔 ∼ 𝑊, for some 𝑊 ∈ 𝒫(R 𝑑 ). ♢ Example 12.10. Each 𝜙(•; 𝜗 𝑖 ) may be chosen as a neural network, for example, with the parameters of that neural network chosen i.i.d. at random from given distribution 𝑞. ♢ The spatial correlation in the random features is a natural object to study; indeed we will see that it leads to a connection between random features and Gaussian process regression. To this end, we introduce kernel (recall Definition 11.17) 𝑐(𝑢, 𝑢 ′ ) := E 𝑞 [︀ 𝜙(𝑢; 𝜗)𝜙(𝑢 ′ ; 𝜗) ]︀ , ( 12.13) where expectation is over 𝜗 ∼ 𝑞. Example 12.11. Consider Example 12.9. We note that 𝑐(𝑢, 𝑢 ′ ) = ∫︁ R 𝑑 𝐼(𝜔)𝑊 (𝜔) 𝑑𝜔, 𝐼(𝜔) = 1 2𝜋 ∫︁ 2𝜋 0 cos (︀ ⟨𝜔, 𝑢⟩ + 𝑏 )︀ cos (︀ ⟨𝜔, 𝑢 ′ ⟩ + 𝑏 )︀ 𝑑𝑏 = 1 2 cos (︀ ⟨𝜔, 𝑢 -𝑢 ′ ⟩ )︀ . Thus the resulting kernel depends only on 𝑢 -𝑢 ′ : 𝑐(𝑢, 𝑢 ′ ) = c(𝑢 -𝑢 ′ ) for some c : R → R. We say that the random feature is stationary. ♢ Gaussian Processes In this section we introduce Gaussian processes. We link them to random features, through the notions of kernel (Definition 11.17) and RKHS (Definition 11.19), in Subsection 12.4.1; we study regression with Gaussian processes in Subsection 12.4.2. Kernels And RKHSs In the theory of Gaussian process regression, kernels are used to specify the covariance function of a Gaussian process model; the covariance function quantifies spatial correlations of the process and determines a choice of RKHS in which to formulate the regression problem. In this subsection we show, under mild assumptions, an explicit construction of the RKHS associated with the covariance function of a Gaussian process on a compact domain. In the following Subsection 12.4.2 we formulate and study Gaussian process regression in this RKHS. Let 𝐷 ⊂ R 𝑑 be compact and let 𝑐 be a symmetric, non-negative definite and continuous kernel, interpreted as the covariance function of a Gaussian process. The integral operator 𝒞 : 𝐿 2 (𝐷) → 𝐿 2 (𝐷) given by (𝒞𝜙)(𝑢) = ∫︁ 𝐷 𝑐(𝑢, 𝑢 ′ )𝜙(𝑢 ′ ) 𝑑𝑢 ′ , 𝜙 ∈ 𝐿 2 (𝐷), (12.14) is called the covariance operator with kernel 𝑐. As discussed in the bibliography Section 12.6, our assumptions on 𝑐 imply that the covariance operator admits eigenpairs (𝜎 𝑖 , 𝜙 𝑖 ) ∞ 𝑖=1 satisfying 𝒞𝜙 𝑖 = 𝜎 𝑖 𝜙 𝑖 , where the eigenvalues (𝜎 𝑖 ) ∞ 𝑖=1 are non-negative, and, without loss of generality, decreasingly ordered; and where the eigenfunctions (𝜙 𝑖 ) ∞ 𝑖=1 form an orthonormal basis of 𝐿 2 (𝐷). Furthermore, Mercer's theorem (see the bibliography Section 12.6) ensures that the eigenfunctions corresponding to non-zero eigenvalues are continuous on 𝐷 (which implies that pointwise evaluations are well defined) and that the covariance function can be represented using the eigenpairs of 𝒞 as follows: 𝑐(𝑢, 𝑢 ′ ) = ∞ ∑︁ 𝑖=1 𝜎 𝑖 𝜙 𝑖 (𝑢)𝜙 𝑖 (𝑢 ′ ), 𝑢, 𝑢 ′ ∈ 𝐷, (12.15) where the convergence is absolute and uniform. This representation will be repeatedly used in what follows. Next, we seek to explicitly characterize the RKHS corresponding to kernel 𝑐 using again the eigenpairs of 𝒞. To that end, let ⟨•, •⟩ 𝐿 2 denote the inner product in 𝐿 2 (𝐷), and define 𝒦 := {︂ 𝑓 ∈ 𝐿 2 (𝐷) : ∞ ∑︁ 𝑖=1 ⟨𝑓, 𝜙 𝑖 ⟩ 2 𝐿 2 𝜎 𝑖 < ∞ }︂ . The set 𝒦 equipped with inner product ⟨•, •⟩ 𝒦 as now defined is a Hilbert space: ⟨𝑓, 𝑔⟩ 𝒦 = ∞ ∑︁ 𝑖=1 ⟨𝑓, 𝜙 𝑖 ⟩ 𝐿 2 ⟨𝑔, 𝜙 𝑖 ⟩ 𝐿 2 𝜎 𝑖 , 𝑓, 𝑔 ∈ 𝒦. The following proposition shows that 𝒦 is an RKHS with reproducing kernel 𝑐. Proposition 12.12. For 𝑓 ∈ 𝒦 and 𝑢 ∈ R 𝑑 , it holds that ⟨𝑓, 𝑐(𝑢, •)⟩ 𝒦 = 𝑓 (𝑢). (12.16) Consequently, 𝒦 is an RKHS with reproducing kernel 𝑐. Proof. We have ⟨𝑓, 𝑐(𝑢, •)⟩ 𝒦 = ∞ ∑︁ 𝑖=1 ⟨𝑓, 𝜙 𝑖 ⟩ 𝐿 2 ⟨𝑐(𝑢, •), 𝜙 𝑖 ⟩ 𝐿 2 𝜎 𝑖 . Note that, using (12.15), we have ⟨𝑐(𝑢, •), 𝜙 𝑖 ⟩ 𝐿 2 = ∞ ∑︁ 𝑗=1 𝜎 𝑗 𝜙 𝑗 (𝑢)⟨𝜙 𝑗 , 𝜙 𝑖 ⟩ 𝐿 2 = 𝜎 𝑖 𝜙 𝑖 (𝑢), (12.17) where the last equality follows since the (𝜙 𝑖 ) ∞ 𝑖=1 are orthonormal in 𝐿 2 (𝐷). Hence, ⟨𝑓, 𝑐(𝑢, •)⟩ 𝒦 = ∞ ∑︁ 𝑖=1 ⟨𝑓, 𝜙 𝑖 ⟩ 𝐿 2 𝜎 𝑖 𝜙 𝑖 (𝑢) 𝜎 𝑖 = 𝑓 (𝑢). Regression We work under Data Assumption 12.1 in the setting 𝜆 > 0. We assume that 𝒦 is an RKHS with reproducing kernel 𝑐, as constructed in the previous subsection, and seek to find approximation to 𝜓 † , function 𝜓 ⋆ , through the following minimization problem: 𝜓 ⋆ ∈ arg min 𝜓∈𝒦 J 𝑁 (𝜓), (12.18a) J 𝑁 (𝜓) := 1 2 𝑁 ∑︁ 𝑛=1 |𝑦 (𝑛) -𝜓(𝑢 (𝑛) )| foot_15 + 𝜆 2 ‖𝜓‖ 2 𝒦 . (12.18b) Remark 12.13. This infinite dimensional optimization problem may be viewed as a function space version of (12.9) where we minimize over functions 𝜓 ∈ 𝒦, rather than parameter 𝜃 ∈ Θ ⊂ R 𝑑 , and where we have allowed for observational noise; that is, 𝜆 > 0. We thus see a connection between neural network and Gaussian process regression. Furthermore, the minimization problems (12.12), from random features based regression, and (12.18) from Gaussian process regression, are also connected. To see this assume that, for 𝜗 𝑖 ∼ 𝑞 i.i.d., we have 𝜙(𝑢; 𝜗 𝑖 ) ∈ 𝒦 for all 𝑖 ∈ {1, . . . , 𝑑 𝐿 }. If the RKHS 𝒦 in (12.18) is replaced by the RKHS 𝒦 𝑑 𝐿 = {︁ 𝜓 ∈ 𝒦 : 𝜓(𝑢; 𝛽) := 𝑑 𝐿 ∑︁ 𝑖=1 𝛽 𝑖 𝜙(𝑢; 𝜗 𝑖 ), 𝛽 𝑖 ∈ R ∀𝑖 ∈ {1, . . . , 𝑑 𝐿 } }︁ , then the optimization reduces to one over 𝛽 ∈ R 𝑑 𝐿 and it may be shown that this yields (12.12). Thus (12.12) may be viewed as approximation of (12.18) via the Monte Carlo approximation 𝒦 𝑑 𝐿 of the RKHS 𝒦. ♢ Remark 12.14. The minimization problem (12.18) may be derived by application of a generalization of Bayes Theorem 1.3, 2 from R 𝑑 to 𝐿 2 (𝐷), and then a generalization of the notion of MAP estimator -Definition 1.9 -also from R 𝑑 to 𝐿 2 (𝐷). The Bayesian inverse problem is defined as follows. We place as prior on 𝜓 a Gaussian random field with mean zero and non-negative definite continuous covariance function 𝑐(𝑢, 𝑢 ′ ); this covariance function can be equivalently seen as specifying a Gaussian measure 𝒩 (0, 𝒞) in 𝐿 2 (𝐷), where 𝒞 is the covariance operator given by (12.14) . References that discuss how Gaussian priors on function space can be equivalently specified using the covariance function (random field perspective) and the covariance operator (Gaussian measure perspective) are discussed in the bibliography Section 12.6. The likelihood is defined by assuming noisy data (𝜆 > 0) in (12.1) , where 𝜉 (𝑛) ∼ 𝒩 (0, 𝐼) i.i.d. Then the negative log-likelihood is given by 1 2𝜆 𝑁 ∑︁ 𝑛=1 |𝑦 (𝑛) -𝜓(𝑢 (𝑛) )| 2 . Because the prior on 𝑢 is Gaussian and the log-likelihood is quadratic in 𝑢, the generalization of Bayes Theorem 1.3 shows that the posterior is conjugate to the prior and is Gaussian. The posterior mean, which is also a form of MAP estimator, then satisfies (12.18) . We remark that minimization of (12.12) also has a similar interpretation as a MAP estimator, but with respect to a Gaussian process with mean zero and covariance function a random approximation of 𝑐(𝑢, 𝑢 ′ ). ♢ The infinite dimensional optimization problem over 𝒦 has the following remarkable property, which is often referred to as the representer theorem, and demonstrates that the optimization problem is intrinsically finite dimensional: Furthermore, the coefficients 𝛼 ⋆ solve the following quadratic minimization problem: 𝛼 ⋆ ∈ arg min 𝛼∈R 𝑁 A 𝑁 (𝛼), A 𝑁 (𝛼) := 1 2 𝑁 ∑︁ 𝑟=1 ⃒ ⃒ ⃒𝑦 (𝑟) - 𝑁 ∑︁ 𝑛=1 𝛼 𝑛 𝑐(𝑢 (𝑟) , 𝑢 (𝑛) ) ⃒ ⃒ ⃒ 2 + 𝜆 2 𝑁 ∑︁ 𝑛,𝑟=1 𝛼 𝑛 𝛼 𝑟 𝑐(𝑢 (𝑟) , 𝑢 (𝑛) ). Remark 12.16. In the context of supervised learning, notice that the preceding theorem shows that the optimization problem (12.18) is in fact finite dimensional in nature, despite its infinite dimensional formulation. Note, furthermore, the key fact that solution to the optimization problem is defined entirely by knowledge of the kernel 𝑐 and by the data {︁ 𝑢 (𝑛) , 𝑦 (𝑛)   }︁ 𝑁 𝑛=1 . ♢ Proof of Theorem 12.15. Using the reproducing property (12.16), the objective function J 𝑁 in (12.18) can be expressed as J 𝑁 (𝜓) = 1 2 𝑁 ∑︁ 𝑛=1 |𝑦 (𝑛) -𝜓(𝑢 (𝑛) )| 2 + 𝜆 2 ‖𝜓‖ 2 𝒦 = 1 2 𝑁 ∑︁ 𝑛=1 |𝑦 (𝑛) -⟨𝑐(•, 𝑢 (𝑛) ), 𝜓⟩ 𝒦 | 2 + 𝜆 2 ‖𝜓‖ 2 𝒦 Writing J 𝑁 (𝜓+ℎ) = J 𝑁 (𝜓)+⟨𝐷J 𝑁 (𝜓), ℎ⟩ 𝒦 +𝒪(‖ℎ‖ 2 𝒦 ) identifies the variational derivative 𝐷J 𝑁 (𝜓) of J 𝑁 , evaluated at any point 𝜓. Setting this derivative to zero at minimizer (𝑛) ). This shows that 𝜓 ⋆ (𝑢) is in the linear span of {𝑐(•, 𝑢 (𝑛) )} 𝑁 𝑛=1 , establishing the first part of the result. 𝜓 = 𝜓 ⋆ gives 𝜆𝜓 ⋆ (𝑢) = 𝑁 ∑︁ 𝑛=1 (︀ 𝑦 (𝑛) -⟨𝑐(•, 𝑢 (𝑛) ), 𝜓 ⋆ ⟩ 𝒦 )︀ 𝑐(•, 𝑢 (𝑛) ) = 𝑁 ∑︁ 𝑛=1 (︀ 𝑦 (𝑛) -𝜓 ⋆ (𝑢 (𝑛) ) )︀ 𝑐(•, 𝑢 For the second part, let J 𝑁 (𝜓), (12.20a) 𝒦 J 𝑁 (𝜓) := 1 2 𝑁 ∑︁ 𝑛=1 |𝑦 (𝑛) -𝜓(𝑢 (𝑛) )| 2 + 𝜆 2 ‖𝜓‖ 2 𝒦 . (12.20b) Because of the established form (12.19) of the minimizer 𝜓 ⋆ of (12.18) we see that 𝜓 ⋆ will also be the minimizer of (12.20) . Finally, using (12.17) ‖𝜓‖ 2 𝒦 = ∞ ∑︁ 𝑖=1 1 𝜎 𝑖 ⟨ 𝑁 ∑︁ 𝑛=1 𝛼 𝑛 𝑐(•, 𝑢 (𝑛) ), 𝜙 𝑖 ⟩ 𝐿 2 ⟨ 𝑁 ∑︁ 𝑟=1 𝛼 𝑟 𝑐(•, 𝑢 (𝑟) ), 𝜙 𝑖 ⟩ 𝐿 2 = ∞ ∑︁ 𝑖=1 1 𝜎 𝑖 𝑁 ∑︁ 𝑛=1 𝛼 𝑛 𝜎 𝑖 𝜙 𝑖 (𝑢 (𝑛) ) 𝑁 ∑︁ 𝑟=1 𝛼 𝑟 𝜎 𝑖 𝜙 𝑖 (𝑢 (𝑟) ) = 𝑁 ∑︁ 𝑛,𝑟=1 𝛼 𝑛 𝛼 𝑟 ∞ ∑︁ 𝑖=1 𝜎 𝑖 𝜙 𝑖 (𝑢 (𝑛) )𝜙 𝑖 (𝑢 (𝑟) ) = 𝑁 ∑︁ 𝑛,𝑟=1 𝛼 𝑛 𝛼 𝑟 𝑐(𝑢 (𝑛) , 𝑢 (𝑟) ), where the last equality follows from (12.15) . This concludes the proof. We define 𝐶 ∈ R 𝑁 ×𝑁 to be the matrix with entries 𝐶 𝑛𝑟 = 𝑐(𝑢 (𝑛) , 𝑢 (𝑟) ) and 𝑆 to be its inverse, with entries 𝑆 𝑛𝑟 ; notice that 𝐶 is positive definite (and hence invertible, so that 𝑆 is defined) provided that the kernel 𝑐 is positive definite. Then we have: Corollary 12.17. If 𝑝 ⋆ ∈ R 𝑁 is vector with entries 𝑝 ⋆ 𝑛 = 𝜓 ⋆ (𝑢 (𝑛) ), then 𝑝 ⋆ = 𝐶𝛼 ⋆ and 𝑝 ⋆ ∈ arg min 𝑝∈R 𝑁 J 𝑁 (𝑝), J 𝑁 (𝑝) := 1 2 𝑁 ∑︁ 𝑛=1 |𝑦 (𝑛) -𝑝 𝑛 | 2 + 𝜆 2 𝑁 ∑︁ 𝑛,𝑟=1 𝑝 𝑛 𝑝 𝑟 𝑆 𝑛,𝑟 . Approximation Properties All of the three methods described in the three preceding sections can, in principle, approximate continuous functions to arbitrary accuracy, over a compact set. Doing so requires sufficient data. Furthermore, in the case of neural networks, this also requires sophisticated optimization techniques to obtain close to optimal solutions; in contrast, the random features and Gaussian process approaches only require solution of linear systems, resulting from a quadratic optimization problem. The methods may be used to approximate sufficiently regular functions 𝜓 † . Typical instances of the resulting approximation theory for the three supervised learning techniques take the following form. Assume that 𝐷 ⊂ R 𝑑 is bounded and open. Let B be a compact set in 𝐶(𝐷, R). Then, for given 𝛿 > 0, there is a volume of data 𝑁 and parametric choice in the supervised learning technique such that, with high probability with respect to the data, sup 𝜓 † ∈B sup 𝑢∈𝐷 |𝜓 † (𝑢) -𝜓 ⋆ (𝑢)| < 𝛿. (12.21) Theory establishing results of this form is cited in the bibliography which follows. Bibliography The subject of neural network function approximation, the core task of supervised learning, is overviewed in [127] . Universal approximation theorems may be found in [117, 341] . The use of random features was popularized as a methodology, and analyzed rigorously, in the collection of papers [350, 351, 352] . Gaussian processes are described in [446] ; in that text the mean function and covariance function characterize the Gaussian probability measure on function space. In [64] Gaussian probability measure on separable Banach space 𝑋 is defined by asking that all linear functionals are Gaussian. Subsequently the mean, viewed as a point in 𝑋, and the covariance operator, mapping 𝑋 * into 𝑋, may be defined. In this Gaussian measure perspective, the RKHS is referred to as the Cameron-Martin space. Section 4 of the lecture notes [191] provides a concise, readable, introduction to the subject of Gaussian measure on separable Banach space. Kernel-based methods more generally are described in [329] . Error estimates for interpolation using Gaussian processes are developed in [442] . The link between regression and Bayesian inversion with Gaussian process priors is developed in depth in [114, 435] . Mercer's theorem is established in [304] and it can be now found in numerous textbooks; see for example [134] , which also contains additional background on the spectral decomposition of the covariance operator used in Subsection 12.4.1, and [404] in the context of uncertainty quantification. In RKHS theory, the spectral decomposition characterizes the RKHS of symmetric, positive definite continuous kernels, as discussed in this chapter. Mercer's theorem is also useful in the theory of integral equations and in the study of stochastic processes, where it underpins the derivation of Karhunen-Loève expansions [336] . A definition of MAP estimator in infinite dimensions, appropriate for the discussion in Remark 12.14 may be found in [120] ; see also [260, 249, 254, 30] for related discussion. We have articulated a specific link between neural networks and random features; a related concept underpins the subject of the neural tangent kernel [229] . The use of various machine-learning inspired approximation methods in the solution of PDEs may be found in [354, 101, 318, 252] . Chapter 13 Unsupervised Learning And Generative Modeling This chapter is concerned with the subject of unsupervised learning. The following data assumption is made: Data Assumption 13.1. We have available data in the form 𝑈 := {𝑢 (𝑛) } 𝑁 𝑛=1 , ( 13.1) assumed to be drawn i.i.d. from (unknown) probability density function ϒ on R 𝑑 . This should be compared with the supervised learning Data Assumption 12.1 (or the generalization discussed in Remark 12.6), where we are given input-output pairs, linked through an unknown function. In this chapter, through the Data Assumption 13.1, we have access to the empirical density (see Definition 12.3) ϒ 𝑁 (𝑢) = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑢 (𝑛) (𝑢). (13. 2) The goal is to understand the data set summarized by measure ϒ 𝑁 . One widely used approach is to study clustering within the data; we will not pursue this important topic because it is not of direct relevance to the solution of inverse problems and data assimilation, our focus in these notes. Instead we pursue the approach of generative modeling: we try and create new samples from ϒ, given ϒ 𝑁 . We illustrate the idea of generative modeling with two examples. Example 13.2. A natural starting point for generative modeling is to discuss density estimation. This methodology seeks to approximate ϒ(𝑢), from within some class of densities, on the basis of the data summarized in ϒ 𝑁 (𝑢). The goal is not always generative modeling: it may be to have a smooth density which can be differentiated or otherwise manipulated in a way that ϒ 𝑁 (𝑢) cannot. However, some classes of approximate ϒ can be used to generate new samples. An example is when the approximation is a Gaussian mixture. ♢ Example 13.3. We introduce the measure transport task of determining function 𝑔 : R 𝑑𝑧 → R 𝑑 so that, given probability density function 𝜁 on R 𝑑𝑧 and probability density function ϒ on R 𝑑 , 𝑔 ♯ 𝜁 = ϒ; here 𝑔 ♯ denotes the pushforward operation 1 and so, explicitly, we are seeking 𝑔 so that, if 𝑧 ∼ 𝜁, then 𝑔(𝑧) ∼ ϒ. The task of identifying 𝑔 must be undertaken empirically, since only ϒ 𝑁 is available to us, not ϒ. A common setting is to assume that 𝜁 is Gaussian. Then the aim is to find map 𝑔 so that samples from Gaussian 𝜁, when pushed forward under 𝑔, will look like samples from ϒ. We will also study relaxations of this problem in which, rather than pushforward the Gaussian 𝜁 under 𝑔, instead 𝜁 is convolved with a Gaussian whose mean is function 𝑔. ♢ In Section 13.1 we describe the subject of density estimation, introduced in Example 13.2, and taking a particular perspective on it which links to several themes arising in other places in the notes. In the remainder of the chapter we study variants on the idea of measure transport illustrated in Example 13.3. All the methods connect the target measure ϒ, which we wish to sample, and defined on R 𝑑 , to a measure on a latent space R 𝑑𝑧 . In Section 13.2 we study a general approach to measure transport akin to the density estimation approach studied in Section 13.1; typically these methods employ 𝑑 𝑧 = 𝑑. Normalizing flows are studied in 13.3; whilst 𝑑 𝑧 ̸ = 𝑑 is possible here too, in the continuum limit, using neural ODEs, this methodology has at its core an invertible mapping with 𝑑 𝑧 = 𝑑. In Section 13.4 we study score-based approaches that build on Langevin sampling. Sections 13.5 and 13.6 introduce autoencoders and variational autoencoders respectively, where an approximate inverse for 𝑔 is sought, although typically 𝑑 𝑧 < 𝑑 so caution is required to define this carefully. Generative adversarial networks (GANs) are introduced in Section 13.7, applicable with 𝑑 𝑧 ̸ = 𝑑. Density Estimation Consider the problem of estimating the probability density function underlying data 𝑈 := {𝑢 (𝑛) } 𝑁 𝑛=1 ; we assume the data points 𝑢 (𝑛) ∈ R 𝑑 are drawn i.i.d. from the same distribution. Let 𝒫(R 𝑑 ) denote the set of all probability density functions on R 𝑑 . We seek to solve the problem by finding a probability density function from a tractable class of probability density functions 𝒬 ⊂ 𝒫(R 𝑑 ), for the purpose of computations and for the purpose of revealing an explicit, interpretable form. To be concrete we assume that, for some parameter set Θ ⊆ R 𝑝 , 𝒬 comprises a set of probability density functions 𝑞(•; 𝜃) ∈ 𝒫(R 𝑑 ) for each 𝜃 ∈ Θ. Our objective is to find P(𝜃|𝑈 ) by applying Bayesian inference, as in Chapter 1. If we place prior P(𝜃) on unknown parameter 𝜃, then, noticing that P(𝑈 |𝜃) = Π 𝑁 𝑛=1 𝑞(𝑢 (𝑛) ; 𝜃), we obtain from Theorem 1.3 that the posterior on 𝜃|𝑈 is given by P(𝜃|𝑈 ) ∝ Π 𝑁 𝑛=1 𝑞(𝑢 (𝑛) ; 𝜃)P(𝜃). (13.3) If we seek a maximum a posteriori (MAP) estimator for 𝜃, then, following the developments in Section 1.2, 𝜃 ⋆ ∈ arg min 𝜃∈Θ J 𝑁 MAP (𝜃), J 𝑁 MAP (𝜃) = - 𝑁 ∑︁ 𝑛=1 log 𝑞(𝑢 (𝑛) ; 𝜃) -log P(𝜃). 1 Recall that pushforward is defined in the preface. Indeed in the nomenclature of Section 1.2 the first term in the definition of J 𝑁 MAP (•) is the loss function (1.6 ) and the second is the regularizer (1.7). If we drop the regularizer, corresponding to what is sometimes termed a flat prior, then we obtain the maximum likelihood estimation (MLE) problem 𝜃 ⋆ ∈ arg min 𝜃∈Θ J 𝑁 MLE (𝜃), J 𝑁 MLE (𝜃) = - 𝑁 ∑︁ 𝑛=1 log 𝑞(𝑢 (𝑛) ; 𝜃). This MLE may be derived by an alternative methodology that avoids appealing to the Bayesian picture. To see this consider the minimization problem 𝜃 ⋆ ∈ arg min 𝜃∈Θ F(𝜃), F(𝜃) := D KL (︀ ϒ‖𝑞(•; 𝜃) )︀ . For this to be useful in the current context we need to be able to minimize it, approximately, given only samples from ϒ, and not ϒ itself. The KL divergence defining the objective F, has the form over 𝜃. Of course we cannot evaluate expectation under ϒ exactly and we must work under Data Assumption 13.1. The desired minimization can be achieved, approximately, given only samples from ϒ : replacing expectation with respect to ϒ with expectation with respect to the empirical density ϒ 𝑁 defined in (13.2), we obtain D KL (︀ ϒ‖𝑞(•; 𝜃) )︀ = E 𝑢∼ϒ -E 𝑢∼ϒ 𝑁 [︁ log 𝑞(𝑢; 𝜃) ]︁ = - 1 𝑁 𝑁 ∑︁ 𝑛=1 log 𝑞(𝑢 (𝑛) ; 𝜃) = 1 𝑁 J 𝑁 MLE (𝜃). Remark 13.4. Thus finding the MLE may be formulated as attempting to find the closest point from 𝒬 to ϒ in the KL divergence. Note, however, that the constant term E 𝑢∼ϒ log ϒ(𝑢), which we removed from the optimization since it does not depend on 𝜃, is undefined if ϒ is replaced by ϒ 𝑁 . This issue arises elsewhere in these notes, and in particular in the next subsection. We emphasize the issue here, in the context of density estimation, since it constitutes the canonical setting in which it arises. ♢ Transport Methods Recall Definition 11.7 of a transport map. We continue with the task of characterizing ϒ ∈ 𝒫(R 𝑑 ), from samples. The approach undertaken in this subsection is to find, approximately, an invertible transport map 𝑔 : R 𝑑 → R 𝑑 and probability measure 𝜁 ∈ 𝒫(R 𝑑 ) with the property that ϒ = 𝑔 ♯ 𝜁; equivalently, since 𝑔 is invertible, 𝜁 = (𝑔 -1 ) ♯ ϒ. The desiderata are that 𝜁 be easy to sample, so that (approximate) samples from ϒ may be generated by applying (an approximation of) 𝑔 to samples from 𝜁; and that an approximation of 𝑔 may be determined given only ϒ 𝑁 as defined in (13.2) . Imagine that a suitable choice for 𝜁 has been made; a Gaussian is a common choice. Let Θ ⊆ R 𝑝 . We introduce a parametric family of functions 𝑔(•; •) : R 𝑑 × Θ → R 𝑑 . We then seek parameter 𝜃 ∈ Θ ⊆ R 𝑝 such that 𝑔(•; 𝜃) : R 𝑑 → R 𝑑 realizes the desired approximation. To achive this goal, we consider the optimization problem 𝜃 ⋆ ∈ arg min 𝜃∈Θ F(𝜃), (13.4a ) F(𝜃) = D KL (︀ ϒ‖𝑔(•; 𝜃) ♯ 𝜁 )︀ . (13.4b) We define the resulting approximate pushforward map by 𝑔 ⋆ = 𝑔(•; 𝜃 ⋆ ). Notice that this approach is a special case of the density estimation approach in the preceding Section 13.1: we parameterize the approximating densities 𝑞(•; 𝜃) = 𝑔(•; 𝜃) ♯ 𝜁 as pushforward of a fixed reference density 𝜁 by a parameterized transport map 𝑔(•; 𝜃). Remark 13.5. Note that there exist perfect transport maps 𝑔 perfect , for example an optimal transport, for which D KL (ϒ‖(𝑔 perfect ) ♯ 𝜁) = 0. We aim to get as close as possible to a perfect transport solution, within our parametric class, by minimizing F over 𝜃. In practice, since ϒ is typically unknown, this task needs to be undertaken based on the data summarized in the empirical measure ϒ 𝑁 given in (13.2). Whilst optimal transportstudied in Section 11.1.3 in the context of transport distances between probability measures-provides a deep mathematical structure within which to consider the measure transport problem, there are numerous applications where the optimality constraint on the transport confers few advantages over other transports; indeed application of such optimality criteria may unnecessarily complicate the computational task of finding a transport with the desired properties. For these reasons, in this chapter we focus on other approaches to transport that relax the optimality constraint, but preserve the core idea of sampling probability measures by constructing transports from a known distribution 𝜁, that we know how to sample, to the target ϒ. The resulting methodologies resonate with the Monge formulation of optimal transport, but we do not invoke the same optimality criterion. ♢ As in Section 13.1, to optimize F(•) we will identify J(•), a function which differs from the KL divergence only by a constant independent of 𝜃, and optimize J 𝑁 (•), an empirical approximation of J(•) found by replacing ϒ with ϒ 𝑁 . Lemma 11.40 is useful in determining an explicit form of J. We now deploy this lemma to find a pair of useful expressions for J(•). Using the expression for the KL divergence we find that F(𝜃) = D KL (︀ ϒ‖𝑔(•; 𝜃) ♯ 𝜁 )︀ = E 𝑢∼ϒ [︁ log ϒ(𝑢) -log 𝑔 ♯ 𝜁(𝑢) ]︁ . Thus, noting that the desired minimization is independent of 𝜃-independent constants, and using (11.20b) we define J(𝜃) = -E 𝑢∼ϒ [︁ log 𝜁 ∘ 𝑔 -1 (𝑢; 𝜃) + log det𝐷 𝑢 (𝑔 -1 )(𝑢; 𝜃) ]︁ . ( 13.5) Note that F(𝜃) = J(𝜃)+𝑐 where constant 𝑐 is independent of parameter 𝜃 to be optimized over. Our desired minimization problem for F(•) is thus equivalent to minimizing (13.5). Furthermore, using (11.19 ) in (13.5), we may write J(𝜃) = -E 𝑢∼ϒ [︁ log 𝜁 ∘ 𝑔 -1 (𝑢; 𝜃) -log det𝐷 𝑢 𝑔 (︀ 𝑔 -1 (𝑢; 𝜃); 𝜃 )︀ ]︁ . (13.6) In practice, ϒ is only available empirically and hence we work under Data Assumption 13.1. Hence, the objective (13.5) or (13.6 ) can be approximated as follows: J 𝑁 (𝜃) = - 1 𝑁 𝑁 ∑︁ 𝑛=1 [︁ log 𝜁 ∘ 𝑔 -1 (𝑢 (𝑛) ; 𝜃) + log det𝐷 𝑢 (𝑔 -1 )(𝑢 (𝑛) ; 𝜃) ]︁ = - 1 𝑁 𝑁 ∑︁ 𝑛=1 [︁ log 𝜁 ∘ 𝑔 -1 (𝑢 (𝑛) ; 𝜃) -log det𝐷 𝑢 𝑔 (︀ 𝑔 -1 (𝑢 (𝑛) ; 𝜃); 𝜃 )︀ ]︁ . ( 13.7) Then the forward map 𝑔 ⋆ is obtained at 𝜃 ⋆ which minimizes J 𝑁 (•). Remark 13.6. By minimizing (13.7) we thus find an approximate expression for ϒ as the pushforward under 𝑔 ⋆ (•) = 𝑔(•, 𝜃 * ) of 𝜁 using (11.20) . That is, ϒ(𝑢) ≈ 𝑔 ⋆ ♯ 𝜁(𝑢) = 𝜋 ∘ 𝑔 -1 (𝑢; 𝜃 * )det𝐷 𝑢 𝑔 -1 (𝑢, 𝜃 * ). (13.8) Once we have the map 𝑔 ⋆ , we can generate new (approximate) samples from ϒ by sampling 𝜁 and applying 𝑔 ⋆ . An important point to note here is that, to determine 𝑔 ⋆ , minimization of either form of the loss function J 𝑁 in (13.7) requires evaluation of 𝑔 -1 . Thus 𝑔 must be readily invertible. Section 13.3 addresses this issue in the context of normalizing flows. ♢ Remark 13.7. The ideas in this section are also useful in the context where ϒ is a known simple measure, from which samples are easily drawn, and 𝜁 is a more complicated measure which we wish to characterize and sample from; in such a setting, which we consider in Section 4.1, the objective is to determine 𝑇 = 𝑔 -1 so that 𝜁 = (𝑔 -1 ) ♯ ϒ. Since (13.5) is expressed entirely in terms of 𝑇 = 𝑔 -1 , and not 𝑔 itself, it is possible to approach this problem by directly parameterizing 𝑇 = 𝑔 -1 rather than 𝑔. ♢ Normalizing Flows The transport approach from Section 13.2 requires invertibility of the map 𝑔, and efficient computation of the determinant of the Jacobian. Invertibility of the map 𝑔 is also useful in other contexts. Normalizing flows address this issue; normalizing reflects the fact that distribution 𝜁 is often a Gaussian, whilst flow connotes the breaking up of the map 𝑔 into a sequence of simpler maps which are themselves parameterized, rather than parameterizing 𝑔 itself. 2 To this end we fix 𝐽 ∈ N and introduce the iteration the 𝐽-fold composition of 𝐻(•; 𝜃). The proposed method is thus simply a transport of the type discussed in Section 13.2, with a specific construction of 𝑔 in compositional form. The remainder of the section comprises two components: firstly showing specifics of the formulation of the minimization problems (13.5) or (13.6 ) in this compositional setting; and secondly showing a continuum limit of the compositional setting, which recovers neural ODEs. Remark 13.8. We have concentrated on the simple setting where the same map 𝐻 is used at each step of the composition; this simplifies the exposition, especially in the continuum limit. However it is commonplace to compose different map at each step, as discussed in Remark 13.9; furthermore Remark 13.11 highlights the implications of doing so in the neural ODEs setting. ♢ Structure In The Optimization Problem For 𝜃 We assume that 𝐻(•; 𝜃) : R 𝑑 → R 𝑑 is invertible, with inverse 𝐻 -1 (•; 𝜃) : R 𝑑 → R 𝑑 , and differentiable, with derivative 𝐷𝐻(•; 𝜃) : R 𝑑 → R 𝑑×𝑑 . Assume that 𝑧 ∼ 𝜁 and let 𝑝 𝑗 (𝑣 𝑗 ) denote the density of 𝑣 𝑗 . Then the goal of normalizing flow is to choose parameter 𝜃 in 𝐻 so that 𝑝 𝐽 ≈ ϒ and hence (approximately) 𝑢 ∼ ϒ; this is potentially a more straightforward task than working with a directly parameterized 𝑔, the approach described in Section 13.2. Let 𝑣 𝑗 = 𝐻 (𝑗) (𝑧; 𝜃), where 𝐻 (𝑗) (•; 𝜃) denotes the 𝑗-fold composition of 𝐻(•; 𝜃) and recall that 𝑢 = 𝑔(𝑧; 𝜃). Note, also, that 𝑧 is found as the 𝐽-fold composition of 𝐻 -1 (•; 𝜃) evaluated at 𝑢. Formulae (11.19) , (11.20) show that, since 𝑣 𝑗 = 𝐻 -1 (𝑣 𝑗+1 ; 𝜃), Note that F(𝜃) given by (13.4) differs from J(𝜃) = -E 𝑢∼ϒ log 𝑔(•; 𝜃) ♯ 𝜁 by a constant that is independent of 𝜃. Thus, with the specified relationships defined between (𝑧, {𝑣 𝑗 }) and 𝑢, we may consider the following specific case of (13.6), using the normalization flow structure: log 𝑝 𝑗+1 (𝑣 𝑗+1 ) = log 𝑝 𝑗 (𝑣 𝑗 ) -log det𝐷𝐻(𝑣 𝑗 ; 𝜃). J(𝜃) = -E 𝑢∼ϒ [︁ log 𝜁(𝑧) - 𝐽-1 ∑︁ 𝑗=0 log det𝐷𝐻(𝑣 𝑗 ; 𝜃) ]︁ . (13.12) As before we set 𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃) and define the resulting approximate pushforward map by 𝑔 ⋆ = 𝑔(•; 𝜃 ⋆ ). Note that 𝑔(•; 𝜃 ⋆ ) is found from the 𝐽-fold composition of 𝐻(•; 𝜃 ⋆ ). Again, in practice the expectation over ϒ in (13.12) can be approximated using the empirical density ϒ 𝑁 . Remark 13.9. The preceding setting can be generalized to one in which each map in the composition is different and carries its own set of parameters to be optimized, so that (13.10) is replaced by 𝑔(•; 𝜃) = 𝐻 𝐽-1 (︀ •; 𝜃 (𝐽-1) )︀ ∘ 𝐻 𝐽-2 (︀ •; 𝜃 (𝐽-2) )︀ ∘ • • • ∘ 𝐻 0 (︀ •; 𝜃 (0) )︀ , ( 13.13) and 𝜃 = {︀ 𝜃 (𝑗) }︀ 𝐽-1 𝑗=0 . Then, for example, each 𝐻 𝑗 may have a triangular structure with respect to some ordering of the elements of R 𝑑 . Recall that the term triangular refers to the Jacobian of 𝐻 𝑗 being a (lower-) triangular matrix. One parameterization of a triangular map is given by 𝐻 𝑗 (𝑣; 𝜃) = ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ 𝑣 1 𝑓 2 (𝑣 1 ; 𝜃) + 𝑣 2 . . . 𝑓 𝑑 (𝑣 1 , . . . , 𝑣 𝑑-1 ; 𝜃) + 𝑣 𝑑-1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ , where 𝑓 𝑘 (𝑣 1 , . . . , 𝑣 𝑘-1 ; 𝜃) are parameterized functions. Since triangular maps are readily designed to be invertible, this approach can be useful; and choosing different orderings at each step and hence different 𝐻 𝑗 at each step, mitigates against bias from any particular ordering. ♢ Neural ODEs Neural ODEs are a special case of transport maps where map 𝑔 is the solution map of an ODE with flow defined by a parameterized vector field; finding 𝑔 is replaced by the task of finding the vector field. A continuum limit of normalizing flows can be used to construct neural ODEs. To this end we define 𝑑𝑣 𝑑𝑡 = ℎ(𝑣; 𝜃), 𝑣(0) = 𝑧. (13.14) Noting that 𝑣(𝑡) depends on 𝑧 we set 𝑔 𝑡 (𝑧; 𝜃) = 𝑣(𝑡) and 𝑔(𝑧; 𝜃) = 𝑔 1 (𝑧; 𝜃) = 𝑣 (1) . An advantage of the continuum perspective is that the inverse of map 𝑔 is readily computed as follows. Define the equation 𝑑𝑤 𝑑𝑡 = -ℎ(𝑤; 𝜃), 𝑤(0) = 𝑢. Then 𝑔 -1 𝑡 (𝑢; 𝜃) = 𝑤(𝑡) and, in particular, 𝑔 -1 (𝑢; 𝜃) = 𝑤(1). Lemma 13.10. Assume that in (13.14) the initial condition satisfies 𝑧 ∼ 𝜁. Then log 𝑔 ♯ 𝜁(𝑢) = log 𝜁(𝑧) - ∫︁ 1 0 div ℎ (︀ 𝑣(𝑠); 𝜃 )︀ 𝑑𝑠. (13.15) Proof. The approach we take is to work with the Euler approximation of the continuum formulation, and pass to the limit of infinitesimally small time-step. This may be achieved by considering the discrete normalizing flow with 𝐻(•; 𝜃) = Id + Δ𝑡ℎ(•; 𝜃); setting 𝐽Δ𝑡 = 1 and taking Δ → 0 will deliver the desired limit. Note that the discrete and continuum pictures are connected by 𝑣 𝑗 ≈ 𝑣(𝑗Δ𝑡). We do not provide details of the limit Δ → 0, instead proceeding purely formally. We first note that det𝐷𝐻 = det(𝐼 𝑑 + Δ𝑡𝐷ℎ) = 1 + Δ𝑡Tr𝐷ℎ + 𝒪(Δ𝑡 2 ). Now note that Tr 𝐷ℎ = div ℎ. Thus, substituting this expression for det𝐷𝐻 into (13.11) , expanding the logarithm in powers of Δ𝑡, summing over 𝑗 and letting Δ𝑡 → 0 yields the desired result. The continuum analog of (13.12) is then J(𝜃) = -E 𝑢∼ϒ [︁ log 𝜁(𝑧) - ∫︁ 1 0 div ℎ (︀ 𝑣(𝑠); 𝜃 )︀ 𝑑𝑠 ]︁ , ( 13.16) where 𝑧 = 𝑔 -1 (𝑢; 𝜃) and 𝑣(𝑡) = 𝑔 -1 1-𝑡 (𝑢; 𝜃). As before, up to an additive constant, J(𝜃) = D KL (ϒ‖𝑔 ♯ 𝜁). Minimizing J over 𝜃, to obtain 𝜃 ⋆ , thus leads to 𝑔 ⋆ (•) = 𝑔(•; 𝜃 ⋆ ) and (𝑔 ⋆ ) ♯ 𝜁 ≈ ϒ. Remark 13.11. Using the setting of Remark 13.9 leads to a generalized form of neural ODE taking the form 𝑑𝑣 𝑑𝑡 = ℎ (︀ 𝑣, 𝑡; 𝜃(𝑡) )︀ , 𝑣(0) = 𝑧. (13.17) Optimization is now over a function of time, 𝜃(𝑡), and some form of regularization is needed to enforce continuity of 𝜃(•). For example we may employ Tikhonov regularization, penalizing the 𝐿 2 ([0, 𝑇 ]; R 𝑑 ) norm of 𝜃, replacing (13.16) by J(𝜃) = -E 𝑢∼ϒ [︁ log 𝜁(𝑧) - ∫︁ 1 0 div ℎ (︀ 𝑣(𝑠), 𝑠; 𝜃(𝑠) )︀ 𝑑𝑠 ]︁ + 𝜆 ∫︁ 1 0 ⃒ ⃒ ⃒ 𝑑𝜃 𝑑𝑡 (𝑠) ⃒ ⃒ ⃒ 2 𝑑𝑠. (13.18) Notice that 𝑧 = 𝑣 0 and 𝑣(𝑠) are functions of 𝑢 = 𝑣 (1) , defined by the backward evolution of the ODE (13.17). ♢ Score-Based Approaches We have overviewed a variety of techniques for generative modeling, starting with density estimation, with our focus being variants on transport-based methods. An important shift of perspective, leading to alternative methodologies, follows from understanding the role of the Langevin equation in generating samples from a measure with probability density ϒ. We start by defining the Langevin equation 𝑑𝑢 𝑑𝑡 = 𝐷 log ϒ + √ 2 𝑑𝑊 𝑑𝑡 , 𝑢(0) ∼ 𝜁. ( 13 .19) Under certain tail and smoothness assumptions on ϒ this equation is ergodic: for suitable classes of test function 𝜙 : R 𝑑 → R, we have that, almost surely with respect to 𝑢(0) and 𝑊 (•) (assumed independent of one another), lim 𝑇 →∞ 1 𝑇 ∫︁ 𝑇 0 𝜙 (︀ 𝑢(𝑡) )︀ 𝑑𝑡 = E 𝑢∼ϒ [︀ 𝜙(𝑢) ]︀ . Similar convergence results hold in law. Such a method may be viewed as a stochastic transport, over an infinite time horizon, of 𝜁 into ϒ; stochastic because it depends on Brownian motion 𝑊 (•) and infinite time horizon because 𝑢(0) ∼ 𝜁 whilst 𝑢(𝑇 ) ∼ ϒ as 𝑇 → ∞. This discussion of ergodicity suggests sampling via use of the gradient of the log-density of the measure ϒ: 𝐷 log ϒ. This vector field on R 𝑑 is known as the score function. Discretization of the Langevin equation in time, for example by the Euler-Maruyama method, provides a useful proposal distribution for the Metropolis-Hastings MCMC methods introduced in Section 15.5. However, in this section we do not assume that we are given ϒ, only samples from it. In this setting, learning the score function and then solving the Langevin equation over an infinite time horizon, approximately, provides a method for generating new samples from ϒ, based on learning the score. However, other variants on this idea are available and we now present some of them. The starting point is to identify a function 𝑠 𝜃 : R 𝑑 → R 𝑑 so that 𝑠 𝜃 ≈ 𝐷 log ϒ. We are given only samples from ϒ: we do not assume access to the target density ϒ itself. We assume that 𝜃 is chosen from set Θ ⊆ R 𝑝 . A natural objective results in the following regression problem: J(𝜃; ϒ) = ∫︁ |𝑠 𝜃 (𝑢) -𝐷 log ϒ(𝑢)| 2 ϒ(𝑢) 𝑑𝑢, (13.20a) 𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃). (13.20b) Then 𝑠 ⋆ = 𝑠 𝜃 ⋆ is the approximate score at the optimal 𝜃. Remark 13.12. Let ϒ 𝜃 be a distribution whose score is given by 𝑠 𝜃 = 𝐷 log ϒ 𝜃 . Then, the objective J corresponds to the squared Fisher divergence 𝑑 𝐹 (ϒ‖ϒ 𝜃 ) 2 := ∫︁ |𝐷 log ϒ 𝜃 (𝑢) -𝐷 log ϒ(𝑢)| 2 ϒ(𝑢) 𝑑𝑢. ♢ This approach looks attractive, but approximate evaluation of the objective J, using only knowledge of the empirical measure 13.2, and directly employing (13.20), is not feasible in practice. The root cause is the presence of the true score 𝐷 log ϒ(𝑢) in the objective. The reader will note that log ϒ(𝑢) also entered the objective J for density estimation, as described in Section 13.1. However in that context the terms involving log ϒ(𝑢) decouple from those involving the unknown parameter 𝜃. That does not happen in (13.20) in an obvious way. However, the following two propositions show that J can be minimized using integration by parts, resulting in redefined objectives that only depend on ϒ in an outer expectation. The first such redefined objective is given in the following proposition. Using integration by parts and that ϒ is a density which decays to zero as |𝑢| → ∞, we can write the second term as ∫︁ ⟨︀ 𝑠 𝜃 (𝑢), 𝐷 log ϒ(𝑢) ⟩︀ ϒ(𝑢) 𝑑𝑢 = ∫︁ ⟨︀ 𝑠 𝜃 (𝑢), 𝐷ϒ(𝑢) ⟩︀ 𝑑𝑢 = - ∫︁ div 𝑠 𝜃 (𝑢)ϒ(𝑢) 𝑑𝑢. We can thus rewrite the objective J as J(𝜃; ϒ) = ∫︁ (︁ |𝑠 𝜃 (𝑢)| 2 + 2 div 𝑠 𝜃 (𝑢) )︁ ϒ(𝑢) 𝑑𝑢 + 𝐾, 𝐾 = ∫︁ |𝐷 log ϒ(𝑢)| 2 ϒ(𝑢) 𝑑𝑢. Since 𝐾 is independent of 𝜃 and finite by assumption, we deduce that the minimizers of J(𝜃; ϒ) coincide with those defined in (13.21). Remark 13.14. We note that, in contrast to (13.20) , the objective (13.21) requires evaluation of the gradient of the approximate score function 𝐷𝑠 𝜃 , in order to evaluate the divergence term. This gradient is a Hessian matrix of size 𝑑×𝑑 and can be challenging to compute and store in high-dimensions. However, one may leverage techniques from randomized linear algebra to estimate the trace of this matrix. The need to evaluate the Hessian can also be avoided by learning the score not of ϒ but of its convolution with a Gaussian. ♢ To avoid the need to evaluate the Hessian we introduce the idea of denoising scorematching. This approach estimates the score function for an approximate distribution defined by the convolution of ϒ with a kernel 𝑝 𝜎 : R 𝑑 × R 𝑑 → R + with bandwidth 𝜎 > 0. That is, ϒ 𝜎 (𝑢) := ∫︁ 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤) 𝑑𝑤. (13.23) For example, 𝑝 𝜎 can be a Gaussian kernel (recall the Definition 11.17 of kernel) 𝑝 𝜎 (𝑢, 𝑤) ∝ exp (︁ - 1 2𝜎 2 |𝑢 -𝑤| 2 )︁ . ( 13 .24) This choice is motivated by the facts that: (i) for bandwidth 𝜎 → 0 we obtain that 𝑝 𝜎 (•; 𝑤) converges to the Dirac measure 𝛿 𝑤 , if appropriately normalized, and so ϒ 𝜎 (𝑢) converges to ϒ(𝑢), the original distribution; (ii) on the other hand, larger bandwidths smooth larger-scale features of ϒ. The following proposition shows that score-matching for ϒ 𝜎 can be performed without knowing the score of ϒ directly. Proposition 13.15. Assume that ∫︁ |𝐷 log ϒ 𝜎 (𝑢)| 2 ϒ 𝜎 (𝑢) 𝑑𝑢 + ∫︁ ∫︁ |𝐷 𝑢 log 𝑝 𝜎 (𝑢, 𝑤)| 2 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑢) 𝑑𝑤𝑑𝑢 < ∞. Then, the optimal score that minimizes the loss function J(𝜃; ϒ 𝜎 ), found from (13.20) by replacing ϒ with ϒ 𝜎 , is given by 𝑠 ⋆ = 𝑠 𝜃 ⋆ where 𝜃 ⋆ ∈ arg min 𝜃∈Θ ∫︁ ∫︁ |𝑠 𝜃 (𝑢) -𝐷 𝑢 log 𝑝 𝜎 (𝑢, 𝑤)| 2 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤) 𝑑𝑢𝑑𝑤. (13.25) Proof. The score of the smoothed distribution 𝑝 𝜎 is given by 𝐷 log ϒ 𝜎 (𝑢) = 𝐷ϒ 𝜎 (𝑢) ϒ 𝜎 (𝑢) = 1 ϒ 𝜎 (𝑢) ∫︁ 𝐷 𝑢 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤) 𝑑𝑤 = 1 ϒ 𝜎 (𝑢) ∫︁ 𝐷 𝑢 (︀ log 𝑝 𝜎 (𝑢, 𝑤) )︀ 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤) 𝑑𝑤. (13.26) Thus, the inner product of 𝑠 𝜃 and the score of the smoothed distribution is given by ∫︁ ⟨︀ 𝑠 𝜃 (𝑢), 𝐷 log ϒ 𝜎 (𝑢) ⟩︀ ϒ 𝜎 (𝑢) 𝑑𝑢 = ∫︁ ∫︁ ⟨︀ 𝑠 𝜃 (𝑢), 𝐷 𝑢 log 𝑝 𝜎 (𝑢, 𝑤) ⟩︀ 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤) 𝑑𝑤𝑑𝑢. Furthermore ∫︁ |𝑠 𝜃 (𝑢)| 2 ϒ 𝜎 (𝑢) 𝑑𝑢 = ∫︁ ∫︁ |𝑠 𝜃 (𝑢)| 2 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤) 𝑑𝑤𝑑𝑢 by definition of ϒ 𝜎 . Substituting the two preceding expressions into the objective J(𝜃; ϒ 𝜎 ) gives J(𝜃; ϒ 𝜎 ) = ∫︁ ∫︁ |𝑠 𝜃 (𝑢) -𝐷 𝑢 log 𝑝 𝜎 (𝑢, 𝑤)| 2 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤) 𝑑𝑢𝑑𝑤 + 𝐾, 𝐾 = ∫︁ |𝐷 log ϒ 𝜎 (𝑢)| 2 ϒ 𝜎 (𝑢) 𝑑𝑢 -∫︁ ∫︁ |𝐷 𝑢 log 𝑝 𝜎 (𝑢, 𝑤)| 2 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤) 𝑑𝑢𝑑𝑤. Noting that 𝐾 is independent of 𝜃 and finite by assumption, we deduce that the minimizers of J(𝜃; ϒ 𝜎 ) coincide with those defined in (13.25) . We now restrict our attention to the specific case where 𝑝 𝜎 (𝑢, 𝑤) is the distribution of a random variable 𝑢|𝑣 defined by (13.24) . The score of smoothed distribution ϒ 𝜎 that is computed by Proposition 13.15 can then be expressed in terms of a conditional expectation of the underlying random variables. The next lemma presents this result, which is known as Tweedie's formula. Lemma 13.16. Let 𝑢 = 𝑤 + 𝜎𝜂 where 𝑤 ∼ ϒ and 𝜂 ∼ 𝒩 (0, 𝐼 𝑑 ). Define 𝜋 𝑤 to be the posterior density foot_17 𝜋 𝑤 (𝑤|𝑢) = 1 ϒ 𝜎 (𝑢) 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤), where 𝑝 𝜎 (•; 𝑤) is the Gaussian kernel given by (13.24). Then the score 𝐷 log ϒ 𝜎 (𝑢) satisfies the identity E 𝑤∼𝜋 𝑤 [𝑤] = 𝑢 + 𝜎 2 𝐷 log ϒ 𝜎 (𝑢). (13.27) Proof. First note that the score of the Gaussian kernel is 𝐷 log 𝑝 𝜎 (𝑢, 𝑤) = - (𝑢 -𝑤) 𝜎 2 . Using this identity in (13.26) we can write the score of the smoothed distribution as 𝐷 log ϒ 𝜎 (𝑢) = 1 ϒ 𝜎 (𝑢) ∫︁ 𝐷 𝑢 (︀ log 𝑝 𝜎 (𝑢, 𝑤) )︀ 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤) 𝑑𝑤 = ∫︁ -(𝑢 -𝑤) 𝜎 2 𝑝 𝜎 (𝑢, 𝑤)ϒ(𝑤) ϒ 𝜎 (𝑢) 𝑑𝑤 = 1 𝜎 2 E 𝑤∼𝜋 𝑤 [𝑤 -𝑢], and the desired result follows. Tweedie's formula shows that computing a score function is related to an inverse problem. It may be rearranged to give: 𝐷 log ϒ 𝜎 (𝑢) = 1 𝜎 2 (︁ E 𝑤∼𝜋 𝑤 [𝑤] -𝑢 )︁ . Given a noisy realization 𝑢 of the random variable 𝑤, the score is given in terms of the conditional expectation E[𝑤|𝑢], which is the posterior mean for the unknown 𝑤. Notice that E[𝑤|𝑢] is the function of 𝑢 that minimizes E|𝑤 -𝜙(𝑢)| 2 over measurable maps 𝜙 : R 𝑑 → R 𝑑 . Thus, to learn 𝐷 log ϒ 𝜎 (𝑢) we can seek parameterized map 𝜙 𝜃 : R 𝑑 × Θ → R 𝑑 that minimizes E|𝑤 -𝜙 𝜃 (𝑢)| 2 . This problem can be empirically approximated without having access to the density ϒ. Specifically, given only data {𝑤 (𝑛) } 𝑁 𝑛=1 independently sampled from ϒ, we set 𝑢 (𝑛) := 𝑤 (𝑛) + 𝜎𝜂 (𝑛) , where 𝜂 (𝑛) ∼ 𝒩 (0, 𝐼 𝑑 ) i.i.d. We then set 𝜃 ⋆ ∈ arg min 𝜃 ∑︀ 𝑁 𝑛=1 |𝑤 (𝑛) -𝜙 𝜃 (𝑢 (𝑛) )| 2 , define 𝜙 ⋆ = 𝜙 𝜃 ⋆ and approximate 𝐷 log ϒ 𝜎 (𝑢) ≈ 1 𝜎 2 (︀ 𝜙 ⋆ (𝑢) -𝑢 )︀ . Autoencoders Autoencoders are primarily used as a technique for uncovering latent low-dimensional structure in high dimensional data. We introduce them here and then demonstrate in the next section that a natural probabilistic relaxation of the idea leads to generative models termed variational autoencoders. Transport ideas may be used to (approximately) represent a complicated probability measure, perhaps only known through samples, as the pushforward of a simpler measure. In this section we go a step further, by seeking the simpler measure in a latent space of lower dimension than that of the ambient space where the data lives. Autoencoders are one natural approach to such problems. The basic idea of autoencoders is to find an approximate factorization of the identity which is accurate in the support of density ϒ, but using only the empirical approximation of ϒ by ϒ 𝑁 . Let 𝑓 : R 𝑑 × Θ 𝑓 → R 𝑑𝑧 and 𝑔 : R 𝑑𝑧 × Θ 𝑔 → R 𝑑 where Θ 𝑓 ⊆ R 𝑝 𝑓 and Θ 𝑔 ⊆ R 𝑝𝑔 ; in particular each of 𝑓 and 𝑔 can be a neural network, as defined in Section 12.2, generalized to vector-valued output. Recalling that Id denotes the identity mapping on R 𝑑 , define foot_18 J(𝜃 𝑓 , 𝜃 𝑔 ) = ⃦ ⃦ ⃦Id -𝑔 (︀ 𝑓 (•; 𝜃 𝑓 ); 𝜃 𝑔 )︀ ⃦ ⃦ ⃦ 2 𝐻 ϒ := E 𝑢∼ϒ ⃒ ⃒ 𝑢 -𝑔 (︀ 𝑓 (𝑢; 𝜃 𝑓 ); 𝜃 𝑔 )︀⃒ ⃒ 2 . Now consider the following optimization problem: (𝜃 ⋆ 𝑓 , 𝜃 ⋆ 𝑔 ) ∈ arg min (𝜃 𝑓 ,𝜃𝑔)∈Θ 𝑓 ×Θ𝑔 J(𝜃 𝑓 , 𝜃 𝑔 ). (13.28) We then define 𝑓 ⋆ (𝑢) = 𝑓 (𝑢; 𝜃 ⋆ 𝑓 ) and 𝑔 ⋆ (𝑧) = 𝑔(𝑧; 𝜃 ⋆ 𝑔 ). Roughly speaking, and dropping the dependence on parameters for expository purposes, we are seeking functions 𝑓 and 𝑔 such that 𝑔 (︀ 𝑓 (𝑢) )︀ ≈ 𝑢. In practice the optimization is implemented empirically, and we minimize J 𝑁 (𝜃 𝑓 , 𝜃 𝑔 ) = ⃦ ⃦ ⃦Id -𝑔 (︀ 𝑓 (•; 𝜃 𝑓 ); 𝜃 𝑔 )︀ ⃦ ⃦ ⃦ 2 𝐻 𝑁 ϒ = 1 𝑁 𝑁 ∑︁ 𝑛=1 ⃒ ⃒ 𝑢 (𝑛) -𝑔 (︀ 𝑓 (𝑢 (𝑛) ; 𝜃 𝑓 ); 𝜃 𝑔 )︀⃒ ⃒ 2 . Remark 13.17. This approach reduces the autoencoder to a particular form of supervised learning in which the inputs and outputs are equal, so we seek to approximate the identity, and in which we impose a specific structure on the class of approximating function -as composition of 𝑔 with 𝑓 . We refer to R 𝑑𝑧 as the latent space and note that a primary application of the methodology is to identify latent spaces of dimension 𝑑 𝑧 which is much less than the dimension 𝑑 of the data space. The approximate factorization of the identity found by composing 𝑔 with 𝑓 provides a way of moving between the data space and the latent space. ♢ Example 13.18. We demonstrate that principal component analysis, often referred to simply as PCA, may be viewed as a form of autoencoder. Define 𝑚 = E 𝑢∼ϒ 𝑢, 𝐶 = E 𝑢∼ϒ [︁ (𝑢 -𝑚) ⊗ (𝑢 -𝑚) ]︁ . Then consider the eigenpairs (𝜙 (𝑖) , 𝜆 (𝑖) ) ∈ R 𝑑 × R + solving 𝐶𝜙 (𝑖) = 𝜆 (𝑖) 𝜙 (𝑖) , |𝜙 (𝑖) | = 1. The eigenvalues are positive and we assume them to be decreasingly ordered with respect to 𝑖. Moreover, the eigenvectors are orthonormal. For given 𝑑 𝑧 ≤ 𝑑, we now define the maps 𝑓 (𝑢) = (︁ ⟨𝑢, 𝜙 (1) ⟩, . . . , ⟨𝑢, 𝜙 (𝑑𝑧) ⟩ )︁ , 𝑔(𝑧) = 𝑑𝑧 ∑︁ 𝑗=1 𝑧 𝑗 𝜙 (𝑗) . Then, their composition gives us 𝑔 (︀ 𝑓 (𝑢) )︀ = 𝑑𝑧 ∑︁ 𝑗=1 ⟨𝑢, 𝜙 (𝑗) ⟩𝜙 (𝑗) = (︁ 𝑑𝑧 ∑︁ 𝑗=1 𝜙 (𝑗) ⊗ 𝜙 (𝑗) )︁ 𝑢. Thus, the PCA method may be viewed as truncating the representation of the identity defined by the spectral theorem for positive, symmetric matrices. The specific basis used is defined by ϒ but it is approximated in practice knowing only ϒ 𝑁 by computing the empirical mean and covariance. ♢ Variational Autoencoders In the preceding section, note that 𝜁 = (𝑓 ⋆ ) ♯ ϒ gives the (approximate) distribution in the latent space, but that we do not specify 𝜁. Furthermore the mapping 𝑔 (approximately) solves a measure transport problem from 𝜁 to ϒ, but choice of measure 𝜁 is not within our control. It is natural to ask whether the methodology can be generalized to settings in which it is desirable to impose a specified distribution 𝜁. This leads to the topic of variational autoencoders which we now outline. Let 𝜋 be a coupling of ϒ and 𝜁 and note that the following two identities hold: 𝜋(𝑢, 𝑧) = P(𝑢|𝑧)𝜁(𝑧), 𝜋(𝑢, 𝑧) = P(𝑧|𝑢)ϒ(𝑢). The idea of the variational autoencoder is to approximate the two conditional densities P(𝑢|𝑧), P(𝑧|𝑢), appearing in these identities, by parameterized families; and then to align the two different expressions for the coupling. To be explicit we assume that 𝑢|𝑧 ∼ 𝒩 (︀ 𝑔(𝑧; 𝜃 𝑔 ), 𝜎 2 𝑔 𝐼 )︀ , 𝑧|𝑢 ∼ 𝒩 (︀ 𝑓 (𝑢; 𝜃 𝑓 ), 𝜎 2 𝑓 𝐼 )︀ , noting that this is a relaxation of the setting for autoencoders; in particular the domains and ranges of 𝑓, 𝑔 are as specified in Section 13.5. Thus parameter choices to define 𝑓 and 𝑔 are made to ensure that the two resulting approximate expressions for 𝜋(𝑢, 𝑧) are close. Invoking these Gaussian approximations for the conditionals we obtain the following two approximations for the coupling density: 𝜋 𝑔 (𝑢, 𝑧) = 1 𝑍 𝑔 exp (︁ - 1 2𝜎 2 𝑔 |𝑢 -𝑔(𝑧; 𝜃 𝑔 )| 2 )︁ 𝜁(𝑧), 𝜋 𝑓 (𝑢, 𝑧) = 1 𝑍 𝑓 exp (︁ - 1 2𝜎 2 𝑓 |𝑧 -𝑓 (𝑢; 𝜃 𝑓 )| 2 )︁ ϒ(𝑢). A common choice is to assume that 𝜁 is the density of a standard unit Gaussian; we make this assumption throughout what follows. We will only discuss learning of the parameters defining (𝑓, 𝑔), but (𝜎 𝑔 , 𝜎 𝑓 ) could also be learned. We ask that 𝜋 𝑓 and 𝜋 𝑔 are close in KL divergence, with 𝜋 𝑓 in the first argument. Then, using the Gaussian assumptions on the conditionals and on 𝜁, leads to insightful explicit calculations. To see this, we first note that D KL (𝜋 𝑓 ‖𝜋 𝑔 ) = E 𝑢∼ϒ [︂ E 𝑧∼P(𝑧|𝑢) [︁ 1 2𝜎 2 𝑔 |𝑢 -𝑔(𝑧; 𝜃 𝑔 )| 2 ]︁ + D KL (︀ P(•|𝑢)‖𝜁 )︀ ]︂ + const, where the constant is independent of the parameters we wish to learn and where P(𝑧|𝑢) and 𝜁(𝑧) are given by their assumed Gaussian structure. Using the expression in Example 11.35 for the KL divergence between two Gaussians we obtain D KL (𝜋 𝑓 ‖𝜋 𝑔 ) = E 𝑢∼ϒ [︂ E 𝑧∼𝒩 (︀ 𝑓 (𝑢;𝜃 𝑓 ),𝜎 2 𝑓 𝐼 )︀ [︁ 1 2𝜎 2 𝑔 ⃒ ⃒ 𝑢 -𝑔(𝑧; 𝜃 𝑔 ) ⃒ ⃒ 2 ]︁ + 1 2 |𝑓 (𝑢; 𝜃 𝑓 )| 2 ]︂ + const. Noting that 𝑧 ∼ 𝒩 (︀ 𝑓 (𝑢; 𝜃 𝑓 ), 𝜎 2 𝑓 𝐼 )︀ is the same in law as 𝑓 (𝑢; 𝜃 𝑓 )+𝜎 𝑓 𝜉 where 𝜉 ∼ 𝒩 (0, 𝐼), we may write this divergence as D KL (𝜋 𝑓 ‖𝜋 𝑔 ) = J(𝜃 𝑓 , 𝜃 𝑔 ) + const, where J(𝜃 𝑓 , 𝜃 𝑔 ) = E 𝑢∼ϒ [︂ E 𝜉∼𝒩 (0,𝐼) [︁ 1 2𝜎 2 𝑔 ⃒ ⃒ ⃒𝑢 -𝑔 (︀ 𝑓 (𝑢; 𝜃 𝑓 ) + 𝜎 𝑓 𝜉; 𝜃 𝑔 )︀ ⃒ ⃒ ⃒ 2 ]︁ + 1 2 |𝑓 (𝑢; 𝜃 𝑓 )| 2 ]︂ and the constant term is independent of (𝜃 𝑓 , 𝜃 𝑔 ). This constitutes a regularized version of the standard autoencoder; in particular the preceding expression for the divergence clearly regularizes the basic concept that 𝑔 (︀ 𝑓 (𝑢) )︀ ≈ 𝑢 under density ϒ. Remark 13.19. In practice this is implemented with expectation over ϒ approximated empirically by ϒ 𝑁 . This leaves an optimization problem in which the objective is defined via expectation over 𝜉. Stochastic gradient descent from Chapter 15 may be used to tackle this problem. After training, approximate samples from ϒ may be obtained by sampling 𝑧 ∼ 𝜁 and then sampling 𝑢|𝑧 ∼ 𝒩 (︀ 𝑔(𝑧; 𝜃 𝑔 ), 𝜎 2 𝑔 )︀ . ♢ 13.7 Generative Adversarial Networks The variational autoencoder trains a probabilistic model for 𝑢 ∈ R 𝑑 by learning P(𝑢|𝑧) and specifying P(𝑧). The generative adversarial network is another method for training such a probabilistic model, which also allows for low dimensional latent space. To be concrete we assume that it has the same structural form, namely 𝑢|𝑧 ∼ 𝒩 (︀ 𝑔(𝑧; 𝜃 𝑔 ), 𝜎 2 𝑔 𝐼 )︀ , (13.29a) 𝑧 ∼ 𝒩 (︀ 0, 𝐼 )︀ . (13.29b) Once again the domain and range of 𝑔 is as specified in Section 13.5. With the probabilistic construction (13.29), samples from 𝑢 are created from samples from 𝑧. We write the resulting density for 𝑢, under this generative model, as ϒ 𝑔 ; here 𝑔 connotes the fact that the model is generative. It is desired that samples from ϒ 𝑔 are similar to those specified by the data, which is drawn from density ϒ. The generative adversarial network starts by defining a discriminator d : R 𝑑 × Θ d → [0, 1], to be thought of as taking values which are probabilities. Here Θ d ⊂ R 𝑝 d . It is instructive to think of d(𝑢; 𝜃 d ) as being the probability that 𝑢 is drawn from the density ϒ, and 1 -d(𝑢; 𝜃 d ) as the probability of 𝑢 being drawn from the generative model ϒ 𝑔 . We then define 𝑣 : Θ 𝑔 × Θ d → R by 𝑣(𝜃 𝑔 , 𝜃 d ) = E 𝑢∼ϒ [︁ log d(𝑢; 𝜃 d ) ]︁ + E 𝑢∼ϒ𝑔 [︁ log (︀ 1 -d(𝑢; 𝜃 d ) )︀ ]︁ . The optimal parameter values are defined by ︀ 𝜃 d (𝜃 𝑔 ) ∈ arg max 𝜃 d 𝑣(𝜃 𝑔 , 𝜃 d ), 𝜃 ⋆ 𝑔 ∈ arg min 𝜃𝑔 𝑣 (︀ 𝜃 𝑔 , ̃︀ 𝜃 d (𝜃 𝑔 ) )︀ , 𝜃 ⋆ d = ̃︀ 𝜃 d (𝜃 ⋆ 𝑔 ). In the maximization step, the parameters of the discriminator are chosen to maximize 𝑣 -the discriminator acts adversarially to try and find data under ϒ 𝑔 which does not look like data under ϒ. In the minimization step the generator acts to reduce 𝑣 to try and make the two data sources look similar. Ideally, through an iterative process, a solution is found in which d(•; 𝜃 ⋆ d ) ≡ 1 2 , so that the data and generated data are indistinguishable. The value of 𝜃 ⋆ 𝑔 defines the generative model once this indistinguishable state has been reached. Bibliography Density estimation is covered in numerous texts; we refer the reader to the book [388] and the literature cited there. For a foundational reference concerning the use of normalizing flows in machine learning see [363] ; more recent applications may be found in [447] , [156] . The neural ODE perspective was popularized in the paper [98] ; earlier papers [189, 136] anticipated the idea but did not make a practical tool in the way that the neural ODE paper [98] did. The subject of optimal transport is overviewed and systematically developed in [426] ; computational methodology is overviewed and developed in [339] . Triangular transport maps are overviewed as a method for sampling in [295] . Score-based methods have become popular in the context of generative diffusion models [393] . The integration by parts method and denoising score-matching approaches were first proposed in [223] and [427] , respectively. These approximate scores have been used for sampling using Langevin dynamics; see [273, 60] for sampling guarantees. They have also been used in other unsupervised learning tasks, like learning probability graphical models. Autoencoders have a long history; see [179, 207, 385] and the citations therein. Variational autoencoders were introduced concurrently in [247] and [364] . For overviews of variational autoencoders see [452, 248] . The idea of conducting unsupervised learning with GANs was introduced in [178] . Although not covered here, because our focus is on generative modeling, clustering is a widely used methodology in unsupervised learning; it is overviewed in [430] . The paper [320] describes an underlying mathematical framework, based on eigenvalue perturbation theory. Understanding large data limits of spectral clustering methods is a subject developed in [431, 214] . The use of graph Laplacians, which underpin spectral clustering, in the solution of inverse problems, is undertaken in [162, 135, 164, 197, 377, 199] . Basic Analog Forecasting Let d be a distance-like deterministic scoring rule on R 𝑑 from Definition 11.65, recalling that any norm on R 𝑑 provides an example. Analog forecasting works as follows. Let 1 𝑛 ⋆ ∈ arg min 𝑛∈N d(𝑣 0 , 𝑣 † 𝑛 ). (14.1) Then the prediction is, for 𝑛 ⋆ ≤ 𝑁 -𝑞, 𝑣 𝑞 = 𝑣 † 𝑛 ⋆ +𝑞 ; for 𝑛 ⋆ > 𝑁 -𝑞 no prediction is made because the data does not support doing so. The intuition behind the forecast is simple to articulate: find the closest point in the data set to 𝑣 0 and ask what happened 𝑞 time units later in the data set; this is used as the prediction. However, the predictions produced by analog forecasting are discontinuous with respect to changes in 𝑣 0 , since the closest point to 𝑣 0 can change discontinuously as 𝑣 0 is varied continuously. In the next subsection we describe a generalization which resolves this undesirable discontinuous behaviour. But before addressing this issue we describe a different limitation of analog forecasting. Suppose that {𝑣 † 𝑛 } is generated by the deterministic dynamical system 𝜓, so that 𝑣 † 𝑛+1 = 𝜓(𝑣 † 𝑛 ). (14.2a) Suppose further that 𝜓 has a global attractor, supporting an invariant measure 𝜇. We employ the following informal definition of an ergodic dynamical system. The process {𝑣 † 𝑛 }, in R 𝑑 , is ergodic if, for a sufficiently wide class 2 of test functions 𝜙, lim 𝑀 →∞ 1 𝑀 𝑀 -1 ∑︁ 𝑚=0 𝜙(𝑣 † 𝑚 ) = ∫︁ R 𝑑 𝜙(𝑣) 𝜇(𝑑𝑣), ( 14.3) with the convergence being almost sure with respect to 𝑣 † 0 drawn from 𝜇. Since the process is deterministic, it is possible that measure 𝜇 will not have density with respect to Lebesgue measure which is why, in contrast to much of the remainder of the notes, we have not used a probability density function in this definition. Remark 14.2. The success of analog forecasting depends on the Lyapunov exponents of the system that generated {𝑣 † 𝑛 }. Under conditions described by Oseledet's Theorem (see Bibliography Section 14.5), closely related to the preceding informal definition of ergodicity, we can define the maximal Lyapunov exponent of 𝜓 as 𝜆 max = lim 𝑛→∞ lim 𝜖→0 + 1 𝑛 log |𝜓 (𝑛) (𝑣 † + 𝜖𝑒) -𝜓 (𝑛) (𝑣 † )| 𝜖 , ( 14.4) 1 The following does not uniquely define the forecast since the argmin may deliver more than one index. We do not dwell on this issue here as the smoothed forecasting methodology from subsection 14.1.2 addresses it in a systematic fashion. 2 Our definition is informal precisely because we are not specifying the precise class of test functions. for 𝜇-almost every 𝑣 † and almost every vector 𝑒 picked uniformly at random from the unit sphere. For chaotic systems the maximal exponent will be larger than zero. For simplicity consider the case 𝜓(𝑣) = 𝐴𝑣 where 𝐴 is positive and symmetric (and hence diagonalizable) and the spectral radius of 𝐴, 𝜆, is attained at unique eigenvector 𝜙. This does not satisfy the standard assumptions of the theory, but is simple enough to provide intuition. Provided that the projection of 𝑒 onto 𝜙 is non-zero (which will happen almost surely) it is clear that 𝜆 max = log(𝜆). Thus the maximal Lyapunov exponent delivers, through exponentiation, the typical growth rate of perturbations to the system. We now discuss how Lyapunov exponents manifest in analog forecasting. Recall that the analog forecast from 𝑣 0 is found by identifying 𝑛 such that 𝑣 † 𝑛 is the closest point in the data set to 𝑣 0 and setting 𝑣 𝑞 = 𝑣 † 𝑛+𝑞 = 𝜓 (𝑞) (𝑣 † 𝑛 ). Thus, approximately for |𝑣 † 𝑛 -𝑣 0 | small, we have that on average |𝑣 𝑞 -𝜓 (𝑞) (𝑣 0 )| = |𝜓 (𝑞) (𝑣 † 𝑛 ) -𝜓 (𝑞) (𝑣 0 )| ≈ |𝑣 † 𝑛 -𝑣 0 |𝑒 𝜆max𝑞 . ( 14.5) when 𝜆 max > 0 this leads to a potentially large error whenever 𝑞 is large. ♢ Kernel Analog Forecasting The idea we deploy here to overcome the discontinuous behavior of the simple analog forecasting method in the previous section is an intuitive one: rather than predict using a single closest point in the data set, we predict using multiple points in the data set, but weight them smoothly according to their relevance to the initial point 𝑣 0 from which we wish to make a forecast. To effect this smooth weighting we use kernels as introduced in Definition 11.17. Let 𝑝 : R 𝑑 × R 𝑑 → R + be a positive-definite kernel -see Definition 11.17. Then make prediction 𝑣 𝑞 = 1 (𝑁 + 1 -𝑞) 𝑁 -𝑞 ∑︁ 𝑛=0 𝑝(𝑣 0 , 𝑣 † 𝑛 )𝑣 † 𝑛+𝑞 . ( 14.6) This method, too, has a simple intuitive explanation, generalizing analog forecasting. Rather than forecasting using 𝑞 steps ahead from a single closest point in the data set to the initial condition, 𝑞-step forecasts are made from the first (𝑁 + 1 -𝑞) points in the data set. These are then weighted according to how close the starting point in the data set is to 𝑣 0 ; the kernel performs this weighting. A typical choice of kernel is the radial basis function kernel 𝑝(𝑣, 𝑤) = exp(-|𝑣 -𝑤| 2 /𝜆). The choice of 𝜆 is crucial to the success of the methodology and should be viewed as a hyperparameter, tuned to the specific data set to hand. Remark 14.3. Assuming that the kernel is continuous in its arguments it follows that this forecast will be continuous with respect to 𝑣 0 . Note that we have only used points in the data set from which it is possible to forecast forward 𝑞 steps. In fact we can only do this for a subset of points in the data given by Data Assumption 14.1. Thus the effective size of the data set is 𝑁 + 1 -𝑞 in (14.6) . ♢ Recurrent Structure As in the previous section we seek to forecast in a purely data-driven way, using Data Assumption 14.1. Here we attempt to forecast by learning a map consistent with the data given in Data Assumption 14.1, and then use that map to forecast. In its simplest form the map is from R 𝑑 into itself, and this map can be composed with itself 𝑞 times, and applied to any initial point 𝑣 0 , to provide a forecast 𝑣 𝑞 . This idea is explained in Subsection 14.2.1. The underlying assumption in trying to predict using a map from R 𝑑 into itself is that the data is generated by a Markovian model on R 𝑑 . If the data is not compatible with this Markovian assumption then memory needs to be accounted for in predicting. In Subsection 14.2.2 we explain how this memory may be accounted for by learning a Markovian model on R 𝑑+𝑟 , where 𝑟 itself is a hyper-parameter to be learned. Subsection 14.2.3 makes a link between the the methods introduced in Subsection 14.2.2 and the idea of random features introduced in Subsection 12.3. Markovian Prediction We seek to explain the data {𝑣 † 𝑛 } 𝑁 𝑛=0 from Data Assumption 14.1 as being the outcome of deterministic Markovian model on R 𝑑 , of form 𝑣 𝑛+1 = 𝜓(𝑣 𝑛 ; 𝜃). ( 14.7) Function 𝜓(•; 𝜃) : R 𝑑 → R 𝑑 may be chosen, for example, as a neural network, a random features model, or the mean of a Gaussian process, as in Chapter 12. We wish to choose 𝜃 ∈ Θ ⊆ R 𝑑 𝜃 so that the data provided by Data Assumption 14.1 is plausibly generated, approximately, by (14.7) . We can view this as a generalization of the supervised learning problem, defined by Data Assumption 12.1, where the inputs are no longer i.i.d. . With this connection in mind, we propose to learn 𝜃 as follows J 𝑁 (𝜃) = 1 𝑁 𝑁 -1 ∑︁ 𝑛=0 ⃒ ⃒ 𝑣 † 𝑛+1 -𝜓(𝑣 † 𝑛 ; 𝜃) ⃒ ⃒ 2 , (14.8a) 𝜃 ⋆ ∈ arg min 𝜃∈Θ J 𝑁 (𝜃). ( 14.8b) However the are additional features that result from assuming the data comes (at least approximately) from a deterministic map on R 𝑑 . In particular, as well as expecting that data points indexed by integers separated by one are linked via 𝜓(•; 𝜃), we also expect that data points separated by integer 𝑚 are linked by 𝜓 (𝑚) (•; 𝜃), the 𝑚-fold composition of 𝜓(•; 𝜃) with itself. Using this idea with 𝑚 = 2 suggests, for example, consideration of the loss function, for some 𝜆 > 0, J 𝑁 (𝜃) = 1 𝑁 𝑁 -1 ∑︁ 𝑛=0 ⃒ ⃒ 𝑣 † 𝑛+1 -𝜓(𝑣 † 𝑛 ; 𝜃) ⃒ ⃒ 2 + 𝜆 1 (𝑁 -1) 𝑁 -2 ∑︁ 𝑛=0 ⃒ ⃒ 𝑣 † 𝑛+2 -𝜓 (2) (𝑣 † 𝑛 ; 𝜃) ⃒ ⃒ 2 , ( 14 .9a) 𝜃 ⋆ ∈ arg min 𝜃∈Θ J 𝑁 (𝜃). (14.9b) Remark 14.4. There are many variants on the idea expressed in the optimization problem (14.9), often referred to as roll-out. For example here we have rolled-out over 𝑚 = 2 steps, but other choices of roll-out duration 𝑚 may be employed. Furthermore the roll-out can be employed in a two-step optimization process, first solving (14.8) and only then solving (14.9), initialized at the outcome of the first optimization problem; this is sometimes referred to as fine-tuning. Since neither optimization will be exact in general, this will deliver a different parameter choice than that arising from directly attempting to solve (14.9). Lyapunov exponents (recall (14.4)) play a role in interpreting the incorporation of roll-out. In general, the larger 𝑚, the harder it is to fit the data to 𝑚-fold compositions of 𝜓(•; 𝜃). In particular, for chaotic systems, the maximal Lyapunov exponent will be larger than zero, leading to sensitivity in maps defined by 𝑚-fold composition for larger 𝑚. The two-stage process, where a parameter ball-park is identified, without roll-out, and then it is adjusted to reflect a roll-out, over 𝑚 = 2 or more compositions, can help in training on chaotic systems. ♢ The following two definitions will be useful to us: Definition 14.5. A discrete-time dynamical system giving rise to data {𝑣 𝑛 } 𝑛∈Z + is said to be strictly stationary if, for all 𝑝 ∈ Z + , the joint distribution of (𝑣 𝑛 , . . . , 𝑣 𝑛+𝑝 ) is the same for all 𝑛 ∈ Z + . It is said to be wide-sense stationary if, for all 𝑝 ∈ Z + , the first and second moments of (𝑣 𝑛 , . . . , 𝑣 𝑛+𝑝 ) are the same for all 𝑛 ∈ Z + . ♢ The definition applies to both deterministic and random dynamical systems. In the deterministic setting of (14.2) randomness can only enter through the initial condition. It is then natural that this randomness is given by invariant measure 𝜇. Given a stationary process, and assuming ergodicity, it is also natural to make the following definition. Definition 14.6. Consider a stationary and ergodic process and assume that pair (𝑣 𝑛 , 𝑣 𝑛+𝑚 ) is distributed according to correlation density 𝜋 𝑚 (𝑣 0 , 𝑣 𝑚 ) in 𝒫(R 𝑑 × R 𝑑 ). The twopoint correlation function of a stationary process is function 𝑐 : Z + → R given by 𝑐(𝑚) := E (𝑣,𝑤)∼𝜋𝑚(•,•) [𝑣𝑤 ⊤ ]. ♢ In the deterministic setting (14.2) the correlation density 𝜓 𝑚 (•, •) is simply the pushforward of 𝜇 under the map 𝑢 ∈ R 𝑑 ↦ → (︀ 𝑢, 𝜓 (𝑚) (𝑢) )︀ ∈ R 𝑑 × R 𝑑 . The population-level optimization problem that the optimization problem (14.8) approximates may be defined as follows. Assume that data is given in the form of the correlation density function 𝜋 1 (𝑣, 𝑤) in 𝒫(R 𝑑 × R 𝑑 ), of a stationary ergodic process. Then define J(𝜃) = E (𝑣,𝑤)∼𝜋 1 (•,•) ⃒ ⃒ 𝑤 -𝜓(𝑣; 𝜃) ⃒ ⃒ 2 , (14.10a) 𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃). (14.10b) If data {𝑣 † 𝑛 } 𝑁 -1 𝑛=0 is derived from such a stationary ergodic process, then J(𝜃) is the pointwise limit of J 𝑁 (𝜃), by ergodicity. We see that if we include roll-out over two steps than the natural population-level data assumption is that we are given, not only 𝜋 1 (𝑣, 𝑤) from a stationary ergodic process, but also the correlation density function 𝜋 2 (𝑣, 𝑤) in 𝒫(R 𝑑 × R 𝑑 ), of the same stationary ergodic process. The limiting optimization problem is J(𝜃) = E (𝑢,𝑣)∼𝜋 1 (•,•) ⃒ ⃒ 𝑣 -𝜓(𝑢; 𝜃) ⃒ ⃒ 2 + 𝜆E (𝑢,𝑤)∼𝜋 2 (•,•) ⃒ ⃒ 𝑤 -𝜓 (2) (𝑢; 𝜃) ⃒ ⃒ 2 , (14.11a) 𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃). (14.11b) These ideas can be readily generalized to roll-out over 𝑚 steps. Memory And Prediction The premise of the preceding subsection is that data in Data Assumption 14.1 derives from a Markov model for 𝑣 𝑛 , such as (14.2), and there often arise settings where this is not the case, even approximately. Attempting to fit a model of the form (14.7), using optimization problems such as (14.8) or (14.9), will typically lead to large values of the objective function at the optimum, and to poor predictions, when the data is not consistent with a Markovian model. In such a setting it is of interest to introduce an unobserved latent variable {𝑟 𝑛 } 𝑁 -1 𝑛=0 , with 𝑟 𝑛 ∈ R 𝑟 and then to hypothesize a Markov model for the pair (𝑣 𝑛 , 𝑟 𝑛 ) of the form 𝑣 𝑛+1 = 𝜓(𝑣 𝑛 , 𝑟 𝑛 ; 𝜃 𝑣 ), (14.12a) 𝑟 𝑛+1 = 𝜙(𝑣 𝑛 , 𝑟 𝑛 ; 𝜃 𝑟 ). (14.12b) If 𝜓, 𝜙 are neural networks then the overall structure is known as a recurrent neural network, or RNN for short. We define 𝜃 = (𝜃 𝑣 , 𝜃 𝑟 ) ∈ Θ = Θ 𝑣 × Θ 𝑣 and learn 𝜃 by solving the minimization problem J 𝑁 (𝜃) = 1 𝑁 𝑁 -1 ∑︁ 𝑛=0 ⃒ ⃒ 𝑣 † 𝑛+1 -𝜓(𝑣 † 𝑛 , 𝑟 𝑛 ; 𝜃 𝑣 ) ⃒ ⃒ 2 , ( 14.13a ) 𝑟 𝑛+1 = 𝜙(𝑣 † 𝑛 , 𝑟 𝑛 ; 𝜃 𝑟 ), 𝑟 0 = 0, (14.13b ) 𝜃 ⋆ ∈ arg min 𝜃∈Θ J 𝑁 (𝜃). ( 14.13c) The key fact about (14.12b) is that, once 𝑟 0 = 0 is fixed, it defines a unique function, for each 𝑛, taking the history {𝑣 ℓ } 𝑛-1 ℓ=0 into 𝑟 𝑛 : .14) Thus (14.12a) becomes memory dependent as a closed model for the evolution of 𝑣 𝑛 . Nonetheless we retain the benefits of being Markovian with respect to the pair (𝑣 𝑛 , 𝑟 𝑛 ). 𝑟 𝑛 = ℱ 𝑛 (︀ {𝑣 𝑚 } 𝑛-1 𝑚=0 ; 𝜃 𝑟 ). ( 14 Remark 14.7. The choice 𝑟 0 = 0 is arbitrary and a change of initial condition can always be shifted to the origin by redefining the right-hand side. However it is also possible to learn the best choice of 𝑟 0 , using data assimilation, given training data in form of an observed time-series in the space containing elements of the sequence {𝑣 𝑛 }. ♢ Remark 14.8. The need for memory in time-series prediction can be motivated not only for non-Markovian systems, but also for Markovian but partially observed systems. One way to see this is using Takens' Theorem, which says (roughly) that under quite general conditions, given partial observations of a Markovian system but enough memory, one can reconstruct the original dynamics. We state Takens' Theorem informally; see the bibliography for references. Suppose that 𝜓 : R 𝑑 → R 𝑑 defines a deterministic dynamical system, 𝑣 † 𝑗+1 = 𝜓(𝑣 † 𝑗 ). Let ℎ : R 𝑑 → R be a scalar-valued observation function and define vector 𝑉 𝑗 ∈ R 𝑚 by 𝑉 𝑗 := (︁ ℎ(𝑣 † 𝑗-(𝑚-1) ), ℎ(𝑣 † 𝑗-(𝑚-2) ), . . . , ℎ(𝑣 † 𝑗 ) )︁ . Using map 𝜓 it follows that 𝑉 𝑗+1 = Ψ(𝑉 𝑗 ) for some Ψ : R 𝑚 → R 𝑚 . Provided 𝑚 is large enough, then for generic choices of ℎ this new dynamical system on R 𝑚 is equivalent to the original one on R 𝑑 in the sense that they are related by a smooth, invertible change of coordinates. ♢ Memory And Reservoir Computing Including memory is important in many time series prediction tasks, as the data may not come from a Markov process. However, learning RNN structure can be challenging for two reasons: (i) because 𝑟 𝑛 is an unobserved latent variable; (ii) and because, within the context of gradient-based methods (Chapter 15) propagation of gradients of 𝜃 𝑟 through the model is difficult. Reservoir computing is one approach to overcome this. The model deployed is again (14.12); however, rather than learning 𝜃 𝑟 , that parameter is simply picked at random from some probability measure on space of appropriate dimension and the following optimization problem is solved: J 𝑁 (𝜃 𝑣 ) = 1 𝑁 𝑁 -1 ∑︁ 𝑛=0 ⃒ ⃒ 𝑣 † 𝑛+1 -𝜓(𝑣 † 𝑛 , 𝑟 𝑛 ; 𝜃 𝑣 ) ⃒ ⃒ 2 , ( 14.15a ) 𝑟 𝑛+1 = 𝜙(𝑣 † 𝑛 , 𝑟 𝑛 ; 𝜃 𝑟 ), 𝑟 0 = 0, (14.15b ) 𝜃 ⋆ ∈ arg min 𝜃𝑣∈Θ𝑣 J 𝑁 (𝜃 𝑣 ). (14.15c) Crucially the optimization is only over 𝜃 𝑣 as 𝜃 𝑟 is fixed at a randomly chosen value. Remark 14.9. Typically the method requires 𝑟 of higher dimension than for the RNN in order to be successful. This is related to the issue of how expressive randomly chosen functions are. Indeed there is a connection to the random features methodology of Section 12.3. To see this, recall that equation (14.15b) reveals that 𝑟 𝑛 is defined by (14.14) . Thus, once 𝜃 𝑟 is chosen at random, 𝑟 𝑛 is a random function of the history of the observed data up to time 𝑛 -1. ♢ Linear Autoregressive Models The models we have fit to data so far are all deterministic. Here we take the first step towards learning stochastic models of the data. We again work under Data Assumption 14.1. We start by defining the vector autoregressive (VAR) process that will be our object of study in this section. Definition 14.10. Let {𝑣 𝑛 } 𝑛∈Z + be an R 𝑑 -valued sequence. The 𝑝th-order VAR model, written VAR(𝑝), is 𝑣 𝑛+1 = 𝑐 + 𝑝-1 ∑︁ ℓ=0 𝐴 ℓ 𝑣 𝑛-ℓ + 𝜀 𝑛 , (14.16) where the 𝐴 ℓ ∈ R 𝑑×𝑑 are fixed matrices, 𝑐 ∈ R 𝑑 is a fixed vector, and 𝜀 𝑛 ∼ 𝒩 (0, 𝐶) is an i.i.d. mean-zero Gaussian sequence in R 𝑑 with fixed covariance 𝐶 ∈ R 𝑑×𝑑 . ♢ Remark 14.11. When 𝑣 𝑛 is a scalar, the process is known simply as an autoregressive (AR) model. Note that it is sometimes useful to consider 𝜀 𝑛 to be a non-Gaussian strictly stationary and ergodic process, but we restrict to the i.i.d Gaussian model here for simplicity. As well as generalizing the foregoing sections by allowing for stochasticity in the dynamics, this model also generalizes what we have seen in previous sections in its treatment of memory. In Subsection 14.2.1 we employed a deterministic Markovian model in R 𝑑 with no memory. And in Subsection 14.2.2 we employed a model which, when viewed as a dynamics model in R 𝑑 , had memory that, for each 𝑛, reaches all the way back to time 0 -this is demonstrated in (14.14) . The VAR process has memory, if 𝑝 > 1, but the memory is over a fixed finite window, the same for every 𝑛 and determined by 𝑝 -the memory does not stretch back all the way to time 0. ♢ In the following we take 𝑐 = 0 for simplicity. But the methodological development, and theorem at the end of the section, are readily modified to accommodate the setting 𝑐 ̸ = 0. We also assume that 𝐶 is known and positive definite, so that the unknown parameters of the model are 𝜃 := (𝐴 0 , . . . , 𝐴 𝑝-1 ). (Learning 𝐶 as well is possible, but more complicated to state.) We write the optimization problem J 𝑁 (𝜃) = 1 (𝑁 -𝑝 + 1) 𝑁 -1 ∑︁ 𝑛=𝑝-1 ⃒ ⃒ 𝐶 -1 2 (︀ 𝑣 † 𝑛+1 - 𝑝-1 ∑︁ ℓ=0 𝐴 ℓ 𝑣 † 𝑛-ℓ )︀⃒ ⃒ 2 , (14.17a) 𝜃 ⋆ ∈ arg min 𝜃∈Θ J 𝑁 (𝜃). (14.17b) If we assume that the data is generated by a strictly stationary process then there is a population-level limit of this minimization problem. It may be expressed in terms of the distribution of the underlying stationary process on the data. We omit details, but conclude with an example of a strictly stationary process of VAR type. Example 14.12. Consider the following VAR(1) process with 𝑐 = 0: 𝑣 𝑛+1 = 𝐴𝑣 𝑛 + 𝜀 𝑛 with 𝜀 𝑛 ∼ 𝒩 (0, 𝐶). Let 𝐴 ∈ R 𝑑×𝑑 and assume that the Lyapunov equation 𝐶 ∞ = 𝐴𝐶 ∞ 𝐴 ⊤ + 𝐶 has a unique solution. Then the process is stationary if 𝑣 0 ∼ 𝒩 (0, 𝐶 ∞ ). Indeed then 𝑣 𝑛 ∼ 𝒩 (0, 𝐶 ∞ ) for all 𝑛 ∈ Z + . To establish these facts we first use a straightforward induction to show that if 𝑣 0 is Gaussian then 𝑣 𝑛 is Gaussian for all 𝑛 ∈ Z + . Assume that 𝑣 0 ∼ 𝒩 (𝑚 0 , 𝐶 0 ). Then 𝑣 𝑛 ∼ 𝒩 (𝑚 𝑛 , 𝐶 𝑛 ) and 𝑚 𝑛+1 = 𝐴𝑚 𝑛 , ( 14.18a ) 𝐶 𝑛+1 = 𝐴𝐶 𝑛+1 𝐴 ⊤ + 𝐶. ( 14.18b) These equations have steady state with mean 0 and covariance 𝐶 ∞ . Thus we have established that, if 𝑣 0 ∼ 𝒩 (0, 𝐶 ∞ ) then 𝑣 𝑛 ∼ 𝒩 (0, 𝐶 ∞ ) for all 𝑛 ∈ Z + . The vector (𝑣 𝑛 , . . . , 𝑣 𝑛+𝑝 ) will then also be jointly Gaussian with mean zero. To establish strong stationarity it suffices to show that the covariance of (𝑣 𝑛 , . . . , 𝑣 𝑛+𝑝 ) is independent of 𝑛, for any 𝑝 ∈ Z + . To do this it is sufficient to show that the covariance of 𝑣 𝑛 with 𝑣 𝑛+𝑚 is 𝑛-independent, for any 𝑚 ∈ Z + . We note that 𝑣 𝑛+𝑚 = 𝐴 𝑚 𝑣 𝑛 + 𝑚 ∑︁ 𝑗=1 𝐴 𝑚-𝑗 𝜀 𝑛+𝑗 . Because the sequence {𝜀 𝑛 } 𝑛∈N is i.i.d. it follows that, for 𝑚 ∈ N, E𝑣 𝑛+𝑚 𝑣 ⊤ 𝑛 = 𝐴 𝑚 E𝑣 𝑛 𝑣 ⊤ 𝑛 + 𝑚 ∑︁ 𝑗=1 𝐴 𝑚-𝑗 E𝜀 𝑛+𝑗 𝑣 ⊤ 𝑛 , = 𝐴 𝑚 𝐶 ∞ . As desired this depends only on 𝑚 and not on 𝑛 and the desired result is established. ♢ Transport-Based Auto-Regressive Models In this section we extend the linear autoregressive models with additive Gaussian noise, from Section 14.3, to a more general nonlinear and non-Gaussian probabilistic form. We do this by modelling non-Gaussian distributions as the pushforward of a Gaussian variable. This is an idea we introduced through transport, in Section 13.2, in the context of generative modelling. We again employ Data Assumption 14.1. To motivate the approach we adopt, we first note that the VAR process (14.16) from Section 14.3 may be formulated through use of a conditional probability kernel 𝜋 in the form 𝑣 𝑛+1 ∼ 𝜋(•|𝑣 𝑛 , . . . , 𝑣 𝑛-𝑝+1 ; 𝜃) = 𝒩 (︁ 𝑐 + 𝑝-1 ∑︁ ℓ=0 𝐴 ℓ 𝑣 𝑛-ℓ , 𝐶 )︁ (14.19) where, if (𝑐, 𝐶) are assumed known, 𝜃 = (𝐴 0 , . . . , 𝐴 𝑝-1 ). The noise structure assumed in Section 14.3 also implies that the draws 𝑣 𝑛+1 are conditionally independent for each 𝑛. The transition kernel in the VAR model is defined by a standard Gaussian on R 𝑑 ; we generalize this structure in what follows, letting 𝜚(𝑣) denote the standard Gaussian density on R 𝑑 . We introduce a transport 𝑇 ∈ R 𝑝𝑑 × R 𝑑 × Θ, depending on parameters 𝜃 ∈ Θ ⊆ R 𝑞 , so that 𝑣 𝑛+1 ∼ 𝜋(•|𝑣 𝑛 , . . . , 𝑣 𝑛-𝑝+1 ; 𝜃) = 𝑇 (𝑣 𝑛 , . . . , 𝑣 𝑛-𝑝+1 , • ; 𝜃) ♯ 𝜚. ( 14 .20) We again assume conditional independence of the draws 𝑣 𝑛+1 for each 𝑛. We aim to choose parameter 𝜃 so that this model explains the data given in Data Assumption 14.1. If 𝑇 (𝑣 𝑛 , . . . , 𝑣 𝑛-𝑝+1 , • ; 𝜃) is invertible, then we can use the change-of-variables formula from Lemma 11.40 to express the density of the transformed variable as 𝜋(𝑣 𝑛+1 |𝑣 𝑛 , . . . , 𝑣 𝑛-𝑝+1 ; 𝜃) (14.21) = 𝜚 (︀ 𝑇 (𝑣 𝑛 , . . . , 𝑣 𝑛-𝑝+1 , • ; 𝜃 )︀ -1 | 𝑣 𝑛+1 ) det 𝐷 𝑣 𝑛+1 𝑇 (︀ 𝑣 𝑛 , . . . , 𝑣 𝑛-𝑝+1 , •; 𝜃 )︀ -1 | 𝑣 𝑛+1 . This expression is useful for formulating an optimization problem to determine the choice of 𝜃. Example 14.13. The VAR (14.19) falls into the general class (14.20) with 𝑇 given by 𝑇 (𝑣 𝑛 , . . . , 𝑣 𝑛-𝑝+1 , 𝑧; 𝜃) = 𝑐 + 𝑝-1 ∑︁ ℓ=0 𝐴 ℓ 𝑣 𝑛-ℓ + 𝐶 1/2 𝑧. where, if (𝑐, 𝐶) are assumed known, 𝜃 = (𝐴 0 , . . . , 𝐴 𝑝-1 ). ♢ To simplify the exposition we continue our discussion of transport-based learning of probabilistic models in the restricted setting where 𝑝 = 1. In this case, our goal is to find invertible transports 𝑇 : R 𝑑 × R 𝑑 × Θ → R 𝑑 so that 𝜋(𝑣 𝑛+1 |𝑣 𝑛 , 𝜃) := 𝑇 (𝑣 𝑛 , 𝜖; 𝜃) ♯ 𝜚(𝑣 𝑛+1 ) models the conditional distribution for the state at the next time. To simplify the notation, we denote the inverse map 𝑇 (𝑣 𝑛 , •) -1 | 𝑣 𝑛+1 by 𝑆(𝑣 𝑛 , 𝑣 𝑛+1 ; 𝜃). The following result, which provides a canonical working example in the 𝑝 = 1 setting, follows directly from (14.21). Proposition 14.14. Consider the probabilistic model (14.20) in the case where 𝑝 = 1. If 𝑇 (𝑣, 𝜖; 𝜃) = 𝜓(𝑣; 𝜃) + 𝐶 1/2 𝜖, for some map 𝜓 : R 𝑑 × Θ → R 𝑑 and positive definite matrix 𝐶 ∈ R 𝑑×𝑑 , then we have 𝑆(𝑣, 𝑤; 𝜃) = 𝐶 -1/2 (︀ 𝑤 -𝜓(𝑣; 𝜃) )︀ , 𝜋(𝑤|𝑣; 𝜃) = 𝜚 (︁ 𝐶 -1/2 (︀ 𝑤 -𝜓(𝑣; 𝜃) )︀ )︁ det 𝐶 -1/2 , = 1 √︁ (2𝜋) 𝑑 |𝐶| exp (︁ -|𝑤 -𝜓(𝑣; 𝜃)| 2 𝐶 )︁ . That is, the conditional distribution for the next state 𝑣 𝑛+1 , 𝜋( •|𝑣 𝑛 ; 𝜃), is a multivariate Gaussian of the form 𝒩 (︀ 𝜓(𝑣 𝑛 ), 𝐶 )︀ . Given a sequence of data, we can learn the transport parameters by maximizing the log-likelihood of consecutive data pairs {(𝑣 † 𝑛 , 𝑣 † 𝑛+1 )} 𝑁 -2 𝑛=0 under the model. From (14.21) we obtain, in case 𝑝 = 1, 𝜋(𝑣 𝑛+1 |𝑣 𝑛 ; 𝜃) = 𝜚 (︀ 𝑆(𝑣 𝑛 , 𝑣 𝑛+1 ; 𝜃 )︀ det 𝐷 𝑣 𝑛+1 𝑆(𝑣 𝑛 , 𝑣 𝑛+1 ; 𝜃). (14.22) Because of the assumed conditional independence structure, 𝜋(𝑣 𝑛+1 |𝑣 𝑛 , 𝜃) defines a Markov chain on R 𝑑 . If we assume that 𝑣 † 0 ∼ 𝑝 0 then the probability of sequence {𝑣 † 𝑛 } 𝑛∈N from Data Assumption 14.1 is, using the Markov structure, given by (︁ Π 𝑁 -1 𝑛=0 𝜋(𝑣 † 𝑛+1 |𝑣 † 𝑛 ; 𝜃) )︁ 𝑝 0 (𝑣 † 0 ). We assume that 𝑝 0 does not depend on 𝜃. Then, up to a 𝜃-independent constant, and again assuming a standard Gaussian reference 𝜚, the scaled negative log-likelihood is determined by the loss function L(𝜃) = - 1 𝑁 𝑁 -1 ∑︁ 𝑛=0 log 𝜋(𝑣 † 𝑛+1 |𝑣 † 𝑛 ; 𝜃) = - 1 𝑁 𝑁 -1 ∑︁ 𝑛=0 [︁ log 𝜚 (︀ 𝑆(𝑣 † 𝑛 , 𝑣 † 𝑛+1 ; 𝜃) )︀ + log det 𝐷 𝑣 𝑛+1 𝑆(𝑣 † 𝑛 , 𝑣 † 𝑛+1 ; 𝜃) ]︁ = 1 𝑁 𝑁 -1 ∑︁ 𝑛=0 [︂ 1 2 |𝑆(𝑣 † 𝑛 , 𝑣 † 𝑛+1 ; 𝜃)| 2 -log det 𝐷 𝑣 𝑛+1 𝑆(𝑣 † 𝑛 , 𝑣 † 𝑛+1 ; 𝜃) ]︂ + 𝑐, ( 14.23) where 𝑐 is a constant that is again independent of 𝜃. The optimal transport parameters are then given by 𝜃 ⋆ ∈ arg min 𝜃 L(𝜃). Remark 14.15. Here we have assumed that 𝑝 0 is 𝜃-independent. Another natural situation is where 𝑝 0 is the invariant measure associated with the process giving rise to data in Data Assumption 14.1. In this case 𝑝 0 depends on 𝜃 and the estimation problem is more complicated. ♢ Example 14.16. A commonly used non-Gaussian model is based on a nonlinear diagonal transformation of the form 𝑆(𝑣 𝑛 , 𝑣 𝑛+1 ; 𝜃) = 𝑠(𝑣 𝑛+1 -𝜓(𝑣 𝑛 ; 𝜃); 𝜃), where 𝑠(•; 𝜃) : R 𝑑 → R 𝑑 is an invertible function, with inverse 𝑠 -1 (•; 𝜃) : R 𝑑 → R 𝑑 , for all 𝜃 ∈ Θ, and 𝜓 ∈ R 𝑑 × Θ → R 𝑑 . This can be re-written in terms of the direct transport as 𝑇 (𝑣 𝑛 , 𝜖; 𝜃) = 𝜓(𝑣 𝑛 ; 𝜃) + 𝑠 -1 (𝜖; 𝜃), 𝜖 ∼ 𝜚. That is, the next state is given by the previous state with additive noise, which is non-Gaussian for nonlinear 𝑠. The joint distribution for the states at all times 𝑛 that follow this process is often referred to as a non-paranormal or Gaussian copula model. ♢ Example 14.17. Assume that 𝑆(𝑣 𝑛 , 𝑣 𝑛+1 ; 𝜃) = 𝑣 𝑛+1 -𝜓(𝑣 𝑛 ; 𝜃), as in Proposition (14.14) with 𝐶 = 𝐼. Then we have 𝐷 𝑣 𝑛+1 𝑆(𝑣 𝑛 , 𝑣 𝑛+1 ) = 𝐼. Thus, the learning problem based on the loss function in (14.23) reduces to 𝜃 ⋆ ∈ arg min 𝜃 1 2𝑁 𝑁 -1 ∑︁ 𝑛=0 [︁ |𝑣 † 𝑛+1 -𝜓(𝑣 † 𝑛 ; 𝜃)| 2 ]︁ . That is, we can identify the model parameters by solving a supervised learning problem for the nonlinear map 𝜓. This is the setting of Subsection 14.2.1. ♢ Remark 14.18. In Chapter 4 we show how to model non-Gaussian distributions, also using transport, in the context of Bayesian inverse problems. ♢ Bibliography An extensive introduction to linear autoregressive processes, including the conditions for stationarity and ergodicity, is provided in [196] . The Lorenz '63 model is proved to be ergodic in [419, 420] providing a useful example. Indeed in this context central limit theorems have been proved [216] , characterizing the fluctuations about the law of large numbers limit in (14.3). Analog forecasting was first introduced by Lorenz in [286] . A historical overview of analog forecasting and extensions, as well as estimates of error scaling, is given in [144] . The kernel analog regularization of Lorenz's idea was introduced and then studied in [170, 13] . The key message contained in Remark 14.2 is that purely data-driven methods of this type will be limited by sensitivity to initial conditions. In contrast the data assimilation methods, which combine data with knowledge of the model, can overcome sensitivity to initialization; the mechanism by which this occurs is explained in [374] and in [309] and also underlies Theorem 6.26 concerning the effect of model error (rather than the effect of initialization error). Memory in the context of learning model error is discussed in [276] . Takens' Theorem and other embedding theorems are reviewed in [381] . The Remark 14.7 highlights the use of data assimilation in training data-driven models. Incorporating data assimilation into the training process can be very useful, especially when learning from noisy observations and with unobserved latent variables; see Chapter 8 and the bibliography therein. Furthermore, if available, it is often beneficial to train on analyses produced by an imperfect model rather than on observations directly; see Subsection 6.4.3 and [69] . General non-linear autoregressive models have been considered for time-series modeling using various probabilistic models, such as conditional normalizing flows, in [325, 358] . Some of these models rely on recurrent neural networks architectures that can summarize the past states and extract relevant features for making predictions. Chapter 15 Optimization and Sampling This chapter provides an introduction to various topics in optimization and sampling that are particularly pertinent to machine learning, inverse problems and data assimilation. Section 15.1 is concerned with gradient-based optimization, including deterministic and stochastic gradient descent. In Section 15.2 we discuss Newton and Gauss-Newton optimization methods for nonlinear least squares; these algorithms use full or partial Hessian information to define a preconditioner and thereby accelerate the convergence. Section 15.3 describes derivative-free ensemble Kalman methods for nonlinear least squares; these methods, which are intimately related to ensemble Kalman filters for data assimilation studied in Chapter 6, rely on the first two empirical moments of an ensemble of particles to determine the search direction and to define a preconditioner. Section 15.4 is devoted to expectation maximization, an important optimization algorithm to compute maximum likelihood estimators when the likelihood is intractable but can be expressed as the marginal of a tractable extended likelihood. Section 15.5 describes the Metropolis-Hastings algorithm and Gibbs samplers, two Markov chain Monte Carlo algorithms to produce approximate samples from a target distribution. Finally, since many optimization and sampling algorithms involve derivatives of the objective and computing these derivatives can be a limiting computational bottleneck, in Section 15.6 we study auto-differentiation. These techniques have greatly contributed to the scalability and efficiency of modern optimization and sampling algorithms in machine learning and related tasks. Other topics in optimization and sampling are referenced in the bibliography Section 15.7 Before moving into these sections we recall a definitions that will be useful in what follows. We also recall a related definition that is useful in other chapters. ♢ Under these conditions on J, a solution to (15.1) exists for all initial conditions 𝑢 0 ∈ R 𝑑 and for all positive times 𝑡. Straightforward calculation reveals that, for all 𝑢 0 ∈ R 𝑑 , and for all 𝑡 ≥ 0, 𝑑J (︀ 𝑢(𝑡) )︀ 𝑑𝑡 = -|𝐷J (︀ 𝑢(𝑡) )︀ | 2 . This implies that J(•) decreases along trajectories, unless the trajectory is at a critical point of J(•). As a consequence, time-discretization of equation ( 15 .1) provides the basis of algorithms to minimize J. This is motivation for the following: In the simplest case, 𝛼 𝑗 = 𝛼 is some fixed constant time-step. Often, however, the time-step sequence {𝛼 𝑗 } 𝑗∈Z + is determined online as the algorithm progresses, with 𝛼 𝑗 depending on {𝑢 𝑖 } 𝑗 𝑖=1 . In either case, knowing how to choose the time-step to ensure that J decreases at each step, unless it is at a critical point, requires knowledge of the properties of the vector field 𝐷J. To this end, the next theorem considers a case where such knowledge is encoded in assumptions about the Hessian of J. ♢ The following theorem demonstrates that, under certain assumptions on the Hessian, a specific fixed 𝛼 leads to a linear convergence of gradient descent to the global minimum. Theorem 15.6. Let J ∈ 𝐶 2 (R 𝑑 , R + ) and 0 < ℓ ≤ 𝐿 < ∞. Assume that Hessian 𝐷 2 J ∈ 𝐶(R 𝑑 , R 𝑑×𝑑 ) satisfies, for all 𝑢 ∈ R 𝑑 , ℓ𝐼 ⪯ 𝐷 2 J(𝑢) ⪯ 𝐿𝐼. Then J(•) has a unique global minimum 𝑢 ⋆ . Now consider the gradient descent algorithm with step size 𝛼 𝑗 ≡ 1/𝐿. Then, at iteration 𝑗 ≥ 1, J(𝑢 𝑗 ) -J(𝑢 ⋆ ) ≤ where in the last line we have used that 𝛼 𝑗 = 1/𝐿. Now take 𝑣 = 𝑢 ⋆ and 𝑢 = 𝑢 𝑗 in (15.3b) and multiply both sides by a negative sign to get J(𝑢 𝑗 ) -J(𝑢 ⋆ ) ≤ ⟨𝐷J(𝑢 𝑗 ), 𝑢 𝑗 -𝑢 ⋆ ⟩ - ℓ 2 |𝑢 ⋆ -𝑢 𝑗 | 2 . Adding and subtracting the norm of the gradient to the right-hand side, we have J(𝑢 𝑗 ) -J(𝑢 ⋆ ) ≤ ⟨𝐷J(𝑢 𝑗 ), 𝑢 𝑗 -𝑢 ⋆ ⟩ - ℓ 2 |𝑢 ⋆ -𝑢 𝑗 | 2 = 1 2ℓ |𝐷J(𝑢 𝑗 )| 2 - 1 2ℓ |𝐷J(𝑢 𝑗 )| 2 + ⟨𝐷J(𝑢 𝑗 ), 𝑢 𝑗 -𝑢 ⋆ ⟩ - ℓ 2 |𝑢 ⋆ -𝑢 𝑗 | 2 = 1 2ℓ |𝐷J(𝑢 𝑗 )| 2 - 1 2 ⃒ ⃒ ⃒ ⃒ ⃒ √︂ 1 ℓ 𝐷J(𝑢 𝑗 ) - √ ℓ(𝑢 ⋆ -𝑢 𝑗 ) ⃒ ⃒ ⃒ ⃒ ⃒ 2 ≤ 1 2ℓ |𝐷J(𝑢 𝑗 )| 2 . Rearranging the terms we have -|𝐷J(𝑢 𝑗 )| 2 ≤ -2ℓ (︀ J(𝑢 𝑗 ) -J(𝑢 ⋆ ) )︀ . Substituting this on the right-hand side of (15.4) gives us J(𝑢 𝑗+1 ) ≤ J(𝑢 𝑗 ) -ℓ 𝐿 (︀ J(𝑢 𝑗 ) -J(𝑢 ⋆ ) )︀ . If we subtract J(𝑢 ⋆ ) from both sides and let 𝑒 𝑗+1 = J(𝑢 𝑗+1 ) -J(𝑢 ⋆ ), we have 𝑒 𝑗+1 ≤ 𝑒 𝑗 - ℓ 𝐿 𝑒 𝑗 = (︂ 1 - ℓ 𝐿 )︂ 𝑒 𝑗 for all 𝑗 ≥ 1. By induction on 𝑗, we obtain the desired result. Remark 15.7. Consider convex functions with Lipschitz gradients, properties that follow from the Hessian bounds in the preceding theorem. For such objective functions there are modifications of gradient descent that have better convergence rates than the one stated above. For instance, Nesterov's accelerated gradient descent has a quadratic convergence rate, which is optimal for this class of functions. We will not discuss these other methods here but provide references in the bibliography. ♢ Stochastic Gradient Descent Here we consider the minimization of an objective function J : R 𝑑 → R that is defined by an expectation of a function 𝐹 : R 𝑑 × R 𝑑𝑧 → R. That is, J(𝑢) = ∫︁ 𝐹 (𝑢, 𝑧) 𝑑𝜁(𝑧), (15.5) where 𝜁 denotes the probability density function of the random variable 𝑧 ∈ R 𝑑𝑧 . While explicit evaluations of J or its gradients are challenging as a result of the expectation, we assume that it is possible to evaluate 𝐷 𝑢 𝐹 (𝑢, 𝑧) for any realization of the random variable 𝑧. Assuming that 𝜁 is replaced by an empirical approximation 𝜁 𝑁 = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝛿 𝑧 (𝑛) , with the 𝑧 (𝑛) sampled i.i.d. from 𝜁, then J is replaced by J 𝑁 (𝑢) = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝐹 (𝑢, 𝑧 (𝑛) ). (15.6) The gradient of J 𝑁 is then given by 𝐷J 𝑁 (𝑢) = 1 𝑁 𝑁 ∑︁ 𝑛=1 𝐷 𝑢 J(𝑢, 𝑧 (𝑛) ). (15.7) This expression for 𝐷J 𝑁 (𝑢) has the interpretation as a Monte Carlo estimator of the gradient 𝐷J(𝑢) defined by (15.5) . For large 𝑁 , the evaluation of the sum (15.7) may be too expensive. Instead, one can pick 𝑀 < 𝑁 i.i.d. samples from 𝜁 𝑁 ; this corresponds to picking 𝑀 random indices {𝜔 (𝑚) } 𝑀 𝑚=1 , where the elements of this set 𝜔 (𝑚) are sampled uniformly with replacement from 1, . . . , 𝑁 . Then the gradient can be approximated as 𝐷J 𝑁 (𝑢) ≈ 1 𝑀 𝑀 ∑︁ 𝑚=1 𝐷 𝑢 𝐹 (𝑢, 𝑧 (𝜔 (𝑚) ) ). This is known as a mini-batch. Carrying out gradient descent with (for example) an independent mini-batch is a widely used methodology for minimizing J 𝑁 given by (15.6): Definition 15.8. Given function 𝐹 ∈ 𝐶 1,0 (R 𝑑 × R 𝑑𝑧 , R), stochastic gradient descent with respect to J defined by (15.6), refers to the iterative generation of sequence {𝑢 𝑗 } 𝑗∈Z + from a sequence of step-sizes {𝛼 𝑗 } 𝑗∈Z + and 𝑗-indexed sets {𝜔 (𝑚) 𝑗 } 𝑀 𝑚=1 with points sampled uniformly with replacement from 1, . . . , 𝑁 , independently for each 𝑗, by picking any 𝑢 0 ∈ R 𝑑 and then iterating as follows: 𝑢 𝑗+1 = 𝑢 𝑗 -𝛼 𝑗 𝑀 𝑀 ∑︁ 𝑚=1 𝐷 𝑢 𝐹 (𝑢 𝑗 , 𝑧 (𝜔 (𝑚) 𝑗 ) ). ♢ Remark 15.9. Even if the full gradient with respect to the empirical measure (15.7) can be feasibly computed, the stochasticity in stochastic gradient descent can be helpful in escaping from local minima for non-convex objective functions; in the context of optimizing neural networks it is also sometimes advocated as a method to avoid overfitting. ♢ Newton and Gauss-Newton Newton's Method Newton's method is a common root-finding algorithm, which benefits from quadratic convergence, in contrast to the linear convergence of fixed point methods based on the contraction mapping principle. It can also be applied to optimization, as we now show, leading to a methodology that improves on the linear convergence of gradient descent. ♢ Note that critical points of J are fixed points of the Newton iteration. Newton's method proceeds by optimizing successive quadratic approximations to J, as the following theorem shows. Proof. We first approximate J around 𝑢 𝑗 by its second-order Taylor expansion: J(𝑢 𝑗 + 𝑣) ≈ J(𝑢 𝑗 ) + ⟨𝐷J(𝑢 𝑗 ), 𝑣⟩ + 1 2 ⟨𝑣, 𝐷 2 J(𝑢 𝑗 )𝑣⟩, (15.9) where 𝑣 is any vector and the approximation is good when 𝑣 is small. Since J is convex, the Hessian is positive semidefinite, and the right-hand side of (15.9) is convex as a function of 𝑣, meaning that it can be minimized by setting the derivative to 0. Differentiating the approximation of J(𝑢 𝑗 + 𝑣) given in (15.9) with respect to 𝑣, and then setting this derivative to 0, gives 𝐷J(𝑢 𝑗 ) + 𝐷 2 J(𝑢 𝑗 )𝑣 = 0, and thus 𝑣 = -(︀ 𝐷 2 J(𝑢 𝑗 ) )︀ -1 𝐷J(𝑢 𝑗 ). Gauss-Newton Let G ∈ 𝐶 𝐷G(𝑢) )︀ ⊤ (︀ 𝐷G(𝑢) )︀ is invertible for all 𝑢 ∈ R 𝑑 , the Gauss-Newton method for minimizing nonlinear least squares problems of the form (15.10) refers to the iterative generation of sequence {𝑢 𝑗 } 𝑗∈Z + by picking any 𝑢 0 ∈ R 𝑑 and then iterating as follows: 𝑢 𝑗+1 = 𝑢 𝑗 - (︁ (︀ 𝐷G(𝑢 𝑗 ) )︀ ⊤ (︀ 𝐷G(𝑢 𝑗 ) )︀ )︁ -1 (︀ 𝐷G(𝑢 𝑗 ) )︀ ⊤ G(𝑢 𝑗 ). ♢ Gauss-Newton results from applying Newton's method (15.8) to (15.10a), and neglecting the second-order terms of G in the Hessian of J. Notice that 𝐷J(𝑢) = (︀ 𝐷G(𝑢) )︀ ⊤ G(𝑢), and that, entry-wise, Substituting these expressions into Newton's method (15.8) and setting the second term in (15.11) to 0, recovers the Gauss-Newton method. Making this approximation is computationally expedient as it avoids computation of the Hessians of the G 𝑘 . It is also a good approximation near minimizers of G which make the G 𝑘 small, so that the neglected term is itself small, when iterates approach the minimizer. Ensemble Kalman Inversion This section discusses Ensemble Kalman inversion (EKI) as a general-purpose ensemble Kalman method for nonlinear least squares. Compared to the variants on gradient descent and Newton methods studied in the previous two sections, EKI has the advantage of being derivative-free. For this reason, EKI is widely used in inverse problems with complex forward models; indeed, as its name suggests, EKI was first developed as a method for numerical solution of inverse problems, and only more recently formulated as a general-purpose and broadly useful optimization algorithm. Consider the cost function J(𝑢) = 1 2 |𝑦 -𝐺(𝑢)| 2 Γ , for some (possibly) nonlinear function 𝐺 : R 𝑑 → R 𝑘 , point 𝑦 ∈ R 𝑘 and matrix Γ ≻ 0. Notice that this nonlinear least squares objective is of the form in equation (15.10a); this can be seen by setting 𝑚 := 𝑘 and G(𝑢) := 𝑦 -𝐺(𝑢). Notice further that in the language of Chapter 1, the objective function J(𝑢) = -log P(𝑦|𝑢) = -log l(𝑦|𝑢) is the negative log-likelihood function determined by inverse problem (1.1) with Gaussian noise 𝜂 ∼ 𝒩 (0, Γ). Therefore, minimizing J(𝑢) corresponds to maximizing the likelihood, and this interpretation underpins the original derivation of the method. EKI involves first drawing an initial set of 𝑁 particles from a pre-specified probability measure; to be concrete we will employ a Gaussian with a given mean 𝑚 0 and covariance 𝐶 0 . Thus 𝑢 (𝑛) 0 ∼ 𝒩 (𝑚 0 , 𝐶 0 ) i.i.d., 𝑛 = 1, . . . , 𝑁. EKI then proceeds to evolve these particles for iterations 𝑗 = 1, . . . , 𝐽 using an iteration that allows the individual particles to interact through empirical covariance information. The resulting algorithm takes the form )︀ -1 . Remark 15.13. The algorithm produces an ensemble {𝑢 (𝑛) 𝑗 } 𝑁 𝑛=1 that, for each 𝑗, remains in the linear span of the initial ensemble {𝑢 (𝑛) 0 } 𝑁 𝑛=1 . Thus the algorithm may be viewed as seeking to minimize the objective function over a finite-dimensional subspace. In the case of linear 𝐺(•) the methodology is amenable to a complete theoretical analysis, demonstrating sublinear convergence. However variants on it, including Tikhonov regularization, restore linear convergence . Continuous time limits may also be derived, and then analyzed analogously in the linear setting. It is empirically observed that EKI often provides good approximations of the minimizer of the loss function in the nonlinear setting, and does so for relatively small 𝐽 and 𝑁 . See citations in the bibliography Section 15.7. ♢ Remark 15.14. As the name suggests, there is a link between EKI and the ensemble Kalman filter (EnKF) for data assimilation, defined in Subsection 6.3.5. One way of seeing this connection is via the formulation of certain maximum likelihood optimization problems as filtering problems. Another way of viewing the connection is to note that, at every iteration, EKI approximates the solution to a nonlinear inverse problem using an ensemble approximation to the solution of a linear Gaussian inverse problem (Example 1.12) and formally replacing the linear forward model with the nonlinear one. We also note that in the setting of linear 𝐺 the EKI methodology is connected to a preconditioned gradient descent These connections are detailed in the bibliography Section 15.7. ♢ Expectation Maximization The expectation maximization (EM) algorithm is a general-purpose approach to maximum likelihood estimation with latent variables. Let 𝑢 ∈ R 𝑑 be the parameter of interest, 𝑦 ∈ R 𝑘 the data, and 𝑧 ∈ R 𝑑𝑧 a latent variable. We seek to compute the maximum likelihood estimator of parameter 𝑢 given data 𝑦; that is, we seek to maximize objective J : R 𝑑 → R given by J(𝑢) := P(𝑦|𝑢) = ∫︁ P(𝑦, 𝑧|𝑢) 𝑑𝑧. (15.12) The EM algorithm is useful when we cannot readily evaluate the likelihood J(𝑢) = P(𝑦|𝑢) or compute its gradient, but evaluating the full likelihood P(𝑦, 𝑧|𝑢) is easier. As explained in Chapter 8, this situation arises naturally in data assimilation when estimating parameters in the dynamics model, in which case the hidden state represents the latent variable. The objective function in (15.12) for the EM algorithm resembles that in (15.5) for stochastic gradient descent, where intractable objective J(𝑢) is defined by averaging tractable objective 𝐹 (𝑢, 𝑧) over auxiliary variables 𝑧 ∈ R 𝑑𝑧 . Notice, however, that in this section we depart from the convention in previous sections and formulate the optimization problem in terms of maximizing rather than minimizing objective J. A key component in the derivation of the EM algorithm is encapsulated in the following definition: ♢ The following result establishes a lower bound on the log-likelihood function log J(𝑢) = log P(𝑦|𝑢) in terms of an ELBO. Note that P(𝑦, • |𝑢) is not normalized when viewed as a probability density for variable 𝑧 alone; however, it is of course normalized with respect to the joint distribution on the pair (𝑦, 𝑧). The following theorem quantifies the difference between log J(𝑢) and ℒ(𝑞, 𝑢). The proof is structurally identical to that of the variational formulation of Bayes Theorem in Chapter 2. A similar result is also used in the proof of Theorem 7.8 in Chapter 7. + log J(𝑢), as desired. Recall our goal of maximizing, with respect to parameter 𝑢, the likelihood function J(𝑢) = P(𝑦|𝑢). Since the logarithm is monotonic increasing on the positive reals, this task is equivalent to maximizing the log-likelihood log J(𝑢) = log P(𝑦|𝑢). Our starting point is to use (15.13) for this purpose. Note that the identity depends on a free choice of the probability density function 𝑞. The EM algorithm is an iterative method which generates sequence {𝑢 𝑗 , 𝑞 𝑗 } 𝑗∈Z + by alternating the maximization of ℒ(𝑞, 𝑢) with respect to 𝑞 and 𝑢 : 1. Given 𝑢 𝑗 , choose 𝑞 𝑗 to maximize ℒ(• , 𝑢 𝑗 ). 2. Given 𝑞 𝑗 , choose 𝑢 𝑗+1 to maximize ℒ(𝑞 𝑗 , • ). Step 1 yields 𝑞 𝑗 (𝑧) = P(𝑧|𝑦, 𝑢 𝑗 ); this is because (15.13) shows that the maximization of ℒ(𝑞, 𝑢 𝑗 ) over 𝑞 is the same as minimization of D KL We thus obtain the identity log J(𝑢 𝑗 ) = ℒ(𝑞 𝑗 , 𝑢 𝑗 ). (15.15) Step 2 proceeds by noting that ℒ(𝑞 𝑗 , 𝑢) = ∫︁ log P(𝑦, 𝑧|𝑢) P(𝑧|𝑦, 𝑢 𝑗 ) 𝑑𝑧 + 𝑐, (15.16) where 𝑐 is independent of 𝑢. Hence, the quantity to maximize is the expected value of the joint log-density log P(𝑦, 𝑧|𝑢) with respect to 𝑞 𝑗 (𝑧) = P(𝑧|𝑦, 𝑢 𝑗 ). The E-step corresponds to computing ℒ(𝑞 𝑗 , 𝑢) from (15.16); the M-step corresponds to maximizing the resulting expression over 𝑢 to obtain 𝑢 𝑗+1 . Combining these two steps gives the EM algorithm: The following result shows that the EM framework has the desirable property that the likelihood function increases monotonically along iterates 𝑢 𝑗 . The first inequality follows because 𝑢 𝑗+1 ∈ arg max 𝑢 ℒ(𝑞 𝑗 , 𝑢); the second follows from (15.14) and the fact that all divergences are non-negative; the final equality follows from (15.14) and (15.15) . Remark 15.20. As a consequence of Theorem 15.19 it is possible to deduce-under mild assumptions-that the iterates 𝑢 𝑗 of the EM algorithm converge, as 𝑗 → ∞, to a local maximizer of the likelihood function. It is important to note, however, that the expectation in the E-step and the optimization in the M-step are often intractable. Monte Carlo, filtering, or smoothing algorithms may be employed to approximate the Estep, and optimization algorithms to approximate the M-step. Such approximations can cause loss of monotonicity and can interfere with the convergence guarantees provided by Theorem 15.19. ♢ Markov Chain Monte Carlo In this section we turn our attention to the problem of drawing samples from a given target distribution 𝜋. We will focus on Markov chain Monte Carlo (MCMC) algorithms, a wide class of methods that share the idea of generating samples from a Markov chain for which the target is an invariant distribution. To describe MCMC we first recall that a Markov kernel is a function 𝑝 : R 𝑑 × R 𝑑 → R satisfying: • 𝑝(𝑢, 𝑣) ≥ 0 for all 𝑢, 𝑣 ∈ R 𝑑 ; and • ∫︀ R 𝑑 𝑝(𝑢, 𝑣) 𝑑𝑣 = 1 for all 𝑢 ∈ R 𝑑 . For fixed 𝑢 ∈ R 𝑑 , 𝑝(𝑢, •) defines a probability density function on R 𝑑 ; if the chain is currently at 𝑢 ∈ R 𝑑 then 𝑝(𝑢, •) enables calculation of the probability with which the chain will be in any given Borel subset of R 𝑑 after one time-step. The idea of MCMC is to define a Markov kernel with the property that the target 𝜋 is an invariant distribution for the kernel. This means that if we sample an initial draw 𝑢 (0) ∼ 𝜋 and subsequently draw 𝑢 (𝑛+1) ∼ 𝑝(𝑢 (𝑛) , •), 0 ≤ 𝑛 ≤ 𝑁 -1, (15.17) then it holds that 𝑢 (𝑛) ∼ 𝜋 for all 1 ≤ 𝑛 ≤ 𝑁. Moreover, it is possible to show under mild assumptions that even when the initial draw 𝑢 (0) is sampled from initial distribution 𝜋 0 different from 𝜋, the distribution of the 𝑛-th draw 𝑢 (𝑛) defined by (15.17) converges to 𝜋 in the large 𝑛 asymptotic. In the next two subsections we introduce two classes of MCMC methods that are widely used in inverse problems, data assimilation and other applications: the Metropolis-Hastings algorithm and the Gibbs sampler. The Metropolis-Hastings Algorithm We consider first the Metropolis-Hastings algorithm, a versatile MCMC approach to define a Markov kernel 𝑝 MH that can be readily sampled and which satisfies detailed balance with respect to 𝜋; that is, for all 𝑢, 𝑣 ∈ R 𝑑 it holds that 𝜋(𝑢)𝑝 MH (𝑢, 𝑣) = 𝜋(𝑣)𝑝 MH (𝑣, 𝑢). (15.18) Integrating over 𝑢 delivers which is the desired invariance property. The key idea behind the Metropolis-Hastings algorithm is that kernel 𝑝 MH satisfying (15.18) can be defined by sampling from a proposal Markov kernel 𝑞, chosen to be easy to sample from, and an accept/reject step. Given the 𝑛-th sample 𝑢 (𝑛) , we obtain the (𝑛 + 1)-th sample through a two-step process. First, we propose 𝑣 * ∼ 𝑞(𝑢 (𝑛) , •) by sampling from the proposal Markov kernel. Second, we accept this proposal and set 𝑢 (𝑛+1) = 𝑣 * with probability 𝑎(𝑢 (𝑛) , 𝑣 * ); otherwise, we reject it and set 𝑢 (𝑛+1) = 𝑢 (𝑛) . The Metropolis-Hastings acceptance probability is defined by 𝑎(𝑢, 𝑣) := min {︂ 1, 𝜋(𝑣) 𝜋(𝑢) 𝑞(𝑣, 𝑢) 𝑞(𝑢, 𝑣) }︂ , 𝑢, 𝑣 ∈ R 𝑑 . (15.19) This definition ensures that the Markov kernel 𝑝 MH that determines the probabilistic transition from 𝑢 (𝑛) to 𝑢 (𝑛+1) satisfies the detailed balance condition (15.18). Algorithm 15.2 outlines the procedure. Algorithm 15.2 Metropolis-Hastings Algorithm 1: Input: Target distribution 𝜋, initial distribution 𝜋 0 , proposal Markov kernel 𝑞, test function 𝑓, number of samples 𝑁. 2: Initial Draw: Draw initial sample 𝑢 (0) ∼ 𝜋 0 . 3: Subsequent Samples: For 𝑛 = 0, 1, . . . , 𝑁 -1 do: 1. Sample 𝑣 ⋆ ∼ 𝑞(𝑢 (𝑛) , •). Update 𝑢 (𝑛+1) = {︃ 𝑣 ⋆ , with probability 𝑎(𝑢 (𝑛) , 𝑣 * ) using (15.19) . 𝑢 (𝑛) , with probability 1 -𝑎(𝑢 (𝑛) , 𝑣 * ). Remark 15.21. Notice that the target distribution only enters the Metropolis-Hastings algorithm in the acceptance probability (15.19) ; since this formula only involves the ratio of the target density at the proposed and current values of the chain, the acceptance probability can be computed even when the target distribution is only known up to normalization constant. ♢ The choice of proposal Markov kernel has a major impact on the performance of the Metropolis-Hastings algorithm. Table 15 .1 contains three illustrative choices of proposal Markov kernel that lead to important instantiations of the Metropolis-Hastings algorithm, known as the random walk Metropolis-Hastings (RWMH) algorithm, the Metropolis-adjusted Langevin algorithm (MALA) and the preconditioned Crank-Nicolson (pCN) method. These three proposals depend on a user-chosen parameter 𝛽, with 𝛽 > 0 for RWMH and MALA, and 𝛽 ∈ (0, 1) for pCN. Proposal 𝑣 * can be obtained by drawing  15.5.2 The Gibbs Sampler Another important MCMC algorithm is the Gibbs sampler, where each one-dimensional coordinate 𝑢 𝑖 of 𝑢 ∈ R 𝑑 is updated in turn by sampling from the target's full conditional distributions given by 𝜋 𝑖 (𝑢 𝑖 |𝑢 -𝑖 ) := 𝜋(𝑢) 𝜋(𝑢 -𝑖 ) , where 𝜋(𝑢 -𝑖 ) := ∫︁ R 𝜋(𝑢 1 , . . . , 𝑢 𝑖-1 , 𝑣, 𝑢 𝑖+1 , . . . , 𝑢 𝑑 ) 𝑑𝑣. Importantly, the full conditionals are probability densities on R and the cost of sampling from these distributions need not, in general, scale with the dimension 𝑑 of the parameter 𝑢 ∈ R 𝑑 . Under mild assumptions, the Gibbs sampler yields a Markov chain whose kernel satisfies detailed balance -and hence invariance-with respect to the target 𝜋. For 𝑛 = 0, 1, . . . , 𝑁 -1 4: For 𝑖 = 1, . . . , 𝑑 1. Sample 𝑢 (𝑛+1) 𝑖 ∼ 𝜋 𝑖 (︁ • |𝑢 (𝑛+1) 1 , . . . , 𝑢 (𝑛+1) 𝑖-1 , 𝑢 (𝑛) 𝑖+1 , . . . , 𝑢 (𝑛) 𝑑 )︁ . 2. Set 𝑢 (𝑛+1) = (︁ 𝑢 (𝑛+1) 1 , . . . , 𝑢 (𝑛+1) 𝑖-1 , 𝑢 (𝑛+1) 𝑖 , 𝑢 (𝑛) 𝑖+1 , . . . , 𝑢 (𝑛) 𝑑 )︁ . 5: End for 6: Record sample 𝑢 (𝑛+1) . 7: End for 8: Output: Samples {𝑢 (𝑛) } 𝑁 𝑛=1 and approximation E 𝜋 [𝑓 (𝑢 (𝑛) )] ≈ 1 𝑁 ∑︀ 𝑁 𝑛=1 𝑓 (𝑢 (𝑛) ). Remark 15.23. The Metropolis-Hastings algorithm and the Gibbs sampler were developed independently and have distinct flavors: in Metropolis-Hastings we propose an update 𝑣 * of all coordinates of 𝑢 at once, and accept or reject this update probabilistically; in contrast, in the Gibbs sampler we update each coordinate in turn by sampling from the target's full conditional, and there is no accept/reject step. Despite these differences, it is possible to view the Gibbs sampler as a Metropolis-Hastings method in which at each iteration a proposal Markov kernel is defined via a full conditional. With such a choice of proposal Markov kernel, the acceptance probability in the accept/reject step of the Metropolis-Hasting algorithm automatically simplifies to be 1, explaining the lack of an accept/reject step in the Gibbs sampler. ♢ Rather than updating a single coordinate at a time, it is often useful to perform Gibbs updates block-wise. To explain this idea, let 𝑢 = (𝑢 𝑎 𝑢 𝑏 ) ∈ R 𝑑 with 𝑢 𝑎 ∈ R 𝑑𝑎 , 𝑢 𝑏 ∈ R 𝑑 𝑏 and 𝑑 𝑎 + 𝑑 𝑏 = 𝑑. We can then update 𝑢 𝑎 and 𝑢 𝑏 in turn by sampling from the target's full conditionals 𝜋 𝑎 (𝑢 𝑎 |𝑢 -𝑎 ) := 𝜋(𝑢) 𝜋(𝑢 -𝑎 ) , 𝜋 𝑏 (𝑢 𝑏 |𝑢 -𝑏 ) := 𝜋(𝑢) 𝜋(𝑢 -𝑏 ) , where 𝜋(𝑢 -𝑎 ) := ∫︁ R 𝑑 𝑏 𝜋(𝑣, 𝑢 𝑏 ) 𝑑𝑣, 𝜋(𝑢 -𝑏 ) := ∫︁ R 𝑑𝑎 𝜋(𝑢 𝑎 , 𝑣) 𝑑𝑣. The procedure is outlined in Algorithm 15.4: Algorithm 15.4 Block-Wise Gibbs Sampler 1: Input: Target distribution 𝜋, initial distribution 𝜋 0 , test function 𝑓, number of samples 𝑁. 2: Initial Draw: Draw initial sample 𝑢 (0) ∼ 𝜋 0 . 3: Subsequent Samples: For 𝑛 = 0, 1, . . . , 𝑁 -1 1. Sample 𝑢 (𝑛+1) 𝑎 ∼ 𝜋 𝑎 (︁ • |𝑢 (𝑛) -𝑎 )︁ . 2. Sample 𝑢 (𝑛+1) 𝑏 ∼ 𝜋 𝑏 (︁ • |𝑢 (𝑛+1) -𝑏 )︁ . 3. Set 𝑢 (𝑛+1) = (︀ 𝑢 (𝑛+1) 𝑎 , 𝑢 (𝑛+1) 𝑏 )︀ . 4: End for 5: Output: Samples {𝑢 (𝑛) } 𝑁 𝑛=1 and approximation E 𝜋 [𝑓 (𝑢 (𝑛) )] ≈ 1 𝑁 ∑︀ 𝑁 𝑛=1 𝑓 (𝑢 (𝑛) ). Remark 15.24. The Gibbs sampler tends to converge more quickly when highly correlated coordinates are blocked together. On the other hand conditionals can become hard to sample from if many coordinates are blocked together. Optimizing this trade-off is central to the successful implementation of the Gibbs sampler. In determining how to optimize this trade-off it is useful to note that, when sampling from the full conditionals is challenging, a Metropolis-Hastings algorithm can be employed to sample from them. The resulting algorithm, which goes under the name of Metropolis-within-Gibbs, preserves the desired stationary distribution if any number of Metropolis-Hastings steps is used, instead of an exact sample from a conditional. However replacing exact condtional sampling with approximate Metropolis-Hastings steps will typically slow down the rate of convergence of the overall algorithm. ♢ Automatic Differentiation Gradients (and higher-order derivatives) are often difficult to obtain in closed form for complex cost functions. Finite difference approximations, on the other hand, are often inaccurate and expensive for high-dimensional problems. Automatic differentiation involves repeated application of chain rule on elementary operations that enables computing derivatives accurately to working precision. That is, suppose an output variable 𝑦 ∈ R 𝑑𝑛 is related to an input variable 𝑥 ∈ R 𝑑 0 by some function 𝑓 , i.e., 𝑦 = 𝑓 (𝑥), and suppose that the Jacobian of 𝑓 is not readily available in closed form. We assume the implementation of 𝑓 in computer code is made up of 𝑛 elementary operations 𝑓 𝑖 : R 𝑑 𝑖-1 → R 𝑑 𝑖 (for instance, addition, multiplication, logarithms, etc.), such that 𝑓 (𝑥) = 𝑓 𝑛 ∘ 𝑓 𝑛-1 ∘ • • • ∘ 𝑓 1 (𝑥). (15.21) We assume further that the Jacobians for these elementary operations, 𝐷𝑓 𝑖 : R 𝑑 𝑖-1 → R 𝑑 𝑖 ×𝑑 𝑖-1 , are available in closed form. The goal of automatic differentiation is to evaluate 𝐷𝑓 (𝑥) from the Jacobians of the elementary operations. We define the identity function 𝑔 0 = 𝑥 and then recursively define function 𝑔 𝑖 = 𝑓 𝑖 (𝑔 𝑖-1 ); note that then 𝑓 = 𝑔 𝑛 . By representing the output 𝑔 𝑖 of each step in the recursion by a node, we can use a graph to represent the function evaluation where the directed edges encode the input dependencies 𝑔 𝑖-1 (the parents) that are required in the evaluation of 𝑓 𝑖 . This is known as the computational graph. The nodes without parents correspond to the original inputs 𝑥 to the function 𝑓 . After constructing the graph, we can define the forward mode as the process of computing the output 𝑦, and possibly the gradient of 𝑓 , by traversing the graph starting from the inputs 𝑥. The reverse mode starts with the final output node and traverses the graph back to the inputs to compute the gradient of 𝑓 . The following two subsections demonstrate how the full Jacobian of 𝑓 is assembled in forward and reverse mode automatic differentiation. Remark 15.25. In realistic implementations, instead of multiplying the full Jacobians of each operation, only the partial derivatives of variables that interact with each other in the computational graph are used. For simplicity of exposition, we present the algorithms with the full Jacobians here. Assuming for simplicity that all the 𝑑 𝑖 except for 𝑑 0 and 𝑑 𝑛 are fixed at 𝑑, we have a cost c 𝑓 given by c 𝑓 = 𝑑 2 (𝑛 -2)𝑑 0 + 𝑑 𝑛 𝑑𝑑 0 . (15.23) We now introduce as different way of computing 𝐷𝑓 (•) which reverses the order of arithmetic operations, resulting in a different computational cost c 𝑟 . The difference in computational cost compared to forward mode differentiation stems from the fact that, while matrix multiplication is associative, the complexity is not. Reverse Mode Reverse mode automatic differentiation also computes the product of Jacobians (15.22) , but instead of computing the product from right to left, it computes it from left to right. The algorithmic implementation requires a forward pass to evaluate the function and a backwards pass to compute the derivative: Algorithm 15.6 Reverse Mode Automatic Differentiation 1: Input: Functions {𝑓 𝑖 (•)} 𝑛 𝑖=1 , corresponding Jacobians {𝐷𝑓 𝑖 (•)} 𝑛 𝑖=1 , and input 𝑥. 2: Set 𝑔 1 = 𝑓 1 (𝑥). Again assuming that all the 𝑑 𝑖 are fixed at 𝑑 except for 𝑑 0 and 𝑑 𝑛 , the total cost c 𝑟 is given by c 𝑟 = 𝑑 2 (𝑛 -2)𝑑 𝑛 + 𝑑 𝑛 𝑑𝑑 0 . (15.24) We may now compare c 𝑓 and c 𝑟 given by (15.23) and (15.24) . This comparison shows that forward mode is advantageous when 𝑑 𝑛 is large whereas reverse mode is advantageous when 𝑑 0 is large. As an example, if 𝑑 0 is fixed and 𝑑 𝑛 = 𝑑, then, in the large 𝑑 asymptotic, forward mode has quadratic cost in 𝑑 whereas reverse mode has cubic cost in 𝑑. In contrast, if 𝑑 𝑛 is fixed and 𝑑 0 = 𝑑, then, in the large 𝑑 asymptotic, forward mode has cubic cost in 𝑑 whereas reverse mode has quadratic cost in 𝑑. Remark 15.26. In many machine learning tasks inputs are high dimensional but outputs are real-valued; in that setting, reverse mode differentiation is preferred. Backpropagation, a common method for computing the gradient of the objective function with respect to the weights of a neural network, is a special case of reverse mode differentiation. ♢ Bibliography Optimization is reviewed in a number of textbooks, including [72, 323] . We refer to [68] for an accessible survey article on optimization for machine learning. Convergence proofs for gradient descent and stochastic gradient descent may be found in [378] in simple settings. Nesterov's accelerated gradient method and its rate of convergence is discussed in detail in [319] . Gradient descent can be formulated in continuous time as a gradient flow; see [192, 15] . Newton and Gauss-Newton methods are surveyed in many textbooks; see for instance [323] . The papers [47, 46] established a direct connection between Gauss-Newton methods and the extended Kalman filter (ExKF) discussed in Subsection 6.3.3. The origins of ensemble Kalman methods for inverse problems stem from the observation that the analysis step in data assimilation involves solution of an inverse problem; this observation is formalized, for instance, in [360] . Early developments of ensemble Kalman methods for inverse problems include [100, 138] . For a history of the use of ensemble Kalman methods to solve inverse problems, see the bibliography sections in [89, 80] . Ensemble Kalman inversion (EKI) as defined here was introduced in [227] , and various adaptations of it may be found in [224, 225, 88, 226, 89] ; it is also discussed in the final chapter of the textbook [378] , which follows the presentation in [89] . For analysis in continuous time see [383, 384] . A mean field perspective on EKI and an interpretation of the method as a covariance-preconditioned gradient flow is provided in [80] . Variants of EKI are discussed in [220, 221] . The use of EKI to train machine learning models, or machine learning models embedded within physical models, is discussed in [253, 283, 449, 93] . There is another growing class of methodologies, which also solve optimization problems in a derivative-free fashion, based on the idea of interacting particle systems which achieve consensus [342] . In contrast to ensemblebased methods, these methods do not invoke a Gaussian approximation and hence can achieve provably good approximations of global optima on a wide-range of problems [87] ; however they have yet to be shown to be effective on the large-scale applications where ensemble Kalman methods excel. The EM framework for maximum likelihood estimation was introduced in [126] ; see [303] for a review and see [370] for an insightful connection to Gibbs sampling. The generalization of the EM framework for optimization problems where the objective does not necessarily involve conditional expectations is known as the Minorize-Maximization (MM), or equivalently Majorize-Minimization, algorithm. An overview of MM algorithms and their properties can be found in [263] . Reinforcement learning, a form of online optimization, is seeing growing use in numerous applications of machine learning. For historical underpinnings of the field see [233, 407] ; for recent developments see [443, 26] . Bayesian optimization is another popular online optimization method [153] based on the idea of building a surrogate model of the objective by evaluating it at carefully chosen query points. Bayesian optimization is a particularly useful global optimization method in applications where the objective function is expensive to evaluate and the dimension is moderate. As discussed for instance in [165, 373] , optimization and sampling are closely related. A workhouse sampling algorithm for Bayesian computation is Monte Carlo Markov Chain (MCMC) that builds a sequence of correlated samples from a target distribution; see [159, 306] for a general introduction. MCMC methods can also be useful for nonconvex optimization by constructing a target distribution from the objective function. We refer to Section 1.6 for more references and discussion on Monte Carlo methods, including quasi Monte Carlo (QMC) and sequential Monte Carlo (SMC). A detailed overview of automatic differentiation is given in [185] . For a survey of automatic differentiation in machine learning, we refer to [44] . Automatic differentiation is a central tool in machine learning, allowing users to implement highly complex neural network architectures without worrying about the calculation of gradients to optimize the parameters. Automatic differentiation has also contributed to popularize gradientbased sampling algorithms based on Langevin or Hamiltonian dynamics in applications where calculation of derivatives was previously unfeasible. Automatic differentiation is now available in most programming languages as a package or extension. Alphabetical Index Symbols 3DVar, 80, 85-88, 101, 102, 105, 139, 157, 158 -like, 88, 96 accuracy, 101 amortized, 158 cycled, 86, 105 ensemble, 96 4DVar, 80, 97, 105, 107, 115, 117, 157, 158 amortized, 159 strong-constraint, 98 weak-constraint, 97, 98 A activation function, 202 algorithm, 36, 37, 57, 63, 73 auto-differentiation, 259, 260 EM, 121, 250, 253 energy distance gradient, 200 filtering, 81, 85, 115, 149 gradient descent, 244 machine learning, 173 MCMC, 75, 261 offline, 84 online, 84 random, 63 sampling, 199 smoothing, 81, 115 state estimation, 138 transport, 133 variational, 109 amortization, 2, 59, 110, 112, 113, 115, 137, 157 gap, 76 analysis, 82, 86, 88, 93, 126, 128, 133, 145 approximate Bayesian computation, 24 approximation, 201, 211 attractor global, 232 auto-differentiation, 87, 126, 127, 131, 132, 136, 169 autoencoder, 43, 107, 225, 227, 229 variational, 76, 107, 225-229 autoregressive, 238 AR, 238 linear model, 100, 231, 237 VAR, 237 vector, 237 B backpropagation, 260 Bayes Theorem, 16, 18, 51, 60, 63, 82, 109, 208, 252 variational, 34, 43, 110 Bayesian, 15, 26, 30, 131 inference, 50, 131, 214 inverse problem, 15, 21, 29, 30, 46 inversion, 15, 30, 211 Bayesian optimization, 31, 261 C Cameron-Matin, 211 Cauchy-Schwarz inequality, 175, 177 chaos, 233, 235 Cholesky, 54, 115 clustering, 213, 229 contraction mapping principle, 247 convergence linear, 244, 247, 250 quadratic, 247 variational, 160, 161 smoothing, amortized, 166 spectral theorem, 226 spread-error ratio, 97 state, 119, 121, 123, 130-132 state augmentation, 132 state estimation, 81, 85-87, 89, 133 state estimator, 97, 98 state-space, 132 stationary random feature, 206 strictly, 235 wide-sense, 235 stochastic dynamics model, 79, 82, 99, 119, 120, 123, 127 supervised learning, 3, 53, 120, 125, 135, 158, 201, 211, 213 surrogate, 25, 26, 28-31, 80, 101, 103, 104, 121, 131 T Takens' Theorem, 237 target distribution, 257, 258 time-causal, 81 time-series forecasting, 3 topic models, 41 total variation distance, 184 training data, 45 transport, 2, 3, 43, 45, 50-52, 135, 177, 216, 217, 225, 239, 242 amortized, 161 Knothe-Rosenblatt, 76 map, 178 measure, 216, 226 optimal, 177, 178, 199, 216, 229 optimal, Kantorovich, 178, 199 optimal, Monge, 178, 199, 216 prior-to-posterior, 52 transports, 59 triangular block, 66 Tweedie's formula, 224 U UKF, 88 uncertainty quantification, 211 unscented Kalman filter, 88 unsupervised learning, 3, 213, 229 V variational 3-dimensional, see 3DVar 4-dimensional, see 4DVar autoencoder, see autoencoder, variational Bayes, 149 Bayesian method, 50, 131, 132 characterization of total variation, 176 formulation, 33 formulation of Bayes Theorem, 106, 109, 252 formulation of filtering, 111 formulation of smoothing, 110 inference, 51 variational Bayes, 2 verification, 200 W well-posed, 15, 21, 23, 27, 28, 30 Theorem 1 . 3 . 13 Let Assumption 1.1 and Data Assumption 1.2 hold, and assume further that 𝑍 = 𝑍(𝑦) ) P(𝑢) 𝑑𝑢 = 𝑍, and so the assumption that 𝑍 > 0 ensures that the data could indeed have been obtained from the posulated model. Since 𝑍 > 0 we have, by combining the two identities above, the objective function J(•) is equivalent to determining a MAP estimator defined by maximizing the posterior probability density function 𝜋 𝑦 (•). Example 1 . 15 (Example 1 . 16 ( 115116 Posterior Covariance and Variance). Letting 𝑓 : R 𝑑 → R 𝑑 × R 𝑑 be the outer product 𝑓 (𝑢) = (𝑢 -𝑢 PM ) ⊗ (𝑢 -𝑢 PM ), we obtain the posterior covarianceE 𝜋 𝑦 [(𝑢 -𝑢 PM ) ⊗ (𝑢 -𝑢 PM )].The trace of the posterior variance, given byE 𝜋 𝑦 [|𝑢 -𝑢 PM | 2 ]gives the posterior variance, which quantifies the spread of the posterior distribution. ♢ Posterior Probabilities). l 𝛿 (𝑦|𝑢)| ≤ 𝜙(𝑢)𝛿, for some 𝜙(𝑢) satsifyingE 𝜌 [𝜙 2 (𝑢)] ≤ 𝐾 2 1 ; (ii) sup 𝑢∈R 𝑑 (| √︀ l(𝑦|𝑢)| + | √︀ l 𝛿 (𝑦|𝑢)|) ≤ 𝐾 2 . Example 1 . 22 . 122 For a linear forward model, 𝐺(𝑢) = 𝐴𝑢, the data model (1.17) becomes 𝑦 = 𝐴𝑢 + (𝐴𝜗 + 𝜂). (︀ 𝑦 -𝐺(𝑢) )︀ and l 𝛿 (𝑢) = 𝜈 (︀ 𝑦 -𝐺 𝛿 (𝑢) )︀ denote the true and approximate likelihoods. Thus we may define the true and approximate posterior distributions by 𝜋 𝑦 (𝑢) = 1 𝑍 l(𝑢)𝜌(𝑢) and 𝜋 𝑦 𝛿 (𝑢) = 1 𝑍 𝛿 l 𝛿 (𝑢)𝜌(𝑢); (i) sup 𝑢∈𝐷 | √︀ l(𝑢) -√︀ l 𝛿 (𝑢)| ≤ 𝐾 1 𝛿; (ii) sup 𝑢∈𝐷 (| √︀ l(𝑢)| + | √︀ l 𝛿 (𝑢)|) ≤ 𝐾 2 .Remark 1.26. Parts (i) and (ii) of Assumption 1.25 can be verified for surrogate models obtained using training data of the form in Data Assumption 1.23. Approximation theoretic results for supervised learning are discussed in Chapter 12, see (12.21) in particular. ♢ Note also that combining (i) and (ii) gives sup 𝑢∈𝐷 |l(𝑢) -l 𝛿 (𝑢)| ≤ 𝐾 1 𝐾 2 𝛿. (1.23) Theorem 1 . 27 . 127 Under Assumption 1.25 there exist Δ, 𝑐 > 0 such that, for all 𝛿 ∈ (0, Δ),D H (𝜋 𝑦 , 𝜋 𝑦 𝛿 ) ≤ 𝑐𝛿. Proposition 2 . 12 . 212 Define the unnormalized density ̃︀ 𝜋(𝑢) = 𝜌(𝑢)l(𝑦|𝑢) and define the normalization constant 𝑍 by 𝜋 = 𝑍 -1 ̃︀ 𝜋, assuming that 𝑍 > 0. Then, the maximizer𝑞 opt ∈ arg max 𝑞∈𝒫 ELBO(̃︀ 𝜋, 𝑞)is attained at the posterior distribution 𝑞 opt = 𝜋. Furthermore, the maximum value is ELBO(̃︀ 𝜋, 𝑞 opt ) = log(𝑍). Example 3 . 1 . 31 Consider finding 𝑢 from 𝑦 where 𝑦 = 𝑢 + 𝜂 Assumption 3 . 5 .Theorem 3 . 6 . 3536 The prior distributions 𝜌 and 𝜌 ′ are both supported on bounded open set 𝐷 ⊂ R 𝑑 . There exists 𝐾 ∈ (0, ∞) such that sup 𝑢∈𝐷 l(𝑢) = 𝐾. Let Assumption 3.5 hold. Consider posteriors 𝜋, 𝜋 ′ in (3.3), (3.5) corresponding, respectively, to priors 𝜌, 𝜌 ′ . Then, we have log 𝜌 ∘ 𝑇 (𝑢; 𝜃) + log l ∘ 𝑇 (𝑢; 𝜃) + log det𝐷 𝑢 𝑇 (𝑢; 𝜃) Algorithm 4 . 1 41 Transport-assisted MCMC1: Input: Target distribution 𝜋, Number of samples 𝑁 2: Define preconditioned target distribution 𝑇 Remark 5 . 10 . 510 The likelihood function may require marginalizing with respect to a latent random variable 𝑧. That is,l(𝑦|𝑢) = ∫︁ R 𝑑𝑧 P(𝑦|𝑢, 𝑧)P(𝑧|𝑢) 𝑑𝑧. Theorem 5 . 23 . 523 If 𝜚 2 is a log-concave reference density, then the optimization problem Figure 6 . 1 61 Figure 6.1 Dynamics and observation models underlying data assimilation problems. Figure 6 . 2 62 Figure 6.2 Prediction and analysis steps combined. this Gaussian mixture. Consider now the empirical measure, defined by these samples: Example 9 . 3 . 93 The canonical example of a distance-like deterministic scoring rule is d(𝑣, 𝑤) = |𝑣 -𝑤| 2 , the squared Euclidean distance.♢ Theorem 9 . 7 . 97 Consider the 3DVar algorithm (9.13) with Ψ(•) = 𝐴• and ℎ(•) = 𝐻•. Remark 9 . 15 . 915 For the transport map 𝑇 in(9.29)  we have removed explicit dependence on the ensemble {̂︀ 𝑣 (ℓ) 𝑗+1 } 𝑁 ℓ=1 . Note, however, that this dependence is present in the optimal choice of parameter 𝜃 : the permutation-invariant dependence on {̂︀ 𝑣 (ℓ) 𝑗+1 } 𝑁 ℓ=1 , encoded in the optimal 𝜃 determined by use of Data Assumption 9.12, represents dependence on the empirical approximation ̂︀ )} 𝑁 ℓ=1 here are different from the {𝑣 (ℓ) Remark 9 . 22 . 922 We have not commented on how the particles from time step 𝑗 are chosen: {𝑣 (𝑚) 𝑗 } 𝑀 𝑚=1 for the bootstrap particle filter, and the particles {𝑣 (𝑛) )} 𝑁 ℓ=1 here are different from the {𝑣 (ℓ)𝑗+1 } 𝑀 ℓ=1 used to define 𝜋 est 𝑗+1 . We then define 𝜃 ⋆ by J 𝑀,𝑁 (𝜃) = D MMD .5a)𝜃 ⋆ ∈ arg min 𝜃∈Θ J(𝜃).(10.5b)Remark 10.4. Remark 10 . 9 . 109 The two terms in the loss function L 𝑁 have opposing behavior. While minimizing the first term encourages the map 𝑇 (•, ̂︀ 𝑦 (𝑚) 𝑗+1 ; 𝜃) to push-forward the forecast ensemble to the state ̂︀ 𝑣 (𝑚) 𝑗+1 matching the observation ̂︀ 𝑦 (𝑚) Theorem 10 . 14 . 1014 Let 𝑆 be an affine transport map, invertible with respect to 𝑣, of the form 𝑆(𝑦, 𝑣; 𝜃) = 𝐴(𝑣 + 𝐵𝑦 + 𝑐), 𝑛=1 ∼ 𝜋 𝑗 , we can sample synthetic observations using the dynamics and observation model (6.1), (6.2). That is, Figure 11 . 1 111 Figure 11.1 Diagram representing relationships between three different ways of quantifying closeness between probability measures. Metrics impose more restrictive conditions than divergences: only some divergences are metrics. Furthermore a subset of expected scoring rules --those based on strictly proper scoring rules-lead to divergences, and in some cases to metrics. 2 . 2 Positive: D(𝜌, 𝜚) = 0 if and only if 𝜌 = 𝜚. 3. Symmetric: D(𝜌, 𝜚) = D(𝜚, 𝜌). 4. Sub-additive (triangle inequality): D(𝜌, 𝜚) ≤ D(𝜌, 𝜋) + D(𝜋, 𝜚) for all 𝜋 ∈ 𝒫(R 𝑑 ). Example 11 . 9 . 119 Recall Example 11.4 and again consider the Gaussians 𝜌 = 𝒩 (𝑚 𝜌 , Σ 𝜌 ) and 𝜚 = 𝒩 (𝑚 𝜚 , Σ 𝜚 ). It may be shown that, if the metric d induced by the Euclidean norm | • | is used, then Lemma 11 . 10 . 1110 Let 𝜌, 𝜚 be the densities of two real-valued random variables with invertible cumulative distribution functions 𝐹𝜌 , 𝐹 𝜚 : R → [0, 1]. Then, for 𝑝 ≥ 1, the Wasserstein-𝑝 distance with metric d induced by the Euclidean norm | • | has the form Figure 11 . 2 112 Figure 11.2 Integration of the difference between cumulative distribution functions (left) and their inverses (right) Example 11 . 23 . 1123 for all 𝑓 in the RKHS defined by 𝑐(•, •) implies that 𝜌 = 𝜚. Characteristic kernels lead to the MMD being a metric; see the bibliography Section 11.4. ♢ Gaussian and Laplace kernels are examples of characteristic kernels. Choosing 𝑐(𝑢, 𝑢 ′ ) = -|𝑢 -𝑢 ′ |, the negative of the Euclidean distance, produces the energy distance, which we now define. It is also a metric, despite corresponding to a negative kernel. See the bibliography Section 11.4. ♢ Definition 11.24. Lemma 11 . 37 . 1137 Let 𝜌(𝑢, 𝑣) = 𝜌(𝑢)𝜌(𝑣|𝑢) and 𝜚(𝑢, 𝑣) = 𝜚(𝑢)𝜚(𝑣|𝑢) be two joint probability densities. It holds thatD KL (︀ Proof. It suffices to prove only the first inequality since it implies the second by Lemma 11.2. Define function 𝜙 : R + ↦ → R by 𝜙(𝑥) := 𝑥 -1 -log 𝑥. Definition 11 . 59 . 1159 The logarithmic score LS :𝒫(R 𝑑 ) × R 𝑑 → R is defined by LS(𝜌, 𝑣) := -log 𝜌(𝑣). (11.36) The expected logarithmic score LS : 𝒫(R 𝑑 ) × 𝒫(R 𝑑 ) → R is then defined by LS(𝜌, 𝜚) := E 𝑣∼𝜚 [LS(𝜌, 𝑣)] = ∫︁ LS(𝜌, 𝑣)𝜚(𝑣) 𝑑𝑣. Definition 11 . 61 . 1161 The Dawid-Sebastiani score DS : 𝒫(R 𝑑 ) × R 𝑑 → R is defined byDS(𝜌, 𝑣) := |𝑣 -𝑚| 2𝑚 and 𝐶 are the mean and covariance of 𝜌. The expected Dawid-Sebastiani score DS : 𝒫(R 𝑑 ) × 𝒫(R 𝑑 ) → R is then defined by DS(𝜌, 𝜚) := ∫︁ DS(𝜌, 𝑣)𝜚(𝑣) 𝑑𝑣. (11.39) ♢ Remark 11.62. The Dawid-Sebastiani score is equivalent, up to a linear transformation, to the logarithmic score in the Gaussian case. It can be used, however, for general distributions with finite mean and covariance. ♢ Definition 11 . 63 . 1163 Given noise distribution 𝑟(• |•) and probability 𝜌 define︀ 𝜚(̃︀ 𝑣) = ∫︁ Definition 12 . 4 .Example 12 . 5 . 124125 An activation function 𝜎 : R → R is a monotonic non-decreasing function.It is extended to 𝜎: R 𝑠 → R 𝑠 pointwise: 𝜎(𝑢) 𝑚 = 𝜎(𝑢 𝑚 ) for 𝑢 = (𝑢 1 , . . . , 𝑢 𝑚 , . . . , 𝑢 𝑠 ) and 𝜎(𝑢) = (︀ 𝜎(𝑢) 1 , . . . , 𝜎(𝑢) 𝑚 , . . . , 𝜎(𝑢) 𝑠 )︀ . ♢The Rectified Linear Unit (ReLU) activation function is 𝜎(𝑢) = max(𝑢, 0). The Gaussian Error Linear Unit (GELU) activation function is 𝜎(𝑢) = 𝑢𝐹 (𝑢), where 𝐹 is the cumulative distribution function of the scalar unit centred Gaussian. The Scaled Exponential Linear Unit (SELU) activation function is𝜎(𝑢) Example 12 . 9 . 129 Random Fourier features take the form𝜙(𝑢; 𝜗) = cos (︀ ⟨𝜔, 𝑢⟩ + 𝑏 )︀ Theorem 12 . 15 . 1215 Function 𝜓 ⋆ solving (12.18) has the form𝜓 ⋆ (𝑢) = 𝑁 ∑︁ 𝑛=1 𝛼 ⋆𝑛 𝑐(𝑢, 𝑢(𝑛) ).(12.19) 𝑁 = {︁ 𝜓 ∈ 𝒦 : 𝜓 = 𝑁 ∑︁ 𝑛=1 𝛼 𝑛 𝑐(•, 𝑢 (𝑛) ), 𝛼 𝑛 ∈ R ∀𝑛 ∈ {1, . . . , 𝑁 } }︁ and define the optimization problem 𝜓 ⋆ ∈ arg min 𝜓∈𝒦 𝑁 𝑢) -log 𝑞(𝑢; 𝜃)]︁ , which involves the unknown density ϒ. Since the optimization is over the parameter 𝜃 in the density 𝑞(•; 𝜃), minimizing F over 𝜃 is the same as minimizing-E 𝑢∼ϒ [︁log 𝑞(𝑢; 𝜃)]︁ 𝑣 𝑗+1 = 𝐻(𝑣 𝑗 ; 𝜃), 𝑗 = 0, . . . , 𝐽 -1, (13.9a) 𝑣 0 = 𝑧, (13.9b) where we define 𝑢 := 𝑣 𝐽 ; here 𝐻(•; 𝜃) : R 𝑑 → R 𝑑 . Then 𝑢 = 𝑔(𝑧; 𝜃) where 𝑔(•; 𝜃) = 𝐻(•; 𝜃) ∘ 𝐻(•; 𝜃) ∘ • • • ∘ 𝐻(•; 𝜃), (13.10) By induction, we obtain that log 𝑝 𝐽 (𝑣 𝐽 ) = log 𝑝 0 (𝑣 0 ) -𝐽-1 ∑︁ 𝑗=0 log det𝐷𝐻(𝑣 𝑗 ; 𝜃), and hence log 𝑔 ♯ 𝜁(𝑢) = log 𝜁(𝑧) -𝐽-1 ∑︁ 𝑗=0 log det𝐷𝐻(𝑣 𝑗 ; 𝜃). (13.11) Proposition 13 . 13 . 2 ⟨︀ 13132 Assume the score of ϒ is square-integrable, i.e.,∫︀ |𝐷 log ϒ(𝑢)| 2 ϒ(𝑢) 𝑑𝑢 < ∞.Then, the optimal score that minimizes J(𝜃; ϒ) as defined in(13.20) is given by𝑠 ⋆ = 𝑠 𝜃 ⋆ where 𝜃 ⋆ ∈ arg min 𝜃∈Θ ∫︁ (︁ |𝑠 𝜃 (𝑢)| 2 + 2 div 𝑠 𝜃 (𝑢)Expanding the Euclidean norm, the least-squares objective (13.20) yields J(𝜃; ϒ) = ∫︁ (︁ |𝑠 𝜃 (𝑢)| 2 -𝑠 𝜃 (𝑢), 𝐷 log ϒ(𝑢) ⟩︀ + |𝐷 log ϒ(𝑢)| 2 )︁ ϒ(𝑢) 𝑑𝑢. Definition 15 . 1 .Definition 15 . 3 . 151153 A function J : R 𝑑 → R is called convex if, for all 𝑢, 𝑣 ∈ R 𝑑 and for all 𝜃 ∈ [0, 1], we have thatJ(𝜃𝑢 + (1 -𝜃)𝑣) ≤ 𝜃J(𝑢) + (1 -𝜃)J(𝑣).A function is called strictly convex if the inequality is strict for 𝑢 ̸ = 𝑣. ♢ Definition 15.2. A set 𝑆 in a vector space is convex if, for all 𝑢, 𝑣 ∈ 𝑆 and for all 𝜃 ∈ [0, 1], we have that 𝜃𝑢 + (1 -𝜃)𝑣 ∈ 𝑆.Our starting point is the idea of gradient flow:Given function J ∈ 𝐶 1 (R 𝑑 , R + )with compact level sets, gradient flow refers to the solution generated by the differential equation 𝑑𝑢 𝑑𝑡 = -𝐷J(𝑢), 𝑢(0) = 𝑢 0 . (15.1) Definition 15 . 4 .♢Remark 15 . 5 . 154155 Given function J ∈ 𝐶 1 (R 𝑑 , R), gradient descent refers to the iterative generation of sequence {𝑢 𝑗 } 𝑗∈Z + from a sequence of step-sizes {𝛼 𝑗 } 𝑗∈Z + by picking an 𝑢 0 ∈ R 𝑑 and then iterating as follows:𝑢 𝑗+1 = 𝑢 𝑗 -𝛼 𝑗 𝐷J(𝑢 𝑗 ), 𝑗 ∈ Z + . (15.2)Note that equation (15.2) corresponds to Euler discretization of (15.1). 1 2 ⟨︀(ℓ 2 |𝑣 -𝑢| 2 . 2 = 2 = 122222 The existence of a unique global minimizer is a consequence of the convexity of J implied by the bounds on the Hessian. By Taylor's theorem for a twice-differentiable function J we haveJ(𝑣) = J(𝑢) + ⟨︀ 𝐷J(𝑢), 𝑣 -𝑢 ⟩︀ + 𝑣 -𝑢), 𝐻(𝑣 -𝑢)From the uniform bound on the Hessian, we have the upper and lower boundsJ(𝑣) ≤ J(𝑢) + ⟨︀ 𝐷J(𝑢), 𝑣 -𝑢 ⟩︀ + 𝐿 2 |𝑣 -𝑢| 2 , (15.3a) J(𝑣) ≥ J(𝑢) + ⟨︀ 𝐷J(𝑢), 𝑣 -𝑢 ⟩︀ + (15.3b) Taking 𝑣 = 𝑢 𝑗+1 and 𝑢 = 𝑢 𝑗 in (15.3a), and employing the gradient descent update 𝑢 𝑗+1 = 𝑢 𝑗 -𝛼 𝑗 𝐷J(𝑢 𝑗 ), we have J(𝑢 𝑗+1 ) ≤ J(𝑢 𝑗 ) + ⟨𝐷J(𝑢 𝑗 ), 𝑢 𝑗+1 -𝑢 𝑗 ⟩ + 𝐿 2 |𝑢 𝑗+1 -𝑢 𝑗 | J(𝑢 𝑗 ) -𝛼 𝑗 |𝐷J(𝑢 𝑗 )| 2 + 𝐿 2 |𝛼 𝑗 𝐷J(𝑢 𝑗 )| J(𝑢 𝑗 ) -1 2𝐿 |𝐷J(𝑢 𝑗 )| 2 , (15.4) Definition 15 . 10 . 1510 Given function J ∈ 𝐶 2 (R 𝑑 , R) with invertible Hessian, Newton's method refers to the iterative generation of sequence {𝑢 𝑗 } 𝑗∈Z + by picking an 𝑢 0 ∈ R 𝑑 and then iterating as follows:𝑢 𝑗+1 = 𝑢 𝑗 -(︀ 𝐷 2 J(𝑢 𝑗 ) )︀ -1 𝐷J(𝑢 𝑗 ). (15.8) Theorem 15 . 11 . 1511 If J is convex, then J(𝑢 𝑗+1 ) is the local minimum of the quadratic approximation of J around 𝑢 𝑗 . Definition 15 . 12 . 1512 1 (R 𝑑 , R 𝑚 ) and let G = (G 1 , . . . , G 𝑚 ) where G 𝑖 ∈ 𝐶 1 (R 𝑑 , R), 1 ≤ 𝑖 ≤ 𝑚.Consider the nonlinear least squares optimization problemGiven function G ∈ 𝐶 1 (R 𝑑 , R 𝑚 ) such that ( (︀ 𝐷 2 G 2 ) ⊤ 𝐷G(𝑢) 𝑖 (𝑢) )︀ 𝑎𝑏 . (15.11) 𝑢(𝑛)𝑗+1 = 𝑢 (𝑛) 𝑗 + 𝐾 𝑗+1 (︀ 𝑦 -𝜂 (𝑛) 𝑗+1 -𝐺(𝑢 (𝑛) 𝑗 ) )︀ , 𝑛 = 1, . . . , 𝑁, 𝜂 (𝑛) 𝑗+1 ∼ 𝒩 (0, Γ) i.i.d.,where the matrix 𝐾 𝑗+1 is calculated according to= 𝐶 𝑔𝑔 𝑗+1 + Γ, 𝐾 𝑗+1 = 𝐶 𝑢𝑔 𝑗+1 Definition 15 . 15 .♢Remark 15 . 16 . 15151516 The evidence lower bound (ELBO) of a probability density 𝑞 in R 𝑑𝑧 with respect to an unnormalized density ̃︀ 𝜋 in R 𝑑𝑧 is given byELBO(̃︀ 𝜋, 𝑞) = E 𝑞 [log ̃︀ 𝜋(𝑧)] -E 𝑞 [log 𝑞(𝑧)].Recall the KL divergence introduced in Definition 11.34. Note that if 𝜋 is a normalized probability density then ELBO(𝜋, 𝑞) = -D KL (𝑞‖𝜋). Lemma 15 . 17 .= 1517 Let 𝑞 be a probability density function in R 𝑑𝑧 and let P(𝑦, • |𝑢), viewed as a function of 𝑧 ∈ R 𝑑𝑧 with 𝑦 ∈ R 𝑘 and 𝑢 ∈ R 𝑑 fixed, be the unnormalized density of latent variable 𝑧 ∈ R 𝑑𝑧 . It holds thatℒ(𝑞, 𝑢) := ELBO (︀ P(𝑦, • |𝑢), 𝑞 )︀ ≤ log J(𝑢).Proof. This lower bound follows by using Jensen inequality: log J(𝑢) = log ∫︁ ELBO(P(𝑦, • |𝑢), 𝑞). Theorem 15 . 18 . 1518 Let 𝑞 be a probability density function over latent variable 𝑧 ∈ R𝑑𝑧 . It holds that log J(𝑢) = ℒ(𝑞, 𝑢) + D KL Using the definition of ℒ(𝑞, 𝑢), product rule, and the Definition 11.34 of the KL divergence, we have ℒ(𝑞, 𝑢) = ∫︁ log (︂ P(𝑦, 𝑧|𝑢) 𝑞(𝑧) • |𝑦, 𝑢 𝑗 ) )︀ over 𝑞. Density 𝑞 𝑗 (𝑧) = P(𝑧|𝑦, 𝑢 𝑗 ) achieves the global minimum of this non-negative function and we have that D KL (︀ 𝑞 𝑗 ‖ P(• |𝑦, 𝑢) Algorithm 15 . 1 Maximization 1 : 2 : 5 : 151125 ExpectationInput: Initialization 𝑢 0 .For 𝑗 = 0, 1, . . . do the following expectation and maximization steps:3: E-Step: Compute E 𝑧∼P(𝑧|𝑦,𝑢 𝑗 )𝑦, 𝑧|𝑢) P(𝑧|𝑦, 𝑢 𝑗 ) 𝑑𝑧. 4: M-Step: Compute 𝑢 𝑗+1 ∈ arg max 𝑢 E 𝑧∼P(𝑧|𝑦,𝑢 𝑗 ) Output: Parameter estimates {𝑢 𝑗 } 𝑗≥0 . Theorem 15 . 19 . 1519 Let {𝑢 𝑗 } 𝑗≥0 be the iterates of the EM Algorithm 15.1. Then,J(𝑢 𝑗 ) ≤ J(𝑢 𝑗+1 ).Proof. Since the logarithm is an increasing function in its domain, it suffices to show that log J(𝑢 𝑗 ) ≤ log J(𝑢 𝑗+1 ). Let 𝑞 𝑗 (𝑧) = P(𝑧|𝑦, 𝑢 𝑗 ). Using the log-likelihood characterization in Theorem 15.18, it holds thatlog J(𝑢 𝑗+1 ) = ℒ(𝑞 𝑗 , 𝑢 𝑗+1 ) + D KL (︀ 𝑞 𝑗 ‖ P(• |𝑦, 𝑢 𝑗+1 ) )︀ ≥ ℒ(𝑞 𝑗 , 𝑢 𝑗 ) + D KL (︀ 𝑞 𝑗 ‖ P(• |𝑦, 𝑢 𝑗+1 ) )︀ ≥ ℒ(𝑞 𝑗 , 𝑢 𝑗 ) + D KL (︀ 𝑞 𝑗 ‖ P(• |𝑦, 𝑢 𝑗 ) )︀= log J(𝑢 𝑗 ). )𝑝 MH (𝑢, 𝑣)𝑑𝑢 = 𝜋(𝑣) 4 : 4 Output: Samples {𝑢 (𝑛) } 𝑁 𝑛=1 and approximation E 𝜋 [𝑓 (𝑢 (𝑛) )] ≈ 1 𝑁 ∑︀ 𝑁 𝑛=1 𝑓 (𝑢(𝑛) ). ( 1 - 1 𝑞(𝑢, •) = 𝒩 (𝑢, 𝛽𝐼 𝑑 ) Metropolis-Adjusted Langevin 𝑞(𝑢, •) = 𝒩 (𝑢 + 𝛽 log 𝐷𝜋(𝑢), 𝛽𝐼 𝑑 ) Preconditioned Crank-Nicolson 𝑞(𝑢, •) = 𝒩 (︀ 𝛽) 1/2 𝑢, 𝛽𝐼 𝑑 )︀ Algorithm 15 . 3 Sampler 1 : 2 : 3 : 153123 GibbsInput: Target distribution 𝜋, initial distribution 𝜋 0 , test function 𝑓, number of samples 𝑁. Initial Draw: Draw initial sample 𝑢 (0) ∼ 𝜋 0 .Subsequent Samples: ♢ 15 . 6 . 1 Algorithm 15 . 5 Forward Mode Automatic Differentiation 1 : 15611551 Forward ModeForward mode automatic differentiation proceeds to accumulate the incremental function evaluations along with their derivatives. With 𝑔 𝑖 recursively defined as above we may write, for 𝑖 ∈ {1, . . . , 𝑛},𝑔 𝑖 (𝑥) = (𝑓 𝑖 ∘ • • • ∘ 𝑓 1 )(𝑥).By the chain rule we have that𝐷 𝑥 𝑓 (𝑥) = Input: Functions {𝑓 𝑖 (•)} 𝑛 𝑖=1 , corresponding Jacobians {𝐷𝑓 𝑖 (•)} 𝑛 𝑖=1, and input 𝑥. 2: Set 𝑔 1 = 𝑓 1 (𝑥) and 𝐽 1 = 𝐷𝑓 1 (𝑥). 3: For 𝑖 = 2, . . . , 𝑛: set 𝑔 𝑖 = 𝑓 𝑖 (𝑔 𝑖-1 ) and 𝐽 𝑖 = (︀ 𝐷𝑓 𝑖 (𝑔 𝑖-1 ) )︀ 𝐽 𝑖-1 . 4: Output: Function output 𝑦 = 𝑓 (𝑥) = 𝑔 𝑛 and derivative 𝐷𝑓 (𝑥) = 𝐽 𝑛 . The number of floating point operations required to multiply the Jacobians 𝐽 𝑖 in forward mode automatic differentiation to compute 𝐷𝑓 (•) is of order 𝑑 0 𝑛-1 ∑︁ 𝑖=1 𝑑 𝑖+1 𝑑 𝑖 . 3 : 3 For 𝑖 = 2, . . . , 𝑛: set and store 𝑔 𝑖 = 𝑓 𝑖 (𝑔 𝑖-1 ). 4: Set 𝐽 𝑛 = 𝐷𝑓 𝑛 (𝑔 𝑛-1 ). 5: For 𝑖 = 𝑛 -1, . . . , 1 set 𝐽 𝑖 = 𝐽 𝑖+1 𝐷𝑓 𝑖 (𝑔 𝑖-1 ). 6: Output: Function output 𝑦 = 𝑓 (𝑥) = 𝑔 𝑛 and derivative 𝐷𝑓 (𝑥) = 𝐽 1 . This leads to the number of floating point operations required to multiply the Jacobians 𝐽 𝑖 to compute 𝐷𝑓 (•) being of order 𝑑 𝑛 𝑛-1 ∑︁ 𝑖=1 𝑑 𝑖-1 𝑑 𝑖 . Table 15 . 1 151 Three representative instantiations of the Metropolis-Hastings algorithm, obtained by choosing specific proposal Markov kernels. The parameter 𝛽 in the proposal kernels allows control of the acceptance rate.Gaussian sample 𝜉 ∼ 𝒩 (0, 𝐼 𝑑 ) and setting, respectively,𝑣 * := 𝑢 + 𝛽 1/2𝜉, (RWMH proposal) 𝑣 * := 𝑢 + 𝛽 log 𝐷𝜋(𝑢) + 𝛽 1/2 𝜉, (MALA proposal) 𝑣 * := (1 -𝛽) 1/2 𝑢 + 𝛽 1/2 𝜉. (pCN proposal) In each case, the parameter 𝛽 controls how far proposed sample 𝑣 * ∼ 𝑞(𝑢, •) is, on average, from the current state 𝑢 ∈ R 𝑑 of the chain. Smaller 𝛽 leads to 𝑣 * being closer to 𝑢, and thus to Metropolis-Hastings algorithms with slow exploration but high acceptance rate. Many theoretical and heuristic guidelines are available to choose the step-size 𝛽. This choice can be made offline (i.e. before implementing the algorithm) but also online (i.e. adapting the choice as the chain runs, based on its output). 15.22.RWMH was the first MCMC algorithm to be developed, and it is still widely used due to its simplicity. MALA adds gradient information from the target distribution so that proposed samples tend to move to regions of higher target density; this algorithm is closely related to the score-based approach for generative modeling considered in Section 13.4; indeed the MALA proposal arises from Euler-Maruyama discretization of the Langevin equation. The pCN proposal modifies RWMH to obtain a proposal Markov kernel that satisfies detailed balance with respect to Gaussian density 𝜌 := 𝒩 (0, 𝐼 𝑑 ). In particular if 𝜋(𝑢) ∝ exp This idea can be generalized to settings where 𝜌 is any Guassian measure; it is useful for sampling target distributions arising as posterior distributions arising in Bayesian inverse problems with Gaussian prior 𝜌. ♢ Remark (︀ -Φ(𝑢) )︀ 𝜌(𝑢) then for the pCN proposal the acceptance probability (15.19) becomes 𝑎(𝑢, 𝑣) := min {︂ 1, exp (︀ Φ(𝑢) -Φ(𝑣) )︀ }︂ , 𝑢, 𝑣 ∈ R 𝑑 . (15.20) When there is no possibility of confusion, we will simply write 𝜋 for the posterior probability density function, rather than 𝜋 𝑦 . Use of 𝐷 and 𝐷 here denote derivatives with respect to 𝑢, then evaluated at 𝑢 = 𝑚 ⋆ + (Σ ⋆ ) 1/2 𝜉. Strictly speaking this is nonlinear when viewed on the vector space 𝐿 1 (R 𝑑 ; R), which contains probability density functions. This is the analog of equation (4.3), the objective function in the case of a fixed single instance of data 𝑦. The map is called block-triangular because, if the map is differentiable, then its Jacobian is given by a block-triangular matrix. Also known as Schur product, this computes the elementwise product of two matrices of the same dimension. Computed, for example, using a deterministic scoring rule from Definition 11.65. Again recall the notation from Remark 7.1. Again, recall the notation from Remark 7.1. Here the integrals are over R 𝑑 . For simplicity, in this chapter we do not write the domain of integration when it is clear from the context. See Bibliography Section 11.4 for citation. We give a reference for(11.25)  in the bibliography; its proof is omitted here for reasons of brevity. For ease of presentation, we assume throughout invertibility of the cumulative density function; the ideas generalize to the non-invertible case by considering the generalized inverse distribution function. Deep if 𝐿 is large enough; often three or more is considered large in this context. Lower values are sometimes referred to as shallow neural networks. Note that in this interpretation the unknown on which we seek a posterior distribution is now 𝜓 ∈ 𝐿 2 (𝐷) (whereas in Bayes Theorem 1.3 it was 𝑢 ∈ R 𝑑 ). And, although we will derive a continuous time analog of normalizing flows, the neural ODE, the use of flow here should not be confused with its use in the theory of continuous time dynamical systems. Note that this is the correct normalization by(13.23). The notation 𝐻 ϒ is used in the next chapter, equation (12.2), in the context of real-valued functions and is readily generalized to R 𝑑 -valued functions."
}