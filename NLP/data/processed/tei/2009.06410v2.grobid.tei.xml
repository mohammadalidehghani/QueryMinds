<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beneficial and Harmful Explanatory Machine Learning</title>
				<funder>
					<orgName type="full">UK&apos;s EPSRC Human-Like Computing Network</orgName>
				</funder>
				<funder>
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -405630557</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-02-25">25 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lun</forename><surname>Ai</surname></persName>
							<email>lun.ai15@imperial.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
							<email>s.muggleton@imperial.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">CÃ©line</forename><surname>Hocquette</surname></persName>
							<email>celine.hocquette16@imperial.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Gromowski</surname></persName>
							<email>mark.gromowski@uni-bamberg.de</email>
						</author>
						<author>
							<persName><forename type="first">Ute</forename><surname>Schmid</surname></persName>
							<email>ute.schmid@uni-bamberg.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="laboratory">Cognitive Systems Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Cognitive Systems Group</orgName>
								<orgName type="institution">University of Bamberg</orgName>
								<address>
									<settlement>Bamberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Bamberg</orgName>
								<address>
									<settlement>Bamberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beneficial and Harmful Explanatory Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-25">25 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">A121962C0315DD83C8CBB3E066673924</idno>
					<idno type="arXiv">arXiv:2009.06410v2[cs.AI]</idno>
					<note type="submission">Received: date / Accepted: date</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie's definition of Ultra-Strong Machine Learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work to our knowledge has examined the potential harmfulness of machine's involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a recent paper <ref type="bibr" target="#b43">[44]</ref> the authors provided an operational definition for comprehensibility of logic programs and used this, in experiments with humans, to provide the first demonstration of Michie's Ultra-Strong Machine Learning (USML). The authors demonstrated USML via empirical evidence that humans improve out-of-sample performance in concept learning from a training set E when presented with a first-order logic theory which has been machine learned from E. The improvement of human performance indicates a beneficial effect of comprehensible machine learned models on human skill acquisition. The present paper investigates the explanatory effects of machine's involvement in human skill acquisition of simple games. In particular, we have focused on a two-player game as the material for experimentation which was designed to be isomorphic to Noughts and Crosses but features a different spatial arrangement of the game. Our results indicate that when a machine learned theory is used to teach strategies to humans in a noise-free setting, in some cases the human's out-of-sample performance is reduced. This degradation of human performance is recognised to indicate the existence of harmful explanations. Textual and visual explanations 1 are shown to treated participants along with a training example for winning a two player game isomorphic to Noughts and Crosses. Textual explanations were generated from the rules learned by our Meta-Interpretive exPlainable game learner M IP lain.</p><p>In the current paper, which extends our previous work on the phenomenon of USML, both beneficial and harmful effects of a machine learned theory are explored in the context of simple games. Our definition of explanatory effects is based on human out-of-sample performance in the presence of natural language and visual explanation generated from a machine learned theory (Figure <ref type="figure" target="#fig_0">1</ref>). The analogy between understanding a logic program via declarative reading and understanding a piece of natural language text allows the explanatory effects of a machine learned theory to be investigated.</p><p>The results of relevant Cognitive Science literature allow the properties of a logic theory which are harmful to human comprehension to be characterised. Our approach is based on developing a framework describing a cognitive window which involves bounds with regard to 1) descriptive complexity of a theory and 2) execution stack requirements for knowledge application. We hypothesise that a machine learned theory provides a harmful explanation to humans when theory complexity is high and execution is cognitively challenging. Our proposed cognitive window model is confirmed by empirical evidence collected from multiple experiments involving human participants of various backgrounds.</p><p>We summarise our main contributions as follows:</p><p>-We define a measure to evaluate beneficial/harmful explanatory effects of machine learned theory on human comprehension. -We develop a framework to assess a cognitive window of a machine learned theory.</p><p>The approach encompasses theory complexity and the required execution stack. -Our quantitative and qualitative analyses of the experimental results demonstrate that a machine learned theory has a harmful effect on human comprehension when its search space is too large for human knowledge acquisition and it fails to incorporate executional shortcuts.</p><p>This paper is arranged as follows. In Section 2, we discuss existing work relevant to the paper. The theoretical framework with relevant definitions is presented in Section 3. We describe our experimental framework and the experimental hypotheses in Section 4. Section 5 describes several experiments involving human participants on two simple games. We examine the impact of a cognitive window on the explanatory effects of a machine learned theory based on human performance and verbal input. In Section 6, we conclude our work and comment on our analytical results -only a short and simple-to-execute theory can have a beneficial effect on human comprehension. We discuss potential extensions to the current framework, curriculum learning and behavioural cloning, for enhancing explanatory effects of a machine learned theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>This section summarises related research of game learning and familiarises the reader with the core motivations for our work. We first present a short overview of related investigations in explanatory machine learning of games. Subsequently, we cover various approaches for teaching and learning between humans and machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Explanatory machine learning of games</head><p>Early approaches to learning game strategies <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b51">52]</ref> used the decision tree learner ID3 to classify minimax depth-of-win for positions in chess end games. These approaches used carefully selected board attributes as features. However, chess experts had difficulty understanding the learned decision tree due to its high complexity <ref type="bibr" target="#b35">[36]</ref>.</p><p>Methods for simplifying decision trees without compromising their accuracy have been investigated <ref type="bibr" target="#b52">[53]</ref> on the basis that simpler models are more comprehensible to humans. An early Inductive Logic Programming (ILP) <ref type="bibr" target="#b44">[45]</ref> approach learned optimal chess endgame strategies at depth 0 or 1 <ref type="bibr" target="#b7">[8]</ref>. An informal complexity constraint was applied which limits the number of clauses used in any predicate definition to 7 Â± 2 clauses. This number is based on the hypothesised limit on human short term memory capacity of 7 Â± 2 chunks <ref type="bibr" target="#b38">[39]</ref>. A different approach involving the augmentation of training data with high-level annotations was explored in <ref type="bibr" target="#b25">[26]</ref>. Initialisation requires explanations to be provided for the target data set and the predicative accuracy of explanations is evaluated similarly to the predicative accuracy of labels.</p><p>The earliest reinforcement learning system M EN ACE (Matchbox Educable Noughts And Crosses Engine) <ref type="bibr" target="#b34">[35]</ref> was specifically designed to learn an optimal agent policy for Noughts and Crosses. Later, Q-Learning <ref type="bibr" target="#b70">[71]</ref> and Deep Reinforcement Learning were spawned and have led to a variety of applications including the Atari 2600 games <ref type="bibr" target="#b42">[43]</ref> and the game of Go <ref type="bibr" target="#b64">[65]</ref>. While these systems defeated the strongest human players, they lack the ability to explain the encoded knowledge to humans. Recent approaches such as <ref type="bibr" target="#b71">[72]</ref> have aimed to explain the policies learned by these models, but the learned strategy is implicitly encoded into the continuous parameters of the policy function which makes their operation opaque to humans. Relational Reinforcement Learning <ref type="bibr" target="#b21">[22]</ref> and Deep Relational Reinforcement Learning <ref type="bibr" target="#b72">[73]</ref> have attempted to address these drawbacks by incorporating the use of relational biases to enhance human understandability. Alternatively, case-based policy summary can be provided based on sets of carefully selected states of an agent as representatives of a larger state space to allow humans to gain a limited understanding in short time <ref type="bibr" target="#b3">[4]</ref>.</p><p>In <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, the author provided a survey of most relevant work in explainable AI and argued that explanatory functionalities were mostly subjective to the developer's view. However, there is a general lack of demonstration on explanatory effect which should be examined by empirical trials and no existing framework accounts for the explanatory harmfulness of machine learned models. In the context of game playing, we propose a theoretical framework with support of empirical results to characterise helpfulness and harmfulness of machine learning on human comprehension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Explanations for human problem solving and sequential decision making</head><p>Human problem solving relies on varying degrees of implicit and explicit knowledgethat is system 1 and system 2 <ref type="bibr" target="#b30">[31]</ref> -depending on the problem domain and occasionally on experience of a person <ref type="bibr" target="#b20">[21]</ref>. Implicit knowledge which is not available for inspection and verbalisation, is acquired by practice and highly automated <ref type="bibr" target="#b49">[50]</ref>. In contrast, explicit knowledge, alternatively named declarative knowledge, is inspectable and can be communicated to others <ref type="bibr" target="#b15">[16]</ref>. For cognitive puzzles such as Tower of Hanoi, it has been shown that parts of the problem solving skills are represented in an explicit way in the form of rules <ref type="bibr" target="#b59">[60]</ref>. Communication of problem solving knowledge can be realised in the form of explanations. However, it has been demonstrated in several psychological studies that learners often cannot profit from verbal information when the specific problem solving context is not available to the learners <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>. However, for intelligent tutoring, it has been suggested that explanations in the form of rules as well as of examples can support learning when given in a specific task context <ref type="bibr" target="#b55">[56]</ref>. Furthermore, it has been shown that learning by doing in combination with explicit verbalisation in the form of explanations is a highly effective learning strategy for cognitive tasks <ref type="bibr" target="#b1">[2]</ref>.</p><p>One can assume that requirements for explanations to be helpful are different for one-shot classification problems and sequential decision making problems. Explaining the classification decision of a learned model usually refers to the specific instance that is being classified. For example, explanation provided by an intelligent system for identifying the presence of a specific tumor given the image of a tissue sample may include a visual demonstration of the tumor specific tissue and textual information about the size and the position of the tumor in relation to other types of tissue <ref type="bibr" target="#b57">[58]</ref>. In contrast, explaining the decision for a specific action in sequential decision making has to take into account not only the effect of this decision on the current state but also its possible effect on future states <ref type="bibr" target="#b8">[9]</ref>. Sequential decision making is typical for puzzles such as Tower of Hanoi and for single-person as well as multi-person games. Currently, the function of explanation in games is mostly studied in the context of deep reinforcement learning for Arcade games. One approach is to visualise an agent's current state and factors which affect the agent's decision making <ref type="bibr" target="#b28">[29]</ref>. An exception is a method which summarizes an agent's strategy in a video <ref type="bibr" target="#b60">[61]</ref>. In this work, agents do not play optimally and the videos are used to allow the human to assess the capabilities of the agent. For the Ms. Pacman game, it has been demonstrated that visual highlighting can be combined with textual explanations <ref type="bibr" target="#b69">[70]</ref>. Studies were pointed out in <ref type="bibr" target="#b66">[67]</ref> to emphasise a trustworthiness issue of intelligent systems that user's decision making may over-rely on explanatory information provided by intelligent systems even when systems are inaccurate or inappropriate. However, to our knowledge, it has not yet been investigated in what way human comprehension of the agent's behavior profits from multi-modal explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Two-way learning between human and machine</head><p>As an emerging sub-field of AI, Machine Teaching <ref type="bibr" target="#b23">[24]</ref> provides an algorithmic model for quantifying the teaching effort and a framework for identifying an optimized teaching set of examples to allow maximum learning efficiency for the learner. The learner is usually a machine learning model of a human in a hypothesised setting. In education, machine teaching has been applied to devise intelligent tutoring systems to select examples for teaching <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b53">54]</ref>. On the other hand, rule-based logic theories are important mechanisms of explanation. Rule-based knowledge representations are generalised means of concept encoding and have a structure analogous to human conception. Mechanisms of logical reasoning, induction and abduction, have long been shown to be highly related to human concept attainment and information processing <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b26">27]</ref>. Additionally, humans' ability to apply recursion plays a key role in understanding of relational concepts and semantics of language <ref type="bibr" target="#b24">[25]</ref> which are important for communication.</p><p>The process of reconstructing implicit target knowledge which is easy to operate but difficult to describe via machine learning has been explored under the topic of Behavioural Cloning. The cloning of human operation sequence has been applied in various domains such as piloting <ref type="bibr" target="#b37">[38]</ref> and crane operation <ref type="bibr" target="#b68">[69]</ref>. The cloned human knowledge and experience are more dependable and less error-prone due to perceptual and executional inconsistency being averaged across the original behavioural trace. To our knowledge, no existing work has attempted to estimate human errors and target these mistakes in interactive teaching sessions for achieving a measurable "clean up" effect <ref type="bibr" target="#b36">[37]</ref> from machine explanations.</p><p>3 Theoretical framework 3.1 Meta-interpretive learning of simple games ILP [45] is a form of machine learning that uses logic programming to represent examples and the background knowledge. The learner aims to induce a hypothesis as a logic program which, together with the background knowledge, entails all of the positive examples and none of the negative examples. Meta-Interpretive Learning (MIL) [47, 48] is a sub-field of ILP which supports predicate invention, dependent learning [34], learning of recursions and higher-order programs. Given an input (B, M, E+, E-) where the background knowledge B is a first-order logic program, meta-rules M are second-order clauses, positive examples E+ and negative examples E-are ground atoms, a MIL algorithm returns a logic program hypothesis H such that M âªH âª B |= E+ and M âªH âª B |= E-. The background knowledge B contains primitives which are definitions of concepts represented in the form of predicates. The meta-rules (for examples see Figure <ref type="figure">3</ref>) contain existentially quantified second-order variables and universally quantified first-order variables. They clarify the declarative bias employed for substitutions of second-order Skolem constants. The resulting first-order theories are thus strictly logical generalisation of the meta-rules. The MIL game learning framework MIGO <ref type="bibr" target="#b45">[46]</ref> is a purely symbolic system based on the adapted Prolog meta-interpreter Metagol <ref type="bibr" target="#b18">[19]</ref>. MIGO learns exclusively from positive examples by playing against the optimal opponent. For Noughts and Crosses and Hexapawn, MIGO learns a rule-like symbolic game strategy (Table <ref type="table" target="#tab_0">1</ref>) that supports human understanding and was demonstrated to converge using less training data compared to Deep and classical Q-Learning. MIGO is provided with a set of three relational primitives, move/2, won/1, drawn/1 which are a move generator, a won and a drawn classifier respectively. These primitives represent the minimal information which a human would know before playing Noughts and Crosses and Hexapawn. For successive values of k, MIGO learns a series of inter-related definitions for predicates win_k/2 for playing as either X or O. These predicates define maintenance of minimax win in k-ply.</p><p>Table <ref type="table">2</ref>: The logic program learned by M IP lain represents a strategy for the first player to win at different depths of the game. The predicate win_3_4/1 can be reduced to win_3_4(A) : -win_2(A, B) by removing literals after unfolding. The program learned by M IP lain can be described as: a board A is won at depth 1 if there exists a move from A to B such that B is won; a board A is won at depth 2 if there exists a move from A to B such that X has exactly two pairs and O has no pairs in B; a board A is won at depth 3 if there exists a move from A to B such that X has exactly one pair in B and there exists a move from B to C such that X does not have any pair in C and C is won at depth 2 for X. win_3_2(A):-move(A,B),win_3_3(B). win_3_3(A):-number_of_pairs(A,x,0),win_3_4(A).</p><p>win_3_4(A):-win_2(A,B),win_2_1(B).</p><formula xml:id="formula_0">X O O X O O X X O X O O O X X O O X O p1/2 skip2words/2 skip1wordcopy1wordend/2 skip1word/2 copyalphanum/2 skipalphanum/2 skiprest/2 skip1/2 copy1wordend/2</formula><p>Fig. <ref type="figure">2</ref>: O has two pairs represented in green and X has no pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-rule</head><formula xml:id="formula_1">P (A, B) â Q(A, B), R(B). P (A) â Q(A, B), R(B). P (A) â Q(A, S, T ), R(A). P (A) â Q(A, S, T ), R(A, U, V ).</formula><p>Fig. <ref type="figure">3</ref>: Letters P, Q, R, S, T, U, V denote existentially quantified second-order variables and A, B, C are universally quantified first-order variables.</p><p>We introduce M IP lain<ref type="foot" target="#foot_0">foot_0</ref> , a variant of M IGO which focuses on learning the task of winning for the game of Noughts and Crosses. In addition to learning from positive examples, M IP lain identifies moves which are negative examples for the task of winning. When a game is drawn or lost for the learner, the corresponding path in the game tree is saved for later backtracking following the most updated strategy. M IP lain performs a selection of hypotheses based on the efficiency of hypothesised programs using M etaopt <ref type="bibr" target="#b19">[20]</ref>.</p><p>An additional primitive number_of _pairs/3 is provided to M IP lain which depicts the number of pairs for a player (X or O) on a given board. A pair is the alignment of two marks of one player, the third square of this line being empty. An example of pairs is shown in Figure <ref type="figure">2</ref>. This additional primitive serves as an executional shortcut that reduces the depth of the search when executing the learned strategy. Furthermore, M IP lain is given the meta-rules described in Figure <ref type="figure">3</ref>, which are two variants of the postcon meta-rule with monadic or dyadic head, and two variants of the conjunction meta-rule with more than two arguments in either the first or both body literals where existentially quantified argument variables are bound to constants. These meta-rules allow projections of higher dimension predicate definitions onto a monadic setting, therefore enabling the learning of programs with higher-arity predicates. The learned strategy presented in Table <ref type="table">2</ref> describes patterns in game positions in a rule-like manner that the player's optimal move has to satisfy. Due to the instantiation of argument in primitive number_of _pairs/3, M IP lain learns a program for playing as X assuming X starts the game. For successive values of k, win_k/2 are inter-related predicates which specify status of the game in terms of the number of pairs owned by player X or O and that reflect advantage of player X over player O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Explanatory effectiveness of a machine learned theory</head><p>We extend the machine-aided human comprehension of examples in <ref type="bibr" target="#b43">[44]</ref>  and <ref type="figure">C(D</ref>, <ref type="figure">H</ref>, <ref type="figure">E</ref>) denotes the unaided human comprehension of examples where D is a logic program representing the definition of a target predicate, H is a group of humans and E is a set of examples. Based on the analogy between declarative understanding of a logic program and understanding of a natural language explanation, we describe measures for estimating the degree to which the output of a symbolic machine learning algorithm<ref type="foot" target="#foot_1">foot_1</ref> can be simulated by humans and aid comprehension.  </p><formula xml:id="formula_2">E ex (D, H, M (E)) = C ex (D, H, M (E)) -C(D,</formula><formula xml:id="formula_3">-M (E) learned from examples E is beneficial to H if E ex (D, H, M (E)) &gt; 0 -M (E) learned from examples E is harmful to H if E ex (D, H, M (E)) &lt; 0 -Otherwise, M (E)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>learned from examples E does not have observable effect on H</head><p>Within the scope of this work, we relate the explanatory effectiveness of a theory to performance which means that a harmful explanation provided by the machine degrades comprehension of the task and therefore reduces performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cognitive window of a machine learned theory</head><p>In this section, we suggest a window of a machine learned theory that constraints its explanatory effectiveness. A basic assumption of cognitive psychology and artificial intelligence is that human information processing can be modelled in analogy to symbol manipulation of computers -respectively its formal characterisation of a Turing Machine <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref>. More specifically, computational models of cognition share the view that intelligent action is based on manipulation of representations in working memory. In consequence, human inferential reasoning is limited by working memory capacity which corresponds to limitations of tape length and instruction complexity in Turing Machines.</p><p>Besides general restrictions of human information processing, performance can be influenced by internal or environmental disruptions such that the given competencies of a human in a specific domain are not always reflected in observable actions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b63">64]</ref>. However, it can be assumed that humans -at least in domains of higher cognition -are able to explain their actions by verbalising the rules which they applied to produce a given result <ref type="bibr" target="#b58">[59]</ref>. Although rules in general can be classified as procedural knowledge, the ability to verbalise rules makes them part of declarative memory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b58">59]</ref>. For complex domains, the rules which govern action generation will typically be computationally complex as measured by the Kolmogorov complexity <ref type="bibr" target="#b31">[32]</ref>. One can assume that increase in complexity can have a negative effect on performance.</p><p>In language processing and in general problem solving, hierarchisation of complex action sequences can make information processing more efficient. Typically, a general goal is broken down into sub-goals as it has been proposed in production system models <ref type="bibr" target="#b48">[49]</ref> as well as in the context of analogical problem solving <ref type="bibr" target="#b13">[14]</ref>. Rules which guide problem solving behaviour, for instance in puzzles such as Tower of Hanoi or games such as Noughts and Crosses, might be learned. From a declarative perspective, such learned rules correspond to explicit representations of a concept such as the win-in-two-steps move introduced above.</p><p>Studies of rule-based concept acquisition suggest that human concept learning can be characterised as search in a pool of possible hypotheses which are explored in some order of preference <ref type="bibr" target="#b12">[13]</ref>. This observation relates to the concept of version space learning introduced in machine learning <ref type="bibr" target="#b41">[42]</ref>. Therefore, for the purpose of experimentation in a noise-free setting, we assume that a) human learners are version space learners with limited hypothesis space search capability and that they use meta-rules to learn sub-goal structure and primitives as background knowledge. We also assume that b) rules can be represented explicitly in a declarative, verbalisable form. Finally, we postulate the existence of a cognitive window such that a machine learned theory can be an effective explanation if it satisfies two constraints: 1) a hypothesised human learning procedure which has a limited search space and 2) a knowledge application model based on the Kolmogorov complexity <ref type="bibr" target="#b31">[32]</ref>. For the following definitions, we restrict ourselves to learning datalog programs which may take predicates as arguments for representing different data structures but do not include function symbols.</p><p>Conjecture 1 (Cognitive bound on the hypothesis space size, B(P, H)): Consider a symbolic machine learned datalog program P using p predicate symbols and m meta-rules each having at most j body literals. Given a group of humans H, B(P, H) is a population-dependent bound on the size of hypothesis space such that at most n clauses in P can be comprehended by all humans in H and B(P, H) = m n p (1+j)n based on the MIL complexity analysis from <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>When learned knowledge is cognitively challenging, execution overflows human working memory and instruction stack. We then expect decision making to be more error prone and the task performance of human learners to be less dependable. To account for the cognitive complexity of applying a machine learned theory, we define the cognitive resource of a logic term and atom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4 (Cognitive cost of a logic term and atom, C(T )):</head><p>Given T a logic term or atom, the cost of C(T ) can be computed as follows:  <ref type="figure">(e</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">e</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">o</ref>)) = 10.</p><formula xml:id="formula_4">-C( ) = C(â¥) = 1 -A variable V has cost C(V ) = 1 -A constant c has cost C(c)</formula><p>Note that we compute cognitive costs of programs without redundancy since repeated literals in programs learned by M IGO and M IP lain were removed after unfolding for generating explanations which are presented to human populations. Also, a game position can be represented by different data types. We ignore the cost due to implementation and only count digits and marks.</p><p>Example 2 An atom <ref type="figure">win_2(b(e</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">e</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">o</ref>), X) with variable X has a cognitive cost C( <ref type="figure">win_2(b(e</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">e</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">o)</ref>, <ref type="figure">X</ref>)) = 12.</p><p>Example 3 A primitive move(S1, S2) which is an atom with variables S1 and S2 has a cognitive cost C(move(S1, S2)) = 3.</p><p>We model the inferential process of evaluating training and testing examples as querying a database of datalog programs. The evaluation of a query represents a mental application of a piece of knowledge given a training or testing example. The cost of evaluating a query is estimated based on run-time execution stack of a datalog program. In this work, we neglect the cost of computing the sub-goals of a primitive and compute its cost as if it were a normal predicate for simplicity.</p><p>Definition 5 (Execution stack of a datalog program, S(P, q)): Given a query q, the execution stack S(P, q) of a datalog program P is a finite set of atoms or terms evaluated during the execution of P to compute an answer for q. An evaluation in which an answer to the query is found ends with value , and an evaluation in which no answer to the query is found ends with â¥. <ref type="figure">Cog(P</ref>, <ref type="figure">q</ref>)): Given a query q, and let St represent S(P, q), the cognitive cost of a datalog program P is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 6 (Cognitive cost of a datalog program,</head><formula xml:id="formula_5">Cog(P, q) = tâSt C(t)</formula><p>Example 4 The primitive move/2 outputs a valid Noughts and Crosses state from a given input game state; the query is move( <ref type="figure">b(x</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">x</ref>, <ref type="figure">e</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">o)</ref>, <ref type="figure">S</ref>). <ref type="figure">/2</ref>, <ref type="figure">move(b(x</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">x</ref>, <ref type="figure">e</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">o)</ref>, <ref type="figure">S</ref>)) C(T ) move <ref type="figure">(b(x</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">x</ref>, <ref type="figure">e</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">o)</ref>, <ref type="figure">S</ref>) <ref type="figure" target="#fig_4">12  move(b(x</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">x</ref>, <ref type="figure">e</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">o)</ref>, <ref type="figure">b(x</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>, <ref type="figure">e</ref>, <ref type="figure">x</ref>, <ref type="figure">e</ref>, <ref type="figure">o</ref>, <ref type="figure">x</ref>, <ref type="figure">o</ref>)) <ref type="figure">move(b(x</ref>,<ref type="figure">x</ref>,<ref type="figure">o</ref>,<ref type="figure">e</ref>,<ref type="figure">x</ref>,<ref type="figure">e</ref>,<ref type="figure">o</ref>,<ref type="figure">e</ref>,<ref type="figure">o)</ref>, <ref type="figure">S</ref>)) <ref type="bibr" target="#b33">34</ref> The maintenance cost of task goals in working memory affects performance of problem solving <ref type="bibr" target="#b14">[15]</ref>. Background knowledge provides key mappings from solutions obtained in other domains or past experience <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b50">51]</ref> and grants shortcuts for the construction of the current solution process. We expect that when knowledge that provides executional shortcuts is comprehended, the efficiency of human problem solving could be improved due to a lower demand for cognitive resource. Contrarily, in the absence of informative knowledge, performance would be limited by human operational error and would not be better than solving the problem directly. To account for the latter case, we define the cognitive cost of a problem solution that requires the minimum amount of information about the task. Given a machine learning algorithm M using primitives Ï and examples E, a minimum primitive solution program MÏ (E) is learned by using the smallest subset of Ï such that MÏ (E) is consistent with E. A minimum primitive solution program is defined to not use more auxiliary knowledge than necessary but does not necessarily have the minimum cognitive cost over all programs learned with examples E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(move</head><formula xml:id="formula_6">21 1 Cog(move/2,</formula><p>Remark 1 Given that the training examples of Noughts and Crosses are winnable and M IP lain uses the set of primitives Ï = {move/2, won/1, number_of _pairs/3}, a minimum primitive solution program is produced by M IGO. This is because M IGO uses primitives {move/2, won/1} which is a strict subset of Ï for making a move and deciding a win when the input is winnable. Primitives move/2 and won/1 are also the necessary and sufficient primitives to win Noughts and Crosses and no theory can be learned using a subset of Ï with the cardinality of one. <ref type="figure">CogP (E</ref>, <ref type="figure">M</ref> , <ref type="figure">Ï</ref>, <ref type="figure">q</ref>)): Given examples E, primitive set Ï, a query q and a symbolic machine learning algorithm M that learns a minimum primitive solution, the cognitive cost of a problem solution is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 8 (Cognitive cost of a problem solution,</head><formula xml:id="formula_7">CogP (E, M , Ï, q) = Cog( MÏ (E), q)</formula><p>where MÏ (E) is a minimum primitive solution program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2</head><p>The program P learned by M IP lain has less cognitive cost than the one learned by M IGO except for queries concerning win_1/2. Given sufficient examples E, M IGO's learning algorithm as M , primitive set used by M IP lain Ï = {move/2, won/1, number_of _pairs/3}, based on Definition 5 to 8, we have Cog(P, x 1 ) = CogP (E, M , Ï, x 1 ), Cog(P, x 2 ) &lt; CogP (E, M , Ï, x 2 ) and Cog(P, x 3 ) &lt; CogP (E, M , Ï, x 3 ) where x i = win i (s i , V ) in which s i represents a position winnable in i moves and V is a variable.</p><p>We give a definition of human cognitive window based on theory complexity during knowledge acquisition and theory execution cost during knowledge application. A machine learned theory has 1) a harmful explanatory effect when its hypothesis space size exceeds the cognitive bound and 2) no beneficial explanatory effect if its cognitive cost is not sufficiently lower than the cognitive cost of the problem solution.</p><p>Conjecture 2 (Cognitive window of a machine learned theory): Given a logic program D representing the definition of a target predicate, a symbolic machine learning algorithm M , a symbolic minimum primitive solution learning algorithm M and examples E, M (E) is a machine learned theory using the primitive set Ï and belongs to a program class with hypothesis space S. For a group of humans H, E ex satisfies both</p><formula xml:id="formula_8">1. E ex (D, H, M (E)) &lt; 0 if |S| &gt; B(M (E), H) 2. E ex (D, H, M (E)) â¤ 0 if Cog(M (E), x) â¥ CogP (E, M , Ï, x) for queries x that h â H have to perform after study</formula><p>We use the defined variant of Kolmogorov complexity as a measure to approximate cognitive cost of applying sequential actions which does not take empirical data as input. In the following sections 4 and 5, we concentrate on collecting empirical evidence to support the existence of a cognitive window with bounds ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>) on the explanatory effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental framework</head><p>In this section, we describe an experimental framework for assessing the impact of cognitive window on the explanatory effects of a machine learned theory. Our experimental framework involves 1) a set of criteria for evaluating the participants' learning quality from their own textual descriptions of learned strategies and 2) an outline of experimental hypotheses. For game playing, we assume humans are able to explain actions by verbalising procedural rules of strategy. We expect textual answers to provide insights about human decision making and knowledge acquisition. The quality of textual answers can be affected by multiple factors such as motivation, familiarity with the introduced concepts and understanding of the game rules. We take into account these factors in the evaluation criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 9 (Primitive coverage of a textual answer):</head><p>A textual answer correctly describes a primitive if the semantic meaning of the primitive is unambiguously stated in the response. The primitive coverage is the number of primitives in a symbolic machine learned theory that are described correctly in a textual answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 10 (Quality of a textual answer, Q(r)):</head><p>A textual answer r is checked against the specifications from Table <ref type="table" target="#tab_2">3</ref> in an increasing order from criteria level 1 to level 4. Q(r) is the highest level i that r can satisfy. When a response does not satisfy any of the higher levels, the quality of this response is the lowest level 0.</p><p>To illustrate, we consider the predicate win_2/2 learned by M IP lain (Table <ref type="table">2</ref>). Primitive predicates are move/2 and number_of _pairs/3. We present in Table <ref type="table" target="#tab_2">3</ref> a number of examples of textual answers. A high quality response reflects a high motivation and good understanding of game concepts and strategy. On the other hand, a poor quality response demonstrates a lack of motivation or poor understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 11 (High (HQ) / low (LQ) quality textual answer):</head><p>A HQ response rh has Q(rh) â¥ 3 and a LQ response rl has Q(rl) &lt; 3.</p><p>We define the following null hypotheses to be tested in Section 5 and describe the motivations. Let M denote a symbolic machine learning algorithm. E stands for ) the textual answer quality of learned knowledge reflects comprehension, 2) there exist cognitive bounds for humans to provide textual answers of higher quality and 3) the machine learned theory helps improve the quality of textual answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H1: Unaided human comprehension C(D, H, E) and machine-explained human comprehension C ex (D, H, M (E))</head><p>manifest in textual answer quality Q(r). We examine if high post-training accuracy correlates with high response quality and high primitive coverage of each question category. H2: Difficulty for human participants to provide textual answer increases with quality Q(r). We examine if the proportion of textual answers reduces with respect to high response quality and high primitive coverage of each question category. H3: Machine learned theory M (E) improves textual answer quality Q(r). We examine if machine-aided learning results in more HQ responses.</p><p>The impact of a cognitive window on explanatory effects is tested via the following hypotheses. Ï is a set of primitives introduced to H. Let x denote the set of questions that human h â H answers after learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H4: Learning a complex theory (|S| &gt; B(M (E), H)) exceeding the cognitive bound</head><p>leads to a harmful explanatory effect (E ex (D, H, M (E)) &lt; 0). We examine if the post-training accuracy, after studying a machine learned theory that participants cannot recall fully, is worse than the accuracy following self-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H5: Applying a theory without a low cognitive cost (Cog(M (E), x) â¥ CogP (E, M , Ï, x))</head><p>does not lead to a beneficial explanatory effect (E ex (D, H, M (E)) â¤ 0). We examine if the post-training accuracy, after studying a machine learned theory that is cognitively costly, is equal to or worse than the accuracy following self-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section introduces the materials and experimental procedure which we designed to examine the explanatory effects of a machine learned theory on human learners. Afterwards, we describe the experiment interface and present experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Materials</head><p>We assume that Noughts and Crosses is a widely known game a lot of participants of the experiments are familiar with. This might result in many participants already playing optimally before receiving explanations, leaving no room for potential performance increase. In order to address this issue, the Island Game was designed as a problem isomorphic to Noughts and Crosses. <ref type="bibr" target="#b65">[66]</ref> define isomorphic problems as "problems whose solutions and moves can be placed in one-to-one relation with the solutions and moves of the given problem". This changes the superficial presentation of a problem without modifying the underlying structure. Several findings imply that this does not impede solving the problem via analogical inference if the original problem is consciously recognized as an analogy; on the other hand, the prior step of initially identifying a helpful analogy via analogical access is highly influenced by superficial similarity <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">55]</ref>. Given that the Island Game presents a major re-design of the game surface, we expect that participants will less likely recall prior experience of Noughts and Crosses that would facilitate problem solving, leading to less optimal play initially and more potential for performance increase. The Island Game (Figure <ref type="figure" target="#fig_6">4</ref>) contains three islands, each with three territories on which one or more resources are marked. The winning condition is met when a player controls either all territories on one island or three instances of the same resource. The nine territories resemble the nine fields in Noughts and Crosses and the structure of the original game is maintained in regard to players' turns, possible moves, board states and win conditions. This isomorphism masks a number of spatial relations that represent the membership of a field to a win condition. In this way, the fields can be rearranged in an arbitrary order without changing the structure of the game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Methods and design</head><p>We use two experiment interfaces, one for Noughts and Crosses and another one for the Island Game. A human player always plays as player one (X for Noughts and Crosses and Blue for the Island Game) and starts the game. For both, we adopt a two-group pre-training post-training design (Table <ref type="table">4</ref>). We first introduce to participants rules of the game and the concept of pairs. In the pre-training stage, performance of participants in both self learning and machine-aided learning groups are measured in an identical way. During training, they are able to see correct answers of some game positions. In the post-training, performance of both self-learning and machine-aided groups are evaluated in the exact same way as in the pre-training. This experiment setting allows to evaluate the degree of change in performance as the result of explanations. Each question in pre-and post-training is the presentation of a board for which it is the participant's turn to play. They are asked to select what they consider to be the optimal move. A question category of win i denotes a game position winnable in i moves of the human player. An exemplary question is shown in the Figure <ref type="figure" target="#fig_6">4</ref>. The post-training questions are rotated and flipped from pre-training Table <ref type="table">4</ref>: Summary of experiment parts. Participants played one mock game against a random computer player for the more difficult Island Game. After selecting a move in training and regardless of its correctness, participants received the labels of the two moves presented; treated participants additionally received explanations generated from M IP lain's learned program. We introduced the primitive set used by M IP lain. The treatment was applied to the machine-aided group. Various studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b55">56]</ref> suggested explanations are most effective for human learning when presented with examples and in a specific task context. Therefore, we have employed textual explanations to verbalise machine learned knowledge for a sequence of game states and these textual explanations are grounded to instantiate game states in the context to provide visualisation of game boards. During treatment, we present both visual and textual explanations in order for participants who are not familiar with the designed game domain to profit the most from explanations. Learned first-order theories have been translated with manual adjustments based on primitives provided to all participants and to M IP lain. An exemplary explanation is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Both visual and textual explanations preserve the structure of hypotheses without redundancy and account for the reasons that make a move correct (highlighted in green). Contrastive explanations are presented for the possible sequence of wrong moves in participant's turns (highlighted in red) by comparing against M IP lain's learned theory. Conversely, during training, the self-learning group did not receive the treatment and was presented with similar game positions without the corresponding visual and textual explanations. For the Island Game experiments, we recorded an English description of the strategy they used for each of the selected post-training questions. Participants are presented previously submitted answers, one at a time along with a text input box for written answers. Moves for these open questions are selected from post-training with a preference order from wrong and hesitant moves to consistently correct moves. We associate hesitant answers with higher response times. A total of six questions are selected based on individual performance during the post-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment results</head><p>We conducted three trial experiments<ref type="foot" target="#foot_2">foot_2</ref> using the interface with Noughts and Crosses questions and explanations. These experiments were carried out on three samples: an undergraduate student group from Imperial College London, a junior student group from a German middle school and a mixed background group from Amazon Mechanical Turk<ref type="foot" target="#foot_3">foot_3</ref> (AMT). No consistent explanatory effects could be observed for any of the mentioned samples. The problem solving strategy that humans apply can be affected by factors such as task familiarity, problem difficulty, and motivation. For instance, <ref type="bibr" target="#b56">[57]</ref> suggested that a rather superficial analogical transfer of a strategy is applied when a problem is too difficult or when there is no reason to gain a more general understanding of a problem. Given that the majority of subjects achieved reasonable initial performance, we ascribe the reason of such results to experience with the game and complexity of explanations. The game familiarity of adult groups led to less potential for performance improvement. Early middle school students had limited attention and were overwhelmed by information intake. Alternatively, we focused on specially designed experiment materials in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Island Game with open questions</head><p>A sample from Amazon Mechanical Turk and a student sample from the University of Bamberg participated in experiments 4 that used the interface with Island Game questions and explanations. To test hypotheses H1 to H5, we employed a quantitative analysis on test performance and a qualitative analysis on textual answers. A subsample with a mediocre performance on pre-training questions of all categories within one standard deviation of the mean was selected for the performance analysis. This aims to discount the ceiling effect (initial performance too high) and outliers (e.g. struggling to use the interface). We employed 5% significance levels for testing experimental results.</p><p>From AMT sample, we had 90 participants who were 18 to above 65 years old. A sub-sample of 58 participants with a mediocre initial performance was randomly partitioned into two groups, MS (Mixed background Self learning n = 29) and MM (Mixed background Machine-aided learning, n = 29). A different sub-sample of 30 participants completed open questions and was randomly split into two groups,  MSR (Mixed background Self learning and strategy Recall, n = 15) and MMR (Mixed background Machine-aided learning and strategy Recall, n = 15). As shown in Figure 5a, in category win_2, MM post-training had a better comprehension (p = 0.028) than MS post-training while MM and MS had similar pre-training performance in this category. Results in category win_2 indicate that explanations have a beneficial effect on MM. However, MM did not have a better comprehension on win_1 than MS given the same initial performance. In addition, MM had the same initial performance as MS in category win_3 but MM's performance reduced after receiving explanations of win_3 (p = 0.005). From a group of students involved in a Cognitive Systems course at the University of Bamberg, we had 13 participants who were 18 to 24 years old and a few outliers between 25 and 54 years. All participants were asked to complete open questions and were randomly split into two groups, SSR (Student Self learning and strategy Recall, n = 4) and SMR (Student Machine-aided learning and strategy Recall, n = 9). A sub-sample of 9 with a mediocre initial performance was randomly divided into SS (Student Self learning, n = 2) and SM (Student Machine-aided learning, n = 7). The imbalance in the student sample was caused by a number of participants leaving during the experiment. The machine-aided learning results show large performance variances in post-training as evidence for insignificant levels of performance degradation.</p><p>In Table <ref type="table" target="#tab_3">5</ref>, we identified that participants who were able to provide high quality responses for their test answers scored higher on these questions. This is not the case for win_3, however, due to the high difficulty of providing good description of strategy for win_3 category. Additionally, in the win_2 category, both machineaided groups (MMR: 2/(2+35), SMR: 9/(9+14)) have greater proportions of high quality responses than self learning groups (MSR: 1/(1+32), SSR: 1/(1+8)). Also, we observed a pattern in which there are less HQ responses than LQ responses in win_1 and win_2 categories. This pattern is more significant in win_2 category.</p><p>Figure <ref type="figure">6</ref> illustrates the difficulty of providing good quality textual answer for the non-trivial category win_3. Since win_1 contains only two predicates, we examined (b) The proportion of quality textual answers decreases with respect to the number of primitives covered.</p><p>Fig. <ref type="figure">6</ref>: win_3 reuses win_2 and uses four number_of _pairs/3 when unfolded. In Figure <ref type="figure">6b</ref>, both mixed background groups (MSR and MMR) had lower proportions of responses covering one predicate than student groups (SSR and SMR). Mixed background and student groups could not provide a significant proportion of response covering more than one and two primitives respectively (Figure <ref type="figure">6a</ref>).</p><p>primitive coverage of non-trivial categories win_2 and win_3. However, for clarity of presentation, we only show category win_3 which has more remarkable trends. When counting primitives based on Definition 9, we only consider the constraint number_of _pairs/3 and ignore the move generator move/2 as participants were required to make a move when they answered a question.</p><p>In Figure <ref type="figure">6a</ref>, we plotted primitive coverage against the accuracy of post-training answers that were selected as open questions. We observed a major monotonically increasing trend in accuracy with respect to primitive coverage. This indicates that high matching between textual answers and the machine learned theory correlates with high performance. In Figure <ref type="figure">6b</ref>, we observed downward curves for MSR and MMR in the number of textual answers from the lower to the higher primitive coverage. More responses were provided by SSR and SMR covering one primitive than MSR and MMR. Participants gave very few responses that cover more than</p><p>Table 6: Hypotheses concerning quality of textual answers and comprehension. C stands for confirmed, N denotes not confirmed, H stands for hypothesis. Test outcomes are presented for win_1, win_2 and win_3 categories.  <ref type="table">2</ref>, the results suggest an increasing difficulty to provide complete strategy descriptions beyond two (mixed background groups) and four (student groups) clauses of win_3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>Results concerning null hypotheses H1 to H5 are summarised in Table <ref type="table">6</ref> and <ref type="table" target="#tab_5">7</ref>. We assume that (H1 Null) comprehension does not correlate with textual answer quality.</p><p>To examine this hypothesis, we analyse the results in two steps. First, results of HQ responses in two categories (Table <ref type="table" target="#tab_3">5</ref>) suggest that being able to provide better textual answers of strategy corresponds to a high comprehension. Second, we examined the coverage of primitives (specifically for LQ responses of win_3) in textual answers (Figure <ref type="figure">6a</ref>). Evidence in all categories shows a correlation between comprehension and the degree of textual answer matching with explanations. We reject the null hypothesis in all categories which implies the confirmation of H1.</p><p>In addition, we assume that (H2 Null) the difficulty for human participants to provide textual answer is not affected by textual answer quality. Since high response quality is difficult to achieve (Table <ref type="table" target="#tab_3">5</ref>) and it is challenging to correctly describe all primitives (Figure <ref type="figure">6b</ref>), we reject this null hypothesis for all categories and confirm H2 as it is increasingly difficult for participants to provide higher quality textual answer. Hence, two additional trends we observed from the same figure suggest two mental barriers of learning. As we assume a human sample is a collection of version space learners, the search space of participants is limited to programs of size two (mixed background groups) and four (student groups). When H is taken as the student sample and P to be the machine learned theory on winning the Island Game, the cognitive bound B(P, H) = m 4 * p 4(j+1) = 4 4 * 2 12 corresponds to the hypothesis space size for programs with four clauses (four metarules are used with at most two body literals in each clause, primitives are move/2 and number_of _pairs/3).</p><p>Furthermore, we assume that (H3 Null) machine learned theory does not improve textual answer quality. Results (Table <ref type="table" target="#tab_3">5</ref>) show higher proportion of HQ responses for machine-aided learning than self-learning in category win_2. Thus, for win_2, we reject this null hypothesis which means H3 is confirmed in category win_2 where the machine explanations result in more high quality textual answers being provided.</p><p>We assume that (H4 Null) learning a descriptively complex theory does not affect comprehension harmfully. When P is the program learned by M IP lain, B(P, H) for two samples correspond to program class with size no larger than 4. Only win_3 We reject this null hypothesis since no significant beneficial effect has been observed in category win_1. Therefore, we confirm H5 -knowledge application requiring much cognitive resource does not result in better comprehension. The performance analysis (Figure <ref type="figure" target="#fig_8">5a</ref>) demonstrates a comprehension difference between self learning and machine-aided learning in category win_2. An explanatory effect has not been observed for the student sample. While the conflicting results suggest that a larger sample size would likely ensure consistency of statistical evidence, the patterns in results suggest more significant results in category win_2 than win_1 and win_3. The predicate win_2 in the program learned by M IP lain satisfies both constraints on hypothesis space bound for knowledge acquisition and cognitive cost for knowledge application. In addition, the cognitive window explains the lack of beneficial effects of predicates win_1 and win_3. The former does not have a lower cognitive cost for execution so that operational errors cannot be reduced, thus there has been no observable effects. The latter is a complex rule with a larger hypothesis space for human participants to search from and harmful effects have been observed due to partial knowledge being learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and further work</head><p>While the focus of explainable AI approaches has been on explanations of classifications <ref type="bibr" target="#b0">[1]</ref>, we have investigated explanations in the context of game strategy learning. In addition, we have explored both beneficial and harmful sides of the AI's explanatory effect on human comprehension. Our theoretical framework involves a cognitive window to account for the properties of a machine learned theory that lead to improvement or degradation of human performance. The presented empirical studies have shown that explanations are not helpful in general but only if they are of appropriate complexity -being neither informatively overwhelming nor more cognitively expensive than the solution to a problem itself. It would appear that complex machine learning models and models which cannot provide abstract descriptions of internal decisions are difficult to be explained effectively. However, it remains an open question how one can examine non-explainability. This is an important question since a positive outcome implies the limit of scientific explanations. In this  work, a conservative approach has been taken and we have obtained preliminary results from a rather narrow domain. We have acknowledged that participant groups vary greatly in size which might be extended with studies on a broader range of problems with larger samples. Similar metrics that relate to explanatory effects but expand beyond symbolic machine learning have great potentials for future work. The noise-free framework for cognitive window in this work might also be extended with hypotheses that take inconsistency of data into consideration.</p><p>To explain a strategy, typically goals or sub-goals must be related to actions which can fulfill these goals. If the strategy involves to keep in mind a stack of open sub-goals -as for example the Tower of Hanoi <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b58">59]</ref> -explanations might become more complex than figuring out the action sequence. Based on <ref type="bibr" target="#b12">[13]</ref>, knowledge is learned by humans in an incremental way, which was recently emphasized by <ref type="bibr" target="#b74">[75]</ref> on human category learning. Given problems whose solutions can be effectively divided into sufficiently small parts, a potential approach to improve explanatory effectiveness of a machine learned theory is to process complex concepts into smaller chunks by initially providing simple-to-execute and short sub-goal explanations. Mapping input to another sub-goal output thus consumes lower cognitive resources and improvement in performance is more likely. It is worth investigating for future work a teaching procedure involving a sequence of teaching sessions that issues increasingly difficult tasks and explanations. Yet, Abstract descriptions might be generated in the form of invented predicates as it has been shown in previous work on ILP as an approach to USML <ref type="bibr" target="#b43">[44]</ref>. An example for such an abstract description for the investigated game is the predicate number_of _pairs/3. Therefore, learning might be organised incrementally, guided by a curriculum <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b67">68]</ref>.</p><p>In addition, the current teaching procedure, which only specifies humans as learners, could be augmented to enable two-way learning between human and machine. Human decisions might be machine learned and explanations would be provided based on estimation of human errors during the course of training. A simple demonstration of this idea is presented in Figure <ref type="figure" target="#fig_5">7</ref>. We would like to explore, in the future, an interactive procedure in which a machine iteratively re-teaches human learners by targeting human learning errors via specially tailored explanations. <ref type="bibr" target="#b11">[12]</ref> suggested it is crucial for machine produced clones to be able to represent goal-oriented knowledge which is in a form that is similar to human conceptual structure. Hence, MIL is an appropriate candidate for cloning since it is able to iteratively learn complex concepts by inventing sub-goal predicates. We hope to incorporate cloning to predict and target mistakes in human learned knowledge from answers in a sequence of re-training. We expect a "clean up" on operation errors of human behaviours from empirical experiments by presenting appropriate explanations in re-training. Such corrections and improvements guided by identified errors in a human strategy are also helpful in the context of intelligent tutoring <ref type="bibr" target="#b73">[74]</ref> where classic strategies such as algorithmic debugging <ref type="bibr" target="#b62">[63]</ref> can be applied to make humans and machines learn from each other.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Interface featuring an example of the Island Game that is isomorphic to Noughts and Crosses. Players occupy cells in turns which have resources marked as symbols and a player wins if he or she controls three cells on the same island or three pieces of the same resource. Human participants, who play Blue, are confronted with a game position and have to choose between two alternative moves that are highlighted in yellow. When Blue owns two cells on the same island or two pieces of the same resource, related cells or resources are highlighted in bold. More details of the material design are given in Section 5.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>,B):-move(A,B),won(B). 2 win_2(A,B):-move(A,B),win_2_1(B). win_2_1(A):-number_of_pairs(A,x,2), number_of_pairs(A,o,0). 3 win_3(A,B):-move(A,B),win_3_1(B). win_3_1(A):-number_of_pairs(A,x,1),win_3_2(A).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 1 (</head><label>1</label><figDesc>Machine-explained human comprehension of examples, C ex (D, H, M (E))): Given a logic program D representing the definition of a target predicate, a group of humans H, a theory M (E) learned using machine learning algorithm M and examples E, the machine-explained human comprehension of examples E is the mean accuracy with which a human h â H after brief study of an explanation based on M (E) can classify new material selected from the domain of D. Definition 2 (Explanatory effect of a machine learned theory, E ex (D, H, M (E))): Given a logic program D representing the definition of a target predicate, a group of humans H, a symbolic machine learning algorithm M , the explanatory effect of the theory M (E) learned from examples E is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>H, E)Definition 3 (Beneficial/harmful effect of a machine learned theory): Given a logic program D representing the definition of a target predicate, a group of humans H, a symbolic machine learning algorithm M :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Example 1</head><label>1</label><figDesc>which is the number of digits and characters in c-An atom Q(T 1 , T 2 , ...) has cost C(Q(T 1 , T 2 , ...)) = 1 + C(T 1 ) + C(T 2 )+ . . .The Noughts and Crosses position inFigure 2 is represented by the atom b(e, x, o, e, e, x, o, e, o), where b is a predicate representing a board, e is an empty field, o and x are marks on the board. It has cognitive cost C(b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Definition 7 (</head><label>7</label><figDesc>Minimum primitive solution program, MÏ (E)): Given a set of primitives Ï and examples E, a datalog program learned from examples E using a symbolic machine learning algorithm M and a set of primitives Ï â Ï is a minimum primitive solution program MÏ (E) if and only if for all sets of primitives Ï â Ï where |Ï | &lt; |Ï | and for all symbolic machine learning algorithm M using Ï , there exists no machine learned program M (E) that is consistent with examples E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Example of pre-and post-training question for the Island Game. A board is presented to the participant to select a move that he or she thinks is optimal.</figDesc><graphic coords="15,193.00,370.03,207.42,131.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>(a) Mixed background self learning and machine-aided learning. (b) Student self learning and machine-aided learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Number of correct answers in pre-and post-training with respect to question categories.</figDesc><graphic coords="17,83.31,89.45,155.55,158.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>H win_1 win_2 win_3 H1</head><label>win_3</label><figDesc>Human comprehensions manifest in textual answer quality C C C H2 Difficulty for human participants to provide textual answer increases with textual answer quality C C C H3 Machine learned theory improves textual answer quality N C N two primitives. Based on the learned theory 6 of M IP lain in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>Participant's strategy: win_1(A,B):-move(A,B), number_of_pairs(B,o,0). Correct strategy: win_1(A,B):-move(A,B), won(B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Left: participant's chosen move from the initial position in Figure 4. Right: M etagol one-shot learns from participant's move a program representing his strategy. The learned program represents a strategy to prevent player Orange (who would play O in Noughts and Crosses) from occupying the entire island No.3 rather than going for a full occupancy on island No.1 which is an immediate win and a mismatch between learned and taught knowledge.</figDesc><graphic coords="21,91.95,89.45,155.56,80.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A set of win rules is learned by M IGO. MIGO's background knowledge contains a general move generator move/2 and a won classifier won/1 to encode the minimum rules of the game. The program is dyadic and win_1/2 can be reduced to win_1(A, B) : -move(A, B), won(B) by removing literals after unfolding. A more detailed description of the program learned by M IGO was given in<ref type="bibr" target="#b45">[46]</ref>.</figDesc><table><row><cell>Depth</cell><cell>Rules</cell></row><row><cell>1</cell><cell>win_1(A,B):-win_1_1_1(A,B),won(B).</cell></row><row><cell></cell><cell>win_1_1_1(A,B):-move(A,B),won(B).</cell></row><row><cell>2</cell><cell>win_2(A,B):-win_2_1_1(A,B),not(win_2_1_1(B,C)).</cell></row><row><cell></cell><cell>win_2_1_1(A,B):-move(A,B), not(win_1(B,C)).</cell></row><row><cell>3</cell><cell>win_3(A,B):-win_3_1_1(A,B),not(win_3_1_1(B,C)).</cell></row><row><cell></cell><cell>win_3_1_1(A,B):-win_2_1_1(A,B), not(win_2(B,C)).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Criteria for evaluating textual answers and examples for category win_2/2.</figDesc><table><row><cell>Q(r)</cell><cell>Criteria</cell><cell>Exemplary r</cell></row><row><cell>Level 0</cell><cell>r does not fit into any of the categories</cell><cell>"Follow the instructions."</cell></row><row><cell></cell><cell>below</cell><cell></cell></row><row><cell>Level 1</cell><cell>One or more primitives in the machine</cell><cell>"This move gives me a pair."</cell></row><row><cell></cell><cell>learned theory, directly or by synonyms,</cell><cell></cell></row><row><cell></cell><cell>are described correctly in r</cell><cell></cell></row><row><cell>Level 2</cell><cell>All primitives in the machine learned</cell><cell>"I should have picked this move to pre-</cell></row><row><cell></cell><cell>theory, directly or by synonyms, are de-</cell><cell>vent the opponent and get two attacks."</cell></row><row><cell></cell><cell>scribed correctly in r</cell><cell></cell></row><row><cell>Level 3</cell><cell>r is unambiguous and all primitives are de-</cell><cell>"This move gives me two attacks and</cell></row><row><cell></cell><cell>scribed correctly, directly or by synonyms,</cell><cell>prevents the opponent from getting a</cell></row><row><cell></cell><cell>in the same order as in the executional</cell><cell>pair."</cell></row><row><cell></cell><cell>stack of the machine learned theory</cell><cell></cell></row><row><cell>Level 4</cell><cell>r explains one or more primitives in the</cell><cell>"This is a good move because by making</cell></row><row><cell></cell><cell>machine learned theory in correct causal</cell><cell>two pairs and blocking the opponent,</cell></row><row><cell></cell><cell>relations, directly or by synonyms</cell><cell>the opponent cannot win in one turn</cell></row><row><cell></cell><cell></cell><cell>and can only block one of my pairs."</cell></row></table><note><p>examples, D is a logic program representing the definition of a target predicate, H is a group of participants sampled from a human population. M (E) denotes a machine learned theory which belongs to a definite clause program class with hypothesis space S. M denotes a minimum primitive solution learning algorithm. First, we are interested in demonstrating whether 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>The number and accuracy of HQ and LQ responses for groups MSR, MMR, SSR, SMR and each question category. For win_3, the most mentally challenging category of all three, no HQ response was given.</figDesc><table><row><cell></cell><cell></cell><cell>win_1</cell><cell>win_2</cell><cell>win_3</cell></row><row><cell>MSR</cell><cell>No. HQ / post-train accuracy</cell><cell>9 / 0.889</cell><cell>1 / 1.0</cell><cell>-</cell></row><row><cell></cell><cell>No. LQ / post-train accuracy</cell><cell>19 / 0.421</cell><cell>32 / 0.406</cell><cell>29 / 0.517</cell></row><row><cell>MMR</cell><cell>No. HQ / post-train accuracy</cell><cell>8 / 1.00</cell><cell>2 / 1.00</cell><cell>-</cell></row><row><cell></cell><cell>No. LQ / post-train accuracy</cell><cell>16 / 0.250</cell><cell>35 / 0.486</cell><cell>29 / 0.483</cell></row><row><cell>SSR</cell><cell>No. HQ / post-train accuracy</cell><cell>6 / 1.00</cell><cell>1 / 1.00</cell><cell>-</cell></row><row><cell></cell><cell>No. LQ / post-train accuracy</cell><cell>0 / 0.00</cell><cell>8 / 0.750</cell><cell>9 / 0.667</cell></row><row><cell>SMR</cell><cell>No. HQ / post-train accuracy</cell><cell>9 / 1.00</cell><cell>9 / 0.778</cell><cell>-</cell></row><row><cell></cell><cell>No. LQ / post-train accuracy</cell><cell>3 / 0.00</cell><cell>14 / 0.571</cell><cell>19 / 0.737</cell></row><row><cell cols="2">(a) The accuracy of textual answers in-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">creases with respect to the number of prim-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">itives covered.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Hypotheses concerning cognitive window and explanatory effects. C stands for confirmed, H stands for hypothesis, T stands for test outcome.</figDesc><table><row><cell>H</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>MIPlain source is available at https://github.com/LAi1997/MIPlain.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Within the scope of this work, we focus on the symbolic subset of machine learning. However, more general definitions are possible and might be provided by taking into account, for instance, post-hoc interpretations generated from neural networks<ref type="bibr" target="#b57">[58]</ref> and policy summaries extracted from agent-based systems<ref type="bibr" target="#b3">[4]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Raw data are available upon request from the authors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>AMT (www.mturk.com) is an online crowdsourcing platform which we used to recruit experiment participants.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>The translation of the learned theory into textual and visual explanations does not contain redundant parts.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The contribution of the authors from University of Bamberg is part of a project funded by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -405630557</rs> (PainFaceReader). The second author acknowledges support from the <rs type="funder">UK's EPSRC Human-Like Computing Network</rs>, for which he acts as director.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Peeking inside the black-box: A survey on explainable artificial intelligence (XAI)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An effective metacognitive strategy: Learning by doing and explaining with a computer-based cognitive tutor</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aleven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Koedinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="179" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memory for goals: An activation-based model</title>
		<author>
			<persName><forename type="first">E</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Trafton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="39" to="83" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sarne</surname></persName>
		</author>
		<title level="m">Summarizing agent strategies. Autonomous Agent Multi-Agent System</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The role of examples and rules in the acquisition of a cognitive skill</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fincham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Douglass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology: learning, memory, and cognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">932</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rules of the Mind, chapter The Tower of Hanoi and goal structures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushmerick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<editor>L. Erlbaum</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="121" to="142" />
			<pubPlace>Hillsdale, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Use of Analogy in a Production System Architecture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="267" to="297" />
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning Optimal Chess Strategies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Oxford University Press, Inc</publisher>
			<biblScope unit="page" from="291" to="309" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning and sequential decision making</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Implicit learning in the control of complex systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Broadbent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Lawrence Erlbaum Associates, Inc</publisher>
			<biblScope unit="page" from="131" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Behavioural cloning: Phenomena, results and problems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>UrbanÄiÄ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sammut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFAC Proceedings Volumes</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="143" to="149" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A study of thinking: With an appendix on language by</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bruner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Goodnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956">1956</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Derivational analogy: A theory of reconstructive problem solving and expertise acquisition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What one intelligence test measures: A theoretical account of the processing in the raven progressive matrices test</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Just</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="404" to="431" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ohlsson</surname></persName>
		</author>
		<title level="m">Complex Declarative Learning</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Aspects of the theory of syntax</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>M.I.T. Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficiently learning efficient programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cropper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Imperial College London</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Cropper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</author>
		<ptr target="https://github.com/metagol/metagol" />
		<title level="m">Metagol system</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning efficient logic programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cropper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1063" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A theory of implicit and explicit knowledge</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dienes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="735" to="808" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relational reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>DÅ¾eroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Driessens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="7" to="52" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analogical reminding: A good match is hard to find</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gentner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Landers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Systems, Man and Cybernetics</title>
		<meeting>the International Conference on Systems, Man and Cybernetics</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the complexity of teaching</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="20" to="31" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Fitch</surname></persName>
		</author>
		<title level="m">The faculty of language: what is it, who has it, and how did it evolve? Science</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="1569" to="1579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ted: Teaching ai to explain its decisions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E A</forename><surname>Mojsilovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Abduction in Natural Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hobbs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Peter Peregrinus</publisher>
			<pubPlace>Stevenage</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Surface and structural similarity in analogical transfer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Holyoak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="332" to="340" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transparency and explanation in deep reinforcement learning neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sycara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2018 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="144" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Johnson-Laird</surname></persName>
		</author>
		<title level="m">Mental Models: Towards a Cognitive Science of Language, Inference, and Consciousness</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Harvard University Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Thinking, fast and slow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Macmillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On tables of random numbers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhya: The Indian Journal of Statistics, Series A</title>
		<imprint>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="369" to="375" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relationship of selected cognitive abilities to concept attainment and information processing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lemke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klausmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational psychology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bias reformulation for one-shot function induction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dechter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd European Conference on Artificial Intelligence (ECAI 2014)</title>
		<meeting>the 23rd European Conference on Artificial Intelligence (ECAI 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="525" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Experiments on the mechanization of game-learning part i. characterization of the model and its parameters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="236" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inductive rule generation in the context of the fifth generation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Workshop</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Cognitive models from sub cognitive skills</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hayes-Michie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Peter Peregrinus</publisher>
			<biblScope unit="page" from="71" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Building symbolic representations of intuitive real-time skills from performance data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Camacho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Intelligence</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The magical number seven, plus or minus two: Some limits on our capacity for processing information</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Psychological Review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="81" to="97" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Explainable ai: Beware of inmates running the asylum or: How i learnt to stop worrying and love the social and behavioural sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sonenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI Workshop Explainable Artif. Intell</title>
		<meeting>IJCAI Workshop Explainable Artif. Intell<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generalization as search</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="203" to="226" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E A</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ultra-strong machine learning: comprehensibility of programs learned with ilp</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamaddoni-Nezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Besold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Inductive logic programming</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Gen. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="295" to="318" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Machine discovery of comprehensible strategies for simple games using meta-interpretive learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hocquette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Generation Computing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="203" to="217" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Meta-interpretive learning of higher-order dyadic datalog: Predicate invention revisited</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Joint Conference Artificial Intelligence</title>
		<meeting>the 23rd International Joint Conference Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1551" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Meta-interpretive learning: application to grammatical inference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pahlavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamaddoni-Nezhad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="25" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unified Theories of Cognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Harvard University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Mechanisms of skill acquisition and the law of practice. Cognitive skills and their acquisition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Rosenbloom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981. 1981</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Journal of experimental psychology. Learning, memory, and cognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Novick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Holyoak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="398" to="415" />
		</imprint>
	</monogr>
	<note>Mathematical problem solving by analogy</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning Efficient Classification Procedures and Their Application to Chess End Games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="463" to="482" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Simplifying decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Man-Machine Studies</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="221" to="234" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Faster teaching via pomdp planning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Rafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shafto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="page" from="1290" to="1332" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Selecting analogous problems: Similarity versus inclusiveness</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Ackinclose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Use of examples and procedures in problem solving</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bolstad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">753</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Empirical evidence for derivational analogy</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual conference of the cognitive science society</title>
		<meeting>the 21st annual conference of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mutual explanations for cooperative decision making in medicine</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Finzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KI-KÃ¼nstliche Intelligenz</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="233" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Inductive rule learning on the knowledge level</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kitzelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="237" to="248" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Implicit learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Seger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">163</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Interestingness elements for explainable reinforcement learning: Understanding agents&apos; capabilities and limitations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sequeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gervasio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">288</biblScope>
			<biblScope unit="page">103367</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Automatic induction of classification rules for a chess endgame</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niblett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Chess</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Clarke</surname></persName>
		</editor>
		<meeting><address><addrLine>Pergammon, Oxford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="73" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Algorithmic program debugging. acm distinguished dissertation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Performance and competence in second language acquisition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shohamy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="138" to="151" />
			<pubPlace>United Kingdom</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E A</forename><surname>Van Den Driessche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The understanding process: Problem isomorphs</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="165" to="190" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Explanations considered harmful? user interactions with machine learning systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bussone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)</title>
		<meeting>the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The teaching size: computable teachers and learners for universal languages</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Telle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>HernÃ¡ndez-Orallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ferri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1653" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Reconstructing human skill with machine learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>UrbanÄiÄ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bratko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Artificial Intelligence</title>
		<meeting>the 11th European Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="498" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Verbal explanations for deep reinforcement learning neural networks with attention on extracted features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sycara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 28th IEEE International Conference on Robot and Human Interactive Communication</title>
		<meeting><address><addrLine>RO-MAN</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Learning from Delayed Rewards</title>
		<author>
			<persName><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Graying the black box: Understanding dqns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Zrihem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with relational inductive biases</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E A</forename><surname>Babuschkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Automatic generation of analogous problems to help resolving misconceptions in an intelligent tutor system for written subtraction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops Proceedings for the Twenty-fourth International Conference on Case-Based Reasoning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1815</biblScope>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A human like incremental decision tree algorithm: Combining rule learning, pattern induction, and storing examples</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LWDA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Machine teaching: An inverse problem to machine learning and an approach toward optimal education</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4083" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
