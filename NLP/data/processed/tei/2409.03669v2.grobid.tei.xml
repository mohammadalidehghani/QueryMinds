<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION</title>
				<funder>
					<orgName type="full">Hightech Agenda Bavaria</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-05">5 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Edgar</forename><surname>Wolf</surname></persName>
							<email>edgar.wolf@hs-kempten.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences</orgName>
								<address>
									<settlement>Kempten</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Windisch</surname></persName>
							<email>tobias.windisch@hs-kempten.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences</orgName>
								<address>
									<settlement>Kempten</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-05">5 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">444824BE6EDBD0BD2E924496C321E1BA</idno>
					<idno type="arXiv">arXiv:2409.03669v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Process curves are multivariate finite time series data coming from manufacturing processes. This paper studies machine learning that detect drifts in process curve datasets. A theoretic framework to synthetically generate process curves in a controlled way is introduced in order to benchmark machine learning algorithms for process drift detection. An evaluation score, called the temporal area under the curve, is introduced, which allows to quantify how well machine learning models unveil curves belonging to drift segments. Finally, a benchmark study comparing popular machine learning approaches on synthetic data generated with the introduced framework is presented that shows that existing algorithms often struggle with datasets containing multiple drift segments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Manufacturing lines typically consist of processes arranged sequentially, each using techniques like casting, forming, or joining to shape components to their final specifications. Advanced sensor technology enables precise monitoring of key performance indicators, like force, pressure, or temperature, over time. IoT-enabled systems now commonly store the data obtained, called process curves, facilitating analysis across both single components and entire production sequences <ref type="bibr" target="#b0">[1]</ref>. Issues like anomalous batches, tool wear, or miscalibrations can degrade performance, often subtly, by causing gradual shifts in process curves. Thus, detecting process drifts is key to keep unplanned downtimes and scrap parts at bay. In high-volume production, this is particularly challenging due to the rapid data generation and complexity of multi-variable curves and hence these settings have been an ideal application for machine learning methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Although process curves are multivariate time-series, process drift detection should not be confused with drift detection in time series <ref type="bibr" target="#b8">[9]</ref> or drifts in profile data <ref type="bibr" target="#b9">[10]</ref> (see also Figure <ref type="figure" target="#fig_0">1</ref>)). Typically, statistical drift detection methods from time series analysis are not direct applicable, not alone because process curves are highdimensional objects, but also because of high autocorrelation among their sample axis. As a consequence, deep learning techniques, most prominently dimensionality reduction methods like autoencoders <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, have become increasingly popular <ref type="bibr" target="#b15">[16]</ref> as they allow to first learn a low-dimensional representation of the high-dimensional input and then analyse the learned latent variables with classic statistical tools, like sliding Kolmogorov-Smirnov tests <ref type="bibr" target="#b16">[17]</ref>, Hellinger-distance based techniques <ref type="bibr" target="#b17">[18]</ref>, or by facilitating the Maximum Mean Discrepancy <ref type="bibr" target="#b18">[19]</ref>. For many machine learning applications relevant for manufacturing, estab- lished ways and datasets to benchmark the performance of algorithms exist, like for causal discovery in quality data <ref type="bibr" target="#b19">[20]</ref>, anomaly detection in images from optical inspections <ref type="bibr" target="#b20">[21]</ref>, or reinforcement learning in continuous control tasks <ref type="bibr" target="#b21">[22]</ref>. However, for process drift detection, such a framework is yet missing to the best of our knowledge. This may be due to the following two reasons: The lack of both, publicly available datasets and a suitable evaluation metric. Beside a few publicly released datasets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, most of the existing work does not release any data to the public, making it impossible to test other detectors on the same dataset. Often, this is due to privacy issues and fear of leaking information to competitors. In addition, as datasets for process drifts are inherently non identically and identically distributed (iid), any sort of test and train splits introduced significant biases making evaluation of algorithms hard if only one variant of a dataset is available Finally, process drift detection is by definition an unsupervised learning task, but to benchmark detectors, a ground truth is required, labeling precisely when a drift starts and when it ends. This seamlessly leads to the second challenge, namely the missing evaluation metric. As in any machine learning task, the metric depends on the precise application and a trustworthy ground truth. A commonly used metric used in research and practice to measure the statistical performance of a binary classifier, often independent of the application, is the area under the ROC curve <ref type="bibr" target="#b25">[26]</ref> -short AUC. However, the usage of the AUC is typically only applicable in settings where data is assumed to be iid, unlike in drift detection. In our work, we want to exactly address these issues by introducing a benchmarking framework for researchers, allowing them to reliably validate their process drift detection algorithms. At a high level, our main contributions are:</p><p>• We present a simple, yet flexible and effective theoretic framework to generate synthetic process curve datasets including drifts with a validated ground truth (Section 2 and Section 3) which also allows feeding of curves from real processes. • We introduce an evaluation metric called temporal area under the curve (TAUC) in Section 4, which aims to take the temporal context of a detection into account.</p><p>• We conduct a short benchmark study in Section 5 as a proof of concept for the effectiveness of both, our TAUC metric and our proposed data generation method to measure the predictive power of drift detectors.</p><p>Our work is based on preliminary results of the first author <ref type="bibr" target="#b26">[27]</ref>. In this work, we provide additionally insights into the introduced metric, introduce a variant called soft TAUC and compare it in depth with existing metrics. Moreover, we substantially generalize the data synthetization framework, for instance by allowing higher-order derivatives, and we generate more sophisticated datasets for the benchmark study. We also release the code that helps to generate process curves to benchmark drift detectors, which is freely available under <ref type="url" target="https://github.com/edgarWolf/driftbench">https://  github.com/edgarWolf/driftbench</ref>. Its optimization back-end is implemented in Jax <ref type="bibr" target="#b27">[28]</ref> allowing a fast GPU-based generation of process curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Statistical framework to model process drifts</head><p>In this section, we formalize what we consider as process curves and drifts within. Generally speaking, process curve datasets are datasets consisting of finitely many multivariate time series each having finitely many steps. We formally model a process curve as a finite time-series (Y (x)) x∈I with Y (x) ∈ R c , I ⊂ R a finite set, and where Y : R → R c represent physical properties of the process to be measured and x an independent variable, often the time. In staking processes, for instance, Y is the measured force and x the walked path of the press (compare also <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">Figure 9]</ref>). Another example are pneumatic test stations, where Y might be a pressure measured over time x. In bolt fastening processes, Y represents the torque measured over the angle x <ref type="bibr" target="#b7">[8]</ref>. We call the number of variables c ∈ N in the curve the dimension of the process curve and write [T ] for the set {1, . . . , T } and often refer to it as temporal axis.</p><p>Whenever a manufacturing process finishes its work on a component, a process curve is yielded. Thus, when the same process is executed on multiple times sequentially, a long sequence C 1 , C 2 , . . . , C t , . . . , C T with T ∈ N of process curves is obtained where each C t arises under slightly different physical conditions Y 1 , . . . , Y T , i.e., C t = (Y t (x + ϵ x ) + ϵ y ) x∈It , where ϵ x and ϵ y represents measurement noise or inaccuracies. In theory, also the sets I t can vary for each t ∈ [T ], for instance due to different offsets. Wearout or tool degradation affects the process curves gradually and to model their deformation along the execution axis, we assume that there exists functions f : R k × R → R c and w : [T ] → R k such that for all t ∈ [T ] and x ∈ I t :</p><p>(2.1)</p><formula xml:id="formula_0">f (w(t), x) = Y t (x)</formula><p>where the function f is a proxy for the physics underneath the process. The vector w(t) ∈ R k represents environmental properties of the t-th execution, and some of its coordinates correspond to component properties, some to properties of the machine. Without restricting generality and to keep notation simple, we will assume for the remainder that c = 1, as the multivariate case is a straight-forward application of our approach by modeling each variable in Y individually (see also Remark 3.1). Assuming only component variance and no tool degradation, we could assume that w(t) is sampled in each process execution from a fixed but unknown distribution on R k , like w(t) ∼ N µ,σ with fixed µ ∈ R k and σ ∈ R k×k for all t ∈ [T ]. As mentioned, tool degradation, in contrast, affects the process from execution to execution, i.e., the parameters of the distribution shift over time leading to a deformation of the observed process curve. Such process drifts should not be confused with concept drifts, where the goal is typically to analyse the declining performance of a trained machine learning model when new data starts to differ from the train data <ref type="bibr" target="#b28">[29]</ref>. Moreover, detecting drifts in process curves is different to detecting drift in profile data <ref type="bibr" target="#b9">[10]</ref>, where one typically is interested in drifts among the curves yielded by a single execution, not in drifts over multiple executions. A similar application is the identification of drifts within profile data, where typically one process execution yields a sequence of process curves of fixed size, like in spectroscopy when one curve is some intensity over time which is measured for different wavelengths <ref type="bibr" target="#b29">[30]</ref>. One way to model process drifts is to model the evolution of the latent parameters w(t), like using a dynamical system. For instance, in control theory <ref type="bibr" target="#b30">[31]</ref>, w(t) is considered as latent state of a system which evolves over the executions t and one observes a multivariate output Y (t) ∈ R |It| with Y (t) = Y t (I t ). Introducing a control vector u(t) ∈ R p , w(t) can be considered as state variable w(t) of the system that evolves over time and is influenced by a control vector u(t) ∈ R p such that ∂ t w(t) = h(w(t), u(t), t) and Y (t) = f (w(t)) holds for all t ∈ N. Here, however, one has to precisely model how the state w changes over executions and how it is affected by interventions u and has to solve challenging non-linear differential equations. However, as we will argue, the degradation of the curve can be described directly in curve space in many scenarios. Thus, we directly model the transformation of the process curves in curve space by letting certain support points of the curve move in a controlled way: Definition 2.1 (Support points). Let f : R k × R → R be an i-times differentiable function, i ∈ N, and ∂ i</p><p>x f be the i-th derivative of f according to the second argument. Let x, y ∈ R n , then (x, y) is a support point of i-th order for f at w ∈ R k if ∂ i x f (w, x j ) = y j for all j ∈ [n]. Support points can be considered as points surpassed by the graph of f (w, •) : R → R (see visualization on the left in Figure <ref type="figure" target="#fig_2">3</ref>). Typically, such support points are physically motivated and if latent properties of the process change, certain support points change their position in curve space. For instance, in a staking process, the position x(t) and value y(t) of the maximal force, i.e., where the first derivative is zero, starts shifting (see Figure <ref type="figure" target="#fig_1">2</ref>). That is, we can describe this behavior by modelling the support points (x(t), y(t)) and (x(t), 0) of first and second order respectively, i.e. f (w(t), x(t)) = y(t) and ∂ 1</p><p>x f (w(t), x(t)) = 0. We formalize in Section 3 how we can use this to generate process curves and drifts synthetically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data generation</head><p>Let f : R k × R → R be as in Section 2 a proxy for the physical relations for given manufacturing process. In this section, we build our synthetization framwork upon the setup introduced in Section 2. Here, we neither focus on how w(t) behaves in latent space, nor on how f is formulated exactly. Instead of modeling the evolution of w(t) with a dynamic system, our idea is to model the behavior of support points over process executions in curve space and to seek for parameters w(t) using non-linear optimization satisfying the support point conditions from Definition 2.1. For the remainder of this section, we explain how w(t) can be computed given the support points. Thus, assume we have for each process execution Instead of modelling w(t) explicitly, we compute w(t) implicitly such that (3.1) is satisfied. For instance, if f is l + 2-times differentiable in its second argument and if ∂ 2 w ∂ i x f exists, we can solve (3.1) individually for all t ∈ [T ] using second-order quasi-Newton methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">Chapter 3]</ref> for the objective function</p><formula xml:id="formula_1">t ∈ [T ] support points (x 1 (t), y 1 (t)), . . . (x l (t), y l (t)) with with x i (t), y i (t) ∈ R n i , that is, (3.1) ∂ i x f (w(t), x i j (t)) = y i j (t) ∀j ∈ [n i ].</formula><formula xml:id="formula_2">(3.2) w(t) = arg min w∈R k l i=1 n i j=1 D i • ∂ i x f (w, x i j (t)) -y i j (t)</formula><p>where D 1 , . . . , D l are constants to account for the different value ranges of the functions ∂ i x f . By solving Thus, solving (3.2) for each t ∈ [T ], we obtain a sequence w(1), . . . , w(T ) ∈ R k and consequently, we get a sequence of functions f (w(1), •), . . . , f (w(t), •). Now, these functions can be evaluated on arbitrarily sets I t ⊂ R whose point not necessarily need to be equidistant. Setting C t = f (w(t), I t )+ϵ y ∈ R |It| , we finally obtain a sequence of process curves C 1 , . . . , C T . A compact overview of the data generation method is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Generation of process curves</head><formula xml:id="formula_3">Input: f : R k × R → R, x i (1), y i (1), . . . x i (T ), y i (T ) ∈ R n i , i ∈ [l], x ∈ R, ∆x ∈ R &gt;0 , m ∈ N. Output: Process curves C 1 , . . . , C T . 1: for t ∈ [T ] do 2:</formula><p>Compute solution w(t) for (3.2) using support points (x 1 (t), y 1 (t)), . . . , (x l (t), x l (t))</p><p>3:</p><formula xml:id="formula_4">I t ← {x + j • ∆x + ϵ x : j ∈ [m]} 4: C t ← f (w(t), I t ) + ϵ y 5: end for 6: return C 1 , . . . , C T x i j (t) j-th support point for ∂ i x f with j ∈ [n i ] i-th derivative of f with i ∈ [l]</formula><p>t-th process execution with t ∈ [T ] Fig. <ref type="figure">4</ref>. Short overview of our notation.</p><p>Its left to show how to generate the support points as input for Algorithm 1. One way is to use support points of a real process curve dataset, and using Algorithm 1 to create semisynthetic copy of it. In a fully synthetic setting, the support points at execution t ∈ [T ], the support points (x i (t), y i (t)) can be sampled from a distribution on R n i respectively, whose statistical properties change over the temporal axis. For instance, y i (t) ∼ N µ i (t),σ with µ i : [T ] → R n i encoding the drift behavior over the temporal axis for the support points. Another free parameter of Algorithm 1 is the function f to use. In principle, f can be chosen from any parametrized function set, like B-splines, Gaussian processes <ref type="bibr" target="#b32">[33]</ref>, neural networks <ref type="bibr" target="#b33">[34]</ref>, or Kolmogorov-Arnold networks <ref type="bibr" target="#b34">[35]</ref>. In Appendix B, we showcase in depth an example where f is a polynomial. Remark 3.1 (Multivariate data). Our theoretic framework extends naturally to multivariate time series data, where each dimension d ∈ [c] (or signal) has its own function f d . If they do not share their latent information w d (t), then Algorithm 1 can be executed for each dimension individually. If they share some latent information, then (3.2) can be extended by summing all support point conditions for all f 1 , . . . , f c . Remark 3.2 (Profile data). Our theoretic framework is also capable to generate profile data with drifts holding both, drifts within a profile and drifts over executions. This can be obtained, for instance, by describing how the support points should behave in each profile and for subsequent profiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The temporal area under the curve</head><p>Different usecases require different performance metrics to evaluate algorithms. In classification, for instance, sometimes avoiding false positives is, sometimes avoiding false negatives. However, when it comes to general benchmarking classifiers somewhat independently of their precise application in the sense to see how well their response correlates to the actual class label, the AUC <ref type="bibr" target="#b25">[26]</ref> is frequently used. However, the vanilla AUC takes samples independently of their temporal context, that is, independent of samples from the previous and next process execution. Thus, we construct in this section a more suitable metric to measure the predictive power of machine learning models for process drift detection. In order to do so, we first formalize what we understand as a process drift and which assumptions we require. Let C 1 , . . . , C T be a sequence of process curves and let D ⊂ [T ] be the set of curve indices belonging to drifts. Our first assumption is that drifts, different from point anomalies, appear sequentially and can be uniquely decomposed into disjoint segments: The drift segments can be considered as a partition of the smallest consecutive drifts which cannot decomposed any further into smaller segments. Now, assume we also have the output s ∈ R T of a detector where each coordinate s t quantifies how likely the curve C t of the it-h process execution belongs to a drift, that is, the higher s t the more likely the detector classifies t ∈ D (see also Figure <ref type="figure" target="#fig_3">5</ref>). By choosing a threshold τ ∈ R, we can construct a set D(s, τ</p><formula xml:id="formula_5">Definition 4.1 (Drift segments). Let D ⊂ [T ]. Then a series of subsets D 1 , . . . , D k ⊂ D is a partition of drift segments if there exists 1 ≤ l 1 &lt; h 1 &lt; l 2 &lt; h 2 &lt; . . . , &lt; l k &lt; h k ≤ T such that for all i, we have D i = [l i , h i ] and D = ∪ k i=1 D i .</formula><formula xml:id="formula_6">) := {t ∈ [T ] : s t ≥ τ }</formula><p>which serves as a possible candidate for D. Clearly, if τ 1 ≥ τ 2 , then D(s, τ 1 ) ⊆ D(s, τ 2 ). Its also straight-forward to see that for every τ , the set D(s, τ ) decomposes uniquely into drift segments D1 , . . . , Dl as defined in Definition 4.1 and that the length and number of these atomic segments depends on τ . Now, to quantify the predictive power of the detector yielding s, one needs to quantify how close D(s, •) is to D when τ varies. There are many established set-theoretic measurements that are widely used in practice to quantify the distance between two finite and binary sets A and B, like the Jaccard index |A∩B| |A∪B| , the Hamming distance |A \ B| + |B \ A|, or the Overlap coefficient |A∩B| min(|A|,|B|)</p><p>just to name a few. Most metrics, however, have as a build-in assumption that the elements of the set are iid and hence the temporal context is largely ignored making them unsuitable for process drift detection. Moreover, for most detectors we have to select a discrimination threshold τ , making evaluation cumbersome as it requires to tune the threshold on a separate held-out dataset. Moreover, in most practical scenarios, D is only a small subset and thus the evaluation metric has to consider highly imbalanced scenarios as well. Clearly, detectors are required where all true drift segments D i are overlapped by predicted drift segments. For this, let</p><formula xml:id="formula_7">L i := {j ∈ [l] : D i ∩ Dj ̸ = ∅}. Clearly, L i ∩ L i+1 ̸ = ∅ if D i</formula><p>and D i+1 both intersect with a predicted drift segment. Now, the set T i := ∪ j∈L i Dj which is the union of all predictive segments intersecting with D i serves as a candidate for D i . To measure how well D i is covered -or overlapped -by T i we define the soft overlap score inspired by the Overlap coefficient as follows:</p><formula xml:id="formula_8">(4.1) sOLS(D i , s, τ ) := |T i | max(T i ∪ D i ) -min(T i ∪ D i ) + 1</formula><p>Obviously, an sOLS of 1 is the best possible and this is reached if and only if T i = D i . It is easy to see that for fixed D i , the enlargement of T i beyond the boundaries of D i improves the overlap score, as |T i | increases and one of either max(T i ∪ D i ) ormin(T i ∪ D i ) increases as well. A special case is if D i is completely covered by T i , i.e. D i ⊆ T i , then it follows that T i is an interval as well and thus sOLS(D i , s, τ ) = 1. When T i enlarges, then the number of false positives, i. into account. To also take false negatives into account, the enumerator in (4.2) could be changed as follows, yielding our final definition of the overlap score:</p><formula xml:id="formula_9">(4.2) OLS(D i , s, τ ) := |T i ∩ D i | max(T i ∪ D i ) -min(T i ∪ D i ) + 1 .</formula><p>Algorithm 2 illustrates in detail how the Overlap score OLS(D, s, τ ) can be computed algorithmically. Our score considers both, the OLS and the FPR, which mutually influence each Algorithm 2 Overlap score OLS(D, s, τ )</p><formula xml:id="formula_10">Input: D ⊂ [T ], s ∈ R T , τ ∈ R . Output: Overlap score . 1: D 1 , . . . , D k ← find drift segments of D 2: D1 , . . . , Dl ← find drift segments of D(s, τ ) 3: o ← 0 ∈ R k 4: for i ∈ [k] do 5: L i ← {j ∈ [l] : Dj ∩ D i ̸ = ∅} ▷ All</formula><p>predicted drift segments overlapping with D i 6: T i ← ∪ j∈L i Dj ▷ Union of all segments intersecting with D i 7:</p><formula xml:id="formula_11">o i ← |T i ∩D i | max(T i ∪D i )-min(T i ∪D i )+1</formula><p>▷ fraction of overlap 8: end for 9: return 1 k k i=1 o i other. In the computation of the AUC, any threshold τ from [min t (s t ), max t (s t )] yields a pair of false positive rate FPR(D, s, τ ) and true positive rate TPR(D, s, τ ) which can be drawn as a curve in the space where FPR is on the x-axis and TPR on the y-axis. Similarly, we define the temporal area under the curve, or just TAUC, as the area under the FPR-OLS curve while the discrimination threshold τ varies. We refer to the soft TAUC, or just sTAUC, to the area under the FPR-sOLS curve (see Figure <ref type="figure">7</ref>).</p><p>Note that the integral of the curve can be computed using two different methods, the step rule and the trapezoidal rule and depending on which method is used, the value of the score may differ. We showcase this behavior in detail for trivial detectors in Appendix D. In Appendix C, we investigate in several synthetic cases in depth the differences and similarities between sTAUC, TAUC, and AUC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Next, we benchmark existing algorithms on data generated with our framework driftbench and reporting the TAUC. All datasets and algorithms used are available in the repository of driftbench. The goal of the benchmark is to provide a proof of concept for our score and data generation method, not to be very comprehensive on the model side. Thus, based on our literature research in Section 1 we have hand-selected a small set of typically used model patterns drift detectors used in practice consists of (see Section 5.1).</p><p>The basic evaluation loop follows a typical situation from manufacturing, where process engineers have to identify time periods within a larger curve datasets where the process has drifted. Thus, all models consume as input a process curve dataset C 1 , . . . , C T and do not have access to the ground truth D, which is the set of curves belonging to a drift (see Section 5.3). Afterwards, each model predicts for each curve C t from this dataset a score s t ∈ R, and afterwards, the TAUC, sTAUC, and AUC are computed for s = (s 1 , . . . , s T ). To account for robustness, we generate each dataset of a predefined specification five times for a different random seed each, leading to slightly different datasets of roughly same complexity. All models are trained unsupervised, i.e. without any information of the true drift segments. 5.1. Algorithms. The algorithms used can be decomposed into multiple steps (see also Figure <ref type="figure">8</ref>), but not all algorithms use all steps. First, there are may some features extracted from each curve. Afterwards, a sliding window collects and may aggregate these such that a score is computed.</p><p>C 1 C 2 . . . C t . . . C T e 1 e 2 . . . e t . . . e T a 1 a 2 . . . a t . . . a T s 1 s 2 . . .</p><p>s t . . . s T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Windowing and aggregation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score computation</head><p>Fig. <ref type="figure">8</ref>. A high-level overview of the elementary tasks of the detectors used. 5.1.1. Feature extraction. In this step, we use autoencoders <ref type="bibr" target="#b10">[11]</ref> to compute a k-dimensional representation e t ∈ R k for each high-dimensional process curve C t with k small. The indention behind is to estimate an inverse of the unknown function f and to recover information about the support points used. Moreover, we also apply deterministic aggregations over the xinformation of each curve C t . 5.1.2. Windowing and aggregation. In this step, the algorithms may aggregate the data from the previous step using a fixed window of size m that is applied in a rolling fashion along the process iterations. One aggregation we use is to first compute for each coordinate j ∈ [k] of e t ∈ R k with t ≥ m the rolling mean a t,j = 1 m t i=t-m+1 e i,j . These values can then further be statistically aggregated, like by taking the maximum a t := max{a t,j : j ∈ [k]}. 5.1.3. Score computing. Goal of this step is to compute a threshold which correlates with the ground truth, that is, the larger the higher the possibility of a drift. Here, we may also aggregate previous features in a rolling fashion. The simplest aggregation we use is to compute the euclidean distance of subsequent elements s t = ∥a ta t-1 ∥ 2 which is just the absolute difference if a t and a t-1 are scalars. If a t is a scalar, we also can compute the rolling standard deviation, again over a window of size m, like this:</p><formula xml:id="formula_12">s t = 1 m -1 t j=t-m+1 a j - 1 m t i=t-m+1 a i 2 .</formula><p>Another approach follows a probabilistic path by testing if a set of subsequent datapoints {a t-m+1 , . . . , a t } come from the same distribution as a given reference set. In our study, we use a windowed version <ref type="bibr" target="#b16">[17]</ref> of the popular Kolmogorov-Smirnov test <ref type="bibr" target="#b35">[36]</ref>, often called KSWIN, which makes no assumption of the underlying data distribution. However, this can only be applied when a t is a scalar. More particularly, we define two window sizes, m r for the reference data and m o for the observation. The windows are offset by constant δ &gt; 0. We then invoke the KS-test and receive a p-value p t , which is small if the datasets come from different distributions. Thus, one way to derive a final score is to compute s t = log(1 + 1 pt ). Another probabilistic method we use in our study based on a multivariate statistical test is the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b18">[19]</ref>. This method uses feature mappings based on kernels, and calculates the distance between the means in these mappings. MMD also makes no assumption about the underlying distribution, and works on multidimensional data. We use this method in the same way using two windows as described in the KS-test. We also evaluate algorithms that derive their score based on a similarity search within {a 1 , . . . , a t }. Here, we use clustering algorithms, like the popular k-means algorithm, and use the euclidean distance to the computed cluster center of a t as s t . Another way is to fit a probability density function on s t , like a mixture of Gaussian distributions, and to set s t as the log likelihood of a t within this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Algorithm Overview.</head><p>Here is a short summary of the algorithms used in our benchmark study:</p><p>• RollingMeanDifference(m r ) First, the rolling mean over a window of size m r is computed over all values for the respective curves in the window. Afterwards, the maximum value for each curve is taken and the absolute difference between two consecutive maximum values is computed. • RollingMeanStandardDeviation(m r ) First, the rolling mean over a window of size m r is computed over all values for the respective curves in the window. We also choose the maximum value of these computed values per curve. Then, we compute the standard deviation using the same window for this one-dimensional input.</p><p>• SlidingKSWIN(m r , m o , δ): We compute the mean value for each curve and apply a sliding KS-test on this aggregated data. We use two windows of size m r and m o where the windows are offset by δ. • Cluster(n c ): A cluster algorithm performed on the raw curves using n c clusters where score is distance to the closest cluster center. • AE(k)-mean-KS(m r , m o , δ): First, an autoencoder is applied extracting computing k many latent dimensions. Afterwards, the mean across all k latent dimensions is computed. Finally, a sliding KS-test is applied with two windows of sizes m r and m o , where the windows are offset by δ. • AE(k)-MMD(m r , m o , δ): First, an autoencoder is applied extracting computing k many latent dimensions. Afterwards, a k-dimensional sliding MMD-test is applied with two windows of sizes m r and m o , where the windows are offset by δ. of staking processes (see also <ref type="bibr">[20, A.1]</ref>) where we used f (w, x) = 7 i=0 w i • x i as function to generate them. The dataset-2 consists of T = 10.000 curves, each called on |I] = 100 equidistant values between [0, 4], i.e., x = 0 and ∆x = 0.04. On the other hand, dataset-3 consists of T = 30.000 curves each having |I| = 400 values between [0, 4]. Both datasets have drifts that concern a movement of the global maximum together with drifts where only information of first order changes over time. In the generation process of dataset-1, we used f (w, x) = w 0 • x • sin(π • xw 1 ) + w 2 • x and generated T = 10.000 many curves, each having |I| = 100 datapoints. It only holds a single drift, where the global minimum at drifts consistently over a small period of time along the x-axis. In all datasets, the relative number of curves belonging to a drift is very small: roughly 1 percent in dataset-1, 2 percent in dataset-2, and 0.1 percent in dataset-3. Particularly, dataset-k has k many drift segments. To generate a drift segment [t 0 , t 1 ] for a given support point where the value should change linearly from a to b (see also Section B), we sampled from normal distributions N µ(t),σ with fixed σ and mean</p><formula xml:id="formula_13">µ(t) =      a, if t &lt; t 0 b • t-t 0 t 1 -t 0 + a, if t 0 ≤ t ≤ t 1 b, if t 1 &lt; t . 5.4.</formula><p>Results. The result of our benchmark study is shown in Figure <ref type="figure" target="#fig_0">10</ref>. Generally, there is a discrepancy in detectors of the highest AUC and the highest TAUC. More concrete, the larger the number of true drift segments in a dataset is, the larger the discrepancy (see also Figure <ref type="figure" target="#fig_0">11</ref>). For instance, the RandomGuessDetector reached the highest AUC score on dataset-1, where it ranges on all three datasets among the last ranks in the TAUC score. On all datasets, autoencoder-based systems reach among the best detectors for both, TAUC and AUC. Those using a multivariate test in their latent space reach better scores than these using an aggregation of multiple uni-variate tests. Although some cluster-based systems archive good AUC scores on dataset dataset-3, none of the benchmarked algorithms is capable to compute a score that can be used to recover the true drift segments, resulting in small TAUC scores for all algorithms (see also Figure <ref type="figure" target="#fig_11">14</ref>). The respective predictions over the temporal dimension of the best detectors are shown in Appendix A in more detail, where it also becomes visible that detectors with higher TAUC better recover the true drift segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This work shows how algorithms designed to detect process drifts can be benchmarked in robust and reliable way. We have introduced a scalable and controllable data generation method that creates process curves datasets with drifts and a verified ground truth. In our approach, process curve datasets can be solely generated by modelling the behavior of support points over the temporal axis and using non-linear optimization. We then introduce and study the novel TAUC score which is particularly designed to evaluate the performance of drift detectors on their temporal consistency over sequential process executions. We proved the effectiveness of our approach in a small benchmark study. Our results reveal that existing algorithms often struggle with datasets containing multiple drift segments, underscoring the need for further research.</p><p>0.0 0.2 0.4 TAUC AE-MMD (lr=0.0001, num epochs=10) AE-MMD (lr=0.0001, num epochs=100) AE-MMD (lr=0.0001, num epochs=50) AE-mean-KSWIN (lr=0.0001, num epochs=10) AE-mean-KSWIN (lr=0.0001, num epochs=100) AE-mean-KSWIN (lr=0.001, num epochs=50) ClusterDetector (method=gaussian mixture, n centers=10) ClusterDetector (method=gaussian mixture, n centers=5) ClusterDetector (method=kmeans, n centers=10) ClusterDetector (method=kmeans, n centers=5) RandomGuessDetector RollingMeanDifferenceDetector (window size=20) RollingMeanDifferenceDetector (window size=40) RollingMeanStandardDeviationDetector SlidingKSWINDetector 0.25 0.50 0.75 sTAUC dataset-1 dataset-2 dataset-3 0.25 0.50 0.75 AUC 0.50 0.75 AUC 0.2 0.4 TAUC k = 1 (corr=0.74) 0.25 0.50 0.75 AUC 0.0 0.1 0.2 TAUC k = 2 (corr=0.71) 0.25 0.50 0.75 AUC 0.05 0.10 0.15 TAUC k = 3 (corr=0.47)   w i • x i with w ∈ R 6 . We simulate 2000 process executions and thus sample 2000 process curves. The shape of each curve is defined by its support points. We are only interested in its curvature in I = [0, 4]. First, we want to add a condition onto the start and end of the interval, namely that f (w, 0) = 4 and f (w, 4) = 5. Moreover, we would like to have a global maximum at x = 2, which means the first order derivative</p><formula xml:id="formula_14">∂ 1 x f (w, 2) = 4 i=1 i • w i • 2 i-1</formula><p>should be zero and its second order derivate</p><formula xml:id="formula_15">∂ 2 x f (w, 2) = 3 i=1 i • (i -1) • w i • 2 i-2</formula><p>should be smaller than zero. Here, we want it to be -1. Finally, we want to the curve to be concave at around x = -1. All in all, these conditions result into the following equations, some of them are visualized in Figure <ref type="figure" target="#fig_3">15</ref>:</p><formula xml:id="formula_16">∂ 0 x f (w, 2) = 7 ∂ 1 x f (w, 2) = 0 ∂ 2 x f (w, 2) = -1 ∂ 0 x f (w, 0) = 4 ∂ 0 x f (w, 4) = 5 ∂ 2 x f (w, 1) = -1</formula><p>Then, we let the data drift at some particular features. We simulate a scenario, where the peak at x 0 1 and x 1 0 moves from the x-position 2 to 3 during the process executions t = 1000 until t = 1300. Thus, we let x 0 1 and x 1 0 drift from 2 to 3, resulting in a change of position of the peak. We let the corresponding y-values y 0 1 = 7 and y 1 0 = 0 unchanged. Now, we can solve each of the 2000 optimization problems, which results in 2000 sets of coefficients for each process curve, such that the conditions are satisfied. By evaluating f with the retrieved coefficients in our region of interest [0, 4], we get 2000 synthesized process curves with a drift present at our defined drift segment from t = 1000 until t = 1300. 0 250 500 750 1000 1250 1500 1750 2000 Process executions 0 1 2 3 4 x 0 0(t) x 0 1(t) x 0 2(t) t = 1000 t = 1150 t = 1300 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.0 4.5 5.0 5.5 6.0 6.5 7.0 t = 1000 t = 1300 t = 1150 x 0 1 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 x 0 250 500 750 1000 1250 1500 1750 2000 Pr oce ss exe cut ion s 4.0 4.5 5.0 5.5 6.0 6.5 7.0 Y Fig. 16. Visualization of the drift applied on x 0 1 in this example, with respective curves. The left figure shows how the x 0 i values change over time. Only x 0 1 changes, as by our drift definition from the process executions t = 1000 until t = 1300 linearly from 2 to 3, the others remain unchanged. The middle figure shows the respective curves, color-coded to the dots in the left figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. TAUC vs AUC</head><p>In this section we explore in depth the similarities and differences of the TAUC introduced in Section 4 and the established AUC. This is done along synthetic predictions. C.1. Lagged prediction. The first example we look at is a typical scenario that appears if window-based approaches are used, namely that the prediction lags a bit behind of the true window, but still the detector overlaps a significant proportion of the drift segment (see Figure <ref type="figure" target="#fig_14">17</ref>. Other than the TPR, the sOLS rewards these predictors and thus the sTAUC shows a larger value than the AUC. C.2. Change point detection. Another typical scenario is that a detector shows significantly large values at the start and end of the true drift segment, but sag in between (see Figure <ref type="figure" target="#fig_15">18</ref>). This could appear when using methods based on detecting change points. In principal, the detector correctly identifies the temporal context of the drift segment, although showing lower scores while the curves drift. Such predictions also score higher values in the sTAUC than the AUC. C.3. Varying length and position of predicted segments. A situation where the sTAUC coincides with the AUC mostly is in when only one true and predicted drift segment exist (see Figure <ref type="figure" target="#fig_0">19</ref>). In cases where the center of the predicted segment coincides with the center of the true segment, the AUC and sTAUC match almost exactly when the length of the predicted segment is varied (see left graphic in Figure <ref type="figure" target="#fig_1">20</ref>). If the predicted segment has fixed P denotes the portion of drifts in y and k denotes the number of drift segments in y. In case of the step function, the computed score will always be 0, since the constructed curve only contains one step from [0, 1) with a OLS-value of 0, and only reaches a OLS-value of P k when reaching a FPR of 1 on the x-axis. Hence, the area under this constructed curve is always 0. When using the trapezoidal rule, we linearly interpolate the two obtained trivial points of the curve, thus constructing a line from (0, 0) to (1, P K ). The TAUC is then given by the area under this line, which is equal to P 2k . Now suppose a detector which never indicates a drift, called NeverGuesser. Then we receive (0, 0) as our only point, which does not construct a curve and thus does not have an area under it. Hence, the TAUC for this trivial detection is 0 in both cases.  In order to investigate how the TAUC behaves with an increasing number of segments k in y, we simulate such inputs with a trivial detection and compute the resulting values for the TAUC. We choose an input length of n = 1000. When using the step rule, the TAUC is always 0 as expected, since the only step always retains its area under the curve of 0. But when looking at the obtained TAUC values when using the trapezoidal integration rule, we can clearly see the TAUC decreasing when k increases. This decreasing behaviour can be approximated by 1 2k , since the TAUC for a trivial detection with k segments in case of the trapezoidal rule can be computed with P 2k and 0 &lt; P ≤ 1. Thus, the limit of the TAUC computed with the trapezoidal integration rule with increasing k follows as: lim</p><p>k→∞ P 2k = 0 0 100 200 300 400 500 k 0.0 0.1 0.2 0.3 0.4 0.5 TAUC TAUC scores 1 2k</p><p>Fig. <ref type="figure" target="#fig_2">23</ref>. Visualization of the TAUC with the trapezoidal integration rule, when increasing k, alongside an approximation 1 2k . The TAUC gets closer to 0 with increasing k.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of different kinds of time series data from manufacturing processes and drifts within.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Samples from a process curve (left) as well as a sequence of curve samples (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of the data synthetization given a function f (w, x) = 5 i=0 w i • x i . Left figure shows f (w, •) solved for concrete x i , y i (red points).Right figure shows sequence f (w 1 , •), . . . , f (w 100 , •) where gaussian noise was added on one coordinate in y 1 (t) (corresponding coordinate x 1 (t) is marked with a dashed line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Applying a process drift detector on each process curves yields a score s which needs to be compared to the ground truth D for each threshold τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 D3Fig. 6 .</head><label>26</label><figDesc>Fig. 6. Temporal arrangements of true and predicted drift segments as input for Algorithm 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>e. the time points t contained in some Di and in the complement D := [T ] \ D of the ground truth D, enlarges as well. Thus, the predictive power of a detector is shown in the overlap score as well as the created false positive rate FPR(D, s, τ ) := | D(s, τ ) ∩ D| |D| .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Example 4 . 2 .Fig. 7 .</head><label>427</label><figDesc>Fig.7. The TPR, sOLS, and OLS when the FPR varies for the synthetic prediction on the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>5. 3 .Fig. 9 .</head><label>39</label><figDesc>Fig. 9. The datasets used in our benchmark study. The true drift segments are marked in green. Lower figures show selected curves, whose color encodes the process iteration t ∈ [T ] -blue marks smaller t values, red larger ones. Recall that dataset-k has k many drift segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .Fig. 11 .Figure 12 ,Fig. 12 .Fig. 13 .</head><label>1011121213</label><figDesc>Fig. 10. Benchmark results on dataset-1, dataset-2, and dataset-3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Best detectors on dataset-3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>i=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .Fig. 16 .</head><label>1516</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>. Visualization of some process curves in the example dataset. The red dots indicate support points with first order information given. The green line visualizes the slope at the green dot, encoded by the condition for the first derivative. The purple dashed line indicates the curvature at the corresponding x-value, encoded by the condition for the second derivative. From t = 1000 to t = 1300, the x-value of the maximum moves from 2 to 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 17 .</head><label>17</label><figDesc>Fig.<ref type="bibr" target="#b16">17</ref>. Prediction of a detector that lags behind the ground truth (left) and its curves underneath the TAUC and AUC (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 18 .</head><label>18</label><figDesc>Fig.<ref type="bibr" target="#b17">18</ref>. Prediction of a detector that shows high scores at the boundary of the true drift segment only (left) and its curves underneath the TAUC and AUC (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><figDesc>integration rule =0.<ref type="bibr" target="#b24">25</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 21 .</head><label>21</label><figDesc>Fig.<ref type="bibr" target="#b20">21</ref>. Visualization of a concrete curve used to calculate the TAUC with its TAUC-score. Left figure shows the constructed curve when using the step rule, while the right figure shows the curve when calculating the TAUC using the trapezoidal integration rule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. Visualization of the behaviour of the constructed curve for the TAUC on increasing number of segments k. The left figure shows that the TAUC for the computation with the step rule always remains 0. The right figure shows that the area under the line decreases with increasing k, resulting in a lower TAUC value in case of the trapezoidal integration rule.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by the <rs type="funder">Hightech Agenda Bavaria</rs>. The authors are grateful to <rs type="person">Matthias Burkhardt</rs>, <rs type="person">Fabian Hueber</rs>, <rs type="person">Kai Müller</rs>, and <rs type="person">Ulrich Göhner</rs> for helpful discussions. We also thank the anonymous referees for helpful comments and suggestions.</p><p>Data availability. The generated data and implemented algorithms are implemented in a python package driftbench which is freely available under <ref type="url" target="https://github.com/edgarWolf/driftbench">https://github.com/edgarWolf/  driftbench</ref>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conflict of interests. The authors provide no conflict of interest associated with the content of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Data generation with polynomials</head><p>In this section, we demonstrate the data generation method introduced in Section 3 along an example involving a polynomial f : R 6 × R → R of degree five, i.e. f (w, x) = length that equals the length of the true segment and the position of its center is varied from 50 to 350, AUC and sTAUC coincide mostly, but the sTAUC shows a faster rise when the predicted segment overlaps with the true segment due to the effects explained in Section C.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. TAUC for trivial detector</head><p>To get a better understanding of the TAUC, we showcase the behavior on trivial detectors based on the structure of the ground truth. Suppose two pair of points (FPR i , OLS i ) and (FPR i+1 , OLS i+1 ) of the constructed curve. Then the two methods for computing the TAUC are the following:</p><p>• Trapezoidal rule: Construct the curve by linearly interpolating OLS i and OLS i+1 in between FPR i and FPR i+1 and then calculate the area under the curve by using the trapezoidal integration rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Step rule:</head><p>Construct the curve by filling the values in between FPR i and FPR i+1 with a constant value of OLS i and then calculate the area under the curve by using the step rule. For example, take the trivial detector that always predicts a drift, called AlwaysGuesser. Then we receive the two points (0, 0) and (1, P k ) as the only two points of the curve, where</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intelligent manufacturing in the context of industry 4.0: A review</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="616" to="630" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tool wear monitoring of a retrofitted cnc milling machine using artificial neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Hesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manufacturing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Migration from the traditional to the smart factory in the die-casting industry: Novel process data acquisition and fault detection based on artificial neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Materials Processing Technology</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page">116972</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">7th CIRP Global Web Conference -Towards shifted production value stream patterns through inference of data, models, and technology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kißkalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meiners</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Metzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia CIRP</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="49" to="54" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>CIRPe</publisher>
		</imprint>
	</monogr>
	<note>Machine learning in production -potentials, challenges and exemplary applications</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inline drift detection using monitoring systems and machine learning in selective laser melting</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joffre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Arvieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Le Guen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Engineering Materials</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2020">2000660. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian autoencoders for drift detection in industrial environments</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">X</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brintrup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Workshop on Metrology for Industry 4.0 and IoT</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="627" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine learning based screw drive state detection for unfastening screw connections</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al Assadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Holtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nägele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nitsche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Manufacturing Systems</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="19" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Process curve analysis with machine learning on the example of screw fastening and press-in processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meiners</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia CIRP</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="166" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A benchmark and survey of fully unsupervised concept drift detectors on real-world data streams</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lukats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Characterization of non-linear profiles variations using mixed-effect models and wavelets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Paynabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IIE Transactions</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="290" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning Internal Representations by Error Propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR 2014</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">April 14-16, 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational Autoencoders and Nonlinear ICA: A Unifying Framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="2207" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A variational autoencoder for a semiconductor fault detection model robust to process drift due to incomplete maintenance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Manufacturing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="529" to="540" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Manufacturing process curve monitoring with deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meiners</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manufacturing Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="15" to="18" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reactive soft prototype computing for concept drift streams</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heusinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-M</forename><surname>Schleif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">416</biblScope>
			<biblScope unit="page" from="340" to="351" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hellinger distance based drift detection for nonstationary environments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ditzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Symposium on Computational Intelligence in Dynamic and Uncertain Environments (CIDUE)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">causalAssembly: Generating realistic production data for benchmarking causal discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Göbler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Windisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pychynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonntag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Conference on Causal Learning and Rasoning</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Didelez</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">236</biblScope>
			<biblScope unit="page" from="609" to="642" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1038" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Benchmarking deep reinforcement learning for continuous control</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Milling data set, BEST Lab, UC Berkeley, NASA Prognostics Data Repository, NASA Ames Research Center</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agogino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goebel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smart data collection system for brownfield cnc milling machines: A new benchmark dataset for data-driven machine monitoring</title>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Tnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Diepold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Leading manufacturing systems transformation -Proceedings of the 55th CIRP Conference on Manufacturing Systems</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="131" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Overview of publicly available degradation data sets for tasks within prognostics and health management</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mauthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Huber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The meaning and use of the area under a receiver operating characteristic (ROC) curve</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Mcneil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Anomalieerkennung in hoch-dimensionalen Sensordaten</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024-04">April 2024</date>
		</imprint>
		<respStmt>
			<orgName>University of Applied Sciences, Kempten</orgName>
		</respStmt>
	</monogr>
	<note>master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van-Derplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Concept drift detection based on typicality and eccentricity</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T P</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Guedes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="13795" to="13808" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A wavelet-based penalized mixed-effects decomposition for multichannel profile detection of in-line raman spectroscopy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1258" to="1271" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Brogan</surname></persName>
		</author>
		<title level="m">Modern Control Theory</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Practical Methods of Optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<title level="m">Gaussian Processes in Machine Learning</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Kan: Kolmogorov-arnold networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ruehle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soljačić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.19756</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Computing the two-sided kolmogorov-smirnov distribution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Ecuyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
