<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Insights From Insurance for Fair Machine Learning</title>
				<funder>
					<orgName type="full">International Max Planck Research School for Intelligent Systems</orgName>
				</funder>
				<funder>
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-01-23">23 Jan 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christian</forename><surname>Fröhlich</surname></persName>
							<email>christian.froehlich@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<settlement>Tübingen AI Center</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
							<email>bob.williamson@uni-tuebingen.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<settlement>Tübingen AI Center</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Insights From Insurance for Fair Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-01-23">23 Jan 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">6C938C4DA78D9C6EA7B825712BDC497E</idno>
					<idno type="arXiv">arXiv:2306.14624v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.</p><p>See Baker ( , p.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Insurance is "interestingly uninteresting". In this work, we argue that in fact insurance is far from uninteresting and indeed a rich source of inspiration and insight to scholarship interested in social issues surrounding machine learning, specifically the field now known as fair machine learning. Our proposal is that insurance can be viewed as an analogon to machine learning with respect to these issues arising from the social situatedness. While machine learning is a relatively recent technology, debates regarding social issues in the context of insurance have been ongoing for a long time. Thus, we argue that taking inspiration from studies of insurance can contribute to a more integrative view of machine learning systems as socio-technical systems <ref type="bibr">(Selbst et al., )</ref>.</p><p>Both machine learning and insurance are firmly based on a statistical, probabilistic mode of reasoningan actuarial mode. Indeed, insurance can be viewed as the first commercial test of probability theory <ref type="bibr">(Gigerenzer et al., ; McFall, )</ref>. Insurance, a technology for doing risk, transforms uncertainty into calculable risk <ref type="bibr">(Lehtonen &amp; Van Hoyweghen, )</ref>. The key idea is to share the risk of a loss in a collective, organized through an abstract mutuality; due to the 'law' of large numbers, uncertainty thus becomes manageable and the effect of chance can be offset <ref type="bibr">(Ewald, )</ref>. In this way, insurance creates a "community of fate" in the face of uncertainty <ref type="bibr">(Heimer, )</ref>. To enter into this community (the insurance pool), the insurer demands a certain fee, called premium, from the policyholder.</p><p>In insurance, questions of fairness inevitably arise, and have been the subject of much debate. The central point of debate is the tension between risk assessment and distribution <ref type="bibr">(Abraham, )</ref>. In other words, who is to be mutualized in the pool. Some form of segmentation is found in many insurantial arrangements: the pool of policyholders can be stratified by separating high and low risk individuals. But the specific nature that such segmentation <ref type="bibr">McFall et al. (</ref> ) call insurance "interestingly uninteresting", referring to how insurance is "hugely underresearched" given its societal importance, which is typically not recognized <ref type="bibr">(Ewald, )</ref>.</p><p>takes typically depends not only on risk assessment, but on further considerations such as assignment of responsibility, modulated by social context; in this way, insurance is not a neutral technology <ref type="bibr">(Baker &amp; Simon, ; Glenn, a)</ref>.</p><p>Our non-comprehensive outline of the history of insurance illustrates how uncertainty, fairness and responsibility interact, and can be entangled and disentangled. From this background, we can extract conceptual insights which also apply to machine learning. The tension between risk assessment and distribution is mirrored in formal fairness principles: solidarity, which can be linked to independence in fair machine learning, contrasts with actuarial fairness, linked to calibration. Briefly, actuarial fairness demands that each policyholder should pay only for their own risk, that is, mutualization should occur only between individuals with the same 'true' risk. In contrast, solidarity calls for equal contribution to the pool. On one level of this text, we problematize actuarial fairness (by extension, calibration) as a notion of fairness in the normative sense by taking inspiration from insurance. This perspective is aligned with recent proposals that stress the discrepancy of formal algorithmic fairness and "substantive" fairness <ref type="bibr">(Green, )</ref>, which some prefer to call justice <ref type="bibr">(Vredenburgh, )</ref>. Parallel to this runs a distinct textual level, where we emphasize two intricately interacting themes: responsibility and tensions between aggregate and individual. Both entail criticism of actuarial fairness, but we suggest that they additionally provide much broader, fruitful lessons for machine learning from insurance.</p><p>At the highest level of abstraction, our goal is to establish a general conceptual bridge between insurance and machine learning. Traversing this bridge, machine learning scholars can obtain new perspectives on the social situatedness of a probabilistic, statistical technology -we attempt to offer a new 'cognitive toolkit' for thinking about the social situatedness of machine learning. Our point of view is that fairness cannot be reduced to a formal, mathematical issue, but that it requires taking broader social context into account, reasoning for instance about responsibility. And for this, we suggest, insurance is an insightful analogon. Therefore, our objective is to furnish the reader with a guide that charts the landscape of insurance with respect to social issues and to establish links to machine learning.</p><p>On a formal level, we use the following analogy. In a machine learning task, we are given some features X and associated outcomes Y , which we attempt to approximate by predictions Ŷ . The structural relation to insurance is established by conceiving of X as the features of policyholders (e.g. age, gender) with outcomes Y (e.g. having an accident or not), and the task is to set a corresponding premium Ŷ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Brief History of Insurance Rationalities</head><p>Insurance is not a monolithic technology, but rather a general principle of risk management, which is instantiated in multiple distinct forms. Insurance and the conceptual resources it deploys are not immutable and stable over time, as exemplified by Baker's (</p><p>) study on moral hazard, attesting to their evolving nature. In this section, we provide a succinct, necessarily non-comprehensive description of three historical modes of insurantial operation: the welfare state, neoclassical economics and personalized insurance. Throughout, we focus on the role that uncertainty, fairness and responsibility play in each of the three modes. To each mode we ascribe a set of attitudes towards these three aspects and in what manner they are entangled or disentangled. Fairness conceptions in insurance are contingent upon prevailing societal norms, particularly regarding responsibility, but concurrently insurance shapes the moral fabric of the society in which it is embedded <ref type="bibr">(Glenn, b; Van Hoyweghen et al., ; Lehtonen &amp; Liukko, )</ref>. Furthermore, fairness conceptions in insurance are historically intertwined with their accompanying (statistical) epistemologies, ways of 'knowing' the risk in the face of uncertainty. A common thread is also that distinct forms of insurance correspond to distinct ways of governing society. Importantly, we do not want to suggest a linear historic progression here -different forms of insurance co-exist at any given time. For instance, contemporary health care systems tend to operate with the logic of the welfare state, while actuarial fairness undergirds the private insurance sector. What follows is our synthesis of the literature, particularly drawing on the works of Ewald ( ) and Frezal &amp; Barry (</p><p>), with a discerning focus on elucidating the intricate interplay between uncertainty, fairness, and responsibility. With this background, we are then able to extract conceptual lessons that apply also to machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Broad Solidarity in the Welfare State</head><p>In his seminal work L'Ètat providence, Ewald (</p><p>) gave an influential account of the rise of the welfare state and explicates how insurance became its prime way of government. The point of departure is the predominance of liberal reasoning, which operates with the categories of fault and foresight in risk management. Here, it is presumed that the occurrence of an accident (a damage, loss or injury) must be due to fault, a lack of foresight. Liberal thought held individuals responsible for their own fate and inequalities were naturalized as just consequences of individual responsibility <ref type="bibr">(Landes,</ref> ). An accident implies a trial under the "regime of juridical responsibility" <ref type="bibr">(Ewald, )</ref>, where the goal is trying to establish the fault of one party. Responsibility is then borne by the person who is assigned fault, who is seen as having caused the accident. In turn, voluntary charity is the preferred means of supporting the poor and unlucky. With the rise of industrialization in different countries, an "accident crisis" unfolded roughly between and (Krippner, ): the number of workplace accidents dramatically increased and there appeared a new regularity at the aggregate level, suggesting a kind of determinism in the phenomenon. This led to the objectification of the accident and its management became a question of the collective, the social. By objectification, we understand an act of aggregation combined with the law of large numbers. As a consequence, we find a conception of insurance as broad solidarity and the rise of the welfare states. While at the individual level it was impossible to predict who will suffer an accident, the new regularity observed at the aggregate level could provide an effective pattern of risk management. Since equal ignorance in the fate of uncertainty was emphasized <ref type="bibr">(Ewald, )</ref>, solidarity, implying equal contribution to the pool, was considered fair. Indeed, this conception of insurance was so firmly based on the aggregate that it led Ewald (</p><p>) to assert that Strictly speaking there is no such thing as an individual risk; otherwise insurance would be no more than a wager. Risk only becomes something calculable when it is spread over a population. The work of the insurer is, precisely, to constitute that population by selecting and dividing risks.</p><p>[..] It makes each person a part of the whole.</p><p>The business of insurance, then, was construed as the constitution of abstract mutualities <ref type="bibr">(Lehtonen &amp; Liukko, )</ref> and the sharing of responsibility to counteract the effect of fate.</p><p>The epistemology of the welfare state is one of the aggregate, the collective, the social. In this respect, it is interesting and instructive how insurance became intertwined with probability and statistics. Early forms of insurance were more like gambling, and insurance was often accused of being immoral and faced prohibitions. Insurance gained more legitimacy when it became based on 'objective' probability and statistics <ref type="bibr">(Daston, , p. ff)</ref>; by the end of the nineteenth century, the morality of insurance was established <ref type="bibr">(Baker, )</ref>. The crucial conceptual move was the marriage of probability theory and statistics. While early probabilists were more concerned with reasonable subjective judgment, the nineteenth century shows an increasing shift towards "objective calculation" <ref type="bibr">(McFall, )</ref>. The idea was that in a large insurance pool losses occur randomly, so the law of large numbers applies and the total loss can be predicted at the level of the aggregate. Frequentist ('objective') probability thus combines aggregate regularity with individual irregularity, as explained by <ref type="bibr">Venn ( , p. )</ref>.</p><p>Typically policyholders are unaware with whom they are mutualized, however, so this mutualization does not require a shared sense of groupness <ref type="bibr">(Krippner &amp; Hirschman, )</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cooper &amp; Grinder (</head><p>) provide some examples. In th century London life insurance could be bought on the life of celebrities, without an insurable interest to the policyholder. In fact, the notion of an insurable interest was put forward by the insurers to counter the allegation of gambling <ref type="bibr">(Baker, ; McFall &amp; Moor, )</ref>. Particularly interesting is also insurance in Islamic law, which prohibits gambling and contracts based on usury: the morality of insurance is justified then by emphasizing the solidaristic nature of the arrangement <ref type="bibr">(Baker, )</ref>, in contrast to the view of insurance as a bilateral contract that is more prevalent in Western societies.</p><p>A key player in this development was Adolphe Quetelet, a Belgian astronomer, who initiated the study of "social physics" by attempting to discover natural laws about human behaviour (McFall, ). Quetelet's innovation was the transposition of one sense of the concept average to a different one. Consider first the familiar aggregating sense of average, where a set of commensurate objects is summarized in a single number. A prima facie different sense of average is as the single true value of some measurement problem, from which one can obtain a set of noisy measurements. The radical conceptual move was then to transpose the second to the first sense, thereby viewing the individuals of a population as many realizations of some abstract average human <ref type="bibr">(Ewald, ; McFall, )</ref>. In this way, the rates of birth, death and other social phenomena could be attributed to this fictional average human. In the context of the workplace accident and insurance, in line with the imaginary of the average human, the focus shifted from the unique, individual experience (the object of a juridical trial) to an objectification based on the average occurrence <ref type="bibr">(Krippner, )</ref>. With regard to how this influences the notion of personhood, Dean ( ) writes Insurance practices displace the abstract, invariant norm of a responsible juridical subject with an individuality relative to other members of an insured population, an 'average sociological individuality'.</p><p>Quetelet's fiction of the average human has made an impact on the insurance sector <ref type="bibr">(McFall, )</ref>: identifying the individual with an average is at the core of actuarial practice. Indeed, the question of how individuals relate to the aggregates they make up is, as we suggest in line with Krippner (</p><p>), runs through the history of insurance. Moreover, we argue in Section that it is a major concern for fair machine learning, too.</p><p>Although insurance increasingly relied on probability and statistics, the available quantification methods at the level of the collective severely limited the possibility of actually 'knowing' the risk of an individual <ref type="bibr">(Barry, )</ref>. With improved actuarial methods, new possibilities for segmentation of the pool have opened up, which have led also to the rise of a new fairness notion that is successively contributing to the erosion of solidarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Neoclassical Economics and Actuarial Fairness</head><p>A distinct mode of insurance, which undergirds contemporary (private) insurance, is based on neo-classical economics, which construes individuals as rational expected utility maximizers -the human is viewed as a homo oeconomicus. The assumption in this paradigm is that insurance is purchased due to risk aversity from the perspective of the policyholder, while the insurer is risk neutral. This configuration of the individual is tied to a different notion of fairness which contrasts starkly with the solidarity of the welfare state: actuarial fairness. The idea is that the pure premium (i.e. what the policyholder pays before adding additional expenses such as for administration) should equal the expected risk for each policyholder. While the idea of "equality in risk" was around for a long time <ref type="bibr">(Heras et al., )</ref>, its modern formulation is due to Arrow ( ). On the one hand, actuarial fairness can be understood as a purely descriptive, technical notion; but it is also advanced in the literature and by insurers as a notion of fairness in a normative sense, as legitimate practice, see for instance <ref type="bibr">(Walters, ; Clifford &amp; Iuculano, ; Daniels, ; Stone, ; Thiery &amp; Van Schoubroeck, )</ref>; Interestingly, actuarial fairness can be traced back to the Aristotelian consistency principle "fairness is to treat equal people equally and unequal people unequally" <ref type="bibr">(Landes, )</ref>, in a context where 'equal' means 'equal risk' <ref type="bibr">(Heras et al., )</ref>.</p><p>The definition and especially the Aristotelian motivation betrays that actuarial fairness is in practice always a groupbased notion (Miller, ), since insurers needed (traditionally, at least) to make use of large segments for calculating expected losses. For this, actuaries choose a set of relevant variables, while ignoring others. This seems fundamentally in conflict with the idea of adjustment to the 'individual risk' of the policyholder -and indeed this</p><p>The original term was "average man".</p><p>was not how actuarial fairness was construed until roughly the s, it remained firmly group-based <ref type="bibr">(Barry, )</ref>. In line with Quetelet, Thiery &amp; Van Schoubroeck ( ) describe the logic succinctly as follows:</p><p>[I]nsurance classification schemes rely on the assumption that individuals answer to the average (stereotypical) characteristics of a group to which they belong.</p><p>Hence the justification for group-based actuarial fairness relies on assuming an "average sociological individuality" (Dean, ) -each individual is assigned to a segment and is then identified with the corresponding average <ref type="bibr">(Krippner &amp; Hirschman, )</ref>. In this sense, we find again the logic of the welfare state but now only segmentwise, with the aspiration to reduce solidarity between groups as much as possible, given practical constraints. In the terminology of <ref type="bibr">Lehtonen &amp; Liukko (</ref> ), this means that ideally only chance solidarity is left, which compensates for the effects of aleatoric uncertainty; in contrast, risk subsidizing solidarity refers to a solidarity between individuals of different expected loss.</p><p>The role of responsibility in actuarial fairness is subtle. Actuarial fairness, when understood as a normative principle, rests on the assumption that people can be held responsible for their individual risk to some extent <ref type="bibr">(Lehtonen &amp; Liukko, )</ref>. However, we should distinguish conceptually between responsibility and responsibilization, that is, holding someone responsible for something. While normative philosophical literature may be careful about this distinction, in practice it can appear blurry. In fact, there are two principles that provide nonresponsibility based reasons for responsibilization in insurance (Andersen &amp; Nielsen, ), therefore in favor of actuarial fairness: moral hazard and adverse selection (see Appendix B). Thus, responsibility is, in the neoclassical framework, not central to actuarial fairness <ref type="bibr">(Landes, )</ref>. However, the role of responsibility (more precisely, responsibility-based reasons for responsibilization) in insurance is currently being emphasized more and more. To this we turn now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. The Climax: Personalized Insurance</head><p>From the s on, the insurance industry was increasingly challenged by anti-discrimination legislation. Social movements attacked the average human (woman) logic that insurance based its actuarially 'fair' premia on. A prominent example is the campaign initiated by the National Organization for Women (NOW) <ref type="bibr">(Krippner &amp; Hirschman, ; Krippner, )</ref>, aimed at ending gender-based risk segmentation. In line with the civil rights movement, feminists considered such underwriting (that is, risk classification) practices to be unfairly discriminatory, as they rely on group-based generalizations. Instead, they asked for a finer adjustment to individual risk. What was under attack here is the fundamental group-based logic of actuarial fairness. The US supreme court asserted in the context of insurance that "[e]ven a true generalization about [a] class cannot justify class-based treatment" (Norris, , as cited in Avraham ( )). A more recent case is the ECJ Test-Achats ruling in the EU, which highly restricts gender-based underwriting <ref type="bibr">(Rebert &amp; Van Hoyweghen, ; Cevolini &amp; Esposito, )</ref>. In response to such anti-discrimination legislation and in anticipation of a continuation of this trend, the practical meaning of actuarial fairness gradually began to shift. Consequently we find two highly entangled trends in the contemporary insurance industry: the individualization of risk and behaviour-based personalization <ref type="bibr">(Cevolini &amp; Esposito, ; McFall et al., )</ref>.</p><p>The individualization of risk can be understood as taking group-based actuarial fairness to the limit and (hypothetically) forming 'groups of one'; instead of spreading risk over a pool, each policyholder would pay exactly for her own</p><p>We put aside for now the question of whether the conceptual distinction between aleatoric and epistemic uncertainty is well-defined.</p><p>Lehtonen &amp; Liukko ( ) further mention subsidizing income solidarity, occuring when premia are adjusted based on income; this is more like a tax than 'genuine' insurance solidarity.</p><p>We use the term 'responsibilization' in line with Andersen &amp; Nielsen ( ). The term originally appeared in the governmentality literature and refers to a neoliberal mode of governing that frames individuals as autonomous and responsible, see for example <ref type="bibr">(Shamir, ; Pyysiäinen et al., )</ref>.</p><p>individual risk <ref type="bibr">(Cevolini &amp; Esposito, )</ref>. In terms of solidarity, this implies a complete erosion of subsidizing risk solidarity, so that ideally only chance solidarity remains. Rephrasing this, what is now being emphasized is the non-homogeneity within previous groups of policyholders <ref type="bibr">(Barry, )</ref>. To this end, insurers begin to shift the focus from attributes which are considered uncontrollable (e.g. gender, race) to controllable, dynamic data about the individual, and adjust premia accordingly, yielding behaviour-based personalization. Of course, the hope is that behaviour is closely linked to the individual risk, otherwise personalization would hardly be reasonable; in this way individualization and personalization are correlated.</p><p>Personalization is linked to InsurTech, that is, technology-driven innovation in insurance <ref type="bibr">(McFall et al., )</ref>. A prominent example is the use of wearable devices such as fitness trackers in health insurance <ref type="bibr">(Lupton, ; McFall, )</ref>, where discounts and rewards are supposed to incentivize "healthy behaviour". In car insurance, the use of telematics is gaining popularity <ref type="bibr">(Verbelen et al., ; Meyers &amp; Van Hoyweghen, ; Cevolini &amp; Esposito, )</ref>, where a small device installed in the car dynamically provides information about driving behaviour from proxy variables such as speed to the insurer, as well as feedback to the policyholder. Here, the premium is continuously adjusted to behaviour, which contrasts with previous static underwriting. In both examples, the premium is supposed to act on the behaviour with the aim of loss prevention <ref type="bibr">(McFall &amp; Moor, )</ref>; this opens up the possibility for feedback loops. In other words, the premium is performative -see Appendix D.</p><p>The accompanying epistemology associated with the shift towards individualization and personalization is the aim to tailor the premium to the 'individual risk' and it is assumed that a combination of big data (often behavioural, with a fine temporal resolution) and machine learning enables 'knowing' this risk <ref type="bibr">(Cevolini &amp; Esposito, )</ref>. Hence, in this currently unfolding chapter, machine learning enters into a dynamic interaction with insurance; we expect conceptual lessons to flow in both directions in the future (compare also (Williamson,</p><p>)); in this paper, however, we specifically focus on lessons from insurance for machine learning. Barry (</p><p>) describes the new epistemology as follows:</p><p>Hence what was once considered as 'noise,' the individual specificities that had to be averaged out by statistics, is now the core of the analysis and the focus of the new knowledge.</p><p>The upshot, according to <ref type="bibr">Barry (</ref> ), is the "deconstruction of the aggregate viewpoint that produced collectives". Commentators speak of emerging "segments of one" <ref type="bibr">(Prainsack &amp; Van Hoyweghen, )</ref>.</p><p>The individualization and personalization of risk is associated with a shift in fairness: actuarial fairness is taken to the limit and now clearly carries a normative flavour based on a linkage to responsibility. We call this utopia of individual risk adjustment perfect actuarial fairness, to demarcate it from practical, group-based actuarial fairness. Reviving pre-Welfare liberal thought, individual responsibility is stressed <ref type="bibr">(Dean, )</ref>. For example, Ericson et al. ( ) document a shift in the concept of accident; the new rhetoric, speaking of a "crash" in the case of a car accident, underscores that someone must be at fault and thus responsible. Without taking a philosophical stance on actual responsibility, the trend is one of the responsibilization of the individual. Even in the context of health insurance, individuals face such responsibilization <ref type="bibr">(Van Hoyweghen et al., ; Prainsack &amp; Van Hoyweghen, )</ref>. For example, selftracking favors a view of individuals as "managers" of their health <ref type="bibr">(Lupton, ; Sharon, )</ref>. Overall, responsibilization is linked to neoliberal modes of government <ref type="bibr">(Dean, ; Ericson et al., ; Meyers &amp; Van Hoyweghen, )</ref>.</p><p>As a consequence, many commentators argue that personalization undermines solidarity <ref type="bibr">(</ref>Rosanvallon, ; Prainsack &amp; Van Hoyweghen, ; Barry, ; Cevolini &amp; Esposito, ); For instance, Swedloff ( ) claims that big data is in contradiction to the risk-spreading mechanism of insurance and Heras et al. (</p><p>) observe that, when taken to the extreme, actuarial fairness contradicts the very logic of insurance. When risk spreading disappears, insurance becomes more like personal saving. With respect to distributive consequences, the individualization of risk has been "profoundly inegalitarian" <ref type="bibr">(Armstrong, )</ref>. The highest-at-risk individuals can even face exclusion from the pool <ref type="bibr">(Lehtonen &amp; Liukko, ; Cevolini &amp; Esposito, )</ref>.</p><p>In summary, we have described three broad modes of insurance and their associated attitudes towards uncertainty, fairness and responsibility. We now investigate multiple dimensions of responsibility and then establish a link to fair machine learning, where we argue that reflections on responsibility should be foregrounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Responsibility</head><p>Insurance actively constructs and distributes responsibility <ref type="bibr">(Baker, )</ref>; the boundaries of individual responsibility are drawn by accounting for some factors but not for others in the premium. Adding to the terminology of Landes &amp; Holtug ( ), we distinguish four dimensions of responsibility: causal (who is causally responsible for the accident?), control-based (who could control the happening?), moral (who is normatively responsible?) and material (who bears the consequences?). By responsibilization we understand holding individuals responsible, with an emphasis on the material dimension, based on narratives about causal, control-based and moral dimensions of some phenomenon. What is distinctive about insurance as a technology of risk management is the tendency to separate these dimensions <ref type="bibr">(Landes &amp; Holtug, )</ref>. However, we have observed a recent trend towards a renewed entanglement when compared to the mode of the welfare state. In particular, the notion of actuarial fairness has been increasingly linked to responsibility in contrast to mere non-responsibility based responsibilization.</p><p>What is intriguing is also how the entanglement of uncertainty and responsibility has changed historically. A thick veil of ignorance, when the individual level remains out of reach, appears to favor a collective responsibility for risk management; when this veil is gradually lifted, it seems easier to assign responsibility to individuals <ref type="bibr">(Frezal &amp; Barry, ; Barry, )</ref>. However, this is not a necessity: 'knowing' individual risk (to some extent) does not necessarily imply that individuals are morally responsible or that we should hold them materially responsible (see Section . ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Causality and Control</head><p>Causality has received major attention in recent machine learning research and is also widely discussed in the literature on insurance. A highly related, but subtly distinct notion is that of control -indeed, the recent trend of responsibilization can to some extent be explained as a response to legislation that prohibits differentiating premia based on variables beyond individual control <ref type="bibr">(Meyers &amp; Van Hoyweghen, )</ref>. While actuarial practice is correlation-based, causality and control are emphasized by legal commentators on insurance discrimination (see e.g. <ref type="bibr">(Abraham,</ref> ; Gaulding, ; Avraham et al., ; Avraham, )). For example, Avraham ( ) demands that a variable used for calculating premia must be both causally linked to the outcome of interest and within individual control. Why demand a causal relationship? If a variable is merely non-causally correlated with the outcome, then it is a proxy for another variable, which should be used in its place in order to avoid differential inaccuracy <ref type="bibr">(Avraham et al.,</ref> ).</p><p>Another argument is that the importance of causality is derived from control, and that it is control which is at the heart of many controversies. To fix a rough notion of causality, we understand "X causes Y " as "intervening on X changes the probability for Y ", where an intervention means changing the value. In this way, we can view causality as hypothetical control. Further, if an individual can actually intervene on X, then we may say simply that X is under control of the individual. Observe that "X causes Y " is thus a necessary but not sufficient condition for an individual's ability to control Y by controlling X. The importance of control in turn derives from responsibility: how could an individual be responsible (in the moral and perhaps in the material sense) for a variable which is beyond control (Abraham, ; Avraham, )? Mere causality seems insufficient for this. Hence, we view causality as the conceptual entry point to get at control. Moreover, control is key for downstream effects, as we discuss in Appendix D. From a normative perspective on responsibility, the importance of control (or more precisely, choice) is emphasized in theories of luck egalitarianism, often discussed as a justification of risk classification <ref type="bibr">(Knight,</ref> ; Lippert-Rasmussen, ; Huseby, ; <ref type="bibr">Björk et al., )</ref>.</p><p>Conversely, control is arguably not sufficient for responsibility. For instance, control without causality can yield 'discrimination by proxy': in Swedloff's ( ) hypothetical example, liking Vampire novels (arguably within control) is correlated with risky behaviour, but in a non-causal way. This suggests that here a controllable variable works as a proxy for a potentially non-controllable one such as gender. To further complicate matters, the argument by Hu &amp; Kohler-Hausmann (</p><p>) demonstrates that a conflict may arise when a seemingly controllable stands to a non-controllable variable such as gender in a constitutive relation; responsibilizing for the controllable variable then effectively leads to responsibilization for the non-controllable one, too.</p><p>In the context of (fair) machine learning (see Section ), causality has received much attention, but due to the previous considerations we suggest putting reflections on control-based responsibility at the center. This also implies shifting the focus to downstream ('performative') effects of deploying a machine learning system, since consequences of responsibilization are linked to control (see Appendix D). Problematically, however, we must answer the question of what is under control. We now argue, in line with other social studies of insurance scholars <ref type="bibr">(Abraham, ; Gaulding, )</ref>, that the variant of this question which is relevant for insurance and machine learning purposes is in fact fundamentally normative in character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Social Contingencies in Responsibilization: What Is Under Control?</head><p>The distinction of control vs. no-control plays a major role in justifying the responsibilization of individuals, and thus actuarial fairness in the mode of personalization. Here we illustrate with examples from insurance that this dichotomy is a slippery one, however, and thus provides only a shaky basis for actuarial fairness. The question of whether a variable is within individual control is often unclear and in fact insurance scholars have argued that it is a normative question <ref type="bibr">(Abraham, ; Gaulding, )</ref>, not a descriptive one. Our sketch of an argument is as follows.</p><p>An instructive example is risk classification based on smoking in life insurance. It was only in the s that life insurance companies widely started to differentiate premia with respect to smoking, even though it was already known in the s that the associated difference in lifespan is substantial -the decision not to responsibilize depended on the social acceptability of smoking (Wilkie, ; Glenn, b). Today, 'lifestyle'-based responsibilization increasingly plays a role in insurance and smoking is considered a prime example for a variable within individual control <ref type="bibr">(Van Hoyweghen et al., ;</ref> ). We should thus pay close attention to shifting societal narratives about control and responsibility. A contrasting example is the case of HIV in underwriting practices, which Daniels ( ) explicitly contrasts with the (previous) neglect of smoking as a rating variable. Daniels (</p><p>) discusses a widespread denial of health insurance coverage for individuals with HIV with a justification based on actuarial fairness. However, Daniels (</p><p>) suspects that homophobia and social antipathy to drug users may play a role in this, favoring responsibilization for risky 'lifestyle', conceived as controllable.</p><p>As an example, genes might be causally related to some outcome Y of interest (hypothetically controlling genes would change the probability for Y ), but are not actually under control of the individual.</p><p>For constitutive relations, see also the discussion on performativity in Appendix D.</p><p>Recall that in the neoclassical mode, the question of control is disregarded, but it is emphasized in the mode of personalization.</p><p>Underwriting based on 'lifestyle' risks can be most starkly contrasted with the use of genetic information in health and life insurance, which is now tightly regulated in some countries (Van Hoyweghen, ) albeit welcomed by the industry <ref type="bibr">(Rechfeld, )</ref>. The notion that "we are all carriers of genes" has successfully invoked a solidaristic imaginary in this context <ref type="bibr">(Van Hoyweghen et al.,</ref> ): many argue that nobody should be penalized for their genes, since they are clearly beyond individual control. Here, a 'genetic veil of ignorance' is mobilized in the debate. This has even given rise to the paradigm of genetic exceptionalism, holding that genetic information is normatively distinct from other medical information (for a critique see (Lippert-Rasmussen,</p><p>)). In the context of genetic information, solidarity is thus emphasized <ref type="bibr">(Liukko, )</ref>, but this has on the other hand contributed to a responsibilization of 'lifestyle' risk which continues to justify actuarial fairness (Van Hoyweghen,</p><p>).</p><p>Do the previous cases really show that the question of control is a normative one? We are not opposed to the idea that there exists a prior, descriptive question of control: considering all possible actions that an individual can embark on, does one of them intervene on X? In this way, it seems reasonable to say that for instance driving behaviour, but not genes, are controllable. This, however, misses the point as it is not the relevant question in the context of responsibility. Control-relevant actions will bring about different consequences for the individual, and what is more, those consequences will differ among individuals. To give up smoking might give more negative utility to an individual with genetic dispositions that favor addictive behaviour. To move to a less earthquake-prone area, in order to lower one's insurance premium, might require investing a large amount of one's resources. Reasoning about the control dimension of responsibility then amounts to setting the boundaries of which actions we may justifiably demand from an individual, and hence becomes normative in character, in effect a matter of distributive justice. Acknowledging the normative element in questions of control offers a new lens on the fairness of actuarial fairness, demonstrating that actuarial calculations are not as 'objective and neutral' as they are promoted, a point which we further develop in Section D. While our examples are from insurance, we want to transport conceptual insights to machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Link to Fair Machine Learning</head><p>In recent years, as machine learning is increasingly being deployed in sensitive domains, the field of fair machine learning has flourished <ref type="bibr">(Barocas et al., ; Mitchell et al., ; Mehrabi et al., ; Castelnovo et al., )</ref>. Different mathematical formalizations of fairness have been proposed in the literature (see e.g. <ref type="bibr">(Barocas et al.,</ref> )); we here focus on statistical, group-based definitions. As is common in the literature we fix a probability space, for simplicity assumed finite, where we define the following random variables: X ∈ X represents the features of the individuals under consideration, Y represents the true outcomes associated with those individuals, Ŷ represents the predictions generated by our model, and S ∈ S represents a 'sensitive feature' related to the individuals. We assume that S can be perfectly predicted from X, e.g. X = ( X, S) for some X. For simplicity, we assume binary Y ∈ {0, 1} and probabilistic scores Ŷ ∈ [0, 1]. For example, in a credit lending scenario, X contains features such as age, income etc., S could represent "having migrant background" in a binary way, Y indicates whether an individual defaulted or not and Ŷ ∈ [0, 1] represents the probabilistic prediction of the model. Group-fairness definitions are often based on binary decisions, but for the analogy with insurance we use probabilistic scores: we intuitively think of Y as representing the true outcome (an accident, damage or loss) and Ŷ as representing the premium that the insurer (by analogy, the ML engineer) demands for shouldering the risk of the uncertain outcome Y . By ⊥ ⊥ we denote statistical independence of random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition . . A model satisfies independence</head><formula xml:id="formula_0">if Ŷ ⊥ ⊥ S.</formula><p>If, for instance, S represents migrant background, independence demands that the distribution of scores is the same for people with and without migrant background. Independence starkly contrasts with calibration.</p><p>Even granting that such an expression is sensible. In the fair machine learning literature, a sensitive feature relates to membership in a socially salient group, for instance based on gender, race or religion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition . . A model satisfies calibration by groups with respect to</head><formula xml:id="formula_1">S if E[Y | S = s, Ŷ = ŷ] = ŷ, ∀s ∈ S ∀ŷ ∈ [0, 1].</formula><p>As a fairness criterion, calibration embodies the aim of matching our probabilistic predictions well to the true outcomes. If our model predicts a probability of p% for defaulting, then indeed p% should default if our model is adequate. Recently, Höltgen &amp; Williamson (</p><p>) have demonstrated that in fact calibration is a richer notion than what is captured by the traditional definition. While they focus on the case of finite data, we present a corresponding theoretical definition. Assume again some set of groups G ⊆ 2 X . Calibration can then be defined based on this choice of groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition . . A model satisfies (theoretical) calibration with respect to a set of groups G if:</head><formula xml:id="formula_2">∀G ∈ G : E[( Ŷ (X) -Y )|X ∈ G] = 0.</formula><p>While this looks deceptively similar to the recently popularized multi-calibration, it abstracts away from the prediction-based binning, which is partly due to historical reasons <ref type="bibr">(Höltgen &amp; Williamson, )</ref>. To gain intuition, it is instructive to consider what this means in the case of finite data. Assume a finite dataset (X 1 , Y 1 ), .., (X n , Y n ) with associated predictions Ŷ (X 1 ), .., Ŷ (X n ). A set of groups is then equivalent to choosing a subset of the data, i.e. a set G ⊆ 2 {1,..,n} . If we use the empirical distribution associated with this dataset in Definition . , we obtain the following empirical variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition . . A model satisfies (empirical) calibration with respect to a set of groups G if:</head><formula xml:id="formula_3">∀G ∈ G : 1 |G| i∈G ( Ŷ (X i ) -Y i ) = 0.</formula><p>This offers an insurantial interpretation: Ŷ (X i ) is the premium we demand for shouldering the risk of the uncertain Y i . Indeed, calibration is formally reminiscent of the subjective betting interpretation for probability theory proposed by de Finetti ( / ). There, the expectation E[Y ] is viewed as the fair betting price for a gamble Y . Calibration is however not tied to a subjective or objective interpretation of probability, but a criterion for model evaluation.</p><p>Actuarial fairness and calibration are in close correspondence. For a given set of groups G ⊂ 2 X , which we assume forms a partition of X , we define the actuarially fair predictor Ŷaf (x</p><formula xml:id="formula_4">) := E[Y |X ∈ G x ],</formula><p>where G x is the unique group that contains x. The goal of actuarial fairness is to make this partition as fine as possible (cf. Section . , . ).</p><p>Consider first the extreme choice of a single group as the whole population in Definition . . Then calibration in the theoretical (respectively, empirical) case demands that</p><formula xml:id="formula_5">E[ Ŷ (X) -Y ] = 0, 1 n n i=1 ( Ŷ (X i ) -Y i ) = 0,</formula><p>which is called global balance in insurance <ref type="bibr">(Denuit et al., )</ref>; intuitively, we need to collect sufficient premia Ŷ (X i ) to cover all claims Y i . In this case of a single group, calling Ŷaf 'actuarially fair' is some abuse of naming, since in this case it corresponds to full solidarity.</p><p>Proposition . . Given a partition G ⊂ 2 X of X , the actuarially fair predictor Ŷaf satisfies (theoretical) calibration with respect to G, and is furthermore the coarsest calibrated predictor in the sense that any other predictor which is calibrated with respect to G either coincides with it or is not group-wise constant.</p><p>Accordingly, probability refers to indicator gambles, that is, indicator functions of events.</p><p>The trivial proof is in Appendix C. If we had access to the true outcomes Y , we could use them for the finest, calibrated predictions. In this way, calibration is still a coarser criterion as it is based on the expectation, the theoretical average, and thus aligned with actuarial fairness; perfect accuracy is in general not demanded. In the extreme, making the partition finer and finer we reach segments of one. Calibration then demands that Ŷ (x) = E[Y |X = x], which we called perfect actuarial fairness. In this way, we obtain a 'spectrum of calibration', where refining the choice of groups interpolates between two extremes. Group-based actuarial fairness attempts to approximate the extreme of segments of one given practical constraints. Hence, actuarial fairness is well-aligned with fairness-unaware machine learning, where the goal is to approximate the conditional expectation E[Y |X] as closely as possible. In the limit, the distinction between group-based approaches to fairness and individual fairness (in the sense of the machine learning literature <ref type="bibr">(Dwork et al.,</ref> )) then becomes blurry: perfect actuarial fairness corresponds to the notion of individual merit <ref type="bibr">(Joseph et al.,</ref> ), but in line with Binns ( ) we argue in Section that this does not yield actually 'individual' fairness.</p><p>Actuarial fairness is closely related to calibration due to Proposition . ; however finer predictors (e.g. the perfect predictor) satisfy calibration, as well. For a precise conceptual correspondence with perfect actuarial fairness, we could consider the following class of fairness measures, inspired and slightly generalized from Räz (</p><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition . . A fairness measure is probabilistically conservative if it is necessarily satisfied by the perfect actuarially fair predictor</head><formula xml:id="formula_6">Ŷaf (x) = E[Y |X = x].</formula><p>For simplicity and due to the insurantial interpretation, we focus on calibration.</p><p>Not only for calibration, but similarly for independence (Definition . ) the choice of grouping is crucial. At the one extreme, choosing S to be the indicator of the whole population, independence becomes vacuous as it is trivially satisfied. In contrast, we can add more and more groups (sensitive features) for which we demand independence, so that less and less variations in Ŷ are allowed. In the limit, then, we reach full solidarity with a constant Ŷ (see Appendix C. ). Contrasting independence and calibration, the question is whether we allow the prediction Ŷ (the premium) to be sensitive to a certain group or not -where the within-group variation is however neglected.</p><p>Calibration and independence can be mapped onto responsibilization vs. non-responsibilization. The fairness notion that is embodied by calibration is that of accurately reflecting the 'true' probabilities, that is, holding individuals responsible for their risk. In contrast, independence works to decouple predictions from 'true' probabilities to some extent and thus can be viewed as non-responsibilizing -independence is similar, albeit not equivalent, to affirmative action <ref type="bibr">(Räz, )</ref>. As a consequence, we expect different performative effects (see Appendix D), i.e. downstream effects, when applying calibration vs. independence.</p><p>For practice, a simple suggestion is as follows. Since calibration comes practically for free for loss-minimizing predictive models <ref type="bibr">(Barocas et al., , p</ref>. f), we may focus on demanding certain independence relationships. Assume that we have designated a subset of features X R which we aim to responsibilize for, and a set of features X N R which we aim not to responsibilize for. Hence X = (X R , X N R , X other ). Conditional independence <ref type="bibr">(Castelnovo et al.,</ref> ) then demands that</p><formula xml:id="formula_7">Ŷ ⊥ ⊥ X N R | X R</formula><p>The features X other , on which we withhold judgement, can then be used by the model in a way restricted by the conditional independence. In the spirit of Section . , however, the choice of features for (non)responsibilization should be the outcome of a reflexive process of inquiry.</p><p>Beyond calibration and independence, other proposals have been put forward in the machine learning literature, which may also be linked to (non)responsibilization; hence the lessons from insurance can be applied, too. For instance, within an equality of opportunity framework,</p><p>Heidari et al. ( ) suggest splitting the whole set of features Räz ( ) defines a fairness measure as conservative if it is necessarily satisfied by the perfect predictor Ŷ = Y .</p><p>into a set of "accountability" features and "irrelevant" features. From our perspective, this maps onto responsibilization and non-responsibilization. As another prominent example, in a causal fairness framework, Kilbertus et al. ( ) assume that a set of "resolving variables" is given, which are influenced by a sensitive feature in a way that it considered "non-discriminatory"; but the authors do not provide guidance on how to choose them. We believe that the lessons from insurance about causality and control (Section . ), and more broadly on responsibility in general, can on the one hand guide the selection of such features. On the other hand, we have seen that causal and control-based dimensions of responsibility are highly sensitive to social context (Section . ). This highlights the normative element in making such a distinction (responsibilizing or not), which can be problematic and must be recognized as such. The choice can be side stepped by favoring solidarity over actuarial fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tensions between Aggregate and Individuals</head><p>Throughout the history of insurance, and also highly relevant to machine learning, we find tensions between aggregate and individual. The mode of the welfare states operates with the imaginary of a collective, in which the individual is mutualized in solidarity. This aggregate viewpoint, where an individual is always identified with the average of some group, finds continuity in group-based actuarial fairness <ref type="bibr">(Thiery &amp; Van Schoubroeck, )</ref> -consistent with Quetelet's average human. Social movements, however, argued that the group-based actuarially fair price is not fair from the viewpoint of the individiual -the desire was to "navigate the social world unmarked by the social stereotypes (fashioned by actuarial science as 'objective' statistical classifications) [..]" <ref type="bibr">(Krippner, , emphasis in original)</ref>. This critique has prompted a shift in the enactment of actuarial fairness <ref type="bibr">(Meyers &amp; Van Hoyweghen, )</ref>, giving rise to the individualization and personalization of risk and thereby threatening (risk subsidizing) solidarity by dissolving the aggregate. At the heart of this aggregate vs. individual tension is the multifaceted concept of responsibility: what links the individual to the aggregate is mutualization based on establishing shared responsibility. Personhood, being an individual, is deeply intertwined with assigning responsibility, as insurance scholars have pointed out <ref type="bibr">(McFall &amp; Moor, ; Moor &amp; Lury, )</ref>. It is never the whole of a person that a premium is attached to in insurance, but specific, contextually relevant aspects <ref type="bibr">(McFall &amp; Moor, )</ref>, thereby transforming a person into an insurance risk <ref type="bibr">(Van Hoyweghen, ; McFall &amp; Moor, )</ref>. It would be intriguing to explore how personhood is negotiated within machine learning systems, drawing parallels with similar studies in insurance (e.g. <ref type="bibr">(Tanninen, )</ref>).</p><p>The mode of machine learning is a paradoxical one: on the one hand, it fits with the mode of individualization and personalization. The goal is to provide highly tailored predictions for the individual. On the other hand, aggregates are central to the workings of machine learning: they appear in the input data due to categorization processes; second, the fairness of machine learning systems is typically evaluated based on groups (with the exception of individual fairness, see below); third, machine learning in general, whether fairness-unaware or not, rests on aggregate criteria such as average training error. We suggest that a large share of the social worries and issues surrounding machine learning can be understood by framing them in the context of the aggregate vs. individual tension.</p><p>Group-based actuarial fairness, which relies on historical data aggregated by groups, is prone to reproduce past injustice <ref type="bibr">(Daniels, ; Lehtonen &amp; Liukko, )</ref>, see also Appendix D. In contrast, the allure of perfect actuarial fairness associated with the personalization of risk, driven by big data and machine learning, is that it is supposedly individually fair -the goal being 'segments of one' and setting the premium as E[Y |X = x]. However, we contend that this elusive goal cannot be reached. The core issue lies with the 'hidden collective'. The working of a neural network is similarity-based computation, arguably interpolation <ref type="bibr">(Hasson et al., )</ref>. Predictions are invariably grounded in data from individuals similar to you, where the similarity is with respect to the opaque nonlinear character of the network. This argument has been made in the context of insurance: 'individualized' risk is still relative to the other members of the collective <ref type="bibr">(Tanninen, ; Prainsack &amp; Van Hoyweghen, )</ref>. Yet individual justice in an Aristotelian tradition requires treating people as individuals <ref type="bibr">(Thiery &amp; Van Schoubroeck, ; Jorgensen, )</ref>, not based on the data of others. For the same reason, what is called individual fairness in machine learning fails to be genuinely individual, as pointed out by Binns (</p><p>). Problematically the collective is implicit, hidden, in the mode of personalization; without transparency and explainability, individuals cannot recognize their own context. Insurance scholars have also argued that this diminishes opportunity for collective action <ref type="bibr">(Moor &amp; Lury, ; McFall &amp; Moor, ; Krippner &amp; Hirschman, )</ref> -the study of collective action in machine learning has just begun <ref type="bibr">(Hardt et al., )</ref>.</p><p>Another way of framing this consists in problematizing the conceptual foundation of probability and statistics itself. Besides the subjective variant of probability, which we consider unfit for decision making affecting people, probability and statistics are fundamentally based on aggregates <ref type="bibr">(Desrosières, )</ref>. Currently, no viable concept of individual probability is available <ref type="bibr">(Dawid, )</ref>; instead, probability relies on a reference class <ref type="bibr">(Reichenbach, ; Hájek, )</ref>. While multi-calibration aims at finer aggregates, it is still not individual <ref type="bibr">(Dawid, )</ref>. Thus, even speaking of an 'individual probability', which perfect actuarial fairness aims at, has no sound conceptual basis. In fact, Friedman (</p><p>, Chapter ) provides an insightful account for the close link of frequentist probability and insurance in the solidaristic mode of the welfare state. This account invites us to consider a reference class as a class of solidarity, which implies disregarding the quest for the single 'right' reference class and instead recognizing the normative element in this choice. Furthermore, randomness can then also be viewed as a normative assumption in the face of uncertainty, establishing shared responsibility: "anyone of us could have had the accident". Thus, the kind of data that Venn (</p><p>) has in mind, combining aggregate regularity with local irregularity (randomness), corresponds to the prerequisites for insurance. As a consequence, we find normative character in frequentist probability itself and a link to aggregate-based solidarity. Attempting to individualize frequentist probability then raises a paradox. In the context of insurance, Frezal &amp; Barry (</p><p>) have argued that the actuarially fair expected value is only adequate from the economic, aggregate viewpoint of the insurer, but conceptually inadequate (and hence in particular not necessarily 'fair') for the individual (see also <ref type="bibr">(Frezal, )</ref>). Or, in the words of Abraham ( ): "No one has a true expected loss". When taken to the limit, actuarial fairness thus undermines the logic of insurance itself <ref type="bibr">(Heras et al.,</ref> ). In the context of machine learning, we argue that it is problematic to evoke the idea of individual probability (risk), particularly when stakes are high such as in the COMPAS case <ref type="bibr">(Angwin et al.,</ref> ), even when individual probability is beyond reach and has no conceptual foundation to rest on. We thus encourage more modesty about the epistemic potential of machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Conceptual literature at the intersection of fair machine learning and insurance is sparse. Donahue &amp; Barocas ( ) study the problem of externalities of size by taking inspiration from insurance and challenge the clear distinction between actuarial fairness and solidarity as a consequence. Loi &amp; Christen (</p><p>) study the interplay of machine learning and (non)discrimination in an insurance setting from a normative, philosophical perspective. Frees &amp; Huang (</p><p>) and Charpentier ( ) provide broad overviews on discrimination in insurance and consider implications of using machine learning. Xin &amp; Huang (</p><p>) link formal fairness definitions to insurance on a technical level. The closest work to ours that we are aware of is by <ref type="bibr">Barry &amp; Charpentier (</ref> ), who investigate how the use of machine learning in insurance is related to classical fairness debates, but they set other foci. A general difference from ours to related work is that we do not study the use of machine learning in insurance, but are interested in a more abstract conceptual linkage. We also note that Frezal &amp; Barry's ( ) critique of actuarial fairness was a major source of inspiration to us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The main claim of our work is that insurance is an insightful analogon for the social situatedness and impact of machine learning systems. By traversing this conceptual bridge, machine learning scholars can make use of the rich and interdisciplinary literature on insurance. In particular, we suggest that the multifaceted concept of responsibility, tightly linked to causality and control, deserves more attention. We have illustrated problems with actuarial fairness as a notion of fairness in the normative sense. In this way, our suggestions are in line with others who demand moving beyond formal fairness to substantive fairness <ref type="bibr">(Green, )</ref> and argue that accurate predictive models need not be 'fair' <ref type="bibr">(Eidelson, )</ref>.</p><p>While in this text we have focused on social issues, there are also technical lessons that machine learning could take from insurance, a technology for handling uncertainty. For instance, the problems of dataset shift and model ambiguity have been recognized in insurance as well as machine learning; for contributions from insurance see e.g. <ref type="bibr">(Milevsky et al., ; Cabantous, ; Pichler, ; Dietz &amp; Niehörster, )</ref>. On the other hand, the use of machine learning in insurance is increasing. We thus believe that a research agenda linking machine and learning and insurance may lead to a fruitful, two-way interaction of these fields.</p><p>In summary, we offer the following insights. Recent impossibility theorems in the fair machine learning literature <ref type="bibr">(Kleinberg et al., )</ref> are not as surprising when considering them in the light of the old, fundamental tension in insurance between solidarity and actuarial fairness. In essence, this tension is grounded in how individuals are related to the aggregates they form. This relation rests on responsibilization. Responsibility and responsibilization should be conceptually distinguished, even if in the recent mode of personalized insurance (tightly linked to machine learning) the two are increasingly intertwined. For insurance and machine learning purposes, reasoning about responsibility crucially requires reasoning about causality and control. We have emphasized that the relevant control question has a normative flavour, and thus cannot be left to engineers alone. What is under individuals' control is often hotly contested. As a general research heuristic, many case studies by social scholars of insurance can inspire analogous studies in the context of machine learning, covering a diverse set of topics; mining the literature is thus a rich source of inspiration. David Wilkie. Mutuality and solidarity: assessing risks and sharing losses. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, ( ): -, .</p><p>Jon Williamson. A dynamic interaction between machine learning and the philosophy of science. Minds and Machines, ( ): -, .</p><p>Xi Xin and Fei Huang. Antidiscrimination insurance pricing: Regulations, fairness criteria, and models. North American Actuarial Journal, pp. -, .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix B Non-Responsibility Based Reasons for Responsibilization</head><p>The difference between and responsibilization can be subtle. In insurance, two classical principles supply reasons for justifying responsibilization without being based on responsibility, however. We might also call them "efficiency-based" reasons for responsibilization (Andersen &amp; Nielsen, ).</p><p>Adverse selection <ref type="bibr">(George A., ; Thiery &amp; Van Schoubroeck, ; Avraham et al., )</ref> refers to the informational asymmetry between insurer and insured (policyholder) at the time of underwriting. Typically, the insured is better informed about their risk; higher-risk individuals are more likely to seek insurance for protection. The reasoning is then that without segmentation, insurers are more likely to attract high-risk individuals who profit from the subsidy of the pool; ultimately, leading to bankruptcy of the insurer. Thus competition drives increasing segmentation. Public insurance has the benefit of compulsory participation, so that adverse selection cannot occur and solidarity can be implemented. In contrast, adverse selection is advanced by the industry as a justification for actuarial fairness <ref type="bibr">(Miller, )</ref>.</p><p>Moral hazard (Heimer, ; Baker, ; ) (in the context of insurance) on the other hand refers to a performative, behaviour-shaping aspect of insurance premia. Simply put, the idea is that policyholders are more inclined to behave in a risky way due to the protection offered by insurance coverage. For instance, there is less incentive to purchase precautionary measures such as alarm systems. Like adverse selection, moral hazard is invoked as an argument in favor of actuarial fairness. The welfare state is seen as the ultimate source of moral hazard <ref type="bibr">(Ericson et al., )</ref>. For instance, public health insurance might lead to more visits to the doctor. Indeed, moral hazard is the responsibilization force in the neoliberal era <ref type="bibr">(Ericson et al., )</ref>. In particular, moral hazard favors behaviourbased personalization <ref type="bibr">(Verbelen et al., )</ref>. Insurance companies thus increasingly emphasize loss prevention, that is, acting on behaviour <ref type="bibr">(Baker &amp; Simon, ; Cevolini &amp; Esposito, )</ref>. Note that the applicability of moral hazard presupposes control for a feedback loop to exist.</p><p>Both adverse selection and moral hazard concern the performative dimensions of "calculative devices" (Van Hoyweghen,</p><p>). Hence we suggest that these concepts might be usefully transposed onto machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Proposition .</head><p>Proof. Plugging in the definition of the actuarially fair predictor, we find that ∀G ∈ G:</p><formula xml:id="formula_8">E[ Ŷ (X)|X ∈ G] = E[Y |X ∈ G] ( ) ⇔E[(x → E[Y |X ∈ G x ])(X)|X ∈ G] = E[Y |X ∈ G] ( ) ⇔E[Y |X ∈ G] = E[Y |X ∈ G],</formula><p>( )</p><p>Noting the negative connotation in the term moral hazard, Baker ( ) asks: "What, after all, is wrong with enabling people to go to the doctor when they feel the need, and why should we be concerned when they do so?" which is true. For the second statement, observe that Equation implies that a predictor Ŷ which is constant on each group G must equal the conditional expectation for that group.</p><p>C. Independence with respect to all groups implies full solidarity Proposition C. . Assume independence holds with respect to all groups, that is</p><formula xml:id="formula_9">Ŷ ⊥ ⊥ χ A , ∀A ⊆ Ω, A = ∅.</formula><p>Then ∀ω ∈ Ω : P ({ω}) &gt; 0 : Ŷ (x) = c for some c ∈ R.</p><p>Proof. Recall that we assume a finite Ω. Then we can assume without loss of generality that P ({ω}) &gt; 0 ∀ω ∈ Ω, otherwise we could work on an altered probability space by discarding sets of measure zero. From the independence assumption it follows that P ({ω : Ŷ (ω) = y|A) = P ({ω : Ŷ (ω) = y) for any y ∈ R and A ⊆ Ω, A = ∅. Pick any ω 1 so that Ŷ (ω 1 ) = y 1 . Then it must hold P ({ω : Ŷ (ω) = y 1 |{ω 1 }) = 1 = P ({ω : Ŷ (ω) = y 1 }), from which we conclude that Ŷ must be constant on Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Performativity</head><p>A central and unifying theme that emerges when considering social issues surrounding insurance, and as we contend, also machine learning, is that of performativity. This concept allows us to comprehend many of the previously raised points through a new lense. The idea of performativity originates from John L. Austin's seminal work "How to do things with words" <ref type="bibr">(Austin, )</ref>, where Austin notes that the function of language is often not only descriptive, but also has constitutive and causal effects. For example, uttering "I promise" itself constitutes a promise and has the causal effect of establishing certain expectations. Austin (</p><p>) refers to sentences with such performative force as speech acts. Since Austin, the term 'performativity' has travelled far into multiple disciplines and acquired lives of its own, so that it can be hard to pin down precisely a common core (see <ref type="bibr">(Mäki,</ref> ) for a critique). We will use the term in the broadest possible sense to emphasize similarity of perspectives instead of differences. An influential account has been put forward in economics (Callon, ; MacKenzie et al., ; MacKenzie, ), which has inspired also a recent formalization in machine learning <ref type="bibr">(Perdomo et al., ; Hardt et al., )</ref>. The central claim of this line of work is that economics is not simply in the business of describing or representing an independent, passive reality, but also actively shapes it, for instance by encouraging people to act in accordance with its models <ref type="bibr">(Boldyrev &amp; Svetlova, )</ref>. Another general framework and a source of inspiration to us, can be found in the work of Mol ( ). To avoid the dualist connotation of the term performance, implying a 'backstage reality', Mol (</p><p>) instead coins the term enactment, referring to the multiple and ongoing work that sustains a reality:</p><p>It is possible to say that in practices objects are enacted. This suggests that activities take placebut leaves the actors vague. It also suggests that in the act, and only then and there, something is -being enacted. [emphasis in original] Perhaps the simplest way to understand what is at the heart of performativity, we suggest, is to assert that representation and intervention are entangled <ref type="bibr">(Vosselman, )</ref>. Performativity hence contrasts with the commonsense view that perception and action can be neatly separated; in the latter, the task of machine learning is simply to extract patterns from a passive reality 'out there' in an objective way. For instance, <ref type="bibr">Mitchell et al. (</ref> ) distinguish between "world as it is" and "world as it should and could be", mapping onto prediction and decision task. Similarly, However, for a critical examination of whether this dualism is inherently associated with 'performance', see <ref type="bibr">(Hafermalz et al.,</ref> ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kuppler et al. (</head><p>) urge to cleanly separate prediction and decision. A word that frequently occurs in this context is 'bias', which we like to avoid, since we associate it with the notion of an objective 'backstage' reality, whose representation is then distorted.</p><p>What then is the relevance of performativity for insurance, and by analogy, machine learning? Actuarial fairness (calibration), or more broadly the fairness of 'accurate' statistical methods, carries with it an aura of objectivity and neutrality. If we choose responsibilization, then our predictive models better be 'objective and neutral'. Recall that actuarial fairness aims to set premia in accordance with the expected risk for each policyholder, and it is assumed that insurers can know this risk through statistical methods. <ref type="bibr">Glenn ( a)</ref> succinctly captures this as follows:</p><p>[T]here is a general belief that insurance practices are predicated on objective statistics, what has elsewhere been called "the myth of the actuary". The myth of the actuary is the idea that there is a reality in the world that can be captured by rational choice models and statistical analysis-and that insurance companies do this ethically, objectively, and "correctly."</p><p>Such objectivity then is supposed to be a source of authority and fairness. In the words of Van Hoyweghen ( ):</p><p>The dominant view is that insurance technologies of risk assessment are somehow 'measuring', 'observing' or 'describing' peoples' insurance risks. This paper calls for a different approach, namely a pragmatist analysis of the performativity of insurance calculative devices. Contrary to the financial realism of the everyday categories of insurance numbers, I argue that insurance calculative devices not only represent but generate, intervene and rearrange the worlds in which they are deployed. [emphasis added]</p><p>The performativity of insurance and machine learning becomes especially relevant due to ethical implications.</p><p>Many scholars have argued, providing insightful examples, that insurance is fundamentally a normative technology (Baker &amp; Simon, ; Glenn, a; Van Hoyweghen et al., ; ; Lehtonen &amp; Liukko, ; Tanninen, ; Prainsack &amp; Van Hoyweghen,</p><p>), depending on causality, control and responsibility. Doing insurance or machine learning involves enacting certain realities and suppressing others, as we have sketched in Section . . For instance, in the process of collecting data, only some features are considered, and others neglected. Expanding on this, a performativity perspective would emphasize that there is no objective data 'collection' process, that quantification and categorization require significant and ongoing work; such work may be influenced by implicit normative judgements, which becomes ingrained and hidden in the 'representation'. There is now a vibrant, if still nascent, research field on the sociology of quantification (including categorization), owing much to the seminal work of Desrosières (</p><p>); for overviews of this field see (Espeland &amp; Stevens, ; Diaz-Bone &amp; Didier, ; <ref type="bibr">Mennicken &amp; Espeland, )</ref>, where the reader finds plenty of evidence for such work. Central in this research field is again performativity, or what has been called the constitutive potential of quantification <ref type="bibr">(Mennicken &amp; Espeland,</ref> ). As a noteworthy example, it has been demonstrated that the census, through the introduction of statistical categories, can contribute to the establishment of a collective identity among the individuals it aims to describe <ref type="bibr">(Lee, ; Bowker &amp; Star, ; Mora, )</ref>. Thus, a category that was initially intended to merely represent acquires performativity by actively shaping the formation of this particular group <ref type="bibr">(Grommé &amp; Scheel, )</ref>.</p><p>We propose that insurance can act as a model for the performativity of statistical, "calculative devices" (Van Hoyweghen, ) that arise from their social situatedness. In machine learning, performativity shows up in at least two ways: on the one hand, it requires training data, and this training data has been shaped by performative forces in a broader social context -referring to the quantification and categorization processes. Using the To anticipate a criticism, this does of course not imply that arbitrary sets of people can become a mutually recognizing group: the hard conceptual work is to investigate how performativity of groups functions and what its limits are. In Austinian terms, this means delineating the "felicity" conditions, which make performative utterances successful <ref type="bibr">(Brisset, )</ref>.</p><p>data thus imports this performativity into the model. On the other hand, by deploying a machine learning model its predictions may acquire performative force in both a constitutive and causal sense. The predictions may act as interventions and through responsibilization shape the behaviour of people, who for instance may strategically adapt to the predictions -this sense of performativity is also referred to as reactivity (Espeland &amp; Sauder, ; Espeland &amp; Stevens,</p><p>), and is closer to the formalization proposed by Perdomo et al. ( ), which emphasizes the causal dimension, but neglects the constitutive one. The extent of this phenomenon, i.e. how much a company can steer a population using a model, has been termed performative power <ref type="bibr">(Hardt et al., )</ref>.</p><p>As explicated in Section , depending on the choice of fairness metric (or none) and to which features it is applied (i.e. the choice of groups), machine learning can exert responsibilizing and non-responsibilizing force. We suggest that two classes of performative effects can then be broadly distinguished, which is however not a clear dichotomy in light of Section . . When machine learning responsibilizes for a controllable feature, individuals may adapt to the prediction so as to change it -if they receive feedback; this is the hope of personalized insurance, and this setting also motivates the concept of moral hazard (Appendix B). In contrast, blindly applying the principle of actuarial fairness can lead to responsibilizing for non-controllable features, which then runs the risk of reproducing past injustice implicit in the training data. Yet against a background of such past injustice, it is not clear why actuarial fairness should be considered as a principle of justice -this argument has been made both in the insurance <ref type="bibr">(Daniels, ; Lehtonen &amp; Liukko, ; Barry, )</ref> as well as the machine learning literature <ref type="bibr">(Mitchell et al., ; Vredenburgh, ; Green, ; Kasirzadeh, )</ref>; see also <ref type="bibr">(Eidelson, )</ref> -however, the insurance literature provides illuminating examples. In this way, machine learning (resp. insurance) can implicitly responsibilize for sensitive features such as gender or race; the situation is particularly intricate when a feature is considered as controllable which stands in a constitutive relation to a sensitive feature <ref type="bibr">(Hu &amp; Kohler-Hausmann, )</ref>, for instance due to performativity.</p><p>In response to the performativity of machine learning, we advocate for explicit reflection about how performative forces have shaped the present input data, and furthermore how a model in conjunction with a choice of fairness metric might exert performative force by acting on people. Focusing on (non)responsibilization and performativity implies taking a dynamic perspective. Thus, it becomes imperative to foreground and explicitly model the effects of deploying machine learning systems <ref type="bibr">(Hu &amp; Chen, ; Liu et al., ; D'Amour et al., ; Schwöbel &amp; Remmers, )</ref>, constrasting with rather static vocabulary such as bias or discrimination. In this process of reflexive inquiry, we suggest to pay more attention to enactments of causality, control and responsibilityframing them in this way rather than as immutable facts implies making them contestable, that is, putting them on the stage for scrutiny <ref type="bibr">(Glenn, a)</ref>.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>While the case of gender demonstrates that what is considered controllable can change, for insurance purposes, gender is arguably still uncontrollable.One would (in most contexts) not try to personalize premia based on the binary feature 'having attached earlobes'.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Some argue that there is no place for risk subsidizing solidarity in insurance, that insurance is concerned only with chance solidarity. However, the conceptual distinction between these forms of solidarity is unclear and rather heuristic(Frezal &amp; Barry,  ); cf. also the discussion in Section on individual risk.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>We leave open the question to what extent this can be realized by human decision makers. Setting insurance premia based on subjective probabilities seems objectionable when it affects the welfare of people; similarly for machine learning.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was was funded by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</rs>) under Germany's Excellence Strategy -EXC number / -Project number . The authors thank the <rs type="funder">International Max Planck Research School for Intelligent Systems</rs> (<rs type="affiliation">IMPRS-IS</rs>) for supporting Christian Fröhlich. Thanks to <rs type="person">Benedikt Höltgen</rs>, <rs type="person">Sebastian Zezulka</rs>, <rs type="person">Renate Baumgartner</rs> and <rs type="person">Maiju Tanninen</rs> for helpful discussions and comments.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficiency and fairness in insurance risk classification</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">S</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Virginia Law Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Luck egalitarianism, universal health care, and nonresponsibility-based reasons for responsibilization</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Marchman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andersen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Morten</forename><surname>Ebbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juul</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Res Publica</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Machine bias: There&apos;s software used across the country to predict future criminals, and it&apos;s biased against blacks. ProPublica</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Angwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Mattu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Kirchner</surname></persName>
		</author>
		<ptr target="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" />
		<imprint>
			<date>June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Equality, risk and responsibility: Dworkin on the insurance market</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economy and Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uncertainty and the welfare economics of medical care</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><surname>Arrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Economic Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">How to Do Things With Words</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">L</forename><surname>Austin</surname></persName>
		</author>
		<imprint>
			<publisher>Harvard University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discrimination and insurance</title>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Avraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Routledge Handbook of the Ethics of Discrimination. Routledge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding insurance antidiscrimination law</title>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">D</forename><surname>Logue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Schwarcz</surname></persName>
		</author>
		<ptr target="https://scholarship.law.umn.edu/faculty_articles/576" />
	</analytic>
	<monogr>
		<title level="j">Southern California Law Review</title>
		<imprint>
			<date>June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the genealogy of moral hazard</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Texas Law Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Insuring morality</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economy and Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Risk, insurance, and the social construction of responsibility</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Embracing risk: The changing culture of insurance and responsibility</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<ptr target="http://www.fairmlbook.org" />
		<title level="m">Fairness and Machine Learning: Limitations and Opportunities</title>
		<imprint>
			<date>January</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The rationality of the digital governmentality</title>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal for Cultural Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Insurance, big data and changing conceptions of fairness</title>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Sociology/Archives Européennes de Sociologie</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Charpentier</surname></persName>
		</author>
		<title level="m">The fairness of machine learning in insurance: New rags for an old man? arXiv preprint arXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the apparent conflict between individual and group fairness</title>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Binns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Better in theory than in practise? challenges when applying the luck egalitarian ethos in health care policy</title>
		<author>
			<persName><forename type="first">Joar</forename><surname>Björk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert</forename><surname>Helgesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Juth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medicine, Health Care and Philosophy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Enacting dismal science: New perspectives on the performativity of economics</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Boldyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Svetlova</surname></persName>
		</author>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">C</forename><surname>Bowker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Star</forename></persName>
		</author>
		<title level="m">Sorting Things Out: Classification and Its Consequences</title>
		<imprint>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Economics is not always performative: some limits for performativity</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Brisset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Methodology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ambiguity aversion in the field of insurance: Insurers&apos; attitude to imprecise and conflicting probability estimates</title>
		<author>
			<persName><forename type="first">Laure</forename><surname>Cabantous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Decision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m">The laws of the markets</title>
		<editor>
			<persName><forename type="first">Michel</forename><surname>Callon</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ilaria Giuseppina Penco, and Andrea Claudio Cosentini. A clarification of the nuances in the fairness metrics landscape</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Castelnovo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Crupi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greta</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Regoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">From pool to profile: Social consequences of algorithmic prediction in insurance</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Cevolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Esposito</surname></persName>
		</author>
		<imprint>
			<publisher>Big Data &amp; Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From actuarial to behavioural valuation. the impact of telematics on motor insurance</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Cevolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Valuation Studies</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Insurance: Discrimination, biases &amp; fairness</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Charpentier</surname></persName>
		</author>
		<ptr target="https://www.institutlouisbachelier.org/en/insurance-discrimination-biases-fairness/" />
	</analytic>
	<monogr>
		<title level="m">Opinions &amp; Debates</title>
		<imprint>
			<date>June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AIDS and insurance: the rationale for AIDS-related testing</title>
		<author>
			<persName><forename type="first">Karen</forename><forename type="middle">A</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russel</forename><forename type="middle">P</forename><surname>Iuculano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Law Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probability, gambling and the origins of risk management</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Grinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Financial History Magazine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fairness is not static: deeper understanding of long term fairness via simulation studies</title>
		<author>
			<persName><forename type="first">Hansa</forename><surname>Alexander D'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baljekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoni</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Insurability and the HIV epidemic: ethical issues in underwriting</title>
		<author>
			<persName><forename type="first">Norman</forename><surname>Daniels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Milbank Quarterly</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Classical Probability in the Enlightenment</title>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Daston</surname></persName>
		</author>
		<imprint>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On individual risk</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Theory of probability: A critical introductory treatment</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finetti</forename></persName>
		</author>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Risk, calculable and incalculable</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Soziale Welt</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Autocalibration and Tweedie-dominance for insurance pricing with machine learning</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Denuit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Trufin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Insurance: Mathematics and Economics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Alain</forename><surname>Desrosières</surname></persName>
		</author>
		<title level="m">The politics of large numbers: A history of statistical reasoning</title>
		<imprint>
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Introduction: The sociology of quantification -perspectives on an emerging field in the social sciences</title>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Diaz-Bone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Didier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Historical Social Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pricing ambiguity in catastrophe risk insurance</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Falk</forename><surname>Niehörster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Geneva Risk and Insurance Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Better together? how externalities of size complicate notions of solidarity and actuarial fairness</title>
		<author>
			<persName><forename type="first">Kate</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the rd Innovations in Theoretical Computer Science Conference</title>
		<meeting>the rd Innovations in Theoretical Computer Science Conference</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Patterned inequality, compounding injustice, and algorithmic prediction</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Eidelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Law and Equality</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The moral hazards of neo-liberalism: lessons from the private insurance industry</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Ericson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economy and Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rankings and reactivity: How public measures recreate social worlds</title>
		<author>
			<persName><forename type="first">Wendy</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Espeland</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sauder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Sociology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A sociology of quantification</title>
		<author>
			<persName><forename type="first">Wendy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Espeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">L</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Sociology/Archives Européennes de Sociologie</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Die Versicherungs-Gesellschaft</title>
		<author>
			<persName><forename type="first">François</forename><surname>Ewald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kritische Justiz</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">François Ewald. Norms, discipline, and the law</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Insurance and risk</title>
		<author>
			<persName><forename type="first">Francois</forename><surname>Ewald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Foucault effect: Studies in governmentality</title>
		<imprint>
			<publisher>The University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">François Ewald. L&apos;État providence. Grasset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The discriminating (pricing) actuary</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Frees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">North American Actuarial Journal</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Sylvestre</forename><surname>Frezal</surname></persName>
		</author>
		<ptr target="https://www.chaire-pari.fr/wp-content/uploads/2016/09/Alea-and-Heterogeneity_the-Tyrannous" />
		<title level="m">Alea and heterogeneity: the tyrannous conflation</title>
		<imprint>
			<date>June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fairness in uncertainty: Some limits and misinterpretations of actuarial fairness</title>
		<author>
			<persName><forename type="first">Sylvestre</forename><surname>Frezal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business Ethics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Probable Justice: Risk, Insurance, and the Welfare State</title>
		<author>
			<persName><forename type="first">Rachel</forename><forename type="middle">Z</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Race, sex and genetic discrimination in insurance: What&apos;s fair?</title>
		<author>
			<persName><forename type="first">Jill</forename><surname>Gaulding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cornell Law Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The market for lemons: Quality uncertainty and the market mechanism</title>
		<author>
			<persName><forename type="first">Ackerlof</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The empire of chance: How probability changed science and everyday life</title>
		<author>
			<persName><forename type="first">Gerd</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Swijtink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Daston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenz</forename><surname>Kruger</surname></persName>
		</author>
		<imprint>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Postmodernism: the basis of insurance</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">J</forename><surname>Glenn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Risk Management and Insurance Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Risk, insurance, and the changing nature of mutual obligation</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">J</forename><surname>Glenn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Law &amp; Social Inquiry</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Escaping the impossibility of fairness: From formal to substantive algorithmic fairness</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy &amp; Technology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Doing statistics, enacting the nation: The performative powers of categories</title>
		<author>
			<persName><forename type="first">Francisca</forename><surname>Grommé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Scheel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nations and nationalism</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Enactment or performance? a non-dualist reading of Goffman</title>
		<author>
			<persName><forename type="first">Ella</forename><surname>Hafermalz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Boell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Beyond Interpretivism? New Encounters with Technology and Organization: IFIP WG . Working Conference on Information Systems and Organizations, IS&amp;O</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The reference class problem is your problem too</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Hájek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Performative power</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meena</forename><surname>Jagadeesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celestine</forename><surname>Mendler-Dünner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Algorithmic collective action in machine learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mazumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celestine</forename><surname>Mendler-Dünner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijana</forename><surname>Zrnic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the th International Conference on Machine Learning, ICML&apos;</title>
		<meeting>the th International Conference on Machine Learning, ICML&apos;</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Direct fit to nature: An evolutionary perspective on biological and artificial neural networks</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">A</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A moral framework for understanding fair ML through economic models of equality of opportunity</title>
		<author>
			<persName><forename type="first">Hoda</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Loi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Reactive risk and rational action: Managing moral hazard in insurance contracts</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Carol</surname></persName>
		</author>
		<author>
			<persName><surname>Heimer</surname></persName>
		</author>
		<imprint>
			<publisher>University of California Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">What was fair in actuarial fairness?</title>
		<author>
			<persName><forename type="first">Antonio</forename><forename type="middle">J</forename><surname>Heras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Charles</forename><surname>Pradier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Teira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">History of the Human Sciences</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On the richness of calibration</title>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Höltgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A short-term intervention for long-term fairness in the labor market</title>
		<author>
			<persName><forename type="first">Lily</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Wide Web Conference</title>
		<meeting>the World Wide Web Conference</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">What&apos;s sex got to do with machine learning?</title>
		<author>
			<persName><forename type="first">Lily</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issa</forename><surname>Kohler-Hausmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Can luck egalitarianism justify the fact that some are worse off than others</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Huseby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Philosophy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Algorithms and the individual in criminal law</title>
		<author>
			<persName><forename type="first">Renée</forename><surname>Jorgensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Philosophy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fairness in learning: Classic and contextual bandits</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><forename type="middle">H</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Algorithmic fairness and structural injustice: Insights from feminist political philosophy</title>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, AIES</title>
		<meeting>the AAAI/ACM Conference on AI, Ethics, and Society, AIES<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ISBN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Avoiding discrimination through causal reasoning</title>
		<author>
			<persName><forename type="first">Niki</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><forename type="middle">Rojas</forename><surname>Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giambattista</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Inherent Trade-Offs in the Fair Determination of Risk Scores</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Innovations in Theoretical Computer Science Conference (ITCS ), volume</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Luck egalitarianism</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy Compass</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unmasked: A history of the individualization of risk</title>
		<author>
			<persName><forename type="first">Greta</forename><forename type="middle">R</forename><surname>Krippner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Sociological Theory</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The person of the category: the pricing of risk and the politics of classification in insurance and credit</title>
		<author>
			<persName><forename type="first">Greta</forename><forename type="middle">R</forename><surname>Krippner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hirschman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Distributive justice and fairness metrics in automated decision-making</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Kuppler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><forename type="middle">L</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frauke</forename><surname>Kreuter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">How much overlap is there? arXiv preprint arXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Landes</surname></persName>
		</author>
		<ptr target="https://www.centroeinaudi.it/images/abook_file/WP-LPF_6_2013_Landes.pdf" />
		<title level="m">The normative foundations of (social) insurance: Technology, social practices and political philosophy</title>
		<imprint>
			<date>June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">How fair is actuarial fairness</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Landes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business Ethics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Insurance, equality and the welfare state: Political philosophy and (of) public insurance</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Landes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Holtug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Res Publica</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Racial classifications in the US census</title>
		<author>
			<persName><forename type="first">Sharon</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ethnic and Racial Studies</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The forms and limits of insurance solidarity</title>
		<author>
			<persName><forename type="first">Turo-Kimmo</forename><surname>Lehtonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyri</forename><surname>Liukko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business Ethics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Producing solidarity, inequality and exclusion through insurance</title>
		<author>
			<persName><forename type="first">Turo-Kimmo</forename><surname>Lehtonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyri</forename><surname>Liukko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Res publica</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Editorial: Insurance and the economization of uncertainty journal</title>
		<author>
			<persName><forename type="first">Turo-Kimmo</forename><surname>Lehtonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ine</forename><surname>Van Hoyweghen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cultural Economy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Genetic discrimination and health insurance</title>
		<author>
			<persName><forename type="first">Kasper</forename><surname>Lippert-Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Res Publica</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Delayed impact of fair machine learning</title>
		<author>
			<persName><forename type="first">Lydia</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Rolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Simchowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Genetic discrimination, insurance, and solidarity: an analysis of the argumentation for fair risk classification</title>
		<author>
			<persName><forename type="first">Jyri</forename><surname>Liukko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Genetics and Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Choosing how to discriminate: Navigating ethical trade-offs in fair algorithmic design for the insurance sector</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Loi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Christen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy &amp; Technology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">The diverse domains of quantified selves: self-tracking modes and dataveillance</title>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Lupton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economy and Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">An engine, not a camera: How financial models shape markets</title>
		<author>
			<persName><forename type="first">Donald</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m">Do economists make markets?: on the performativity of economics</title>
		<editor>
			<persName><forename type="first">Donald</forename><surname>Mackenzie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabian</forename><surname>Muniesa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Leung-Sea</forename><surname>Siu</surname></persName>
		</editor>
		<imprint>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Performativity: Saving austin from mackenzie</title>
		<author>
			<persName><forename type="first">Uskali</forename><surname>Mäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EPSA perspectives and foundational problems in philosophy of science</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A &apos;good, average man&apos;: Calculation and the limits of statistics in enrolling insurance customers</title>
		<author>
			<persName><forename type="first">Liz</forename><surname>Mcfall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Sociological Review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Personalizing solidarity? the role of self-tracking in health insurance pricing</title>
		<author>
			<persName><forename type="first">Liz</forename><surname>Mcfall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economy and Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Who, or what, is insurtech personalizing?: persons, prices and the historical classifications of risk</title>
		<author>
			<persName><forename type="first">Liz</forename><surname>Mcfall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liz</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distinktion: journal of social theory</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Editorial: The personalisation of insurance: Data, behaviour and innovation</title>
		<author>
			<persName><forename type="first">Liz</forename><surname>Mcfall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ine</forename><surname>Van Hoyweghen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">What&apos;s new with numbers? sociological approaches to the study of quantification</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Mennicken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Espeland</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Enacting actuarial fairness in insurance: From fair discrimination to behaviour-based fairness</title>
		<author>
			<persName><forename type="first">Gert</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ine</forename><surname>Van Hoyweghen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science as Culture</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Killing the law of large numbers: Mortality risk premiums and the sharpe ratio</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Milevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Promislow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Risk and Insurance</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Disparate impact and unfairly discriminatory insurance rates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Casualty Actuarial Society E-Forum</title>
		<meeting><address><addrLine>Winter</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Algorithmic fairness: Choices, assumptions, and definitions</title>
		<author>
			<persName><forename type="first">Shira</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Potash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Amour</surname></persName>
		</author>
		<author>
			<persName><surname>Lum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">The body multiple: Ontology in medical practice</title>
		<author>
			<persName><forename type="first">Annemarie</forename><surname>Mol</surname></persName>
		</author>
		<imprint>
			<publisher>Duke University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Price and the person: Markets, discrimination, and personhood</title>
		<author>
			<persName><forename type="first">Liz</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celia</forename><surname>Lury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cultural Economy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Making Hispanics: How Activists, Bureaucrats, and Media Constructed a New American</title>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Mora</surname></persName>
		</author>
		<imprint>
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Performative prediction</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Perdomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijana</forename><surname>Zrnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celestine</forename><surname>Mendler-Dünner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Insurance pricing under ambiguity</title>
		<author>
			<persName><forename type="first">Alois</forename><surname>Pichler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Actuarial Journal</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Shifting solidarities: Personalisation in insurance and medicine</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Prainsack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ine</forename><surname>Van Hoyweghen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shifting solidarities: Trends and developments in European societies</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Neoliberal governance and &apos;responsibilization&apos; of agents: reassessing the mechanisms of responsibility-shift in neoliberal discursive environments</title>
		<author>
			<persName><forename type="first">Jarkko</forename><surname>Pyysiäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darren</forename><surname>Halpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Guilfoyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distinktion: Journal of Social Theory</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Group fairness: Independence revisited</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Räz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">The right to underwrite gender: The goods &amp; services directive and the politics of insurance pricing</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Rebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ine</forename><surname>Van Hoyweghen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tijdschrift Voor Genderstudies</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Personalised genetic testing and its impact to insurance</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Rechfeld</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<ptr target="https://www.swissre.com/dam/jcr" />
		<title level="m">Swiss Re</title>
		<imprint>
			<date>24995a5d-5b66-42ea-a2b9-660458bc6e26. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">The Theory of Probability: An Inquiry Into the Logical and Mathematical Foundations of the Calculus of Probability</title>
		<author>
			<persName><forename type="first">Hans</forename><surname>Reichenbach</surname></persName>
		</author>
		<imprint>
			<publisher>University of California Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">The New Social Question: Rethinking the Welfare State</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Rosanvallon</surname></persName>
		</author>
		<imprint>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">The long arc of fairness: Formalisations and ethical discourse</title>
		<author>
			<persName><forename type="first">Pola</forename><surname>Schwöbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Remmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Fairness and abstraction in sociotechnical systems</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danah</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorelle</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><surname>Vertesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">The age of responsibilization: On market-embedded morality</title>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economy and Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Self-tracking for health and the quantified self: Re-articulating autonomy, solidarity, and authenticity in an age of personalized healthcare</title>
		<author>
			<persName><forename type="first">Tamar</forename><surname>Sharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy &amp; Technology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<author>
			<persName><forename type="first">Deborah</forename><forename type="middle">A</forename><surname>Stone</surname></persName>
		</author>
		<ptr target="https://prospect.org/health/ad-missions/" />
		<title level="m">Ad missions. The American Prospect</title>
		<imprint>
			<date>May</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Risk classification&apos;s big data (r) evolution</title>
		<author>
			<persName><forename type="first">Rick</forename><surname>Swedloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connecticut Insurance Law Journal</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Contested technology: Social scientific perspectives of behaviour-based insurance</title>
		<author>
			<persName><forename type="first">Maiju</forename><surname>Tanninen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Fairness and equality in insurance classification</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Thiery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Van Schoubroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Geneva Papers on Risk and Insurance-Issues and Practice</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">On the politics of calculative devices: performing life insurance markets</title>
		<author>
			<persName><forename type="first">Ine</forename><surname>Van Hoyweghen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cultural Economy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Genomics and insurance: The lock-in effects of a politics of genetic solidarity</title>
		<author>
			<persName><forename type="first">Ine</forename><surname>Van Hoyweghen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Genomics</title>
		<imprint>
			<publisher>Health and Society</publisher>
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Making the normal deviant: The introduction of predictive medicine in life insurance</title>
		<author>
			<persName><forename type="first">Ine</forename><surname>Van Hoyweghen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klasien</forename><surname>Horstman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Schepers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science &amp; Medicine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Genetic &apos;risk carriers&apos; and lifestyle &apos;risk takers&apos;. which risks deserve our legal protection in insurance?</title>
		<author>
			<persName><forename type="first">Ine</forename><surname>Van Hoyweghen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klasien</forename><surname>Horstman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Schepers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health Care Analysis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">The Logic of Chance</title>
		<author>
			<persName><forename type="first">John</forename><surname>Venn</surname></persName>
		</author>
		<imprint>
			<publisher>MacMillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Unravelling the predictive power of telematics data in car insurance pricing</title>
		<author>
			<persName><forename type="first">Roel</forename><surname>Verbelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrien</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerda</forename><surname>Claeskens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series C (Applied Statistics)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">The &apos;performativity thesis&apos; and its critics: Towards a relational ontology of management accounting</title>
		<author>
			<persName><forename type="first">Ed</forename><surname>Vosselman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accounting and Business Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m">The Oxford Handbook of AI Governance</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Risk classification standards</title>
		<author>
			<persName><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Casualty Actuarial Society</title>
		<meeting>the Casualty Actuarial Society</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
