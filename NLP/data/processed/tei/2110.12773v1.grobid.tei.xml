<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scientific Machine Learning Benchmarks</title>
				<funder ref="#_5sSnTJc #_AZ98GTN">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder>
					<orgName type="full">Alan Turing Institute</orgName>
				</funder>
				<funder>
					<orgName type="full">Wave 1 of The UKRI Strategic Priorities Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Scientific Machine Learning Research Group</orgName>
				</funder>
				<funder>
					<orgName type="full">Benchmarking for AI for Science at Exascale</orgName>
					<orgName type="abbreviated">BASE</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jeyan</forename><surname>Thiyagalingam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Rutherford Appleton Laboratory, Science and Technology Facilities Council</orgName>
								<orgName type="institution">Harwell Campus</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mallikarjun</forename><surname>Shankar</surname></persName>
							<email>shankarm@ornl.gov</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Oak Ridge National Laboratory</orgName>
								<address>
									<addrLine>1 Bethel Valley Road</addrLine>
									<postCode>37831</postCode>
									<settlement>Oak Ridge</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Geoffrey</forename><surname>Fox</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">Computer Science and Biocomplexity Institute</orgName>
								<address>
									<addrLine>994 Research Park Blvd</addrLine>
									<postCode>22911</postCode>
									<settlement>Charlottesville</settlement>
									<region>Virginia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tony</forename><surname>Hey</surname></persName>
							<email>tony.hey@stfc.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Rutherford Appleton Laboratory, Science and Technology Facilities Council</orgName>
								<orgName type="institution">Harwell Campus</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scientific Machine Learning Benchmarks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3886174F388D90CF09C122F830C0CB8D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The breakthrough in Deep Learning neural networks has transformed the use of AI and machine learning technologies for the analysis of very large experimental datasets. These datasets are typically generated by large-scale experimental facilities at national laboratories. In the context of science, scientific machine learning focuses on training machines to identify patterns, trends, and anomalies to extract meaningful scientific insights from such datasets. With a new generation of experimental facilities, the rate of data generation and the scale of data volumes will increasingly require the use of more automated data analysis.</p><p>At present, identifying the most appropriate machine learning algorithm for the analysis of any given scientific dataset is still a challenge for scientists. This is due to many different machine learning frameworks, computer architectures, and machine learning models. Historically, for modelling and simulation on HPC systems such problems have been addressed through benchmarking computer applications, algorithms, and architectures. Extending such a benchmarking approach and identifying metrics for the application of machine learning methods to scientific datasets is a new challenge for both scientists and computer scientists. In this paper, we describe our approach to the development of scientific machine learning benchmarks and review other approaches to benchmarking scientific machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past decade, a sub-field of artificial intelligence (AI), namely Deep Learning (DL) neural networks (or deep neural networks, DNNs), has made significant breakthroughs in many scientifically and commercially 2 important applications <ref type="bibr" target="#b0">1</ref> . Such neural networks are themselves a subset of a wide range of machine learning (ML) methods (Figure <ref type="figure" target="#fig_0">1.</ref>) ML methods have been widely used for many years in several domains of science, but DNNs have been transformational and are gaining a lot of traction in many scientific communities <ref type="bibr" target="#b2">3</ref> . Most of the national laboratories that host large-scale experimental facilities are now relying on DNN-based data analytic methods to extract scientific insights from their increasingly large datasets. A recent spectacular success is DeepMind's use of Deep Learning in their Alpha Fold-1 and Alpha Fold-2 <ref type="bibr" target="#b3">4</ref> solutions to the protein folding 'Grand Challenge'. This promises to transform much of biological science and open up exciting new research avenues. Other domains of science are exploring physical representations of the system with the data-driven learning ability of neural networks. Current developments are towards specialising these ML approaches to be more domain-specific and domain-aware <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> , and aiming to connect the apparent 'black box' successes of DL networks with well-understood approaches from science.</p><p>The overarching scope of ML in science is very broad, including identifying patterns, anomalies, and trends from relevant scientific datasets, and using ML for classification and predicting of those patterns, clustering of data, and generating near-realistic synthetic data. There are three approaches for developing ML-based solutions, namely, supervised, unsupervised, and reinforcement learning. In supervised learning, the ML model is trained for a given task with examples. In order to have examples, the data used for training the ML model must contain the ground truth or labels. Supervised learning is therefore only possible when there is a labelled subset of the data. Once trained, the learned model can be deployed for real-time usage, such as pattern classification or estimation ---which is often referred to as inference.</p><p>Because of the difficulty in generating labelled data for supervised learning, particularly for experimental datasets, it is often difficult to apply supervised learning directly. To circumvent this limitation, training is often performed on simulated data, which provides an opportunity to have relevant labels. However, the simulated data may not be representative of the real data and the model may therefore not perform satisfactorily when used for inferencing. The unsupervised learning technique, in contrast, does not rely on labels. A simple example of this technique is clustering, where the aim is to identify several groups of data points that have common features. Another example is identification of anomalies in data. Example algorithms include k-Means Clustering 8 , Support Vector Machines (SVM) <ref type="bibr" target="#b8">9</ref> , or neural network-based autoencoders <ref type="bibr" target="#b9">10</ref> . Finally, reinforcement learning relies on a trial-and-error approach to learn a given task with the learning system being positively rewarded whenever the system behaves correctly, and penalised whenever it behaved incorrectly <ref type="bibr" target="#b10">11</ref> . Each of these learning paradigms have a large number of algorithms, and modern developmental approaches are often hybrid and use one of more of these techniques together. This leaves a very large choice of ML algorithms for any given problem.</p><p>In practice, the selection of an ML algorithm for a given scientific problem is more complex than just selecting one of the machine learning technologies and any particular algorithm. The selection of the most effective ML algorithm is based on many factors, including the type, quantity, and quality of the training data, the availability of labelled data, the type of problem being addressed (prediction, classification, and so on), the overall accuracy and performance required, and the hardware systems available for training and inferencing. With such a multi-dimensional problem consisting of a choice of ML algorithms, hardware architectures, and a range of scientific problems, selecting an optimal ML algorithm for a given task is not trivial. This constitutes a significant barrier for many scientists wishing to use modern ML methods in their scientific research.</p><p>In this paper we use suitable scientific ML benchmarks to develop guidelines and best practices to assist the scientific community in successfully exploiting these methods. Moreover, developing such guidelines and best practices at the community level will not only benefit the science community but also highlight where further research into ML algorithms, computer architectures, and software solutions for using ML in scientific applications is needed. Such guidelines and best practices need to be based on real-world application examples and relevant data.</p><p>For instance, demonstrating the success of a specific ML technique on a specific scientific problem will assist researchers in applying the technique to similar problems. We refer to the development of guidelines and best practices as Benchmarking. In our case, this is very specific to ML techniques applied to scientific datasets. The applications used to demonstrate the guideline and best practices are referred to as Benchmarks.</p><p>The notion of benchmarking computer systems and applications has been a fundamental cornerstone of computer science, particularly for compiler, architectural and system development, with a key focus on using benchmarks for ranking systems, such as the Top500 or Green500 <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> . However, our notion of scientific ML benchmarking has a different focus. Firstly, these machine learning benchmarks can be considered as blueprints for use on a range of scientific problems, and hence are aimed at fostering the use of ML in science more generally. Secondly, by using these ML benchmarks, a number of aspects in an ML ecosystem can be compared and contrasted. For example, it is possible to rank different computer architectures for their performance, or to rank different ML algorithms for their effectiveness. Thirdly, these ML benchmarks are accompanied by relevant dataset(s) on which the training and/or inference will be based. This is different to conventional benchmarks for high-performance computing (HPC) where there is little dependency on datasets. The establishment of a set of open curated datasets with associated ML benchmarks is therefore an important step for scientists to be able to effectively utilise ML methods in their research and also to identify further directions for ML research.</p><p>In this paper, we first discuss what we mean by scientific machine learning benchmarks, the scope of such benchmarks, and the challenges in creating such benchmarks. We then review a number of benchmarking initiatives in light of this discussion. The paper is organised as follows. In Section 2, we discuss the primary considerations in designing benchmarks to advance the application of ML methods for scientific research along with relevant examples. We then define the scope and challenges around establishing such scientific machine learning benchmarks in Section 3. In Section 4, we review a number of ML benchmarking initiatives in light of our discussions in Sections 2 and 3. We then discuss SciMLBench, one of the most recent and versatile scientific ML benchmarking initiatives, in Section 5. We summarise our findings and conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Machine Learning Benchmarks for Science</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Elements of a Benchmark for Science</head><p>As discussed above, a scientific ML benchmark is underpinned by a scientific problem and should have two elements: (a) the dataset on which this benchmark is trained or inferenced upon, and (b) a reference implementation, which can be in any programming language (e.g., Python, C++). The scientific problem can be from many different scientific domains. A collection of such benchmarks can make up a benchmark suite as illustrated in Figure <ref type="figure" target="#fig_2">2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Focus of Benchmarking</head><p>There are three separate aspects of scientific benchmarking that apply in the context of ML benchmarks for science, namely scientific ML benchmarking, application benchmarking and system benchmarking. We illustrate these in Figure <ref type="figure" target="#fig_3">3</ref>. • Scientific ML Benchmarking: This is concerned with algorithmic improvements that help reach the scientific targets specified for a given dataset. Here we wish to test algorithms and their performance on fixed data assets, typically with the same underlying hardware and software environment. This type of benchmark is characterized by the dataset together with some specific scientific objectives. The data is obtained from a scientific experiment and should be rich enough to allow different methods of analysis and exploration. Examples of metrics could include the F1 score for training accuracy and time-to-solution.</p><p>• Application Benchmarking: This aspect of ML benchmarks is concerned with exploring the performance of the complete ML application when using different hardware and software environments. A typical performance target would be time-to-solution. This type of benchmark is characterized by the ML application and data which can then be used to evaluate the performance of the overall system (hardware, software libraries, runtime environments, filesystems, etc.) in the context of the given application and data. Examples of metrics could include a throughput measure (e.g., images per second), time-to-solution of the application, and investigation of the scaling properties of the application.</p><p>• System Benchmarking: This is concerned with investigating performance effects of the system hardware architecture on improving the scientific outcomes/targets. These benchmarks have similarities with application benchmarks, but they are characterized by primarily focusing on a specific operation that exercises a particular part of the system, independent of the broader system environment. Suitable metrics could be time-to-solution, the number of floating-point operations per second (FLOP/s) achieved, or aspects of network and data movement performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Examples of Scientific Machine Learning Benchmarks</head><p>Scientific ML Benchmarks are ML applications that solve a particular scientific problem from a specific scientific domain. For example, this can be as simple as an application that classifies the experimental data in some way, or as complex as inferring the properties of a material from neutron scattering data.</p><p>Some examples are:</p><p>1. Inferring the structure of multi-phase materials from X-ray diffuse multiple scattering data.</p><p>Here, the machine learning is used to automatically identify the phases of materials using classification. This is an example from the materials science domain.</p><p>2. Estimating the photometric redshifts of galaxies from survey data. Here, the machine learning is used for estimation. This example is drawn from astronomy <ref type="bibr" target="#b16">17</ref> .</p><p>3. Clustering of microcracks in a material using X-ray scattering data. Here, the machine learning uses an unsupervised learning technique.</p><p>4. Removing noise from microscope data to improve the quality of images. Machine learning is used for its capability to perform high quality regression of pixel values.</p><p>We provide more detailed examples in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Benchmarking Process</head><p>Although it is possible to provide a collection of ML-specific scientific applications (with relevant datasets)</p><p>as benchmarks for any of the purposes mentioned above, the exact process of benchmarking requires the following processes:</p><p>• Metrics of Choice: First, depending on the focus (see Section 2), the exact metric by which different benchmarks are compared may vary. For example, if science is the focus, then this metric may vary from benchmark to benchmark. However, if the focus is system-level benchmarking, it is possible to agree on a common set of metrics that can span across a range of applications.</p><p>• Framework: Providing just a collection of disparate applications without a coherent mechanism for evaluation will require users perform several fairly complex benchmarking operations for their specific goals. Ideally, therefore, the benchmark suite should also offer a framework that helps in achieving these goals and that unifies aspects that are common to all applications in the suite, such as portability, flexibility, and logging.</p><p>• Reporting and Compliance: Finally, how these results are reported is important. In many cases, a benchmark framework as discussed above addresses this concern. However, there are often some specific compliance aspects that must be followed to ensure that the benchmarking process is carried out fairly across different hardware platforms.</p><p>There are also a number of challenges which need to be addressed when dealing with the development of ML benchmarks. These are:</p><p>• Data: In the previous section, we highlighted the significance of data when using ML for scientific problems. The availability of curated, large-scale, scientific datasets -which can be either experimental or simulated data -is the key to developing useful ML benchmarks for science.</p><p>Although much scientific data is openly available, the curation, maintenance, and distribution of large-scale datasets for public consumption is a challenging process. A good benchmarking suite needs to provide a wide range of curated scientific datasets coupled with the applications.</p><p>Reliance on external datasets has the danger of not having full control or even access to those datasets.</p><p>• Distribution: A scientific ML benchmark constitutes a reference implementation and relevant dataset and both these must be available to the users. Since realistic dataset sizes can be in the terabytes (TB) range, the access and downloading of the datasets is not always straightforward.</p><p>• Coverage: Benchmarking is a very broad topic and providing benchmarks to cover the different focus areas highlighted above, across a range of scientific disciplines, is not a trivial task. A good benchmark suite should provide a good coverage of methods and goals and should be extensible.</p><p>• Extensibility: Although the notion of scientific machine learning benchmarks can be interesting for scientists, it can be very time-consuming to develop benchmark-specific codes. If the original scientific application needs substantial refactoring to be converted into a benchmark, this will not be an attractive option for scientists. Any benchmarking framework should therefore try to minimise the amount of code refactoring required for conversion into a benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Review of Benchmarking Initiatives</head><p>Comparing different ML techniques is not a new requirement and is increasingly becoming common in ML research. In fact, this approach has been fundamental for the development of various ML techniques.</p><p>For example, the ImageNet <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref> dataset spurred a competition to improve computer image analysis and understanding and been widely recognized for driving innovation in deep learning. However, providing a blueprint of applications, guidelines, and best practices in the context of scientific machine learning is a relatively new requirement. There have been a number of efforts on this aspect that address some of the challenges we highlighted in Section 3. In this brief review of these benchmarking initiatives, we explicitly exclude conventional benchmarking activities in other areas of computer science, such as benchmarks for HPC systems, compilers, and sub-systems such as memory, storage, and networking <ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20</ref> .</p><p>Instead of giving an exhaustive technical review covering very fine-grained aspects, we give a very highlevel review of the various ML benchmark initiatives here, focussing on the requirements discussed in Sections 2 and 3. We shall therefore cover the following aspects in our review:</p><p>1. Benchmark Focus: Science, Application (End-to-End), and System.</p><p>2. Benchmark Process: Metrics, Framework, and Reporting &amp; Compliance.</p><p>3. Benchmark Challenges: Data, Distribution, Coverage, and Extensibility.</p><p>In the context of ML benchmarking, there are a several initiatives such as Deep500 21 , RLBench <ref type="bibr" target="#b21">22</ref> , CORAL-2 <ref type="bibr" target="#b22">23</ref> , DAWNBench <ref type="bibr" target="#b23">24</ref> , AI Bench 25 , MLCommons <ref type="bibr" target="#b25">26</ref> , SciML-Bench 27 , as well as widely recognised Community</p><p>Competitions (such as those organized by Kaggle 28 ). We review these initiatives below and note that a specific benchmarking initiative may or may not support all the aspects listed above or, in some cases, may only offer partial support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deep 500</head><p>The Deep500 21 initiative proposes a customizable and modular software infrastructure to aid in comparing the wide range of deep learning frameworks, algorithms, libraries, and techniques. The key idea behind Deep500 is its modular design, where deep learning is factorized into four distinct levels: operators, network processing, training, and distributed training. While this approach aims to be neutral and overarching, and able to accommodate a wide variety of techniques and methods, the process of mapping a code to a new framework has impeded its adoption for new benchmark development. Furthermore, despite its key focus on deep learning, neural networks, and a very customisable framework, benchmarks or applications are not included by default and are left for the end user to provide, as is support for reporting. The main limitation is the lack of a suite of representative benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RLBench</head><p>RLBench 22 is a benchmark and learning environment featuring hundreds of unique, hand-crafted tasks.</p><p>The focus is on a set of tasks to evaluate new algorithmic developments around reinforcement learning, imitation learning, multi-task learning, geometric computer vision, and in particular, few-shot learning.</p><p>The tasks are very specific and can be considered as building blocks of large-scale applications. However, the environment currently lacks support for the classes of benchmarking discussed in section 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CORAL-2</head><p>The CORAL-2 <ref type="bibr" target="#b22">23</ref> benchmarks are computational problems relevant to a scientific domain or to data science, and are typically backed by a community code. Vendors are then expected to evaluate and optimize these codes to demonstrate the value of their proposed hardware in accelerating computational science. This allows a vendor to rigorously demonstrate the performance capabilities and characteristics of a proposed machine on a benchmark suite that should be relevant for computational scientists. The machine learning and data science tools in CORAL-2 include a number of ML techniques across two suites, namely, the big data analytics (BDS) and deep learning (DLS) suites. While the BDS suite covers conventional ML techniques, such as principal components analysis (PCA), k-means clustering, and support vector machines (SVM), the DLS relies on the ImageNet <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref> and CANDLE <ref type="bibr" target="#b27">29</ref> benchmarks which are primarily used for testing scalability aspects rather than purely focussing on the science. Similarly, the BDS suite aims to exercise the memory constraints (PCA), computing capabilities (SVM), and/or both these aspects (k-Means) and is also concerned with communication characteristics. Although these benchmarks are oriented at machine learning, the constraints and benchmark targets are narrowly specified and emphasize scalability capabilities. The overall coverage of science in the CORAL-2 benchmark suite is quite broad but the footprint of the ML techniques is limited to the BDS and DLS suites and there is little focus on scientific data distribution for algorithm improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">AI Bench</head><p>The AI Bench initiative is supported by the International Open Benchmark Council (Bench Council) <ref type="bibr" target="#b24">25</ref> . The Council is a non-profit international organization that aims to promote standardizing, benchmarking, evaluating, and incubating Big Data, AI, and other emerging technologies. The scope of AI Bench is very comprehensive and includes a broad range of internet services, including search engines, social networks, and e-commerce. The underlying ML-specific tasks in these areas include image classification, image generation, translation (image-to-text, image-to-image, text-to-image, text-to text), object detection, text summarisation, advertising, and natural language processing. The relevant datasets are open, and the primary metric is system performance for a fixed target. AI Bench currently lacks a specific science focus and a framework but the AI Bench environment does enforce some level of compliance for reporting ranking information of hardware systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">DawnBench</head><p>DawnBench <ref type="bibr" target="#b23">24</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Benchmarks from MLCommons Working Groups</head><p>MLCommons is an international initiative aimed at improving all aspects of the ML landscape and covers benchmarking, datasets, and best practices. The consortium has several working groups around various different focii for ML applications. Among these working groups, two are of interest here: HPC and Science. The MLCommons HPC benchmark <ref type="bibr" target="#b25">26</ref> suite focuses on scientific applications that use ML, and especially on deep learning (DL) at HPC scale. The codes and data are specified in such a way that execution of the benchmarks on supercomputers will help understand detailed aspects of system performance. The focus is on performance characteristics particularly relevant to HPC applications such as model-system interactions, optimization of the workload execution, and reducing throughput or execution bottlenecks. The HPC orientation also drives this effort towards exploration of benchmark scalability.</p><p>By contrast, the MLCommons Science benchmark 31 suite focuses specifically on the application of ML methods to scientific applications and includes application examples across several scientific domains.</p><p>However, the suite currently lacks a supportive framework for running the benchmarks but, as with the rest of the MLCommons, does enforce compliance for reporting of the results. The benchmarks cover the three areas of benchmarking -science, application, and system -as described in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">SciMLBench</head><p>The Scientific Machine Learning Benchmark suite -or SciMLBench <ref type="bibr" target="#b26">27</ref> -has been developed by the Scientific Machine Learning Research Group at the Rutherford Appleton Laboratory in the UK. The suite of benchmarks is specifically focussed on scientific machine learning and covers nearly every aspect of the cases we discussed in Sections 2 and 3. We provide a fuller description of the SciMLBench in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Community Competitions</head><p>Although community-based competitions, such as those organised by Kaggle 28 , can be seen as a benchmarking activity, these competitions are do not have a coherent methodology or a controlled approach for developing benchmarks. In particular the competitions do not provide a framework for running the benchmarks nor do they consider data distribution methods. Each competition is tailored on its own and relies on its own dataset, set of rules, and compliance metrics. The competitions are also typically short-lived. Although such challenge competitions can provide a blueprint for using ML technologies for specific research communities, they are unlikely to deliver best practices or guidelines for the long-term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The SciML-Bench Approach</head><p>The SciMLBench approach was developed by the Scientific Machine Learning Group at the Rutherford Appleton Laboratory in collaboration with researchers at Oak Ridge National Laboratory and at the University of Virginia. Among all the approaches we reviewed in Section 4, only the SciMLBench benchmark suite addresses nearly all of the concerns raised in Section 2. To the best of our knowledge, the SciMLBench approach is unique in its versatility compared to the other approaches and its key focus is on scientific machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Core Components</head><p>The SciMLBench has three components, namely:</p><p>• Benchmarks: The benchmarks are machine learning applications performing a specific scientific task, written in Python. These are included by default, and users are not required to find applications on their own. On the scale of micro-apps, mini-apps, and apps, these codes are fullfledged applications. Each benchmark aims to solve a specific scientific problem (see Section 2.3</p><p>for science examples).</p><p>• Datasets: Each benchmark relies on one or more datasets which can be used, for example, for training and/or inferencing. These datasets are open, task-or domain-specific, and FAIR compliant. Since most of these datasets are large, they are hosted separately, on one of our Lab servers (or mirrors), and are automatically or explicitly downloaded on demand.</p><p>• Framework: The framework serves two purposes. Firstly, at the user level, it facilitates an easier approach to the actual benchmarking, logging, and reporting of the results. Secondly, at the developer level, it provides a coherent API for unifying and simplifying the development of ML benchmarks.</p><p>The SciML framework is the basic fabric upon which the benchmarks are built. It is both extensible and customizable and offers a set of application programming interfaces (APIs). These APIs enable easier development of benchmarks based on this framework. The framework is architecture-independent and the minimum system requirement is determined by the specific benchmark. There is a built-in logging mechanism that captures all potential system-level and benchmark-level outputs during execution. The central component that links benchmarks, datasets, and the framework is a configuration tool that the framework relies on. The most attractive part of the framework is the possibility of simply using existing codes as benchmarks with only a few API calls necessary to register the benchmarks. Finally, the framework is designed with scalability in mind, so that benchmarks can be run on any computer ranging from a single system to a large-scale supercomputer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Benchmarks and Datasets</head><p>The currently released version of SciMLBench has three benchmarks with their associated datasets. The benchmarks from this release represent scientific problems drawn from material sciences and from environmental sciences, namely:</p><p>1. Diffuse Multiple Scattering (DMS_Structure, Material Sciences): This benchmark uses machine learning for classifying the structure of multi-phase materials from X-ray scattering patterns. More specifically, the machine learning based approach enables automatic identification of phases. This application is particularly useful for the material science community as diffuse multiple scattering allows investigation of multi-phase materials from a single measurement -something not possible with standard X-ray experiments. However, manual analysis of the data can be extremely laborious, involving searching of patterns to identify important motifs (triple intersections) that allow for inference of information. This is a multi-label classification problem (as opposed to a binary classification problem as in the Cloud masking example discussed below). The benchmark relies on a simulated dataset of size 8.6GB with three-channel images of resolution 487x195 pixels.</p><p>2. Cloud Masking (SLSTR_Cloud, Environmental Sciences): Given a set of satellite images, the challenge for this benchmark to classify each pixel of each satellite image as either cloud or as non-cloud (clear sky). This problem is known as 'cloud masking' and is crucial for several important applications in earth observation. In a conventional, non-ML setting, this task is typically performed using either thresholding or Bayesian methods. The benchmark exercises deep learning and includes two datasets, DS1-Cloud and DS2-Cloud, with sizes of 180GB and 1.2TB, respectively. The datasets contain multi-spectral images with resolution of 2400 x 3000 pixels or 1200 x 1500 pixels. 3. Electron Microscopy Image Denoising (EM_Denoise): This benchmark uses machine learning for removing noise from electron microscopic images. This improves the signal to noise ratio of the image and is often used as a precursor to more complex techniques such as surface reconstruction or tomographic projections. Effective denoising can facilitate low-dose experiments in producing images with a quality comparable that obtained in high-dose experiments. Likewise, greater time resolution can also be achieved with the aid of effective image denoising procedures. This benchmark exercises complex deep learning techniques on a simulated dataset of size 5GB, consisting of 256x256 images covering noised and denoised (ground truth) datasets.</p><p>The next release of the suite will include several more examples from various other domains with large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Benchmark Focus</head><p>With the full-fledged capability of the framework to log all activities, and with a detailed set of metrics, it is possible for the framework to collect a wide range of performance details that can later be used for deciding the focus. For example, SciMLBench can be used for science benchmarking (to improve scientific results through different ML approaches), application-level benchmarking, and system-level benchmarking (gathering end-to-end performance including IO and network performance). This is made possible thanks to the framework's detailed logging mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Benchmarking Process</head><p>With the framework handling most of the complexity of collecting performance data, there is an excellent opportunity to cover a wide range of metrics (even retrospectively after the benchmarks have been run)</p><p>and have the ability to control the reporting and compliance through controlled runs. However, it is worth noting that although the framework supports and collects a wide range of runtime and science performance aspects, the choice is left to the user to decide the ultimate metrics to be reported. For example, the performance data collected by the framework can be used to generate a final figure of merit to compare different ML models or hardware systems for the same problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Data Curation and Distribution</head><p>SciMLBench employs a carefully designed curation and distribution mechanism: a) Each benchmark has one or more associated datasets. These benchmark-dataset associations are specified through a configuration tool which is not only framework friendly but also interpretable by scientists.</p><p>b) As the scientific datasets are usually large, they are not maintained along with the code. Instead, they are maintained in a separate object storage, whose exact locations are visible to the benchmarking framework and to users. c) Users downloading benchmarks will only download the reference implementations (code) and not the data. This enables fast downloading of the benchmarks and the framework. Since not all datasets will be of interest to everyone, this approach prevents unnecessary downloading of large datasets.</p><p>d) The framework takes the responsibility for downloading datasets on demand or when the user launches the benchmarking process.</p><p>This process is illustrated Figure <ref type="figure" target="#fig_5">4</ref> below. In addition to these basic operational aspects, the benchmark datasets are stored in an object storage to enable better resiliency and repair mechanisms compared to simple file storage. The datasets are also mirrored in several locations to enable the framework to choose the data source closest to the location of the user. The datasets are also regularly backed-up as they constitute valuable digital assets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Extensibility and Coverage</head><p>The overall design of the SciMLBench supports several user scenarios: the ability to add new benchmarks with little knowledge of the framework, ease-of-use, platform interoperability, and ease of customization.</p><p>The overall design relies on two application programming Interface (API) calls which are illustrated with a number of toy examples as well as with some practical examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Outlook and Conclusions</head><p>In this paper, we have highlighted the need for scientific machine learning benchmarks and explained how they differ from conventional benchmarking initiatives. Furthermore, we have highlighted the challenges in developing a suite of useful scientific machine learning benchmarks. These challenges span a number of issues ranging from the intended focus of the benchmarks (Section 2.2) and the benchmarking processes (section 2.3), to challenges around actually developing a useful ML benchmark suite (section 3).</p><p>A useful scientific machine learning suite must therefore go beyond just providing a disparate collection of ML-based scientific applications. The critical aspect here is to provide support for end users not only to</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: AI, Machine Learning and Deep Learning (Adopted from Workshop Report on Basic Research Needs for Scientific Machine Learning 2 )</figDesc><graphic coords="2,88.50,113.28,435.32,208.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>(a) Elements of a Scientific ML Benchmark (b) Building a Scientific ML Benchmark Suite</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Notion of an ML Benchmark and a Benchmark Suite</figDesc><graphic coords="5,304.90,273.21,183.66,106.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Different focus areas of benchmarking</figDesc><graphic coords="5,130.00,529.48,352.16,162.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>is a benchmark suite for end-to-end deep learning training and inference. The end-to-end aspect here is ideal for application and system level benchmarking. Instead of focussing on model accuracy, DawnBench provides common deep learning workloads for quantifying training time, training cost, inference latency, and inference cost across different optimization strategies, model architectures, software frameworks, clouds, and hardware. There are two key benchmarks in the suite -image classification (using the ImageNet and CIFAR10 30 datasets) and Natural Language Processing-based Question Answering (based on the Stanford Question Answering Dataset or SQuAD) that covers both training and inference. DawnBench does not offer a notion of a framework and does not have focus on science. With key metrics around time and cost (for training and inference), DawnBench is predominantly targeted towards end-to-end system and algorithmic performance. Although the datasets are public and open, no distribution mechanisms have been adopted by DawnBench.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Moving the benchmark datasets to the evaluation point</figDesc><graphic coords="16,136.50,113.28,339.20,143.84" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements:</head><p>We would like to thank <rs type="person">Samuel Jackson</rs>, <rs type="person">Kuangdai Leng</rs>, <rs type="person">Keith Butler</rs> and <rs type="person">Juri Papay</rs> from the <rs type="funder">Scientific Machine Learning Research Group</rs> at the <rs type="institution">Rutherford Appleton Laboratory</rs>, <rs type="person">Junqi Yin</rs> and <rs type="person">Aristeidis Tsaris</rs> from <rs type="affiliation">Oak Ridge National Laboratories</rs>, and the <rs type="institution">MLCommons Science Working Group</rs> for valuable discussions. This work was supported by <rs type="funder">Wave 1 of The UKRI Strategic Priorities Fund</rs> under the <rs type="funder">EPSRC</rs> Grant <rs type="grantNumber">EP/T001569/1</rs>, particularly the "AI for Science" theme within that grant, by the <rs type="funder">Alan Turing Institute</rs>, and by the <rs type="funder">Benchmarking for AI for Science at Exascale (BASE)</rs> project under the <rs type="funder">EPSRC</rs> Grant <rs type="grantNumber">EP/V001310/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5sSnTJc">
					<idno type="grant-number">EP/T001569/1</idno>
				</org>
				<org type="funding" xml:id="_AZ98GTN">
					<idno type="grant-number">EP/V001310/1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>be able to effectively use the ML benchmarks but also to enable them to develop new benchmarks and extend the suite for their own purposes.</p><p>Our review covered a number of contemporary efforts for developing ML benchmarks. Only a subset of these efforts has a focus of machine learning for scientific applications. Furthermore, we also noted that nearly all of these initiatives do not consider the problem of the efficient distribution of large datasets.</p><p>The majority of the approaches rely on externally sourced datasets with the implicit assumption that users will take care of the data issues. We concluded the paper with a more detailed review of our SciMLBench initiative which includes a benchmark framework that not only addresses the majority of these concerns but is also designed for easy extensibility.</p><p>The characteristics of these ML benchmark initiatives are summarised in Table <ref type="table">1</ref> below. In qualitatively assessing how far each approach addresses the concerns discussed in Section 2, we have indicated whether they offer no support (none), or partial or questionable support (partial) or fully support the concern (full). To ease readability and understanding, we use a traffic-light coloured glyphs of red, amber and green (, and ) for none, partial or full support, respectively. </p><p>The table shows that the benchmarking community has several issues to address to ensure that the scientific community is equipped with right set of tools to become more efficient in leveraging the use of ML technologies in science.</p><p>Contributions: JT, MS, GF and TH conceptualised the idea of Scientific Benchmarking. JT designed the SciMLBench framework, data architecture and conceptualised the overarching set of features. TH has overseen the overall developmental efforts along with JT, MS and GF. All have contributed towards the writing of the manuscript.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Deep Learning Revolution</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.2172/1478744</idno>
		<title level="m">Workshop Report on Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Machine learning and big scientific data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thiyagalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of Royal Society A</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page">20190054</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">It will change everything&apos;: DeepMind&apos;s AI makes gigantic leap in solving protein structures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Callaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="page" from="203" to="204" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hamiltonian Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpretable, calibrated neural networks for analysis and understanding of inelastic neutron scattering data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thiyagalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Perring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Condensed Matter</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSTOR Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Unsupervised</forename><surname>Autoencoders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Architectures ; Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<title level="m">Proceedings of ICML Workshop on Unsupervised and Transfer Learning</title>
		<meeting>ICML Workshop on Unsupervised and Transfer Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName><surname>Hpc Challenge</surname></persName>
		</author>
		<author>
			<persName><surname>Benchmark</surname></persName>
		</author>
		<title level="m">Encyclopedia of Parallel Computing</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Padua</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="844" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Splash-3: A properly synchronized benchmark suite for contemporary research</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sakalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leonardsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="101" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Bailey</surname></persName>
		</author>
		<title level="m">NAS Parallel Benchmarks. in Encyclopedia of Parallel Computing</title>
		<meeting><address><addrLine>Padua, D</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1254" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">HPL -a Portable Implementation of the High-Performance Linpack Benchmark for Distributed-Memory Computers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Petitet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cleary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TOP500</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luszczek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Parallel Computing</title>
		<meeting><address><addrLine>Padua, D</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2055" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Benchmarking and scalability of machine-learning methods for photometric redshift estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Henghes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pettitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thiyagalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">505</biblScope>
			<biblScope unit="page" from="4847" to="4856" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><surname>Benchmarks</surname></persName>
		</author>
		<title level="m">Encyclopedia of Parallel Computing</title>
		<meeting><address><addrLine>Padua, D</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1886" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="66" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RLBench: The Robot Learning Benchmark &amp; Learning Environment</title>
		<author>
			<persName><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rovick Arrojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Benchmarks</title>
		<ptr target="https://asc.llnl.gov/coral-2-benchmarks" />
		<imprint>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">DAWNBench : An End-to-End Deep Learning Benchmark and Competition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Coleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Benchcouncil</surname></persName>
		</author>
		<author>
			<persName><surname>Bench</surname></persName>
		</author>
		<ptr target="https://www.benchcouncil.org/aibench/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hpc</forename><surname>Mlcommons</surname></persName>
		</author>
		<author>
			<persName><surname>Benchmark</surname></persName>
		</author>
		<ptr target="https://mlcommons.org/en/groups/training-hpc/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">SciMLBench: A Benchmarking Suite for AI for Science</title>
		<author>
			<persName><forename type="first">Jeyan</forename><surname>Thiyagalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kuangdai</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juri</forename><surname>Papay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mallikarjun</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://github.com/stfc-sciml/sciml-bench" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Performance, Energy, and Scalability Analysis and Improvement of Parallel Cancer Deep Learning CANDLE Benchmarks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Conference on Parallel Processing</title>
		<meeting>the 48th International Conference on Parallel Processing</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">CIFAR-10 Dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Canadian Institute for Advanced Research)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
