<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-09-08">8 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-08">8 Sep 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">A75DA72FCD8A237234AB03D712156A71</idno>
					<idno type="arXiv">arXiv:1909.03550v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We use the following mathematical notation in this writeup:</p><p>• Vectors are denoted by boldface lower-case letters such as x ∈ R d . Coordinates of vectors are denoted by underscore notation x i or regular brackets x(i).</p><p>• Matrices are denoted by boldface upper-case letters such as X ∈ R m×n . Their coordinates by X(i, j), or X ij .</p><p>• Functions are denoted by lower case letters f : R d → R.</p><p>• The k-th differential of function f is denoted by</p><p>The gradient is denoted without the superscript, as ∇f .</p><p>• We use the mathcal macro for sets, such as K ⊆ R d .</p><p>• We denote the gradient at point x t as ∇ xt , or simply ∇ t .</p><p>• We denote the global or local optima of functions by x .</p><p>• We denote distance to optimality for iterative algorithms by h t = f (x t ) -f (x ).</p><p>• Euclidean distance to optimality is denoted d t = x t -x .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preface</head><p>This text was written to accompany a series of lectures given at the Machine Learning Summer School Buenos Aires, following a lecture series at the Simons Center for Theoretical Computer Science, Berkeley. It was extended for the course COS 598D -Optimization for Machine Learning, Princeton University, Spring 2019.</p><p>I am grateful to Paula Gradu for proofreading parts of this manuscript. I'm also thankful for the help of the following students and colleagues for corrections and suggestions to this text: Udaya Ghai, John Hallman, Noé Pion, Xinyi Chen. Chapter 1</p><note type="other">iii Preface</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The topic of this lecture series is the mathematical optimization approach to machine learning.</p><p>In standard algorithmic theory, the burden of designing an efficient algorithm for solving a problem at hand is on the algorithm designer. In the decades since in the introduction of computer science, elegant algorithms have been designed for tasks ranging from finding the shortest path in a graph, computing the optimal flow in a network, compressing a computer file containing an image captured by digital camera, and replacing a string in a text document.</p><p>The design approach, while useful to many tasks, falls short of more complicated problems, such as identifying a particular person in an image in bitmap format, or translating text from English to Hebrew. There may very well be an elegant algorithm for the above tasks, but the algorithmic design scheme does not scale.</p><p>As Turing promotes in his paper <ref type="bibr" target="#b85">[83]</ref>, it is potentially easier to teach a computer to learn how to solve a task, rather than teaching it the solution for the particular tasks. In effect, that's what we do at school, or in this lecture series... The machine learning approach to solving problems is to have an automated mechanism for learning an algorithm. Consider the problem of classifying images into two categories: those containing cars and those containing chairs (assuming there are only two types of images in the world). In ML we train (teach) a machine to achieve the desired functionality. The same machine can potentially solve any algorithmic task, and differs from task to task only by a set of parameters that determine the functionality of the machine. This is much like the wires in a computer chip determine its functionality. Indeed, one of the most popular machines are artificial neural networks.</p><p>The mathematical optimization approach to machine learning is to view the process of machine training as an optimization problem. If we let w ∈ R d be the parameters of our machine (a.k.a. model), that are constrained to be in some set K ⊆ R d , and f the function measuring success in mapping examples to their correct label, then the problem we are interested in is described by the mathematical optimization problem of min w∈K f (w) <ref type="bibr">(1.1)</ref> This is the problem that the lecture series focuses on, with particular emphasis on functions that arise in machine learning and have special structure that allows for efficient algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Examples of optimization problems in machine learning 1.Empirical Risk Minimization</head><p>Machine learning problems exhibit special structure. For example, one of the most basic optimization problems in supervised learning is that of fitting a model to data, or examples, also known as the optimization problem of Empirical Risk Minimization (ERM). The special structure of the problems arising in such formulations is separability across different examples into individual losses.</p><p>An example of such formulation is the supervised learning paradigm of linear classification. In this model, the learner is presented with positive and negative examples of a concept. Each example, denoted by a i , is represented in Euclidean space by a d dimensional feature vector. For example, a common representation for emails in the spam-classification problem are binary vectors in Euclidean space, where the dimension of the space is the number of words in the language. The i'th email is a vector a i whose entries are given as ones for coordinates corresponding to words that appear in the email, and zero otherwise 1 . In addition, each example has a label b i ∈ {-1, +1}, corresponding to whether the email has been labeled spam/not spam. The 1 Such a representation may seem naïve at first as it completely ignores the words' order of appearance and their context. Extensions to capture these features are indeed studied in the Natural Language Processing literature.</p><p>goal is to find a hyperplane separating the two classes of vectors: those with positive labels and those with negative labels. If such a hyperplane, which completely separates the training set according to the labels, does not exist, then the goal is to find a hyperplane that achieves a separation of the training set with the smallest number of mistakes.</p><p>Mathematically speaking, given a set of m examples to train on, we seek x ∈ R d that minimizes the number of incorrectly classified examples, i.e. min</p><formula xml:id="formula_0">x∈R d 1 m i∈[m] δ(sign(x a i ) = b i ) (1.2)</formula><p>where sign(x) ∈ {-1, +1} is the sign function, and δ(z) ∈ {0, 1} is the indicator function that takes the value 1 if the condition z is satisfied and zero otherwise.</p><p>The mathematical formulation of the linear classification above is a special case of mathematical programming <ref type="bibr">(1.1)</ref>, in which</p><formula xml:id="formula_1">f (x) = 1 m i∈[m] δ(sign(x a i ) = b i ) = E i∼[m] [ℓ i (x)],</formula><p>where we make use of the expectation operator for simplicity, and denote ℓ i (x) = δ(sign(x a i ) = b i ) for brevity. Since the program above is nonconvex and non-smooth, it is common to take a convex relaxation and replace ℓ i with convex loss functions. Typical choices include the means square error function and the hinge loss, given by</p><formula xml:id="formula_2">ℓ a i ,b i (x) = max{0, 1 -b i • x a i }.</formula><p>This latter loss function in the context of binary classification gives rise to the popular soft-margin SVM problem.</p><p>Another important optimization problem is that of training a deep neural network for binary classification. For example, consider a dataset of images, represented in bitmap format and denoted by {a i ∈ R d |i ∈ [m]}, i.e. m images over n pixels. We would like to find a mapping from images to the two categories, {b i ∈ {0, 1}} of cars and chairs. The mapping is given by a set of parameters of a machine class, such as weights in a neural network, or values of a support vector machine. We thus try to find the optimal parameters that match a i to b, i..e min</p><formula xml:id="formula_3">w∈R d f (w) = E a i ,b i [ℓ(f w (a i ), b i )] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Matrix completion and recommender systems</head><p>Media recommendations have changed significantly with the advent of the Internet and rise of online media stores. The large amounts of data collected allow for efficient clustering and accurate prediction of users' preferences for a variety of media. A well-known example is the so called "Netflix challenge"-a competition of automated tools for recommendation from a large dataset of users' motion picture preferences.</p><p>One of the most successful approaches for automated recommendation systems, as proven in the Netflix competition, is matrix completion. Perhaps the simplest version of the problem can be described as follows.</p><p>The entire dataset of user-media preference pairs is thought of as a partially-observed matrix. Thus, every person is represented by a row in the matrix, and every column represents a media item (movie). For simplicity, let us think of the observations as binary-a person either likes or dislikes a particular movie. Thus, we have a matrix M ∈ {0, 1, * } n×m where n is the number of persons considered, m is the number of movies at our library, and 0/1 and * signify "dislike", "like" and "unknown" respectively:</p><formula xml:id="formula_4">M ij =           </formula><p>0, person i dislikes movie j 1, person i likes movie j * , preference unknown . The natural goal is to complete the matrix, i.e. correctly assign 0 or 1 to the unknown entries. As defined so far, the problem is ill-posed, since any completion would be equally good (or bad), and no restrictions have been placed on the completions.</p><p>The common restriction on completions is that the "true" matrix has low rank. Recall that if a matrix X ∈ R n×m has rank k ≤ ρ = min{n, m} then it can be written as</p><formula xml:id="formula_5">X = U V , U ∈ R n×k , V ∈ R k×m .</formula><p>The intuitive interpretation of this property is that each entry in M can be explained by only k numbers. In matrix completion this means, intuitively, that there are only k factors that determine a persons preference over movies, such as genre, director, actors and so on. Now the simplistic matrix completion problem can be well-formulated as in the following mathematical program. Denote by • OB the Euclidean</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">EXAMPLES OF OPTIMIZATION PROBLEMS IN MACHINE LEARNING7</head><p>norm only on the observed (non starred) entries of M , i.e.,</p><formula xml:id="formula_6">X 2 OB = M ij = * X 2 ij .</formula><p>The mathematical program for matrix completion is given by min</p><formula xml:id="formula_7">X∈R n×m 1 2 X -M 2 OB s.t. rank(X) ≤ k.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3">Learning in Linear Dynamical Systems</head><p>Many learning problems require memory, or the notion of state. This is captured by the paradigm of reinforcement learning, as well of the special case of control in Linear Dynamical Systems (LDS). LDS model a variety of control and robotics problems in continuous variables. The setting is that of a time series, with following parameters:</p><p>1. Inputs to the system, also called controls, denoted by u 1 , ..., u T ∈ R n .</p><p>2. Outputs from the system, also called observations, denoted y 1 , ..., y T ∈ R m .</p><p>3. The state of the system, which may either be observed or hidden, denoted x t , ..., x T ∈ R d .</p><p>4. The system parameters, which are transformations matrices A, B, C, D in appropriate dimensions.</p><p>In the online learning problem of LDS, the learner iteratively observes u t , y t , and has to predict ŷt+1 . The actual y t is generated according to the following dynamical equations:</p><formula xml:id="formula_8">x t+1 = Ax t + Bu t + ε t y t+1 = Cx t+1 + Du t + ζ t ,</formula><p>where ε t , ζ t are noise which is distributed as a Normal random variable.</p><p>Consider an online sequence in which the states are visible. At time t, all system states, inputs and outputs are visible up to this time step. The learner has to predict y t+1 , and only afterwards observes u t+1 .x t+1 , y t+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAPTER 1. INTRODUCTION</head><p>One reasonable way to predict y t+1 based upon past observations is to compute the system, and use the computed transformations to predict. This amounts to solving the following mathematical program: min A,B, Ĉ, D τ &lt;t (x τ +1 -Ax τ + Bu τ ) 2 + (y τ +1 -Ĉx τ + Du τ ) 2 , and then predicting ŷt+1 = Ĉ Â(x t + Bu t ) + Du t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Why is mathematical programming hard?</head><p>The general formulation (1.1) is NP hard. To be more precise, we have to define the computational model we are working in as well as and the access model to the function.</p><p>Before we give a formal proof, the intuition to what makes mathematical optimization hard is simple to state. In one line: it is the fact that global optimality cannot be verified on the basis of local properties.</p><p>Most, if not all, efficient optimization algorithms are iterative and based on a local improvement step. By this nature, any optimization algorithm will terminate when the local improvement is no longer possible, giving rise to a proposed solution. However, the quality of this proposed solution may differ significantly, in general, from that of the global optimum.</p><p>This intuition explains the need for a property of objectives for which global optimality is locally verifiable. Indeed, this is exactly the notion of convexity, and the reasoning above explains its utmost importance in mathematical optimization.</p><p>We now to prove that mathematical programming is NP-hard. This requires discussion of the computational model as well as access model to the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1">The computational model</head><p>The computational model we shall adopt throughout this manuscript is that of a RAM machine equipped with oracle access to the objective function f : R d → R and constraints set K ⊆ R d . The oracle model for the objective function can be one of the following, depending on the specific scenario:</p><formula xml:id="formula_9">1. Value oracle: given a point x ∈ R d , oracle returns f (x) ∈ R. 2. Gradient (first-order) oracle: given a point x ∈ R d , oracle returns the gradient ∇f (x) ∈ R d . 3. k-th order differential oracle: given a point x ∈ R d , oracle returns the tensor ∇ k f (x) ∈ R d k .</formula><p>The oracle model for the constraints set is a bit more subtle. We distinguish between the following oracles:</p><p>1. Membership oracle: given a point x ∈ R d , oracle returns one if</p><p>x ∈ K and zero otherwise.</p><p>2. Separating hyperplane oracle: given a point x ∈ R d , oracle either returns "Yes" if x ∈ K, or otherwise returns a hyperplane h ∈ R d such that h x &gt; 0 and ∀y ∈ K , h y ≤ 0.</p><p>3. Explicit sets: the most common scenario in machine learning is one in which K is "natural", such as the Euclidean ball or hypercube, or the entire Euclidean space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2">Hardness of constrained mathematical programming</head><p>Under this computational model, we can show:</p><p>Lemma 1.1. Mathematical programming is NP-hard, even for a convex continuous constraint set K and quadratic objective functions.</p><p>Informal sketch. Consider the MAX-CUT problem: given a graph G = (V, E), find a subset of the vertices that maximizes the number of edges cut. Let A be the negative adjacency matrix of the graph, i.e.</p><formula xml:id="formula_10">A ij =    -1, (i, j) ∈ E 0, o/w</formula><p>Also suppose that A ii = 0. Next, consider the mathematical program:</p><formula xml:id="formula_11">min f A (x) = 1 4 (x Ax -2|E|) (1.3) x ∞ = 1 .</formula><p>Consider the cut defined by the solution of this program, namely</p><formula xml:id="formula_12">S x = {i ∈ V |x i = 1}, for x = x . Let C(S)</formula><p>denote the size of the cut specified by the subset of edges S ⊆ E. Observe that the expression 1 2 x Ax, is exactly equal to the number of edges that are cut by S x minus the number of edges that are uncut. Thus, we have</p><formula xml:id="formula_13">1 2 xAx = C(S x ) -(E -C(S x )) = 2C(S x ) -E,</formula><p>and hence f (x) = C(S x ). Therefore, maximizing f (x) is equivalent to the MAX-CUT problem, and is thus NP-hard. We proceed to make the constraint set convex and continuous. Consider the mathematical program</p><formula xml:id="formula_14">min {f A (x)} (1.4) x ∞ ≤ 1 .</formula><p>This is very similar to the previous program, but we relaxed the equality to be an inequality, consequently the constraint set is now the hypercube. We now claim that the solution is w.l.o.g. a vertex. To see that, consider y(x) ∈ {±1} d a rounding of x to the corners defined by:</p><formula xml:id="formula_15">y i = y(x) i =    1, w.p. 1+x i 2 -1, w.p. 1-x i 2 Notice that E[y] = x , ∀i = j . E[y i y j ] = x i x j ,</formula><p>and therefore E[y(x) Ay(x)] = x Ax. We conclude that the optimum of mathematical program 1.4 is the same as that for 1.3, and both are NPhard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 2</head><p>Basic concepts in optimization and analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic definitions and the notion of convexity</head><p>We consider minimization of a continuous function over a convex subset of Euclidean space. We mostly consider objective functions that are convex. In later chapters we relax this requirement and consider non-convex functions as well.</p><p>Henceforth, let K ⊆ R d be a bounded convex and compact set in Euclidean space. We denote by D an upper bound on the diameter of K: ∀x, y ∈ K, xy ≤ D.</p><p>A set K is convex if for any x, y ∈ K, all the points on the line segment connecting x and y also belong to K, i.e.,</p><formula xml:id="formula_16">∀α ∈ [0, 1], αx + (1 -α)y ∈ K. A function f : K → R is convex if for any x, y ∈ K ∀α ∈ [0, 1], f (αx + (1 -α)y) ≤ αf (x) + (1 -α)f (y).</formula><p>Gradients and subgradients. The set of all subgradients of a function f at x, denoted ∂f (x), is the set of all vectors u such that</p><formula xml:id="formula_17">f (y) ≥ f (x) + u (y -x).</formula><p>It can be shown that the set of subgradients of a convex function is always non-empty. Suppose f is differentiable, let ∇f (x)[i] = ∂ ∂x i f (x) be the vector of partial derivatives according to the variables, called the gradient. If the gradient ∇f (x) exists, then ∇f (x) ∈ ∂f (x) and ∀y ∈ K</p><formula xml:id="formula_18">f (y) ≥ f (x) + ∇f (x) (y -x).</formula><p>Henceforth we shall denote by ∇f (x) the gradient, if it exists, or any member of ∂f (x) otherwise.</p><p>We denote by G &gt; 0 an upper bound on the norm of the subgradients of f over K, i.e., ∇f (x) ≤ G for all x ∈ K. The existence of Such an upper bound implies that the function f is Lipschitz continuous with parameter</p><formula xml:id="formula_19">G, that is, for all x, y ∈ K |f (x) -f (y)| ≤ G x -y .</formula><p>Smoothness and strong convexity. The optimization and machine learning literature studies special types of convex functions that admit useful properties, which in turn allow for more efficient optimization. Notably, we say that a function is α-strongly convex if</p><formula xml:id="formula_20">f (y) ≥ f (x) + ∇f (x) (y -x) + α 2 y -x 2 . A function is β-smooth if f (y) ≤ f (x) + ∇f (x) (y -x) + β 2 y -x 2 .</formula><p>The latter condition is implied by a slightly stronger Lipschitz condition over the gradients, which is sometimes used to defined smoothness, i.e., ∇f (x) -∇f (y) ≤ β xy .</p><p>If the function is twice differentiable and admits a second derivative, known as a Hessian for a function of several variables, the above conditions are equivalent to the following condition on the Hessian, denoted ∇ 2 f (x):</p><formula xml:id="formula_21">Smoothness: -βI ∇ 2 f (x) βI Strong-convexity: αI ∇ 2 f (x),</formula><p>where A B if the matrix B -A is positive semidefinite.</p><p>When the function f is both α-strongly convex and β-smooth, we say that it is γ-well-conditioned where γ is the ratio between strong convexity and smoothness, also called the condition number of f</p><formula xml:id="formula_22">γ = α β ≤<label>1</label></formula><p>2.1. BASICS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Projections onto convex sets</head><p>In the following algorithms we shall make use of a projection operation onto a convex set, which is defined as the closest point inside the convex set to a given point. Formally,</p><formula xml:id="formula_23">Π K (y) arg min x∈K x -y .</formula><p>When clear from the context, we shall remove the K subscript. It is left as an exercise to the reader to prove that the projection of a given point over a closed non-empty convex set exists and is unique. The computational complexity of projections is a subtle issue that depends much on the characterization of K itself. Most generally, K can be represented by a membership oracle-an efficient procedure that is capable of deciding whether a given x belongs to K or not. In this case, projections can be computed in polynomial time. In certain special cases, projections can be computed very efficiently in near-linear time.</p><p>A crucial property of projections that we shall make extensive use of is the Pythagorean theorem, which we state here for completeness:  We note that there exists a more general version of the Pythagorean theorem. The above theorem and the definition of projections are true and valid not only for Euclidean norms, but for projections according to other distances that are not norms. In particular, an analogue of the Pythagorean theorem remains valid with respect to Bregman divergences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Introduction to optimality conditions</head><p>The standard curriculum of high school mathematics contains the basic facts concerning when a function (usually in one dimension) attains a local optimum or saddle point. The KKT (Karush-Kuhn-Tucker) conditions generalize these facts to more than one dimension, and the reader is referred to the bibliographic material at the end of this chapter for an in-depth rigorous discussion of optimality conditions in general mathematical programming.</p><p>For our purposes, we describe only briefly and intuitively the main facts that we will require henceforth. We separate the discussion into convex and non-convex programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimality for convex optimization</head><p>A local minimum of a convex function is also a global minimum (see exercises at the end of this chapter). We say that x is an ε-approximate optimum if the following holds:</p><formula xml:id="formula_24">∀x ∈ K . f (x ) ≤ f (x) + ε.</formula><p>The generalization of the fact that a minimum of a convex differentiable function on R is a point in which its derivative is equal to zero, is given by the multi-dimensional analogue that its gradient is zero:</p><formula xml:id="formula_25">∇f (x) = 0 ⇐⇒ x ∈ arg min x∈R n f (x).</formula><p>We will require a slightly more general, but equally intuitive, fact for constrained optimization: at a minimum point of a constrained convex function, the inner product between the negative gradient and direction towards the interior of K is non-positive. This is depicted in Figure <ref type="figure" target="#fig_11">2</ref>.2, which shows that -∇f (x ) defines a supporting hyperplane to K. The intuition is that if the inner product were positive, one could improve the objective by moving in the direction of the projected negative gradient. This fact is stated formally in the following theorem.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Solution concepts for non-convex optimization</head><p>We have seen in the previous chapter that mathematical optimization is NPhard. This implies that finding global solutions for non-convex optimization is NP-hard, even for smooth functions over very simple convex domains. We thus consider other trackable concepts of solutions.</p><p>The most common solution concept is that of first-order optimality, a.k.a. saddle-points or stationary points. These are points that satisfy ∇f (x ) = 0.</p><p>Unfortunately, even finding such stationary points is NP-hard. We thus settle for approximate stationary points, which satisify ∇f (x ) ≤ ε. A more stringent notion of optimality we may consider is obtained by looking at the second derivatives. We can require they behave as for global minimum, see figure 2.3. Formally, we say that a point x is a second-order local minimum if it satisfies the two conditions:</p><formula xml:id="formula_26">∇f (x ) ≤ ε , ∇ 2 f (x ) - √ εI.</formula><p>The differences in approximation criteria for first and second derivatives is natural, as we shall explore in non-convex approximation algorithms henceforth. We note that it is possible to further define optimality conditions for higher order derivatives, although this is less useful in the context of machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Potentials for distance to optimality</head><p>When analyzing convergence of gradient methods, it is useful to use potential functions in lieu of function distance to optimality, such as gradient norm and/or Euclidean distance to optimality. The following relationships hold between these quantities.</p><p>Lemma 2.3. The following properties hold for α-strongly-convex functions and/or β-smooth functions over Euclidean space R d .</p><formula xml:id="formula_27">1. α 2 d 2 t ≤ h t 2. h t ≤ β 2 d 2 t 3. 1 2β ∇ t 2 ≤ h t 4. h t ≤ 1 2α ∇ t 2</formula><p>Proof.</p><p>1. h t ≥ α 2 d 2 t : By strong convexity, we have</p><formula xml:id="formula_28">h t = f (x t ) -f (x ) ≥ ∇f (x ) (x t -x ) + α 2 x t -x 2 = α 2 x t -x 2</formula><p>where the last inequality follows since the gradient at the global optimum is zero.</p><formula xml:id="formula_29">2. h t ≤ β 2 d 2 t : By smoothness, h t = f (x t ) -f (x ) ≤ ∇f (x ) (x t -x ) + β 2 x t -x 2 = β 2 x t -x 2</formula><p>where the last inequality follows since the gradient at the global optimum is zero.</p><formula xml:id="formula_30">3. h t ≥ 1 2β ∇ t 2 :</formula><p>Using smoothness, and let</p><formula xml:id="formula_31">x t+1 = x t -η∇ t for η = 1 β , h t = f (x t ) -f (x ) ≥ f (x t ) -f (x t+1 ) ≥ ∇f (x t ) (x t -x t+1 ) -β 2 x t -x t+1 2 = η ∇ t 2 -β 2 η 2 ∇ t 2 = 1 2β ∇ t 2 . 4. h t ≤ 1 2α ∇ t 2 :</formula><p>We have for any pair x, y ∈ R d :</p><formula xml:id="formula_32">f (y) ≥ f (x) + ∇f (x) (y -x) + α 2 x -y 2 ≥ min z∈R d f (x) + ∇f (x) (z -x) + α 2 x -z 2 = f (x) - 1 2α ∇f (x) 2 . by taking z = x - 1 α ∇f (x)</formula><p>In particular, taking x = x t , y = x , we get</p><formula xml:id="formula_33">h t = f (x t ) -f (x ) ≤ 1 2α ∇ t 2 .</formula><p>(2.1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gradient descent and the Polyak stepsize</head><p>The simplest iterative optimization algorithm is gradient descent, as given in Algorithm 1. We analyze GD with the Polyak stepsize, which has the advantage of not depending on the strong convexity and/or smoothness parameters of the objective function.</p><p>Algorithm 1 GD with the Polyak stepsize 1: Input: time horizon T , x 0 2: for t = 0, . . . , T -1 do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Set η t = ht ∇t 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>x t+1 = x t -η t ∇ t 5: end for 6: Return x = arg min xt {f (x t )} To prove convergence bounds, assume ∇ t ≤ G, and define:</p><formula xml:id="formula_34">B T = min Gd 0 √ T , 2βd<label>2</label></formula><p>0 T , 3G 2 αT , βd 2 0 1 -α 4β T Theorem 2.4. (GD with the Polyak Step Size) Algorithm 1 attains the following regret bound after T steps: h(x) = min 0≤t≤T {h t } ≤ B T Theorem 2.4 directly follows from the following lemma. Let 0 ≤ γ ≤ 1, define R T,γ as follows:</p><formula xml:id="formula_35">R T,γ = min Gd 0 √ γT , 2βd 2 0 γT , 3G 2 γαT , βd 2 0 1 -γ α 4β T .</formula><p>Lemma 2.5. For 0 ≤ γ ≤ 1, suppose that a sequence x 0 , . . . x t satisfies:</p><formula xml:id="formula_36">d 2 t+1 ≤ d 2 t -γ h 2 t ∇ t 2 (2.2)</formula><p>then for x as defined in the algorithm, we have:</p><formula xml:id="formula_37">h(x) ≤ R T,γ .</formula><p>Proof. The proof analyzes different cases:</p><p>1. For convex functions with gradient bounded by G,</p><formula xml:id="formula_38">d 2 t+1 -d 2 t ≤ - γh 2 t ∇t 2 ≤ - γh 2 t G 2</formula><p>Summing up over T iterations, and using Cauchy-Schwartz, we have</p><formula xml:id="formula_39">1 T t h t ≤ 1 √ T t h 2 t ≤ G √ γT t (d 2 t -d 2 t+1 ) ≤ Gd 0 √ γT .</formula><p>2. For smooth functions whose gradient is bounded by G, Lemma 2.3 implies:</p><formula xml:id="formula_40">d 2 t+1 -d 2 t ≤ - γh 2 t ∇ t 2 ≤ - γh t 2β .</formula><p>This implies</p><formula xml:id="formula_41">1 T t h t ≤ 2βd 2 0 γT .</formula><p>3. For strongly convex functions, Lemma 2.3 implies:</p><formula xml:id="formula_42">d 2 t+1 -d 2 t ≤ -γ h 2 t ∇ t 2 ≤ -γ h 2 t G 2 ≤ -γ α 2 d 4 t 4G 2 .</formula><p>In other words,</p><formula xml:id="formula_43">d 2 t+1 ≤ d 2 t (1 -γ α 2 d 2 t 4G 2 ) . Defining a t := γ α 2 d 2 t</formula><p>4G 2 , we have:</p><formula xml:id="formula_44">a t+1 ≤ a t (1 -a t ) .</formula><p>This implies that a t ≤ 1 t+1 , which can be seen by induction 1 . The proof is completed as follows 2 :</p><formula xml:id="formula_45">1 T /2 T t=T /2 h 2 t ≤ 2G 2 γT T t=T /2 (d 2 t -d 2 t+1 ) = 2G 2 γT (d 2 T /2 -d 2 T ) = 8G 4 γ 2 α 2 T (a T /2 -a T ) ≤ 9G 4 γ 2 α 2 T 2 .</formula><p>Thus, there exists a t for which h 2 t ≤ 9G 4 γ 2 α 2 T 2 . Taking the square root completes the claim. 4. For both strongly convex and smooth functions:</p><formula xml:id="formula_46">d 2 t+1 -d 2 t ≤ -γ h 2 t ∇ t 2 ≤ - γh t 2β ≤ -γ α 4β d 2 t</formula><p>Thus,</p><formula xml:id="formula_47">h T ≤ βd 2 T ≤ βd 2 0 1 -γ α 4β T .</formula><p>This completes the proof of all cases.</p><p>1 That a0 ≤ 1 follows from Lemma 2.3. For t = 1, a1 ≤ 1 2 since a1 ≤ a0(1 -a0) and 0 ≤ a0 ≤ 1. For the induction step, at ≤ at-1(1 -at-1)</p><formula xml:id="formula_48">≤ 1 t (1 -1 t ) = t-1 t 2 = 1 t+1 ( t 2 -1 t 2 ) ≤ 1 t+1 .</formula><p>2 This assumes T is even. T odd leads to the same constants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Exercises</head><p>1. Write an explicit expression for the gradient and projection operation (if needed) for each of the example optimization problems in the first chapter.</p><p>2. Prove that a differentiable function f (x) : R → R is convex if and only if for any x, y ∈ R it holds that f (x) -f (y) ≤ (x -y)f (x).</p><p>3. Recall that we say that a function f : R n → R has a condition number γ = α/β over K ⊆ R d if the following two inequalities hold for all x, y ∈ K:</p><formula xml:id="formula_49">(a) f (y) ≥ f (x) + (y -x) ∇f (x) + α 2 x -y 2 (b) f (y) ≤ f (x) + (y -x) ∇f (x) + β 2 x -y 2 For matrices A, B ∈ R n×n we denote A B if A -B is positive semidefinite. Prove that if f is twice differentiable and it holds that βI ∇ 2 f (x)</formula><p>αI for any x ∈ K, then the condition number of f over K is α/β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Prove:</head><p>(a) The sum of convex functions is convex. (b) Let f be α 1 -strongly convex and g be α 2 -strongly convex. Then f + g is (α 1 + α 2 )-strongly convex. (c) Let f be β 1 -smooth and g be β 2 -smooth. Then f + g is (β 1 + β 2 )smooth.</p><p>5. Let K ⊆ R d be closed, compact, non-empty and bounded. Prove that a necessary and sufficient condition for ΠK(x) to be a singleton, that is for | ΠK(x)| = 1, is for K to be convex.</p><p>6. Prove that for convex functions, ∇f (x) ∈ ∂f (x), that is, the gradient belongs to the subgradient set.</p><p>7. Let f (x) : R n → R be a convex differentiable function and K ⊆ R n be a convex set. Prove that x ∈ K is a minimizer of f over K if and only if for any y ∈ K it holds that (y -x ) ∇f (x ) ≥ 0.</p><p>8. Consider the n-dimensional simplex</p><formula xml:id="formula_50">∆ n = {x ∈ R n | n i=1 x i = 1, x i ≥ 0 , ∀i ∈ [n]}.</formula><p>Give an algorithm for computing the projection of a point x ∈ R n onto the set ∆ n (a near-linear time algorithm exists).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Bibliographic remarks</head><p>The reader is referred to dedicated books on convex optimization for much more in-depth treatment of the topics surveyed in this background chapter.</p><p>For background in convex analysis see the texts <ref type="bibr" target="#b13">[11,</ref><ref type="bibr" target="#b70">68]</ref>. The classic textbook <ref type="bibr" target="#b14">[12]</ref> gives a broad introduction to convex optimization with numerous applications. For an adaptive analysis of gradient descent with the Polyak stepsize see <ref type="bibr" target="#b35">[33]</ref>.</p><p>Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Gradient Descent</head><p>The most important optimization algorithm in the context of machine learning is stochastic gradient descent (SGD), especially for non-convex optimization and in the context of deep neural networks. In this chapter we spell out the algorithm and analyze it up to tight finite-time convergence rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training feedforward neural networks</head><p>Perhaps the most common optimization problem in machine learning is that of training feedforward neural networks. In this problem, we are given a set of labelled data points, such as labelled images or text. Let {x i , y i } be the set of labelled data points, also called the training data.</p><p>The goal is to fit the weights of an artificial neural network in order to minimize the loss over the data. Mathematically, the feedforward network is a given weighted a-cyclic graph G = (V, E, W ). Each node v is assigned an activation function, which we assume is the same function for all nodes, denoted σ : R d → R. Using a biological analogy, an activation function σ is a function that determines how strongly a neuron (i.e. a node) 'fires' for a given input by mapping the result into the desired range, usually [0, 1] or [-1, 1] . Some popular examples include:</p><formula xml:id="formula_51">• Sigmoid: σ(x) = 1 1+e -x</formula><p>• Hyperbolic tangent: tanh(x) = e x -e -x e x +e -x</p><p>• Rectified linear unit: ReLU (x) = max{0, x} (currently the most widely used of the three)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAPTER 3. STOCHASTIC GRADIENT DESCENT</head><p>The inputs to the input layer nodes is a given data point, while the inputs to to all other nodes are the output of the nodes connected to it. We denote by ρ(v) the set of input neighbors to node v. The top node output is the input to the loss function, which takes its "prediction" and the true label to form a loss.</p><p>For an input node v, its output as a function of the graph weights and input example x (of dimension d), which we denote as</p><formula xml:id="formula_52">v(W, x) = σ i∈d W v,i x i</formula><p>The output of an internal node v is a function of its inputs u ∈ ρ(v) and a given example x, which we denote as</p><formula xml:id="formula_53">v(W, x) = σ   u∈ρ(v) W uv u(W, x)  </formula><p>If we denote the top node as v 1 , then the loss of the network over data point (x i , y i ) is given by ℓ(v 1 (W, x i ), y i ).</p><p>The objective function becomes</p><formula xml:id="formula_54">f (W ) = E x i ,y i ℓ(v 1 (W, x i ), y i )</formula><p>For most commonly-used activation and loss functions, the above function is non-convex. However, it admits important computational properties. The most significant property is given in the following lemma. The proof of this lemma is left as an exercise, but we sketch the main ideas. For every variable W uv , we have by linearity of expectation that</p><formula xml:id="formula_55">∂ ∂W uv f (W ) = E x i ,y i ∂ ∂W uv ℓ(v 1 (W, x i ), y i ) .</formula><p>Next, using the chain rule, we claim that it suffices to know the partial derivatives of each node w.r.t. its immediate daughters. To see this, let us write the derivative w.r.t. W uv using the chain rule:</p><formula xml:id="formula_56">∂ ∂W uv ℓ(v 1 (W, x i ), y i ) = ∂ℓ ∂v 1 • ∂v 1 ∂W uv = ∂ℓ ∂v 1 • v 2 ∈ρ(v 1 ) ∂v 1 ∂v 2 • ∂v j ∂W uv = ... = ∂ℓ ∂v 1 • v 2 ∈ρ(v 1 ) ∂v 1 ∂v 2 • ... • v k j ∈ρ(v k-1 ) • ∂v k ∂W uv</formula><p>We conclude that we only need to obtain the E partial derivatives along the edges in order to compute all partial derivatives of the function. The actual product at each node can be computed by a dynamic program in linear time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gradient descent for smooth optimization</head><p>Before moving to stochastic gradient descent, we consider its deterministic counterpart: gradient descent, in the context of smooth non-convex optimization. Our notion of solution is a point with small gradient, i.e. ∇f (x) ≤ ε.</p><p>As we prove below, this requires O(</p><p>1 ε 2 ) iterations, each requiring one gradient computation. Recall that gradients can be computed efficiently, linear in the number of edges, in feed forward neural networks. Thus, the time to obtain a ε-approximate solution becomes O( |E|m ε 2 ) for neural networks with E edges and over m examples. Algorithm 2 Gradient descent 1: Input: f , T , initial point x 1 ∈ K, sequence of step sizes {η t } 2: for t = 1 to T do 3:</p><p>Let y t+1 = x t -η t ∇f (x t ), x t+1 = ΠK (y t+1 ) 4: end for 5: return x T +1</p><p>Although the choice of η t can make a difference in practice, in theory the convergence of the vanilla GD algorithm is well understood and given in the following theorem. Below we assume that the function is bounded such that |f (x)| ≤ M . Theorem 3.2. For unconstrained minimization of β-smooth functions and η t = 1 β , GD Algorithm 2 converges as</p><formula xml:id="formula_57">1 T t ∇ t 2 ≤ 4M β T .</formula><p>Proof. Denote by ∇ t the shorthand for ∇f (x t ), and</p><formula xml:id="formula_58">h t = f (x t ) -f (x * ).</formula><p>The Descent Lemma is given in the following simple equation,</p><formula xml:id="formula_59">h t+1 -h t = f (x t+1 ) -f (x t ) ≤ ∇ t (x t+1 -x t ) + β 2 x t+1 -x t 2 β-smoothness = -η t ∇ t 2 + β 2 η 2 t ∇ t 2 algorithm defn. = - 1 2β ∇ t 2 choice of η t = 1 β</formula><p>Thus, summing up over T iterations, we have</p><formula xml:id="formula_60">1 2β T t=1 ∇ t 2 ≤ t (h t -h t+1 ) = h 1 -h T +1 ≤ 2M</formula><p>For convex functions, the above theorem implies convergence in function value due to the following lemma, Lemma 3.3. A convex function satisfies</p><formula xml:id="formula_61">h t ≤ D ∇ t ,</formula><p>and an α-strongly convex function satisfies</p><formula xml:id="formula_62">h t ≤ 1 2α ∇ t 2 .</formula><p>Proof. The gradient upper bound for convex functions gives</p><formula xml:id="formula_63">h t ≤ ∇ t (x * -x t ) ≤ D ∇ t</formula><p>The strongly convex case appears in Lemma 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Stochastic gradient descent</head><p>In the context of training feed forward neural networks, the key idea of Stochastic Gradient Descent is to modify the updates to be:</p><formula xml:id="formula_64">W t+1 = W t -η ∇ t (3.1)</formula><p>where</p><formula xml:id="formula_65">∇ t is a random variable with E[ ∇ t ] = ∇ f (W t ) and bounded second moment E[ ∇ t 2 2 ] ≤ σ 2 .</formula><p>Luckily, getting the desired ∇ t random variable is easy in the posed problem since the objective function is already in expectation form so:</p><formula xml:id="formula_66">∇f(W ) = ∇ E x i ,y i [ℓ(v 1 (W, x i ), y i )] = E x i ,y i [∇ℓ(v 1 (W, x i ), y i )].</formula><p>Therefore, at iteration t we can take ∇ t = ∇ℓ(v 1 (W, x i ), y i ) where i ∈ {1, ..., m} is picked uniformly at random. Based on the observation above, choosing ∇ t this way preserves the desired expectation. So, for each iteration we only compute the gradient w.r.t. to one random example instead of the entire dataset, thereby drastically improving performance for every step. It remains to analyze how this impacts convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Stochastic gradient descent</head><p>1: Input: f , T , initial point x 1 ∈ K, sequence of step sizes {η t } 2: for t = 1 to T do 3:</p><p>Let y t+1 = x t -η t ∇f (x t ), x t+1 = ΠK (y t+1 ) 4: end for 5: return x T +1 Theorem 3.4. For unconstrained minimization of β-smooth functions and</p><formula xml:id="formula_67">η t = η = M βσ 2 T , SGD Algorithm 3 converges as E 1 T t ∇ t 2 ≤ 2 M βσ 2 T .</formula><p>Proof. Denote by ∇ t the shorthand for ∇f (x t ), and</p><formula xml:id="formula_68">h t = f (x t ) -f (x * ).</formula><p>The stochastic descent lemma is given in the following equation,</p><formula xml:id="formula_69">E[h t+1 -h t ] = E[f (x t+1 ) -f (x t )] ≤ E[∇ t (x t+1 -x t ) + β 2 x t+1 -x t 2 ] β-smoothness = -E[η∇ t ∇t ] + β 2 η 2 E ∇t 2 algorithm defn. = -η ∇ t 2 + β 2 η 2 σ 2 variance bound.</formula><p>Thus, summing up over T iterations, we have for</p><formula xml:id="formula_70">η = M βσ 2 T , E 1 T T t=1 ∇ t 2 ≤ 1 T η t E [h t -h t+1 ] + η β 2 σ 2 ≤ M T η + η β 2 σ 2 = M βσ 2 T + 1 2 M βσ 2 T ≤ 2 M βσ 2 T .</formula><p>We thus conclude that O( 1 ε 4 ) iterations are needed to find a point with ∇f (x) ≤ ε, as opposed to O( 1 ε 2 ). However, each iteration takes O(|E|) time, instead of O(|E|m) time for gradient descent. This is why SGD is one of the most useful algorithms in machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bibliographic remarks</head><p>For in depth treatment of backpropagation and the role of deep neural networks in machine learning the reader is referred to <ref type="bibr" target="#b27">[25]</ref>.</p><p>For detailed rigorous convergence proofs of first order methods, see lecture notes by Nesterov <ref type="bibr" target="#b59">[57]</ref> and Nemirovskii <ref type="bibr" target="#b55">[53,</ref><ref type="bibr" target="#b56">54]</ref>, as well as the recent text <ref type="bibr" target="#b15">[13]</ref>.</p><p>Chapter 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization and Non-Smooth Optimization</head><p>In previous chapter we have introduced the framework of mathematical optimization within the context of machine learning. We have described the mathematical formulation of several machine learning problems, notably training neural networks, as optimization problems. We then described as well as analyzed the most useful optimization method to solve such formulations: stochastic gradient descent.</p><p>However, several important questions arise:</p><p>1. SGD was analyzed for smooth functions. Can we minimize non-smooth objectives? In this chapter we address the first two, and devote the rest of this manuscript/course to the last question.</p><p>How many examples are needed to learn a certain concept? This is a fundamental question of statistical/computational learning theory that has been studied for decades (see end of chapter for bibliographic references).</p><p>The classical setting of learning from examples is statistical. It assumes examples are drawn i.i.d from a fixed, arbitrary and unknown distribution. The mathematical optimization formulations that we have derived for the ERM problem assume that we have sufficiently many examples, such that optimizing a certain predictor/neural-network/machine on them will result in a solution that is capable of generalizing to unseen examples. The number of examples needed to generalize is called the sample complexity of the problem, and it depends on the concept we are learning as well as the hypothesis class over which we are trying to optimize.</p><p>There are dimensionality notions in the literature, notably the VCdimension and related notions, that give precise bounds on the sample complexity for various hypothesis classes. In this text we take an algorithmic approach, which is also deterministic. Instead of studying sample complexity, which is non-algorithmic, we study algorithms for regret minimization. We will show that they imply generalization for a broad class of machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A note on non-smooth optimization</head><p>Minimization of a function that is both non-convex and non-smooth is in general hopeless, from an information theoretic perspective. The following image explains why. The depicted function on the interval [0, 1] has a single local/global minimum, and if the crevasse is narrow enough, it cannot be found by any method other than extensive brute-force search, which can take arbitrarily long. Since non-convex and non-smooth optimization is hopeless, in the context of non-smooth functions we only consider convex optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Minimizing Regret</head><p>The setting we consider for the rest of this chapter is that of online (convex) optimization. In this setting a learner iteratively predicts a point x t ∈ K in a convex set K ⊆ R d , and then receives a cost according to an adversarially chosen convex function f t ∈ F from family F.</p><p>The goal of the algorithms introduced in this chapter is to minimize worst-case regret, or difference between total cost and that of best point in hindsight:</p><formula xml:id="formula_71">regret = sup f 1 ,...,f T ∈F T t=1 f t (x t ) -min x∈K T t=1 f t (x) .</formula><p>In order to compare regret to optimization error it is useful to consider the average regret, or regret/T . Let xT = 1 T T t=1 x t be the average decision. If the functions f t are all equal to a single function f :</p><formula xml:id="formula_72">K → R, then Jensen's inequality implies that f (x T ) converges to f (x ) if the average regret is vanishing, since f (x T ) -f (x ) ≤ 1 T T t=1 [f (x t ) -f (x )] = regret T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Regret implies generalization</head><p>Statistical learning theory for learning from examples postulates that examples from a certain concept are sampled i.i.d. from a fixed and unknown distribution. The learners' goal is to choose a hypothesis from a certain hypothesis class that can generalize to unseen examples. More formally, let D be a distribution over labelled examples {a</p><formula xml:id="formula_73">i ∈ R d , b i ∈ R} ∼ D. Let H = {x} , x : R d →</formula><p>R be a hypothsis class over which we are trying to learn (such as linear separators, deep neural networks, etc.). The generalization error of a hypothesis is the expected error of a hypothesis over randomly chosen examples according to a given loss function ℓ : R × R → R, which is applied to the prediction of the hypothesis and the true label, ℓ(x(a i ), b i ). Thus,</p><formula xml:id="formula_74">error(x) = E a i ,b i ∼D [ℓ(x(a i ), b i )].</formula><p>An algorithm that attains sublinear regret over the hypothesis class H, w.r.t. loss functions given by f t (x) = f a,b (x) = ℓ(x(a), b), gives rise to a generalizing hypothesis as follows.</p><p>Lemma 4.1. Let x = x t for t ∈ [T ] be chose uniformly at random from {x 1 , ..., x T }.Then, with expectation taken over random choice of x as well as choices of</p><formula xml:id="formula_75">f t ∼ D, E[error(x)] ≤ E[error(x * )] + regret T Proof. By random choice of x, we have E[f (x)] = E 1 T t f (x t )</formula><p>Using the fact that f t ∼ D, we have</p><formula xml:id="formula_76">E[error(x)] = E f ∼D [f (x)] = E ft [ 1 T t f t (x t )] ≤ E ft [ 1 T t f t (x )] + regret T = E f [f (x )] + regret T = E f [error(x )] + regret T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Online gradient descent</head><p>Perhaps the simplest algorithm that applies to the most general setting of online convex optimization is online gradient descent. This algorithm is an online version of standard gradient descent for offline optimization we have seen in the previous chapter. Pseudo-code for the algorithm is given in Algorithm 4, and a conceptual illustration is given in Figure <ref type="figure">4</ref>.2.</p><p>In each iteration, the algorithm takes a step from the previous point in the direction of the gradient of the previous cost. This step may result in a point outside of the underlying convex set. In such cases, the algorithm projects the point back to the convex set, i.e. finds its closest point in the convex set. Despite the fact that the next cost function may be completely different than the costs observed thus far, the regret attained by the algorithm is sublinear. This is formalized in the following theorem (recall the definition of G and D from the previous chapter).  Play x t and observe cost f t (x t ).</p><formula xml:id="formula_77">regret T = T t=1 f t (x t ) -min x ∈K T t=1 f t (x ) ≤ 3GD √ T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Update and project:</p><formula xml:id="formula_78">y t+1 = x t -η t ∇f t (x t ) x t+1 = Π K (y t+1 ) 5: end for Proof. Let x ∈ arg min x∈K T t=1 f t (x). Define ∇ t ∇f t (x t ). By convexity f t (x t ) -f t (x ) ≤ ∇ t (x t -x ) (4.1)</formula><p>We first upper-bound ∇ t (x t -x ) using the update rule for x t+1 and Theorem 2.1 (the Pythagorean theorem):</p><formula xml:id="formula_79">x t+1 -x 2 = Π K (x t -η t ∇ t ) -x 2 ≤ x t -η t ∇ t -x 2 (4.2)</formula><p>Hence,</p><formula xml:id="formula_80">x t+1 -x 2 ≤ x t -x 2 + η 2 t ∇ t 2 -2η t ∇ t (x t -x ) 2∇ t (x t -x ) ≤ x t -x 2 -x t+1 -x 2 η t + η t G 2 (4.3)</formula><p>Summing (4.1) and ( <ref type="formula">4</ref>.3) from t = 1 to T , and setting</p><formula xml:id="formula_81">η t = D G √ t (with 1 η 0 0): 2 T t=1 f t (x t ) -f t (x ) ≤ 2 T t=1 ∇ t (x t -x ) ≤ T t=1 x t -x 2 -x t+1 -x 2 η t + G 2 T t=1 η t ≤ T t=1 x t -x 2 1 η t - 1 η t-1 + G 2 T t=1 η t 1 η 0 0, x T +1 -x * 2 ≥ 0 ≤ D 2 T t=1 1 η t - 1 η t-1 + G 2 T t=1 η t ≤ D 2 1 η T + G 2 T t=1 η t telescoping series ≤ 3DG √ T .</formula><p>The last inequality follows since</p><formula xml:id="formula_82">η t = D G √ t and T t=1 1 √ t ≤ 2 √ T .</formula><p>The online gradient descent algorithm is straightforward to implement, and updates take linear time given the gradient. However, there is a projection step which may take significantly longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Lower bounds</head><p>Theorem 4.3. Any algorithm for online convex optimization incurs Ω(DG √ T ) regret in the worst case. This is true even if the cost functions are generated from a fixed stationary distribution.</p><p>We give a sketch of the proof; filling in all details is left as an exercise at the end of this chapter.</p><p>Consider an instance of OCO where the convex set K is the n-dimensional hypercube, i.e.</p><formula xml:id="formula_83">K = {x ∈ R n , x ∞ ≤ 1}.</formula><p>There are 2 n linear cost functions, one for each vertex v ∈ {±1} n , defined as</p><formula xml:id="formula_84">∀v ∈ {±1} n , f v (x) = v x.</formula><p>Notice that both the diameter of K and the bound on the norm of the cost function gradients, denoted G, are bounded by</p><formula xml:id="formula_85">D ≤ n i=1 2 2 = 2 √ n, G = n i=1 (±1) 2 = √ n</formula><p>The cost functions in each iteration are chosen at random, with uniform probability, from the set {f v , v ∈ {±1} n }. Denote by v t ∈ {±1} n the vertex chosen in iteration t, and denote f t = f vt . By uniformity and independence, for any t and</p><formula xml:id="formula_86">x t chosen online, E vt [f t (x t )] = E vt [v t x t ] = 0. However, E v 1 ,...,v T min x∈K T t=1 f t (x) = E   min x∈K i∈[n] T t=1 v t (i) • x i   = n E - T t=1 v t (1) i.i.d. coordinates = -Ω(n √ T ).</formula><p>The last equality is left as exercise 3.</p><p>The facts above nearly complete the proof of Theorem 4.3; see the exercises at the end of this chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Online gradient descent for strongly convex functions</head><p>The first algorithm that achieves regret logarithmic in the number of iterations is a twist on the online gradient descent algorithm, changing only the step size. The following theorem establishes logarithmic bounds on the regret if the cost functions are strongly convex.</p><p>Theorem 4.4. For α-strongly convex loss functions, online gradient descent with step sizes η t = 1 αt achieves the following guarantee for all</p><formula xml:id="formula_87">T ≥ 1 regret T ≤ T t=1 1 αt ∇ t 2 ≤ G 2 2α (1 + log T ). Proof. Let x ∈ arg min x∈K T t=1 f t (x). Recall the definition of regret regret T = T t=1 f t (x t ) - T t=1 f t (x ).</formula><p>Define ∇ t ∇f t (x t ). Applying the definition of α-strong convexity to the pair of points x t ,x * , we have</p><formula xml:id="formula_88">2(f t (x t ) -f t (x )) ≤ 2∇ t (x t -x ) -α x -x t 2 . (4.4)</formula><p>We proceed to upper-bound ∇ t (x t -x ). Using the update rule for x t+1 and the Pythagorean theorem 2.1, we get</p><formula xml:id="formula_89">x t+1 -x 2 = Π K (x t -η t ∇ t ) -x 2 ≤ x t -η t ∇ t -x 2 .</formula><p>Hence,</p><formula xml:id="formula_90">x t+1 -x 2 ≤ x t -x 2 + η 2 t ∇ t 2 -2η t ∇ t (x t -x ) and 2∇ t (x t -x ) ≤ x t -x 2 -x t+1 -x 2 η t + η t ∇ t 2 . (4.5)</formula><p>Summing (4.5) from t = 1 to T , setting η t = 1 αt (define 1 η 0 0), and combining with (4.4), we have:</p><formula xml:id="formula_91">2 T t=1 (f t (x t ) -f t (x )) ≤ T t=1 x t -x 2 1 η t - 1 η t-1 -α + T t=1 η t ∇ t 2 since 1 η 0 0, x T +1 -x * 2 ≥ 0 = 0 + T t=1 1 αt ∇ t 2 ≤ G 2 α (1 + log T )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Online Gradient Descent implies SGD</head><p>In this section we notice that OGD and its regret bounds imply the SGD bounds we have studied in the previous chapter. The main advantage are the guarantees for non-smooth stochastic optimization, and constrained optimization.</p><p>Recall that in stochastic optimization, the optimizer attempts to minimize a convex function over a convex domain as given by the mathematical program:</p><formula xml:id="formula_92">min x∈K f (x).</formula><p>However, unlike standard offline optimization, the optimizer is given access to a noisy gradient oracle, defined by</p><formula xml:id="formula_93">O(x) ∇x s.t. E[ ∇x ] = ∇f (x) , E[ ∇x 2 ] ≤ G 2</formula><p>That is, given a point in the decision set, a noisy gradient oracle returns a random vector whose expectation is the gradient at the point and whose second moment is bounded by G 2 . We will show that regret bounds for OCO translate to convergence rates for stochastic optimization. As a special case, consider the online gradient descent algorithm whose regret is bounded by</p><formula xml:id="formula_94">regret T = O(DG √ T )</formula><p>Applying the OGD algorithm over a sequence of linear functions that are defined by the noisy gradient oracle at consecutive points, and finally returning the average of all points along the way, we obtain the stochastic gradient descent algorithm, presented in Algorithm 5.</p><p>Algorithm 5 stochastic gradient descent</p><formula xml:id="formula_95">1: Input: f , K, T , x 1 ∈ K, step sizes {η t } 2: for t = 1 to T do 3:</formula><p>Let ∇t = O(x t ) and define:</p><formula xml:id="formula_96">f t (x) ∇t , x 4:</formula><p>Update and project:</p><formula xml:id="formula_97">y t+1 = x t -η t ∇t x t+1 = Π K (y t+1 ) 5: end for 6: return xT 1 T T t=1 x t</formula><p>Theorem 4.5. Algorithm 5 with step sizes</p><formula xml:id="formula_98">η t = D G √ t guarantees E[f (x T )] ≤ min x ∈K f (x ) + 3GD √ T Proof.</formula><p>By the regret guarantee of OGD, we have</p><formula xml:id="formula_99">E[f (x T )] -f (x ) ≤ E[ 1 T t f (x t )] -f (x ) convexity of f (Jensen) ≤ 1 T E[ t ∇f (x t ), x t -x ] convexity again = 1 T E[ t ∇t , x t -x ] noisy gradient estimator = 1 T E[ t f t (x t ) -f t (x )] Algorithm 5, line<label>(3)</label></formula><formula xml:id="formula_100">≤ regret T T definition ≤ 3GD √ T theorem 4.2</formula><p>It is important to note that in the proof above, we have used the fact that the regret bounds of online gradient descent hold against an adaptive adversary. This need arises since the cost functions f t defined in Algorithm 5 depend on the choice of decision x t ∈ K.</p><p>In addition, the careful reader may notice that by plugging in different step sizes (also called learning rates) and applying SGD to strongly convex functions, one can attain Õ(1/T ) convergence rates. Details of this derivation are left as exercise 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Exercises</head><p>1. Prove that SGD for a strongly convex function can, with appropriate parameters η t , converge as Õ( 1 T ). You may assume that the gradient estimators have Euclidean norms bounded by the constant G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Design an OCO algorithm that attains the same asymptotic regret</head><p>bound as OGD, up to factors logarithmic in G and D, without knowing the parameters G and D ahead of time.</p><p>3. In this exercise we prove a tight lower bound on the regret of any algorithm for online convex optimization.</p><p>(a) For any sequence of T fair coin tosses, let N h be the number of head outcomes and N t be the number of tails. Give an asymptotically tight upper and lower bound on</p><formula xml:id="formula_101">E[|N h -N t |] (i.e.</formula><p>, order of growth of this random variable as a function of T , up to multiplicative and additive constants).</p><p>(b) Consider a 2-expert problem, in which the losses are inversely correlated: either expert one incurs a loss of one and the second expert zero, or vice versa. Use the fact above to design a setting in which any experts algorithm incurs regret asymptotically matching the upper bound.</p><p>(c) Consider the general OCO setting over a convex set K. Design a setting in which the cost functions have gradients whose norm is bounded by G, and obtain a lower bound on the regret as a function of G, the diameter of K, and the number of game iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Bibliographic remarks</head><p>The OCO framework was introduced by Zinkevich in <ref type="bibr" target="#b90">[87]</ref>, where the OGD algorithm was introduced and analyzed. Precursors to this algorithm, albeit for less general settings, were introduced and analyzed in <ref type="bibr" target="#b49">[47]</ref>. Logarithmic regret algorithms for Online Convex Optimization were introduced and analyzed in <ref type="bibr" target="#b34">[32]</ref>. For more detailed exposition on this prediction framework and its applications see <ref type="bibr" target="#b33">[31]</ref>.</p><p>The SGD algorithm dates back to Robbins and Monro <ref type="bibr" target="#b69">[67]</ref>. Application of SGD to soft-margin SVM training was explored in <ref type="bibr" target="#b76">[74]</ref>. Tight convergence rates of SGD for strongly convex and non-smooth functions were only recently obtained in <ref type="bibr" target="#b37">[35]</ref>, <ref type="bibr" target="#b64">[62]</ref>, <ref type="bibr" target="#b78">[76]</ref>.</p><p>Chapter 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization</head><p>In this chapter we consider a generalization of the gradient descent called by different names in different communities (such as mirrored-descent, or regularized-follow-the-leader). The common theme of this generalization is called Regularization, a concept that is founded in generalization theory. Since this course focuses on optimization rather than generalization, we shall refer the reader to the generalization aspect of regularization, and focus hereby on optimization algorithms.</p><p>We start by motivating this general family of methods using the fundamental problem of decision theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Motivation: prediction from expert advice</head><p>Consider the following fundamental iterative decision making problem:</p><p>At each time step t = 1, 2, . . . , T , the decision maker faces a choice between two actions A or B (i.e., buy or sell a certain stock). The decision maker has assistance in the form of N "experts" that offer their advice. After a choice between the two actions has been made, the decision maker receives feedback in the form of a loss associated with each decision. For simplicity one of the actions receives a loss of zero (i.e., the "correct" decision) and the other a loss of one.</p><p>We make the following elementary observations:</p><p>1. A decision maker that chooses an action uniformly at random each iteration, trivially attains a loss of T 2 and is "correct" 50% of the time.</p><p>2. In terms of the number of mistakes, no algorithm can do better in the worst case! In a later exercise, we will devise a randomized setting in which the expected number of mistakes of any algorithm is at least T 2 .</p><p>We are thus motivated to consider a relative performance metric: can the decision maker make as few mistakes as the best expert in hindsight? The next theorem shows that the answer in the worst case is negative for a deterministic decision maker.</p><formula xml:id="formula_102">Theorem 5.1. Let L ≤ T</formula><p>2 denote the number of mistakes made by the best expert in hindsight. Then there does not exist a deterministic algorithm that can guarantee less than 2L mistakes.</p><p>Proof. Assume that there are only two experts and one always chooses option A while the other always chooses option B. Consider the setting in which an adversary always chooses the opposite of our prediction (she can do so, since our algorithm is deterministic). Then, the total number of mistakes the algorithm makes is T . However, the best expert makes no more than T  2 mistakes (at every iteration exactly one of the two experts is mistaken). Therefore, there is no algorithm that can always guarantee less than 2L mistakes. This observation motivates the design of random decision making algorithms, and indeed, the OCO framework gracefully models decisions on a continuous probability space. Henceforth we prove Lemmas 5.3 and 5.4 that show the following: Theorem 5.2. Let ε ∈ (0, 1  2 ). Suppose the best expert makes L mistakes. Then:</p><p>1. There is an efficient deterministic algorithm that can guarantee less than 2(1 + ε)L + 2 log N ε mistakes;</p><p>2. There is an efficient randomized algorithm for which the expected number of mistakes is at most (1 + ε)L + log N ε .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">The weighted majority algorithm</head><p>The weighted majority (WM) algorithm is intuitive to describe: each expert i is assigned a weight W t (i) at every iteration t. Initially, we set W 1 (i) = 1 for all experts i ∈ [N ]. For all t ∈ [T ] let S t (A), S t (B) ⊆ [N ] be the set of experts that choose A (and respectively B) at time t. Define,</p><formula xml:id="formula_103">W t (A) = i∈St(A) W t (i) W t (B) = i∈St(B) W t (i)</formula><p>and predict according to</p><formula xml:id="formula_104">a t = A if W t (A) ≥ W t (B) B otherwise.</formula><p>Next, update the weights W t (i) as follows:</p><formula xml:id="formula_105">W t+1 (i) = W t (i) if expert i was correct W t (i)(1 -ε) if expert i was wrong ,</formula><p>where ε is a parameter of the algorithm that will affect its performance. This concludes the description of the WM algorithm. We proceed to bound the number of mistakes it makes.</p><p>Lemma 5.3. Denote by M t the number of mistakes the algorithm makes until time t, and by M t (i) the number of mistakes made by expert i until time t. Then, for any expert i ∈ [N ] we have</p><formula xml:id="formula_106">M T ≤ 2(1 + ε)M T (i) + 2 log N ε .</formula><p>We can optimize ε to minimize the above bound. The expression on the right hand side is of the form f (x) = ax + b/x, that reaches its minimum at x = b/a. Therefore the bound is minimized at ε = log N/M T (i).</p><p>Using this optimal value of ε, we get that for the best expert i</p><formula xml:id="formula_107">M T ≤ 2M T (i ) + O M T (i ) log N .</formula><p>Of course, this value of ε cannot be used in advance since we do not know which expert is the best one ahead of time (and therefore we do not know the value of M T (i )). However, we shall see later on that the same asymptotic bound can be obtained even without this prior knowledge.</p><p>Let us now prove Lemma 5.3.</p><p>Proof. Let Φ t = N i=1 W t (i) for all t ∈ [T ], and note that Φ 1 = N . Notice that Φ t+1 ≤ Φ t . However, on iterations in which the WM algorithm erred, we have</p><formula xml:id="formula_108">Φ t+1 ≤ Φ t (1 - ε 2 ),</formula><p>the reason being that experts with at least half of total weight were wrong (else WM would not have erred), and therefore</p><formula xml:id="formula_109">Φ t+1 ≤ 1 2 Φ t (1 -ε) + 1 2 Φ t = Φ t (1 - ε 2 ).</formula><p>From both observations,</p><formula xml:id="formula_110">Φ t ≤ Φ 1 (1 - ε 2 ) Mt = N (1 - ε 2 ) Mt .</formula><p>On the other hand, by definition we have for any expert i that</p><formula xml:id="formula_111">W T (i) = (1 -ε) M T (i) .</formula><p>Since the value of W T (i) is always less than the sum of all weights Φ T , we conclude that</p><formula xml:id="formula_112">(1 -ε) M T (i) = W T (i) ≤ Φ T ≤ N (1 - ε 2 ) M T .</formula><p>Taking the logarithm of both sides we get</p><formula xml:id="formula_113">M T (i) log(1 -ε) ≤ log N + M T log (1 - ε 2 ).</formula><p>Next, we use the approximations</p><formula xml:id="formula_114">-x -x 2 ≤ log (1 -x) ≤ -x 0 &lt; x &lt; 1 2 ,</formula><p>which follow from the Taylor series of the logarithm function, to obtain that</p><formula xml:id="formula_115">-M T (i)(ε + ε 2 ) ≤ log N -M T ε 2 ,</formula><p>and the lemma follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Randomized weighted majority</head><p>In the randomized version of the WM algorithm, denoted RWM, we choose expert i w.p. p t (i) = W t (i)/ N j=1 W t (j) at time t.</p><p>Lemma 5.4. Let M t denote the number of mistakes made by RWM until iteration t. Then, for any expert i ∈ [N ] we have</p><formula xml:id="formula_116">E[M T ] ≤ (1 + ε)M T (i) + log N ε .</formula><p>The proof of this lemma is very similar to the previous one, where the factor of two is saved by the use of randomness:</p><p>Proof. As before, let Φ t = N i=1 W t (i) for all t ∈ [T ], and note that Φ 1 = N . Let mt = M t -M t-1 be the indicator variable that equals one if the RWM algorithm makes a mistake on iteration t. Let m t (i) equal one if the i'th expert makes a mistake on iteration t and zero otherwise. Inspecting the sum of the weights:</p><formula xml:id="formula_117">Φ t+1 = i W t (i)(1 -εm t (i)) = Φ t (1 -ε i p t (i)m t (i)) p t (i) = Wt(i) j Wt(j) = Φ t (1 -ε E[ mt ]) ≤ Φ t e -ε E[ mt] . 1 + x ≤ e x</formula><p>On the other hand, by definition we have for any expert i that</p><formula xml:id="formula_118">W T (i) = (1 -ε) M T (i)</formula><p>Since the value of W T (i) is always less than the sum of all weights Φ T , we conclude that</p><formula xml:id="formula_119">(1 -ε) M T (i) = W T (i) ≤ Φ T ≤ N e -ε E[M T ] .</formula><p>Taking the logarithm of both sides we get</p><formula xml:id="formula_120">M T (i) log(1 -ε) ≤ log N -ε E[M T ]</formula><p>Next, we use the approximation</p><formula xml:id="formula_121">-x -x 2 ≤ log (1 -x) ≤ -x , 0 &lt; x &lt; 1 2 to obtain -M T (i)(ε + ε 2 ) ≤ log N -ε E[M T ],</formula><p>and the lemma follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Hedge</head><p>The RWM algorithm is in fact more general: instead of considering a discrete number of mistakes, we can consider measuring the performance of an expert by a non-negative real number ℓ t (i), which we refer to as the loss of the expert i at iteration t. The randomized weighted majority algorithm guarantees that a decision maker following its advice will incur an average expected loss approaching that of the best expert in hindsight. Historically, this was observed by a different and closely related algorithm called Hedge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 6 Hedge</head><p>1:</p><formula xml:id="formula_122">Initialize: ∀i ∈ [N ], W 1 (i) = 1 2: for t = 1 to T do 3:</formula><p>Pick i t ∼ R W t , i.e., i t = i with probability x t (i) = Wt(i) j Wt(j)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Incur loss ℓ t (i t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Update weights W t+1 (i) = W t (i)e -εℓt(i) 6: end for Henceforth, denote in vector notation the expected loss of the algorithm by</p><formula xml:id="formula_123">E[ℓ t (i t )] = N i=1 x t (i)ℓ t (i) = x t ℓ t Theorem 5.5. Let ℓ 2</formula><p>t denote the N -dimensional vector of square losses, i.e., ℓ 2 t (i) = ℓ t (i) 2 , let ε &gt; 0, and assume all losses to be non-negative. The Hedge algorithm satisfies for any expert i ∈ [N ]:</p><formula xml:id="formula_124">T t=1 x t ℓ t ≤ T t=1 ℓ t (i ) + ε T t=1 x t ℓ 2 t + log N ε</formula><p>Proof. As before, let Φ t = N i=1 W t (i) for all t ∈ [T ], and note that Φ 1 = N . Inspecting the sum of weights:</p><formula xml:id="formula_125">Φ t+1 = i W t (i)e -εℓt(i) = Φ t i x t (i)e -εℓt(i) x t (i) = Wt(i) j Wt(j) ≤ Φ t i x t (i)(1 -εℓ t (i) + ε 2 ℓ t (i) 2 )) for x ≥ 0, e -x ≤ 1 -x + x 2 = Φ t (1 -εx t ℓ t + ε 2 x t ℓ 2 t ) ≤ Φ t e -εx t ℓt+ε 2 x t ℓ 2 t . 1 + x ≤ e x</formula><p>On the other hand, by definition, for expert i we have that</p><formula xml:id="formula_126">W T (i ) = e -ε T t=1 ℓt(i )</formula><p>Since the value of W T (i ) is always less than the sum of all weights Φ t , we conclude that</p><formula xml:id="formula_127">W T (i ) ≤ Φ T ≤ N e -ε t x t ℓt+ε 2 t x t ℓ 2 t .</formula><p>Taking the logarithm of both sides we get</p><formula xml:id="formula_128">-ε T t=1 ℓ t (i ) ≤ log N -ε T t=1 x t ℓ t + ε 2 T t=1</formula><p>x t ℓ 2 t and the theorem follows by simplifying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Regularization framework</head><p>In the previous section we studied the multiplicative weights update method for decision making. A natural question is: couldn't we have used online gradient descent for the same exact purpose? Indeed, the setting of prediction from expert advice naturally follows into the framework of online convex optimization. To see this, consider the loss functions given by</p><formula xml:id="formula_129">f t (x) = ℓ t x = E i∼x [ℓ t (i)],</formula><p>which capture the expected loss of choosing an expert from distribution x ∈ ∆ n as a linear function.</p><p>The regret guarantees we have studied for OGD imply a regret of</p><formula xml:id="formula_130">O(GD √ T ) = O( √ nT ).</formula><p>Here we have used the fact that the Eucliean diameter of the simplex is two, and that the losses are bounded by one, hence the Euclidean norm of the gradient vector ℓ t is bounded by √ n. In contrast, the Hedge algorithm attains regret of O( √ T log n) for the same problem. How can we explain this discrepancy?!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">The RFTL algorithm</head><p>Both OGD and Hedge are, in fact, instantiations of a more general metaalgorithm called RFTL (Regularized-Follow-The-Leader).</p><p>In an OCO setting of regret minimization, the most straightforward approach for the online player is to use at any time the optimal decision (i.e., point in the convex set) in hindsight. Formally, let</p><formula xml:id="formula_131">x t+1 = arg min x∈K t τ =1 f τ (x).</formula><p>This flavor of strategy is known as "fictitious play" in economics, and has been named "Follow the Leader" (FTL) in machine learning. It is not hard to see that this simple strategy fails miserably in a worst-case sense. That is, this strategy's regret can be linear in the number of iterations, as the following example shows:</p><formula xml:id="formula_132">Consider K = [-1, 1], let f 1 (x) = 1 2</formula><p>x, and let f τ for τ = 2, . . . , T alternate between -x or x. Thus,</p><formula xml:id="formula_133">t τ =1 f τ (x) =    1 2 x, t is odd -1 2 x, otherwise</formula><p>The FTL strategy will keep shifting between x t = -1 and x t = 1, always making the wrong choice.</p><p>The intuitive FTL strategy fails in the example above because it is unstable. Can we modify the FTL strategy such that it won't change decisions often, thereby causing it to attain low regret?</p><p>This question motivates the need for a general means of stabilizing the FTL method. Such a means is referred to as "regularization".</p><p>Algorithm 7 Regularized Follow The Leader 1: Input: η &gt; 0, regularization function R, and a convex compact set K. Predict x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Observe the payoff function f t and let ∇ t = ∇f t (x t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Update</p><formula xml:id="formula_134">x t+1 = arg min x∈K η t s=1 ∇ s x + R(x)</formula><p>7: end for</p><p>The generic RFTL meta-algorithm is defined in Algorithm 7. The regularization function R is assumed to be strongly convex, smooth, and twice differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Mirrored Descent</head><p>An alternative view of this algorithm is in terms of iterative updates, which can be spelled out using the above definition directly. The resulting algorithm is called "Mirrored Descent".</p><p>OMD is an iterative algorithm that computes the current decision using a simple gradient update rule and the previous decision, much like OGD. The generality of the method stems from the update being carried out in a "dual" space, where the duality notion is defined by the choice of regularization: the gradient of the regularization function defines a mapping from R n onto itself, which is a vector field. The gradient updates are then carried out in this vector field.</p><p>For the RFTL algorithm the intuition was straightforward-the regularization was used to ensure stability of the decision. For OMD, regularization has an additional purpose: regularization transforms the space in which gradient updates are performed. This transformation enables better bounds in terms of the geometry of the space.</p><p>The OMD algorithm comes in two flavors: an agile and a lazy version. The lazy version keeps track of a point in Euclidean space and projects onto the convex decision set K only at decision time. In contrast, the agile version maintains a feasible point at all times, much like OGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 8 Online Mirrored Descent</head><p>1: Input: parameter η &gt; 0, regularization function R(x).</p><p>2: Let y 1 be such that ∇R(y 1 ) = 0 and x 1 = arg min x∈K B R (x||y 1 ).</p><p>3: for t = 1 to T do 4:</p><p>Play x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Observe the payoff function f t and let ∇ t = ∇f t (x t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Update y t according to the rule:</p><formula xml:id="formula_135">[Lazy version] ∇R(y t+1 ) = ∇R(y t ) -η ∇ t [Agile version] ∇R(y t+1 ) = ∇R(x t ) -η ∇ t</formula><p>Project according to B R :</p><formula xml:id="formula_136">x t+1 = arg min x∈K B R (x||y t+1 ) 7: end for</formula><p>A myriad of questions arise, but first, let us see how does this algorithm give rise to both OGD.</p><p>We note that there are other important special cases of the RFTL metaalgorithm: those are derived with matrix-norm regularization-namely, the von Neumann entropy function, and the log-determinant function, as well as self-concordant barrier regularization. Perhaps most importantly for optimization, also the AdaGrad algorithm is obtained via changing regularizationwhich we shall explore in detail in the next chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Deriving online gradient descent</head><p>To derive the online gradient descent algorithm, we take R(x) = 1 2 xx 0 2 2 for an arbitrary x 0 ∈ K. Projection with respect to this divergence is the standard Euclidean projection (left as an exercise), and in addition, ∇R(x) = x-x 0 . Hence, the update rule for the OMD Algorithm 8 becomes:</p><formula xml:id="formula_137">x t = Π K (y t ), y t = y t-1 -η∇ t-1</formula><p>lazy version</p><formula xml:id="formula_138">x t = Π K (y t ), y t = x t-1 -η∇ t-1 agile version</formula><p>The latter algorithm is exactly online gradient descent, as described in Algorithm 4 in Chapter 4. Furthermore, both variants are identical for the case in which K is the unit ball.</p><p>We later prove general regret bounds that will imply a O(GD √ T ) regret for OGD as a special case of mirrored descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Deriving multiplicative updates</head><p>Let R(x) = x log x = i x i log x i be the negative entropy function, where log x is to be interpreted elementwise. Then ∇R(x) = 1 + log x, and hence the update rules for the OMD algorithm become:</p><formula xml:id="formula_139">x t = arg min x∈K B R (x||y t ), log y t = log y t-1 -η∇ t-1</formula><p>lazy version</p><formula xml:id="formula_140">x t = arg min x∈K B R (x||y t ), log y t = log x t-1 -η∇ t-1 agile version</formula><p>With this choice of regularizer, a notable special case is the experts problem we encountered in §5.1, for which the decision set K is the n-</p><formula xml:id="formula_141">dimensional simplex ∆ n = {x ∈ R n + | i x i = 1}.</formula><p>In this special case, the projection according to the negative entropy becomes scaling by the ℓ 1 norm (left as an exercise), which implies that both update rules amount to the same algorithm:</p><formula xml:id="formula_142">x t+1 (i) = x t (i) • e -η∇t(i) n j=1</formula><p>x t (j) • e -η∇t(j) , which is exactly the Hedge algorithm! The general theorem we shall prove henceforth recovers the O( √ T log n) bound for prediction from expert advice for this algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Technical background: regularization functions</head><p>In the rest of this chapter we analyze the mirrored descent algorithm. For this purpose, consider regularization functions, denoted R : K → R, which are strongly convex and smooth (recall definitions in §2.1).</p><p>Although it is not strictly necessary, we assume that the regularization functions in this chapter are twice differentiable over K and, for all points x ∈ int(K) in the interior of the decision set, have a Hessian ∇ 2 R(x) that is, by the strong convexity of R, positive definite.</p><p>We denote the diameter of the set K relative to the function R as</p><formula xml:id="formula_143">D R = max x,y∈K {R(x) -R(y)}</formula><p>Henceforth we make use of general norms and their dual. The dual norm to a norm • is given by the following definition:</p><formula xml:id="formula_144">y * max x ≤1</formula><p>x, y</p><p>A positive definite matrix A gives rise to the matrix norm x A = √ x Ax. The dual norm of a matrix norm is x * A = x A -1 . The generalized Cauchy-Schwarz theorem asserts x, y ≤ x y * and in particular for matrix norms, x, y ≤ x A y * A . In our derivations, we usually consider matrix norms with respect to ∇ 2 R(x), the Hessian of the regularization function R(x). In such cases, we use the notation</p><formula xml:id="formula_145">x y x ∇ 2 R(y)</formula><p>and similarly</p><formula xml:id="formula_146">x * y x ∇ -2 R(y)</formula><p>A crucial quantity in the analysis with regularization is the remainder term of the Taylor approximation of the regularization function, and especially the remainder term of the first order Taylor approximation. The difference between the value of the regularization function at x and the value of the first order Taylor approximation is known as the Bregman divergence, given by Definition 5.6. Denote by B R (x||y) the Bregman divergence with respect to the function R, defined as</p><formula xml:id="formula_147">B R (x||y) = R(x) -R(y) -∇R(y) (x -y)</formula><p>For twice differentiable functions, Taylor expansion and the mean-value theorem assert that the Bregman divergence is equal to the second derivative at an intermediate point, i.e., (see exercises)</p><formula xml:id="formula_148">B R (x||y) = 1 2 x -y 2 z ,</formula><p>for some point z ∈ [x, y], meaning there exists some α ∈ [0, 1] such that z = αx + (1 -α)y. Therefore, the Bregman divergence defines a local norm, which has a dual norm. We shall denote this dual norm by</p><formula xml:id="formula_149">• * x,y • * z .</formula><p>With this notation we have</p><formula xml:id="formula_150">B R (x||y) = 1 2</formula><p>xy 2 x,y .</p><p>In online convex optimization, we commonly refer to the Bregman divergence between two consecutive decision points</p><formula xml:id="formula_151">x</formula><p>t and x t+1 . In such cases, we shorthand notation for the norm defined by the Bregman divergence with respect to R on the intermediate point in [x t , x t+1 ] as • t • xt,x t+1 . The latter norm is called the local norm at iteration t. With this notation, we have B R (x t ||x t+1 ) = 1 2 x t -x t+1 2 t . Finally, we consider below generalized projections that use the Bregman divergence as a distance instead of a norm. Formally, the projection of a point y according to the Bregman divergence with respect to function R is given by arg min x∈K B R (x||y)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Regret bounds for Mirrored Descent</head><p>In this subsection we prove regret bounds for the agile version of the RFTL algorithm. The analysis is quite different than the one for the lazy version, and of independent interest.</p><p>Theorem 5.7. The RFTL Algorithm 8 attains for every u ∈ K the following bound on the regret:</p><formula xml:id="formula_152">regret T ≤ 2η T t=1 ∇ t * 2 t + R(u) -R(x 1 ) η .</formula><p>If an upper bound on the local norms is known, i.e. ∇ t * t ≤ G R for all times t, then we can further optimize over the choice of η to obtain</p><formula xml:id="formula_153">regret T ≤ 2D R G R √ 2T .</formula><p>Proof. Since the functions f t are convex, for any x * ∈ K,</p><formula xml:id="formula_154">f t (x t ) -f t (x * ) ≤ ∇f t (x t ) (x t -x * ).</formula><p>The following property of Bregman divergences follows easily from the definition: for any vectors x, y, z,</p><formula xml:id="formula_155">(x -y) (∇R(z) -∇R(y)) = B R (x, y) -B R (x, z) + B R (y, z).</formula><p>Combining both observations,</p><formula xml:id="formula_156">2(f t (x t ) -f t (x * )) ≤ 2∇f t (x t ) (x t -x * ) = 1 η (∇R(y t+1 ) -∇R(x t )) (x * -x t ) = 1 η [B R (x * , x t ) -B R (x * , y t+1 ) + B R (x t , y t+1 )] ≤ 1 η [B R (x * , x t ) -B R (x * , x t+1 ) + B R (x t , y t+1 )]</formula><p>where the last inequality follows from the generalized Pythagorean inequality (see <ref type="bibr" target="#b17">[15]</ref> Lemma 11.3), as x t+1 is the projection w.r.t the Bregman divergence of y t+1 and x * ∈ K is in the convex set. Summing over all iterations,</p><formula xml:id="formula_157">2regret ≤ 1 η [B R (x * , x 1 ) -B R (x * , x T )] + T t=1 1 η B R (x t , y t+1 ) ≤ 1 η D 2 + T t=1 1 η B R (x t , y t+1 ) (5.1)</formula><p>We proceed to bound B R (x t , y t+1 ). By definition of Bregman divergence, and the generalized Cauchy-Schwartz inequality,</p><formula xml:id="formula_158">B R (x t , y t+1 ) + B R (y t+1 , x t ) = (∇R(x t ) -∇R(y t+1 )) (x t -y t+1 ) = η∇f t (x t ) (x t -y t+1 ) ≤ η ∇f t (x t ) * x t -y t+1 ≤ 1 2 η 2 G 2 * + 1 2 x t -y t+1 2 .</formula><p>where in the last inequality follows from (a -b) 2 ≥ 0. Thus, by our assumption B R (x, y) ≥ 1 2 xy 2 , we have</p><formula xml:id="formula_159">B R (x t , y t+1 ) ≤ 1 2 η 2 G 2 * + 1 2 x t -y t+1 2 -B R (y t+1 , x t ) ≤ 1 2 η 2 G 2 * .</formula><p>Plugging back into Equation (5.1), and by non-negativity of the Bregman divergence, we get</p><formula xml:id="formula_160">regret ≤ 1 2 [ 1 η D 2 + 1 2 ηT G 2 * ] ≤ DG * √ T , by taking η = D 2 √ T G *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Exercises</head><p>1. (a) Show that the dual norm to a matrix norm given by A 0 corresponds to the matrix norm of A -1 .</p><p>(b) Prove the generalized Cauchy-Schwarz inequality for any norm, i.e., x, y ≤ x y * 2. Prove that the Bregman divergence is equal to the local norm at an intermediate point, that is:</p><formula xml:id="formula_161">B R (x||y) = 1 2 x -y 2 z ,</formula><p>where z ∈ [x, y] and the interval [x, y] is defined as</p><formula xml:id="formula_162">[x, y] = {v = αx + (1 -α)y , α ∈ [0, 1]} 3. Let R(x) = 1 2</formula><p>xx 0 2 be the (shifted) Euclidean regularization function. Prove that the corresponding Bregman divergence is the Euclidean metric. Conclude that projections with respect to this divergence are standard Euclidean projections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Prove that both agile and lazy versions of the OMD meta-algorithm</head><p>are equivalent in the case that the regularization is Euclidean and the decision set is the Euclidean ball.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>For this problem the decision set is the n-dimensional simplex. Let R(x) = x log x be the negative entropy regularization function. Prove that the corresponding Bregman divergence is the relative entropy, and prove that the diameter D R of the n-dimensional simplex with respect to this function is bounded by log n. Show that projections with respect to this divergence over the simplex amounts to scaling by the ℓ 1 norm.</p><formula xml:id="formula_163">6. * A set K ⊆ R d is symmetric if x ∈ K implies -x ∈ K.</formula><p>Symmetric sets gives rise to a natural definition of a norm. Define the function</p><formula xml:id="formula_164">• K : R d → R as x K = arg min α&gt;0 1 α x ∈ K</formula><p>Prove that • K is a norm if and only if K is convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Bibliographic Remarks</head><p>Regularization in the context of online learning was first studied in <ref type="bibr" target="#b28">[26]</ref> and <ref type="bibr" target="#b50">[48]</ref>. The influential paper of Kalai and Vempala <ref type="bibr" target="#b47">[45]</ref> coined the term "follow-the-leader" and introduced many of the techniques that followed in OCO. The latter paper studies random perturbation as a regularization and analyzes the follow-the-perturbed-leader algorithm, following an early development by <ref type="bibr" target="#b31">[29]</ref> that was overlooked in learning for many years.</p><p>In the context of OCO, the term follow-the-regularized-leader was coined in <ref type="bibr" target="#b75">[73,</ref><ref type="bibr" target="#b73">71]</ref>, and at roughly the same time an essentially identical algorithm was called "RFTL" in <ref type="bibr">[1]</ref>. The equivalence of RFTL and Online Mirrored Descent was observed by <ref type="bibr" target="#b36">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 6 Adaptive Regularization</head><p>In the previous chapter we have studied a geometric extension of online / stochastic / determinisitic gradient descent. The technique to achieve it is called regularization, and we have seen how for the problem of prediction from expert advice, it can potentially given exponential improvements in the dependence on the dimension.</p><p>A natural question that arises is whether we can automatically learn the optimal regularization, i.e. best algorithm from the mirrored-descent class, for the problem at hand?</p><p>The answer is positive in a strong sense: it is theoretically possible to learn the optimal regularization online and in a data-specific way. Not only that, the resulting algorithms exhibit the most significant speedups in training deep neural networks from all accelerations studied thus far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Adaptive Learning Rates: Intuition</head><p>The intuition for adaptive regularization is simple: consider an optimization problem which is axis-aligned, in which each coordinate is independent of the rest. It is reasonable to fine tune the learning rate for each coordinate separately -to achieve optimal convergence in that particular subspace of the problem, independently of the rest.</p><p>Thus, it is reasonable to change the SGD update rule from x t+1 ← x t -η∇ t , to the more robust</p><formula xml:id="formula_165">x t+1 ← x t -D t ∇ t ,</formula><p>where D t is a diagonal matrix that contains in coordinate (i, i) the learning rate for coordinate i in the gradient. Recall from the previous sections that the optimal learning rate for stochastic non-convex optimization is of the order O( 1 √ t ). More precisely, in Theorem 3.4, we have seen that this learning rate should be on the order of O( 1 √ tσ 2 ), where σ 2 is the variance of the stochastic gradients. The empirical estimator of the latter is i&lt;t ∇ i 2 . Thus, the robust version of stochastic gradient descent for smooth nonconvex optimization should behave as the above equation, with</p><formula xml:id="formula_166">D t (i, i) = 1 i&lt;t ∇ t (i) 2 .</formula><p>This is exactly the diagonal version of the AdaGrad algorithm! We continue to rigorously derive it and prove its performance guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">A Regularization Viewpoint</head><p>In the previous chapter we have introduced regularization as a general methodology for deriving online convex optimization algorithms. Theorem 5.7 bounds the regret of the Mirrored Descent algorithm for any strongly convex regularizer as</p><formula xml:id="formula_167">regret T ≤ max u∈K 2 t ∇ t * 2 t B R (u||x 1 ).</formula><p>In addition, we have seen how to derive the online gradient descent and the multiplicative weights algorithms as special cases of the RFTL methodology. We consider the following question: thus far we have thought of R as a strongly convex function. But which strongly convex function should we choose to minimize regret? This is a deep and difficult question which has been considered in the optimization literature since its early developments.</p><p>The ML approach is to learn the optimal regularization online. That is, a regularizer that adapts to the sequence of cost functions and is in a sense the "optimal" regularization to use in hindsight. We formalize this in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Tools from Matrix Calculus</head><p>Many of the inequalities that we are familiar with for positive real numbers hold for positive semi-definite matrices as well. We henceforth need the following inequality, which is left as an exercise, Proposition 6.1. For positive definite matrices A B 0:</p><formula xml:id="formula_168">2Tr((A -B) 1/2 ) + Tr(A -1/2 B) ≤ 2Tr(A 1/2 ).</formula><p>Next, we require a structural result which explicitly gives the optimal regularization as a function of the gradients of the cost functions. For a proof see the exercises. Proposition 6.2. Let A 0. The minimizer of the following minimization problem:</p><formula xml:id="formula_169">min X Tr(X -1 A) subject to X 0 Tr(X) ≤ 1, is X = A 1/2 /Tr(A 1/2</formula><p>), and the minimum objective value is Tr 2 (A 1/2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">The AdaGrad Algorithm and Its Analysis</head><p>To be more formal, let us consider the set of all strongly convex regularization functions with a fixed and bounded Hessian in the set</p><formula xml:id="formula_170">∀x ∈ K . ∇ 2 R(x) = ∇ 2 ∈ H {X ∈ R n×n ; Tr(X) ≤ 1 , X 0}</formula><p>The set H is a restricted class of regularization functions (which does not include the entropic regularization). However, it is a general enough class to capture online gradient descent along with any rotation of the Euclidean regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 9 AdaGrad (Full Matrix version)</head><p>1: Input: parameters η, x 1 ∈ K.</p><p>2: Initialize:</p><formula xml:id="formula_171">S 0 = G 0 = 0, 3: for t = 1 to T do 4:</formula><p>Predict x t , suffer loss f t (x t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Update:</p><formula xml:id="formula_172">S t = S t-1 + ∇ t ∇ t , G t = S t 1/2 y t+1 = x t -ηG -1 t ∇ t x t+1 = arg min x∈K y t+1 -x 2 Gt 6: end for</formula><p>The problem of learning the optimal regularization has given rise to Algorithm 9, known as the AdaGrad (Adaptive subGradient method) algorithm. In the algorithm definition and throughout this chapter, the notation A -1 refers to the Moore-Penrose pseudoinverse of the matrix A. Perhaps surprisingly, the regret of AdaGrad is at most a constant factor larger than the minimum regret of all RFTL algorithm with regularization functions whose Hessian is fixed and belongs to the class H. The regret bound on AdaGrad is formally stated in the following theorem. Then for any x ∈ K,</p><formula xml:id="formula_173">regret T (AdaGrad) ≤ 2D min H∈H t ∇ t * 2 H .</formula><p>Before proving this theorem, notice that it delivers on one of the promised accounts: comparing to the bound of Theorem 5.7 and ignoring the diameter D and dimensionality, the regret bound is as good as the regret of RFTL for the class of regularization functions.</p><p>We proceed to prove Theorem 6.3. First, a direct corollary of Proposition 6.2 is that Corollary 6.4.</p><formula xml:id="formula_174">min H∈H t ∇ t * 2 H = min H∈H Tr(H -1 t ∇ t ∇ t ) = Tr t ∇ t ∇ t = Tr(G T )</formula><p>Hence, to prove Theorem 6.3, it suffices to prove the following lemma.</p><p>Lemma 6.5.</p><formula xml:id="formula_175">regret T (AdaGrad) ≤ 2DTr(G T ) = 2D min H∈H t ∇ t * 2 H .</formula><p>Proof. By the definition of y t+1 :</p><formula xml:id="formula_176">y t+1 -x = x t -x -ηG t -1 ∇ t ,<label>(6.1)</label></formula><p>and</p><formula xml:id="formula_177">G t (y t+1 -x ) = G t (x t -x ) -η∇ t . (6.2)</formula><p>Multiplying the transpose of (6.1) by ( <ref type="formula" target="#formula_176">6</ref>.2) we get</p><formula xml:id="formula_178">(y t+1 -x ) G t (y t+1 -x ) = (x t -x ) G t (x t -x ) -2η∇ t (x t -x ) + η 2 ∇ t G -1 t ∇ t . (6.3)</formula><p>Since x t+1 is the projection of y t+1 in the norm induced by G t , we have (see</p><formula xml:id="formula_179">§2.1.1) (y t+1 -x ) G t (y t+1 -x ) = y t+1 -x 2 Gt ≥ x t+1 -x 2 Gt .</formula><p>This inequality is the reason for using generalized projections as opposed to standard projections, which were used in the analysis of online gradient descent (see §4.4 Equation (4.2)). This fact together with (6.3) gives</p><formula xml:id="formula_180">∇ t (x t -x ) ≤ η 2 ∇ t G -1 t ∇ t + 1 2η x t -x 2 Gt -x t+1 -x 2 Gt .</formula><p>Now, summing up over t = 1 to T we get that</p><formula xml:id="formula_181">T t=1 ∇ t (x t -x ) ≤ η 2 T t=1 ∇ t G -1 t ∇ t + 1 2η x 1 -x 2 G 0 (6.4) + 1 2η T t=1 x t -x 2 Gt -x t -x 2 G t-1 - 1 2η x T +1 -x 2 G T ≤ η 2 T t=1 ∇ t G -1 t ∇ t + 1 2η T t=1 (x t -x ) (G t -G t-1 )(x t -x ).</formula><p>In the last inequality we use the fact that G 0 = 0. We proceed to bound each of the terms above separately.</p><p>Lemma 6.6. With S t , G t as defined in Algorithm 9,</p><formula xml:id="formula_182">T t=1 ∇ t G -1 t ∇ t ≤ 2 T t=1 ∇ t G -1 T ∇ t ≤ 2Tr(G T ).</formula><p>Proof. We prove the lemma by induction. The base case follows since</p><formula xml:id="formula_183">∇ 1 G -1 1 ∇ 1 = Tr(G -1 1 ∇ 1 ∇ 1 ) = Tr(G -1 1 G 2 1 ) = Tr(G 1 ).</formula><p>Assuming the lemma holds for T -1, we get by the inductive hypothesis</p><formula xml:id="formula_184">T t=1 ∇ t G -1 t ∇ t ≤ 2Tr(G T -1 ) + ∇ T G -1 T ∇ T = 2Tr((G 2 T -∇ T ∇ T ) 1/2 ) + Tr(G -1 T ∇ T ∇ T ) ≤ 2Tr(G T ).</formula><p>Here, the last inequality is due to the matrix inequality 6.1. Lemma 6.7.</p><formula xml:id="formula_185">T t=1 (x t -x ) (G t -G t-1 )(x t -x ) ≤ D 2 Tr(G T ).</formula><p>Proof. By definition S t S t-1 , and hence G t G t-1 . Thus,</p><formula xml:id="formula_186">T t=1 (x t -x ) (G t -G t-1 )(x t -x ) ≤ T t=1 D 2 λ max (G t -G t-1 ) ≤ D 2 T t=1 Tr(G t -G t-1 ) A 0 ⇒ λ max (A) ≤ Tr(A) = D 2 T t=1 (Tr(G t ) -Tr(G t-1</formula><p>)) linearity of the trace</p><formula xml:id="formula_187">≤ D 2 Tr(G T ).</formula><p>Plugging both lemmas into Equation (6.4), we obtain</p><formula xml:id="formula_188">T t=1 ∇ t (x t -x ) ≤ ηTr(G T ) + 1 2η D 2 Tr(G T ) ≤ 2DTr(G T ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Diagonal AdaGrad</head><p>The AdaGrad algorithm maintains potentially dense matrices, and requires the computation of the square root of these matrices. This is usually prohibitive in machine learning applications in which the dimension is very large. Fortunately, the same ideas can be applied with almost no computational overhead on top of vanilla SGD, using the diagonal version of AdaGrad given by:</p><formula xml:id="formula_189">Algorithm 10 AdaGrad (diagonal version) 1: Input: parameters η, x 1 ∈ K. 2: Initialize: S 0 = G 0 = 0, 3: for t = 1 to T do 4:</formula><p>Predict x t , suffer loss f t (x t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Update:</p><formula xml:id="formula_190">S t = S t-1 + diag(∇ t ∇ t ), G t = S t 1/2 y t+1 = x t -ηG -1 t ∇ t x t+1 = arg min x∈K y t+1 -x 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gt</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6: end for</head><p>In contrast to the full-matrix version, this version can be implemented in linear time and space, since diagonal matrices can be manipulated as vectors. Thus, memory overhead is only a single d-dimensional vector, which is used to represent the diagonal preconditioning (regularization) matrix, and the computational overhead is a few vector manipulations per iteration.</p><p>Very similar to the full matrix case, the diagonal AdaGrad algorithm can be analyzed and the following performance bound obtained: Theorem 6.8. Let {x t } be defined by Algorithm 10 with parameters η = D ∞ , where</p><formula xml:id="formula_191">D ∞ = max u∈K u -x 1 ∞ ,</formula><p>and let diag(H) be the set of all diagonal matrices in H. Then for any x ∈ K,</p><formula xml:id="formula_192">regret T (D-AdaGrad) ≤ 2D ∞ min H∈diag(H) t ∇ t * 2 H .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">State-of-the-art: from Adam to Shampoo and beyond</head><p>Since the introduction of the adaptive regularization technique in the context of regret minimization, several improvements were introduced that now compose state-of-the-art. A few notable advancements include:</p><p>AdaDelta: The algorithm keeps an exponential average of past gradients and uses that in the update step.</p><p>Adam: Adds a sliding window to AdaGrad, as well as adding a form of momentum via estimating the second moments of past gradients and adjusting the update accordingly.</p><p>Shampoo: Interpolates between full-matrix and diagonal adagrad in the context of deep neural networks: use of the special layer structure to reduce memory constraints.</p><p>AdaFactor: Suggests a Shampoo-like approach to reduce memory footprint even further, to allow the training of huge models.</p><p>GGT: While full-matrix AdaGrad is computationally slow due to the cost of manipulating matrices, this algorithm uses recent gradients (a thin matrix G), and via linear algebraic manipulations reduces computation by never computing GG , but rather only G G, which is low dimensional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SM3 , ET:</head><p>Diagonal AdaGrad requires an extra O(n) memory to store diag(G t ). These algorithms, inspired by AdaFactor, approximate G t as a low rank tensor to save memory and computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Exercises</head><p>1. * Prove that for positive definite matrices A B 0 it holds that</p><formula xml:id="formula_193">(a) A 1/2 B 1/2 (b) 2Tr((A -B) 1/2 ) + Tr(A -1/2 B) ≤ 2Tr(A 1/2 ).</formula><p>2. * Consider the following minimization problem where A 0:</p><formula xml:id="formula_194">min X Tr(X -1 A) subject to X 0 Tr(X) ≤ 1.</formula><p>Prove that its minimizer is given by X = A 1/2 /Tr(A 1/2 ), and the minimum is obtained at Tr 2 (A 1/2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Bibliographic Remarks</head><p>The AdaGrad algorithm was introduced in <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b20">18]</ref>, its diagonal version was also discovered in parallel in <ref type="bibr" target="#b54">[52]</ref>. Adam <ref type="bibr" target="#b48">[46]</ref> and RMSprop <ref type="bibr" target="#b41">[39]</ref> are widely used methods based on adaptive regularization. A cleaner analysis was recently proposed in <ref type="bibr" target="#b29">[27]</ref>, see also <ref type="bibr" target="#b19">[17]</ref>.</p><p>Adaptive regularization has received much attention recently, see e.g., <ref type="bibr" target="#b62">[60,</ref><ref type="bibr" target="#b88">85]</ref>. Newer algorithmic developments on adaptive regularization include Shampoo <ref type="bibr" target="#b30">[28]</ref>, GGT <ref type="bibr">[3]</ref>, AdaFactor <ref type="bibr" target="#b79">[77]</ref>, Extreme Tensoring <ref type="bibr" target="#b18">[16]</ref> and SM3 <ref type="bibr" target="#b8">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 7 Variance Reduction</head><p>In the previous chapter we have studied the first of our three acceleration techniques over SGD, adaptive regularization, which is a geometric tool for acceleration. In this chapter we introduce the second first-order acceleration technique, called variance reduction. This technique is probabilistic in nature, and applies to more restricted settings of mathematical optimization in which the objective function has a finite-sum structure. Namely, we consider optimization problems of the form</p><formula xml:id="formula_195">min x∈K f (x) , f (x) = 1 m m i=1 f i (x) . (7.1)</formula><p>Such optimization problems are canonical in training of ML models, convex and non-convex. However, in the context of machine learning we should remember that the ultimate goal is generalization rather than training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Variance reduction: Intuition</head><p>The intuition for variance reduction is simple, and comes from trying to improve the naive convergence bounds for SGD that we have covered in the first lesson.</p><p>Recall the SGD update rule x t+1 ← x t -η ∇t , in which ∇t is an unbiased estimator for the gradient such that</p><formula xml:id="formula_196">E[ ∇t ] = ∇ t , E[ ∇t 2 2 ] ≤ σ 2 .</formula><p>We have seen in Theorem 3.4, that for this update rule,</p><formula xml:id="formula_197">E 1 T t ∇ t 2 ≤ 2 M βσ 2 T .</formula><p>The convergence is proportional to the second moment of the gradient estimator, and thus it makes sense to try to reduce this second moment. The variance reduction technique attempts to do so by using the average of all previous gradients, as we show next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Setting and definitions</head><p>We consider the ERM optimization problem over an average of loss functions. Before we begin, we need a few preliminaries and assumptions:</p><p>1. We denote distance to optimality according to function value as</p><formula xml:id="formula_198">h t = f (x t ) -f (x * ),</formula><p>and in the k'th epoch of an algorithm, we denote</p><formula xml:id="formula_199">h k t = f (x k t ) -f (x * ).</formula><p>2. We denote hk = max 4h k 0 , 8αD 2 k over an epoch.</p><p>3. Assume all stochastic gradients have bounded second moments ∇t</p><formula xml:id="formula_200">2 2 ≤ σ 2 .</formula><p>4. We will assume that the individual functions f i in formulation (7.1) are also β-smooth and have β-Lipschitz gradient, namely</p><formula xml:id="formula_201">∇f i (x) -∇f i (y) ≤ β x -y .</formula><p>5. We will use, proved in Lemma 2.3, that for β-smooth and α-strongly convex f we have</p><formula xml:id="formula_202">h t ≥ 1 2β ∇ t 2 and α 2 d 2 t = α 2 x t -x * 2 ≤ h t ≤ 1 2α ∇ t 2 .</formula><p>6. Recall that a function f is γ-well-conditioned if it is β-smooth, αstrongly convex and γ ≤ α β .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">The variance reduction advantage</head><p>Consider gradient descent for γ-well conditioned functions, and specifically used for ML training as in formulation (7.1) . It is well known that GD attains linear convergence rate as we now prove for completeness:</p><p>Theorem 7.1. For unconstrained minimization of γ-well-conditioned functions and η t = 1 β , the Gradient Descent Algorithm 2 converges as</p><formula xml:id="formula_203">h t+1 ≤ h 1 e -γt .</formula><p>Proof.</p><formula xml:id="formula_204">h t+1 -h t = f (x t+1 ) -f (x t ) ≤ ∇ t (x t+1 -x t ) + β 2 x t+1 -x t 2 β-smoothness = -η t ∇ t 2 + β 2 η 2 t ∇ t 2 algorithm defn. = - 1 2β ∇ t 2 choice of η t = 1 β ≤ - α β h t . by (2.1)</formula><p>Thus,</p><formula xml:id="formula_205">h t+1 ≤ h t (1 - α β ) ≤ • • • ≤ h 1 (1 -γ) t ≤ h 1 e -γt</formula><p>where the last inequality follows from 1 -x ≤ e -x for all x ∈ R.</p><p>However, what is the overall computational cost? Assuming that we can compute the gradient of each loss function corresponding to the individual training examples in O(d) time, the overall running time to compute the gradient is O(md).</p><p>In order to attain approximation ε to the objective, the algorithm requires O( <ref type="formula" target="#formula_22">1</ref>γ log 1 ε ) iterations, as per the Theorem above. Thus, the overall running time becomes O( md γ log 1 ε ). As we show below, variance reduction can reduce this running time to be O((m</p><formula xml:id="formula_206">+ 1 γ2 )d log 1 ε )</formula><p>, where γ is a different condition number for the same problem, that is in general smaller than the original. Thus, in one line, the variance reduction advantage can be summarized as:</p><formula xml:id="formula_207">md γ log 1 ε → (m + 1 γ2 )d log 1 ε .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">A simple variance-reduced algorithm</head><p>The following simple variance-reduced algorithm illustrates the main ideas of the technique. The algorithm is a stochastic gradient descent variant which proceeds in epochs. Strong convexity implies that the distance to the optimum shrinks with function value, so it is safe to decrease the distance upper bound every epoch.</p><p>The main innovation is in line 7, which constructs the gradient estimator. Instead of the usual trick -which is to sample one example at random -here the estimator uses the entire gradient computed at the beginning of the current epoch.</p><formula xml:id="formula_208">Algorithm 11 Epoch GD 1: Input: f , T , x 1 0 ∈ K, upper bound D 1 ≥ x 1 0 -x * , step sizes {η t } 2: for k = 1 to log 1 ε do 3: Let B D k (x k 0 ) be the ball of radius D k around x k 0 . 4: compute full gradient ∇ k 0 = ∇f (x k 0 ) 5:</formula><p>for t = 1 to T do 6:</p><formula xml:id="formula_209">Sample i t ∈ [m] uniformly at random, let f t = f it . 7: construct stochastic gradient ∇k t = ∇f t (x k t ) -∇f t (x k 0 ) + ∇ k 0 8: Let y k t+1 = x k t -η t ∇k t , x t+1 = Π B D k (x k 0 ) (y t+1 ) 9:</formula><p>end for 10:</p><formula xml:id="formula_210">Set x k+1 0 = 1 T T t=1 x k t . D k+1 ← D k /2. 11: end for 12: return x 0 T +1</formula><p>The main guarantee for this algorithm is the following theorem, which delivers upon the aforementioned improvement, Theorem 7.2. Algorithm 11 returns an ε-approximate solution to optimization problem (7.1) in total time</p><formula xml:id="formula_211">O m + 1 γ2 d log 1 ε . Let γ = α β &lt; γ.</formula><p>Then the proof of this theorem follows from the following lemma.</p><p>Lemma 7.3. For T = Õ 1 γ2 , we have</p><formula xml:id="formula_212">E[ hk+1 ] ≤ 1 2 hk .</formula><p>Proof. As a first step, we bound the variance of the gradients. Due to the fact that</p><formula xml:id="formula_213">x k t ∈ B D k (x k 0 ), we have that for k &gt; k, x k t -x k t 2 ≤ 4D 2 k . Thus, ∇k t 2 = ∇f t (x k t ) -∇f t (x k 0 ) + ∇f (x k 0 ) 2 definition ≤ 2 ∇f t (x k t ) -∇f t (x k 0 ) 2 + 2 ∇f (x k 0 ) 2 (a + b) 2 ≤ 2a 2 + 2b 2 ≤ 2 β2 x k t -x k 0 2 + 4βh k 0 smoothness ≤ 8 β2 D 2 k + 4βh k 0 projection step ≤ β2 1 α hk + 4βh k 0 ≤ hk ( β2 α + β)</formula><p>Next, using the regret bound for strongly convex functions, we have</p><formula xml:id="formula_214">E[h k+1 0 ] ≤ E[ 1 T t h k t ] Jensen ≤ 1 αT E[ t 1 t ∇k t 2 ] Theorem 4.4 ≤ 1 αT t 1 t hk ( β2 α + β) above ≤ log T T hk ( 1 γ2 + 1 γ ) γ = α β</formula><p>Which implies the Lemma by choice of T , definition of hk = max 4h k 0 , 8αD 2 k , and exponential decrease of D k .</p><p>The expectation is over the stochastic gradient definition, and is required for using Theorem 4.4.</p><p>To obtain the theorem from the lemma above, we need to strengthen it to a high probability statement using a martingale argument. This is possible since the randomness in construction of the stochastic gradients is i.i.d.</p><p>The lemma now implies the theorem by noting that O(log 1 ε ) epochs suffices to get ε-approximation. Each epoch requires the computation of one full gradient, in time O(md), and Õ( 1 γ2 ) iterations that require stochastic gradient computation, in time O(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Bibliographic Remarks</head><p>The variance reduction technique was first introduced as part of the SAG algorithm <ref type="bibr" target="#b72">[70]</ref>. Since then a host of algorithms were developed using the technique. The simplest exposition of the technique was given in <ref type="bibr" target="#b46">[44]</ref>. The exposition in this chapter is developed from the Epoch GD algorithm <ref type="bibr" target="#b39">[37]</ref>, which uses a related technique for stochastic strongly convex optimization, as developed in <ref type="bibr" target="#b89">[86]</ref>.</p><p>Chapter 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nesterov Acceleration</head><p>In previous chapters we have studied our bread and butter technique, SGD, as well as two acceleration techniques of adaptive regularization and variance reduction. In this chapter we study the historically earliest acceleration technique, known as Nesterov acceleration, or simply "acceleration".</p><p>For smooth and convex functions, Nesterov acceleration improves the convergence rate to optimality to O( <ref type="formula" target="#formula_22">1</ref>T 2 ), a quadratic improvement over vanilla gradient descent. Similar accelerations are possible when the function is also strongly convex: an accelerated rate of e -√ γT , where γ is the condition number, vs. e -γT of vanilla gradient descent. This improvement is theoretically very significant.</p><p>However, in terms of applicability, Nesterov acceleration is theoretically the most restricted in the context of machine learning: it requires a smooth and convex objective. More importantly, the learning rates of this method are very brittle, and the method is not robust to noise. Since noise is predominant in machine learning, the theoretical guarantees in stochastic optimization environments are very restricted.</p><p>However, the heuristic of momentum, which historically inspired acceleration, is extremely useful for non-convex stochastic optimization (although not known to yield significant improvements in theory).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Algorithm and implementation</head><p>Nesterov acceleration applies to the general setting of constrained smooth convex optimization:</p><formula xml:id="formula_215">min x∈R d f (x). (<label>8.1)</label></formula><p>For simplicity of presentation, we restrict ourselves to the unconstrained convex and smooth case. Nevertheless, the method can be extended to constrained smooth convex, and potentially strongly convex, settings.</p><p>The simple method presented in Algorithm 12 below is computationally equivalent to gradient descent. The only overhead is saving three state vectors (that can be reduced to two) instead of one for gradient descent. The following simple accelerated algorithm illustrates the main ideas of the technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 12 Simplified Nesterov Acceleration</head><p>1: Input: f , T , initial point x 0 , parameters η, β, τ .</p><p>2: for t = 1 to T do 3:</p><p>Set x t+1 = τ z t + (1 -τ )y t , and denote ∇ t+1 = ∇f (x t+1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Let</p><formula xml:id="formula_216">y t+1 = x t+1 -1 β ∇ t+1 5:</formula><p>Let z t+1 = z t -η∇ t+1 6: end for</p><formula xml:id="formula_217">7: return x = 1 T t x t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Analysis</head><p>The main guarantee for this algorithm is the following theorem. </p><formula xml:id="formula_218">η∇ t+1 (z t -x * ) ≤ 2η 2 β(f (x t+1 ) -f (y t+1 )) + z t -x * 2 -z t+1 -x * 2 .</formula><p>Proof. The proof is very similar to that of Theorem 4.2. By definition of z t ,</p><formula xml:id="formula_219">1 z t+1 -x * 2 = z t -η∇ t+1 -x * 2 = z t -x * 2 -η∇ t+1 (z t -x * ) + η 2 ∇ t+1 2 ≤ z t -x * 2 -η∇ t+1 (z t -x * ) + 2η 2 β(f (x t+1 ) -f (y t+1 )) Lemma 2.3 part 3</formula><p>1 Henceforth we use Lemma 2.3 part 3. This proof of this Lemma shows that for</p><formula xml:id="formula_220">y = x -1 β ∇f (x), it holds that f (x) -f (y) ≥ 1 2β ∇f (x) 2 .</formula><p>Lemma 8.3. For 2ηβ = 1-τ τ , we have that</p><formula xml:id="formula_221">η∇ t+1 (x t+1 -x * ) ≤ 2η 2 β(f (y t ) -f (y t+1 )) + z t -x * 2 -z t+1 -x * 2 .</formula><p>Proof.</p><formula xml:id="formula_222">η∇ t+1 (x t+1 -x * ) -η∇ t+1 (z t -x * ) = η∇ t+1 (x t+1 -z t ) = (1-τ )η τ ∇ t+1 (y t -x t+1 ) τ (x t+1 -z t ) = (1 -τ )(y t -x t+1 ) ≤ (1-τ )η τ (f (y t ) -f (x t+1 )). convexity</formula><p>Thus, in combination with Lemma 8.2, and the condition of the Lemma, we get the inequality.</p><p>We can now sketch the proof of the main theorem.</p><p>Proof. Telescope Lemma 8.3 for all iterations to obtain:</p><formula xml:id="formula_223">T h T = T (f (x) -f (x * )) ≤ t ∇ t (x t -x * ) ≤ 2ηβ t (f (y t ) -f (y t+1 )) + 1 η t z t -x * 2 -z t+1 -x * 2 ≤ 2ηβ(f (y 1 ) -f (y T +1 )) + 1 η z 1 -x * 2 -z T +1 -x * 2 ≤ √ 2βh 1 D, optimizing η</formula><p>where h 1 is an upper bound on the distance f (y 1 ) -f (x * ), and D bounds the Euclidean distance of z t to the optimum. Thus, we get a recurrence of the form</p><formula xml:id="formula_224">h T ≤ √ h 1 T .</formula><p>Restarting Algorithm 12 and adapting the learning rate according to h T gives a rate of convergence of O( 1 T 2 ) to optimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Bibliographic Remarks</head><p>Accelerated rates of order O( 1 T 2 ) were obtained by Nemirovski as early as the late seventies. The first practically efficient accelerated algorithm is due to Nesterov <ref type="bibr" target="#b58">[56]</ref> , see also <ref type="bibr" target="#b59">[57]</ref>. The simplified proof presented hereby is due to <ref type="bibr" target="#b7">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 9</head><p>The conditional gradient method</p><p>In many computational and learning scenarios the main bottleneck of optimization, both online and offline, is the computation of projections onto the underlying decision set (see §2.1.1). In this chapter we discuss projection-free methods in convex optimization, and some of their applications in machine learning.</p><p>The motivating example throughout this chapter is the problem of matrix completion, which is a widely used and accepted model in the construction of recommendation systems. For matrix completion and related problems, projections amount to expensive linear algebraic operations and avoiding them is crucial in big data applications.</p><p>Henceforth we describe the conditional gradient algorithm, also known as the Frank-Wolfe algorithm. Afterwards, we describe problems for which linear optimization can be carried out much more efficiently than projections. We conclude with an application to exploration in reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Review: relevant concepts from linear algebra</head><p>This chapter addresses rectangular matrices, which model applications such as recommendation systems naturally. Consider a matrix X ∈ R n×m . A non-negative number σ ∈ R + is said to be a singular value for X if there are two vectors</p><formula xml:id="formula_225">u ∈ R n , v ∈ R m such that X u = σv, Xv = σu.</formula><p>The vectors u, v are called the left and right singular vectors respectively. The non-zero singular values are the square roots of the eigenvalues of the matrix XX (and X X). The matrix X can be written as</p><formula xml:id="formula_226">X = U ΣV , U ∈ R n×ρ , V ∈ R ρ×m ,</formula><p>where ρ = min{n, m}, the matrix U is an orthogonal basis of the left singular vectors of X, the matrix V is an orthogonal basis of right singular vectors, and Σ is a diagonal matrix of singular values. This form is called the singular value decomposition for X.</p><p>The number of non-zero singular values for X is called its rank, which we denote by k ≤ ρ. The nuclear norm of X is defined as the ℓ 1 norm of its singular values, and denoted by</p><formula xml:id="formula_227">X * = ρ i=1 σ i</formula><p>It can be shown (see exercises) that the nuclear norm is equal to the trace of the square root of the matrix times its transpose, i.e.,</p><formula xml:id="formula_228">X * = Tr( √ X X)</formula><p>We denote by A • B the inner product of two matrices as vectors in R n×m , that is</p><formula xml:id="formula_229">A • B = n i=1 m j=1 A ij B ij = Tr(AB )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Motivation: matrix completion and recommendation systems</head><p>Media recommendations have changed significantly with the advent of the Internet and rise of online media stores. The large amounts of data collected allow for efficient clustering and accurate prediction of users' preferences for a variety of media. A well-known example is the so called "Netflix challenge"-a competition of automated tools for recommendation from a large dataset of users' motion picture preferences. One of the most successful approaches for automated recommendation systems, as proven in the Netflix competition, is matrix completion. Perhaps the simplest version of the problem can be described as follows.</p><p>The entire dataset of user-media preference pairs is thought of as a partially-observed matrix. Thus, every person is represented by a row in the matrix, and every column represents a media item (movie). For simplicity, let us think of the observations as binary-a person either likes or dislikes a particular movie. Thus, we have a matrix M ∈ {0, 1, * } n×m where n is the number of persons considered, m is the number of movies at our library, and 0/1 and * signify "dislike", "like" and "unknown" respectively:</p><formula xml:id="formula_230">M ij =            0, person i dislikes movie j 1, person i likes movie j * , preference unknown .</formula><p>The natural goal is to complete the matrix, i.e. correctly assign 0 or 1 to the unknown entries. As defined so far, the problem is ill-posed, since any completion would be equally good (or bad), and no restrictions have been placed on the completions.</p><p>The common restriction on completions is that the "true" matrix has low rank. Recall that a matrix X ∈ R n×m has rank k &lt; ρ = min{n, m} if and only if it can be written as</p><formula xml:id="formula_231">X = U V , U ∈ R n×k , V ∈ R k×m .</formula><p>The intuitive interpretation of this property is that each entry in M can be explained by only k numbers. In matrix completion this means, intuitively, that there are only k factors that determine a persons preference over movies, such as genre, director, actors and so on. Now the simplistic matrix completion problem can be well-formulated as in the following mathematical program. Denote by • OB the Euclidean norm only on the observed (non starred) entries of M , i.e.,</p><formula xml:id="formula_232">X 2 OB = M ij = * X 2 ij .</formula><p>The mathematical program for matrix completion is given by min</p><formula xml:id="formula_233">X∈R n×m 1 2 X -M 2 OB s.t. rank(X) ≤ k.</formula><p>Since the constraint over the rank of a matrix is non-convex, it is standard to consider a relaxation that replaces the rank constraint by the nuclear norm. It is known that the nuclear norm is a lower bound on the matrix rank if the singular values are bounded by one (see exercises). Thus, we arrive at the following convex program for matrix completion: min</p><formula xml:id="formula_234">X∈R n×m 1 2 X -M 2 OB (9.1) s.t. X * ≤ k.</formula><p>We consider algorithms to solve this convex optimization problem next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">The Frank-Wolfe method</head><p>In this section we consider minimization of a convex function over a convex domain.</p><p>The conditional gradient (CG) method, or Frank-Wolfe algorithm, is a simple algorithm for minimizing a smooth convex function f over a convex set K ⊆ R n . The appeal of the method is that it is a first order interior point method -the iterates always lie inside the convex set, and thus no projections are needed, and the update step on each iteration simply requires minimizing a linear objective over the set. The basic method is given in Algorithm 13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 13 Conditional gradient</head><p>1: Input: step sizes {η t ∈ (0, 1], t ∈ [T ]}, initial point x 1 ∈ K.</p><p>2: for t = 1 to T do 3:</p><p>v t ← arg min x∈K x ∇f (x t ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>x t+1 ← x t + η t (v t -x t ). 5: end for Note that in the CG method, the update to the iterate x t may be not be in the direction of the gradient, as v t is the result of a linear optimization procedure in the direction of the negative gradient. This is depicted in Figure <ref type="figure">9</ref>.1.</p><p>The following theorem gives an essentially tight performance guarantee of this algorithm over smooth functions. Recall our notation from Chapter 2: x denotes the global minimizer of f over K, D denotes the diameter of the set K, and h t = f (x t ) -f (x ) denotes the suboptimality of the objective value in iteration t.</p><p>Theorem 9.1. The CG algorithm applied to β-smooth functions with step sizes η t = min{ 2H t , 1}, for H ≥ max{1, h 1 }, attains the following convergence guarantee: Proof. As done before in this manuscript, we denote ∇ t = ∇f (x t ), and also denote H ≥ max{h 1 , 1}, such that η t = min{ 2H t , 1}. For any set of step sizes, we have</p><formula xml:id="formula_235">h t ≤ 2βHD 2 t</formula><formula xml:id="formula_236">f (x t+1 ) -f (x ) = f (x t + η t (v t -x t )) -f (x ) ≤ f (x t ) -f (x ) + η t (v t -x t ) ∇ t + η 2 t β 2 v t -x t 2 β-smoothness ≤ f (x t ) -f (x ) + η t (x -x t ) ∇ t + η 2 t β 2 v t -x t 2 v t optimality ≤ f (x t ) -f (x ) + η t (f (x ) -f (x t )) + η 2 t β 2 v t -x t 2 convexity of f ≤ (1 -η t )(f (x t ) -f (x )) + η 2 t β 2 D 2 . (9.2)</formula><p>We reached the recursion</p><formula xml:id="formula_237">h t+1 ≤ (1 -η t )h t + η 2 t βD 2</formula><p>2 , and by induction,</p><formula xml:id="formula_238">h t+1 ≤ (1 -η t )h t + η 2 t βD 2 2 ≤ (1 -η t ) 2βHD 2 t + η 2 t βD 2 2 induction hypothesis ≤ (1 - 2H t ) 2βHD 2 t + 4H 2 t 2 βD 2 2 value of η t = 2βHD 2 t - 2H 2 βD 2 t 2 ≤ 2βHD 2 t (1 - 1 t ) since H ≥ 1 ≤ 2βHD 2 t + 1 . t-1 t ≤ t t+1 9.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Projections vs. linear optimization</head><p>The conditional gradient (Frank-Wolfe) algorithm described before does not resort to projections, but rather computes a linear optimization problem of the form arg min</p><formula xml:id="formula_239">x∈K x u . (9.3)</formula><p>When is the CG method computationally preferable? The overall computational complexity of an iterative optimization algorithm is the product of the number of iterations and the computational cost per iteration. The CG method does not converge as well as the most efficient gradient descent algorithms, meaning it requires more iterations to produce a solution of a comparable level of accuracy. However, for many interesting scenarios the computational cost of a linear optimization step (9.3) is significantly lower than that of a projection step. Let us point out several examples of problems for which we have very efficient linear optimization algorithms, whereas our state-of-the-art algorithms for computing projections are significantly slower.</p><p>Recommendation systems and matrix prediction. In the example pointed out in the preceding section of matrix completion, known methods for projection onto the spectahedron, or more generally the bounded nuclearnorm ball, require singular value decompositions, which take superlinear time via our best known methods. In contrast, the CG method requires maximal eigenvector computations which can be carried out in linear time via the power method (or the more sophisticated Lanczos algorithm).</p><p>Network routing and convex graph problems. Various routing and graph problems can be modeled as convex optimization problems over a convex set called the flow polytope.</p><p>Consider a directed acyclic graph with m edges, a source node marked s and a target node marked t. Every path from s to t in the graph can be represented by its identifying vector, that is a vector in {0, 1} m in which the entries that are set to 1 correspond to edges of the path. The flow polytope of the graph is the convex hull of all such identifying vectors of the simple paths from s to t. This polytope is also exactly the set of all unit s-t flows in the graph if we assume that each edge has a unit flow capacity (a flow is represented here as a vector in R m in which each entry is the amount of flow through the corresponding edge).</p><p>Since the flow polytope is just the convex hull of s-t paths in the graph, minimizing a linear objective over it amounts to finding a minimum weight path given weights for the edges. For the shortest path problem we have very efficient combinatorial optimization algorithms, namely Dijkstra's algorithm.</p><p>Thus, applying the CG algorithm to solve any convex optimization problem over the flow polytope will only require iterative shortest path computations.</p><p>Ranking and permutations. A common way to represent a permutation or ordering is by a permutation matrix. Such are square matrices over {0, 1} n×n that contain exactly one 1 entry in each row and column.</p><p>Doubly-stochastic matrices are square, real-valued matrices with nonnegative entries, in which the sum of entries of each row and each column amounts to 1. The polytope that defines all doubly-stochastic matrices is called the Birkhoff-von Neumann polytope. The Birkhoff-von Neumann theorem states that this polytope is the convex hull of exactly all n × n permutation matrices.</p><p>Since a permutation matrix corresponds to a perfect matching in a fully connected bipartite graph, linear minimization over this polytope corresponds to finding a minimum weight perfect matching in a bipartite graph.</p><p>Consider a convex optimization problem over the Birkhoff-von Neumann polytope. The CG algorithm will iteratively solve a linear optimization problem over the BVN polytope, thus iteratively solving a minimum weight perfect matching in a bipartite graph problem, which is a well-studied combinatorial optimization problem for which we know of efficient algorithms. In contrast, other gradient based methods will require projections, which are quadratic optimization problems over the BVN polytope.</p><p>Matroid polytopes. A matroid is pair (E, I) where E is a set of elements and I is a set of subsets of E called the independent sets which satisfy various interesting proprieties that resemble the concept of linear independence in vector spaces. Matroids have been studied extensively in combinatorial optimization and a key example of a matroid is the graphical matroid in which the set E is the set of edges of a given graph and the set I is the set of all subsets of E which are cycle-free. In this case, I contains all the spanning trees of the graph. A subset S ∈ I could be represented by its identifying vector which lies in {0, 1} |E| which also gives rise to the matroid polytope which is just the convex hull of all identifying vectors of sets in I. It can be shown that some matroid polytopes are defined by exponentially many linear inequalities (exponential in |E|), which makes optimization over them difficult.</p><p>On the other hand, linear optimization over matroid polytopes is easy using a simple greedy procedure which runs in nearly linear time. Thus, the CG method serves as an efficient algorithm to solve any convex optimization problem over matroids iteratively using only a simple greedy procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">Exercises</head><p>1. Prove that if the singular values are smaller than or equal to one, then the nuclear norm is a lower bound on the rank, i.e., show rank(X) ≥ X * .</p><p>2. Prove that the trace is related to the nuclear norm via</p><formula xml:id="formula_240">X * = Tr( √ XX ) = Tr( √ X X).</formula><p>3. Show that maximizing a linear function over the spectahedron is equivalent to a maximal eigenvector computation. That is, show that the following mathematical program:</p><formula xml:id="formula_241">min X • C X ∈ S d = {X ∈ R d×d , X 0 , Tr(X) ≤ 1},</formula><p>is equivalent to the following:</p><formula xml:id="formula_242">min x∈R d x Cx s.t. x 2 ≤ 1.</formula><p>4. Download the MovieLens dataset from the web. Implement an online recommendation system based on the matrix completion model: implement the OCG and OGD algorithms for matrix completion. Benchmark your results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6">Bibliographic Remarks</head><p>The matrix completion model has been extremely popular since its inception in the context of recommendation systems <ref type="bibr" target="#b82">[80,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b71">69,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" target="#b16">14,</ref><ref type="bibr" target="#b77">75]</ref>. The conditional gradient algorithm was devised in the seminal paper by Frank and Wolfe <ref type="bibr" target="#b23">[21]</ref>. Due to the applicability of the FW algorithm to large-scale constrained problems, it has been a method of choice in recent machine learning applications, to name a few: <ref type="bibr" target="#b44">[42,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b22">20,</ref><ref type="bibr" target="#b32">30,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b74">72,</ref><ref type="bibr" target="#b9">7,</ref><ref type="bibr" target="#b84">82,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b10">8]</ref>.</p><p>The online conditional gradient algorithm is due to <ref type="bibr" target="#b38">[36]</ref>. An optimal regret algorithm, attaining the O( √ T ) bound, for the special case of polyhedral sets was devised in <ref type="bibr" target="#b25">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 10</head><p>Second order methods for machine learning At this point in our course, we have exhausted the main techniques in first-order (or gradient-based) optimization. We have studied the main workhorse -stochastic gradient descent, the three acceleration techniques, and projection-free gradient methods. Have we exhausted optimization for ML?</p><p>In this section we discuss using higher derivatives of the objective function to accelerate optimization. The canonical method is Newton's method, which involves the second derivative or Hessian in high dimensions. The vanilla approach is computationally expensive since it involves matrix inversion in high dimensions that machine learning problems usually require.</p><p>However, recent progress in random estimators gives rise to linear-time second order methods, for which each iteration is as computationally cheap as gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Motivating example: linear regression</head><p>In the problem of linear regression we are given a set of measurements {a i ∈ R d , b i ∈ R}, and the goal is to find a set of weights that explains them best in the mean squared error sense. As a mathematical program, the goal is to optimize:</p><formula xml:id="formula_243">min x∈R d    1 2 i∈[m] a i x -b i 2    , or in matrix form, min x f (x) = 1 2 Ax -b 2 .</formula><p>Here A ∈ R m×d , b ∈ R m . Notice that the objective function f is smooth, but not necessarily strongly convex. Therefore, all algorithms that we have studied so far without exception, which are all first order methods, attain rates which are poly( 1 ε ). However, the linear regression problem has a closed form solution that can be computed by taking the gradient to be zero, i.e. (Ax -b) A = 0, which gives</p><formula xml:id="formula_244">x = (A A) -1 A b.</formula><p>The Newton direction is given by the inverse Hessian multiplied by the gradient, ∇ -2 f (x)∇f (x). Observe that a single Newton step, i.e. moving in the Newton direction with step size one, from any direction gets us directly to the optimal solution in one iteration! (see exercises)</p><p>More generally, Newton's method yields O(log 1 ε ) convergence rates for a large class of functions without dependence on the condition number of the function! We study this property next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Self-Concordant Functions</head><p>In this section we define and collect some of the properties of a special class of functions, called self-concordant functions. These functions allow Newton's method to run in time which is independent of the condition number. The class of self-concordant functions is expressive and includes quadratic functions, logarithms of inner products, a variety of barriers such as the log determinant, and many more.</p><p>An excellent reference for this material is the lecture notes on this subject by Nemirovski <ref type="bibr" target="#b57">[55]</ref>. We begin by defining self-concordant functions. Definition 10.1 (Self-Concordant Functions). Let K ⊆ R n be a non-empty open convex set, and and let f : K → R be a C 3 convex function. Then, f is said to be self-concordant if</p><formula xml:id="formula_245">|∇ 3 f (x)[h, h, h]| ≤ 2(h ∇ 2 f (x)h) 3/2 ,</formula><p>where we have</p><formula xml:id="formula_246">∇ k f (x)[h 1 , . . . , h k ] ∂ k ∂t 1 . . . ∂t k | t 1 =•••=t k f (x + t 1 h 1 + • • • + t k h k ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2.">NEWTON'S METHOD</head><p>Another key object in the analysis of self concordant functions is the notion of a Dikin Ellipsoid, which is the unit ball around a point in the norm given by the Hessian • ∇ 2 f at the point. We will refer to this norm as the local norm around a point and denote it as • x . Formally, Definition 10.2 (Dikin ellipsoid). The Dikin ellipsoid of radius r centered at a point x is defined as</p><formula xml:id="formula_247">E r (x) {y | y -x ∇ 2 f (x) ≤ r}</formula><p>One of the key properties of self-concordant functions that we use is that inside the Dikin ellipsoid, the function is well conditioned with respect to the local norm at the center. The next lemma makes this formal. The proof of this lemma can be found in <ref type="bibr" target="#b57">[55]</ref>.</p><p>Lemma 10.3 (See <ref type="bibr" target="#b57">[55]</ref>). For all h such that h x &lt; 1 we have that</p><formula xml:id="formula_248">(1 -h x ) 2 ∇ 2 f (x) ∇ 2 f (x + h) 1 (1 -h x ) 2 ∇ 2 f (x)</formula><p>Another key quantity, which is used both as a potential function as well as a dampening for the step size in the analysis of Newton's method, is the Newton Decrement:</p><formula xml:id="formula_249">λ x ∇f (x) * x = ∇f (x) ∇ -2 f (x)∇f (x).</formula><p>The following lemma quantifies how λ x behaves as a potential by showing that once it drops below 1, it ensures that the minimum of the function lies in the current Dikin ellipsoid. This is the property which we use crucially in our analysis. The proof can be found in <ref type="bibr" target="#b57">[55]</ref>.</p><p>Lemma 10.4 (See <ref type="bibr" target="#b57">[55]</ref>).</p><formula xml:id="formula_250">If λ x &lt; 1 then x -x * x ≤ λ x 1 -λ x 10.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Newton's method for self-concordant functions</head><p>Before introducing the linear time second order methods, we start by introducing a robust Newton's method and its properties. The pseudo-code is given in Algorithm 14.</p><p>The usual analysis of Newton's method allows for quadratic convergence, i.e. error ε in O(log log 1 ε ) iterations for convex objectives. However, we prefer to present a version of Newton's method which is robust to certain random estimators of the Newton direction. This yields a slower rate of O(log 1 ε ). The faster running time per iteration, which does not require matrix manipulations, more than makes up for this.</p><p>Algorithm 14 Robust Newton's method Input: T,</p><formula xml:id="formula_251">x 1 for t = 1 to T do Set c = 1 8 , η = min{c, c 8λx t }. Let 1 2 ∇ -2 f (x t ) ∇-2 t 2∇ -2 f (x t ). x t+1 = x t -η ∇-2 t ∇f (x t ) end for return x T +1</formula><p>It is important to notice that every two consecutive points are within the same Dikin ellipsoid of radius 1  2 . Denote ∇ t = ∇ xt , and similarly for the Hessian. Then we have:</p><formula xml:id="formula_252">x t -x t+1 2 xt = η 2 ∇ t ∇-2 t ∇ 2 t ∇-2 t ∇ t ≤ 4η 2 λ 2 t ≤ 1 2 .</formula><p>The advantage of Newton's method as applied to self-concordant functions is its linear convergence rate, as given in the following theorem.</p><p>Theorem 10.5. Let f be self-concordant, and f (x 1 ) ≤ M , then</p><formula xml:id="formula_253">h t = f (x t ) -f (x * ) ≤ O(M + log 1 ε )</formula><p>The proof of this theorem is composed of two steps, according to the magnitude of the Newton decrement.</p><p>Phase 1: damped Newton Lemma 10.6. As long as λ x ≥ 1 8 , we have that</p><formula xml:id="formula_254">h t ≤ - 1 4 c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3.">NEWTON'S METHOD FOR SELF-CONCORDANT FUNCTIONS99</head><p>Proof. Using similar analysis to the descent lemma we have that</p><formula xml:id="formula_255">f (x t+1 ) -f (x t ) ≤ ∇ t (x t+1 -x t ) + 1 2 (x t -x t+1 ) ∇ 2 (ζ)(x t -x t+1 ) Taylor ≤ ∇ t (x t+1 -x t ) + 1 4 (x t -x t+1 ) ∇ 2 (x t )(x t -x t+1 ) x t+1 ∈ E 1/2 (x t ) = -η∇ t ∇-2 t ∇ t + 1 4 η 2 ∇ t ∇-2 t ∇ 2 t ∇-2 t ∇ t = -ηλ 2 t + 1 4 η 2 λ 2 t ≤ -1 16 c</formula><p>The conclusion from this step is that after O(M ) steps, Algorithm 14 reaches a point for which λ x ≤ 1 8 . According to Lemma 10.4, we also have that xx *</p><p>x ≤ 1 4 , that is, the optimum is in the same Dikin ellipsoid as the current point.</p><p>Phase 2: pure Newton In the second phase our step size is changed to be larger. In this case, we are guaranteed that the Newton decrement is less than one, and thus we know that the global optimum is in the same Dikin ellipsoid as the current point. In this ellipsoid, all Hessians are equivalent up to a factor of two, and thus Mirrored-Descent with the inverse Hessian as preconditioner becomes gradient descent. We make this formal below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 15 Preconditioned Gradient Descent</head><p>Input: P, T for t = 1 to T do x t+1 = x t -ηP -1 ∇f (x t ) end for return x T +1 Lemma 10.7. Suppose that 1  2 P ∇ 2 f (x) 2P , and x 1 -x * P ≤ 1 2 , then Algorithm 15 converges as</p><formula xml:id="formula_256">h t+1 ≤ h 1 e -1 8 t .</formula><p>This theorem follows from noticing that the function g(z) = f (P -1/2 x) is 1  2 -strongly convex and 2-smooth, and using Theorem 3.2. It can be shown that gradient descent on g is equivalent to Newton's method in f . Details are left as an exercise.</p><p>An immediate corollary is that Newton's method converges at a rate of O(log 1 ε ) in this phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Linear-time second-order methods</head><p>Newton's algorithm is of foundational importance in the study of mathematical programming in general. A major application are interior point methods for convex optimization, which are the most important polynomial-time algorithms for general constrained convex optimization. However, the main downside of this method is the need to maintain and manipulate matrices -namely the Hessians. This is completely impractical for machine learning applications in which the dimension is huge.</p><p>Another significant downside is the non-robust nature of the algorithm, which makes applying it in stochastic environments challenging.</p><p>In this section we show how to apply Newton's method to machine learning problems. This involves relatively new developments that allow for linear-time per-iteration complexity, similar to SGD, and theoretically superior running times. At the time of writing, however, these methods are practical only for convex optimization, and have not shown superior performance on optimization tasks involving deep neural networks.</p><p>The first step to developing a linear time Newton's method is an efficient stochastic estimator for the Newton direction, and the Hessian inverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.1">Estimators for the Hessian Inverse</head><p>The key idea underlying the construction is the following well known fact about the Taylor series expansion of the matrix inverse.</p><p>Lemma 10.8. For a matrix A ∈ R d×d such that A 0 and A ≤ 1, we have that</p><formula xml:id="formula_257">A -1 = ∞ i=0 (I -A) i</formula><p>We propose two unbiased estimators based on the above series. To define the first estimator pick a probability distribution over non-negative integers {p i } and sample î from the above distribution. Let X 1 , . . . X î be independent samples of the Hessian ∇ 2 f and define the estimator as Definition 10.9 (Estimator 1).</p><formula xml:id="formula_258">∇-2 f = 1 p î î j=1 (I -X j )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.">LINEAR-TIME SECOND-ORDER METHODS</head><p>Observe that our estimator of the Hessian inverse is unbiased, i.e. E[ X] = ∇ -2 f at any point. Estimator 1 has the disadvantage that in a single sample it incorporates only one term of the Taylor series.</p><p>The second estimator below is based on the observation that the above series has the following succinct recursive definition, and is more efficient.</p><p>For a matrix A define</p><formula xml:id="formula_259">A -1 j = j i=0 (I -A) i</formula><p>i.e. the first j terms of the above Taylor expansion. It is easy to see that the following recursion holds for</p><formula xml:id="formula_260">A -1 j A -1 j = I + (I -A)A -1 j-1</formula><p>Using the above recursive formulation, we now describe an unbiased estimator of ∇ -2 f by deriving an unbiased estimator ∇-2 f j for ∇ -2 f j . Definition 10.10 (Estimator 2). Given j independent and unbiased samples {X</p><formula xml:id="formula_261">1 . . . X j } of the hessian ∇ 2 f . Define { ∇-2 f 0 . . . ∇-2 f j } recursively as follows ∇-2 f 0 = I ∇-2 f t = I + (I -X j ) ∇-2 f t-1</formula><p>It can be readily seen that E[ ∇-2 f j ] = ∇ -2 f j and therefore E[ ∇-2 f j ] → ∇ -2 f as j → ∞ giving us an unbiased estimator in the limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.2">Incorporating the estimator</head><p>Both of the above estimators can be computed using only Hessian-vector products, rather than matrix manipulations. For many machine learning problems, Hessian-vector products can be computed in linear time. Examples include:</p><p>1. Convex regression and SVM objectives over training data have the form min</p><formula xml:id="formula_262">w f (w) = E i [ℓ(w x i )],</formula><p>where ℓ is a convex function. The Hessian can thus be written as</p><formula xml:id="formula_263">∇ 2 f (w) = E i [ℓ (w x i )x i x i ]</formula><p>Thus, the first Newton direction estimator can now be written as</p><formula xml:id="formula_264">∇2 f (w)∇ w = E j∼D [ j i=1 (I -ℓ (w x i )x i x i )]∇ w .</formula><p>Notice that this estimator can be computed using j vector-vector products if the ordinal j was randomly chosen.</p><p>2. Non-convex optimization over neural networks: a similar derivation as above shows that the estimator can be computed only using Hessianvector products. The special structure of neural networks allow this computation in a constant number of backpropagation steps, i.e. linear time in the network size, this is called the "Pearlmutter trick", see <ref type="bibr" target="#b63">[61]</ref>.</p><p>We note that non-convex optimization presents special challenges for second order methods, since the Hessian need not be positive semidefinite. Nevertheless, the techniques presented hereby can still be used to provide theoretical speedups for second order methods over first order methods in terms of convergence to local minima. The details are beyond our scope, and can be found in <ref type="bibr">[2]</ref>.</p><p>Putting everything together. These estimators we have studied can be used to create unbiased estimators to the Newton direction of the form ∇-2</p><p>x ∇ x for ∇-2</p><p>x which satisfies</p><formula xml:id="formula_265">1 2 ∇ -2 f (x t ) ∇-2 t 2∇ -2 f (x t ).</formula><p>These can be incorporated into Algorithm 14, which we proved is capable of obtaining fast convergence with approximate Newton directions of this form.</p><p>1. The search space is often discrete (for example, number of layers). As such, there is no natural notion of gradient or differentials and it is not clear how to apply the iterative methods we have studied thus far.</p><p>2. Even evaluating the objective function is extremely expensive (think of evaluating the test error of the trained neural network). Thus it is crucial to minimize the number of function evaluations, whereas other computations are significantly less expensive.</p><p>3. Evaluating the function can be done in parallel. As an example, training feedforward deep neural networks over different architectures can be done in parallel.</p><p>More formally, we consider the following optimization problem min</p><formula xml:id="formula_266">x i ∈GF (q i ) f (x),</formula><p>where x is the representation of discrete hyperparameters, each taking value from q i ≥ 2 possible discrete values and thus in GF (q), the Galois field of order q. The example to keep in mind is that the objective f (x) is the test error of the neural network trained with hyperparameters x. Note that x has a search space of size i q i ≥ 2 n , exponentially large in the number of different hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">Hyperparameter optimization algorithms</head><p>The properties of the problem mentioned before prohibits the use of the algorithms we have studied thus far, which are all suitable for continuous optimization. A naive method is to perform a grid search over all hyperparameters, but this quickly becomes infeasible. An emerging field of research in recent years, called AutoML, aims to choose hyperparameters automatically. The following techniques are in common use:</p><p>• Grid search, try all possible assignments of hyperparameters and return the best. This becomes infeasible very quickly with n -the number of hyperparameters.</p><p>• Random search, where one randomly picks some choices of hyperparameters, evaluates their function objective, and chooses the one choice of hyperparameters giving best performance. An advantage of this method is that it is easy to implement in parallel.</p><p>• Successive Halving and Hyperband, random search combined with early stopping using multi-armed bandit techniques. These gain a small constant factor improvement over random search.</p><p>• Bayesian optimization, a statistical approach which has a prior over the objective and tries to iteratively pick an evaluation point which reduces the variance in objective value. Finally it picks the point that attains the lowest objective objective with highest confidence. This approach is sequential in nature and thus difficult to parallelize. Another important question is how to choose a good prior.</p><p>The hyperparameter optimization problem is essentially a combinatorial optimization problem with exponentially large search space. Without further assumptions, this optimization problem is information-theoretically hard. Such assumptions are explored in the next section with an accompanying algorithm.</p><p>Finally, we note that a simple but hard-to-beat benchmark is random search with double budget. That is, compare the performance of a method to that of random search, but allow random search double the query budget of your own method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3">A Spectral Method</head><p>For simplicity, in this section we consider the case in which hyperparameters are binary. This retains the difficulty of the setting, but makes the mathematical derivation simpler. The optimization problem now becomes min x∈{-1,1} n f (x).</p><p>(11.1)</p><p>The method we describe in this section is inspired by the following key observation: although the whole search space of hyperparameters is exponentially large, it is often the case in practice that only a few hyperparameters together play a significant role in the performance of a deep neural network.</p><p>To make this intuition more precise, we need some definitions and facts from Fourier analysis of Boolean functions. where d is the upper bound of polynomial degree, and k is the sparsity of Fourier coefficient α α α (indexed by S) in ℓ 1 sense (which is a convex relaxation of α α α 0 , the true sparsity).</p><p>Remark 11.3. Clearly this assumption does not always hold. For example, many deep reinforcement learning algorithms nowadays rely heavily on the choice of the random seed, which can also be seen as a hyperparameter. If x ∈ {-1, 1} 32 is the bit representation of a int32 random seed, then there is no reason to assume that a few of these bits should play a more significant role than the others.</p><p>Under this assumption, all we need to do now is to find out the few important sets of variables S's, as well as their coefficients α S 's, in the approximation (11.2). Fortunately, there is already a whole area of research, called compressed sensing, that aims to recover a high-dimensional but sparse vector, using only a few linear measurements. Next, we will briefly introduce the problem of compressed sensing, and one useful result from the literature. After that, we will introduce the Harmonica algorithm, which applies compressed sensing techniques to solve the hyperparameter optimization problem (11.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3.1">Background: Compressed Sensing</head><p>The problem of compressed sensing is as follows. Suppose there is a hidden signal x ∈ R n that we cannot observe. In order to recover x, we design a measurement matrix A ∈ R m×n , and obtain noisy linear measurements y = Ax + η η η ∈ R m , where η η η is some random noise. The difficulty arises when we have a limited budget for measurements, i.e. m n. Note that even without noise, recovering x is non-trivial since y = Ax is an underdetermined linear system, therefore if there is one solution x that solves this linear system, there will be infinitely many solutions. The key to this problem is to assume that x is k-sparse, that is, x 0 ≤ k. This assumption has been justified in various real-world applications; for example, natural images tend to be sparse in the Fourier/wavelet domain, a property which forms the bases of many image compression algorithms.</p><p>Under the assumption of sparsity, the natural way to recover x is to solve a least squares problem, subject to some sparsity constraint x 0 ≤ k. However, ℓ 0 norm is difficult to handle, and it is often replaced by ℓ 1 norm, its convex relaxation. One useful result from the literature of compressed sensing is the following. Proposition 11.4 (Informal statement of Theorem 4.4 in <ref type="bibr" target="#b65">[63]</ref>). Assume the ground-truth signal x ∈ R n is k-sparse. Then, with high probability, using a randomly designed A ∈ R m×n that is "near-orthogonal" (random Gaussian matrix, subsampled Fourier basis, etc.), with m = O(k log(n)/ε) and η η η This result is remarkable; in particular, it says that the number of measurements needed to recover a sparse signal is independent of the dimension n (up to a logarithm term), but only depends on the sparsity k and the desired accuracy ε. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3.2">The Spectral Algorithm</head><p>The main idea is that, under Assumption 11.2, we can view the problem of hyperparameter optimization as recovering the sparse signal α α α from linear measurements. More specifically, we need to query T random samples, f (x 1 ), . . . , f (x T ), and then solve the LASSO problem min</p><formula xml:id="formula_267">α α α T t=1 ( |S|≤d α S χS (x t ) -f (x t )) 2 + λ α α α 1 ,<label>(11.4)</label></formula><p>where the regularization term λ α α α 1 controls the sparsity of α α α. Also note that the constraint |S| ≤ d not only implies that the solution is a low-degree polynomial, but also helps to reduce the "effective" dimension of α α α from α S i χS i (x), which involves only a few dimensions of x since the LASSO solution is sparse and low-degree. The next step is to set the variables outside ∪ i∈[s] S i to arbitrary values, and compute a minimizer x * ∈ arg min g(x). In other words, we have reduced the original problem of optimizing f (x) over n variables, to the problem of optimizing g(x) (an approximation of f (x)) over only a few variables (which is now feasible to solve). One remarkable feature of this algorithm is that the returned solution x * may not belong to the samples {x 1 , . . . , x T }, which is not the case for other existing methods (such as random search). Using theoretical results from compressed sensing (e.g. Proposition 11.4), we can derive the following guarantee for the sparse recovery of α α α via LASSO.</p><p>Theorem 11.6 (Informal statement of Lemma 7 in <ref type="bibr" target="#b40">[38]</ref>). Assume f is ksparse in the Fourier expansion. Then, with T = O(k 2 log(n)/ε) samples, the solution of the LASSO problem (11.4) achieves ε accuracy.</p><p>Finally, the above derivation can be considered as only one stage in a multi-stage process, each iteratively setting the value of a few more variables that are the most significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.4">Bibliographic Remarks</head><p>For a nice exposition on hyperparameter optimization see <ref type="bibr" target="#b66">[64,</ref><ref type="bibr" target="#b67">65]</ref>, in which the the benchmark of comparing to Random Search with double queries was proposed.</p><p>Perhaps the simplest approach to HPO is random sampling of different choices of parameters and picking the best amongst the chosen evaluations <ref type="bibr" target="#b11">[9]</ref>. Successive Halving (SH) algorithm was introduced <ref type="bibr" target="#b45">[43]</ref>. Hyperband further improves SH by automatically tuning the hyperparameters in SH <ref type="bibr" target="#b53">[51]</ref>.</p><p>The Bayesian optimization (BO) methodology is currently the most studied in HPO. For recent studies and algorithms of this flavor see <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b80">78,</ref><ref type="bibr" target="#b83">81,</ref><ref type="bibr" target="#b81">79,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b87">84,</ref><ref type="bibr" target="#b42">40]</ref>.</p><p>The spectral approach for hyperparameter optimization was introduced in <ref type="bibr" target="#b40">[38]</ref>. For an in-depth treatment of compressed sensing see the survey of <ref type="bibr" target="#b65">[63]</ref>, and for Fourier analysis of Boolean functions see <ref type="bibr" target="#b61">[59]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Professor Arkadi Nemirovski, Pioneer of mathematical optimization</figDesc><graphic coords="4,194.21,193.82,270.00,370.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Figure 2.1: Pythagorean theorem.</figDesc><graphic coords="21,154.22,380.08,252.01,189.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 2 . 1 (</head><label>21</label><figDesc>Pythagoras, circa 500 BC). Let K ⊆ R d be a convex set, y ∈ R d and x = ΠK(y). Then for any z ∈ K we have yz ≥ xz .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 2 . 2 (</head><label>22</label><figDesc>Karush-Kuhn-Tucker). Let K ⊆ R d be a convex set, x ∈ arg min x∈K f (x). Then for any y ∈ K we have ∇f (x ) (y -x ) ≥ 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 . 2 :</head><label>22</label><figDesc>Figure 2.2: Optimality conditions: negative (sub)gradient pointing outwards.</figDesc><graphic coords="23,136.22,196.65,288.00,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 . 3 :</head><label>23</label><figDesc>Figure 2.3: First and second-order local optima.</figDesc><graphic coords="24,223.03,127.67,216.00,121.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Lemma 3 . 1 (</head><label>31</label><figDesc>Backpropagation lemma). The gradient of f can be computed in time O(|E|).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 . 3 .</head><label>23</label><figDesc>Given an ERM problem (a.k.a. learning from examples, see first chapter), what can we say about generalization to unseen examples? How does it affect optimization? Are there faster algorithms than SGD in the context of ML?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>0 1 Figure 4 . 1 :</head><label>141</label><figDesc>Figure 4.1: Intractability of nonsmooth optimization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Theorem 4 . 2 .</head><label>42</label><figDesc>Online gradient descent with step sizes {η t = D G √ t , t ∈ [T ]} guarantees the following for all T ≥ 1:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: Online gradient descent: the iterate x t+1 is derived by advancing x t in the direction of the current gradient ∇ t , and projecting back into K. Algorithm 4 online gradient descent 1: Input: convex set K, T , x 1 ∈ K, step sizes {η t } 2: for t = 1 to T do 3:</figDesc><graphic coords="45,136.22,127.67,288.00,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 :</head><label>2</label><figDesc>Let x 1 = arg min x∈K {R(x)}. 3: for t = 1 to T do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Theorem 6 . 3 .</head><label>63</label><figDesc>Let {x t } be defined by Algorithm 9 with parameters η = D, where D = max u∈K ux 1 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Theorem 8 . 1 .</head><label>81</label><figDesc>Algorithm 12 converges to an ε-approximate solution to optimization problem (8.1) in O( 1 √ ε ) iterations. The proof starts with the following lemma which follows from our earlier standard derivations. Lemma 8.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 . 1 :</head><label>91</label><figDesc>Figure 9.1: Direction of progression of the conditional gradient algorithm.</figDesc><graphic coords="97,154.22,127.66,252.01,189.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fact 11 . 1 .Assumption 11 . 2 .</head><label>111112</label><figDesc>Any function f : {-1, 1} n → [-1, 1] can be uniquely represented in the Fourier basisf (x) = S⊆[n] α s χS (x),where each Fourier basis functionχS (x) = i∈S x i .is a monomial, and thus f (x) has a polynomial representation.Now we are ready to formalize our key observation in the following assumption:The objective function f in the hyperparameter optimization problem (11.1) is low degree and sparse in the Fourier basis, i.e.f (x) ≈ |S|≤d α S χS (x), α α α 1 ≤ k,(11.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><figDesc>2 = O( √ m),x can be recovered by a convex programmin z∈R n y -Az 2 2 s.t. z 1 ≤ k,(11.3)with accuracy xz 2 ≤ ε.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>1 Remark 11 . 5 . 2 + λ z 1 ,</head><label>111521</label><figDesc>The convex program(11.3) is equivalent to the following LASSO problemmin z∈R n y -Az 2with a proper choice of regularization parameter λ. The LASSO problem is an unconstrained convex program, and has efficient solvers, as per the algorithms we have studied in this course.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><figDesc>2 n to O(n d ), which makes it feasible to solve this LASSO problem. Denote by S 1 , . . . , S s the indices of the s largest coefficients of the LASSO solution, and define g(x) = i∈[s]</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It also depends on the desired high-probability bound, which is omitted in this informal statement.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.6">Bibliographic Remarks</head><p>The modern application of Newton's method to convex optimization was put forth in the seminal work of Nesterov and Nemirovski <ref type="bibr" target="#b60">[58]</ref> on interior point methods. A wonderful exposition is Nemirovski's lecture notes <ref type="bibr" target="#b57">[55]</ref>.</p><p>The fact that Hessian-vector products can be computed in linear time for feed forward neural networks was described in <ref type="bibr" target="#b63">[61]</ref>. Linear time second order methods for machine learning and the Hessian-vector product model in machine learning was introduced in <ref type="bibr" target="#b6">[4]</ref>. This was extended to non-convex optimization for deep learning in <ref type="bibr">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 11 Hyperparameter Optimization</head><p>Thus far in this class, we have been talking about continuous mathematical optimization, where the search space of our optimization problem is continuous and mostly convex. For example, we have learned about how to optimize the weights of a deep neural network, which take continuous real values, via various optimization algorithms (SGD, AdaGrad, Newton's method, etc.).</p><p>However, in the process of training a neural network, there are some meta parameters, which we call hyperparameters, that have a profound effect on the final outcome. These are global, mostly discrete, parameters that are treated differently by algorithm designers as well as by engineers. Examples include the architecture of the neural network (number of layers, width of each layer, type of activation function, ...), the optimization scheme for updating weights (SGD/AdaGrad, initial learning rate, decay rate of learning rate, momentum parameter, ...), and many more. Roughly speaking, these hyperparameters are chosen before the training starts.</p><p>The purpose of this chapter is to formalize this problem as an optimization problem in machine learning, which requires a different methodology than we have treated in the rest of this course. We remark that hyperparameter optimization is still an active area of research and its theoretical properties are not well understood as of this time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Formalizing the problem</head><p>What makes hyperparameters different from "regular" parameters?</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Prove that a single Newton step for linear regression yields the optimal solution</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Let f : R d → R, and consider the affine transformation y = Ax, for A ∈ R d×d being a symmetric matrix</title>
	</analytic>
	<monogr>
		<title level="m">Prove that y t+1 ← y t -η∇f</title>
		<imprint/>
	</monogr>
	<note>y t ) is equivalent to x t+1 ← x t -ηA -2 ∇f (x t</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Prove that the function g(z) defined in phase 2 of the robust Newton algorithm is 1 2 -strongly convex and 2-smooth</title>
		<idno>10.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Competing in the dark: An efficient algorithm for bandit linear optimization</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Conference on Learning Theory</title>
		<meeting>the 21st Annual Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding approximate local minima faster than gradient descent</title>
		<author>
			<persName><forename type="first">Naman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Bullins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 49th Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1195" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Naman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Bullins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02958</idno>
		<title level="m">The case for full-matrix adaptive regularization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Second-order stochastic optimization for machine learning in linear time</title>
		<author>
			<persName><forename type="first">Naman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Bullins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4148" to="4187" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Linear coupling: An ultimate unification of gradient and mirror descent</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Orecchia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.1537</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Memoryefficient adaptive optimization for large-scale learning</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11150</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the equivalence between herding and conditional gradient algorithms</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML &apos;12</title>
		<editor>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</editor>
		<meeting>the 29th International Conference on Machine Learning (ICML-12), ICML &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012-07">July 2012</date>
			<biblScope unit="page" from="1359" to="1366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distributed frank-wolfe algorithm: A unified framework for communication-efficient sparse learning</title>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Bagheri Garakani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno>CoRR, abs/1404.2644</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random search for hyperparameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02">February 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Borwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<title level="m">Convex Analysis and Nonlinear Optimization: Theory and Examples</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004-03">March 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convex optimization: Algorithms and complexity</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="231" to="357" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Prediction, Learning, and Games</title>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Cesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Extreme tensoring for low-memory preconditioning</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04620</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Qi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00553</idno>
		<title level="m">Optimal adaptive and accelerated stochastic gradient descent</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT 2010 -The 23rd Conference on Learning Theory</title>
		<meeting><address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">June 27-29, 2010. 2010</date>
			<biblScope unit="page" from="257" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lifted coordinate descent for learning with trace-norm regularization</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaïd</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Malick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="327" to="336" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="149" to="154" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Approximating semidefinite programs in sublinear time</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1080" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Playing non-linear games with linear oracles</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian optimization with inequality constraints</title>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Zhixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014. 2014</date>
			<biblScope unit="page" from="937" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">General convergence results for linear discriminant updates</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Littlestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="173" to="210" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06569</idno>
		<title level="m">A unified approach to adaptive regularization in online and stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><surname>Shampoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09568</idno>
		<title level="m">Preconditioned stochastic tensor optimization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Approximation to bayes risk in repeated play</title>
		<author>
			<persName><forename type="first">James</forename><surname>Hannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to the Theory of Games</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Dresher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Tucker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1957">1957</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="97" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large-scale image classification with trace-norm regularization</title>
		<author>
			<persName><forename type="first">Zaïd</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattis</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Malick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3386" to="3393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Introduction to online convex optimization</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends Â R in Optimization</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="157" to="325" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Logarithmic regret algorithms for online convex optimization</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="169" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Revisiting the polyak step size</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00313</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Extracting certainty from uncertainty: Regret bounded by variation in costs</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 21st Annual Conference on Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="421" to="436" />
		</imprint>
		<respStmt>
			<orgName>Journal of Machine Learning Research -Proceedings Track</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Projection-free online learning</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2489" to="2512" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hyperparameter optimization: A spectral approach</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Klivans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Annette Shoemaker. Efficient hyperparameter optimization for deep learning algorithms using deterministic RBF surrogates</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taimoor</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">February 4-9, 2017. 2017</date>
			<biblScope unit="page" from="822" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Revisiting frank-wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A simple algorithm for nuclear norm regularized problems</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marek</forename><surname>Sulovský</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Non-stochastic best arm identification and hyperparameter optimization</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">G</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics<address><addrLine>Cadiz, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-09">2016. May 9-11, 2016. 2016</date>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient algorithms for online decision problems</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="307" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Exponentiated gradient versus gradient descent for linear predictors</title>
		<author>
			<persName><forename type="first">Jyrki</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Comput</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="63" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Relative loss bounds for multidimensional regression problems</title>
		<author>
			<persName><forename type="first">Jyrki</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="329" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Block-coordinate frank-wolfe optimization for structural svms</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">W</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pletscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">June 2013. 2013</date>
			<biblScope unit="page" from="53" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Practical large-scale optimization for max-norm regularization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1297" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03">March 2016</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adaptive bound optimization for online convex optimization</title>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Streeter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT 2010 -The 23rd Conference on Learning Theory</title>
		<meeting><address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">June 27-29, 2010. 2010</date>
			<biblScope unit="page" from="244" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Problem Complexity and Method Efficiency in Optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arkadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><surname>Yudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>John Wiley UK/USA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Interior point polynomial time methods in convex programming</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nemirovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interior point polynomial time methods in convex programming</title>
		<author>
			<persName><surname>Nemirovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Mathematics Doklady</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="372" to="376" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Introductory Lectures on Convex Optimization: A Basic Course. Applied Optimization</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Interior Point Polynomial Algorithms in Convex Programming</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nemirovskii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>SIAM, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Analysis of Boolean Functions</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><surname>Donnell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">New adaptive algorithms for online classification</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 24th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="1840" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fast exact multiplication by the hessian</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Making gradient descent optimal for strongly convex stochastic optimization</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Compressive sensing and structured random matrices. Theoretical foundations and numerical methods for sparse recovery</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Rauhut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<ptr target="http://www.argmin.net/2016/06/23/hyperband/" />
		<title level="m">Embracing the random</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">The news on auto-tuning</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<ptr target="http://www.argmin.net/2016/06/20/hypertuning/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fast maximum margin matrix factorization for collaborative prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Jasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on Machine Learning, ICML &apos;05</title>
		<meeting>the 22Nd International Conference on Machine Learning, ICML &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="713" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1951</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex Analysis. Convex Analysis</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Collaborative filtering in a nonuniform world: Learning with the weighted trace norm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2056" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Minimizing finite sums with the stochastic average gradient</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="112" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Online Learning: Theory, Algorithms, and Applications</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>The Hebrew University of Jerusalem</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Large-scale convex minimization with a low-rank constraint</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A primal-dual perspective of online learning algorithms</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Shwartz</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="115" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pegasos: primal estimated sub-gradient solver for svm</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="30" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Collaborative filtering with the trace norm: Learning, bounding, and transducing</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="661" to="678" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04235</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">December 3-6, 2012. 2012</date>
			<biblScope unit="page" from="2960" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Input warping for bayesian optimization of non-stationary functions</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014. 2014</date>
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Learning with Matrix Factorizations</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Multi-task bayesian optimization</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prescott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">December 5-8, 2013. 2013</date>
			<biblScope unit="page" from="2004" to="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Greedy algorithms for structurally constrained high dimensional problems</title>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="882" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Turing</surname></persName>
		</author>
		<title level="m">Computing machinery and intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">236</biblScope>
			<biblScope unit="page" from="433" to="460" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Bayesian optimization in high dimensions via random embeddings</title>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masrour</forename><surname>Zoghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Matheson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">August 3-9, 2013. 2013</date>
			<biblScope unit="page" from="1778" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01811</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Linear convergence with condition number independent access of full gradients</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="980" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Online convex programming and generalized infinitesimal gradient ascent</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning</title>
		<meeting>the 20th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
