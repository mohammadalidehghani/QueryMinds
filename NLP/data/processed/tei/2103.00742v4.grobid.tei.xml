<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automated Machine Learning on Graphs: A Survey</title>
				<funder ref="#_KMg6Cuc #_9d865Fk #_DHUwEP4">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_8TrAKgn">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-12-20">20 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<email>zw-zhang16@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automated Machine Learning on Graphs: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-20">20 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">40E780EA53C805C7A101D399BB53A882</idno>
					<idno type="arXiv">arXiv:2103.00742v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning on graphs has been extensively studied in both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To solve this critical challenge, automated machine learning (AutoML) on graphs which combines the strength of graph machine learning and AutoML together, is gaining attention from the research community. Therefore, we comprehensively survey AutoML on graphs in this paper 1 , primarily focusing on hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We further overview libraries related to automated graph machine learning and in-depth discuss AutoGL, the first dedicated open-source library for AutoML on graphs. In the end, we share our insights on future research directions for automated graph machine learning. This paper is the first systematic and comprehensive review of automated machine learning on graphs to the best of our knowledge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph data is ubiquitous in our daily life. We can use graphs to model the complex relationships and dependencies between entities ranging from small molecules in proteins and particles in physical simulations to large national-wide power grids and global airlines. Therefore, machine learning on graphs has long been an important research direction for both academics and industry <ref type="bibr" target="#b0">[1]</ref>. In particular, network embedding [2; 3; 4; 5] and graph neural networks (GNNs) <ref type="bibr">[6; 7; 8]</ref> have drawn increasing attention in the last decade. They are successfully applied to recommendation systems <ref type="bibr">[9; 10]</ref>, fraud detection <ref type="bibr" target="#b10">[11]</ref>, bioinformatics <ref type="bibr">[12; 13]</ref>, physical simulation <ref type="bibr" target="#b13">[14]</ref>, traffic forecasting <ref type="bibr">[15; 16]</ref>, knowledge representation <ref type="bibr" target="#b16">[17]</ref>, drug re-purposing <ref type="bibr">[18; 19]</ref> and pandemic prediction <ref type="bibr" target="#b19">[20]</ref> for Covid- <ref type="bibr" target="#b18">19</ref>.</p><p>Despite the popularity of graph machine learning algorithms, the existing literature heavily relies on manual hyperparameter or architecture design to achieve the best performance, resulting in costly human efforts when a vast number of models emerge for various graph tasks. Take GNNs as an example. At least one hundred new general-purpose architectures have been published in top-tier machine learning and data mining conferences in the year 2020 alone, not to mention cross-disciplinary researches of task-specific designs. More and more human efforts are inevitably needed if we stick to the manual try-and-error paradigm in designing the optimal algorithms for targeted tasks.</p><p>On the other hand, automated machine learning (AutoML) has been extensively studied to reduce human efforts in developing and deploying machine learning models <ref type="bibr">[21; 22; 23]</ref>. Complete AutoML pipelines have the potential to automate every step of machine learning, including auto data collection and cleaning, auto feature engineering, and auto model selection and optimization, etc. Due to the popularity of deep learning models, hyper-parameter optimization (HPO) <ref type="bibr">[24; 25; 26]</ref> and neural architecture search (NAS) <ref type="bibr" target="#b26">[27]</ref> are most widely studied. AutoML has achieved or surpassed humanlevel performance <ref type="bibr">[28; 29; 30]</ref> with little human guidance in areas such as computer vision <ref type="bibr">[31; 32]</ref>.</p><p>Automated machine learning on graphs, combining the advantages of AutoML and graph machine learning, naturally serves as a promising research direction to further boost the model performance, which has attracted an increasing number of interests from the community. In this paper, we provide a comprehensive and systematic review of automated machine learning on graphs, to the best of our knowledge, for the first time. Specifically, we focus on two major topics: HPO and NAS of graph machine learning. For HPO, we focus on how to develop scalable methods. For NAS, we follow the literature and compare different methods from search spaces, search strategies, and performance estimation strategies. How different methods tackle the challenges of AutoML on graphs are discussed along the way. Then, we review libraries related to automated graph machine learning and discuss AutoGL, the first dedicated framework and open-source library for automated machine learning on graphs. We high-light the design principles of AutoGL and briefly introduce its usages, which are all specially designed for AutoML on graphs. We believe our review in this paper will significantly facilitate and further promote the studies and applications of automated machine learning on graphs.</p><p>The rest of the paper is organized as follows. In Section 2, we point out the challenges for automated graph machine learning and briefly introduce basic formulations of machine learning on graphs and AutoML. We comprehensively review HPO on graph machine learning in Section 3 and NAS for graph machine learning in Section 4, followed by our overview and discussions of related libraries in Section 5. Lastly, we outline future research opportunities in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Automated Machine Learning on Graphs</head><p>Automated machine learning on graphs, which non-trivially combines the strength of AutoML and graph machine learning, faces the following challenges.</p><p>• The uniqueness of graph machine learning: Unlike audio, image, or text, which has a grid structure, graph data lies in a non-Euclidean space <ref type="bibr" target="#b32">[33]</ref>. Thus, graph machine learning usually has unique architectures and designs. For example, typical NAS methods focus on the search space for convolution and recurrent operations, which is distinct from the building blocks of GNNs <ref type="bibr" target="#b33">[34]</ref>.</p><p>• Complexity and diversity of graph tasks: As aforementioned, graph tasks per se are complex and diverse, ranging from node-level to graph-level problems, and with different settings, objectives, and constraints <ref type="bibr" target="#b34">[35]</ref>. How to impose proper inductive bias and integrate domain knowledge into a graph AutoML method is indispensable.</p><p>• Scalability: Many real graphs such as social networks or the Web are incredibly large-scale with billions of nodes and edges <ref type="bibr" target="#b35">[36]</ref>. Besides, the nodes in the graph are interconnected and cannot be treated as independent samples. Designing scalable AutoML algorithms for graphs poses significant challenges since both graph machine learning and AutoML are already notorious for being compute-intensive.</p><p>Approaches with HPO or NAS for graph machine learning reviewed in later sections target handling at least one of these three challenges. We briefly introduce basic problem formulations before moving to the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Machine Learning on Graphs</head><p>Consider a graph G = (V, E) where V = v 1 , v 2 , ..., v |V| is a set of nodes and E ⊆ V × V is a set of edges. The neighborhood of node v i is denoted as</p><formula xml:id="formula_0">N (i) = {v j : (v i , v j ) ∈ E}.</formula><p>The nodes can also have features denoted as F ∈ R |V|×f , where f is the number of features. We use bold uppercases (e.g., X) and bold lowercases (e.g., x) to represent matrices and vectors, respectively. Most tasks of graph machine learning can be divided into the following two categories:</p><p>• Node-level tasks: the tasks are associated with individual nodes or pairs of nodes. Typical examples include node classification and link prediction.</p><p>• Graph-level tasks: the tasks are associated with the whole graph, such as graph classification and graph generation.</p><p>For node-level tasks, graph machine learning models usually learn a node representation H ∈ R |V|×d and then adopt a classifier or predictor on the node representation to solve the task. For graph-level tasks, a representation for the whole graph is learned and fed into a classifier/predictor. GNNs are the current state-of-the-art in learning node and graph representations. The message-passing framework of GNNs <ref type="bibr" target="#b36">[37]</ref> is formulated as follows.</p><formula xml:id="formula_1">m (l) i = AGG (l) a (l) ij W (l) h (l) i , ∀j ∈ N (i)<label>(1)</label></formula><formula xml:id="formula_2">h (l+1) i = σ COMBINE (l) m (l) i , h (l) i ,<label>(2) where h (l)</label></formula><p>i denotes the node representation of node v i in the l th layer, m (l) is the message for node v i , AGG (l) (•) is the aggregation function, a (l) ij denotes the weights from node v j to node v i , COMBINE (l) (•) is the combining function, W (l) are learnable weights, and σ(•) is an activation function. The node representation is usually initialized as node features H (0) = F, and the final representation is obtained after L message-passing layers H = H (L) .</p><p>For the graph-level representation, pooling methods (also called readout) are applied to the node representations</p><formula xml:id="formula_3">h G = POOL (H) ,<label>(3)</label></formula><p>i.e., h G is the representation of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AutoML</head><p>Many AutoML algorithms such as HPO and NAS can be formulated as the following bi-level optimization problem:</p><formula xml:id="formula_4">min α∈A L val (W * (α), α) s.t. W * (α) = arg min W (L train (W, α)) ,<label>(4)</label></formula><p>where α is the optimization objective of the AutoML algorithm, e.g., hyper-parameters in HPO and neural architectures in NAS, A is the feasible space for the objective, and W(α) are trainable weights in the graph machine learning models. Essentially, we aim to optimize the objective in the feasible space so that the model achieves the best results in terms of a validation function, and W * indicates that the weights are fully optimized in terms of a training function. Different Au-toML methods differ in how the feasible space is designed and how the objective functions are instantiated and optimized since directly optimizing Eq. ( <ref type="formula" target="#formula_4">4</ref>) requires enumerating and training every feasible objective, which is prohibitive. Typical formulations of AutoML on graphs need to properly integrate the above formulations in Section 2.1 and Section 2.2 to form a new optimization problem.</p><p>In this section, we review HPO for machine learning on graphs. The main challenge here is scalability, i.e., a real graph can have billions of nodes and edges, and each trial on the graph is computationally expensive. Next, we elaborate on how different methods tackle the efficiency challenge. Notice that we omit some straightforward HPO methods such as random search and grid search <ref type="bibr" target="#b23">[24]</ref>.</p><p>AutoNE <ref type="bibr" target="#b37">[38]</ref> first tackles the efficiency problem of HPO on graphs by proposing a transfer paradigm that samples subgraphs as proxies for the large graph, which is similar in principle to sampling instances in previous HPO methods <ref type="bibr" target="#b38">[39]</ref>. Specifically, AutoNE has three modules: the sampling module, the signature extraction module, and the meta-learning module. In the sampling module, multiple representative subgraphs are sampled from the large graph using a multistart random walk strategy. Then, AutoNE conducts HPO on the sampled subgraphs using Bayesian optimization <ref type="bibr" target="#b25">[26]</ref> and learns representations of subgraphs using the signature extraction module. Finally, the meta-learning module extracts meta-knowledge from HPO results and representations of subgraphs. AutoNE fine-tunes hyper-parameters on the large graph using the meta-knowledge. In this way, Au-toNE achieves satisfactory results while maintaining scalability since multiple HPO trials on the sampled subgraphs and a few HPO trails on the large graph are properly integrated.</p><p>JITuNE <ref type="bibr" target="#b39">[40]</ref> proposes to replace the sampling process of AutoNE with graph coarsening to generate a hierarchical graph synopsis. A similarity measurement module is proposed to ensure that the coarsened graph shares sufficient similarity with the large graph. Compared with sampling, such graph synopsis can better preserve graph structural information. Therefore, JITuNE argues that the best hyperparameters in the graph synopsis can be directly transferred to the large graph. Besides, since the graph synopsis is generated in a hierarchy, the granularity can be more easily adjusted to meet the time constraints of downstream tasks.</p><p>HESGA <ref type="bibr" target="#b40">[41]</ref> proposes another strategy to improve efficiency using a hierarchical evaluation strategy together with evolutionary algorithms. Specifically, HESGA proposes to evaluate the potential of hyper-parameters by interrupting training after a few epochs and calculating the performance gap with respect to the initial performance with random model weights. This gap is used as a fast score to filter out unpromising hyper-parameters. Then, the standard full evaluation, i.e., training until convergence, is adopted as the final assessor to select the best hyper-parameters to be stored in the population of the evolutionary algorithm.</p><p>Besides efficiency, AutoGM <ref type="bibr" target="#b41">[42]</ref> focuses on proposing a unified framework for various graph machine learning algorithms. Specifically, AutoGM finds that many popular GNNs can be characterized in a framework similar to Eq. ( <ref type="formula" target="#formula_1">1</ref>) with five hyper-parameters: the number of message-passing neighbors, the number of message-passing steps, the aggregation function, the dimensionality, and the non-linearity. Au-toGM adopts Bayesian optimization to optimize these hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NAS for Graph Machine Learning</head><p>NAS methods can be compared in three aspects <ref type="bibr" target="#b26">[27]</ref>: search space, search strategy, and performance estimation strategy. Next, we review NAS methods for graph machine learning from these three aspects. We mainly review NAS for GNNs fitting Eq. ( <ref type="formula" target="#formula_1">1</ref>) and summarize the characteristics of different methods in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Search Space</head><p>The first challenge of NAS on graphs is the search space design since the building blocks of graph machine learning are usually distinct from other deep learning models such as CNNs or RNNs. For GNNs, the search space can be divided into the following four categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Micro Search Space</head><p>Following the message-passing framework in Eq. ( <ref type="formula" target="#formula_1">1</ref>), the micro search space defines how nodes exchange messages with others in each layer. Commonly adopted micro search spaces <ref type="bibr">[34; 43]</ref> compose the following components:</p><p>• Aggregation function AGG(•): SUM, MEAN, MAX, and MLP.</p><p>• Aggregation weights aij: common choices are listed in Table <ref type="table">2</ref>.</p><p>• Number of heads when using attentions: 1, 2, 4, 6, 8, 16, etc.</p><p>• Combining function COMBINE(•): CONCAT, ADD, and MLP.</p><p>• Dimensionality of h l : 8, 16, 32, 64, 128, 256, 512, etc.</p><p>• Non-linear activation function σ(•): Sigmoid, Tanh, ReLU, Identity, Softplus, Leaky ReLU, ReLU6, and ELU.</p><p>However, directly searching all these components results in thousands of possible choices in a single message-passing layer. Thus, it may be beneficial to prune the space to focus on a few crucial components depending on applications and domain knowledge <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Macro Search Space</head><p>Similar to residual connections and dense connections in CNNs, node representations in one layer of GNNs do not necessarily solely depend on the immediate previous layer <ref type="bibr">[62; 63]</ref>. These connectivity patterns between layers form the macro search space. Formally, such designs are formulated as</p><formula xml:id="formula_5">H (l) = j&lt;l F jl H (j) ,<label>(5)</label></formula><p>where F jl (•) can be the message-passing layer in Eq. ( <ref type="formula" target="#formula_1">1</ref>), ZERO (i.e., not connecting), IDENTITY (e.g., residual connections), or an MLP. Since the dimensionality of H (j) can vary, IDENTITY can only be adopted if the dimensionality of each layer matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling Methods</head><p>In order to handle graph-level tasks, information from all the nodes is aggregated to form graph-level representations using the pooling operation in Eq <ref type="bibr" target="#b2">(3)</ref>. <ref type="bibr" target="#b50">[51]</ref> propose a pooling search space including row-or column-wise sum, mean, or maximum, attention pooling, attention sum, and flatten. More advanced methods such as hierarchical pooling <ref type="bibr" target="#b63">[64]</ref> could also be added to the search space with careful designs.</p><p>Method Search space Tasks Search Strategy Performance Other Characteristics Micro Macro Pooling HP Layers Node Graph Estimation -GraphNAS [34] Fixed RNN controller + RL --AGNN <ref type="bibr" target="#b42">[43]</ref> Fixed Self-designed controller + RL Inherit weights -SNAG <ref type="bibr" target="#b43">[44]</ref> Fixed RNN controller + RL Inherit weights Simplify the micro search space PDNAS <ref type="bibr" target="#b44">[45]</ref> Fixed Differentiable Single-path one-shot -POSE <ref type="bibr" target="#b45">[46]</ref> Fixed Differentiable Single-path one-shot Support heterogenous graphs NAS-GNN <ref type="bibr" target="#b46">[47]</ref> Fixed Evolutionary algorithm --AutoGraph <ref type="bibr" target="#b47">[48]</ref> Various Evolutionary algorithm --GeneticGNN <ref type="bibr" target="#b48">[49]</ref> Fixed Evolutionary algorithm --EGAN <ref type="bibr" target="#b49">[50]</ref> Fixed Differentiable One-shot Sample small graphs for efficiency NAS-GCN <ref type="bibr" target="#b50">[51]</ref> Fixed Evolutionary algorithm -Handle edge features LPGNAS <ref type="bibr" target="#b51">[52]</ref> Fixed Differentiable Single-path one-shot Search for quantisation options You et al. <ref type="bibr" target="#b52">[53]</ref> Various Random search -Transfer across datasets and tasks SAGS <ref type="bibr" target="#b53">[54]</ref> Fixed Self-designed algorithm --Peng et al. <ref type="bibr" target="#b54">[55]</ref> Fixed CEM-RL <ref type="bibr" target="#b55">[56]</ref> -Search spatial-temporal modules GNAS <ref type="bibr" target="#b56">[57]</ref> Various Differentiable One-shot -AutoSTG <ref type="bibr" target="#b57">[58]</ref> Fixed Differentiable One-shot+meta learning Search spatial-temporal modules DSS <ref type="bibr" target="#b58">[59]</ref> Fixed Differentiable One-shot Dynamically update search space SANE <ref type="bibr" target="#b59">[60]</ref> Fixed Differentiable One-shot -AutoAttend <ref type="bibr" target="#b60">[61]</ref> Fixed Evolutionary algorithm One-shot Cross-layer attention Table <ref type="table">1</ref>: A summary of different NAS methods for graph machine learnings.</p><p>Type Formulation</p><formula xml:id="formula_6">CONST a const ij = 1 GCN a gcn ij = 1 √ |N (i)||N (j)| GAT a gat ij = LeakyReLU (ATT (Wa [hi, hj])) SYM-GAT a sym ij = a gat ij + a gat ji COS a cos ij = cos (Wahi, Wahj) LINEAR a lin ij = tanh (sum (Wahi + Wahj)) GENE-LINEAR a gene ij = tanh (sum (Wahi + Wahj)) W a</formula><p>Table 2: A typical search space of different aggregation weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters</head><p>Besides architectures, other training hyper-parameters can be incorporated into the search space, i.e., similar to jointly conducting NAS and HPO. Typical hyper-parameters include the learning rate, the number of epochs, the batch size, the optimizer, the dropout rate, and the regularization strengths such as the weight decay. These hyper-parameters can be jointly optimized with architectures or separately optimized after the best architectures are found. HPO methods in Section 3 can also be combined here. Another critical choice is the number of message-passing layers. Unlike CNNs, most currently successful GNNs are shallow, e.g., with no more than three layers, possibly due to the over-smoothing problem <ref type="bibr">[65; 62]</ref>. Limited by this problem, the existing NAS methods for GNNs usually preset the number of layers as a small fixed number. How to automatically design deep GNNs while integrating techniques to alleviate over-smoothing remains mostly unexplored. On the other hand, NAS may also bring insights to help to tackle the over-smoothing problem <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Search Strategy</head><p>Search strategies can be broadly divided into three categories: architecture controllers trained with reinforcement learning (RL), differentiable methods, and evolutionary algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controller + RL</head><p>A widely adopted NAS search strategy uses a controller to generate the neural architecture descriptions and train the controller with reinforcement learning to maximize the model performance as rewards. For example, if we consider neural architecture descriptions as a sequence, we can use RNNs as the controller <ref type="bibr" target="#b27">[28]</ref>. Such methods can be directly applied to GNNs with a suitable search space and performance evaluation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differentiable</head><p>Differentiable NAS methods such as DARTS <ref type="bibr" target="#b28">[29]</ref> and SNAS <ref type="bibr" target="#b65">[66]</ref> have gained popularity in recent years. Instead of optimizing different operations separately, differentiable methods construct a single super-network (known as the oneshot model) containing all possible operations. Formally, we denote <ref type="bibr" target="#b5">(6)</ref> where o (x,y) (x) is an operation in the GNN with input x and output y, O are all candidate operations, and z (x,y) are learnable vectors to control which operation is selected. Briefly speaking, each operation is regarded as a probability distribution of all possible operations. In this way, the architecture and model weights can be jointly optimized via gradientbased algorithms. The main challenges lie in making the NAS algorithm differentiable, where several techniques such as Gumbel-softmax <ref type="bibr" target="#b66">[67]</ref> and concrete distribution <ref type="bibr" target="#b67">[68]</ref> are resorted to. When applied to GNNs, slight modification may be needed to incorporate the specific operations defined in the search space.</p><formula xml:id="formula_7">y = o (x,y) (x) = o∈O exp(z (x,y) o ) o ∈O exp(z (x,y) o ) o(x),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evolutionary Algorithms</head><p>Evolutionary algorithms are a class of optimization algorithms inspired by biological evolution. For NAS, randomly generated architectures are considered initial individuals in a population. Then, new architectures are generated using mutations and crossover operations on the population. The architectures are evaluated and selected to form the new population, and the same process is repeated. The best architectures are recorded while updating the population, and the final solutions are obtained after sufficient updating steps.</p><p>For GNNs, regularized evolution (RE) NAS <ref type="bibr" target="#b31">[32]</ref> has been widely adopted. RE's core idea is an aging mechanism, i.e., in the selection process, the oldest individuals in the population are removed. Genetic-GNN <ref type="bibr" target="#b68">[69]</ref> also proposes an evolution process to alternatively update the GNN architecture and the learning hyper-parameters to find the best fit of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combinations</head><p>It is also feasible to combine these three types of search strategies mentioned above. For example, AGNN <ref type="bibr" target="#b42">[43]</ref> proposes a reinforced conservative search strategy by adopting both RNNs and evolutionary algorithms in the controller and train the controller with RL. By only generating slightly different architectures, the controller can find well-performing GNNs more efficiently. <ref type="bibr" target="#b54">[55]</ref> adopt CEM-RL <ref type="bibr" target="#b55">[56]</ref>, which combines evolutionary and differentiable methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Estimation Strategy</head><p>Due to the large number of possible architectures, it is infeasible to fully train each architecture independently. Next, we review some performance estimation strategies.</p><p>A commonly adopted "trick" to speed up performance estimation is to reduce fidelity <ref type="bibr" target="#b26">[27]</ref>, e.g., by reducing the number of epochs or the number of data points. This strategy can be directly generalized to GNNs.</p><p>Another strategy successfully applied to CNNs is sharing weights among different models, known as parameter sharing or weight sharing <ref type="bibr" target="#b29">[30]</ref>. For differentiable NAS with a large one-shot model, parameter sharing is naturally achieved since the architectures and weights are jointly trained. However, training the one-shot model may be difficult since it contains all possible operations. To further speed up the training process, single-path one-shot model <ref type="bibr" target="#b69">[70]</ref> has been proposed where only one operation between an input and output pair is activated during each pass.</p><p>For NAS without a one-shot model, sharing weights among different architecture is more difficult but not entirely impossible. For example, since it is known that some convolutional filters are common feature extractors, inheriting weights from previous architectures is feasible and reasonable in CNNs <ref type="bibr" target="#b70">[71]</ref>. However, since there is still a lack of understandings of what weights in GNNs represent, we need to be more cautious about inheriting weights <ref type="bibr" target="#b43">[44]</ref>. AGNN <ref type="bibr" target="#b42">[43]</ref> proposes three constraints for parameter inheritance: same weight shapes, same attention and activation functions, and no parameter sharing in batch normalization and skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussions</head><p>The Search Space Besides the basic search space presented in Section 4.1, different graph tasks may require other search spaces. For example, meta-paths are critical for heterogeneous graphs <ref type="bibr" target="#b45">[46]</ref>, edge features are essential in modeling molecular graphs <ref type="bibr" target="#b50">[51]</ref>, and spatial-temporal modules are needed in skeleton-based recognition <ref type="bibr" target="#b54">[55]</ref>. Sampling mechanisms to accelerate GNNs are also critical, especially for large-scale graphs <ref type="bibr" target="#b33">[34]</ref>. A suitable search space usually requires careful designs and domain knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transferability</head><p>It is non-trivial to transfer GNN architectures across different datasets and tasks due to the complexity and diversity of graph tasks. <ref type="bibr" target="#b71">[72]</ref> adopt a fixed set of GNNs as anchors on different tasks and datasets. Then, the rank correlation serves as a metric to measure the similarities between different datasets and tasks. The best-performing GNNs of the most similar tasks are transferred to solve the target task.</p><p>The Efficiency Challenge of Large-scale Graphs Similar to AutoNE introduced in Section 3, EGAN <ref type="bibr" target="#b49">[50]</ref> proposes to sample small graphs as proxies and conduct NAS on the sampled subgraphs to improve the efficiency of NAS. While achieving some progress, more advanced and principle approaches are further needed to handle billion-scale graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Libraries for AutoML on Graphs</head><p>Publicly available libraries are important to facilitate and advance the research and applications of AutoML on graphs. Popular libraries for graph machine learning include PyTorch Geometric <ref type="bibr" target="#b72">[73]</ref>, Deep Graph Library <ref type="bibr" target="#b73">[74]</ref>, GraphNets <ref type="bibr" target="#b74">[75]</ref>, AliGraph <ref type="bibr" target="#b75">[76]</ref>, Euler <ref type="bibr" target="#b76">[77]</ref>, PBG <ref type="bibr" target="#b77">[78]</ref>, Stellar Graph <ref type="bibr" target="#b78">[79]</ref>, Spektral <ref type="bibr" target="#b79">[80]</ref>, CodDL <ref type="bibr" target="#b80">[81]</ref>, OpenNE <ref type="bibr" target="#b81">[82]</ref>, GEM <ref type="bibr" target="#b82">[83]</ref>, Karateclub <ref type="bibr" target="#b83">[84]</ref>, DIG <ref type="bibr" target="#b84">[85]</ref>, and classical NetworkX <ref type="bibr" target="#b85">[86]</ref>. However, these libraries do not support AutoML.</p><p>On the other hand, AutoML libraries such as NNI <ref type="bibr" target="#b86">[87]</ref>, Au-toKeras <ref type="bibr" target="#b87">[88]</ref>, AutoSklearn <ref type="bibr" target="#b88">[89]</ref>, Hyperopt <ref type="bibr" target="#b89">[90]</ref>, TPOT <ref type="bibr" target="#b90">[91]</ref>, AutoGluon <ref type="bibr" target="#b91">[92]</ref>, MLBox <ref type="bibr" target="#b92">[93]</ref>, and MLJAR <ref type="bibr" target="#b93">[94]</ref> are widely adopted. Unfortunately, because of the uniqueness and complexity of graph tasks, they cannot be directly applied to automate graph machine learning.</p><p>Recently, some HPO and NAS methods for graphs such as AutoNE <ref type="bibr" target="#b37">[38]</ref>, AutoGM <ref type="bibr" target="#b41">[42]</ref>, GraphNAS <ref type="bibr" target="#b33">[34]</ref> Graph-Gym <ref type="bibr" target="#b52">[53]</ref> have open-sourced their codes, facilitating reproducibility and promoting AutoML on graphs. Besides, Au-toGL <ref type="bibr" target="#b94">[95]</ref> <ref type="foot" target="#foot_0">foot_0</ref> , the first dedicated library for automated graph learning, is developed. Next, we review AutoGL in detail. Figure <ref type="figure" target="#fig_0">1</ref> shows the overall framework of AutoGL. The main characteristics of AutoGL are three-folded:</p><p>• Open-source: all the source codes of AutoGL are publicly available under the MIT license.</p><p>• Easy to use: AutoGL is designed to be easy to use. For example, less than ten lines of codes are needed to conduct some quick experiments of AutoGL.</p><p>• Flexible to be extended: AutoGL adopts a modular design with high-level base classes API and extensive documentations, allowing flexible and customized extensions.</p><p>We briefly review the dataset management and four core components of AutoGL: Auto Feature Engineering, Model Training, Hyper-Parameters Optimization, and Auto Ensemble. These components are designed in a modular and objectoriented fashion to enable clear logic flows, easy usages, and flexibility in extending.</p><p>AutoGL Dataset. inherits from PyTorch Geometric <ref type="bibr" target="#b72">[73]</ref>, covering common benchmarks for node and graph classification, including the recent Open Graph Benchmark <ref type="bibr" target="#b34">[35]</ref>.</p><p>Users can easily add customized datasets following documentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auto</head><p>Feature Engineering. module first processes the graph data using three categories of operations: generators, where new node and edge features are constructed; selectors, filtering out and compressing useless and meaningless features; sub-graph generators, generating graph-level features. Convenient wrappers are also provided to support PyTorch Geometric and NetworkX [96]. Model Training. module handles the training and evaluation process of graph tasks with two functional sub-modules: model and trainer. Model handles the construction of graph machine learning models, e.g., GNNs, by defining learnable parameters and the forward pass. Trainer controls the optimization process for the given model. Common optimization methods, training controls, and regularization methods are packaged as high-level APIs.</p><p>Hyper-Parameter Optimization. module conducts HPO for a specified model, covering methods presented in Section 3 such as AutoNE and general-purpose algorithms like random search <ref type="bibr" target="#b23">[24]</ref> and Tree Parzen Estimator <ref type="bibr" target="#b96">[97]</ref>. The model training module can specify the hyper-parameters, their types (e.g., integer, numerical, or categorical), and feasible ranges. Users can also customize HPO algorithms.</p><p>Auto Ensemble. module can automatically integrate the optimized individual models to form a more powerful final model. Two kinds of ensemble methods are provided: voting and stacking. Voting is a simple yet powerful ensemble method that directly averages the output of individual models while stacking trains another meta-model that learns to combine the output of models in a more principled way. The general linear model (GLM) and gradient boosting machine (GBM) are supported as meta-models.</p><p>AutoGL Solver. On top of the four modules, another highlevel API Solver is proposed to control the overall pipeline.</p><p>In the Solver, the four modules are organized to form the Au-toML solution for graph data. The Solver also provides global controls. For example, the time budget can be explicitly set, and the training/evaluation protocols can be selected. AutoGL is still actively updated. Key features to be released shortly include neural architecture search, large-scale datasets support, and more graph tasks. For the most up-todate information, please visit the project homepage. All kinds of inputs and suggestions are also warmly welcomed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Directions</head><p>• Graph models for AutoML: In this paper, we mainly focus on how AutoML methods are extended to graphs. The other direction, i.e., using graphs to help AutoML, is also feasible and promising. For example, we can model neural networks as a directed acyclic graph (DAG) to analyze their structures <ref type="bibr">[98; 72]</ref> or adopt GNNs to facilitate NAS <ref type="bibr">[99; 100; 69]</ref>. Ultimately, we expect graphs and AutoML to form tighter connections and further facilitate each other. • Robustness and explainability: Since many graph applications are risk-sensitive, e.g., finance and healthcare, model robustness and explainability are indispensable for actual usages. Though there exist some initial studies on the robustness <ref type="bibr" target="#b100">[101]</ref> and explainability <ref type="bibr" target="#b101">[102]</ref> of graph machine learning, how to generalize these techniques into AutoML on graphs remains to be further explored <ref type="bibr" target="#b102">[103]</ref>. • Hardware-aware models: To further improve the scalability of automated machine learning on graphs, hardware-aware models may be a critical step, especially in real industrial environments. Both hardware-aware graph models <ref type="bibr" target="#b103">[104]</ref> and hardware-aware AutoML models [105; 106; 107] have been studied, but integrating these techniques still poses significant challenges. • Comprehensive evaluation protocols: Currently, most AutoML on graphs are tested on small traditional benchmarks such as three citation graphs, i.e., Cora, Cite-Seer, and PubMed <ref type="bibr" target="#b107">[108]</ref>. However, these benchmarks have been identified as insufficient to compare different graph machine learning models <ref type="bibr" target="#b108">[109]</ref>, not to mention AutoML on graphs. More comprehensive evaluation protocols are needed, e.g., on recently proposed graph machine learning benchmarks <ref type="bibr">[35; 110]</ref> or new dedicated graph AutoML benchmarks similar to the NASbench series <ref type="bibr" target="#b110">[111]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework of AutoGL.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>AutoGL homepage: https://mn.cs.tsinghua.edu.cn/AutoGL</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by the <rs type="funder">National Key Research and Development Program of China</rs> (No.<rs type="grantNumber">2020AAA0106300</rs>, <rs type="grantNumber">2020AAA0107800</rs>, <rs type="grantNumber">2018AAA0102000</rs>) and <rs type="funder">National Natural Science Foundation of China</rs> No.<rs type="grantNumber">62050110</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KMg6Cuc">
					<idno type="grant-number">2020AAA0106300</idno>
				</org>
				<org type="funding" xml:id="_9d865Fk">
					<idno type="grant-number">2020AAA0107800</idno>
				</org>
				<org type="funding" xml:id="_DHUwEP4">
					<idno type="grant-number">2018AAA0102000</idno>
				</org>
				<org type="funding" xml:id="_8TrAKgn">
					<idno type="grant-number">62050110</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mark</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Networks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE DEBU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph embedding techniques, applications, and performance: A survey</title>
		<author>
			<persName><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KBS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: Problems, techniques, and applications</title>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning disentangled representations for recommendation</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph based anomaly detection and description: a survey</title>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DMKD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Network embedding in biomedical data science</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatiotemporal graph convolutional networks: a deep learning framework for traffic forecasting</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Few-shot link prediction via graph neural networks for covid-19 drug-repurposing</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Vassilis N Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10261</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Network medicine framework for identifying drug-repurposing opportunities for covid-19</title>
		<author>
			<persName><forename type="first">Deisy</forename><surname>Morselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gysi</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Examining covid-19 forecasting using spatio-temporal graph neural networks</title>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automl: A survey of the state-of-the-art</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>KBS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Taking human out of learning applications: A survey on automated machine learning</title>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13306</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automated machine learning: State-of-the-art and open challenges</title>
		<author>
			<persName><forename type="first">Radwa</forename><surname>Elshawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherif</forename><surname>Sakr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02287</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><surname>Michael M Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph neural architecture search</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On power law growth of social networks</title>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Autone: Hyperparameter optimization for massive network embedding</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Paramils: an automatic algorithm configuration framework</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Mengying</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06427</idno>
		<title level="m">Just-in-time hyperparameter tuning for network embedding algorithms</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A novel genetic algorithm with hierarchical evaluation strategy for hyperparameter optimisation of graph neural networks</title>
		<author>
			<persName><forename type="first">Yingfang</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09300</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Autonomous graph mining algorithm search with best speed/accuracy trade-off</title>
		<author>
			<persName><forename type="first">Minji</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICDM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Auto-gnn: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Simplifying architecture search for graph neural network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11652</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Probabilistic dual network architecture search on graphs</title>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09676</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Propagation model search for graph neural networks</title>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03250</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural architecture search in graph neural networks</title>
		<author>
			<persName><forename type="first">Matheus</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BRACIS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Autograph: Automated graph neural network</title>
		<author>
			<persName><forename type="first">Yaoman</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICONIP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Evolutionary architecture search for graph neural networks</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10199</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Efficient graph neural architecture search</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>OpenReview</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph neural network architecture search for molecular property prediction</title>
		<author>
			<persName><forename type="first">Shengli</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Balaprakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Big Data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learned low precision graph neural networks</title>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09232</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Learning graph convolutional network for skeleton-based human action recognition by neural searching</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">CEM-RL: Combining evolutionary and gradient-based methods for policy search</title>
		<author>
			<persName><forename type="first">Sigaud</forename><surname>Pourchot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rethinking graph neural network search from message-passing</title>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Autostg: Neural architecture search for predictions of spatio-temporal graphs</title>
		<author>
			<persName><forename type="first">Zheyi</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">One-shot graph neural architecture search with dynamic search space</title>
		<author>
			<persName><forename type="first">Yanxi</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Search to aggregate neighborhood for graph neural network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDE</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Autoattend: Automated attention representation search</title>
		<author>
			<persName><forename type="first">Chaoyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semisupervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Snas: stochastic neural architecture search</title>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bridging the gap between sample-based and one-shot neural architecture search with bonas</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Graph structure of neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Deep graph library: A graphcentric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Aligraph: A comprehensive graph neural network platform</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><surname>Alibaba</surname></persName>
		</author>
		<author>
			<persName><surname>Euler</surname></persName>
		</author>
		<ptr target="https://github.com/alibaba/euler" />
		<title level="m">A distributed graph deep learning framework</title>
		<imprint>
			<date type="published" when="2019-05">2019. May-2021</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">PyTorch-BigGraph: A Large-scale Graph Embedding System</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SysML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">CSIRO&apos;s Data61</title>
		<ptr target="https://github.com/stellargraph/stellargraph" />
	</analytic>
	<monogr>
		<title level="m">Stellargraph machine learning library</title>
		<imprint>
			<date type="published" when="2018-05">2018. May-2021</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Graph neural networks in tensorflow and keras with spektral</title>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Cogdl: An extensive toolkit for deep learning on graphs</title>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00959</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<ptr target="https://github.com/thunlp/OpenNE" />
		<title level="m">Openne: An open source toolkit for network embedding</title>
		<imprint>
			<date type="published" when="2018-05">2018. May-2021</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
		<respStmt>
			<orgName>Tsinghua University Natural Language Processing Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Gem: A python package for graph embedding methods</title>
		<author>
			<persName><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Dig: A turnkey library for diving into graph deep learning research</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12608</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Exploring network structure, dynamics, and function using networkx</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Hagberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><forename type="middle">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName><surname>Swart</surname></persName>
		</author>
		<editor>Scipy</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Retiarii: A deep learning exploratory-training framework</title>
		<author>
			<persName><forename type="first">Quanlu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Autokeras: An efficient neural architecture search system</title>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Auto-sklearn: efficient and robust automated machine learning</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Evaluation of a tree-based pipeline optimization tool for automating data science</title>
		<author>
			<persName><forename type="first">Randal</forename><forename type="middle">S</forename><surname>Olson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>GECCO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Autogluon-tabular: Robust and accurate automl for structured data</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Erickson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06505</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Axel</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romblay</forename></persName>
		</author>
		<ptr target="https://github.com/AxeldeRomblay/MLBox" />
		<title level="m">Mlbox, machine learning box</title>
		<imprint>
			<date type="published" when="2018-05">2018. May-2021</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Mljar automated machine learning</title>
		<author>
			<persName><surname>Mljar</surname></persName>
		</author>
		<ptr target="https://github.com/mljar/mljar-supervised" />
		<imprint>
			<date type="published" when="2019-05">2019. May-2021</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">AutoGL: A library for automated graph learning</title>
		<author>
			<persName><forename type="first">Chaoyu</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021 GTRL Workshop</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Exploring network structure, dynamics, and function using networkx</title>
		<author>
			<persName><forename type="first">Aric</forename><surname>Hagberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Swart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Chult</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Los Alamos National Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Graph hypernetworks for neural architecture search</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Brp-nas: Prediction-based nas using gcns</title>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Dudziak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Adversarial attack and defense on graph data: A survey</title>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10528</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Explainability in graph neural networks: A taxonomic survey</title>
		<author>
			<persName><surname>Hao Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15445</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Explainable automated graph representation learning with hyperparameter importance</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Hardware acceleration of graph neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Auten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Hardwareaware transformable architecture search with efficient search space</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation. Relational Representation Learning Workshop</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwivedi</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
