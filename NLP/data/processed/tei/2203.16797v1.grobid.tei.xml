<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WHEN PHYSICS MEETS MACHINE LEARNING: A SURVEY OF PHYSICS-INFORMED MACHINE LEARNING A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-04-01">April 1, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chuizheng</forename><surname>Meng</surname></persName>
							<email>chuizhem@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<postCode>90089</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sungyong</forename><surname>Seo</surname></persName>
							<email>sungyongs@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<postCode>90089</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Defu</forename><surname>Cao</surname></persName>
							<email>defucao@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<postCode>90089</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><surname>Griesemer</surname></persName>
							<email>samgriesemer@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<postCode>90089</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<postCode>90089</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WHEN PHYSICS MEETS MACHINE LEARNING: A SURVEY OF PHYSICS-INFORMED MACHINE LEARNING A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-01">April 1, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">B258C9759973A4A725918BD2F7D84D63</idno>
					<idno type="arXiv">arXiv:2203.16797v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Physics-informed machine learning (PIML), referring to the combination of prior knowledge of physics, which is the high level abstraction of natural phenomenons and human behaviours in the long history, with data-driven machine learning models, has emerged as an effective way to mitigate the shortage of training data, to increase models' generalizability and to ensure the physical plausibility of results. In this paper, we survey an abundant number of recent works in PIML and summarize them from three aspects: (1) motivations of PIML, (2) physics knowledge in PIML, (3) methods of physics knowledge integration in PIML. We also discuss current challenges and corresponding research opportunities in PIML.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning/deep learning models have already achieved tremendous success in a number of domains such as computer vision <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5]</ref> and natural language processing <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>, where large amounts of training data and highly expressive neural network architectures together give birth to solutions outperforming previously dominating methods. As a consequence, researchers have also started exploring the possibility of applying machine learning models to advance scientific discovery and to further improve traditional analytical modeling <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21]</ref>.</p><p>While given a set of input and output pairs, deep neural networks are able to extract the complicated relations between the input and output via appropriate optimization over adequate large amount of data, prior knowledge still acts as an important role in finding the optimal solution. As the high level extraction of data distributions and task properties, prior knowledge, if incorporated properly, can provide rich information not existing or hard to extract in limited training data, and helps improve the data efficiency, the ability to generalize, and the plausibility of resulting models.</p><p>Physics knowledge, which has been collected and validated explicitly both theoretically and empirically in the long history, contains tremendous abstraction and summary of natural phenomena and human behaviours in many important scientific and engineering applications. Thus in this paper, we focus on the topic of integrating prior physics knowledge into machine learning models, i.e. physics-informed machine learning (PIML). Compared to the integration of other types of prior knowledge, such as knowledge graphs, logic rules and human feedback <ref type="bibr" target="#b22">[22]</ref>, the integration of physics knowledge requires specific design due to its special properties and forms.</p><p>In this paper, we survey a wide range of recent works in PIML and summarize them from three aspects. (1) Motivations of PIML, which can be further categorized to using machine learning to serve tasks in physics domains and incorporating physics principles to existing machine learning models for real-world tasks. <ref type="bibr" target="#b2">(2)</ref> Physics knowledge in PIML, each type of which is a general principle covering a wide range of problems. (3) Methods of physics knowledge integration in PIML. Depending on the location of knowledge integration, we categorize the methods into data enhancement, neural network architecture design, and physics-informed optimization.</p><p>The paper is organized as follows. Sec 2 analyzes two main categories of motivations using PIML: one mainly serves tasks in physics domain, while the other serves real-world problems. Sec 3 introduces several general physics principles widely used in PIML. Sec 4 investigates methods of physics knowledge integration. Sec 5 discusses challenges and potential future research directions of PIML. Sec 6 serves as the summary of the whole paper.</p><p>2 Motivations of PIML 2.1 ML for Physics: Enhancement of Physics Models via Data-Driven Methods Physical science problems involves various data-intensive tasks including spatiotemporal data modeling, causal reasoning, computer vision, probabilistic inference and so on. Since machine learning methods have achieved great success in these tasks, using machine learning models for furthering scientific discovery in physics has received increasing interest in recent years. Compared to existing numerical or pure physics based methods, physics-informed machine learning methods have advantages in flexibility, generalizability, and computation cost. Meanwhile, they still enforce physical plausibility. In this section, we introduce recent developments of exploiting machine learning for several physics related tasks, including surrogate simulation, data-driven PDE solvers, parameterization of physics models, reduced-order models, and knowledge discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Simulation</head><p>Many physics-informed machine learning models have been proposed to act as surrogate solutions to numerical simulation in many domains, such as turbulence simulation <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>, climate simulation <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref>, and particle system simulation <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref>. Compared to numerical simulation, neural network based machine learning models enjoy following advantages. (1) Lower implementation costs. To build a high-quality numerical simulator, researchers usually need years of engineering effort and must choose numerous physically meaningful parameters depending on the task. Instead, machine learning models can be trained directly from a large amount of observed data.</p><p>(2) Stronger ability to generalize. Machine learning models can share the same architecture for different types of problems in the same category, and then can be further specialized for each problem with observed data. For example, <ref type="bibr" target="#b30">[30]</ref> proposes a general framework to learn particle simulation, and can generalize across fluid, rigid , and deformable material systems. (3) Lower computation costs. Existing neural network building blocks such as multilayer perceptrons and convolutions can be efficiently accelerated by various hardware including CPU, GPU, FPGA and ASIC, giving advantages of computation costs to neural networks composed of these blocks. In fluid flow prediction, the model proposed in <ref type="bibr" target="#b34">[34]</ref> achieves prediction results close to ground truth simulation at a running speed 60x faster than the numerical method.</p><p>Meanwhile, due to the large and complex optimization space of neural networks, it is critical to incorporate inductive bias of physics knowledge about the task into either the training data, the model architecture, or the optimization process. This ensures greater physical plausibility of resulting models, further improving the robustness in real-world settings. The detailed techniques of integration will be introduced in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Data-driven PDE Solver</head><p>Many real-world problems from physical systems are mostly about how to describe observations based on partial differential equations and numerically solve the equations. There are many traditional methods to numerically solve PDEs such as spectral methods, finite-difference methods (FDM), finite element method (FEM), finite volume method (FVM), etc. All of these methods are numerical and they require proper discretization or a finite number of steps to approximate continuous solution.</p><p>Combining machine learning techniques with PDE models has a long history in machine learning. <ref type="bibr" target="#b35">[35]</ref> introduce a method to reconstruct the deterministic portion of the equations of motion directly from a data series. This approach employs an informational measure of model optimality to guide searching through the space of dynamical systems. <ref type="bibr" target="#b36">[36]</ref> present a framework for computer-aided multi scale analysis, which enables models at a fine (microscopic/stochastic) level of description to perform modeling tasks at a coarse (macroscopic/systems) level. These macroscopic modeling tasks, yielding information over long time and large scales, are accomplished through approximately initialized calls to the microscopic similar for only short times and small spatial domains.</p><p>More recently, <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref> introduce a solution to solve PDE in a data-driven manner. Rather than analytically solving a given equation, they infer solutions to targeted PDE via supervised learning. Given an input tuple (x, t), they compute spatial and time derivatives from black-box models and then the output are connected based on a form of PDE to update all learnable parameters in the black-box models. This method does not require any discretization and it is fully data-driven to find a surrogate model. <ref type="bibr" target="#b39">[39]</ref> approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal dataset. <ref type="bibr" target="#b40">[40]</ref> introduce a technique based on the singular vector canonical correlation analysis (SVCCA) for measuring the generality of neural network layers across a continuously-parameterized set of tasks. They illustrate this method by studying generality in neural networks trained to solve parameterized boundary value problems based on the Poisson partial differential equation. <ref type="bibr" target="#b41">[41]</ref> directly model the mapping between a PDE's solution and its initial conditions via message passing in the spatial domain. The following work <ref type="bibr" target="#b42">[42]</ref> further extends the modeling to the frequency domain, which inherently generalizes to multiple spatial resolutions.</p><p>In many physical systems, the governing equations are known with high confidence, but direct numerical solution is prohibitively expensive. Often this situation is alleviated by writing effective equations to approximate dynamics below the grid scale. This process is often impossible to perform analytically and is often ad hoc. <ref type="bibr" target="#b43">[43]</ref> propose data-driven discretization, a method that uses machine learning to systematically derive discretizations for continuous physical systems. <ref type="bibr" target="#b44">[44]</ref> target the problem of reducing numerical errors of iterative PDE solvers and compare different learning approaches for finding complex correction functions. They integrate the PDE solver into the training loop and thereby allow the model to interact with the PDE during training.</p><p>Downscaling Directly solving PDEs require spatial and temporal discretization and finer resolution is more desired to capture physically reliable solutions. However, it increases the computational cost and modeling complexity. Downscaling techniques have been widely used as a solution to capture physical variables that need to be modeled at a finer resolution from a coarser resolution. Recently, artificial neural networks have shown a lot of promise for this problem, given their ability to model non-linear relationships. <ref type="bibr" target="#b45">[45]</ref> present a downscaling algorithms using neural networks to leverage the relationships between Satellite precipitation estimates (SPEs) and cloud optical and microphysical properties in northeast Austria. <ref type="bibr" target="#b46">[46]</ref> present DeepSD (Statistically Downscaling), a generalized stacked super resolution convolutional neural network (SRCNN) framework for statistical downscaling of climate variables. DeepSD augments SRCNN with multi-scale input channels to maximize predictability in statistical downscaling. <ref type="bibr" target="#b47">[47]</ref> introduce a data-driven framework for the identification of unavailable coarse-scale PDEs from microscopic observations via machine-learning algorithms. Specifically, using Gaussian processes, artificial neural networks, and/or diffusion maps, the proposed framework uncovers the relation between the relevant macroscopic space fields and their time evolution (the right-hand side of the explicitly unavailable macroscopic PDE)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Parameterization</head><p>A common technique used in numerical models describing complex physics phenomena is replacing dynamic systems that are hard to model with parameterized simple processes, which is named parameterization. While the traditional way to decide parameters is to minimize the discrepancy between models' output and observed data by conducting grid search or Bayesian inference within a certain parameter space, recent works have adopted advances in machine learning as the approximation of unknown dynamics.</p><p>In geology, <ref type="bibr" target="#b48">[48]</ref> study the application of Wasserstein GAN <ref type="bibr" target="#b49">[49]</ref> for the parametrization of geological models. The effectiveness of the method is assessed for uncertainty propagation tasks using several test cases involving different permeability patterns and subsurface flow problems. <ref type="bibr" target="#b50">[50]</ref> develop a new predictor for near-bed suspended sediment reference concentration under unbroken waves using genetic programming, a machine learning technique.</p><p>In meteorological science, <ref type="bibr" target="#b51">[51]</ref> show that a neural network-based parameterization is successfully trained using a nearglobal aqua-planet simulation with a 4-km resolution (NG-Aqua). The neural network predicts the apparent sources of heat and moisture averaged onto (160 km 2 ) grid boxes. <ref type="bibr" target="#b52">[52]</ref> present a novel approach to convective parameterization based on machine learning, using an aquaplanet with prescribed sea surface temperatures as a proof of concept. A deep neural network is trained with a super-parameterized version of a climate model in which convection is resolved by thousands of embedded 2-D cloud resolving models.</p><p>In chemistry, supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. <ref type="bibr" target="#b53">[53]</ref> reformulate existing models into a single common framework we call Message Passing Neural Networks and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark. <ref type="bibr" target="#b54">[54]</ref> utilize molecular graph data for property prediction based on spatial graph convolution neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Reduced-Order Models</head><p>Instead of directly modeling the complex dynamics in the observation space, researchers have developed simplified but more interpretable models (reduced-order models, ROMs) in the hidden space, which is usually derived from the observation space via dimensionality reduction. In recent works, machine learning models have emerged as effective tools to discover such hidden spaces and more information preserving transformations between them and the observation space.</p><p>In dynamic system analysis, Koopman theory is a typical ROM. Koopman theory is based on the insight that the state space of a non-linear dynamic system can be encoded into an infinite-dimensional space where the dynamics is linear <ref type="bibr" target="#b55">[55]</ref>. In practice, people assume the infinite-dimensional space can be approximated with a finite-dimensional space. The key problem is then to find a proper pair of encoder/decoder to map from/to the state space to/from the hidden space. Traditionally, people construct the encoder/decoder with hand-crafted functions, such as the identity function in Dynamic Mode Decomposition (DMD) <ref type="bibr" target="#b56">[56]</ref>, nonlinear functions in Extended DMD (EDMD) <ref type="bibr" target="#b57">[57]</ref>, and kernel functions in Kernel DMD (KDMD) <ref type="bibr" target="#b58">[58]</ref>. However, hand-crafted functions may fail to fit complex dynamic systems and are hard to design without domain-specific knowledge. Thus, recent works <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b61">61]</ref> construct encoders/decoders using neural networks as trainable universal approximators. They demonstrate that the combination of neural networks and Koopman theory achieves comparable or even higher performance than the Koopman approximators with hand-crafted mapping functions, while enjoying the ability to generalize to multiple datasets with the same design. <ref type="bibr" target="#b59">[59]</ref> further shows that the integration of Koopman theory allows the model to adapt to new systems with unknown dynamics faster than pure neural networks.</p><p>In fluid dynamics, ROMs is largely considered due to the unprecedented physical insight into turbulence offered by high-fidelity computational fluid dynamics (CFD). <ref type="bibr" target="#b62">[62]</ref> develop a dimensionality reduction method called Non-Intrusive Reduced Order Model (NIROM) for predicting the turbulent air flows found within an urban environment. <ref type="bibr" target="#b63">[63]</ref> demonstrate a deep learning based approach to build a ROM using the POD basis of canonical DNS datasets, for turbulent flow control applications. They find that a type of Recurrent Neural Network, the Long Short Term Memory (LSTM) which has been primarily utilized for problems like speech modeling and language translation, shows attractive potential in modeling temporal dynamics of turbulence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Causality</head><p>Causal Discovery and Causal Inference in Time Series How to discover the underlying causal structure is a fundamental problem and has still been actively studied. <ref type="bibr" target="#b64">[64,</ref><ref type="bibr">65,</ref><ref type="bibr" target="#b65">66]</ref> introduce the problem and provided a mathematical framework for causal reasoning and inference under causal graphical models (also known as Bayesian networks (BN) <ref type="bibr" target="#b66">[67]</ref>). <ref type="bibr" target="#b67">[68]</ref> formalize a concept of quantifiable causality in time series, called Granger causality. From the pioneering works, learning causal associations from time series has been an emerging topic in machine learning and deep learning community as well. <ref type="bibr" target="#b68">[69]</ref> propose a method to distinguish direct from indirect dependencies and common drivers among multiple time series to reconstruct a causal network. <ref type="bibr" target="#b69">[70]</ref> quantify causal associations in nonlinear time series and <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref> provide promising applications of the causal discovery in time series. <ref type="bibr" target="#b72">[73]</ref> introduce a smooth acyclicity constraint to multivariate time series inspired by <ref type="bibr" target="#b73">[74]</ref> that consider a causal discovery as a purely continuous optimization problem.</p><p>For causal inference part, G-computation formula, g-estimation of structural nested mean models <ref type="bibr" target="#b74">[75]</ref>, and inverse probability of treatment weighting in marginal structural models (MSMs) <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77]</ref> rely on linear predictors for estimation to estimate treatment effects. Recurrent marginal structural networks (RMSNs) <ref type="bibr" target="#b77">[78]</ref> is proposed to further improve MSM's ability by capturing nonlinear dependencies. In addition, Gaussian process <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref> has been tailored to estimate treatment response in a continuous-time settings in order to incorporate non-deterministic quantification. Furthermore, <ref type="bibr" target="#b80">[81]</ref> and <ref type="bibr" target="#b81">[82]</ref> theoretically prove that observed proxy variables can be used to capture hidden confounders and estimate treatment effects. TSD <ref type="bibr" target="#b82">[83]</ref> introduces recurrent neural networks in the factor model to estimate the dynamics of confounders. In a similar vein, <ref type="bibr" target="#b83">[84]</ref> propose a sequential deconfounder to infer hidden confounders by using Gaussian process latent variable model. DTA <ref type="bibr" target="#b84">[85]</ref> estimates treatment effects under dynamic setting using observed data as noisy proxies. Besides, DSW <ref type="bibr" target="#b85">[86]</ref> infers the hidden confounders by using a deep recursive weighted neural network that combines current treatment assignment and historical information. DNDC <ref type="bibr" target="#b86">[87]</ref> aims to learn how hidden confounders behave over time by using current network observation data and historical information. Although many works are successful to discover unknown causal structure from observational data directly and make effective causal inference, there are few works to leverage explicit causal relations from physical knowledge to improve data-driven models.</p><p>Counterfactual Analysis of Physical Dynamics Counterfactual analysis in the physical world is typically concerned with analytically predicting the effects of various types of interventions/treatments, including the physical laws of current environment <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b89">90]</ref>, the actions of the agent itself <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b93">94]</ref>, and the decision outcomes of other agents in multi-agent systems <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b95">96]</ref>. With the development of 3D simulation in the field of machine learning, several benchmarks performing counterfactual analysis in physics have emerged. CLEVRER <ref type="bibr" target="#b96">[97]</ref> asks agents to answer a counterfactual question after observing a video showing the movement and collision of a 3D object. Physics 101 <ref type="bibr" target="#b97">[98]</ref> presents a video benchmark containing over 101 real-world objects physically interacting in four different physical scenarios. <ref type="bibr" target="#b98">[99]</ref> proposes a counterfactual benchmark with two tasks: a scene where balls interact according to unknown interaction laws (e.g., gravity or elasticity), and a scene where clothes are folded by the wind. The agent needs to discover causal relationships between counterfactual variables and objects and then predicts future frames. CoPhy <ref type="bibr" target="#b99">[100]</ref> separates the observed experiments from those of counterfactuals and includes three complex 3D scenarios involving rigid body dynamics.</p><p>The counterfactual analysis of the physics-informed models relies on the separation between the physical information features and the remaining features <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102]</ref> and may incorporate additional information based on the available prior knowledge of the scene <ref type="bibr" target="#b102">[103]</ref>. Recently, PhyDNet <ref type="bibr" target="#b103">[104]</ref> explicitly disentangles PDE dynamics from unknown complementary information. The interpretable intuitive physical model <ref type="bibr" target="#b104">[105]</ref> proposes an encoder-decoder framework for predicting future collision event frames. The encoder layer infers physical properties, such as mass and friction, from the input frames. Then, the decoder decomposes the potential physical vectors by outputting optical flow. PIP <ref type="bibr" target="#b105">[106]</ref> uses a deep generative model to build approximate mental simulations by generating a framework for future physical interactions and then employs selective temporal attention in the form of spanwise selection to predict the outcome of physical interactions. CWMs <ref type="bibr" target="#b98">[99]</ref> allow unsupervised modeling of relationships between the intervened observations and the alternative futures by learning an estimator of the latent confounding factors. Cophy <ref type="bibr" target="#b99">[100]</ref> predicts alternative outcomes of a physical experiment by estimating the potential performance of confounding factors. Filter-Cophy <ref type="bibr" target="#b106">[107]</ref> further learns and acts on a suitable hybrid potential representation based on a combination of dense features, sets of 2D keypoints, and an additional latent vector per keypoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Physics for ML: Improvement of Data-Driven Models from External Knowledge</head><p>Data-driven methods represented by machine learning and neural networks have achieved great success recently in a wide range of real-world problems due to the universal approximation ability of neural networks <ref type="bibr" target="#b107">[108]</ref>, the increase of available data for model training, and the rapid development of hardware neural network accelerators. However, the optimization of neural networks is highly non-convex, and its convergence to global minima is hard to achieve in practice. Optimization processes converging to local minima without constraints may lead to models with limited generalization ability or results violating existing knowledge including commonsense, logic rules, and physics laws <ref type="bibr" target="#b22">[22]</ref>. In this work, we focus on the integration of physics knowledge in machine learning, and the following sections will introduce the integration in multiple domains of machine learning respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Supervised Learning</head><p>Typical supervised learning tasks including classification and forecasting cover various real-world applications from multi-agent systems, computer vision, time series analysis, and spatio-temporal data modeling. While deep neural network-based methods have been the dominating solution in these areas recently, people have also explored several novel approaches with the incorporation of physics knowledge in tasks on data from physics-related scenarios, such as object-centric data, spatio-temporal data, and geometry data.</p><p>Object-centric Data Object-centric data is generated by systems composed of multiple discrete objects. Examples of real-world object-centric data include trajectory data from multi-agent systems <ref type="bibr" target="#b108">[109,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b110">111]</ref>, position and velocity data from networks of motion sensors <ref type="bibr" target="#b109">[110,</ref><ref type="bibr" target="#b111">112]</ref>, and molecule data <ref type="bibr" target="#b53">[53]</ref>. Graph neural network (GNN)-based methods <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b110">111]</ref> achieve state-of-the-art results on most tasks with object-centric data due to the matching of its inductive bias <ref type="bibr" target="#b112">[113]</ref> and the interacting physics property in object-centric data. <ref type="bibr" target="#b114">[115]</ref> demonstrates that GNNs generalize well in many popular object-centric tasks because the forwarding process of GNNs aligns to the underlying reasoning process. In addition to this alignment, other types of physics knowledge are also integrated in recent works. <ref type="bibr" target="#b115">[116]</ref> obtains a joint model for the total energy and interatomic forces of molecules that follows the energyconserving law. <ref type="bibr" target="#b116">[117]</ref> proposes Deep Lagrangian Networks (DeLaN) for robotics, on which Lagrangian Mechanics is imposed and physical plausibility is maintained. <ref type="bibr" target="#b108">[109]</ref> designs models for forecasting n-body systems that learn and respect exact conservation laws -Hamiltonian mechanics -in an unsupervised manner. Lagrangian Neural Networks (LNNs) in <ref type="bibr" target="#b117">[118]</ref> further parameterize arbitrary Lagrangians using neural networks without canonical coordinates or restrictions of the functional form of learned energies, and can be applied to graphs and continuous systems. <ref type="bibr" target="#b59">[59]</ref> learns compositional Koopman operators using GNNs and shows better efficiency and generalization than existing GNN baselines in multi-object systems such as ropes and soft robots.</p><p>Spatio-Temporal Data Spatio-temporal data records the dynamics of values of interest in multiple locations within a period of time. The most common form of spatio-temporal data is video, where all locations are aligned to rectangular mesh grids in some specific area. Spatio-temporal data can also be generated from networks of irregularly spaced sensors in domains such as traffic <ref type="bibr" target="#b118">[119]</ref>, weather <ref type="bibr" target="#b119">[120]</ref>, and electricity <ref type="bibr" target="#b120">[121]</ref>. Since the underlying processes of spatio-temporal data are usually governed by physics laws, physics-informed methods have potentials to further improve the performance of neural network models. <ref type="bibr" target="#b103">[104]</ref> explicitly disentangles known PDE dynamics from unknown factors and performs PDE-constrained prediction in latent-space, both of which contributes to video forecasting performance. <ref type="bibr" target="#b121">[122]</ref> mimics the pipeline of physical flow simulation, and evolves and accumulates point features in point clouds using flow velocities generated from a high-dimensional force field. It demonstrates the efficacy of the proposed method in various point cloud classification and segmentation problems. <ref type="bibr" target="#b122">[123]</ref> learns finite differences of sparsely available data points inspired by physics equations, and shows the superiority in synthetic graph signal prediction and real-world weather forecasting tasks. <ref type="bibr" target="#b123">[124]</ref> proposes a general continuous-time differential model for dynamical systems, which admits arbitrary space and time discretizations, and enables efficient neural PDE inference.</p><p>Manifold Data Manifold data describes signals defined on non-planar surfaces such as spheres and surfaces of complex 3D objects, where the Euclidean geometry only holds locally near each point. Examples are magnetoencephalography (MEG) brain activity signal data <ref type="bibr" target="#b124">[125]</ref> (on sphere), omnidirectional image data <ref type="bibr" target="#b125">[126]</ref> (on 3D objects), and human scan data <ref type="bibr" target="#b126">[127]</ref> (on 3D objects). Without the restriction that data points are distributed on planes, general manifold data comes with richer information about the space including local structures and symmetries of the space. Meanwhile, existing convolution-based neural network architectures are either no longer applicable in nonplanar manifolds (CNNs) or not capable of fully exploiting the spatial information (GNNs). Instead, physics-informed neural networks incorporate established knowledge of manifolds into the construction of new types of convolutions for manifolds and bridge the gap. <ref type="bibr" target="#b127">[128]</ref> constructs intrinsic CNN-like architectures on non-planar surfaces under the geodesic polar coordinate system. <ref type="bibr" target="#b128">[129]</ref> alternatively uses the principal curvature directions as fixed gauges to construct convolutions in corresponding tangent fields. <ref type="bibr" target="#b129">[130]</ref> constructs convolution kernels as rotating filters and collects the strongest responses among all possible directions. <ref type="bibr" target="#b130">[131,</ref><ref type="bibr" target="#b131">132]</ref> further extends convolution filters to be gauge-equivariant via parallel transporting geometric features to the same vector space before applying the filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Model-Based Control</head><p>Due to the integration of physics knowledge, physics-informed machine learning models enjoy better physical plausibility, higher data efficiency and stronger generalization ability compared to pure neural network models, all of which are critical properties for constructing a good model describing the relation between the control input and the state transition of dynamic systems. Recent works have shown that physics-informed machine learning models achieve significant success in model predictive control and model-based reinforcement learning. <ref type="bibr" target="#b132">[133]</ref> integrates a deep neural network (DNN) that learn high-order interactions into the dynamics model, and constrain the Lipschitz constant of the DNN to guarantee system stability. <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b133">134]</ref> enforce Lagrangian and Hamiltonian dynamics in the modeling of underlying system dynamics respectively, and both outperform model learning approaches without physics knowledge in trajectory tracking error, learning speed, and robustness. <ref type="bibr" target="#b59">[59]</ref> shows that dynamics learned via compositional Koopman operators can quickly adapt to new environments of unknown physical parameters in online learning. <ref type="bibr" target="#b134">[135,</ref><ref type="bibr" target="#b135">136]</ref> augment partial differential equations (PDEs) that approximately describe continuous physical systems with controllable force terms, and demonstrate that proposed methods successfully control the evolution of complex physical systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Physics Knowledge in PIML</head><p>In this section, we introduce several categories of general physics knowledge integrated in PIML. While there is much more domain/task-specific knowledge that can be incorporated for corresponding solutions, each category we introduce in this section covers a wide range of problems and inspires a series of works generally applicable to them instead of leading to only one or two task-specific solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classical Mechanics and Energy Conservation Laws</head><p>Newtonian, Lagrangian, and Hamiltonian mechanics are three typical approaches describing systems of classical mechanics. While the Newtonian mechanics has been widely used to describe the relations among locations, velocities, accelerations and forces, Lagrangian and Hamiltonian mechanics provide effective tools to enforce laws of conservation of energy in the modeling of dynamic systems. Since Newtonian mechanics described as the famous Newton's Three Laws has been widely known, here we only focus on Lagrangian and Hamiltonian mechanics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lagrangian Mechanics</head><p>The Lagrangian mechanics defines the Lagrangian function L of generalized coordinates q and its gradient w.r.t time q to fully describe dynamics of a mechanical system. Usually L is chose as the difference between the kinetic energy T and the potential energy V , i.e. L(q, q) ≡ T (q, q) -V (q). The defined Lagrangian function must satisfy the principle of stationary action: the real physical trajectory of a system will always take is the one in which the action happens to be stationary, which is mathematically expressed as:</p><formula xml:id="formula_0">δ t2 t1 Ldt = 0.<label>(1)</label></formula><p>We can further derive the important Euler-Lagrange equation from Eq 1:</p><formula xml:id="formula_1">d dt ∇ q L = ∇ q L.<label>(2)</label></formula><p>According to Noether's theorem, the energy of a system is conserved if the system has a time-translation symmetry, i.e. if the Lagrangian function does not explicitly depend on time ( ∂L ∂t = 0). Therefore, as long as we model the Lagrangian function as a function of q, q without the explicit time term t and satisfies Eq 2, it automatically satisfies the law of conservation of energy. With Eq 2 we can derive the expression of q with the chain rule <ref type="bibr" target="#b117">[118]</ref>:</p><formula xml:id="formula_2">(∇ q ∇ T q L) q + (∇ q ∇ T q L) q = ∇ q L. (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>When L is modeled as a differential function such as neural networks, we can solve q as:</p><formula xml:id="formula_4">q = (∇ q ∇ T q L) -1 [∇ q L -(∇ q ∇ T q L) q].<label>(4)</label></formula><p>Usually one system's q and q can be observed as spatial coordinates and speeds of objects, thus the system can be solved with (q, q)| t=T = T 0 ( q, q)dt. For systems with non-conservative forces, Eq 2 can be extended as</p><formula xml:id="formula_5">d dt ∇ q L -∇ q L = τ ,<label>(5)</label></formula><p>where τ are generalized forces and can be used for controllers such as a PD-Controller <ref type="bibr" target="#b116">[117]</ref>.</p><p>Hamiltonian Mechanics The Hamiltonian mechanics defines the Hamiltonian function H as the function of a pair of variables (q, p), where q is the vector of spatial coordinates of system objects and p is the vector of their momentum.</p><p>(q, p) must satisfy the canonical condition:</p><formula xml:id="formula_6">p = ∇ q L,<label>(6)</label></formula><p>where L is the Lagrangian function of the system. The Hamiltonian function is defined as follows:</p><formula xml:id="formula_7">H(q, p) = q • p -L.<label>(7)</label></formula><p>Combine Eq 6, Eq 7 and Eq 2 we can derive:</p><formula xml:id="formula_8">ṗ = -∇ q H, q = ∇ p H.<label>(8)</label></formula><p>With Eq 8, we can verify that</p><formula xml:id="formula_9">dH dt = q∇ q H + ṗ∇ p H = 0.<label>(9)</label></formula><p>In the context of classical mechanics, we have p = M q, T = 1 2 qT M q, L = T -V , then Eq 7 can be rewritten as H(q, p) = qT M q -T + V = T + V , where M is the mass matrix of the system. Here the Hamiltonian function H is exactly the total energy of the system. Therefore, Eq 9 shows that systems described with Hamiltonian mechanics follows the law of conservation of energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Symmetry, Invariant and Equivariant Functions</head><p>A symmetry defined on an object or system is some transformation that keeps certain properties unchanged. Typical symmetries include shifts in visual object classification problems, rotations in molecule property prediction problems, and permutations in particle systems. For one object or system, its symmetries form a symmetry group, where the following rules are satisfied: (1) associativity, (2) identity, (3) inverse, (4) closure. Some common symmetry groups are: T (n) (n-dim translation group), O(n) (n-dim distance-perserving group including both rotation and reflection), SO(n) (n-dim rotation group), E(n) (n-dim Euclidean group, including translation, rotation and reflection).</p><p>On a set Ω representing a domain, we can define a group action as a mapping (g, u) → g.u, where g is one element of a symmetry group G, u and g.u are two points in Ω. A group action shall satisfy associativity: g.(h.u) = (gh).u for all g, h ∈ G and u ∈ Ω. In comparison, group action on a signal space X : Ω → R n is defined as:</p><formula xml:id="formula_10">(g.x)(u) = x(g -1 u).<label>(10)</label></formula><p>We can verify that the above definition satisfies the associativity:</p><formula xml:id="formula_11">(g.(h.x))(u) = (h.x)(g -1 (u)) = x((gh) -1 u) = ((gh).x)(u).<label>(11)</label></formula><p>On a domain Ω ∈ R n , we can define an n-dimensional real representation of a group G as a map ρ : G → R n×n , connecting each g ∈ G to an invertible matrix ρ(g), and satisfying the associtivity ρ(gh) = ρ(g)ρ(h) for all g, h ∈ G.</p><p>For a symmetry group G we have the definitions of G-invariant and G-equivariant. A function f :</p><formula xml:id="formula_12">X (Ω) → Y is G-invariant if f (ρ(g)x) = f (x) for all g ∈ G and x ∈ X (Ω), and it is G-equivariant if f (ρ(g)x) = ρ(g)f (x).</formula><p>By stacking multiple neural network layers(functions), each of which satisfies either equivariance or invariance under some symmetry group, we can incorporate the knowledge of symmetries of domains into the resulting network. Table 1: Neural network architectures, symmetry groups and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Numerical Methods for Partial Differential Equations (PDEs)</head><p>A generic form of PDEs describing the evolution of a continuous value v(x, t) is as follows:</p><formula xml:id="formula_13">∂v ∂t = F (t, x, v, ∂v ∂x i , ∂ 2 v ∂x i ∂x j , . . . ).<label>(12)</label></formula><p>Finite Difference Method Finite difference methods approximate spatial derivatives on the right hand side of Eq 12 at a certain point x 0 as a linear combination of function values at N neighbors of x 0 . In 1D case, the approximation of the l-th order spatial derivative can be formulated as (here we omit t):</p><formula xml:id="formula_14">∂ l f ∂x l x=x0 = N n=1 α n f (x 0 + h n ).<label>(13)</label></formula><p>α 1 , α 2 , . . . , α n can be solved via expanding α 1 f (x 0 + h 1 ), α 2 f (x 0 + h 2 ), . . . , α N f (x 0 + h N ) at x 0 to the N -th order:</p><formula xml:id="formula_15">     α 1 f (x 0 + h 1 ) = α 1 N -1 k=0 1 k! ∂ k f ∂x k h k 1 + O(h N -1 1 ) . . . α N f (x 0 + h N ) = α N N -1 k=0 1 k! ∂ k f ∂x k h k N + O(h N -1 N ) ,<label>(14)</label></formula><p>which can be summed together as</p><formula xml:id="formula_16">N n=1 α n f (x 0 + h n ) = N -1 k=0 ( 1 k! N i=1 α i h k i ) ∂ k f ∂x k + O(max i∈[N ] h N -1 i )<label>(15)</label></formula><p>To solve α 1 , α 2 , . . . , α n for a certain order l, we can let the multiplier of ∂ k f ∂x k be 1 and others be 0, and solve following linear equations:</p><formula xml:id="formula_17">          1 1 . . . 1 h 1 1 h 1 2 . . . h 1 N . . . . . . . . . h l 1 h l 2 . . . h l N . . . . . . . . . h N -1 1 h N -1 2 . . . h N -1 N               α 1 α 2 . . . α N     =          0! • 0 1! • 0 . . . l! • 1 . . . (N -1)! • 0          ,<label>(16)</label></formula><p>the solution α * 1 , . . . , α * N of which satisfies</p><formula xml:id="formula_18">∂ k f ∂x k = N n=1 α * n f (x 0 + h n ) + O(max i∈[N ] h N -1 i ).<label>(17)</label></formula><p>The above method can be extended to multi-dimensional cases with multi-dimensional Taylor expansion. After approximating spatial derivatives on the left hand side, Eq 12 can be solved with standard techniques of numerical integration. While Eq 16 can fully determine all coefficients, recent works <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b140">141]</ref> relax it by removing some constraining equations and use neural networks to learn undetermined coefficients to combine prior knowledge with stronger expressivity.</p><p>Finite Volume Method Unlike finite difference method, finite volume method represent the value v(x, t) with its averages over a grid cell: v i (t) = ∆x -1 xi+∆x/2 xi-∆x/2 v(x ′ , t)dx ′ . Similar to the finite difference method, spatial derivatives can also be estimated as the linear combination of averaged cell values:</p><formula xml:id="formula_19">∂ l v ∂x l = N i=1 α<label>(l)</label></formula><p>i v i , and the coefficients are estimated in the same way as finite difference method.</p><p>For finite volume methods, the equation must be able to be rewritten as:</p><formula xml:id="formula_20">∂v ∂t = ∂ ∂x J(t, x, v, ∂v ∂x i , ∂ 2 v ∂x i ∂x j , . . . ),<label>(18)</label></formula><p>where J is called a flux and has an analytical form derived from the original PDE. The evolution along time can be carried out in following steps: (1) spatial derivatives are estimated on the boundary between grid cells; (2) the flux J is calculated with approximated derivatives using its analytical form; (3) the temporal derivative of averaged cell values is calculated via subtracting J ath the cell's left and right boundaries. The final step can be conducted with techniques that promote stability, such as monotone numerical fluxes and Godunov flux <ref type="bibr" target="#b43">[43]</ref>. Recent works have integrated datadriven models in step (1) to improve the estimation of spatial derivatives. For example, <ref type="bibr" target="#b43">[43]</ref> estimate coefficients with the combination of neural network results and numerical results, and <ref type="bibr" target="#b141">[142]</ref> uses policies trained via reinforcement learning to determine coefficients for estimation.</p><p>Finite Element Method Finite element method divide a space into small parts (elements) and approximate the PDE on each. As illustrated in <ref type="bibr" target="#b142">[143]</ref>, we use the Poisson's equation to show the basic idea of finite element method.</p><p>The formulation of the Poisson's equation is as follows:</p><formula xml:id="formula_21">-∆u = λ in Ω u = 0 on Γ.<label>(19)</label></formula><p>In finite element method, we multiply a test function v on both sides, integrate over Ω, and use integration in parts to derive the weak formulation:</p><formula xml:id="formula_22">a(u, v) = l(λ, v), for all v ∈ V,<label>(20)</label></formula><p>where a(u, v)</p><formula xml:id="formula_23">= Ω ∇u • ∇vdx, l(λ, v) = Ω λvdx.<label>(21)</label></formula><p>Then we construct a finite-dimensional subspace V h ⊂ V, where V h is a piece-wise polynomial function space spanned via {φ 1 , φ 2 , . . . , φ n }. To solve the weak formulation, we transform it to the Galerkin weak formulation, which is an approximation of Eq 20:</p><formula xml:id="formula_24">Au = f ,<label>(22)</label></formula><p>where A ∈ R n×n , A ij = a(φ i , φ j ), u ∈ R n is the solution vector and f ∈ R n is the source vector with f i = l(φ i , λ). The solution of Eq 22 gives the best solution of the PDE in V h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Koopman Theory</head><p>Given a non-linear dynamic system with its state vector at time t denoted as s t ∈ R m . The system can be described as s t+1 = F (s t ). As defined in <ref type="bibr" target="#b55">[55]</ref>, the Koopman operator K F is a linear transformation defined on a function space F by K F g = g • F for every g : R m → R that belongs to the infinite-dimensional Hilbert space F . With the definition we have K F g(s t ) = g • F (s t ) = g(s t+1 ).</p><p>The Koopman theory <ref type="bibr" target="#b55">[55]</ref> guarantees the existence of K, but in practice we often assume the existence of an invariant finite-dimensional subspace G of F spanned by k bases {g 1 , g 2 , . . . , g k }. Define g t = [g 1 (s t ), g 2 (s t ), . . . , g k (s t )] T and g t+1 = [g 1 (s t+1 ), g 2 (s t+1 ), . . . , g k (s t+1 )] T , under the assumption we have g t , g t+1 ∈ G and there exists a Koopman matrix K ∈ R k×k s.t. g t+1 = Kg t . The key problem is to find the pair of mappings between the state space R m and the invariant subspace G ∈ R k : g : R m → R k and g -1 : R k → R m . Recent works <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60]</ref> utilize neural networks as g and g -1 to find the mappings in a data-driven way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods of PIML</head><p>Typical solutions to a problem with machine learning involve three key parts: data, model, and optimization, each of which can be integrated with prior physics knowledge. In the following parts, we will introduce existing techniques of incorporating physics knowledge to each part respectively. However, we should notice that these techniques are not mutually exclusive: physics knowledge can be integrated to more that one parts of the machine learning solution.</p><p>Knowledge Form Integration Method Data Model Optimization Simulation Data Transfer Learning Auxiliary Tasks Computation Graph Fusion Knowledge Based Loss Regularization Domain Knowledge in Analytical Form</p><p>Table <ref type="table">2</ref>: Existing works classified based on forms of physics knowledge and integration methods. For certain types of knowledge forms, inductive bias integrated in reusable computation graphs have advantages over integration with data and optimization.</p><p>In Table <ref type="table">2</ref>, we classify existing works based on the forms of physics knowledge and the integration methods. We notice that for domain knowledge taking analytical forms, existing works integrate the knowledge into all three aspects including data, model, and optimization. However, research works on integrating other general types of physics knowledge, including energy conservation law, symmetry, numerical methods for PDEs, and Koopman theory, mainly focus on incorporating corresponding knowledge into computation graphs. The main reason is that such general physics knowledge is possible to be transformed to inductive bias in reusable network architectures, which has advantages over data augmentation and physics knowledge based loss functions in terms of prediction performance and data efficiency <ref type="bibr" target="#b136">[137]</ref>. This is due to that (1) general physics knowledge applies to various problems and thus leads to general network architectures, and (2) has simpler forms that can be translated to combinations of a limited number of differentiable operators compared to complex numerical simulators designed for domain-specific problems such as weather and turbulence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Physics-Informed Data Enhancement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Data Generated from Simulation</head><p>The Universal Approximation Theorem <ref type="bibr" target="#b107">[108]</ref> guarantees that multilayer neural networks with as few as one hidden layer can approximate any continuous function from one finite dimensional space to another to any desired degree of accuracy. Therefore, one straight forward method to incorporate physics knowledge into neural networks is to generate training data from the desired physics knowledge. When the data amount is abundant, the neural network is expressive enough and trained properly, the trained neural network will be able to approximate the behavior of the physics knowledge governing the data generation. Usually the neural network models can be accelerated with hardware such as GPU, FPGA and ASIC, thus they can act as good surrogate models with much lower computation costs while maintaining comparable accuracy to the numerical simulation.</p><p>[26] present a benchmark dataset from results of numerical global weather simulation with high computation cost, and provide scores of deep learning models. Results show that data-driven models trained with simulation data can achieve competitive results compared to numerical solutions while enjoying lower computation costs. <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b143">144]</ref> utilize different variants of message passing graph neural networks <ref type="bibr" target="#b168">[169,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b113">114]</ref> respectively and train them with the simulation data of particle systems. Compared to the simulation methods generating datasets, the trained surrogate model can accurately predict the dynamics of a wide range of physical systems within the same architecture, and runs orders of magnitude faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Transfer Learning</head><p>For real-world tasks suffering from data limitation or labeling difficulties, the integration of prior physics knowledge about the tasks is critical. Simulators constructed with such physics knowledge can provide large amount of data with high label quality, and can be used to pre-train models. However, the differences between the target real-world data distributions and simulation data distributions call for techniques of transfer learning to mitigate the gap.</p><p>[145] present a system for training object detection models with synthetic images. This work adopts the technique of domain randomization, where important parameters of simulators -including lighting, pose, object textures -are randomized in non-realistic ways to encourage the model to learn essential features. <ref type="bibr" target="#b145">[146]</ref> uses off-the-shelf simulators to render synthetic data for training a grasping system together with pixel-level domain adaptation between synthetic images and real-world ones. The utilization of synthetic data reduces the required amount of real-world samples by up to 50 times. <ref type="bibr" target="#b146">[147]</ref> transfer driving policies trained from simulation to reality via modularity and abstraction, where the driving policy is exposed to segmentation results of input scenes and target way points, instead of raw perceptual input or low-level vehicle dynamics. <ref type="bibr" target="#b147">[148]</ref> incorporate the task-specific prior knowledge into the model and pre-train it with synthetic data generated by imperfect physical models, which allows the model to get close enough to the target solution and only a small amount of real-world data is needed for refining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Multitask Learning and Meta Learning with Auxiliary Tasks</head><p>Synthetic data generated from physics-based simulators can also be used to construct auxiliary learning tasks for improving the model's performance on the target task with techniques of multitask learning and meta learning. <ref type="bibr" target="#b115">[116]</ref> constructs the auxiliary task as the prediction of interatomic forces for the main task of molecular energy prediction. The labels of the auxiliary task and the main task are generated from simulation at the same time, while the prediction of the auxiliary task is produced via differentiating the energy prediction model. Both tasks are used to train the model simultaneously. <ref type="bibr" target="#b161">[162]</ref> adopts the multitask learning scheme by learning shared representations between multiple related PDEs, which are generated by varying coefficients, for better generalizability of the proposed neural network based PDE solver. <ref type="bibr" target="#b162">[163]</ref> proposes a spatiotemporal forecasting model with decoupled spatial and temporal modules, where the spatial module is PDE-independent and are trained via model agnostic meta learning (MAML) <ref type="bibr" target="#b169">[170]</ref> for fast adaptation on new tasks, while the task-dependent temporal module is trained from scratch for each task.</p><p>4.2 Physics-Informed Neural Network Architecture Design</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Physics-Informed Computation Graph</head><p>A typical way of physics-informed neural network architecture design is to design computation graphs that mimic the behavior of physics knowledge based methods. While the specific method is highly dependent on the physics knowledge, a general idea is to start from some existing physics based solution, then replace difficult-to-estimate variables with outputs of neural networks, or relax some fixed parameters by enabling them to adapt to the data. In following paragraphs, we will introduce several knowledge-specific neural network designs as well as techniques to directly fuse deep learning models with physics based solutions as a hybrid model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Energy Conservation Laws</head><p>As introduced in Sec 3.1, Lagrangian and Hamiltonian mechanics are powerful in enforcing energy conservation laws, thus a series of recent works develop neural network architectures based on them to incorporate the energy conservation property. <ref type="bibr" target="#b116">[117]</ref> proposes a network topology named Deep Lagrangian Networks (DeLaN) encoding the Lagrange-Euler PDE originating from Lagrangian Mechanics, which can be trained with standard optimizers while maintaining physical plausibility. <ref type="bibr" target="#b117">[118]</ref> designs Lagrangian Neural Networks (LNNs) to model arbitrary Lagrangian functions via neural networks, and solve the dynamics of the system with a numerical expression derived from the Euler-Lagrangian equation, where gradients from auto differentiation are utilized. Similarly, <ref type="bibr" target="#b108">[109]</ref> (HNN) models the Hamiltonian function with a neural network. The derivatives of spatial coordinates and momentum with respect to time are derived from Eq 8. <ref type="bibr" target="#b153">[154]</ref> proposes Neural Hamiltonian Flow (NHF), which is a powerful normalising flow model using Hamiltonian dynamics as the invertible function to model expressive densities. In NHF, the density is decomposed into the "coordinate" part and the "momentum" part in the hidden space, both of which are then propagated with Eq 8. The propagation is (1) invertible and (2) has the volume ("energy" with respect to the hidden space) preserving property, which satisfies the requirement of normalising flows. Compared to other flowbased approaches, NHF enjoys higher computational efficiency since it avoids the expensive step of calculating the trace of Jacobians. <ref type="bibr" target="#b133">[134]</ref> designs the computation graph of the proposed neural network following the Hamiltonian dynamics with control to incorporate the corresponding inductive bias. It further proposes a parameterization that can enforce the Hamiltonian mechanics with coordinates embedded in a high-dimensional space or velocity data instead of momentum.</p><p>Symmetry Table <ref type="table" target="#tab_0">1</ref> in Sec 3.2 has shown the connection between some widely used neural network architectures and the corresponding symmetry groups. Here we introduce methods incorporating other types of symmetry groups into neural network architectures. Based on the representations of symmetry groups adopted, all the methods we introduce can be categorized to methods using (1) invariant treatment of coordinates (2) irreducible representations or (3) regular representations.</p><p>Invariant Treatment of Coordinates Depending on the symmetry group, spatial coordinates should be properly processed instead of being used as raw inputs. <ref type="bibr" target="#b154">[155]</ref> develops equivariant message passing to E(3) via letting messages passed among nodes only depend on distances, which follows the property that E(3) preserves distances between nodes. The same technique is also used in <ref type="bibr" target="#b115">[116]</ref>. <ref type="bibr" target="#b155">[156]</ref> propose a set of transformation invariant and equivariant GNN models by tweaking the definition of an adjacency matrix named isometric adjacency matrix, which can be viewed as a weighted adjacency matrix for each direction and reflects spatial information.</p><p>Irreducible Representations All elements of a roto-translation group can be transformed into an irreducible form: a vector that is rotated by a block diagonal matrix. The full set of equivariant mappings for some symmetry group can be solved with equivariance constraint over convolution kernels. The solutions form a linear combination of equivariant basis matrices, which can be used for equivariant convolutions. <ref type="bibr" target="#b156">[157]</ref> gives a general solution of the kernel space constraint for arbitrary representations of the Euclidean group E(2) and its subgroups, which forms a wide range of equivariant network architectures. <ref type="bibr" target="#b157">[158]</ref> develops convolution filters locally equivariant to 3D rotations, translations, and permutations, which are built from spherical harmonics. <ref type="bibr" target="#b158">[159]</ref> further enhances <ref type="bibr" target="#b157">[158]</ref> with the self-attention mechanism.</p><p>Regular Representations Regular representation approaches store copies of latent feature embeddings for all elements of a symmetry group. To mitigate this computational burden, some recent works such as <ref type="bibr" target="#b159">[160,</ref><ref type="bibr" target="#b160">161]</ref> use Lie groups as the tool for rapid prototyping across various symmetry groups. Only the exponential and logarithm maps are required for incorporating equivariance to a new symmetry group.</p><p>Numerical Methods Sec 3.3 introduces several numerical methods for solving PDEs. In this paragraph, we introduce some recent works integrating numerical solutions for each method.</p><p>Finite Difference Method <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b140">141]</ref> propose learnable differential operators by learning convolution kernels to approximate unknown nonlinear responses in PDEs. All kernels are properly constrained by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters, which originates from wavelet theory. These constraints ensure both the model's ability to identify PDEs and its expressivity. <ref type="bibr" target="#b163">[164]</ref> proposes an efficient convolution kernel on unstructured grids of spherical signals using parameterized finite difference operators. <ref type="bibr" target="#b122">[123]</ref> leverages differences of sparsely available data from physical systems via the spatial difference layer (SDL). SDL is inspired by finite difference operators on graph and triangulated mesh and replaces fixed parameters in these operators with the output of a GNN capturing spatial information of the input data.</p><p>Finite Volume Method <ref type="bibr" target="#b43">[43]</ref> adopts CNNs to generate coefficients for approximating spatial derivatives, followed by the standard finite volume method. CNNs are optimized end-to-end to best satisfy the equations on low resolution grids, and produce accurate numerical results: it can be integrated at resolutions 4-8x coarser than is possible with standard finite volume methods. <ref type="bibr" target="#b141">[142]</ref> creates new PDE solvers based on the WENO scheme <ref type="bibr" target="#b170">[171]</ref> via generating its coefficients from a learned policy network trained with reinforcement learning.</p><p>Finite Element Method <ref type="bibr" target="#b164">[165]</ref> mimics the behavior of finite element analysis: it assigns nodes of a GNN to selected spatial locations and uses message passing on the graph to model the relationship between an initial function and a resulting function defined in the same space. Both the locations of nodes and their connectivity can be optimized to focus on the most important parts of the space. <ref type="bibr" target="#b142">[143]</ref> proposes a two-stage optimization framework for PDE-constrained optimization problem. At the first stage, the framework obtains a surrogate model to prediction solutions of finite element method directly from control parameters. At the second stage, the framework performs gradient-based PDEconstrained optimization. <ref type="bibr" target="#b165">[166]</ref> introduces convolution operators on unstructured point clouds based on Generalized Moving Least Squares (GMLS), which is a non-parametric technique in finite element method <ref type="bibr" target="#b171">[172]</ref> for estimating linear bounded functionals from scattered data.</p><p>Koopman Theory <ref type="bibr" target="#b61">[61]</ref> first combines deep learning models with the Koopman operator. It utilizes the power of deep learning to identify nonlinear coordinates on which the dynamics are globally linear using the Koopman operator. The resulting method benefits from both the power and generality of deep learning models and the physical interpretability of Koopman embeddings. <ref type="bibr" target="#b167">[168]</ref> proposes minimization of the residula sum of squares of linear leastsquares regression to estimate the encoders and decoders that maps data into the Koopman invariant subspaces where the linear regression fits well. <ref type="bibr" target="#b59">[59]</ref> extends deep learning based Koopman operators to scenarios with multiple objects. It adopts GNNs to encode object-centric states and uses a block-wise linear state transition matrix (Koopman matrix) to enforce the shared structure among objects. <ref type="bibr" target="#b60">[60]</ref> incorporates the consistency by penalizing the consistency mismatch of forward and backward Koopman matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Fusion of Deep Learning and Physics-Based Modules</head><p>In addition to previously introduced methods that mimic the behaviour of physics based solutions in the computation graph design of neural network layers, deep learning and physics-based methods can also be fused in a higher level: modules constructed with standard neural network blocks and physics rules work interactively but only expose input/output to each other.</p><p>[149] adopts a graphical model derived from equations of motion in a physics model to predict the next future state while adding it with the predicted residual part from a GNN module. <ref type="bibr" target="#b166">[167]</ref> presents HybridNet, a framework combining data-driven deep learning and model-driven computation for reliable spatiotemporal evolution prediction. The deep learning part, Convolutional LSTM (ConvLSTM) works as the backbone to predict the evolution of external input to the system. The model-driven part, Cellular Neural Network (CeNN), transforms numerical computation in PDE solvers and is able to infer unknown physical parameters. <ref type="bibr" target="#b34">[34]</ref> proposes a hybrid approach for fluid flow prediction containing two components: one GNN-based module operating directly on the original fine-grained non-uniform mesh used in CFD, and one CFD solver operating on a much coarser resolution. The output of the CFD solver is upsampled to the fine-grained mesh and then concatenated to the hidden embeddings from GNN layers. The hybrid model generalizes better than pure GNN-based approaches and is still faster than directly running CFD simulation on the original mesh. <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b103">104]</ref> uses constrained convolution kernels to extract approximated spatial derivatives as features, which are fed as the input to the following neural network layers that capture unknown dynamics and give the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Physics-Informed Optimization</head><p>Prior physics knowledge can also be integrated into the optimization process in the form of loss functions directly derived from task-specific knowledge or regularizations from physics principles. The integration of physics knowledge in optimization targets reshapes the optimization space and encourages the training process to converge to physical plausible solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Task-Specific Knowledge Based Loss Terms</head><p>A variety of physics knowledge can be described in the form of PDEs, which provides connections between the spatial and temporal derivatives as well as constraints of values of interest on boundaries. <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b149">150]</ref> adopts multi-layer perceptrons to directly model the mapping from input spatial and temporal coordinates to the value of PDE solutions. Instead of only minimizing the prediction error between the output and solutions from numerical methods, the series of works adds loss terms enforcing the equation structure, which penalizes the violation of PDEs using derivatives from auto-differentiation of the neural network solution. <ref type="bibr" target="#b23">[23]</ref> optimizes the model for super-resolution of turbulent flows by minimizing a weighted combination of two losses: one is the norm of the difference between predictions and ground truth values, the other is the norm of residues of the governing PDEs. <ref type="bibr" target="#b150">[151]</ref> predicts the traffic flow with a neural network taking time and coordinates as input, and constructs physical discrepancy loss terms with an existing second-order traffic model. <ref type="bibr" target="#b147">[148]</ref> trains the proposed lake temperature prediction model with generalized loss function to include the physical consistency-based penalty, which encourages the consistency between lake energy and energy fluxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Regularization</head><p>[133] adopts a deep neural network to predict the unknown disturbance forces in the controller of drones. To guarantee the system's stability, the authors first derive the overall stability and robustness requirement indicating constraints on the Lipschitz constant, then minimizes its upper bound -the spectral norm of weights in each layer -together with the prediction error in the optimization. <ref type="bibr" target="#b134">[135,</ref><ref type="bibr" target="#b135">136]</ref> both augment purely physics laws/rule-based prediction models with learnable control signals to mitigate the approximation errors of prior knowledge. While they differ in terms of processes for solving PDEs, both have constraints minimizing the norm of control signals, which originates from the least action principle <ref type="bibr" target="#b172">[173]</ref>.</p><p>5 Challenges and Future Directions Existing works require expertise of the domain-specific knowledge of tasks to incorporate the most appropriate physics knowledge. While this serves the purpose of leveraging domain-knowledge to mitigate the deficiencies of pure datadriven methods, it lacks the flexibility of identifying the correct physics knowledge depending on the task. For example, <ref type="bibr" target="#b23">[23]</ref> directly uses the governing equation of the Rayleigh-Benard instability problem as the prior knowledge for turbulence super-resolution, while <ref type="bibr" target="#b24">[24]</ref> chooses the derived Hybrid RANS(Reynolds-averaged Navier-Stokes)-LES(Large Eddy Simulation) Coupling method for turbulence prediction. Although both super-resolution and prediction tasks are defined on the same turbulence system governed by the same physics laws, choosing the form of physics laws (the original form/the derived approximation form) to incorporate is heuristic.</p><p>Research Direction 1: Automatic Identification of Proper Physics Knowledge To Incorporate A promising research direction is to reach the middle ground between the domain-specific knowledge and the pure data-driven way.</p><p>Here we discuss some potential approaches to realizing it.</p><p>Neural Architecture Search (NAS) The development of NAS allows the automatic design of neural network architectures and NAS methods have outperformed manually designed architectures on many tasks including image classification and semantic segmentation <ref type="bibr" target="#b173">[174]</ref>. By restricting the available physics knowledge within a pre-selected search space and enabling the model to discover the optimal knowledge or combination of knowledge using NAS techniques, the resulting architecture can reach the balance between exploiting prior knowledge and adapting to observed data. <ref type="bibr" target="#b174">[175]</ref> presents a neural block dynamics design space of neural network components that encompasses various state-space models as the search space for NAS, and models given by NAS in such a space achieve highly accuracy with physically consistent results.</p><p>Automatic Modularization of Network Architectures Modularization of network architectures plays a key part in physics-informed network architecture design. For example, DeLaN <ref type="bibr" target="#b116">[117]</ref> and HNN <ref type="bibr" target="#b108">[109]</ref> contain modules estimating the Lagrangian/Hamiltonian function of the system and following modules deriving the prediction. <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b122">123]</ref> separate modules approximating spatial and temporal derivatives in the network. Modularity provides better generalization ability <ref type="bibr" target="#b114">[115]</ref>, and some modules can further be trained in multiple tasks and benefit the overall performance on all tasks <ref type="bibr" target="#b175">[176,</ref><ref type="bibr" target="#b162">163]</ref>. However, existing works still require a certain selection of physics knowledge to guide the modularization. Some recent works have started exploring discovering functional modules automatically during the training process. For example, <ref type="bibr" target="#b176">[177]</ref> divides models into reusable modules and task-specific modules by estimating the variance of parameters across tasks. <ref type="bibr" target="#b177">[178]</ref> lets multiple groups of recurrent cells compete with each other so that they are only updated at time steps where they are most relevant. This enables the model to learn modular structures and leads to improved generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Challenge 2: Lack of Benchmarks and Evaluations of PIML Methods</head><p>Comprehensive benchmarks have shown as great boosters for the development of corresponding research areas. Examples include ImageNet Large Scale Visual Recognition Challenge(ILSVRC) <ref type="bibr" target="#b178">[179]</ref> and Common Objects in Context(COCO) <ref type="bibr" target="#b179">[180]</ref> from computer vision, Workshop on Statistical Machine Translation (WMT) <ref type="bibr" target="#b180">[181]</ref> and Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b181">[182]</ref> from natural language processing. However, due to the complexity and heterogeneity of problem settings, PIML still lacks comprehensive benchmarks for evaluating various methods of knowledge integration, which creates barriers in the development of PIML. First, most problems in PIML come from physics or engineering applications, where acquiring the data and formalizing the task can be challenging for researchers without domain knowledge and experience. Second, existing works such as <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b122">123,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b139">140,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b34">34]</ref> heavily rely on heterogeneous domain-specific datasets, which greatly increases the difficulty of fairly comparing different PIML methods.</p><p>Research Direction 2: Comprehensive Benchmarks for PIML Methods Constructing comprehensive benchmarks for PIML is of great need for boosting its development. According to the above discussion, ideal benchmarks</p><p>(1) must provide publicly available and organized datasets, and formulate benchmark tasks as typical machine learning tasks, such as classification and regression; (2) must be general enough to accommodate various PIML methods as well as data-driven and pure physics-based methods. Recent development along this direction includes WeatherBench <ref type="bibr" target="#b26">[26]</ref> and Open Graph Benchmark (OGB) <ref type="bibr" target="#b182">[183]</ref>. WeatherBench provides processed weather data together with clearly defined tasks and evaluation metrics for medium-range weather forecasting. It also presents performance of purely data-driven models and numerical models as baselines. OGB offers data of protein structures and molecule structures, and construct node/link/graph property prediction tasks, where performance of PIML methods and data-driven baselines can be directly compared. Established theories and empirical conclusions of neural network architectures and optimization methods are mostly developed in areas where neural network methods first gain advantages, such as computer vision and natural language processing. However, they may no longer be effective in PIML. The reason is that PIML methods usually involve explicit use of gradients in forwarding processes and objective functions, leading to the existence of highorder derivatives in the backward process, which shapes optimization spaces significantly different from typical deep learning models. For example, <ref type="bibr" target="#b117">[118]</ref> notices that regular parameter initialization methods such as Kaiming <ref type="bibr" target="#b183">[184]</ref> and Xavier <ref type="bibr" target="#b184">[185]</ref> are insufficient since the unusual optimization objective is very nonlinear. <ref type="bibr" target="#b185">[186]</ref> also empirically demonstrates that the widely used Rectified Linear Unit (ReLU) activation is not effective in the physics-informed PINN architecture proposed by <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref>.</p><p>Research Direction 3: Novel Neural Network Designs for PIML The drastic differences in network architecture and objectives between PIML and conventional deep learning tasks signify the importance of novel neural network designs for PIML from both the architecture and the optimization aspects.</p><p>From the architecture perspective, since many PIML methods involve the utilization of gradients from the autodifferentiation of neural networks, designing new architectures/components better preserving gradient information is one promising direction. <ref type="bibr" target="#b186">[187]</ref> leverages periodic activation functions for implicit neural representations and demonstrate that they are ideally suited for representing complex natural signals and their spatial/temporal derivatives.</p><p>From the optimization perspective, multiple objectives (including both the task objective and the physics-informed constraints) may contradict with each other and lead to suboptimal results under vanilla optimization methods. <ref type="bibr" target="#b187">[188]</ref> notices the discrepancy of updating directions between the boundary constraint loss and the approximation loss in PINN <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref>, and proposes the dynamic pulling method (DPM) to align their updating directions, which significantly improves the extrapolation performance of PINNs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5. 1</head><label>1</label><figDesc>Challenge 1: Handcrafted Selection of Physics Knowledge for Incorporation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5. 3</head><label>3</label><figDesc>Challenge 3: Suboptimal Existing Neural Network Architectures and Optimization Methods for PIML</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 [</head><label>1</label><figDesc>137] connects some neural network architectures with their corresponding symmetry groups and domains.</figDesc><table><row><cell>Architecture</cell><cell cols="2">Symmetry group G Domain Ω</cell></row><row><cell cols="2">CNN Spherical CNN [138] Intrinsic Mesh CNN [128, 129] Manifold Grid Sphere / SO(3) GNN Graph Deep Sets [139] Set Transformer Complete Graph LSTM 1D Grid</cell><cell>Translation Rotation SO(3) Isometry Iso(Ω) / Gauge symmetry SO(2) Permutation Σ n Permutation Σ n Permutation Σ n Time warping</cell></row></table></figure>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>In this paper, we provide a thorough and comprehensive survey of existing works in PIML. We summarize them from three aspects: (1) motivations of PIML, (2) physics knowledge in PIML, (3) methods of knowledge integration in PIML. In the end, we discuss existing challenges of PIML and indicate potential future research directions accordingly. We expect the paper can serve as the guide for PIML users to select proper physics knowledge and appropriate integration methods, as well as the guide for PIML researchers to identify existing gaps and promising research directions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Koopman</forename><surname>Theory</surname></persName>
		</author>
		<idno>168][59][60] References</idno>
		<imprint>
			<biblScope unit="volume">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10">October 2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Machine learning methods in the environmental sciences: Neural networks and kernels</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Statistics, data mining, and machine learning in astronomy: a practical Python guide for the analysis of survey data</title>
		<author>
			<persName><forename type="first">Željko</forename><surname>Ivezić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Connolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">T</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Princeton University Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Theory-guided data science: A new paradigm for scientific discovery from data</title>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Karpatne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gowtham</forename><surname>Atluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Faghmous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Auroop</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagiza</forename><surname>Samatova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2318" to="2331" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Machine learning for the geosciences: Challenges and opportunities</title>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Karpatne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imme</forename><surname>Ebert-Uphoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Ravela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Ali Babaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1544" to="1554" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning in fluid dynamics</title>
		<author>
			<persName><forename type="first">Kutz</forename><surname>Nathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fluid Mechanics</title>
		<imprint>
			<biblScope unit="volume">814</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning and process understanding for data-driven earth system science</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Reichstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustau</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjorn</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Carvalhais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">566</biblScope>
			<biblScope unit="issue">7743</biblScope>
			<biblScope unit="page" from="195" to="204" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Successful leveraging of image processing and machine learning in seismic structural interpretation: A review</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Amir Shafiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazeed</forename><surname>Alaudah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Leading Edge</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="451" to="461" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Informed machine learning-a taxonomy and survey of integrating knowledge into learning systems</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Laura Von Rueden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Beckh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raoul</forename><surname>Giesselbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Birgit</forename><surname>Heese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annika</forename><surname>Pfrommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajkumar</forename><surname>Pick</surname></persName>
		</author>
		<author>
			<persName><surname>Ramamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12394</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meshfreeflownet: a physics-constrained deep continuous space-time super-resolution framework</title>
		<author>
			<persName><forename type="first">Soheil</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Tchelepi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mr</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards physics-informed deep learning for turbulent flow prediction</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1457" to="1466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enforcing physical constraints in cnns through differentiable pde layer</title>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weatherbench: a benchmark data set for data-driven weather forecasting</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Rasp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Peter D Dueben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Scher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soukayna</forename><surname>Weyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Mouatadid</surname></persName>
		</author>
		<author>
			<persName><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advances in Modeling Earth Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>e2020MS002203</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving data-driven global weather prediction using deep convolutional neural networks on a cubed sphere</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Weyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Dale R Durran</surname></persName>
		</author>
		<author>
			<persName><surname>Caruana</surname></persName>
		</author>
		<idno>MS002109</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Advances in Modeling Earth Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning for post-processing ensemble weather forecasts</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Grönquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikoli</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dueben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page">20200092</biblScope>
			<date type="published" when="2021">2194. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Physics-informed machine learning: case studies for weather and climate modelling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page">20200093</biblScope>
			<date type="published" when="2021">2194. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8459" to="8468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids</title>
		<author>
			<persName><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Tedrake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lagrangian fluid simulation with continuous convolutions</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Prantl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Thuerey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning mesh-based simulation with graph networks</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining differentiable pde solvers and graph neural networks for fluid flow prediction</title>
		<author>
			<persName><forename type="first">Filipe</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avila</forename><surname>Belbute-Peres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Economon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2402" to="2411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Equations of motion from a data series</title>
		<author>
			<persName><forename type="first">P</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Crutchfield</surname></persName>
		</author>
		<author>
			<persName><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex systems</title>
		<imprint>
			<biblScope unit="page">121</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Equation-free, coarse-grained multiscale computation: Enabling mocroscopic simulators to perform system-level analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ioannis G Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>William Gear</surname></persName>
		</author>
		<author>
			<persName><surname>Hyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Panagiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olof</forename><surname>Kevrekidid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Runborg</surname></persName>
		</author>
		<author>
			<persName><surname>Theodoropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="762" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10561</idno>
		<title level="m">Physics informed deep learning (part i): Datadriven solutions of nonlinear partial differential equations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10566</idno>
		<title level="m">Datadriven discovery of nonlinear partial differential equations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>part ii Physics informed deep learning</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep hidden physics models: Deep learning of nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06637</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural networks trained to solve differential equations learn general representations</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Magill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrick</forename><forename type="middle">W</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Zongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burigede</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03485</idno>
		<title level="m">Neural operator: Graph kernel network for partial differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fourier neural operator for parametric partial differential equations</title>
		<author>
			<persName><forename type="first">Zongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Borislavov Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning data-driven discretizations for partial differential equations</title>
		<author>
			<persName><forename type="first">Yohai</forename><surname>Bar-Sinai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Hickey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Brenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15344" to="15349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers</title>
		<author>
			<persName><forename type="first">Kiwon</forename><surname>Um</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Holl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Downscaling satellite precipitation estimates with multiple linear regression, artificial neural networks, and spline interpolation techniques</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Sharifi</surname></persName>
		</author>
		<author>
			<persName><surname>Saghafian</surname></persName>
		</author>
		<author>
			<persName><surname>Steinacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Geophysical Research: Atmospheres</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="789" to="805" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generating high resolution climate change projections through single image super-resolution</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Vandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Kodra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangram</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Auroop R</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><surname>Deepsd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1663" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Coarse-scale pdes from fine-scale observations via machine learning</title>
		<author>
			<persName><forename type="first">Seungjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Kooshkbaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Spiliotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantinos</forename><forename type="middle">I</forename><surname>Siettos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13141</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Parametrization and generation of geological models with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Shing</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">H</forename><surname>Elsheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01810</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Data-driven components in a model of inner-shelf sorted bedforms: a new hybrid model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Eb Goldstein</surname></persName>
		</author>
		<author>
			<persName><surname>Coco</surname></persName>
		</author>
		<author>
			<persName><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><surname>Mo Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Earth Surface Dynamics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Prognostic validation of a neural network unified physics parameterization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName><surname>Brenowitz</surname></persName>
		</author>
		<author>
			<persName><surname>Christopher S Bretherton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geophysical Research Letters</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6289" to="6298" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Could machine learning break the convection parameterization deadlock?</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Gentine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Pritchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Rasp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gael</forename><surname>Reinaudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galen</forename><surname>Yacalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geophysical Research Letters</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5742" to="5751" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Molecule property prediction based on spatial graph embedding</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shugang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3817" to="3828" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hamiltonian systems and transformation in hilbert space</title>
		<author>
			<persName><surname>Bernard O Koopman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the national academy of sciences of the united states of america</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">315</biblScope>
			<date type="published" when="1931">1931</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dynamic mode decomposition of numerical and experimental data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of fluid mechanics</title>
		<imprint>
			<biblScope unit="volume">656</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A data-driven approximation of the koopman operator: Extending dynamic mode decomposition</title>
		<author>
			<persName><forename type="first">Ioannis</forename><forename type="middle">G</forename><surname>Matthew O Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarence</forename><forename type="middle">W</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><surname>Rowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonlinear Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1307" to="1346" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A kernel-based method for data-driven koopman spectral analysis</title>
		<author>
			<persName><forename type="first">Clarence</forename><forename type="middle">W</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Dynamics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="265" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning compositional koopman operators for model-based control</title>
		<author>
			<persName><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Forecasting sequential data using consistent koopman autoencoders</title>
		<author>
			<persName><forename type="first">Omri</forename><surname>Azencot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Benjamin Erichson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="475" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep learning for universal linear embeddings of nonlinear dynamics</title>
		<author>
			<persName><forename type="first">Bethany</forename><surname>Lusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">AG Robins, and CC Pain. A reduced order model for turbulent flows in the urban environment using machine learning</title>
		<author>
			<persName><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mottet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Im Navon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Matar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Building and Environment</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="323" to="337" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">A deep learning based approach to reduced order modeling for turbulent flow control using lstm neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><surname>Gaitonde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09269</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">688</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Causal inference in statistics, social, and biomedical sciences</title>
		<author>
			<persName><forename type="first">W</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Investigating causal relations by econometric models and cross-spectral methods. Econometrica</title>
		<author>
			<persName><forename type="first">Clive Wj</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Causal network reconstruction from time series: From theoretical assumptions to practical estimation</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">75310</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Detecting and quantifying causal associations in large nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlene</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4996</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Inferring causation from time series in earth system sciences</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bathiany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Bollt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustau</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dim</forename><surname>Coumou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Deyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlene</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">D</forename><surname>Mahecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Muñoz-Marí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Causal discovery with attention-based convolutional neural networks</title>
		<author>
			<persName><forename type="first">Meike</forename><surname>Nauta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Bucur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christin</forename><surname>Seifert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning and Knowledge Extraction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="312" to="340" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dynotears: Structure learning from time-series data</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Roxana</forename><surname>Pamfil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nisara</forename><surname>Sriwattanaworachai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shaan</forename><surname>Desai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philip</forename><surname>Pilgerstorfer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Konstantinos</forename><surname>Georgatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Beaumont</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1595" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9472" to="9483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Hernán</surname></persName>
		</author>
		<author>
			<persName><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Causal inference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Marginal structural models and causal inference in epidemiology</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>James M Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babette</forename><surname>Angel Hernan</surname></persName>
		</author>
		<author>
			<persName><surname>Brumback</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Estimation of the causal effects of time-varying exposures</title>
		<author>
			<persName><forename type="first">Garrett</forename><surname>Fitzmaurice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Davidian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geert</forename><surname>Verbeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geert</forename><surname>Molenberghs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Longitudinal Data Analysis</title>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="567" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Forecasting treatment responses over time using recurrent marginal structural networks</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="7483" to="7493" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Reliable decision support using counterfactual models</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Schulam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchi</forename><surname>Saria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1697" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Treatment-response models for counterfactual reasoning with continuous-time, continuous-valued interventions</title>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Soleimani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adarsh</forename><surname>Subbaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchi</forename><surname>Saria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02038</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">On measurement bias in causal inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1203.3504</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Measurement bias and effect restoration in causal inference</title>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Kuroki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="437" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Time series deconfounder: Estimating treatment effects over time in the presence of hidden confounders</title>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Bica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Sequential deconfounding for causal inference with unobserved confounders</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Hatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Feuerriegel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09323</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deconfounding temporal autoencoder: estimating treatment effects over time using noisy proxies</title>
		<author>
			<persName><forename type="first">Milan</forename><surname>Kuzmanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Hatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Feuerriegel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Health</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="143" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effects with time-varying confounders</title>
		<author>
			<persName><forename type="first">Ruoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changchang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deconfounding with networked observational data in a dynamic environment</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruocheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="166" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michael B Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00341</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Galileo: Perceiving physical object properties by integrating a physics engine with deep learning</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilker</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning neural network policies with guided policy search under unknown dynamics</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Multi-view reinforcement learning</title>
		<author>
			<persName><forename type="first">Minne</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitham</forename><surname>Bou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Wahlström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Thomas B Schön</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deisenroth</forename><surname>Peter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02251</idno>
		<title level="m">From pixels to torques: Policy learning with deep dynamical models</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Embed to control: A locally linear latent dynamics model for control from raw images</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Watter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joschka</forename><surname>Boedecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Opponent modeling in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1804" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faiz</forename><surname>Punakkath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08087</idno>
		<title level="m">A regularized opponent model with maximum entropy objective</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Clevrer: Collision events for video representation and reasoning</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Learning physical object properties from unlabeled videos</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Causal discovery in physical systems from videos</title>
		<author>
			<persName><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9180" to="9192" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Cophy: Counterfactual learning of physical dynamics</title>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName><forename type="first">L</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">I</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungryull</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3560" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Disentangling physical dynamics from unknown factors for unsupervised video prediction</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11474" to="11484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Interpretable intuitive physics model</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Pip: Physical interaction prediction via mental imagery with span selection</title>
		<author>
			<persName><forename type="first">Jiafei</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samson</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheston</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04683</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Filteredcophy: Unsupervised learning of counterfactual physics in pixel space</title>
		<author>
			<persName><forename type="first">Janny</forename><surname>Steeven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madiha</forename><surname>Nadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Hamiltonian neural networks</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misko</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15353" to="15363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Generative attention networks for multi-agent behavioral modeling</title>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7195" to="7202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and control</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">What can neural networks reason about?</title>
		<author>
			<persName><forename type="first">Keylu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">P-J</forename><surname>Kt Schütt</surname></persName>
		</author>
		<author>
			<persName><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Conference on Neural Information Processing Systems (NIPS 2017)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Deep lagrangian networks: Using physics as model prior for deep learning</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Lagrangian neural networks</title>
		<author>
			<persName><forename type="first">Miles</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Spergel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirley</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Datadriven traffic forecasting</title>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR &apos;18)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieqi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wancai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Connecting the dots: Multivariate time series forecasting with graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="753" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Advectivenet: An eulerian-lagrangian fluidic reservoir for point cloud processing</title>
		<author>
			<persName><forename type="first">Xingzhe</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Lu Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Physics-aware difference graph networks for sparsely-observed dynamics</title>
		<author>
			<persName><forename type="first">Sungyong</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuizheng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Learning continuous-time {pde}s from sparse data with graph neural networks</title>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Iakovlev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Lähdesmäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Frédérick Gusset, and Nathanaël Perraudin</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martino</forename><surname>Milani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Deepsphere: a graph-based spherical cnn</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Faust: Dataset and evaluation for 3d mesh registration</title>
		<author>
			<persName><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3794" to="3801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Anisotropic diffusion descriptors</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="431" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Gauge equivariant convolutional networks and the icosahedral cnn</title>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkay</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Gauge equivariant mesh cnns: Anisotropic convolutions on geometric graphs</title>
		<author>
			<persName><forename type="first">Pim</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Neural lander: Stable drone landing control using learned dynamics</title>
		<author>
			<persName><forename type="first">Guanya</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xichen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon-Jo</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9784" to="9790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Symplectic ode-net: Learning hamiltonian dynamics with control</title>
		<author>
			<persName><forename type="first">Yaofeng</forename><surname>Desmond Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biswadip</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Learning to control pdes with differentiable physics</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Holl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Thuerey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Augmenting physical models with deep networks for complex dynamics forecasting</title>
		<author>
			<persName><surname>Yuan Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><surname>Guen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dona</forename><surname>Jérémie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>De Bezenac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thome</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Spherical CNNs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<title level="m">Deep sets. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Pde-net: Learning pdes from data</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Pde-net 2.0: Learning pdes from data with a numeric-symbolic hybrid deep network</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">399</biblScope>
			<biblScope unit="page">108925</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Learning to discretize: solving 1d scalar conservation laws via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11079</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Amortized finite element analysis for fast pde-constrained optimization</title>
		<author>
			<persName><forename type="first">Tianju</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sigrid</forename><surname>Adriaenssens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10638" to="10647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Learning mesh-based simulation with graph networks</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Training deep networks with synthetic data: Bridging the reality gap by domain randomization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aayush</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Brophy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="969" to="977" />
		</imprint>
	</monogr>
	<note>Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Using simulation and domain adaptation to improve efficiency of deep robotic grasping</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4243" to="4250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Driving policy transfer via modularity and abstraction</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Physics-guided machine learning for scientific discovery: An application in simulating lake temperature profiles</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Willard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Karpatne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">S</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">A</forename><surname>Zwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM/IMS Transactions on Data Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Combining generative and discriminative models for hybrid inference</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13825" to="13835" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Physics-informed deep learning for traffic state estimation: A hybrid paradigm informed by second-order traffic models</title>
		<author>
			<persName><forename type="first">Rongye</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaobin</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="540" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Enforcing deterministic constraints on generative adversarial networks for emulating physical systems</title>
		<author>
			<persName><forename type="first">Zeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Long</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06671</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems</title>
		<author>
			<persName><forename type="first">Jin-Long</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragos</forename><surname>Chirila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">406</biblScope>
			<biblScope unit="page">109209</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Hamiltonian generative networks</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Racanière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">E (n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Isometric transformation invariant and equivariant graph convolutional networks</title>
		<author>
			<persName><forename type="first">Masanobu</forename><surname>Horie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiaki</forename><surname>Hishinuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Mitsume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">General e (2)-equivariant steerable cnns</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Cesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14334" to="14345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Se (3)-transformers: 3d roto-translation equivariant attention networks</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3165" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Lietransformer: Equivariant self-attention for lie groups</title>
		<author>
			<persName><forename type="first">Charline</forename><surname>Michael J Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheheryar</forename><surname>Le Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilien</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4533" to="4543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Adversarial multi-task learning enhanced physics-informed neural networks for solving partial differential equations</title>
		<author>
			<persName><forename type="first">Pongpisit</forename><surname>Thanasutives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Numao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14320</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Physics-aware spatiotemporal modules with auxiliary tasks for meta-learning</title>
		<author>
			<persName><forename type="first">Sungyong</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuizheng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirisha</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Spherical CNNs on unstructured grids</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Chiyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Graph element networks: adaptive, structured computation and memory</title>
		<author>
			<persName><forename type="first">Ferran</forename><surname>Alet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adarsh</forename><surname>Keshav Jeewajee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Bauza</forename><surname>Villalonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Lozano-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="212" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Trask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><forename type="middle">G</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Atzberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05371</idno>
		<title level="m">Gmls-nets: A framework for learning from unstructured data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Hybridnet: integrating model-based and data-driven learning to predict evolution of dynamical systems</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyuan</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Learning koopman invariant subspaces for dynamic mode decomposition</title>
		<author>
			<persName><forename type="first">Naoya</forename><surname>Takeishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshinobu</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takehisa</forename><surname>Yairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Propagation networks for model-based control under partial observation</title>
		<author>
			<persName><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1205" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Essentially non-oscillatory and weighted essentially non-oscillatory schemes for hyperbolic conservation laws</title>
		<author>
			<persName><forename type="first">Chi-Wang</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced numerical approximation of nonlinear hyperbolic equations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="325" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Mls (moving least square)-based finite elements for threedimensional nonmatching meshes and adaptive mesh refinement</title>
		<author>
			<persName><forename type="first">Jae Hyuk</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyoung</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Sam</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods in applied mechanics and engineering</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="2216" to="2228" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">The principle of least action in quantum mechanics</title>
		<author>
			<persName><surname>Richard P Feynman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Feynman&apos;s Thesis-A New Approach To Quantum Theory</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1997" to="2017" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Automating discovery of physics-informed neural state space models via learning and evolution</title>
		<author>
			<persName><forename type="first">Elliott</forename><surname>Skomski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ján</forename><surname>Drgoňa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Tuor</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Learning for Dynamics and Control</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="980" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Neural relational inference with fast modular meta-learning</title>
		<author>
			<persName><forename type="first">Ferran</forename><surname>Alet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Lozano-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaelbling</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11804" to="11815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Modular meta-learning with shrinkage</title>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abram</forename><forename type="middle">L</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feryal</forename><surname>Behbahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2858" to="2869" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shagun</forename><surname>Sodhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Recurrent independent mechanisms</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Adaptive activation functions accelerate convergence in deep and physics-informed neural networks</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Ameya D Jagtap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Em</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="page">109136</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Vincent Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Dpm: A novel training method for physics-informed neural networks in extrapolation</title>
		<author>
			<persName><forename type="first">Jungeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kookjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongeun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yon</forename><surname>Sheo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noseong</forename><surname>Jhin</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8146" to="8154" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
