<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine learning and deep learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christian</forename><surname>Janiesch</surname></persName>
							<email>christian.janiesch@uni-wuerzburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Business Management &amp; Economics</orgName>
								<orgName type="institution">University of Würzburg</orgName>
								<address>
									<addrLine>Sanderring 2</addrLine>
									<postCode>97070</postCode>
									<settlement>Würzburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Zschech</surname></persName>
							<email>patrick.zschech@fau.de</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Systems</orgName>
								<orgName type="institution">Friedrich-Alexander University Erlangen-Nürnberg</orgName>
								<address>
									<addrLine>Lange Gasse 20</addrLine>
									<postCode>90403</postCode>
									<settlement>Nürnberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Heinrich</surname></persName>
							<email>kai.heinrich@ovgu.de</email>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Economics and Management</orgName>
								<orgName type="institution">Otto-von-Guericke-Universität Magdeburg</orgName>
								<address>
									<addrLine>Universitätsplatz 2</addrLine>
									<postCode>39106</postCode>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Machine learning and deep learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">63B6C8E9FCF8BB3E7B0CB6B81BC6E78B</idno>
					<idno type="DOI">10.1007/s12525-021-00475-2</idno>
					<note type="submission">Received: 7 October 2020 / Accepted: 19 March 2021 # The Author(s) 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine learning</term>
					<term>Deep learning</term>
					<term>Artificial intelligence</term>
					<term>Artificial neural networks</term>
					<term>Analytical model building JEL classification C6</term>
					<term>C8</term>
					<term>M15</term>
					<term>O3</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes the capacity of systems to learn from problem-specific training data to automate the process of analytical model building and solve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications, deep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we summarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical underpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and concepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss the challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business. These naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence servitization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>It is considered easier to explain to a child the nature of what constitutes a sports car as opposed to a normal car by showing him or her examples, rather than trying to formulate explicit rules that define a sports car.</p><p>Similarly, instead of codifying knowledge into computers, machine learning (ML) seeks to automatically learn meaningful relationships and patterns from examples and observations <ref type="bibr" target="#b5">(Bishop 2006)</ref>. Advances in ML have enabled the recent rise of intelligent systems with human-like cognitive capacity that penetrate our business and personal life and shape the networked interactions on electronic markets in every conceivable way, with companies augmenting decisionmaking for productivity, engagement, and employee retention <ref type="bibr" target="#b45">(Shrestha et al. 2021)</ref>, trainable assistant systems adapting to individual user preferences <ref type="bibr" target="#b10">(Fischer et al. 2020)</ref>, and trading agents shaking traditional finance trading markets (Jayanth <ref type="bibr" target="#b22">Balaji et al. 2018)</ref>.</p><p>The capacity of such systems for advanced problem solving, generally termed artificial intelligence (AI), is based on analytical models that generate predictions, rules, answers, recommendations, or similar outcomes. First attempts to build analytical models relied on explicitly programming known relationships, procedures, and decision logic into intelligent systems through handcrafted rules (e.g., expert systems for medical diagnoses) <ref type="bibr" target="#b39">(Russell and Norvig 2021)</ref>. Fueled by the practicability of new programming frameworks, data availability, and the broad access to necessary computing power, analytical models are nowadays increasingly built using what is generally referred to as ML <ref type="bibr">(Brynjolfsson and McAfee 2017;</ref><ref type="bibr" target="#b14">Goodfellow et al. 2016)</ref>. ML relieves the human of the burden to explicate and formalize his or her knowledge into a machine-accessible form and allows to develop intelligent systems more efficiently.</p><p>During the last decades, the field of ML has brought forth a variety of remarkable advancements in sophisticated learning algorithms and efficient pre-processing techniques. One of these advancements was the evolution of artificial neural networks (ANNs) towards increasingly deep neural network architectures with improved learning capabilities summarized as deep learning (DL) <ref type="bibr" target="#b14">(Goodfellow et al. 2016;</ref><ref type="bibr" target="#b26">LeCun et al. 2015)</ref>. For specific applications in closed environments, DL already shows superhuman performance by excelling human capabilities <ref type="bibr" target="#b30">(Madani et al. 2018;</ref><ref type="bibr" target="#b46">Silver et al. 2018)</ref>. However, such benefits also come at a price as there are several challenges to overcome for successfully implementing analytical models in real business settings. These include the suitable choice from manifold implementation options, bias and drift in data, the mitigation of black-box properties, and the reuse of preconfigured models (as a service).</p><p>Beyond its hyped appearance, scholars, as well as professionals, require a solid understanding of the underlying concepts, processes as well as challenges for implementing such technology. Against this background, the goal of this article is to convey a fundamental understanding of ML and DL in the context of electronic markets. In this way, the community can benefit from these technological achievementsbe it for the purpose of examining large and high-dimensional data assets collected in digital ecosystems or for the sake of designing novel intelligent systems for electronic markets. Following recent advances in the field, this article focuses on analytical model building and challenges of implementing intelligent systems based on ML and DL. As we examine the field from a technical perspective, we do not elaborate on the related issues of AI technology adoption, policy, and impact on organizational culture (for further implications cf. e.g. <ref type="bibr" target="#b48">Stone et al. 2016</ref>).</p><p>In the next section, we provide a conceptual distinction between relevant terms and concepts. Subsequently, we shed light on the process of automated analytical model building by highlighting the particularities of ML and DL. Then, we proceed to discuss several induced challenges when implementing intelligent systems within organizations or electronic markets. In doing so, we highlight environmental factors of implementation and application rather than viewing the engineered system itself as the only unit of observation. We summarize the article with a brief conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conceptual distinction</head><p>To provide a fundamental understanding of the field, it is necessary to distinguish several relevant terms and concepts from each other. For this purpose, we first present basic foundations of AI, before we distinguish i) machine learning algorithms, ii) artificial neural networks, and iii) deep neural networks. The hierarchical relationship between those terms is summarized in Venn diagram of Fig. <ref type="figure">1</ref>.</p><p>Broadly defined, AI comprises any technique that enables computers to mimic human behavior and reproduce or excel over human decision-making to solve complex tasks independently or with minimal human intervention <ref type="bibr" target="#b39">(Russell and Norvig 2021)</ref>. As such, it is concerned with a variety of central problems, including knowledge representation, reasoning, learning, planning, perception, and communication, and refers to a variety of tools and methods (e.g., case-based reasoning, rule-based systems, genetic algorithms, fuzzy models, multi-agent systems) <ref type="bibr" target="#b6">(Chen et al. 2008)</ref>. Early AI research focused primarily on hard-coded statements in formal languages, which a computer can then automatically reason about based on logical inference rules. This is also known as the knowledge base approach <ref type="bibr" target="#b14">(Goodfellow et al. 2016)</ref>. However, the paradigm faces several limitations as humans generally struggle to explicate all their tacit knowledge that is required to perform complex tasks <ref type="bibr">(Brynjolfsson and McAfee 2017)</ref>.</p><p>Machine learning overcomes such limitations. Generally speaking, ML means that a computer program's performance improves with experience with respect to some class of tasks and performance measures <ref type="bibr" target="#b23">(Jordan and Mitchell 2015)</ref>. As such, it aims at automating the task of analytical model building to perform cognitive tasks like object detection or natural language translation. This is achieved by applying algorithms that iteratively learn from problem-specific training data, which allows computers to find hidden insights and complex patterns without explicitly being programmed <ref type="bibr" target="#b5">(Bishop 2006)</ref>. Especially in tasks related to high-dimensional data such as classification, regression, and clustering, ML shows good applicability. By learning from previous computations and extracting regularities from massive databases, it can help to produce reliable and repeatable decisions. For this reason, ML algorithms have been successfully applied in many areas, such as fraud detection, credit scoring, next-best offer analysis, speech and image recognition, or natural language processing (NLP).</p><p>Based on the given problem and the available data, we can distinguish three types of ML: supervised learning, unsupervised learning, and reinforcement learning. While many applications in electronic markets use supervised learning <ref type="bibr">(Brynjolfsson and McAfee 2017)</ref>, for example, to forecast stock markets (Jayanth <ref type="bibr" target="#b22">Balaji et al. 2018)</ref>, to understand customer perceptions <ref type="bibr" target="#b37">(Ramaswamy and DeClerck 2018)</ref>, to analyze customer needs <ref type="bibr" target="#b25">(Kühl et al. 2020)</ref>, or to search products <ref type="bibr" target="#b4">(Bastan et al. 2020)</ref>, there are implementations of all types, for example, market-making with reinforcement learning <ref type="bibr" target="#b47">(Spooner et al. 2018)</ref> or unsupervised market segmentation using customer reviews <ref type="bibr" target="#b1">(Ahani et al. 2019)</ref>. See Table <ref type="table">1</ref> for an overview of all three types. Depending on the learning task, the field offers various classes of ML algorithms, each of them coming in multiple specifications and variants, including regressions models, instance-based algorithms, decision trees, Bayesian methods, and ANNs.</p><p>The family of artificial neural networks is of particular interest since their flexible structure allows them to be modified for a wide variety of contexts across all three types of ML.</p><p>Inspired by the principle of information processing in biological systems, ANNs consist of mathematical representations of connected processing units called artificial neurons. Like synapses in a brain, each connection between neurons transmits signals whose strength can be amplified or attenuated by a weight that is continuously adjusted during the learning process. Signals are only processed by subsequent neurons if a certain threshold is exceeded as determined by an activation function. Typically, neurons are organized into networks with different layers. An input layer usually receives the data input (e.g., product images of an online shop), and an output layer produces the ultimate result (e.g., categorization of products). In between, there are zero or more hidden layers that are responsible for learning a non-linear mapping between input and output <ref type="bibr" target="#b5">(Bishop 2006;</ref><ref type="bibr" target="#b14">Goodfellow et al. 2016)</ref>. The number of layers and neurons, among other property choices, such as learning rate or activation function, cannot be learned by the learning algorithm. They constitute a model's hyperparameters and must be set manually or determined by an optimization routine.</p><p>Deep neural networks typically consist of more than one hidden layer, organized in deeply nested network architectures. Furthermore, they usually contain advanced neurons in contrast to simple ANNs. That is, they may use advanced operations (e.g., convolutions) or multiple activations in one neuron rather than using a simple activation function. These characteristics allow deep neural networks to be fed with raw input data and automatically discover a representation that is</p><p>Artificial neural networks Deep neural networks e.g., support vector machine, decision tree, k-nearest neighbors, … e.g., shallow autoencoders, … e.g., convolutional neural networks, recurrent neural networks, … Deep learning Shallow machine learning Machine learning Machine learning algorithms Fig. 1 Venn diagram of machine learning concepts and classes (inspired by Goodfellow et al. 2016, p. 9) Table 1 Overview of types of machine learning Type Description Supervised learning Supervised learning requires a training dataset that covers examples for the input as well as labeled answers or target values for the output. An example could be the prediction of active users subscribed to a market platform in a month's time as output (considered as the target variable or y variable) based on different input characteristics, such as the number of sold products or positive user reviews (often referred to as input features or x variables). The pairs of input and output data in the training set are then used to calibrate the open parameters of the ML model. Once the model has been successfully trained, it can be used to predict the target variable y given new or unseen data points of the input features x.</p><p>Regarding the type of supervised learning, we can further distinguish between regression problems, where a numeric value is predicted (e.g., number of users), and classification problems, where the prediction result is a categorical class affiliation such as "lookers" or "buyers".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised learning</head><p>Unsupervised learning takes place when the learning system is supposed to detect patterns without any pre-existing labels or specifications. Thus, training data only consists of variables x with the goal of finding structural information of interest, such as groups of elements that share common properties (known as clustering) or data representations that are projected from a high-dimensional space into a lower one (known as dimensionality reduction) <ref type="bibr" target="#b5">(Bishop 2006)</ref>. A prominent example of unsupervised learning in electronic markets is applying clustering techniques to group customers or markets into segments for the purpose of a more target-group specific communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinforcement learning</head><p>In a reinforcement learning system, instead of providing input and output pairs, we describe the current state of the system, specify a goal, provide a list of allowable actions and their environmental constraints for their outcomes, and let the ML model experience the process of achieving the goal by itself using the principle of trial and error to maximize a reward. Reinforcement learning models have been applied with great success in closed world environments such as games <ref type="bibr" target="#b46">(Silver et al. 2018</ref>), but they are also relevant for multi-agent systems such as electronic markets <ref type="bibr" target="#b35">(Peters et al. 2013)</ref>.</p><p>needed for the corresponding learning task. This is the networks' core capability, which is commonly known as deep learning. Simple ANNs (e.g., shallow autoencoders) and other ML algorithms (e.g., decision trees) can be subsumed under the term shallow machine learning since they do not provide such functionalities. As there is still no exact demarcation between the two concepts in literature (see also <ref type="bibr" target="#b41">Schmidhuber 2015)</ref>, we use a dashed line in Fig. <ref type="figure">1</ref>. While some shallow ML algorithms are considered inherently interpretable by humans and, thus, white boxes, the decision making of most advanced ML algorithms is per se untraceable unless explained otherwise and, thus, constitutes a black box. DL is particularly useful in domains with large and highdimensional data, which is why deep neural networks outperform shallow ML algorithms for most applications in which text, image, video, speech, and audio data needs to be processed <ref type="bibr" target="#b26">(LeCun et al. 2015)</ref>. However, for low-dimensional data input, especially in cases of limited training data availability, shallow ML can still produce superior results <ref type="bibr" target="#b57">(Zhang and Ling 2018)</ref>, which even tend to be better interpretable than those generated by deep neural networks <ref type="bibr" target="#b38">(Rudin 2019)</ref>. Further, while DL performance can be superhuman, problems that require strong AI capabilities such as literal understanding and intentionality still cannot be solved as pointedly outlined in Searle (1980)'s Chinese room argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Process of analytical model building</head><p>In this section, we provide a framework on the process of analytical model building for explicit programming, shallow ML, and DL as they constitute three distinct concepts to build an analytical model. Due to their importance for electronic markets, we focus the subsequent discussion on the related aspects of data input, feature extraction, model building, and model assessment of shallow ML and DL (cf. Figure <ref type="figure">2</ref>). With explicit programming, feature extraction and model building are performed manually by a human when handcrafting rules to specify the analytical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data input</head><p>Electronic markets have different stakeholder touchpoints, such as websites, apps, and social media platforms. Apart from common numerical data, they generate a vast amount of versatile data, in particular unstructured and non-crosssectional data such as time series, image, and text. This data can be exploited for analytical model building towards better decision support or business automation purposes. However, extracting patterns and relationships by hand would exceed the cognitive capacity of human operators, which is why algorithmic support is indispensable when dealing with large and high-dimensional data.</p><p>Time series data implies a sequential dependency and patterns over time that need to be detected to form forecasts, often resulting in regression problems or trend classification tasks. Typical examples involve forecasting financial markets or predicting process behavior <ref type="bibr" target="#b20">(Heinrich et al. 2021)</ref>. Image data is often encountered in the context of object recognition or object counting with fields of application ranging from crop detection for yield prediction to autonomous driving <ref type="bibr" target="#b16">(Grigorescu et al. 2020)</ref>. Text data is present when analyzing large volumes of documents such as corporate e-mails or social media posts. Example applications are sentiment analysis or machine-based translation and summarization of documents <ref type="bibr" target="#b55">(Young et al. 2018)</ref>.</p><p>Recent advancements in DL allow for processing data of different types in combination, often referred to as crossmodal learning. This is useful in applications where content is subject to multiple forms of representation, such as ecommerce websites where product information is commonly represented by images, brief descriptions, and other complementary text metadata. Once such cross-modal representations are learned, they can be used, for example, to improve retrieval and recommendation tasks or to detect misinformation and fraud <ref type="bibr" target="#b4">(Bastan et al. 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature extraction</head><p>An important step for the automated identification of patterns and relationships from large data assets is the extraction of features that can be exploited for model building. In general, a feature describes a property derived from the raw data input with the purpose of providing a suitable representation. Thus, feature extraction aims to preserve discriminatory information and separate factors of variation relevant to the overall learning task <ref type="bibr" target="#b14">(Goodfellow et al. 2016)</ref>. For example, when classifying the helpfulness of customer reviews of an online-shop, useful feature candidates could be the choice of words, the length of the review, and the syntactical properties of the text.</p><p>Shallow ML heavily relies on such well-defined features, and therefore its performance is dependent on a successful extraction process. Multiple feature extraction techniques have emerged over time that are applicable to different types of data. For example, when analyzing time-series data, it is common to apply techniques to extract time-domain features (e.g., mean, range, skewness) and frequency-domain features (e.g., frequency bands) <ref type="bibr" target="#b15">(Goyal and Pabla 2015)</ref>; for image analysis, suitable approaches include histograms of oriented gradients (HOG) <ref type="bibr" target="#b7">(Dalal and Triggs 2005)</ref>, scale-invariant feature transform (SIFT) <ref type="bibr" target="#b29">(Lowe 2004)</ref>, and the Viola-Jones method <ref type="bibr" target="#b49">(Viola and Jones 2001)</ref>; and in NLP, it is common to use term frequency-inverse document frequency (TF-IDF) vectors <ref type="bibr" target="#b40">(Salton and Buckley 1988)</ref>, part-of-speech (POS) tagging, and word shape features <ref type="bibr" target="#b54">(Wu et al. 2018)</ref>. Manual feature design is a tedious task as it usually requires a lot of domain expertise within an application-specific engineering process. For this reason, it is considered time-consuming, labor-intensive, and inflexible.</p><p>Deep neural networks overcome this limitation of handcrafted feature engineering. Their advanced architecture gives them the capability of automated feature learning to extract discriminative feature representations with minimal human effort. For this reason, DL better copes with largescale, noisy, and unstructured data. The process of feature learning generally proceeds in a hierarchical manner, with high-level abstract features being assembled by simpler ones. Nevertheless, depending on the type of data and the choice of DL architecture, there are different mechanisms of feature learning in conjunction with the step of model building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model building</head><p>During automated model building, the input is used by a learning algorithm to identify patterns and relationships that are relevant for the respective learning task. As described above, shallow ML requires well-designed features for this task. On this basis, each family of learning algorithms applies different mechanisms for analytical model building. For example, when building a classification model, decision tree algorithms exploit the features space by incrementally splitting data records into increasingly homogenous partitions following a hierarchical, tree-like structure. A support vector machine (SVM) seeks to construct a discriminatory hyperplane between data points of different classes where the input data is often projected into a higher-dimensional feature space for better separability. These examples demonstrate that there are different ways of analytical model building, each of them with individual advantages and disadvantages depending on the input data and the derived features <ref type="bibr" target="#b24">(Kotsiantis et al. 2006)</ref>.</p><p>By contrast, DL can directly operate on high-dimensional raw input data to perform the task of model building with its capability of automated feature learning. Therefore, DL architectures are often organized as end-to-end systems combining both aspects in one pipeline. However, DL can also be applied only for extracting a feature representation, which is subsequently fed into other learning subsystems to exploit the strengths of competing ML algorithms, such as decision trees or SVMs.</p><p>Various DL architectures have emerged over time <ref type="bibr" target="#b27">(Leijnen and van Veen 2020;</ref><ref type="bibr" target="#b36">Pouyanfar et al. 2019;</ref><ref type="bibr" target="#b55">Young et al. 2018)</ref>. Although basically every architecture can be used for every task, some architectures are more suited for specific data such as time series or images. Architectural variants are mostly characterized by the types of layers, neural units, and connections they use. Table <ref type="table" target="#tab_3">2</ref> summarizes the five groups of convolutional neural networks (CNNs), recurrent neural networks (RNNs), distributed representations, autoencoders, and generative adversarial neural networks (GANs). They provide promising applications in the field of electronic markets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model assessment</head><p>For the assessment of a model's quality, multiple aspects have to be taken into account, such as performance, computational resources, and interpretability. Performance-based metrics evaluate how well a model satisfies the objective specified by the learning task. In the area of supervised learning, there are well-established guidelines for this purpose. Here, it is common practice to use k-fold cross-validation to prevent a model from overfitting and determine its performance on outof-sample data that was not included in the training samples. Cross-validation provides the opportunity to compare the reliability of ML models by providing multiple out-of-sample data instances that enable comparative statistical testing <ref type="bibr" target="#b13">(García and Herrera 2008)</ref>. Regression models are evaluated by measuring estimation errors such as the root mean square error (RMSE) or the mean absolute percentage error (MAPE),</p><p>Input Input Input Handcrafted model building Output Output Explicit programming Shallow machine learning Deep learning Handcrafted feature engineering Automated model building Feature learning + automated model building Output Data input Feature extraction Model building Model assessment Fig. 2 Process of analytical model building (inspired by Goodfellow et al. 2016, p. 10)</p><p>whereas classification models are assessed by calculating different ratios of correctly and incorrectly predicted instances, such as accuracy, recall, precision, and F1 score. Furthermore, it is common to apply cost-sensitive measures such as average cost per predicted observation, which is helpful in situations where prediction errors are associated with asymmetric cost structures <ref type="bibr" target="#b44">(Shmueli and Koppius 2011)</ref>. That is the case, for example, when analyzing transactions in financial markets, and the costs of failing to detect a fraudulent transaction are remarkably higher than the costs of incorrectly classifying a non-fraudulent transaction.</p><p>To identify a suitable prediction model for a specific task, it is reasonable to compare alternative models of varying complexities, that is, considering competing model classes as well as alternative variants of the same model class. As introduced above, a model's complexity can be characterized by several CNNs are mainly applied for tasks related to computer vision and speech recognition. They are able to address tasks involving datasets with spatial relationships, where the columns and rows are not interchangeable (e.g., image data). Their network architecture comprises a series of stages that allow hierarchical feature learning as determined by the respective modeling task. For example, when considering object recognition in images, the first few layers of the network are responsible for extracting basic features in the form of edges and corners. These are then incrementally aggregated into more complex features in the last few layers resembling the actual objects of interest, such as animals, houses, or cars. Subsequently, the auto-generated features are used for prediction purposes to recognize objects of interest in new images <ref type="bibr" target="#b14">(Goodfellow et al. 2016)</ref>.</p><p>Recurrent neural network (RNN) RNNs are designed explicitly for sequential data structures such as time-series data, event sequences, and natural language. Their architecture offers internal feedback loops and therefore enables sequential pattern learning to model time dependencies by forming a memory. Simple RNN architectures are problematic since they suffer from vanishing gradients, resulting in little or no influence of early memories. More sophisticated architectures, such as long short-term memory (LSTM) networks with advanced attention mechanisms, attend to this problem. RNNs are typically applied for time series forecasting, predicting process behavior <ref type="bibr" target="#b20">(Heinrich et al. 2021)</ref>, and NLP tasks such as sequence transduction and neural machine translation <ref type="bibr" target="#b26">(LeCun et al. 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributed representation</head><p>Distributed representations play an essential role in feature learning and language modeling in NLP tasks, where language entities such as words, phrases, and sentences are projected into numerical representations within a unified semantic space in the form of embeddings. Word embeddings, for example, encode discrete words into dense feature vectors with low dimensionality. Thus, in contrast to classic text representation models, such as one-hot encodings and bag-of-words (BoW), word embeddings overcome the problem of sparse encodings while preserving semantic relationships between words. This means that words, which occur in similar contexts in a corpus, are also closely positioned to each other in the vector space. On this basis, advanced language models can be developed to perform challenging downstream tasks, such as question-answering, sentiment analysis, and named entity recognition <ref type="bibr" target="#b28">(Liu et al. 2020)</ref>. Distributed representations are often applied in combination with RNNs to perform tasks with sequential dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Autoencoder</head><p>Autoencoders work similarly to word embeddings since they provide a dense feature representation of the input data. However, they are not limited to natural language data but can be applied to any type of input. Such architectures usually consist of an encoding stage where the input is compressed into a low-dimensional representation and a decoding stage in which the network tries to reconstruct the original input from the learned features. In this way, the network is forced to keep meaningful information in the latent representation while disregarding irrelevant noise <ref type="bibr" target="#b14">(Goodfellow et al. 2016)</ref>. Autoencoders are commonly applied for unsupervised feature learning and dimensionality reduction in combination with other subsequent learning systems. However, due to their capability of quantifying reconstruction errors, which are assumed to be significantly higher for anomalous samples than for regular instances, they can also be applied for detecting anomalies, such as fraudulent activities in financial markets <ref type="bibr" target="#b33">(Paula et al. 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative adversarial neural network (GAN)</head><p>Generative adversarial neural networks belong to the family of generative models that aim at learning a probability distribution over a set of training data so that the network can randomly generate new data samples with some variation. For this purpose, GANs consist of two competing sub-networks. The first network is a generator network that captures the distribution of the input and generates new examples. The second network is a discriminator network trying to distinguish real examples from artificially generated ones. Both networks are trained together in a non-cooperative zero-sum game where one network's gain is another one's loss until the discriminator can no longer distinguish between both types of samples. On this basis, GANs are likely to revolutionize domains in which continuously new content or novel product configurations are created (e.g., the composition of art and music, design of fashion), or where content is converted from one representation to another (e.g., text to image for product descriptions) <ref type="bibr" target="#b32">(Pan et al. 2019)</ref>. At the same time, however, such approaches also pose severe threats with societal implications when abusing them for malicious purposes. In particular, the generation of "deepfake" content in the form of abusive speeches and misleading news to manipulate public opinions or distort financial markets is concerning <ref type="bibr" target="#b52">(Westerlund 2019)</ref>.</p><p>properties such as the type of learning mechanisms (e.g., shallow ML vs. DL), the number and type of manually generated or self-extracted features, and the number of trainable parameters (e.g., network weights in ANNs). Simpler models usually do not tend to be flexible enough to capture (non-linear) regularities and patterns that are relevant for the learning task.</p><p>Overly complex models, on the other hand, entail a higher risk of overfitting. Furthermore, their reasoning is more difficult to interpret (cf. next section), and they are likely to be computationally more expensive. Computational costs are expressed by memory requirements and the inference time to execute a model on new data. These criteria are particularly important when assessing deep neural networks, where several million model parameters may be processed and stored, which places special demands on hardware resources. Consequently, it is crucial for business settings with limited resources (such as environments that heavily rely on mobile devices) to not only select a model at the sweet spot between underfitting and overfitting. They should also to evaluate a model's complexity concerning further trade-off relationships, such as accuracy vs. memory usage and speed <ref type="bibr" target="#b19">(Heinrich et al. 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Challenges for intelligent systems based on machine learning and deep learning</head><p>Electronic markets are at the dawn of a technology-induced shift towards data-driven insights provided by intelligent systems <ref type="bibr" target="#b43">(Selz 2020)</ref>. Already today, shallow ML and DL are used to build analytical models for them, and further diffusion is foreseeable. For any real-world application, intelligent systems do not only face the task of model building, system specification, and implementation. They are prone to several issues rooted in how ML and DL operate, which constitute challenges relevant to the Information Systems community. They do require not only technical knowledge but also involve human and business aspects that go beyond the system's confinements to consider the circumstances and the ecosystem of application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Managing the triangle of architecture, hyperparameters, and training data</head><p>When building shallow ML and DL models for intelligent systems, there are nearly endless options for algorithms or architectures, hyperparameters, and training data <ref type="bibr" target="#b8">(Duin 1994;</ref><ref type="bibr" target="#b20">Heinrich et al. 2021)</ref>. At the same time, there is a lack of established guidelines on how a model should be built for a specific problem to ensure not only performance and costefficiency but also its robustness and privacy. Moreover, as outlined above, there are often several trade-off relations to be considered in business environments with limited resources, such as prediction quality vs. computational costs. Therefore, the task of analytical model building is the most crucial since it also determines the business success of an intelligent system.</p><p>For example, a model that can perform at 99.9% accuracy but takes too long to put out a classification decision is rendered useless and is equal to a 0%-accuracy model in the context of time-critical applications such as proactive monitoring or quality assurance in smart factories. Further, different implementations can only be accurately compared when varying only one of the three edges of the triangle at a time and reporting the same metrics. Ultimately, one should consider the necessary skills, available tool support, and the required implementation effort to develop and modify a particular DL architecture <ref type="bibr" target="#b51">(Wanner et al. 2020</ref>). Thus, applications with excellent accuracy achieved in a laboratory setting or on a different dataset may not translate into business success when applied in a real-world environment in electronic markets as other factors may outweigh the ML model's theoretical achievements. This implies that researchers should be aware of the situational characteristics of a models' real-world application to develop an efficacious intelligent system. It is needless to say that researchers cannot know all factors a priori, but they should familiarize themselves with the fact that there are several architectural options with different baseline variants, which suit different scenarios, each with their characteristic properties. Furthermore, multiple metrics such as accuracy and F1 score should be reviewed on consistent benchmarking data across models before making a choice for a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Awareness of bias and drift in data</head><p>In terms of automated analytical model building, one needs to be aware of (cognitive) biases that are introduced into any shallow ML or DL model by using human-generated data. These biases will be heavily adopted by the model <ref type="bibr" target="#b11">(Fuchs 2018;</ref><ref type="bibr" target="#b21">Howard et al. 2017)</ref>. That is, the models will exhibit the same (human-)induced tendencies that are present in the data or even amplify them. A cognitive bias is an illogical inference or belief that individuals adopt due to flawed reporting of facts or due to flawed decision heuristics <ref type="bibr" target="#b17">(Haselton et al. 2015)</ref>. While data-introduced bias is not a particularly new concept, it is amplified in the context of ML and DL if training data has not been properly selected or pre-processed, has class imbalances, or when inferences are not reviewed responsibly. Striking examples include Amazon's AI recruiting software that showed discrimination against women or Google's Vision AI that produced starkly different image labels based on skin color.</p><p>Further, the validity of recommendations based on data is prone to concept drift, which describes a scenario, where "the relation between the input data and the target variable changes over time" <ref type="bibr" target="#b12">(Gama et al. 2014)</ref>. That is, ML models for intelligent systems may not produce satisfactory results, when historical data does not describe the present situation adequately anymore, for example due to new competitors entering a market, new production capabilities becoming available, or unprecedented governmental restrictions. Drift does not have to be sudden but can be incremental, gradual, or reoccurring <ref type="bibr" target="#b12">(Gama et al. 2014</ref>) and thus hard to detect. While techniques for automated learning exist that involve using trusted data windows and concept descriptions <ref type="bibr" target="#b53">(Widmer and Kubat 1996)</ref>, automated strategies for discovering and solving business-related problems are a challenge <ref type="bibr" target="#b34">(Pentland et al. 2020)</ref>.</p><p>For applications in electronic markets, considering bias is of high importance as most data points will have human points of contact. These can be as obvious as social media posts or as disguised as omitted variables. Further, poisoning attacks during model retraining can be used to purposefully insert deviating patterns. This entails that training data needs to be carefully reviewed for such human prejudgments. Applications based on this data should be understood as inherently biased rather than as impartial AI. This implies that researchers need to review their datasets and make public any biases they are aware of. Again, it is unrealistic to assume that all bias effects can be explicated in large datasets with high-dimensional data. Nevertheless, to better understand and trust an ML model, it is important to detect and highlight those effects that have or may have an impact on predictions. Lastly, as constant drift can be assumed in any real-world electronic market, a trained model is never finished. Companies must put strategies in place to identify, track, and counter concept drift that impacts the quality of their intelligent system's decisions. Currently, manual checks and periodic model retraining prevail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unpredictability of predictions and the need for explainability</head><p>The complexity of DL models and some shallow ML models such as random forest and SVMs, often referred to as of blackbox nature, makes it nearly impossible to predict how they will perform in a specific context <ref type="bibr" target="#b0">(Adadi and Berrada 2018)</ref>. This also entails that users may not be able to review and understand the recommendations of intelligent systems based on these models. Moreover, this makes it very difficult to prepare for adversarial attacks, which trick and break DL models <ref type="bibr" target="#b18">(Heinrich et al. 2020)</ref>. They can be a threat to highstake applications, for example, in terms of perturbations of street signs for autonomous driving <ref type="bibr" target="#b9">(Eykholt et al. 2018</ref>). Thus, it may become necessary to explain the decision of a black-box model also to ease organizational adoption. Not only do humans prefer simple explanations to trust and adopt a model, but the requirement of explainability may even be enforced by law <ref type="bibr" target="#b31">(Miller 2019)</ref>.</p><p>The field of explainable AI (XAI) deals with the augmentation of existing DL models to produce explanations for output predictions. For image data, this involves highlighting areas of the input image that are responsible for generating a specific output decision <ref type="bibr" target="#b0">(Adadi and Berrada 2018)</ref>. Concerning time series data, methods have been developed to highlight the particular important time steps influencing a forecast <ref type="bibr" target="#b3">(Assaf and Schumann 2019)</ref>. A similar approach can be used for highlighting words in a text that lead to specific classification outputs.</p><p>Thus, applications in electronic markets with different criticality and human interaction requirements should be designed or augmented distinctively to address the respective concerns. Researchers must review the applications in particular of DL models for their criticality and accountability. Possibly, they must choose an explainable white-box model over a more accurate black-box model <ref type="bibr" target="#b38">(Rudin 2019)</ref> or consider XAI augmentations to make the model's predictions more accessible to its users <ref type="bibr" target="#b0">(Adadi and Berrada 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resource limitations and transfer learning</head><p>Lastly, building and training comprehensive analytical models with shallow ML or DL is costly and requires large datasets to avoid a cold start. Fortunately, models do not always have to be trained from scratch. The concept of transfer learning allows models that are trained on general datasets (e.g., large-scale image datasets) to be specialized for specific tasks by using a considerably smaller dataset that is problem-specific <ref type="bibr" target="#b36">(Pouyanfar et al. 2019</ref>). However, using pre-trained models from foreign sources can pose a risk as the models can be subject to biases and adversarial attacks, as introduced above. For example, pre-trained models may not properly reflect certain environmental constraints or contain backdoors by inserting classification triggers, for example, to misclassify medical images <ref type="bibr" target="#b50">(Wang et al. 2020)</ref>. Governmental interventions to redirect or suppress predictions are conceivable as well. Hence, in high-stake situations, the reuse of publicly available analytical models may not be an option. Nevertheless, transfer learning offers a feasible option for small and mediumsized enterprises to deploy intelligent systems or enables large companies to repurpose their own general analytical models for specific applications.</p><p>In the context of transfer learning, new markets and ecosystems of AI as a service (AIaaS) are already emerging. Such marketplaces, for example by Microsoft or Amazon Web Services, offer cloud AI applications, AI platforms, and AI infrastructure. In addition to cloudbased benefits for deployments, they also enable transfer learning from already established models to other applications. That is, they allow customers with limited AI development resources to purchase pre-trained models and integrate them into their own business environments (e.g., NLP models for chatbot applications). New types of vendors can participate in such markets, for example, by offering transfer learning results for highly domain-specific tasks, such as predictive maintenance for complex machines. As outlined above, consumers of servitized DL models in particular need to be aware of the risks their black-box nature poses and establish similarly strict protocols as with human operators for similar decisions. As the market of AIaaS is only emerging, guidelines for responsible transfer learning have yet to be established (e.g., <ref type="bibr" target="#b2">Amorós et al. 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>With this fundamentals article, we provide a broad introduction to ML and DL. Often subsumed as AI technology, both fuel the analytical models underlying contemporary and future intelligent systems. We have conceptualized ML, shallow ML, and DL as well as their algorithms and architectures. Further, we have described the general process of automated analytical model building with its four aspects of data input, feature extraction, model building, and model assessment. Lastly, we contribute to the ongoing diffusion into electronics markets by discussing four fundamental challenges for intelligent systems based on ML and DL in real-world ecosystems.</p><p>Here, in particular, AIaaS constitutes a new and unexplored electronic market and will heavily influence other established service platforms. They will, for example, augment the smartness of so-called smart services by providing new ways to learn from customer data and provide advice or instructions to them without being explicitly programmed to do so. We estimate that much of the upcoming research on electronic markets will be against the backdrop of AIaaS and their ecosystems and devise new applications, roles, and business models for intelligent systems based on DL. Related future research will need to address and factor in the challenges we presented by providing structured methodological guidance to build analytical models, assess data collections and model performance, and make predictions safe and accessible to the user.</p><p>Funding This research and development project is funded by the Bayerische Staatsministerium für Wirtschaft, Landesentwicklung und Energie (StMWi) within the framework concept "Informationsund Kommunikationstechnik" (grant no. DIK0143/02) and managed by the project management agency VDI+VDE Innovation + Technik GmbH..</p><p>Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ref type="url" target="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Overview of deep learning architectures</figDesc><table><row><cell>Architecture</cell><cell>Description</cell></row><row><cell>Convolutional neural network</cell><cell></cell></row><row><cell>(CNN)</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Peeking inside the black-box: A survey on explainable artificial intelligence (XAI)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berrada</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2870052</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2018.2870052" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Market segmentation and travel choice prediction in Spa hotels through TripAdvisor&apos;s online reviews</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nilashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sanzogni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weaven</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhm.2019.01.003</idno>
		<ptr target="https://doi.org/10.1016/j.ijhm.2019.01.003" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Hospitality Management</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="52" to="77" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Amorós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hafiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Tol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00610</idno>
		<ptr target="http://arxiv.org/abs/2003.00610" />
		<title level="m">Gimme that model!: A trusted ML model trading protocol</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explainable deep neural networks for multivariate time series predictions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/932</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/932" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6488" to="6490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-modal fashion product search with transformer-based Embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bastan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop -3rd workshop on Computer Vision for Fashion, Art and Design</title>
		<meeting><address><addrLine>Seattle: Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pattern recognition and machine learning (Information science and statistics)</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Business Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2006">2006. 2017</date>
			<publisher>Springer-Verlag New York, Inc. Brynjolfsson</publisher>
		</imprint>
	</monogr>
	<note>The business of artificial intelligence</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Artificial intelligence techniques: An introduction to their use for modelling environmental systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Jakeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Norton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.matcom.2008.01.028</idno>
		<ptr target="https://doi.org/10.1016/j.matcom.2008.01.028" />
	</analytic>
	<monogr>
		<title level="j">Mathematics and Computers in Simulation</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="379" to="400" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2005.177</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2005.177" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Superlearning and neural network magic</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-8655(94)90052-3</idno>
		<ptr target="https://doi.org/10.1016/0167-8655" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="90052" to="90053" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on deep learning visual classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00175</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00175" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1625" to="1634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A taxonomy and archetypes of smart services for smart living</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janiesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Winkelmann</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12525-019-00384-5</idno>
		<ptr target="https://doi.org/10.1007/s12525-019-00384-5" />
	</analytic>
	<monogr>
		<title level="j">Electronic Markets</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="149" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The dangers of human-like Bias in machine-learning algorithms. Missouri S&amp;T&apos;s Peer to Peer</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on concept drift adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Žliobaitė</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouchachia</surname></persName>
		</author>
		<idno type="DOI">10.1145/2523813</idno>
		<ptr target="https://doi.org/10.1145/2523813" />
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An extension on &quot;statistical comparisons of classifiers over multiple data sets&quot; for all pairwise comparisons</title>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">89</biblScope>
			<biblScope unit="page" from="2677" to="2694" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Condition based maintenance of machine tools-A review</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Pabla</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cirpj.2015.05.004</idno>
		<ptr target="https://doi.org/10.1016/j.cirpj.2015.05.004" />
	</analytic>
	<monogr>
		<title level="j">CIRP Journal of Manufacturing Science and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="24" to="35" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey of deep learning techniques for autonomous driving</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grigorescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Trasnea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cocias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Macesanu</surname></persName>
		</author>
		<idno type="DOI">10.1002/rob.21918</idno>
		<ptr target="https://doi.org/10.1002/rob.21918" />
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="362" to="386" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The evolution of cognitive Bias</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Haselton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nettle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Andrews</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470939376.ch25</idno>
		<ptr target="https://doi.org/10.1002/9780470939376.ch25" />
	</analytic>
	<monogr>
		<title level="m">The handbook of evolutionary psychology</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Buss</surname></persName>
		</editor>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="724" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fool me once, shame on you, fool me twice, shame on me: A taxonomy of attack and defense patterns for AI security</title>
		<author>
			<persName><forename type="first">K</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laurisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zschech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th European Conference on Information Systems (ECIS)</title>
		<meeting>the 28th European Conference on Information Systems (ECIS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Is Bigger Always Better? Lessons Learnt from the Evolution of Deep Learning Architectures for Image Classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janiesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zschech</surname></persName>
		</author>
		<ptr target="https://aisel.aisnet.org/sigdsa2019/20" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Pre-ICIS SIGDSA Symposium</title>
		<meeting>the 2019 Pre-ICIS SIGDSA Symposium</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Process data properties matter: Introducing gated convolutional neural networks (GCNN) and key-value-predict attention networks (KVP) for next event prediction with deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zschech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janiesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dss.2021.113494</idno>
		<ptr target="https://doi.org/10.1016/j.dss.2021.113494" />
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page">113494</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="DOI">10.1109/ARSO.2017.8025197</idno>
		<ptr target="https://doi.org/10.1109/ARSO.2017.8025197" />
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO)</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Applicability of deep learning models for stock Price forecasting an empirical study on BANKEX data</title>
		<author>
			<persName><forename type="first">Jayanth</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harish Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2018.10.340</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2018.10.340" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="947" to="953" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Machine learning: Trends, perspectives, and prospects</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aaa8415</idno>
		<ptr target="https://doi.org/10.1126/science.aaa8415" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="issue">6245</biblScope>
			<biblScope unit="page" from="255" to="260" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Machine learning: A review of classification and combining techniques</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kotsiantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Zaharakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Pintelas</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-007-9052-3</idno>
		<ptr target="https://doi.org/10.1007/s10462-007-9052-3" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="190" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supporting customeroriented marketing with artificial intelligence: Automatically quantifying customer needs from social media</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mühlthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goutier</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12525-019-00351-0</idno>
		<ptr target="https://doi.org/10.1007/s12525-019-00351-0" />
	</analytic>
	<monogr>
		<title level="j">Electronic Markets</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="367" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="https://doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Neural Network Zoo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leijnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Veen</surname></persName>
		</author>
		<idno type="DOI">10.3390/proceedings47010009</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>1 0 . 3 3 9 0 / proceedings47010009</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Representation learning for natural language processing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-15-5573-2</idno>
		<ptr target="https://doi.org/10.1007/978-981-15-5573-2" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant Keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:VISI.0000029664.99615.94</idno>
		<ptr target="https://doi.org/10.1023/B:VISI.0000029664.99615.94" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast and accurate view classification of echocardiograms using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arnaout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mofrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arnaout</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-017-0013-1</idno>
		<idno>41746- 017-0013-1</idno>
		<ptr target="https://doi.org/10.1038/s" />
	</analytic>
	<monogr>
		<title level="j">Npj Digital Medicine</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2018.07.007</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2018.07.007" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recent Progress on generative adversarial networks (GANs): A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2905015</idno>
		<ptr target="https://doi.org/10.1109/ACCESS" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="36322" to="36333" />
			<date type="published" when="2019">2019. 2019.2905015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning anomaly detection as support fraud investigation in Brazilian exports and anti-money laundering</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Paula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ladeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Marzagão</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMLA.2016.0172</idno>
		<ptr target="https://doi.org/10.1109/ICMLA.2016.0172" />
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="954" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The dynamics of drift in digitized processes</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kremser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Haerem</surname></persName>
		</author>
		<idno type="DOI">10.25300/MISQ/2020/14458</idno>
		<ptr target="https://doi.org/10.25300/MISQ/2020/14458" />
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="47" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A reinforcement learning approach to autonomous decision-making in smart electricity markets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ketter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saar-Tsechansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-013-5340-0</idno>
		<ptr target="https://doi.org/10.1007/s10994-013-5340-0" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="39" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A survey on deep learning: Algorithms, techniques, and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pouyanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Iyengar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3234150</idno>
		<ptr target="https://doi.org/10.1145/3234150" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ACM Computing Surveys</publisher>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Customer perception analysis using deep learning and NLP</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Declerck</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2018.10.326</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2018.10.326" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="170" to="178" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-019-0048-x</idno>
		<ptr target="https://doi.org/10.1038/s42256-019-0048-x" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Artificial intelligence: A modern approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Pearson</publisher>
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<idno type="DOI">10.1016/0306-4573(88)90021-0</idno>
		<ptr target="https://doi.org/10.1016/0306-4573" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="90021" to="90021" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2014.09.003</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2014.09.003" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Minds, brains, and programs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Searle</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X00005756</idno>
		<imprint>
			<date type="published" when="0417">1980. 4 1 7 -4 2 4</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">From electronic markets to data driven insights</title>
		<author>
			<persName><forename type="first">D</forename><surname>Selz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12525-019-00393-4</idno>
		<idno>12525- 019-00393-4</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Electronic Markets</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="59" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Shmueli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Koppius</surname></persName>
		</author>
		<idno type="DOI">10.2307/23042796</idno>
		<ptr target="https://doi.org/10.2307/23042796" />
		<title level="m">Predictive analytics in information systems research. Management Information Systems Quarterly</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="553" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Augmenting organizational decision-making with deep learning algorithms: Principles, promises, and challenges</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krogh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbusres.2020.09.068</idno>
		<ptr target="https://doi.org/10.1016/j.jbusres.2020.09.068" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business Research</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="588" to="603" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and go through self-C. Janiesch et al. play</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aar6404</idno>
		<ptr target="https://doi.org/10.1126/science.aar6404" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Market making via reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Spooner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fearnley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Savani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koukorinis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04216v1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent systems</title>
		<meeting>the 17th International Conference on Autonomous Agents and MultiAgent systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="434" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Calo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kalyanakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Saxenian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><surname>Milind Tambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Teller</surname></persName>
		</author>
		<ptr target="https://ai100.stanford.edu/2016-report" />
		<title level="m">Artificial Intelligence and Life i n 2 0 3 0</title>
		<meeting><address><addrLine>Stanford University</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>l intelligence (Report of the 2015-2016 study panel</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2001.990517</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2001.990517" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, 1, I-511-I-518</title>
		<meeting>the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, 1, I-511-I-518</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Backdoor attacks against transfer learning with pre-trained deep learning models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nepal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSC.2020.3000900</idno>
		<ptr target="https://doi.org/10.1109/TSC.2020.3000900" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">How much AI do you require? Decision factors for adopting AI technology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janiesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zschech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Conference on Information Systems (ICIS)</title>
		<meeting>the 41st International Conference on Information Systems (ICIS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The emergence of Deepfake technology: A review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Westerlund</surname></persName>
		</author>
		<idno type="DOI">10.22215/timreview/1282</idno>
		<ptr target="https://doi.org/10.22215/timreview/1282" />
	</analytic>
	<monogr>
		<title level="j">Technology Innovation Management Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="52" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning in the presence of concept drift and hidden contexts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kubat</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00116900</idno>
		<ptr target="https://doi.org/10.1007/BF00116900" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="101" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Evaluating the utility of hand-crafted features in sequence labelling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1310</idno>
		<ptr target="https://doi.org/10.18653/v1/D18-1310" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2850" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Recent trends in deep learning based natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>review article</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<idno type="DOI">10.1109/MCI.2018.2840738</idno>
		<ptr target="https://doi.org/10.1109/MCI.2018.2840738" />
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="55" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A strategy to apply machine learning to small datasets in materials science</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41524-018-0081-z</idno>
		<ptr target="https://doi.org/10.1038/s41524-018-0081-z" />
	</analytic>
	<monogr>
		<title level="j">Computational Materials</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
