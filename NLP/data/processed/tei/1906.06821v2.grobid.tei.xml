<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey of Optimization Methods from a Machine Learning Perspective</title>
				<funder ref="#_WkVaCWZ">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_txB9qVN">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-10-23">23 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
							<email>slsun@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<addrLine>3663 North Zhongshan Road</addrLine>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<addrLine>3663 North Zhongshan Road</addrLine>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zehui</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<addrLine>3663 North Zhongshan Road</addrLine>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<addrLine>3663 North Zhongshan Road</addrLine>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<addrLine>3663 North Zhongshan Road</addrLine>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<addrLine>3663 North Zhongshan Road</addrLine>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Zhao</surname></persName>
							<email>jzhao@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<addrLine>3663 North Zhongshan Road</addrLine>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<addrLine>3663 North Zhongshan Road</addrLine>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey of Optimization Methods from a Machine Learning Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-23">23 Oct 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">4A561495E4760622F640ED5B3D37BE44</idno>
					<idno type="arXiv">arXiv:1906.06821v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine learning</term>
					<term>optimization method</term>
					<term>deep neural network</term>
					<term>reinforcement learning</term>
					<term>approximate Bayesian inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R ECENTLY, machine learning has grown at a remarkable rate, attracting a great number of researchers and practitioners. It has become one of the most popular research directions and plays a significant role in many fields, such as machine translation, speech recognition, image recognition, recommendation system, etc. Optimization is one of the core components of machine learning. The essence of most machine learning algorithms is to build an optimization model and learn the parameters in the objective function from the given data. In the era of immense data, the effectiveness and efficiency of the numerical optimization algorithms dramatically influence the popularization and application of the machine learning models. In order to promote the development of machine learning, a series of effective optimization methods were put forward, which have improved the performance and efficiency of machine learning methods.</p><p>From the perspective of the gradient information in optimization, popular optimization methods can be divided into three categories: first-order optimization methods, which are represented by the widely used stochastic gradient methods; high-order optimization methods, in which Newton's method is a typical example; and heuristic derivative-free optimization methods, in which the coordinate descent method is a representative.</p><p>As the representative of first-order optimization methods, the stochastic gradient descent method <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, as well as its variants, has been widely used in recent years and is evolving at a high speed. However, many users pay little attention to the characteristics or application scope of these methods. They often adopt them as black box optimizers, which may limit the functionality of the optimization methods. In this paper, we comprehensively introduce the fundamental optimization methods. Particularly, we systematically explain their advantages and disadvantages, their application scope, and the characteristics of their parameters. We hope that the targeted introduction will help users to choose the first-order optimization methods more conveniently and make parameter adjustment more reasonable in the learning process.</p><p>Compared with first-order optimization methods, highorder methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> converge at a faster speed in which the curvature information makes the search direction more effective. High-order optimizations attract widespread attention but face more challenges. The difficulty in highorder methods lies in the operation and storage of the inverse matrix of the Hessian matrix. To solve this problem, many variants based on Newton's method have been developed, most of which try to approximate the Hessian matrix through some techniques <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In subsequent studies, the stochastic quasi-Newton method and its variants are introduced to extend highorder methods to large-scale data <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>Derivative-free optimization methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> are mainly used in the case that the derivative of the objective function may not exist or be difficult to calculate. There are two main ideas in derivative-free optimization methods. One is adopting a heuristic search based on empirical rules, and the other is fitting the objective function with samples. Derivativefree optimization methods can also work in conjunction with gradient-based methods.</p><p>Most machine learning problems, once formulated, can be solved as optimization problems. Optimization in the fields of deep neural network, reinforcement learning, meta learning, variational inference and Markov chain Monte Carlo encounters different difficulties and challenges. The optimization methods developed in the specific machine learning fields are different, which can be inspiring to the development of general optimization methods.</p><p>Deep neural networks (DNNs) have shown great success in pattern recognition and machine learning. There are two very popular NNs, i.e., convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13]</ref> and recurrent neural networks (RNNs), which play important roles in various fields of machine learning. CNNs are feedforward neural networks with convolution calculation. CNNs have been successfully used in many fields such as image processing <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, video processing <ref type="bibr" target="#b15">[16]</ref> and natural language processing (NLP) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. RNNs are a kind of sequential model and very active in NLP <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Besides, RNNs are also popular in the fields of image processing <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and video processing <ref type="bibr" target="#b24">[25]</ref>. In the field of constrained optimization, RNNs can achieve excellent results <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. In these works, the parameters of weights in RNNs can be learned by analytical methods, and these methods can find the optimal solution according to the trajectory of the state solution. Stochastic gradient-based algorithms are widely used in deep neural networks <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. However, various problems are emerging when employing stochastic gradient-based algorithms. For example, the learning rate will be oscillating in the later training stage of some adaptive methods <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, which may lead to the problem of non-converging. Thus, further optimization algorithms based on variance reduction were proposed to improve the convergence rate <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Moreover, combining the stochastic gradient descent and the characteristics of its variants is a possible direction to improve the optimization. Especially, switching an adaptive algorithm to the stochastic gradient descent method can improve the accuracy and convergence speed of the algorithm <ref type="bibr" target="#b37">[38]</ref>.</p><p>Reinforcement learning (RL) is a branch of machine learning, for which an agent interacts with the environment by trial-and-error mechanism and learns an optimal policy by maximizing cumulative rewards <ref type="bibr" target="#b38">[39]</ref>. Deep reinforcement learning combines the RL and deep learning techniques, and enables the RL agent to have a good perception of its environment. Recent research has shown that deep learning can be applied to learn a useful representation for reinforcement learning problems <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Stochastic optimization algorithms are commonly used in RL and deep RL models.</p><p>Meta learning <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> has recently become very popular in the field of machine learning. The goal of meta learning is to design a model that can efficiently adapt to the new environment with as few samples as possible. The application of meta learning in supervised learning can solve the few-shot learning problems <ref type="bibr" target="#b46">[47]</ref>. In general, meta learning methods can be summarized into the following three types <ref type="bibr" target="#b47">[48]</ref>: metricbased methods <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, model-based methods <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> and optimization-based methods <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b46">[47]</ref>. We will describe the details of optimization-based meta learning methods in the subsequent sections.</p><p>Variational inference is a useful approximation method which aims to approximate the posterior distributions in Bayesian machine learning. It can be considered as an optimization problem. For example, mean-field variational inference uses coordinate ascent to solve this optimization problem <ref type="bibr" target="#b56">[57]</ref>. As the amount of data increases continuously, it is not friendly to use the traditional optimization method to handle the variational inference. Thus, the stochastic variational inference was proposed, which introduced natural gradients and extended the variational inference to large-scale data <ref type="bibr" target="#b57">[58]</ref>.</p><p>Optimization methods have a significative influence on various fields of machine learning. For example, <ref type="bibr" target="#b4">[5]</ref> proposed the transformer network using Adam optimization <ref type="bibr" target="#b32">[33]</ref>, which is applied to machine translation tasks. <ref type="bibr" target="#b58">[59]</ref> proposed superresolution generative adversarial network for image super resolution, which is also optimized by Adam. <ref type="bibr" target="#b59">[60]</ref> proposed Actor-Critic using trust region optimization to solve the deep reinforcement learning on Atari games as well as the MuJoCo environments.</p><p>The stochastic optimization method can also be applied to Markov chain Monte Carlo (MCMC) sampling to improve efficiency. In this kind of application, stochastic gradient Hamiltonian Monte Carlo (HMC) is a representative method <ref type="bibr" target="#b60">[61]</ref> where the stochastic gradient accelerates the step of gradient update when handling large-scale samples. The noise introduced by the stochastic gradient can be characterized by introducing Gaussian noise and friction terms. Additionally, the deviation caused by HMC discretization can be eliminated by the friction term, and thus the Metropolis-Hasting step can be omitted. The hyper-parameter settings in the HMC will affect the performance of the model. There are some efficient ways to automatically adjust the hyperparameters and improve the performance of the sampler.</p><p>The development of optimization brings a lot of contributions to the progress of machine learning. However, there are still many challenges and open problems for optimization problems in machine learning. 1) How to improve optimization performance with insufficient data in deep neural networks is a tricky problem. If there are not enough samples in the training of deep neural networks, it is prone to cause the problem of high variances and overfitting <ref type="bibr" target="#b61">[62]</ref>. In addition, non-convex optimization has been one of the difficulties in deep neural networks, which makes the optimization tend to get a locally optimal solution rather than the global optimal solution. 2) For sequential models, the samples are often truncated by batches when the sequence is too long, which will cause deviation. How to analyze the deviation of stochastic optimization in this case and correct it is vital. 3) The stochastic variational inference is graceful and practical, and it is probably a good choice to develop methods of applying high-order gradient information to stochastic variational inference. 4) It may be a great idea to introduce the stochastic technique to the conjugate gradient method to obtain an elegant and powerful optimization algorithm. The detailed techniques to make improvements in the stochastic conjugate gradient is an interesting and challenging problem.</p><p>The purpose of this paper is to summarize and analyze classical and modern optimization methods from a machine learning perspective. The remainder of this paper is organized as follows. Section II summarizes the machine learning problems from the perspective of optimization. Section III discusses the classical optimization algorithms and their latest developments in machine learning. Particularly, the recent popular optimization methods including the first and second order optimization algorithms are emphatically introduced.</p><p>Section IV describes the developments and applications of optimization methods in some specific machine learning fields. Section V presents the challenges and open problems in the optimization methods. Finally, we conclude the whole paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MACHINE LEARNING FORMULATED AS OPTIMIZATION</head><p>Almost all machine learning algorithms can be formulated as an optimization problem to find the extremum of an objective function. Building models and constructing reasonable objective functions are the first step in machine learning methods. With the determined objective function, appropriate numerical or analytical optimization methods are usually used to solve the optimization problem.</p><p>According to the modeling purpose and the problem to be solved, machine learning algorithms can be divided into supervised learning, semi-supervised learning, unsupervised learning, and reinforcement learning. Particularly, supervised learning is further divided into the classification problem (e.g., sentence classification <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b62">[63]</ref>, image classification <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, etc.) and regression problem; unsupervised learning is divided into clustering and dimension reduction <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optimization Problems in Supervised Learning</head><p>For supervised learning, the goal is to find an optimal mapping function f (x) to minimize the loss function of the training samples,</p><formula xml:id="formula_0">min θ 1 N N i=1 L(y i , f (x i , θ)),<label>(1)</label></formula><p>where N is the number of training samples, θ is the parameter of the mapping function, x i is the feature vector of the ith samples, y i is the corresponding label, and L is the loss function.</p><p>There are many kinds of loss functions in supervised learning, such as the square of Euclidean distance, crossentropy, contrast loss, hinge loss, information gain and so on. For regression problems, the simplest way is using the square of Euclidean distance as the loss function, that is, minimizing square errors on training samples. But the generalization performance of this kind of empirical loss is not necessarily good. Another typical form is structured risk minimization, whose representative method is the support vector machine. On the objective function, regularization items are usually added to alleviate overfitting, e.g., in terms of L 2 norm,</p><formula xml:id="formula_1">min θ 1 N N i=1 L(y i , f (x i , θ)) + λ θ 2 2 . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where λ is the compromise parameter, which can be determined through cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization Problems in Semi-supervised Learning</head><p>Semi-supervised learning (SSL) is the method between supervised and unsupervised learning, which incorporates labeled data and unlabeled data during the training process.</p><p>It can deal with different tasks including classification tasks <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, regression tasks <ref type="bibr" target="#b71">[72]</ref>, clustering tasks <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref> and dimensionality reduction tasks <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>. There are different kinds of semi-supervised learning methods including selftraining, generative models, semi-supervised support vector machines (S3VM) <ref type="bibr" target="#b76">[77]</ref>, graph-based methods, multi-learning method and others. We take S3VM as an example to introduce the optimization in semi-supervised learning.</p><p>S3VM is a learning model that can deal with binary classification problems and only part of the training set in this problem is labeled. Let D l be labeled data which can be represented as D l = {{x 1 , y 1 }, {x 2 , y 2 }, ..., {x l , y l }}, and D u be unlabeled data which can be represented as D u = {x l+1 , x l+2 , ..., x N } with N = l + u. In order to use the information of unlabeled data, additional constraint on the unlabeled data is added to the original objective of SVM with slack variables ζ i . Specifically, define ǫ j as the misclassification error of the unlabeled instance if its true label is positive and z j as the misclassification error of the unlabeled instance if its true label is negative. The constraint means to make N j=l+1 min(ǫ i , ζ i ) as small as possible. Thus, an S3VM problem can be described as</p><formula xml:id="formula_3">min ω +C   l i=1 ζ i + N j=l+1 min(ǫ i , z j )   , subject to y i (w • x i + b) + ζ i ≥ 1, ζ ≥ 0, i = 1, ..., l, w • x j + b + ǫ j ≥ 1, ǫ ≥ 0, j = l + 1, ..., N, -(w • x j + b) + z j ≥ 1, z j ≥ 0,<label>(3)</label></formula><p>where C is a penalty coefficient. The optimization problem in S3VM is a mixed-integer problem which is difficult to deal with <ref type="bibr" target="#b77">[78]</ref>. There are various methods summarized in <ref type="bibr" target="#b78">[79]</ref> to deal with this problem, such as the branch and bound techniques <ref type="bibr" target="#b79">[80]</ref> and convex relaxation methods <ref type="bibr" target="#b80">[81]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization Problems in Unsupervised Learning</head><p>Clustering algorithms <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b83">[84]</ref> divide a group of samples into multiple clusters ensuring that the differences between the samples in the same cluster are as small as possible, and samples in different clusters are as different as possible. The optimization problem for the k-means clustering algorithm is formulated as minimizing the following loss function:</p><formula xml:id="formula_4">min S K k=1 x∈S k x -µ k 2 2 , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where K is the number of clusters, x is the feature vector of samples, µ k is the center of cluster k, and S k is the sample set of cluster k. The implication of this objective function is to make the sum of variances of all clusters as small as possible.</p><p>The dimensionality reduction algorithm ensures that the original information from data is retained as much as possible after projecting them into the low-dimensional space. Principal component analysis (PCA) <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref> is a typical algorithm of dimensionality reduction methods. The objective of PCA is formulated to minimize the reconstruction error as</p><formula xml:id="formula_6">min N i=1 x i -x i 2 2 where x i = D ′ j=1 z i j e j , D ≫ D ′ ,<label>(5)</label></formula><p>where N represents the number of samples, x i is a Ddimensional vector, x i is the reconstruction of x i . z i = {z i 1 , ..., z i D ′ } is the projection of x i in D ′ -dimensional coordinates. e j is the standard orthogonal basis under D ′dimensional coordinates.</p><p>Another common optimization goal in probabilistic models is to find an optimal probability density function of p(x), which maximizes the logarithmic likelihood function (MLE) of the training samples,</p><formula xml:id="formula_7">max N i=1 ln p(x i ; θ).<label>(6)</label></formula><p>In the framework of Bayesian methods, some prior distributions are often assumed on parameter θ, which also has the effect of alleviating overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization Problems in Reinforcement Learning</head><p>Reinforcement learning <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>, unlike supervised learning and unsupervised learning, aims to find an optimal strategy function, whose output varies with the environment. For a deterministic strategy, the mapping function from state s to action a is the learning target. For an uncertain strategy, the probability of executing each action is the learning target. In each state, the action is determined by a = π(s), where π(s) is the policy function.</p><p>The optimization problem in reinforcement learning can be formulated as maximizing the cumulative return after executing a series of actions which are determined by the policy function, <ref type="bibr" target="#b6">(7)</ref> where V π (s) is the value function of state s under policy π, r is the reward, and γ ∈ [0, 1] is the discount factor.</p><formula xml:id="formula_8">max π V π (s) where V π (s) = E ∞ k=0 γ k r t+k |S t = s ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Optimization for Machine Learning</head><p>Overall, the main steps of machine learning are to build a model hypothesis, define the objective function, and solve the maximum or minimum of the objective function to determine the parameters of the model. In these three vital steps, the first two steps are the modeling problems of machine learning, and the third step is to solve the desired model by optimization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FUNDAMENTAL OPTIMIZATION METHODS AND PROGRESSES</head><p>From the perspective of gradient information, fundamental optimization methods can be divided into first-order optimization methods, high-order optimization methods and derivativefree optimization methods. These methods have a long history and are constantly evolving. They are progressing in many practical applications and have achieved good performance. Besides these fundamental methods, preconditioning is a useful technique for optimization methods. Applying reasonable preconditioning can reduce the number of iterations and obtain better spectral characteristics. These technologies have been widely used in practice. For the convenience of researchers, we summarize the existing common optimization toolkits in a table at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. First-Order Methods</head><p>In the field of machine learning, the most commonly used first-order optimization methods are mainly based on gradient descent. In this section, we introduce some of the representative algorithms along with the development of the gradient descent methods. At the same time, the classical alternating direction method of multipliers and the Frank-Wolfe method in numerical optimization are also introduced.</p><p>1) Gradient Descent: The gradient descent method is the earliest and most common optimization method. The idea of the gradient descent method is that variables update iteratively in the (opposite) direction of the gradients of the objective function. The update is performed to gradually converge to the optimal value of the objective function. The learning rate η determines the step size in each iteration, and thus influences the number of iterations to reach the optimal value <ref type="bibr" target="#b89">[90]</ref>.</p><p>The steepest descent algorithm is a widely known algorithm. The idea is to select an appropriate search direction in each iteration so that the value of the objective function minimizes the fastest. Gradient descent and steepest descent are not the same, because the direction of the negative gradient does not always descend fastest. Gradient descent is an example of using the Euclidean norm in steepest descent <ref type="bibr" target="#b90">[91]</ref>.</p><p>Next, we give the formal expression of gradient descent method. For a linear regression model, we assume that f θ (x) is the function to be learned, L(θ) is the loss function, and θ is the parameter to be optimized. The goal is to minimize the loss function with</p><formula xml:id="formula_9">L(θ) = 1 2N N i=1 (y i -f θ (x i )) 2 ,<label>(8)</label></formula><formula xml:id="formula_10">f θ (x) = D j=1 θ j x j ,<label>(9)</label></formula><p>where N is the number of training samples, D is the number of input features, x i is an independent variable with x i = (x i 1 , ..., x i D ) for i = 1, ..., N and y i is the target output. The gradient descent alternates the following two steps until it converges:</p><p>1) Derive L(θ) for θ j to get the gradient corresponding to each θ j :</p><formula xml:id="formula_11">∂L(θ) ∂θ j = - 1 N N i=1 (y i -f θ (x i ))x i j .<label>(10)</label></formula><p>2) Update each θ j in the negative gradient direction to minimize the risk function:</p><formula xml:id="formula_12">θ ′ j = θ j + η • 1 N N i=1 (y i -f θ (x i ))x i j .<label>(11)</label></formula><p>The gradient descent method is simple to implement. The solution is global optimal when the objective function is convex. It often converges at a slower speed if the variable is closer to the optimal solution, and more careful iterations need to be performed.</p><p>In the above linear regression example, note that all the training data are used in each iteration step, so the gradient descent method is also called the batch gradient descent. If the number of samples is N and the dimension of x is D, the computation complexity for each iteration will be O(N D). In order to mitigate the cost of computation, some parallelization methods were proposed <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b92">[93]</ref>. However, the cost is still hard to accept when dealing with large-scale data. Thus, the stochastic gradient descent method emerges.</p><p>2) Stochastic Gradient Descent: Since the batch gradient descent has high computational complexity in each iteration for large-scale data and does not allow online update, stochastic gradient descent (SGD) was proposed <ref type="bibr" target="#b0">[1]</ref>. The idea of stochastic gradient descent is using one sample randomly to update the gradient per iteration, instead of directly calculating the exact value of the gradient. The stochastic gradient is an unbiased estimate of the real gradient <ref type="bibr" target="#b0">[1]</ref>. The cost of the stochastic gradient descent algorithm is independent of sample numbers and can achieve sublinear convergence speed <ref type="bibr" target="#b36">[37]</ref>. SGD reduces the update time for dealing with large numbers of samples and removes a certain amount of computational redundancy, which significantly accelerates the calculation. In the strong convex problem, SGD can achieve the optimal convergence speed <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Meanwhile, it overcomes the disadvantage of batch gradient descent that cannot be used for online learning.</p><p>The loss function <ref type="bibr" target="#b7">(8)</ref> can be written as the following equation:</p><formula xml:id="formula_13">L(θ) = 1 N N i=1 1 2 (y i -f θ (x i )) 2 = 1 N N i=1</formula><p>cost(θ, (x i , y i )).</p><p>(12) If a random sample i is selected in SGD, the loss function will be L * (θ):</p><formula xml:id="formula_14">L * (θ) = cost(θ, (x i , y i )) = 1 2 (y i -f θ (x i )) 2 . (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>The gradient update in SGD uses the random sample i rather than all samples in each iteration,</p><formula xml:id="formula_16">θ ′ = θ + η(y i -f θ (x i ))x i . (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>Since SGD uses only one sample per iteration, the computation complexity for each iteration is O(D) where D is the number of features. The update rate for each iteration of SGD is much faster than that of batch gradient descent when the number of samples N is large. SGD increases the overall optimization efficiency at the expense of more iterations, but the increased iteration number is insignificant compared with the high computation complexity caused by large numbers of samples. It is possible to use only thousands of samples overall to get the optimal solution even when the sample size is hundreds of thousands. Therefore, compared with batch methods, SGD can effectively reduce the computational complexity and accelerate convergence.</p><p>However, one problem in SGD is that the gradient direction oscillates because of additional noise introduced by random selection, and the search process is blind in the solution space. Unlike batch gradient descent which always moves towards the optimal value along the negative direction of the gradient, the variance of gradients in SGD is large and the movement direction in SGD is biased. So, a compromise between the two methods, the mini-batch gradient descent method (MSGD), was proposed <ref type="bibr" target="#b0">[1]</ref>.</p><p>The MSGD uses b independent identically distributed samples (b is generally in 50 to 256 <ref type="bibr" target="#b89">[90]</ref>) as the sample sets to update the parameters in each iteration. It reduces the variance of the gradients and makes the convergence more stable, which helps to improve the optimization speed. For brevity, we will call MSGD as SGD in the following sections.</p><p>As a common feature of stochastic optimization, SGD has a better chance of finding the global optimal solution for complex problems. The deterministic gradient in batch gradient descent may cause the objective function to fall into a local minimum for the multimodal problem. The fluctuation in the SGD helps the objective function jump to another possible minimum. However, the fluctuation in SGD always exists, which may more or less slow down the process of converging.</p><p>There are still many details to be noted about the use of SGD in the concrete optimization process <ref type="bibr" target="#b89">[90]</ref>, such as the choice of a proper learning rate. A too small learning rate will result in a slower convergence rate, while a too large learning rate will hinder convergence, making loss function fluctuate at the minimum. One way to solve this problem is to set up a predefined list of learning rates or a certain threshold and adjust the learning rate during the learning process <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b97">[98]</ref>. However, these lists or thresholds need to be defined in advance according to the characteristics of the dataset. It is also inappropriate to use the same learning rate for all parameters. If data are sparse and features occur at different frequencies, it is not expected to update the corresponding variables with the same learning rate. A higher learning rate is often expected for less frequently occurring features <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p><p>Besides the learning rate, how to avoid the objective function being trapped in infinite numbers of the local minimum is a common challenge. Some work has proved that this difficulty does not come from the local minimum values, but comes from the "saddle point" <ref type="bibr" target="#b98">[99]</ref>. The slope of a saddle point is positive in one direction and negative in another direction, and gradient values in all directions are zero. It is an important problem for SGD to escape from these points. Some research about escaping from saddle points were developed <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref>.</p><p>3) Nesterov Accelerated Gradient Descent: Although SGD is popular and widely used, its learning process is sometimes prolonged. How to adjust the learning rate, how to speed up the convergence, and how to prevent being trapped at a local minimum during the search are worthwhile research directions.</p><p>Much work is presented to improve SGD. For example, the momentum idea was proposed to be applied in SGD <ref type="bibr" target="#b101">[102]</ref>. The concept of momentum is derived from the mechanics of physics, which simulates the inertia of objects. The idea of applying momentum in SGD is to preserve the influence of the previous update direction on the next iteration to a certain degree. The momentum method can speed up the convergence when dealing with high curvature, small but consistent gradients, or noisy gradients <ref type="bibr" target="#b102">[103]</ref>. The momentum algorithm introduces the variable v as the speed, which represents the direction and the rate of the parameter's movement in the parameter space. The speed is set as the average exponential decay of the negative gradient.</p><p>In the gradient descent method, the speed update is v = η • (-∂L(θ) ∂(θ) ) each time. Using the momentum algorithm, the amount of the update v is not just the amount of gradient descent calculated by η • (-∂L(θ) ∂(θ) ). It also takes into account the friction factor, which is represented as the previous update v old multiplied by a momentum factor ranging between [0, 1]. Generally, the mass of the object is set to 1. The formulation is expressed as</p><formula xml:id="formula_18">v = η • (- ∂L(θ) ∂(θ) ) + v old • mtm, (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>where mtm is the momentum factor. If the current gradient is parallel to the previous speed v old , the previous speed can speed up this search. The proper momentum plays a role in accelerating the convergence when the learning rate is small. If the derivative decays to 0, it will continue to update v to reach equilibrium and will be attenuated by friction. It is beneficial for escaping from the local minimum in the training process so that the search process can converge more quickly <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b103">[104]</ref>. If the current gradient is opposite to the previous update v old , the value v old will have a deceleration effect on this search.</p><p>The momentum method with a proper momentum factor plays a positive role in reducing the oscillation of convergence when the learning rate is large. How to select the proper size of the momentum factor is also a problem. If the momentum factor is small, it is hard to obtain the effect of improving convergence speed. If the momentum factor is large, the current point may jump out of the optimal value point. Many experiments have empirically verified the most appropriate setting for the momentum factor is 0.9 <ref type="bibr" target="#b89">[90]</ref>.</p><p>Nesterov Accelerated Gradient Descent (NAG) makes further improvement over the traditional momentum method <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b104">[105]</ref>. In Nesterov momentum, the momentum v old • mtm is added to θ, denoted as θ. The gradient of θ is used when updating. The detailed update formulae for parameters θ are as follows:</p><formula xml:id="formula_20">         θ = θ + v old • mtm, v = v old • mtm + η • (- ∂L( θ) ∂(θ) ), θ ′ = θ + v.<label>(16)</label></formula><p>The improvement of Nesterov momentum over momentum is reflected in updating the gradient of the future position instead of the current position. From the update formula, we can find that Nestorov momentum includes more gradient information compared with the traditional momentum method. Note that Nesterov momentum improves the convergence from</p><formula xml:id="formula_21">O( 1 k ) (after k steps) to O( 1 k 2 )</formula><p>, when not using stochastic optimization <ref type="bibr" target="#b104">[105]</ref>.</p><p>Another issue worth considering is how to determine the size of the learning rate. It is more likely to occur the oscillation if the search is closer to the optimal point. Thus, the learning rate should be adjusted. The learning rate decay factor d is commonly used in the SGD's momentum method, which makes the learning rate decrease with the iteration period <ref type="bibr" target="#b105">[106]</ref>. The formula of the learning rate decay is defined as</p><formula xml:id="formula_22">η t = η 0 1 + d • t ,<label>(17)</label></formula><p>where η t is the learning rate at the tth iteration, η 0 is the original learning rate, and d is a decimal in [0, 1]. As can be seen from the formula, the smaller the d is, the slower the decay of the learning rate will be. The learning rate remains unchanged when d = 0 and the learning rate decays fastest when d = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Adaptive Learning Rate Method:</head><p>The manually regulated learning rate greatly influences the effect of the SGD method. It is a tricky problem for setting an appropriate value of the learning rate <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b106">[107]</ref>. Some adaptive methods were proposed to adjust the learning rate automatically. These methods are free of parameter adjustment, fast to converge, and often achieving not bad results. They are widely used in deep neural networks to deal with optimization problems.</p><p>The most straightforward improvement to SGD is AdaGrad <ref type="bibr" target="#b29">[30]</ref>. AdaGrad adjusts the learning rate dynamically based on the historical gradient in some previous iterations. The update formulae are as follows:</p><formula xml:id="formula_23">             g t = ∂L(θ t ) ∂θ , V t = t i=1 (g i ) 2 + ǫ, θ t+1 = θ t -η g t V t ,<label>(18)</label></formula><p>where g t is the gradient of parameter θ at iteration t, V t is the accumulate historical gradient of parameter θ at iteration t, and θ t is the value of parameter θ at iteration t.</p><p>The difference between AdaGrad and gradient descent is that during the parameter update process, the learning rate is no longer fixed, but is computed using all the historical gradients accumulated up to this iteration. One main benefit of AdaGrad is that it eliminates the need to tune the learning rate manually. Most implementations use a default value of 0.01 for η in <ref type="bibr" target="#b17">(18)</ref>.</p><p>Although AdaGrad adaptively adjusts the learning rate, it still has two issues. 1) The algorithm still needs to set the global learning rate η manually. 2) As the training time increases, the accumulated gradient will become larger and larger, making the learning rate tend to zero, resulting in ineffective parameter update.</p><p>AdaGrad was further improved to AdaDelta <ref type="bibr" target="#b30">[31]</ref> and RMSProp <ref type="bibr" target="#b31">[32]</ref> for solving the problem that the learning rate will eventually go to zero. The idea is to consider not accumulating all historical gradients, but focusing only on the gradients in a window over a period, and using the exponential moving average to calculate the second-order cumulative momentum,</p><formula xml:id="formula_24">V t = βV t-1 + (1 -β)(g t ) 2 , (<label>19</label></formula><formula xml:id="formula_25">)</formula><p>where β is the exponential decay parameter. Both RMSProp and AdaDelta have been developed independently around the same time, stemming from the need to resolve the radically diminishing learning rates of AdaGrad.</p><p>Adaptive moment estimation (Adam) <ref type="bibr" target="#b32">[33]</ref> is another advanced SGD method, which introduces an adaptive learning rate for each parameter. It combines the adaptive learning rate and momentum methods. In addition to storing an exponentially decaying average of past squared gradients V t , like AdaDelta and RMSProp, Adam also keeps an exponentially decaying average of past gradients m t , similar to the momentum method:</p><formula xml:id="formula_26">m t = β 1 m t-1 + (1 -β 1 )g t ,<label>(20)</label></formula><formula xml:id="formula_27">V t = β 2 V t-1 + (1 -β 2 )(g t ) 2 ,<label>(21)</label></formula><p>where β 1 and β 2 are exponential decay rates. The final update formula for the parameter θ is</p><formula xml:id="formula_28">θ t+1 = m t -η √ 1 -β 2 1 -β 1 m t V t + ǫ .<label>(22)</label></formula><p>The default values of β 1 , β 2 , and ǫ are suggested to set to 0.9, 0.999, and 10 -8 , respectively. Adam works well in practice and compares favorably to other adaptive learning rate algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Variance Reduction Methods:</head><p>Due to a large amount of redundant information in the training samples, the SGD methods are very popular since they were proposed. However, the stochastic gradient method can only converge at a sublinear rate and the variance of gradient is often very large. How to reduce the variance and improve SGD to the linear convergence has always been an important problem.</p><p>Stochastic Average Gradient The stochastic average gradient (SAG) method <ref type="bibr" target="#b35">[36]</ref> is a variance reduction method proposed to improve the convergence speed. The SAG algorithm maintains parameter d recording the sum of the N latest gradients {g i } in memory where g i is calculated using one sample i, i ∈ {1, ..., N }. The detailed implementation is to select a sample i t to update d randomly, and use d to update the parameter θ in iteration t:</p><formula xml:id="formula_29">       d = d -ĝit + g it (θ t-1 ), ĝit = g it (θ t-1 ), θ t = θ t-1 - α N d,<label>(23)</label></formula><p>where the updated item d is calculated by replacing the old gradient ĝit in d with the new gradient g it (θ t-1 ) in iteration t, α is a constant representing the learning rate. Thus, each update only needs to calculate the gradient of one sample, not the gradients of all samples. The computational overhead is no different from SGD, but the memory overhead is much larger. This is a typical way of using space for saving time.</p><p>The SAG has been shown to be a linear convergence algorithm <ref type="bibr" target="#b35">[36]</ref>, which is much faster than SGD, and has great advantages over other stochastic gradient algorithms. However, the SAG method is only applicable to the case where the loss function is smooth and the objective function is convex <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b107">[108]</ref>, such as convex linear prediction problems. In this case, the SAG achieves a faster convergence rate than the SGD. In addition, under some specific problems, it can even deliver better convergence than the standard batch gradient descent.</p><p>Stochastic Variance Reduction Gradient Since the SAG method is only applicable to smooth and convex functions and needs to store the gradient of each sample, it is inconvenient to be applied in non-convex neural networks. The stochastic variance reduction gradient (SVRG) <ref type="bibr" target="#b36">[37]</ref> method was proposed to improve the performance of optimization in the complex models.</p><p>The algorithm of SVRG maintains the interval average gradient μ by calculating the gradients of all samples in every w iterations instead of in each iteration:</p><formula xml:id="formula_30">μ = 1 N N i=1 g i ( θ), (<label>24</label></formula><formula xml:id="formula_31">)</formula><p>where θ is the interval update parameter. The interval parameter μ contains the average memory of all sample gradients in the past time for each time interval w. SVRG picks uniform i t ∈ {1, ..., N } randomly, and executes gradient updates to the current parameters:</p><formula xml:id="formula_32">θ t = θ t-1 -η • (g it (θ t-1 ) -g it ( θ) + μ). (<label>25</label></formula><formula xml:id="formula_33">)</formula><p>The gradient is calculated up to two times in each update. After w iterations, perform θ ← θ w and start the next w iterations. Through these update, θ t and the interval update parameter θ will converge to the optimal θ * , and then μ → 0, and</p><formula xml:id="formula_34">g it (θ t-1 ) -g it ( θ) + μ → g it (θ t-1 ) -g it (θ * ) → 0. (<label>26</label></formula><formula xml:id="formula_35">)</formula><p>SVRG proposes a vital concept called variance reduction. This concept is related to the convergence analysis of SGD, in which it is necessary to assume that there is a constant upper bound for the variance of the gradients. This constant upper bound implies that the SGD cannot achieve linear convergence. However, in SVRG, the upper bound of variance can be continuously reduced due to the special update item g it (θ t-1 )g it ( θ) + μ , thus achieving linear convergence <ref type="bibr" target="#b36">[37]</ref>.</p><p>The strategies of SAG and SVRG are related to variance reduction. Compared with SAG, SVRG does not need to maintain all gradients in memory, which means that memory resources are saved, and it can be applied to complex problems efficiently. Experiments have shown that the performance of SVRG is remarkable on a non-convex neural network <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b108">[109]</ref>, <ref type="bibr" target="#b109">[110]</ref>. There are also many variants of such linear convergence stochastic optimization algorithms, such as the SAGA algorithm <ref type="bibr" target="#b110">[111]</ref>.</p><p>6) Alternating Direction Method of Multipliers: Augmented Lagrangian multiplier method is a common method to solve optimization problems with linear constraints. Compared with the naive Lagrangian multiplier method, it makes problems easier to solve by adding a penalty term to the objective. Consider the following example,</p><formula xml:id="formula_36">min {θ 1 (x) + θ 2 (y)|Ax + By = b, x ∈ X , y ∈ Y} . (27)</formula><p>The augmented Lagrange function for problem <ref type="bibr" target="#b26">(27)</ref> is</p><formula xml:id="formula_37">L β (x, y, λ) =θ 1 (x) + θ 2 (y) -λ ⊤ (Ax + By -b) + β 2 ||Ax + By -b|| 2 . (<label>28</label></formula><formula xml:id="formula_38">)</formula><p>When solved by the augmented Lagrangian multiplier method, its tth step iteration starts from the given λ t , and the optimization turns out to</p><formula xml:id="formula_39">(x t+1 , y t+1 ) = arg min {L β (x, y, λ t )|x ∈ X , y ∈ Y} , λ t+1 = λ t -β(Ax t+1 + By t+1 -b).<label>(29)</label></formula><p>Separating the (x, y) sub-problem in ( <ref type="formula" target="#formula_39">29</ref>), the augmented Lagrange multiplier method can be relaxed to the following alternating direction method of multipliers (ADMM) <ref type="bibr" target="#b111">[112]</ref>, <ref type="bibr" target="#b112">[113]</ref>. Its tth step iteration starts with the given (y t , λ t ), and the details of iterative optimization are as follows: The penalty parameter β has a certain impact on the convergence rate of the ADMM. The larger β is, the greater the penalties for the constraint term. In general, a monotonically increasing sequence of {β t } can be adopted instead of the fixed β <ref type="bibr" target="#b113">[114]</ref>. Specifically, an auto-adjustment criterion that automatically adjusts {β t } based on the current value of {x t } during the iteration was proposed, and applied for solving some convex optimization problems <ref type="bibr" target="#b114">[115]</ref>, <ref type="bibr" target="#b115">[116]</ref>.</p><formula xml:id="formula_40">             x t+1 = arg min θ 1 (x) -(λ t ) ⊤ Ax + β 2 ||Con x || 2 |x ∈ X , y t+1 = arg min θ 2 (y) -(λ t ) ⊤ By + β 2 ||Con y || 2 |y ∈ Y , λ t+1 = λ t -β(Ax t+1 + By t+1 -b),<label>(30)</label></formula><p>The ADMM method uses the separable operators in the convex optimization problem to divide a large problem into multiple small problems that can be solved in a distributed manner. In theory, the framework of ADMM can solve most of the large-scale optimization problems. However, there are still some problems in practical applications. For example, if we use a stop criterion to determine whether convergence occurs, the original residuals and dual residuals are both related to β, and β with a large value will lead to difficulty in meeting the convergence conditions <ref type="bibr" target="#b116">[117]</ref>.</p><p>7) Frank-Wolfe Method: In 1956, Frank and Wolfe proposed an algorithm for solving linear constraint problems <ref type="bibr" target="#b117">[118]</ref>. The basic idea is to approximate the objective function with a linear function, then solve the linear programming to find the feasible descending direction, and finally make a onedimensional search along the direction in the feasible domain. This method is also called the approximate linearization method.</p><p>Here, we give a simple example of Frank-Wolfe method. Consider the optimization problem,</p><formula xml:id="formula_41">   min f (x), s.t. Ax = b, x ≥ 0,<label>(31)</label></formula><p>where A is an m × n full row rank matrix, and the feasible region is</p><formula xml:id="formula_42">S = {x|Ax = b, x ≥ 0}. Expand f (x) linearly at x 0 , f (x) ≈ f (x 0 ) + ∇f (x 0 ) ⊤ (x -x 0</formula><p>), and substitute it into equation <ref type="bibr" target="#b30">(31)</ref>. Then we have</p><formula xml:id="formula_43">min f (x t ) + ∇f (x t ) ⊤ (x -x t ), s.t. x ∈ S,<label>(32)</label></formula><p>which is equivalent to</p><formula xml:id="formula_44">min ∇f (x t ) ⊤ x, s.t. x ∈ S.<label>(33)</label></formula><p>Suppose there exist an optimal solution y t , and then there must be ∇f</p><formula xml:id="formula_45">(x t ) ⊤ y t &lt; ∇f (x t ) ⊤ x t , ∇f (x t ) ⊤ (y t -x t ) &lt; 0. (<label>34</label></formula><formula xml:id="formula_46">)</formula><p>So y tx t is the decreasing direction of f (x) at x t . A fetch step of λ t updates the search point in a feasible direction. The detailed operation is shown in Algorithm 1.</p><p>Algorithm 1 Frank-Wolfe Method <ref type="bibr" target="#b117">[118]</ref>, <ref type="bibr" target="#b118">[119]</ref> Input:</p><formula xml:id="formula_47">x 0 , ε ≥ 0, t := 0 Output: x * y t ← min ∇f (x t ) ⊤ x while |∇f (x t ) ⊤ (y t -x t )| &gt; ε do λ t = arg min 0≤λ≤1 f (x t + λ(y t -x t )) x t+1 ≈ x t + λ t (y t -x t ) t := t + 1 y t ← min ∇f (x t ) ⊤ x end while x * ≈ x t</formula><p>The algorithm satisfies the following convergence theorem <ref type="bibr" target="#b117">[118]</ref>:</p><p>(1) x t is the Kuhn-Tucker point of (31) when ∇f (x t ) ⊤ (y tx t ) = 0.</p><p>(2) Since y t is an optimal solution for problem <ref type="bibr" target="#b32">(33)</ref>, the vector d t satisfies d t = y tx t and is the feasible descending direction of f at point x t when ∇f (x t ) ⊤ (y tx t ) = 0.</p><p>The Frank-Wolfe algorithm is a first-order iterative method for solving convex optimization problems with constrained conditions. It consists of determining the feasible descent direction and calculating the search step size. The algorithm is characterized by fast convergence in early iterations and slower in later phases. When the iterative point is close to the optimal solution, the search direction and the gradient direction of the objective function tend to be orthogonal. Such a direction is not the best downward direction so that the Frank-Wolfe algorithm can be improved and extended in terms of the selection of the descending directions <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b120">[121]</ref>, <ref type="bibr" target="#b121">[122]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8) Summary:</head><p>We summarize the mentioned first-order optimization methods in terms of properties, advantages, and disadvantages in Table <ref type="table">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. High-Order Methods</head><p>The second-order methods can be used for addressing the problem where an objective function is highly non-linear and ill-conditioned. They work effectively by introducing curvature information.</p><p>This section begins with introducing the conjugate gradient method, which is a method that only needs first-order derivative information for well-defined quadratic programming, but overcomes the shortcoming of the steepest descent method, and avoids the disadvantages of Newton's method of storing and calculating the inverse Hessian matrix. But note that when applying it to general optimization problems, the secondorder gradient is needed to get an approximation to quadratic programming. Then, the classical quasi-Newton method using second-order information is described. Although the convergence of the algorithm can be guaranteed, the computational process is costly and thus rarely used for solving large machine learning problems. In recent years, with the continuous improvement of high-order optimization methods, more and more high-order methods have been proposed to handle largescale data by using stochastic techniques <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b124">[125]</ref>, <ref type="bibr" target="#b125">[126]</ref>. From this perspective, we discuss several high-order methods including the stochastic quasi-Newton method (integrating the second-order information and the stochastic method) and their variants. These algorithms allow us to use high-order methods to process large-scale data.</p><p>1) Conjugate Gradient Method: The conjugate gradient (CG) approach is a very interesting optimization method, which is one of the most effective methods for solving largescale linear systems of equations. It can also be used for solving nonlinear optimization problems <ref type="bibr" target="#b92">[93]</ref>. As we know, the first-order methods are simple but have a slow convergence speed, and the second-order methods need a lot of resources. Conjugate gradient optimization is an intermediate algorithm, which can only utilize the first-order information for some problems but ensures the convergence speed like high-order methods.</p><p>Early in the 1960s, a conjugate gradient method for solving a linear system was proposed, which is an alternative to Gaussian elimination <ref type="bibr" target="#b126">[127]</ref>. Then in 1964, the conjugate gradient method was extended to handle nonlinear optimization for general functions <ref type="bibr" target="#b92">[93]</ref>. For years, many different algorithms have been presented based on this method, some of which have been widely used in practice. The main features of these algorithms are that they have faster convergence speed than steepest descent. Next, we describe the conjugate gradient method. Consider a linear system,</p><formula xml:id="formula_48">Aθ = b, (<label>35</label></formula><formula xml:id="formula_49">)</formula><p>where A is an n × n symmetric, positive-definite matrix. The matrix A and vector b are known, and we need to solve the value of θ. The problem <ref type="bibr" target="#b34">(35)</ref> can also be considered as an optimization problem that minimizes the quadratic positive definite function,</p><formula xml:id="formula_50">min θ F (θ) = 1 2 θ ⊤ Aθ -bθ + c.<label>(36)</label></formula><p>The above two equations have an identical unique solution. It enables us to regard the conjugate gradient as a method for solving optimization problems. The gradient of F (θ) can be obtained by simple calculation, and it equals the residual of the linear system <ref type="bibr" target="#b92">[93]</ref>:</p><formula xml:id="formula_51">r(θ) = ∇F (θ) = Aθ -b.</formula><p>Definition 1: Conjugate: Given an n×n symmetric positivedefinite matrix A, two non-zero vector d i , d j are conjugate with respect to A if</p><formula xml:id="formula_52">d ⊤ i Ad j = 0.<label>(37)</label></formula><p>A set of non-zero vector {d 1 , d</p><p>2 , d 3 , ...., d n } is said to be conjugate with respect to A if any two unequal vectors are conjugate with respect to A [93]. Next, we introduce the detailed derivation of the conjugate gradient method. θ 0 is a starting point, {d t } n-1 t=1 is a set of conjugate directions. In general, one can generate the update sequence {θ 1 , θ 2 , ...., θ n } by a iteration formula:</p><formula xml:id="formula_53">θ t+1 = θ t + η t d t .<label>(38)</label></formula><p>The step size η t can be obtained by a linear search, which means choosing η t to minimize the object function f (•) along θ t +η t d t . After some calculations (more details in <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b127">[128]</ref>), the update formula of η t is</p><formula xml:id="formula_54">η t = r ⊤ t r t d ⊤ t Ad t . (<label>39</label></formula><formula xml:id="formula_55">)</formula><p>The search direction d t is obtained by a linear combination of the negative residual and the previous search direction,</p><formula xml:id="formula_56">d t = -r t + β t d t-1 ,<label>(40)</label></formula><p>where r t can be updated by r t = r t-1 + η t-</p><p>1 Ad t-1 . The scalar β t is the update parameter, which can be determined by satisfying the requirement that d t and d t-1 are conjugate with respect to A, i.e., d ⊤ t Ad t-1 = 0. Multiplying both sides of the equation (40) by d ⊤ t-1 A, one can obtain β t by</p><formula xml:id="formula_57">β t = d ⊤ t-1 Ar t d ⊤ t-1 Ad t-1 . (<label>41</label></formula><formula xml:id="formula_58">)</formula><p>After several derivations of the above formula according to <ref type="bibr" target="#b92">[93]</ref>, the simplified version of β t is</p><formula xml:id="formula_59">β t = r ⊤ t r t r ⊤ t-1 r t-1 . (<label>42</label></formula><formula xml:id="formula_60">)</formula><p>The CG method, has a graceful property that generating a new vector d t only using the previous vector d t-1 , which does not need to know all the previous vectors d 0 , d</p><p>1 , d 2 . . . d t-2 . The linear conjugate gradient algorithm is shown in Algorithm 2. TABLE I: Summary of First-Order Optimization Methods Method Properties Advantages Disadvantages</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GD</head><p>Solve the optimal value along the direction of the gradient descent. The method converges at a linear rate.</p><p>The solution is global optimal when the objective function is convex.</p><p>In each parameter update, gradients of total samples need to be calculated, so the calculation cost is high.</p><p>SGD <ref type="bibr" target="#b0">[1]</ref> The update parameters are calculated using a randomly sampled mini-batch.</p><p>The method converges at a sublinear rate.</p><p>The calculation time for each update does not depend on the total number of training samples, and a lot of calculation cost is saved.</p><p>It is difficult to choose an appropriate learning rate, and using the same learning rate for all parameters is not appropriate. The solution may be trapped at the saddle point in some cases.</p><p>NAG <ref type="bibr" target="#b104">[105]</ref> Accelerate the current gradient descent by accumulating the previous gradient as momentum and perform the gradient update process with momentum.</p><p>When the gradient direction changes, the momentum can slow the update speed and reduce the oscillation; when the gradient direction remains, the momentum can accelerate the parameter update. Momentum helps to jump out of locally optimal solution.</p><p>It is difficult to choose a suitable learning rate.</p><p>AdaGrad <ref type="bibr" target="#b29">[30]</ref> The learning rate is adaptively adjusted according to the sum of the squares of all historical gradients.</p><p>In the early stage of training, the cumulative gradient is smaller, the learning rate is larger, and learning speed is faster. The method is suitable for dealing with sparse gradient problems.</p><p>The learning rate of each parameter adjusts adaptively.</p><p>As the training time increases, the accumulated gradient will become larger and larger, making the learning rate tend to zero, resulting in ineffective parameter updates. A manual learning rate is still needed. It is not suitable for dealing with non-convex problems.</p><p>AdaDelta/ RMSProp <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> Change the way of total gradient accumulation to exponential moving average.</p><p>Improve the ineffective learning problem in the late stage of AdaGrad. It is suitable for optimizing non-stationary and non-convex problems.</p><p>In the late training stage, the update process may be repeated around the local minimum.</p><p>Adam <ref type="bibr" target="#b32">[33]</ref> Combine the adaptive methods and the momentum method. Use the first-order moment estimation and the secondorder moment estimation of the gradient to dynamically adjust the learning rate of each parameter. Add the bias correction.</p><p>The gradient descent process is relatively stable. It is suitable for most non-convex optimization problems with large data sets and high dimensional space.</p><p>The method may not converge in some cases.</p><p>SAG <ref type="bibr" target="#b35">[36]</ref> The old gradient of each sample and the summation of gradients over all samples are maintained in memory. For each update, one sample is randomly selected and the gradient sum is recalculated and used as the update direction.</p><p>The method is a linear convergence algorithm, which is much faster than SGD.</p><p>The method is only applicable to smooth and convex functions and needs to store the gradient of each sample. It is inconvenient to be applied in nonconvex neural networks.</p><p>SVRG <ref type="bibr" target="#b36">[37]</ref> Instead of saving the gradient of each sample, the average gradient is saved at regular intervals. The gradient sum is updated at each iteration by calculating the gradients with respect to the old parameters and the current parameters for the randomly selected samples.</p><p>The method does not need to maintain all gradients in memory, which saves memory resources. It is a linear convergence algorithm.</p><p>To apply it to larger/deeper neural nets whose training cost is a critical issue, further investigation is still needed.</p><p>ADMM <ref type="bibr" target="#b122">[123]</ref> The method solves optimization problems with linear constraints by adding a penalty term to the objective and separating variables into sub-problems which can be solved iteratively.</p><p>The method uses the separable operators in the convex optimization problem to divide a large problem into multiple small problems that can be solved in a distributed manner. The framework is practical in most largescale optimization problems.</p><p>The original residuals and dual residuals are both related to the penalty parameter whose value is difficult to determine.</p><p>Frank-Wolfe <ref type="bibr" target="#b117">[118]</ref> The method approximates the objective function with a linear function, solves the linear programming to find the feasible descending direction, and makes a one-dimensional search along the direction in the feasible domain.</p><p>The method can solve optimization problems with linear constraints, whose convergence speed is fast in early iterations.</p><p>The method converges slowly in later phases. When the iterative point is close to the optimal solution, the search direction and the gradient of the objective function tend to be orthogonal. Such a direction is not the best downward direction.</p><p>Algorithm 2 Conjugate Gradient Method <ref type="bibr" target="#b127">[128]</ref> Input: A, b, θ 0 Output:</p><formula xml:id="formula_61">The solution θ * r 0 = Aθ 0 -b d 0 = -r 0 , t = 0 while Unsatisfied convergence condition do η t = r ⊤ t rt d ⊤ t Adt θ t+1 = θ t + η t d t r t+1 = r t + η t Ad t β t+1 = r ⊤ t+1 rt+1 r ⊤ t rt d t+1 = -r t+1 + β t+1 d t t = t + 1 end while</formula><p>2) Quasi-Newton Methods: Gradient descent employs firstorder information, but its convergence rate is slow. Thus, the natural idea is to use second-order information, e.g., Newton's method <ref type="bibr" target="#b128">[129]</ref>. The basic idea of Newton's method is to use both the first-order derivative (gradient) and secondorder derivative (Hessian matrix) to approximate the objective function with a quadratic function, and then solve the minimum optimization of the quadratic function. This process is repeated until the updated variable converges.</p><p>The one-dimensional Newton's iteration formula is shown as</p><formula xml:id="formula_62">θ t+1 = θ t - f ′ (θ t ) f ′′ (θ t ) , (<label>43</label></formula><formula xml:id="formula_63">)</formula><p>where f is the object function. More general, the highdimensional Newton's iteration formula is</p><formula xml:id="formula_64">θ t+1 = θ t -∇ 2 f (θ t ) -1 ∇f (θ t ) , t ≥ 0,<label>(44)</label></formula><p>where ∇ 2 f is a Hessian matrix of f . More precisely, if the learning rate (step size factor) is introduced, the iteration formula is shown as</p><formula xml:id="formula_65">d t = -∇ 2 f (θ t ) -1 ∇f (θ t ), θ t+1 = θ t + η t d t ,<label>(45)</label></formula><p>where d t is the Newton's direction, η t is the step size. This method can be called damping Newton's method <ref type="bibr" target="#b129">[130]</ref>.</p><p>Geometrically speaking, Newton's method is to fit the local surface of the current position with a quadratic surface, while the gradient descent method is to fit the current local surface with a plane <ref type="bibr" target="#b130">[131]</ref>.</p><p>Quasi-Newton Method Newton's method is an iterative algorithm that requires the computation of the inverse Hessian matrix of the objective function at each step, which makes the storage and computation very expensive. To overcome the expensive storage and computation, an approximate algorithm was considered which is called the quasi-Newton method. The essential idea of the quasi-Newton method is to use a positive definite matrix to approximate the inverse of the Hessian matrix, thus simplifying the complexity of the operation. The quasi-Newton method is one of the most effective methods for solving non-linear optimization problems. Moreover, the second-order gradient is not directly needed in the quasi-Newton method, so it is sometimes more efficient than Newton's method. In the following section, we will introduce several quasi-Newton methods, in which the Hessian matrix and its inverse matrix are approximated by different methods.</p><p>Quasi-Newton Condition We first introduce the quasi-Newton condition. Assuming that the objective function f can be approximated by a quadratic function, we can extend f (θ) to Taylor series at θ = θ t+1 , i.e.,</p><formula xml:id="formula_66">f (θ) ≈ f (θ t+1 ) + ∇f (θ t+1 ) ⊤ (θ -θ t+1 ) + 1 2 (θ -θ t+1 ) ⊤ ∇ 2 f (θ t+1 )(θ -θ t+1 ).<label>(46)</label></formula><p>Then we can compute the gradient on both sides of the above equation, and obtain</p><formula xml:id="formula_67">∇f (θ) ≈ ∇f (θ t+1 ) + ∇ 2 f (θ t+1 )(θ -θ t+1 ).<label>(47)</label></formula><p>Set θ = θ t in (47), we have</p><formula xml:id="formula_68">∇f (θ t ) ≈ ∇f (θ t+1 ) + ∇ 2 f (θ t+1 )(θ t -θ t+1 ).<label>(48)</label></formula><p>Use B to represent the approximate matrix of the Hessian matrix. Set s t = θ t+1θ t , and u t = ∇f (θ t+1 ) -∇f (θ t ).</p><p>The matrix B t+1 is satisfied that</p><formula xml:id="formula_69">u t = B t+1 s t . (<label>49</label></formula><formula xml:id="formula_70">)</formula><p>This equation is called the quasi-Newton condition, or secant equation.</p><p>The search direction of quasi-Newton method is</p><formula xml:id="formula_71">d t = -B -1 t g t ,<label>(50)</label></formula><p>where g t is the gradient of f , and the update of quasi-Newton is</p><formula xml:id="formula_72">θ t+1 = θ t + η t d t .<label>(51)</label></formula><p>The step size η t is chosen to satisfy the Wolfe conditions, which is a set of inequalities for inexact line searches min ηt f (θ t + η t d t ) <ref type="bibr" target="#b131">[132]</ref>. Unlike Newton's method, quasi-Newton method uses B t to approximate the true Hessian matrix. In the following paragraphs, we will introduce some particular quasi-Newton methods, in which H t is used to express the inverse of B t , i.e., H t = B -1 t . DFP In the 1950s, a physical scientist, William C. Davidon <ref type="bibr" target="#b132">[133]</ref>, proposed a new approach to solve nonlinear problems. Then Fletcher and Powel <ref type="bibr" target="#b133">[134]</ref> explained and improved this method, which sparked a lot of research in the late 1960s and early 1970s <ref type="bibr" target="#b5">[6]</ref>. DFP is the first quasi-Newton method named after the initials of their three names. The DFP correction formula is one of the most creative inventions in the field of non-linear optimization, shown as below:</p><formula xml:id="formula_73">B (DF P ) t+1 = (I - u t s ⊤ t u ⊤ t s t )B t (I - s t u ⊤ t u ⊤ t s t ) + u t u ⊤ t u ⊤ t s t . (<label>52</label></formula><formula xml:id="formula_74">)</formula><p>The update formula of H t+1 is</p><formula xml:id="formula_75">H DF P t+1 = H t - H t u t u ⊤ t H t u ⊤ t H t u t + s t s ⊤ t u ⊤ t s t . (<label>53</label></formula><formula xml:id="formula_76">)</formula><p>BFGS Broyden, Fletcher, Goldfarb and Shanno proposed the BFGS method <ref type="bibr" target="#b134">[135]</ref>, <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b2">[3]</ref>, in which B t+1 is updated according to</p><formula xml:id="formula_77">B (BF GS) t+1 = B t - B t s t s ⊤ t B t s ⊤ t B t s t + u t u ⊤ t u ⊤ t s t .<label>(54)</label></formula><p>The corresponding update of H t+1 is</p><formula xml:id="formula_78">H (BF GS) t+1 = (I - s t u ⊤ t s ⊤ t u t )H t (I - u t s ⊤ t s ⊤ t u t ) + u t s ⊤ t s ⊤ t u t . (<label>55</label></formula><formula xml:id="formula_79">)</formula><p>The quasi-Newton algorithm still cannot solve large-scale data optimization problem because the method generates a sequence of matrices to approximate the Hessian matrix. Storing these matrices needs to consume computer resources, especially for high-dimensional problems. It is also impossible to retain these matrices in the high-speed storage of computers, restricting its use to even small and midsize problems <ref type="bibr" target="#b137">[138]</ref>.</p><p>L-BFGS Limited memory quasi-Newton methods, named L-BFGS <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b138">[139]</ref> is an improvement based on the quasi-Newton method, which is feasible in dealing with the highdimensional situation. The method stores just a few ndimensional vectors, instead of retaining and computing fully dense n × n approximations of the Hessian <ref type="bibr" target="#b139">[140]</ref>. The basic idea of L-BFGS is to store the vector sequence in the calculation of approximation H t+1 , instead of storing complete matrix H t . L-BFGS makes further consolidation for the update formula of H t+1 ,</p><formula xml:id="formula_80">H t+1 = (I - s t u ⊤ t u ⊤ t s t )H t (I - u t s ⊤ t u ⊤ t s t ) + s t s ⊤ t u ⊤ t s t = V ⊤ t H t V t + ρs t s ⊤ t ,<label>(56)</label></formula><p>where</p><formula xml:id="formula_81">V t = I -ρu t s ⊤ t , ρ t = 1 s ⊤ t u t . (<label>57</label></formula><formula xml:id="formula_82">)</formula><p>The above equation means that the inverse Hessian approximation H t+1 can be obtained using the sequence pair {s l , u l } t l=t-p+1 . H t+1 can be computed if we know pairs {s l , y l } t l=t-p+1 . In other words, instead of storing and calculating the complete matrix H t+1 , L-BFGS only computes the latest p pairs of {s l , y l }. According to the equation, a recursive procedure can be reached. When the latest p steps are retained, the calculation of H t+1 can be expressed as <ref type="bibr" target="#b138">[139]</ref> </p><formula xml:id="formula_83">H t+1 = (V ⊤ t V ⊤ t-1 • • • V ⊤ t-p+1 )H 0 t (V t-p+1 V t-p+2 • • • V t ) + ρ t-p+1 (V ⊤ t V ⊤ t-1 • • • V t-p+2 )s t-p+1 s ⊤ t-p+1 (V t-p+2 • • • V t ) + ρ t-p+2 (V ⊤ t V ⊤ t-1 • V ⊤ t-p+3 )s t-p+2 s ⊤ t-p+2 (V t-p+3 • • • V t ) + • • • + ρ t s t s ⊤ t .<label>(58)</label></formula><p>The update direction d t = H t g t can be calculated, where g t is the gradient of the objective function f . The detailed algorithm is shown in Algorithms 3 and 4.</p><p>For more information about BFGS and L-BFGS algorithms, one can refer to <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b137">[138]</ref>. Recently, the batch L-BFGS on machine learning was proposed <ref type="bibr" target="#b140">[141]</ref>, which uses the Algorithm 3 Two-Loop Recursion for H t g t <ref type="bibr" target="#b92">[93]</ref> Input: ∇f t , u t , s t Output:</p><formula xml:id="formula_84">H t+1 g t+1 g t = ∇f t H 0 t = s ⊤ t ut ut 2 I for l = t -1 to t -p do η l = ρ l s ⊤ l g l+1 g l = g l+1 -η l u l end for r t-p-1 = H 0 t g t-p for l = t -p to t -1 do β l = ρ l u ⊤ l ρ l-1 ρ l = ρ l-1 + s l (η l -β l ) end for H t+1 g t+1 = ρ</formula><p>Algorithm 4 Limited-BFGS <ref type="bibr" target="#b138">[139]</ref> Input:</p><formula xml:id="formula_85">θ 0 ∈ R n , ǫ &gt; 0 Output: the solution θ * t = 0 g 0 = ∇f 0 u 0 = 1 s 0 = 1 while g t &lt; ǫ do Choose H 0 t , for example H 0 t = s ⊤ t ut ut 2 I g t = ∇f t d t = -H t g t from</formula><p>Algorithm L-BFGS two-loop recursion for H t g t Search a step size η t through Wolfe rule θ t+1 = θ t + η t d t if k &gt; p then Discard the vector pair {s t-p , y t-p } from storage end if Compute and save s t = θ t+1θ t , u t = g t+1g t t = t + 1 end while overlapping mini-batches for consecutive samples for quasi-Newton update. It means that the calculation of u t becomes u</p><formula xml:id="formula_86">t = ∇ St+1 f (θ t+1 )-∇ St f (θ t )</formula><p>, where S t is a small subset of samples, meanwhile S t+1 and S t are not independent, perhaps containing a relatively large overlap. Some numerical results in <ref type="bibr" target="#b140">[141]</ref> have shown that the modification in L-BFGS is effective in practice.</p><p>3) Stochastic Quasi-Newton Method: In many large-scale machine learning models, it is necessary to use a stochastic approximation algorithm with each step of update based on a relatively small training subset <ref type="bibr" target="#b124">[125]</ref>. Stochastic algorithms often obtain the best generalization performances in largescale learning systems <ref type="bibr" target="#b141">[142]</ref>. The quasi-Newton method only uses the first-order gradient information to approximate the Hessian matrix. It is a natural idea to combine the quasi-Newton method with the stochastic method, so that it can perform on large-scale problems. Online-BFGS and online-LBFGS are two variants of BFGS <ref type="bibr" target="#b123">[124]</ref>.</p><p>Consider the minimization of a convex stochastic function,</p><formula xml:id="formula_87">min θ∈R F (θ) = E[f (θ, ξ)],<label>(59)</label></formula><p>where ξ is a random seed. We assume that ξ represents a sample (or a set of samples) consisting of an input-output pair (x, y). In machine learning x typically represents an input and y is the target output. f usually has the following form:</p><formula xml:id="formula_88">f (θ; ξ) = f (θ; x i , y i ) = l(h(w; x i ); y i ), (<label>60</label></formula><formula xml:id="formula_89">)</formula><p>where h is a prediction model parameterized by θ, and l is a loss function. We define f i (θ) = f (θ; x i , y i ), and use the empirical loss to define the objective,</p><formula xml:id="formula_90">F (θ) = 1 N N i=1 f i (θ).<label>(61)</label></formula><p>Typically, if a large amount of training data is used to train the machine learning models, a better choice is using minibatch stochastic gradient,</p><formula xml:id="formula_91">∇F St (θ t ) = 1 c i∈St ∇f i (θ t ),<label>(62)</label></formula><p>where subset</p><formula xml:id="formula_92">S t ⊂ {1, 2, 3 • • • N } is randomly selected. c is the cardinality of S t and c ≪ N . Let S H t ⊂ {1, 2, 3, • • • , N } be</formula><p>a randomly chosen subset of the training samples and the stochastic Hessian estimate can be</p><formula xml:id="formula_93">∇ 2 F St (θ t ) = 1 c h i∈S H t ∇ 2 f i (θ t ),<label>(63)</label></formula><p>where c h is the cardinality of S H t . With given stochastic gradient, a direct approach to develop stochastic quasi-Newton method is to transform deterministic gradients into stochastic gradients throughout the iterations, such as online-BFGS and online-LBFGS <ref type="bibr" target="#b123">[124]</ref>, which are two stochastic adaptations of the BFGS algorithms. Specifically, following the BFGS described in the previous section, s t , u t are modified as s t := θ t+1θ t and u t := ∇F St (θ t+1 ) -∇F St (θ t ). <ref type="bibr" target="#b63">(64)</ref> One disadvantage of this method is that each iteration requires two gradient estimates. Besides this, a more worrying fact is that updating the inverse Hessian approximations in each step may not be reasonable <ref type="bibr" target="#b142">[143]</ref>. Then the stochastic quasi-Newton (SQN) method was proposed, which is to use sub-sampled Hessian-vector products to update H t by the LBFGS according to <ref type="bibr" target="#b124">[125]</ref>. Meanwhile, the authors proposed an effective approach that decouples the stochastic gradient and curvature estimate calculations to obtain a stable Hessian approximation. In particular, since</p><formula xml:id="formula_94">∇F (θ t+1 ) -∇F (θ t ) ≈ ∇ 2 F (θ t )(θ t+1 -θ t ),<label>(65)</label></formula><p>u t can be rewritten as</p><formula xml:id="formula_95">u t := ∇ 2 F S H t (θ t )s t .<label>(66)</label></formula><p>Based on these techniques, an SQN Framework was proposed, and the detailed procedure is shown in Algorithm 5.</p><p>Algorithm 5 SQN Framework <ref type="bibr" target="#b142">[143]</ref> Input: θ 0 , V , m, η t Output:</p><p>The solution θ * for t=1, 2, 3, 4,....., do s ′ t = H t g t using the two-loop recursion.</p><formula xml:id="formula_96">s t = -η t s ′ t θ t+1 = θ t + s ′ t if</formula><p>update pairs then Compute s t and u t Add a new displacement pair {s t , u t } to V if |V | &gt; m then Remove the eldest pair from V end if end if end for</p><p>In the above algorithm, V = {s t , u t } is a collection of m displacement pairs, and g t is the current stochastic gradient ∇F St (θ t ). Meanwhile, the matrix-vector product H t g t can be computed by a two-loop recursion as described in the previous section. Recently, more and more work has achieved very good results in stochastic quasi-Newton. Specifically, a regularized stochastic BFGS method was proposed, which makes a corresponding analysis of the convergence of this optimization method <ref type="bibr" target="#b143">[144]</ref>. Further, an online L-BFGS was presented in <ref type="bibr" target="#b144">[145]</ref>. A linearly convergent method was proposed <ref type="bibr" target="#b125">[126]</ref>, which combines the L-BFGS method in <ref type="bibr" target="#b124">[125]</ref> with the variance reduction technique. Besides these, a variance reduced block L-BFGS method was proposed, which works by employing the actions of a subsampled Hessian on a set of random vectors <ref type="bibr" target="#b145">[146]</ref>.</p><p>To sum up, we have discussed the techniques of using stochastic methods in second-order optimization. The stochastic quasi-Newton method is a combination of the stochastic method and the quasi-Newton method, which makes the quasi-Newton method extend to large datasets. We have introduced the related work of the stochastic quasi-Newton method in recent years, which reflects the potential of the stochastic quasi-Newton method in machine learning applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Hessian-Free Optimization Method:</head><p>The main idea of Hessian-free (HF) method is similar to Newton's method, which employs second-order gradient information. The difference is that the HF method is not necessary to directly calculate the Hessian matrix H. It estimates the product Hv by some techniques, and thus is called "Hessian free".</p><p>Consider a local quadratic approximation Q θ (d t ) of the object F around parameter θ,</p><formula xml:id="formula_97">F (θ t +d t ) ≈ Q θ (d t ) = F (θ t )+∇F (θ t ) ⊤ d t + 1 2 d ⊤ t B t d t ,<label>(67)</label></formula><p>where d t is the search direction. The HF method applies the conjugate gradient method to compute an approximate solution d t of the linear system,</p><formula xml:id="formula_98">B t d t = -∇F (θ t ),<label>(68)</label></formula><p>where B t = H(θ t ) is the Hessian matrix, but in practice B t is often defined as B t = H(θ t ) + λI, λ ≥ 0 <ref type="bibr" target="#b6">[7]</ref>. The new update is then given by</p><formula xml:id="formula_99">θ t+1 = θ t + η t d t ,<label>(69)</label></formula><p>where η t is the step size that ensures sufficient decrease in the objective function, usually obtained by a linear search.</p><p>According to <ref type="bibr" target="#b6">[7]</ref>, the basic framework of HF optimization is shown in Algorithm 6.</p><p>Algorithm 6 Hessian-Free Optimization Method <ref type="bibr" target="#b6">[7]</ref> Input: θ 0 , ∇f (θ 0 ), λ Output: The solution θ * t = 0 repeat g t = ∇f (θ t ) Compute λ by some methods</p><formula xml:id="formula_100">B t (v) ≡ H(θ t )v + λv Compute the step size η t d t = CG(B t , -g t ) θ t+1 = θ t + η t d t t = t + 1 until satisfy convergence condition</formula><p>The advantage of using the conjugate gradient method is that it can calculate the Hessian-vector product without directly calculating the Hessian matrix. Because in the CGalgorithm, the Hessian matrix is paired with a vector, then we can compute the Hessian-vector product to avoid the calculation of the Hessian inverse matrix. There are many ways to calculate Hessian-vector products, one of which is calculated by a finite difference as <ref type="bibr" target="#b6">[7]</ref> </p><formula xml:id="formula_101">Hv = lim ε→+0 ∇f (θ + εv) -∇f (θ) ε .<label>(70)</label></formula><p>Sub-sampled Hessian-Free Method HF is a well-known method, and has been studied for decades in the optimization literatures, but has shortcomings when applied to deep neural networks with large-scale data <ref type="bibr" target="#b6">[7]</ref>. Therefore, a sub-sampled technique is employed in HF, resulting in an efficient HF method <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b146">[147]</ref>. The cost in each iteration can be reduced by using only a small sample set S to calculate Hv. The objective function has the following form:</p><formula xml:id="formula_102">min F (θ) = 1 N N i=1 f i (θ).<label>(71)</label></formula><p>In the tth iteration, the stochastic gradient estimation can be written as</p><formula xml:id="formula_103">∇F St (θ t ) = 1 |S t | i∈St ∇f i (θ t ),<label>(72)</label></formula><p>and the stochastic Hessian estimate is expressed as</p><formula xml:id="formula_104">∇ 2 F S H t (θ t ) = 1 |S H t | i∈S H t ∇ 2 f i (θ t ). (<label>73</label></formula><formula xml:id="formula_105">)</formula><p>As described above, we can obtain the approximate solution of direction d t by employing the CG method to solve the linear system,</p><formula xml:id="formula_106">∇ 2 F S H t (θ t )d t = -∇F St (θ t ),<label>(74)</label></formula><p>in which the stochastic gradient and stochastic Hessian matrix are used. The basic framework of sub-sampled HF algorithm is given in <ref type="bibr" target="#b146">[147]</ref>.</p><p>A natural question is how to determine the size of S H t . On one hand, S H t can be chosen small enough so that the total cost of CG iteration is not much greater than a gradient evaluation. On the other hand, S H t should be large enough to get useful curvature information from Hessian-vector product. How to balance the size of S H t is a challenge being studied <ref type="bibr" target="#b146">[147]</ref>. 5) Natural Gradient: The natural gradient method can be potentially applied to any objective function which measures the performance of some statistical models <ref type="bibr" target="#b147">[148]</ref>. It enjoys richer theoretical properties when applied to objective functions based on the KL divergence between the model's distribution and the target distribution, or certain approximation surrogates of these <ref type="bibr" target="#b148">[149]</ref>.</p><p>The traditional gradient descent algorithm is based on the Euclidean space. However, in many cases, the parameter space is not Euclidean, and it may have a Riemannian metric structure. In this case, the steepest direction of the objective function cannot be given by the ordinary gradient and should be given by the natural gradient <ref type="bibr" target="#b147">[148]</ref>.</p><p>We consider such a model distribution p(y|x, θ), and π(x, y) is an empirical distribution. We need to fit the parameters θ ∈ R N . Assume that x is an observation vector, and y is its associated label. It has the objective function,</p><formula xml:id="formula_107">F (θ) = E (x,y)∼π [-log p(y|x, θ)],<label>(75)</label></formula><p>and we need to solve the optimization problem,</p><formula xml:id="formula_108">θ * = argmin θ F (θ).<label>(76)</label></formula><p>According to <ref type="bibr" target="#b147">[148]</ref>, the natural gradient can be transformed from a traditional gradient multiplied by a Fisher information matrix, i.e.,</p><formula xml:id="formula_109">∇ N F = G -1 ∇F, (<label>77</label></formula><formula xml:id="formula_110">)</formula><p>where F is the object function, ▽F is the traditional gradient, ▽ N F is the natural gradient, and G is the Fisher information matrix, with the following form:</p><formula xml:id="formula_111">G = E x∼π E y∼p(y|x,θ) ( ∂p(y|x; θ) ∂θ )( ∂p(y|x; θ) ∂θ ) ⊤ .</formula><p>(78) The update formula with the natural gradient is</p><formula xml:id="formula_112">θ t = θ t -η t ∇ N F. (<label>79</label></formula><formula xml:id="formula_113">)</formula><p>We cannot ignore that the application of the natural gradient is very limited because of too much computation. It is expensive to estimate the Fisher information matrix and calculate its inverse matrix. To overcome this limitation, the truncated Newton's method was developed <ref type="bibr" target="#b6">[7]</ref>, in which the inverse is calculated by an iterative procedure, thus avoiding the direct calculation of the inverse of the Fisher information matrix. In addition, the factorized natural gradient (FNG) <ref type="bibr" target="#b149">[150]</ref> and Kronecker-factored approximate curvature (K-FAC) <ref type="bibr" target="#b150">[151]</ref> methods were proposed to use the derivatives of probabilistic models to calculate the approximate natural gradient update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Trust Region Method:</head><p>The update process of most methods introduced above can be described as θ t + η t d t . The displacement of the point in the direction of d t can be written as s t . The typical trust region method (TRM) can be used for unconstrained nonlinear optimization problems <ref type="bibr" target="#b139">[140]</ref>, <ref type="bibr" target="#b151">[152]</ref>, <ref type="bibr" target="#b152">[153]</ref>, in which the displacement s t is directly determined without the search direction d t .</p><p>For the problem min f θ (x), the TRM <ref type="bibr" target="#b139">[140]</ref> uses the secondorder Taylor expansion to approximate the objective function f θ (x), denoted as q t (s). Each search is done within the range of trust region with radius △ t . This problem can be described as</p><formula xml:id="formula_114">   min q t (s) = f θ (x t ) + g ⊤ t s + 1 2 s ⊤ B t s, s.t. ||s t || ≤ △ t ,<label>(80)</label></formula><p>where g t is the approximate gradient of the objective function f (x) at the current iteration point x t , g t ≈ ∇f (x t ), B t is a symmetric matrix, which is the approximation of Hessian matrix ∇ 2 f θ (x t ), and △ t &gt; 0 is the radius of the trust region.</p><p>If the L 2 norm is used in the constraint function, it becomes the Levenberg-Marquardt algorithm <ref type="bibr" target="#b153">[154]</ref>. If s t is the solution of the trust region subproblem (80), the displacement s t of each update is limited by the trust region radius △ t . The core part of the TRM is the update of △ t . In each update process, the similarity of the quadratic model q(s t ) and the objective function f θ (x) is measured, and △ t is updated dynamically. The actual amount of descent in the tth iteration is <ref type="bibr" target="#b139">[140]</ref> </p><formula xml:id="formula_115">△f t = f t -f (x t + s t ).<label>(81)</label></formula><p>The predicted drop in the tth iteration is</p><formula xml:id="formula_116">△q t = f t -q(s t ).<label>(82)</label></formula><p>The ratio r t is defined to measure the approximate degree of both,</p><formula xml:id="formula_117">r t = △f t △q t . (<label>83</label></formula><formula xml:id="formula_118">)</formula><p>It indicates that the model is more realistic than expected when r t is close to 1, and then we should consider expanding △ t . At the same time, it indicates that the model predicts a large drop and the actual drop is small when r t is close to 0, and then we should reduce △ t . Moreover, if r t is between 0 and 1, we can leave △ t unchanged. The thresholds 0 and 1 are generally set as the left and right boundaries of r t <ref type="bibr" target="#b139">[140]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7) Summary:</head><p>We summarize the mentioned high-order optimization methods in terms of properties, advantages and disadvantages in Table <ref type="table">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Derivative-Free Optimization</head><p>For some optimization problems in practical applications, the derivative of the objective function may not exist or is not easy to calculate. The solution of finding the optimal point, in this case, is called derivative-free optimization, which is a discipline of mathematical optimization <ref type="bibr" target="#b154">[155]</ref>, <ref type="bibr" target="#b155">[156]</ref>, <ref type="bibr" target="#b156">[157]</ref>. It can find the optimal solution without the gradient information.</p><p>There are mainly two types of ideas for derivativefree optimization. One is to use heuristic algorithms. It is characterized by empirical rules and chooses methods that have already worked well, rather than derives solutions systematically. There are many types of heuristic optimization methods, including classical simulated annealing arithmetic, genetic algorithms, ant colony algorithms, and particle swarm optimization <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b158">[159]</ref>, <ref type="bibr" target="#b159">[160]</ref>. These heuristic methods usually yield approximate global optimal values, and theoretical support is weak. We do not focus on such techniques in this section. The other is to fit an appropriate function according to the samples of the objective function. This type of method usually attaches some constraints to the search space to derive the samples. Coordinate descent method is a typical derivativefree algorithm <ref type="bibr" target="#b160">[161]</ref>, and it can be extended and applied to optimization algorithms for machine learning problems easily. In this section, we mainly introduce the coordinate descent method.</p><p>The coordinate descent method is a derivative-free optimization algorithm for multi-variable functions. Its idea is that a one-dimensional search can be performed sequentially along each axis direction to obtain updated values for each dimension. This method is suitable for some problems in which the loss function is non-differentiable.</p><p>The vanilla approach is to select a set of bases e 1 , e 2 , ..., e D in the linear space as the search directions and minimizes the value of the objective function in each direction. For the target function L(Θ), when Θ t is already obtained, the jth dimension of Θ t+1 is solved by <ref type="bibr" target="#b154">[155]</ref> θ t+1 j = arg min θj∈R L(θ t+1 1 , ..., θ t+1 j-1 , θ j , θ t j+1 , ..., θ t D ). (84) Thus, L(Θ t+1 ) ≤ L(Θ t ) ≤ ... ≤ L(Θ 0 ) is guaranteed. The convergence of this method is similar to the gradient descent method. The order of update can be an arbitrary arrangement from e 1 to e D in each iteration. The descent direction can be generalized from the coordinate axis to the coordinate block <ref type="bibr" target="#b161">[162]</ref>.</p><p>The main difference between the coordinate descent and the gradient descent is that each update direction in the gradient descent method is determined by the gradient of the current position, which may not be parallel to any coordinate axis. In the coordinate descent method, the optimization direction is fixed from beginning to end. It does not need to calculate the gradient of the objective function. In each iteration, the update is only executed along the direction of one axis, and thus the calculation of the coordinate descent method is simple even for some complicated problems. For indivisible functions, the algorithm may not be able to find the optimal solution in a small number of iteration steps. An appropriate coordinate system can be used to accelerate the convergence. For example, the adaptive coordinate descent method takes principal component analysis to obtain a new coordinate system with as little correlation as possible between the coordinates <ref type="bibr" target="#b162">[163]</ref>. The coordinate descent method still has limitations when performing on the non-smooth objective function, which may fall into a non-stationary point.</p><p>TABLE II: Summary of High-Order Optimization Methods Method Properties Advantages Disadvantages</p><p>Conjugate Gradient <ref type="bibr" target="#b126">[127]</ref> It is an optimization method between the first-order and second-order gradient methods. It constructs a set of conjugated directions using the gradient of known points, and searches along the conjugated direction to find the minimum points of the objective function.</p><p>CG method only calculates the first order gradient but has faster convergence than the steepest descent method.</p><p>Compared with the first-order gradient method, the calculation of the conjugate gradient is more complex.</p><p>Newton's Method <ref type="bibr" target="#b128">[129]</ref> Newton's method calculates the inverse matrix of the Hessian matrix to obtain faster convergence than the first-order gradient descent method.</p><p>Newton's method uses second-order gradient information which has faster convergence than the first-order gradient method. Newton's method has quadratic convergence under certain conditions.</p><p>It needs long computing time and large storage space to calculate and store the inverse matrix of the Hessian matrix at each iteration.</p><p>Quasi-Newton Method <ref type="bibr" target="#b92">[93]</ref> Quasi-Newton method uses an approximate matrix to approximate the the Hessian matrix or its inverse matrix. Popular quasi-Newton methods include DFP, BFGS and LBFGS.</p><p>Quasi-Newton method does not need to calculate the inverse matrix of the Hessian matrix, which reduces the computing time. In general cases, quasi-Newton method can achieve superlinear convergence.</p><p>Quasi-Newton method needs a large storage space, which is not suitable for handling the optimization of large-scale problems.</p><p>Sochastic Quasi-Newton Method <ref type="bibr" target="#b142">[143]</ref>.</p><p>Stochastic quasi-Newton method employs techniques of stochastic optimization. Representative methods are online-LBFGS <ref type="bibr" target="#b123">[124]</ref> and SQN <ref type="bibr" target="#b124">[125]</ref>.</p><p>Stochastic quasi-Newton method can deal with large-scale machine learning problems.</p><p>Compared with the stochastic gradient method, the calculation of stochastic quasi-Newton method is more complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hessian</head><p>Free Method <ref type="bibr" target="#b6">[7]</ref> HF method performs a suboptimization using the conjugate gradient, which avoids the expensive computation of inverse Hessian matrix.</p><p>HF method can employ the secondorder gradient information but does not need to directly calculate Hessian matrices. Thus, it is suitable for high dimensional optimization.</p><p>The cost of computation for the matrixvector product in HF method increases linearly with the increase of training data. It does not work well for largescale problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-sampled Hessian</head><p>Free Method <ref type="bibr" target="#b146">[147]</ref> Sup-sampled Hessian free method uses stochastic gradient and sub-sampled Hessian-vector during the process of updating.</p><p>The sub-sampled HF method can deal with large-scale machine learning optimization problems.</p><p>Compared with the stochastic gradient method, the calculation is more complex and needs more computing time in each iteration.</p><p>Natural Gradient <ref type="bibr" target="#b147">[148]</ref> The basic idea of the natural gradient is to construct the gradient descent algorithm in the predictive function space rather than the parametric space.</p><p>The natural gradient uses the Riemann structure of the parametric space to adjust the update direction, which is more suitable for finding the extremum of the objective function.</p><p>In the natural gradient method, the calculation of the Fisher information matrix is complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Preconditioning in Optimization</head><p>Preconditioning is a very important technique in optimization methods. Reasonable preconditioning can reduce the iteration number of optimization algorithms. For many important iterative methods, the convergence depends largely on the spectral properties of the coefficient matrix <ref type="bibr" target="#b163">[164]</ref>. It can be simply considered that the pretreatment is to transform a difficult linear system Aθ = b into an equivalent system with the same solution but better spectral characteristics. For example, if M is a nonsingular approximation of the coefficient matrix A, the transformed system,</p><formula xml:id="formula_119">M -1 Aθ = M -1 b,<label>(85)</label></formula><p>will have the same solution as the system Aθ = b. But ( <ref type="formula" target="#formula_119">85</ref>) may be easier to solve and the spectral properties of the coefficient matrix M -1 A may be more favorable.</p><p>In most linear systems, e.g., Aθ = b, the matrix A is often complex and makes it hard to solve the system. Therefore, some transformation is needed to simplify this system. M is called the preconditioner. If the matrix after using preconditioner is obviously structured, or sparse, it will be beneficial to the calculation <ref type="bibr" target="#b164">[165]</ref>.</p><p>The conjugate gradient algorithm mentioned previously is the most commonly used optimization method with preconditioning technology, which speeds up the convergence. The algorithm is shown in Algorithm 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Public Toolkits for Optimization</head><p>Fundamental optimization methods are applied in machine learning problems extensively. There are many integrated powerful toolkits. We summarize the existing common optimization toolkits and present them in Table <ref type="table">III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DEVELOPMENTS AND APPLICATIONS FOR SELECTED MACHINE LEARNING FIELDS</head><p>Optimization is one of the cores of machine learning. Many optimization methods are further developed in the face of different machine learning problems and specific application environments. The machine learning fields selected in this</p><p>TABLE III: Available Toolkits for Optimization Toolkit Language Description CVX [166] Matlab CVX is a matlab-based modeling system for convex optimization but cannot handle large-scale problems. <ref type="url" target="http://cvxr.com/cvx/download/">http://cvxr.com/cvx/download/</ref> CVXPY [167] Python CVXPY is a python package developed by Stanford University Convex Optimization Group for solving convex optimization problems. <ref type="url" target="http://www.cvxpy.org/">http://www.cvxpy.org/</ref> CVXOPT [168] Python CVXOPT can be used for handling convex optimization. It is developed by Martin Andersen, Joachim Dahl, and Lieven Vandenberghe. <ref type="url" target="http://cvxopt.org/">http://cvxopt.org/</ref> APM [169] Python APM python is suitable for large-scale optimization and can solve the problems of linear programming, quadratic programming, integer programming, nonlinear optimization and so on. <ref type="url" target="http://apmonitor.com/wiki/index.php/Main/PythonApp">http://apmonitor.com/wiki/index.php/Main/PythonApp</ref> SPAMS [123] C++ SPAMS is an optimization toolbox for solving various sparse estimation problems, which is developed and maintained by Julien Mairal. Available interfaces include matlab, R, python and C++. <ref type="url" target="http://spams-devel.gforge.inria.fr/">http://spams-devel.gforge.inria.fr/</ref> minConf Matlab minConf can be used for optimizing differentiable multivariate functions which subject to simple constraints on parameters. It is a set of matlab functions, in which there are many methods to choose from. <ref type="url" target="https://www.cs.ubc.ca/~schmidtm/Software/minConf.html">https://www.cs.ubc.ca/</ref> ∼ schmidtm/Software/minConf.html tf.train.optimizer [170] Python; C++; CUDA The basic optimization class, which is usually not called directly and its subclasses are often used. It includes classic optimization algorithms such as gradient descent and AdaGrad. <ref type="url" target="https://www.tensorflow.org/api_guides/python/train">https://www.tensorflow.org/api guides/python/train</ref> Algorithm 7 Preconditioned Conjugate Gradient Method [93]</p><formula xml:id="formula_120">Input: A, θ 0 , M , b Output: The solution θ * f 0 = f (θ 0 ) g 0 = ∇f (θ 0 ) = Aθ 0 -b y 0 is the solution of M y = g 0 d 0 = -g 0 t = t while g t = 0 do η t = g ⊤ t yt d ⊤ t Adt θ t+1 = θ t + η t d t g t+1 = g t + η t Ad t y t+1 =solution of M y = g t β t+1 = g ⊤ t+1 yt+1 g ⊤ t dt</formula><p>d t+1 = -y t+1 + β t+1 d t t = t + 1 end while section mainly include deep neural networks, reinforcement learning, variational inference and Markov chain Monte Carlo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optimization in Deep Neural Networks</head><p>The deep neural network (DNN) is a hot topic in the machine learning community in recent years. There are many optimization methods for DNNs. In this section, we introduce them from two aspects, i.e., first-order optimization methods and high-order optimization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) First-Order Gradient Method in Deep Neural Networks:</head><p>The stochastic gradient optimization method and its adaptive variants have been widely used in DNNs and have achieved good performance. SGD introduces the learning rate decay factor and AdaGrad accumulates all previous gradients so that their learning rates are continuously decreased and converge to zero. However, the learning rates of these two methods make the update slow in the later stage of optimization. AdaDelta, RMSProp, Adam and other methods use the exponential averaging to provide effective updates and simplify the calculation. These methods use exponential moving average to alleviate the problems caused by the rapid decay of the learning rate but limit the current learning rate to only relying on a few gradients <ref type="bibr" target="#b33">[34]</ref>. Reddi et al. used a simple convex optimization example to demonstrate that the RMSProp and Adam algorithms could not converge <ref type="bibr" target="#b33">[34]</ref>. Almost all the algorithms that rely on a fixed-size window of the past gradients will suffer from this problem, including AdaDelta and Nesterov-accelerated adaptive moment estimation (Nadam) <ref type="bibr" target="#b170">[171]</ref>.</p><p>It is better to rely on the long-term memory of past gradients rather than the exponential moving average of gradients to ensure convergence. A new version of Adam <ref type="bibr" target="#b33">[34]</ref>, called AmsGrad, uses a simple correction method to ensure the convergence of the model while preserving the original computational performance and advantages. Compared with the Adam method, the AmsGrad makes the following changes to the first-order moment estimation and the second-order moment estimation:</p><formula xml:id="formula_121">       m t = β 1t m t-1 + (1 -β 1t )g t , V t = β 2 V t-1 + (1 -β 2 )g 2 t , Vt = max( Vt-1 , V t ),<label>(86)</label></formula><p>where β 1t is a non-constant which decreases with time, and β 2 is a constant learning rate. The correction is operated in the second-order moment V t , making Vt monotonous. Vt is substantially used in the iteration of the target function. The AmsGrad method takes the long-term memory of past gradients based on the Adam method, guarantees the convergence in the later stage, and works well in applications.</p><p>Further, adjusting parameters β 1 , β 2 at the same time helps to converge to a certain extent. For example, β 1 can decay modestly as β 1t = β1 t , β 1t ≤ β 1 , for all t ∈ [T ]. β 2 can be set as β 2t = 1 -1 t , for all t ∈ [T ], as in AdamNC algorithm <ref type="bibr" target="#b33">[34]</ref>.</p><p>Another idea of combining SGD and Adam was proposed for solving the non-convergence problem of adaptive gradient algorithm <ref type="bibr" target="#b37">[38]</ref>. Adaptive algorithms, such as Adam, converge fast and are suitable for processing sparse data. SGD with momentum can converge to more accurate results. The combination of SGD and Adam develops the advantages of both methods. Specifically, it first trains with Adam to quickly drop and then switches to SGD for precise optimization based on the previous parameters at an appropriate switch point. The strategy is named as switching from Adam to SGD (SWATS) <ref type="bibr" target="#b37">[38]</ref>. There are two core problems in SWATS. One is when to switch from Adam to SGD, the other is how to adjust the learning rate after switching the optimization algorithm. The SWATS approach is described in detail below.</p><p>The movement d Adam of the parameter at iteration t of the Adam is</p><formula xml:id="formula_122">d Adam t = η Adam V t m t ,<label>(87)</label></formula><p>where η Adam is the learning rate of Adam <ref type="bibr" target="#b37">[38]</ref>. The movement d SGD of the parameter at iteration t of the SGD is</p><formula xml:id="formula_123">d SGD t = η SGD g t ,<label>(88)</label></formula><p>where η SGD is the learning rate of SGD and g t is the gradient of the current position <ref type="bibr" target="#b37">[38]</ref>.</p><p>The movement of SGD can be decomposed into the learning rates along Adam's direction and its orthogonal direction. If SGD is going to finish the trajectory but Adam has not finished due to the momentum after selecting the optimization direction, walking along Adam's direction is a good choice for SWATS. At the same time, SWATS also adjusts its optimized trajectory by moving in the orthogonal direction. Let</p><formula xml:id="formula_124">P roj Adam d SGD t = d Adam t ,<label>(89)</label></formula><p>and derive solution</p><formula xml:id="formula_125">η SGD t = (d Adam t ) T d Adam t (d Adam t ) T g t ,<label>(90)</label></formula><p>where P roj Adam means the projection in the direction of Adam. To reduce noise, a moving average can be used to correct the estimate of the learning rate,</p><formula xml:id="formula_126">λ SGD t = β 2 λ SGD t-1 + (1 -β 2 )η SGD t ,<label>(91)</label></formula><formula xml:id="formula_127">λt SGD = λ SGD t 1 -β 2 ,<label>(92)</label></formula><p>where λ SGD t is the first moment of learning rate η SGD , and λt SGD is the learning rate of SGD after converting <ref type="bibr" target="#b37">[38]</ref>. For switch point, a simple guideline | λt SGDλ SGD t | &lt; ǫ is often used <ref type="bibr" target="#b37">[38]</ref>. Although there is no rigorous mathematical proof for selecting this conversion criterion, it performs well across a variety of applications. For the mathematical proof of switch point, further research can be conducted. Although the SWATS is based on Adam, this switching method is also applicable to other adaptive methods, such as AdaGrad and RMSProp. The procedure is insensitive to hyper-parameters and can obtain an optimal solution comparable to SGD, but with a faster training speed in the case of deep networks.</p><p>Recently some researchers are trying to explain and improve the adaptive methods <ref type="bibr" target="#b171">[172]</ref>, <ref type="bibr" target="#b172">[173]</ref>. Their strategies can also be combined with the above switching techniques to enhance the performance of the algorithm.</p><p>General fully connected neural networks cannot process sequential data such as text and audio. Recurrent neural network (RNN) is a kind of neural networks that is more suitable for processing sequential data. It was generally considered that the use of first-order methods to optimize RNN was not effective, because the SGD and its variant methods were difficult to learn long-term dependencies in sequence problems <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b173">[174]</ref>.</p><p>In recent years, a well-designed method of random parameter initialization scheme using only SGD with momentum without curvature information has achieved good results in training RNNs <ref type="bibr" target="#b98">[99]</ref>. In <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b174">[175]</ref>, some techniques for improving the optimization in training RNNs are summarized such as the momentum methods and NAG. The first-order optimization methods have got development for training RNNs, but they still face the problem of slow convergence in deep RNNs. The high-order optimization methods employing curvature information can accelerate the convergence near the optimal value and is considered to be more effective in optimizing DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) High-Order Gradient Method in Deep Neural Networks:</head><p>We have described the first-order optimization method applied in DNNs. As most DNNs use large-scale data, different versions of stochastic gradient methods were developed and have got excellent performance and properties. For making full use of gradient information, the second-order method is gradually applied to DNNs. In this section, we mainly introduce the Hessian-free method in DNN.</p><p>Hessian-free (HF) method has been studied for a long time in the field of optimization, but it is not directly suitable for dealing with neural networks <ref type="bibr" target="#b6">[7]</ref>. As the objective function in DNN is not convex, the exact Hessian matrix may not be positive definite. Therefore, some modifications need to be made so that the HF method can be applied to neural networks <ref type="bibr" target="#b175">[176]</ref>.</p><p>The Generalized Gauss-Newton Matrix One solution is to use the generalized Gauss-Newton (GGN) matrix, which can be seen as an approximation of a Hessian matrix <ref type="bibr" target="#b176">[177]</ref>. The GGN matrix is a provably positive semidefinite matrix, which avoids the trouble of negative curvature. There are at least two ways to derive the GGN matrix <ref type="bibr" target="#b175">[176]</ref>. Both of them require that f (θ) can be expressed as a composition of two functions written as f (θ) = Q(F (θ)) where f (θ) is the object function and Q is convex. The GGN matrix G takes the following form,</p><formula xml:id="formula_128">G = J ⊤ Q ′′ J,<label>(93)</label></formula><p>where J is the Jacobian of F . Damping Methods Another modification to the HF method is to use different damping methods. For example, Tikhonov damping, one of the most famous damping methods, is implemented by introducing a quadratic penalty term into the quadratic model. A quadratic penalty term λ 2 d ⊤ d is added to the quadratic model,</p><formula xml:id="formula_129">Q(θ) := Q(θ) + λ 2 d ⊤ d = f (θ t ) + ∇f (θ t ) ⊤ d + 1 2 d ⊤ Bd,<label>(94)</label></formula><p>where B = H + λI, and λ &gt; 0 determines the "strength" of the damping which is a scalar parameter. Thus, Bv is formulated as Bv = (H + λI)v = Hv + λv. However, the basic Tikhonov damping method is not good in training RNNs <ref type="bibr" target="#b177">[178]</ref>. Due to the complex structure of RNNs, the local quadratic approximation in certain directions in the parameter space, even at very small distances, maybe highly imprecise. The Tikhonov damping method can only compensate for this by increasing punishment in all directions because the method lacks a selective mechanism <ref type="bibr" target="#b175">[176]</ref>. Therefore, the structural damping was proposed, which makes the performance substantially better and more robust.</p><p>The HF method with structural damping can effectively train RNNs <ref type="bibr" target="#b175">[176]</ref>. Now we briefly introduce the HF method with structural damping. Let e(x, θ) mean the vector-value function of θ which can be interpreted as intermediate quantities during the calculation of f (x, θ), where f (x, θ) is the object function. For instance, e(x, θ) might contain the activation function of some layers of hidden units in neural networks (like RNNs). A structural damping can be defined as</p><formula xml:id="formula_130">R(θ) = 1 |S| (x,y)∈S D(e(x, θ), e(x, θ t )),<label>(95)</label></formula><p>where D is a distance function or a loss function. It can prevent a large change in e(x, θ) by penalizing the distance between e(x, θ) and e(x, θ t ). Then, the damped local objective can be written as</p><formula xml:id="formula_131">Q θ (d) ′ = Q θ (d) + µR(d + θ t ) + λ 2 d ⊤ d,<label>(96)</label></formula><p>where µ and λ are two parameters to be dynamically adjusted. d is the direction at the tth iteration. More details of the structural damping can refer to <ref type="bibr" target="#b175">[176]</ref>.</p><p>Besides, there are many second-order optimization methods employed in RNNs. For example, quasi-Newton based optimization and L-BFGS were proposed to train RNNs <ref type="bibr" target="#b178">[179]</ref>, <ref type="bibr" target="#b179">[180]</ref>.</p><p>In order to make the damping method based on punishment work better, the damping parameters can be adjusted continuously. A Levenberg-Marquardt style heuristic method was used to adjust λ directly <ref type="bibr" target="#b6">[7]</ref>. The Levenberg-Marquardt heuristic is described as follows:</p><p>1</p><formula xml:id="formula_132">) If γ &lt; 1 4 λ then λ ← 3 2 λ, 2) If γ &gt; 3 4 λ then λ ← 2 3</formula><p>λ, where γ is a "reduction rate" with the following form,</p><formula xml:id="formula_133">γ = f (θ t-1 + d t ) -f (θ t-1 ) M t-1 (d t ) .<label>(97)</label></formula><p>Sub-sampling As sub-sampling Hessian can be used to handle large-scale data, several variations of the sub-sampling methods were proposed <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, which used either stochastic gradients or exact gradients. These approaches use</p><formula xml:id="formula_134">B t = ∇ 2</formula><p>St f (θ t ) as a Hessian approximation, where S t is a subset of samples. We need to compute the Hessian-vector product in some optimization methods. If we adopt the subsampling method, it also means that we can save a lot of computation in each iteration, such as the method proposed in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Preconditioning Preconditioning can be used to simplify the optimization problems. For example, preconditioning can accelerate the CG method. It is found that diagonal matrices are particularly effective and one can use the following preconditioner <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_135">M = diag( N i=1 ∇f i (θ) ⊙ ∇f i (θ)) + λI α ,<label>(98)</label></formula><p>where ⊙ denotes the element-wise product and the exponent α is chosen to be less than 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization in Reinforcement Learning</head><p>Reinforcement learning (RL) is an important research field of machine learning and is also one of the most popular topics. Agents using deep reinforcement learning have achieved great success in learning complex behavior skills and solved challenging control tasks in high-dimensional primitive perceptual state space <ref type="bibr" target="#b180">[181]</ref>, <ref type="bibr" target="#b181">[182]</ref>, <ref type="bibr" target="#b182">[183]</ref>. It interacts with the environment through the trial-and-error mechanism and learns optimal strategies by maximizing cumulative rewards <ref type="bibr" target="#b38">[39]</ref>.</p><p>We describe several concepts of reinforcement learning as follows:</p><p>1) Agent: making different actions according to the state of the external environment, and adjusting the strategy according to the reward of the external environment. 2) Environment: all things outside the agent that will be affected by the action of the agent. It can change the state and provide the reward to the agent. 3) State s: a description of the environment. 4) Action a: a description of the behavior of the agent. 5) Reward r t (s t-1 , a t-1 , s t ): the timely return value at time t. 6) Policy π(a|s): a function that the agent decides the action a according to the current state s. 7) State transition probability p(s ′ |s, a): the probability distribution that the environment will transfer to state s ′ at the next moment, after the agent selecting an action a based on the current state s. 8) p(s ′ , r|s, a): the probability that the agent transforms to state s ′ and obtains the reward r, where the agent is in state s and selecting the action a.</p><p>Many reinforcement learning problems can be described by Markov decision process (MDP) &lt; S, A, P, γ, r &gt; <ref type="bibr" target="#b38">[39]</ref>, in which S is state space, A is action space, P is state transition probability function, r is reward function and γ is the discount factor 0 &lt; γ &lt; 1. At each time, the agent accepts a state and selects the action from an action set according to the policy. The agent receives feedback from the environment and then moves to the next state. The goal of reinforcement learning is to find a strategy that allows us to get the maximum γ-discounted cumulative reward. The discounted return is calculated by</p><formula xml:id="formula_136">G t = ∞ k=0 γ k r t+k .<label>(99)</label></formula><p>People do not necessarily know the MDP behind the problem. From this point, reinforcement learning is divided into two categories. One is model-based reinforcement learning which knows the MDP of the whole model (including the transition probability P and reward function r), and the other is the model-free method in which the MDP is unknown. Systematic exploration is required in the latter methods.</p><p>The most commonly used value function is the state value function,</p><formula xml:id="formula_137">V π (s) = E π [G t |S t = s],<label>(100)</label></formula><p>which is the expected return of executing policy π from state s. The state-action value function is also essential which is the expected return for selecting action a under state s and policy π,</p><formula xml:id="formula_138">Q π (s, a) = E π [G t |S t = s, A t = a].<label>(101)</label></formula><p>The value function of the current state s can be calculated by the value function of the next state s ′ . The Bellman equations of V π (s) and Q π (s, a) describe the relation by</p><formula xml:id="formula_139">V π (s) = a π(a|s) s ′ ,r p(s ′ , r|s, a)[r(s, a, s ′ ) + γV π (s ′ )],<label>(102)</label></formula><formula xml:id="formula_140">Q π (s, a) = s ′ ,r p(s ′ , r|s, a)[r(s, a, s ′ ) + γ a ′ π(a ′ |s ′ )Q π (s ′ , a ′ )].<label>(103)</label></formula><p>There are many reinforcement learning methods based on value function. They are called value-based methods, which play a significant role in RL. For example, Q-learning <ref type="bibr" target="#b183">[184]</ref> and SARSA <ref type="bibr" target="#b184">[185]</ref> are two popular methods which use temporal difference algorithms. The policy-based approach is to optimize the policy π θ (a|s) directly and update the parameters θ by gradient descent <ref type="bibr" target="#b185">[186]</ref>.</p><p>The actor-critic algorithm is a reinforcement learning method combining policy gradient and temporal differential learning, which learns both a policy and a state value function. It estimates the parameters of two structures simultaneously.</p><p>1) The actor is a policy function, which is to learn a policy π θ (a|s) to obtain the highest possible return.</p><p>2) The critic refers to the learned value function V φ (s), which estimates the value function of the current policy, that is to evaluate the quality of the actor. In the actor-critic method, the critic solves a problem of prediction, while the actor pays attention to the control <ref type="bibr" target="#b186">[187]</ref>. There is more information of actor-critic method in <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b186">[187]</ref> The summary of the value-based method, the policy-based method, and the actor-critic method are as follows:</p><p>1) The value-based method: It needs to calculate value function, and usually gets a definite policy.</p><p>2) The policy-based method: It optimizes the policy π without selecting an action according to value function.</p><p>3) The actor-critic method: It combines the above two methods, and learns both the policy π and the state value function. Deep reinforcement learning (DRL) combines reinforcement learning and deep learning, which defines problems and optimizes goals in the framework of RL, and solves problems such as state representation and strategy representation using deep learning techniques.</p><p>DRL has achieved great success in many challenging control tasks and uses DNNs to represent the control policy. For neural network training, a simple stochastic gradient algorithm or other first-order algorithms are usually chosen, but these algorithms are not efficient in exploring the weight space, which makes DRL methods often take several days to train <ref type="bibr" target="#b59">[60]</ref>. So, a distributed method was proposed to solve this problem, in which parallel actor-learners have a stabilizing effect during training <ref type="bibr" target="#b181">[182]</ref>. It executes multiple agents to interact with the environment simultaneously, which reduces the training time. But this method ignores the sampling efficiency. A scalable and sample-efficient natural gradient algorithm was proposed, which uses a Kronecker-factored approximation method to compute the natural policy gradient update, and employ the update to the actor and the critic (ACKTR) <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization in Meta Learning</head><p>Meta learning <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> is a popular research direction in the field of machine learning. It solves the problem of learning to learn. In the past cognition, the research of machine learning is to obtain a large amount of data in a specific task firstly and then use the data to train the model. In machine learning, adequate training data is the guarantee of achieving good performance. However, human beings can well process new tasks with only a few training samples, which are much more efficient than traditional machine learning methods. The key point could be that the human brain has learned "how to learn" and can make full use of past knowledge and experience to guide the learning of new tasks. Therefore, how to make machines have the ability to learn efficiently like human beings has become a frontier issue in machine learning.</p><p>The goal of meta learning is to design a model that can training well in the new tasks using as few samples as possible without overfitting. The process of adapting to the new tasks is essentially a learning process in the meta-testing, but only with limited samples from new tasks. The application of meta learning methods in supervised learning can solve the few-shot learning problems <ref type="bibr" target="#b46">[47]</ref>.</p><p>As few-shot learning problems receive more and more attention, meta learning is also developing rapidly. In general, meta learning methods can be summarized into the following three types <ref type="bibr" target="#b47">[48]</ref>: metric-based methods <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, model-based methods <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> and optimization-based methods <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b46">[47]</ref>. In this subsection, we focus on the optimization-based meta learning methods. In meta learning, there are usually some tasks with sufficient training samples and a new task with only a few training samples. The main idea can be described as follows: in the meta-train step, sample a task τ from the total task set T , which contains The process of sampling tasks and updating parameters are repeated multiple times. In the meta-test step, the trained metaoptimizer is used for learning a new task.</p><p>Since the purpose of meta learning is to achieve fast learning, a key point is to make the gradient descent more accurately in the optimization. In some meta learning methods, the optimization process itself can be regarded as a learning problem to learn the prediction gradient rather than a determined gradient descent algorithm <ref type="bibr" target="#b187">[188]</ref>. Neural networks with original gradient as input and prediction gradient as output is often used as a meta-optimizer <ref type="bibr" target="#b54">[55]</ref>. The neural work is trained using the training and test samples from other tasks and used in the new task. The parameter update in the process of training is as follows:</p><formula xml:id="formula_141">θ t+1 = θ t + N (g(θ t ), φ), (<label>104</label></formula><formula xml:id="formula_142">)</formula><p>where θ t is the model parameter at the iteration t, and N is the meta-optimizer with parameter φ that learns how to predict the gradient. After training, the meta-optimizer N and its parameter φ are updated according to the loss value in the test samples. The experiments have confirmed that learning neural optimizers is advantageous compared to the most advanced adaptive stochastic gradient optimization methods used in deep learning <ref type="bibr" target="#b54">[55]</ref>. Due to the similarity between the gradient update in backpropagation and the cell state update in the long short-term memory (LSTM), LSTM is often used as the meta-optimizer <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>.</p><p>A model-agnostic meta learning algorithm (MAML) is another method for meta learning which was proposed to learn the parameters of any model subjected to gradient descent methods. It is applicable to different learning problems, including classification, regression and reinforcement learning <ref type="bibr" target="#b46">[47]</ref>. The basic idea of the model-agnostic algorithm is to begin multiple tasks at the same time, and then get the synthetic gradient direction of different tasks, so as to learn a common base model. The main process can be described as follows: in the meta-train step, multiple tasks batch τ i , which contains (D train i , D test i ), are extracted from the total task set T . For all τ i , train and update the parameter θ ′ i with the train samples D train i :</p><formula xml:id="formula_143">θ ′ i = θ -α ∂J τi (θ) ∂(θ) ,<label>(105)</label></formula><p>where α is the learning rate of training process and J τi (θ) is the loss function in task i with training samples D train i . After the training step, use the synthetic gradient direction of these parameters θ ′ i on the test samples D test i of the respective task to update parameter θ:</p><formula xml:id="formula_144">θ = θ -β ∂ τi∼p(T ) J τi (θ ′ i ) ∂(θ) , (<label>106</label></formula><formula xml:id="formula_145">)</formula><p>where β is the meta learning rate of the test process and J τi (θ) is the loss function in task i with test samples D test i . The metatrain step is repeated multiple times to optimize a good initial parameter θ. In the meta-test step, the trained parameter θ is used as the initial parameter such that the model has a maximal performance on the new task. MAML does not introduce additional parameters for meta learning, nor does it require a specific learner architecture. The development of the method is of great significance to the optimization-based meta learning methods. Recently, an expanded task-agnostic meta learning algorithm is proposed to enhance the generalization of metalearner towards a variety of tasks, which achieves outstanding performance on few-shot classification and reinforcement learning tasks <ref type="bibr" target="#b188">[189]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization in Variational Inference</head><p>In the machine learning community, there are many attractive probabilistic models but with complex structures and intractable posteriors, and thus some approximate methods are used, such as variational inference and Markov chain Monte Carlo (MCMC) sampling. Variational inference, a common technique in machine learning, is widely used to approximate the posterior density of the Bayesian model, which transforms intricate inference problems into highdimensional optimization problems <ref type="bibr" target="#b189">[190]</ref>, <ref type="bibr" target="#b190">[191]</ref>. Compared with MCMC, the variational inference is faster and more suitable for dealing with large-scale data. Variational inference has been applied to large-scale machine learning tasks, such as large-scale document analysis, computer vision and computational neuroscience <ref type="bibr" target="#b191">[192]</ref>.</p><p>Variational inference often defines a flexible family of distributions indexed by free parameters on latent variables <ref type="bibr" target="#b189">[190]</ref>, and then finds the variational parameters by solving an optimization problem. Now let us review the principle of variational inference <ref type="bibr" target="#b57">[58]</ref>. Variational inference approximates the true posterior by attempting to minimize the Kullback-Leibler (KL) divergence between a potential factorized distribution and the true posterior.</p><p>Let Z = {z i } represent the set of all latent variables and parameters in the model and X = {x i } be a set of all observed data. The joint likelihood of X and Z is p(Z, X) = p(Z)p(X|Z). In Bayesian models, the posterior distribution p(Z|X) should be computed to make further inference.</p><p>What we need to do is to approximate p(Z|X) with the distribution q(Z) that belongs to a constrained family of distributions. The goal is to make the two distributions as similar as possible. Variational inference chooses KL divergence to measure the difference between the two distributions, that is to minimize the KL divergence of q(Z) and p(Z|X). Here is the formula for the KL divergence between q and p:</p><formula xml:id="formula_146">KL[q(Z)||p(Z|X)] = E q log q(Z) p(Z|X) = E q [log q(Z)] -E q [log p(Z|X)] = E q [log q(Z)] -E q [log p(Z, X)] + log p(X) = -ELBO(q) + const,<label>(107)</label></formula><p>where log p(X) is replaced by a constant because we are only interested in q. With the above formula, we can know KL divergence is difficult to optimize because it requires knowing the distribution that we are trying to approximate. An alternative method is to maximize the evidence lower bound (ELBO), a lower bound on the logarithm of the marginal probability of the observations. We can obtain ELBO's formula as</p><formula xml:id="formula_147">ELBO(q) = E [log p(Z, X)] -E [log q(Z)] .<label>(108)</label></formula><p>Variational inference can be treated as an optimization problem with the goal of minimizing the evidence lower bound. A direct method is to solve this optimization problem using the coordinate ascent, which is called coordinate ascent variational inference (CAVI). CAVI iteratively optimizes each factor of the mean-field variational density, while holding the others fixed <ref type="bibr" target="#b191">[192]</ref>.</p><p>Specifically, variational distribution q has the structure of the mean-field, i.e., q(Z) = M i=1 q i (z i ). With this assumption, we can bring the distribution q into the ELBO, by some derivation according to <ref type="bibr" target="#b56">[57]</ref>, and obtain the following formula:</p><formula xml:id="formula_148">q * i ∝ exp{E -i [log p(z i , Z -i , X)]}.<label>(109)</label></formula><p>Then the CAVI algorithm can be given below in Algorithm 8.</p><p>Algorithm 8 Coordinate Ascent Variational Inference <ref type="bibr" target="#b191">[192]</ref> Input: p(X, Z), X Output:</p><formula xml:id="formula_149">q(Z) = M i=1 q i (z i ) Initialize Variational factors q i (z i ) repeat for i=1,2,3....,M do q * i ∝ exp{E -i [log p(z i , Z -i , X)]} end for Compute ELBO(q): ELBO(q) = E[log p(Z, X)] -E log q(Z) until ELBO converges</formula><p>In traditional coordinate ascension algorithms, the efficiency of processing large data is very low, because each iteration needs to compute all the data, which is very time-consuming. Modern machine learning models often need to analyze and process large-scale data, which is difficult and costly. Stochastic optimization enables machine learning to be extended on massive data <ref type="bibr" target="#b192">[193]</ref>. This reminds us of an attractive technique to handle large data sets: stochastic optimization <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b191">[192]</ref>, <ref type="bibr" target="#b193">[194]</ref>. By introducing stochastic optimization into variational inference, the stochastic variational inference (SVI) was proposed <ref type="bibr" target="#b57">[58]</ref>, in which the exponential family is taken as a typical example.</p><p>Gaussian process (GP) is an important machine learning method based on statistical learning and Bayesian theory. It is suitable for complex regression problems such as high dimensions, small samples, and nonlinearities. GP has the advantages of strong generalization ability, flexible nonparametric inference, and strong interpretability. However, the complexity and storage requirements of accurate solution for GP are high, which hinders the development of GP under large-scale data. The stochastic variational inference method introduced in this section can popularize variational inference on large-scale datasets, but it can only be applied to probabilistic models with factorized structures. For GPs whose observations are correlated with each other, the stochastic variational inference can be adapted by introducing the global inducing variables as variational variables <ref type="bibr" target="#b194">[195]</ref>, <ref type="bibr" target="#b195">[196]</ref>. Specifically, the observations are assumed to be conditionally independent given the inducing variables and the variational distribution for the inducing variables is assumed to have an explicit form. Thus, the resulting GP model can be factorized in a necessary manner, enabling the stochastic variational inference. This method can also be easily extended to models with non-Gaussian likelihood or latent variable models based on GPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Optimization in Markov Chain Monte Carlo</head><p>Markov chain Monte Carlo (MCMC) is a class of sampling algorithms to simulate complex distributions that are difficult to sample directly. It is a practical tool for Bayesian posterior inference. The traditional and common MCMC algorithms include Gibbs sampling, slice sampling, Hamiltonian Monte Carlo (HMC) <ref type="bibr" target="#b196">[197]</ref>, <ref type="bibr" target="#b197">[198]</ref>, Reimann manifold variants <ref type="bibr" target="#b198">[199]</ref>, and so on. These sampling methods are limited by the computational cost and are difficult to extend to large-scale data.This section takes HMC as an example to introduce the optimization in MCMC. The bottleneck of the HMC is that the gradient calculation is costly on large data sets.</p><p>We first introduce the derivation of HMC. Consider the random variable θ, which can be sampled from the posterior distribution, p(θ|D) ∝ exp(-U (θ)),</p><p>where D is the set of observations, and U is the potential energy function with the following formula:</p><formula xml:id="formula_151">U (θ) = -log p(θ|D) = - x∈D log p(x|θ) -log p(θ).<label>(111)</label></formula><p>In HMC <ref type="bibr" target="#b196">[197]</ref>, an independent auxiliary momentum variable r is introduced from Hamiltonian dynamic. The Hamiltonian function and the joint distribution of θ and r are described by</p><formula xml:id="formula_152">H(θ, r) = U (θ) + 1 2 r T M -1 r = U (θ) + K(r),<label>(112) p</label></formula><formula xml:id="formula_153">(θ, r) ∝ exp(-U (θ) - 1 2 r T M -1 r),<label>(113)</label></formula><p>where M denotes the mass matrix, and K(r) is the kinetic energy function. The process of HMC sampling is derived by simulating the Hamiltonian dynamic system,</p><formula xml:id="formula_154">dθ = M -1 rdt, dr = -∇U (θ)dt.<label>(114)</label></formula><p>Hamiltonian dynamic describes the continuous motion of a particle. Hamiltonian equations are numerically approximated by the discretized leapfrog integrator for practical simulating <ref type="bibr" target="#b196">[197]</ref>. The update equations are as follows <ref type="bibr" target="#b196">[197]</ref>:</p><formula xml:id="formula_155">   r i (t + ǫ 2 ) = r i (t) -ǫ 2 dr(t), θ i (t + ǫ) = θ i (t) + ǫdθ(t + ǫ 2 ), r i (t + ǫ) = r i (t + ǫ 2 ) -ǫ 2 dr(t + ǫ).<label>(115)</label></formula><p>In the case of large datasets, the gradient of U (θ) needs to be calculated on the entire data set in each leapfrog iteration. In order to improve the efficiency, the stochastic gradient method was used to calculate ∇U (θ) with a mini-batch D sampled uniformly from D, which reduces the cost of calculation <ref type="bibr" target="#b60">[61]</ref>. However, the gradient calculated in a mini-batch instead of the full dataset will cause noise. According to the central limit theorem, this noisy gradient can be approximated as</p><formula xml:id="formula_156">∇ Ũ (θ) ≈ ∇U (θ) + N (0, V (θ)),<label>(116)</label></formula><p>where gradient noise obeys normal distribution whose covariance is V (θ). If we replace ∇U (θ) by ∇ Ũ (θ) directly, the Hamiltonian dynamics will be changed as</p><formula xml:id="formula_157">dθ = M -1 rdt, dr = -∇U (θ)dt + N (0, 2B(θ)dt),<label>(117)</label></formula><p>where B(θ) = 1 2 ǫV (θ) is the diffusion matrix <ref type="bibr" target="#b60">[61]</ref>. Since the discretization of the dynamical system introduces noise, the Metropolis-Hastings (MH) correction step should be done after the leapfrog step. These MH steps require expensive calculations overall data in each iteration. Beyond that, there is an incorrect stationary distribution <ref type="bibr" target="#b199">[200]</ref> in the stochastic gradient variant of HMC. Thus, Hamiltonian dynamic was further modified, which minimizes the effect of the additional noise, achieves the invariant distribution and eliminates MH steps <ref type="bibr" target="#b60">[61]</ref>. Specifically, a friction term is added to the dynamical process of momentum update:</p><formula xml:id="formula_158">dθ = M -1 rdt, dr = -∇U (θ)dt -BM -1 rdt + N (0, 2B(θ)dt).<label>(118)</label></formula><p>The introduced friction term is helpful for decreasing total energy H(θ, r) and weakening the effects of noise in the momentum update phase. The dynamical system is also the type of second-order Langevin dynamics with friction in physics, which can explore efficiently and counteract the effect of the noisy gradients <ref type="bibr" target="#b60">[61]</ref> and thus no MH correction is required. This second-order Langevin dynamic MCMC method, called SGHMC, is used to deal with sampling problems on large data sets <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b200">[201]</ref>.</p><p>Moreover, HMC is highly sensitive to hyper-parameters, such as the path length (step number) L and the step size ǫ. If the hyper-parameters are not set properly, the efficiency of the HMC will drop dramatically. There are some methods to optimize these two hyper-parameters instead of manually setting them.</p><p>1) Path Length L: The value of path length L has a great influence on the performance of HMC. If L is too small, the distance between the resulting sample points will be very close; if L is too large, the resulting sample points will loop back, resulting in wasted computation. In general, manually setting L cannot maximize the sampling efficiency of the HMC.</p><p>Matthew et al. <ref type="bibr" target="#b201">[202]</ref> proposed an extension of the HMC method called the No-U-Turn sampler (NUTS), which uses a recursive algorithm to generate a set of possible independent samples efficiently, and stops the simulation by discriminating the backtracking automatically. There is no need to set the step parameter L manually. In models with multiple discrete variables, the ability of NUTS to select the track length automatically allows it to generate more valid samples and perform more efficiently than the original HMC.</p><p>2) Adaptive Step Size ǫ: The performance of HMC is highly sensitive to the step size ǫ in leapfrog integrator. If ǫ is too small, the update will slow, and the calculation cost will be high; if ǫ is too large, the rejection rate will be high, resulting in useless updates.</p><p>To set ǫ reasonably and adaptively, a vanishing adaptation of the dual averaging algorithm can be used in HMC <ref type="bibr" target="#b202">[203]</ref>, <ref type="bibr" target="#b203">[204]</ref>. Specifically, a statistic H t = δα t is adopted in dual averaging method, where δ is the desired average acceptance probability, and α t is the current Metropolis-Hasting acceptance probability for iteration t. The statistic H t 's expectation h(ǫ) is defined as</p><formula xml:id="formula_159">h(ǫ) ≡ E t [H t |ǫ t ] ≡ lim T →∞ 1 T E[H t |ǫ t ],<label>(119)</label></formula><p>where ǫ t is the step size for iteration t in the leapfrog integrator. To satisfy h(ǫ) ≡ E t [H t |ǫ t ] = 0, we can derive the update formula of ǫ, i.e., ǫ t+1 = ǫ tη t H t . Tuning ǫ by vanishing adaptation algorithm guarantees that the average acceptance probability of Metropolis verges to a fixed value. The hyper-parameters in the HMC include not only the step size ǫ and the length of iteration steps L, but also the mass M , etc. Optimizing these hyper-parameters can help improve sampling performance <ref type="bibr" target="#b198">[199]</ref>, <ref type="bibr" target="#b204">[205]</ref>, <ref type="bibr" target="#b205">[206]</ref>. It is convenient and efficient to tune the hyper-parameters automatically without cumbersome adjustments based on data and variables in MCMC. These adaptive tuning methods can be applied to other MCMC algorithms to improve the performance of the samplers.</p><p>In addition to second-order SGHMC, stochastic gradient Langevin dynamics (SGLD) <ref type="bibr" target="#b206">[207]</ref> is a first-order Langevin dynamic technique combined with stochastic optimization. Efficient variants of both SGLD and SGHMC are still active <ref type="bibr" target="#b200">[201]</ref>, <ref type="bibr" target="#b207">[208]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CHALLENGES AND OPEN PROBLEMS</head><p>With the rise of practical demand and the increase of the complexity of machine learning models, the optimization methods in machine learning still face challenges. In this part, we discuss open problems and challenges for some optimization methods in machine learning, which may offer suggestions or ideas for future research and promote the wider application of optimization methods in machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Challenges in Deep Neural Networks</head><p>There are still many challenges while optimizing DNNs. Here we mainly discuss two challenges with respect to data and model, respectively. One is insufficient data in training, and the other is a non-convex objective in DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Insufficient Data in Training Deep</head><p>Neural Networks: In general, deep learning is based on big data sets and complex models. It requires a large number of training samples to achieve good training effects. But in some particular fields, finding a sufficient amount of training data is difficult. If we do not have enough data to estimate the parameters in the neural networks, it may lead to high variance and overfitting.</p><p>There are some techniques in neural networks that can be used to reduce the variance. Adding L 2 regularization to the objective is a natural method to reduce the model complexity. Recently, a common method is dropout <ref type="bibr" target="#b61">[62]</ref>. In the training process, each neuron is allowed to stop working with a probability of p, which can prevent the synergy between certain neurons. M subnets can be sampled like bagging by multiple puts and returns <ref type="bibr" target="#b208">[209]</ref>. Each expected result at the output layer is calculated as</p><formula xml:id="formula_160">o = E M [f (x; θ, M )] = M i=1 p(M i )f (x; θ, M i ),<label>(120)</label></formula><p>where p(M i ) is the probability of the ith subnet. Dropout can prevent overfitting and improve the generalization ability of the network, but its disadvantage is increasing the training time as each training changes from the full network to a sub-network <ref type="bibr" target="#b209">[210]</ref>.</p><p>Not only overfitting but also some training details will affect the performance of the model due to the complexity of the DNNs. The improper selection of the learning rate and the number of iterations in the SGD will make the model unable to converge, which makes the accuracy of model fluctuate greatly. Besides, taking an inappropriate black box of neural network construction may result in training not being able to continue, so designing an appropriate neural network model is particularly important. These impacts are even greater when data are insufficient.</p><p>The technology of transfer learning <ref type="bibr" target="#b210">[211]</ref> can be applied to build networks in the scenario of insufficient data. Its idea is that the models trained from other data sources can be reused in similar target fields after certain modifications and improvements, which dramatically alleviates the problems caused by insufficient datasets. Moreover, the advantages brought by transfer learning are not limited to reducing the need for sufficient training data, but also can avoid overfitting effectively and achieve better performance in general. However, if target data is not as relevant to the original training data, the transferred model does not bring good performance.</p><p>Meta learning methods can be used for systematically learning parameter initialization, which ensures that training begins with a suitable initial model. However, it is necessary to ensure the correlation between multiple tasks for meta-training and tasks for meta-testing. Under the premise of models with similar data sources for training, transfer learning and meta learning can overcome the difficulties caused by insufficient training data in new data sources, but these methods usually introduce a large number of parameters or complex parameter adjustment mechanisms, which need to be further improved for specific problems. Therefore, using insufficient data for training DNNs is still a challenge.</p><p>2) Non-convex Optimization in Deep Neural Network: Convex optimization has good properties and a comprehensive set of tools are open to solve the optimization problem. However, many machine learning problems are formulated as non-convex optimization problems. For example, almost all the optimization problems in DNNs are non-convex. Non-convex optimization is one of the difficulties in the optimization problem. Unlike convex optimization, there may be innumerable optimum solutions in its feasible domain in non-convex problems. The complexity of the algorithm for searching the global optimal value is NP-hard <ref type="bibr" target="#b108">[109]</ref>.</p><p>In recent years, non-convex optimization has gradually attracted the attention of researches. The methods for solving non-convex optimization problems can be roughly divided into two types. One is to transform the non-convex optimization into a convex optimization problem, and then use the convex optimization method. The other is to use some special optimization method for solving non-convex functions directly. There is some work on summarizing the optimization methods for solving non-convex functions from the perspective of machine learning <ref type="bibr" target="#b211">[212]</ref>.</p><p>1) Relaxation method: Relax the problem to make it become a convex optimization problem. There are many relaxation techniques, for example, the branch-andbound method called αBB convex relaxation <ref type="bibr" target="#b212">[213]</ref>, <ref type="bibr" target="#b213">[214]</ref>, which uses a convex relaxation at each step to compute the lower bound in the region. The convex relaxation method has been used in many fields. In the field of computer vision, a convex relaxation method was proposed to calculate minimal partitions <ref type="bibr" target="#b214">[215]</ref>. For unsupervised and semi-supervised learning, the convex relaxation method was used for solving semidefinite programming <ref type="bibr" target="#b215">[216]</ref>. 2) Non-convex optimization methods: These methods include projection gradient descent <ref type="bibr" target="#b216">[217]</ref>, <ref type="bibr" target="#b217">[218]</ref>, alternating minimization <ref type="bibr" target="#b218">[219]</ref>, <ref type="bibr" target="#b219">[220]</ref>, <ref type="bibr" target="#b220">[221]</ref>, expectation maximization algorithm <ref type="bibr" target="#b221">[222]</ref>, <ref type="bibr" target="#b222">[223]</ref> and stochastic optimization and its variants <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Difficulties in Sequential Models with Large-Scale Data</head><p>When dealing with large-scale time series, the usual solutions are using stochastic optimization, processing data in mini-batches, or utilizing distributed computing to improve computational efficiency <ref type="bibr" target="#b223">[224]</ref>. For a sequential model, segmenting the sequences can affect the dependencies between the data on the adjacent time indices. If sequence length is not an integral multiple of the mini-batch size, the general operation is to add some items sampled from the previous data into the last subsequence. This operation will introduce the wrong dependency in the training model. Therefore, the analysis of the difference between the approximated solution obtained and the exact solution is a direction worth exploring.</p><p>Particularly, in RNNs, the problem of gradient vanishing and gradient explosion is also prone to occur. So far, it is generally solved by specific interaction modes of LSTM and GRU <ref type="bibr" target="#b224">[225]</ref> or gradient clipping. Better appropriate solutions for dealing with problems in RNNs are still worth investigating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. High-Order Methods for Stochastic Variational Inference</head><p>The high-order optimization method utilizes curvature information and thus converges fast. Although computing and storing the Hessian matrices are difficult, with the development of research, the calculation of the Hessian matrix has made great progress <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b225">[226]</ref>, and the second-order optimization method has become more and more attractive. Recently, stochastic methods have also been introduced into the second-order method, which extends the second order method to large-scale data <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>We have introduced some work on stochastic variational inference. It introduces the stochastic method into variational inference, which is an interesting and meaningful combination. This makes variational inference be able to handle large-scale data. A natural idea is whether we can incorporate secondorder optimization methods (or higher-order) into stochastic variational inference, which is interesting and challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Stochastic Optimization in Conjugate Gradient</head><p>Stochastic methods exhibit powerful capabilities when dealing with large-scale data, especially for first-order optimization <ref type="bibr" target="#b226">[227]</ref>. Then the relevant experts and scholars also introduced this stochastic idea to the second-order optimization methods <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b124">[125]</ref>, <ref type="bibr" target="#b227">[228]</ref> and achieved good results.</p><p>Conjugate gradient method is an elegant and attractive algorithm, which has the advantages of both the firstorder and second-order optimization methods. The standard form of a conjugate gradient is not suitable for a stochastic approximation. Through using the fast Hessiangradient product, the stochastic method is also introduced to conjugate gradient, in which some numerical results show the validity of the algorithm <ref type="bibr" target="#b226">[227]</ref>. Another version of stochastic conjugate gradient method employs the variance reduction technique, and converges quickly with just a few iterations and requires less storage space during the running process <ref type="bibr" target="#b228">[229]</ref>. The stochastic version of conjugate gradient is a potential optimization method and is still worth studying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper introduces and summarizes the frequently used optimization methods from the perspective of machine learning, and studies their applications in various fields of machine learning. Firstly, we describe the theoretical basis of optimization methods from the first-order, high-order, and derivative-free aspects, as well as the research progress in recent years. Then we describe the applications of the optimization methods in different machine learning scenarios and the approaches to improve their performance. Finally, we discuss some challenges and open problems in machine learning optimization methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>where Con x = Ax + By tb and Con y = Ax t+1 + Byb.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>For task τ , train and update the optimizer parameter θ with the training samples D train τ , update the meta-optimizer parameter φ with the test samples D test τ .</figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported by <rs type="funder">NSFC</rs> Project <rs type="grantNumber">61370175</rs> and <rs type="person">Shanghai Sailing</rs> Program <rs type="grantNumber">17YF1404600</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_txB9qVN">
					<idno type="grant-number">61370175</idno>
				</org>
				<org type="funding" xml:id="_WkVaCWZ">
					<idno type="grant-number">17YF1404600</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kidambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditioning of quasi-Newton methods for function minimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Shanno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="647" to="656" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured quasinewton methods for optimization with orthogonality constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2239" to="2269" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compatible natural gradient policy search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pajarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Akrour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quasi-Newton methods, motivation and theory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><surname>Moré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="46" to="89" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning via Hessian-free optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Roosta-Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04738</idno>
		<title level="m">Sub-sampled Newton methods II: local convergence rates</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Subsampled Newton methods with non-uniform sampling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roosta-Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3000" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exact and inexact subsampled newton methods for optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bollapragada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA Journal of Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Derivative-free optimization: a review of algorithms and comparison of software implementations</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Sahinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1247" to="1293" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Derivative-free optimization of noisy functions via quasi-newton methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Berahas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="965" to="993" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2873" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action recognition in video sequences using deep bi-directional LSTM with CNN features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1155" to="1166" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A bi-projection neural network for solving constrained quadratic optimization problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="224" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A complex-valued projection neural network for constrained optimization of real functions in complex variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3227" to="3238" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust regression estimation based on lowdimensional recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5935" to="5946" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two projection neural networks with reduced model complexity for nonlinear programming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">AdaDelta: An adaptive learning rate method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the convergence of Adam and beyond</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A stochastic gradient method with an exponential convergence rate for finite training sets</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2663" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving generalization performance by switching from Adam to SGD</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07628</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learn to swing up and balance a real pole based on raw visual input data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="126" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Playing Atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends in Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning: an overview</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schukat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Howley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAI Intelligent Systems Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="426" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>München, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Metalearning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="46" to="50" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Model vs optimization meta learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<ptr target="http://metalearning-symposium.ml/files/vinyals.pdf" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning WorkShop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Prototypical networks for fewshot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scalable trust-region method for deep reinforcement learning using Kroneckerfactored approximation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5279" to="5288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stochastic gradient Hamiltonian Monte Carlo</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1683" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multichannel variable-size convolution for sentence classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Language Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="204" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Gaussian process approach to remote sensing image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="186" to="197" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Algorithm AS 136: A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">ROCK: A robust clustering algorithm for categorical attributes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="345" to="366" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Adaptive dimension reduction for clustering high dimensional data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Multimodal semi-supervised learning for image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="902" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Semi-supervised classification by low density separation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Semi-supervised regression with co-training</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="908" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Semi-supervised clustering using genetic algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Demiriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Neural Networks in Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="814" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Semi-supervised graph clustering: a kernel approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Semi-supervised dimensionality reduction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="629" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Semi-supervised double sparse graphs based discriminant analysis for dimensionality reduction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="361" to="378" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Semi-supervised support vector machines</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demiriz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information processing systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="368" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Optimization Methods for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cheung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Optimization techniques for semi-supervised support vector machines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="203" to="233" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Branch and bound for semi-supervised support vector machines</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Convex and scalable weakly labeled svms</title>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2151" to="2188" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A survey of recent advances in hierarchical clustering algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A fast and robust general purpose clustering algorithm</title>
		<author>
			<persName><forename type="first">V</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery in Databases and Data Mining</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="208" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A clustering technique for summarizing multivariate data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="153" to="155" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geladi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
		<respStmt>
			<orgName>Chemometrics and Intelligent Laboratory Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Encyclopedia of Statistical Science</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1094" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A parallel gradient descent method for learning in analog VLSI neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alspector</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lippe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="836" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Problem Complexity and Method Efficiency in Optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Yudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Robust stochastic approximation approach to stochastic programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1574" to="1609" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Information-theoretic lower bounds on the oracle complexity of convex optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Learning rate schedules for faster stochastic gradient search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Darken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Signal Processing</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Training recurrent neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Ontario, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Natasha 2: Faster non-convex optimization than SGD</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2675" to="2686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Escaping from saddle pointsonline stochastic gradient for tensor decomposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="797" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">A method for unconstrained convex minimization problem with the rate of convergence O( 1 k 2 )</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Doklady Akademii Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Gradient descent for general reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="968" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Note on learning rate schedules for stochastic optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Darken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="832" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Minimizing finite sums with the stochastic average gradient</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="83" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Variance reduction for faster non-convex optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="699" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Stochastic variance reduction for nonconvex optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A method for nonlinear constraints in minimization problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization</title>
		<imprint>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Transportation network policy modeling with goal targets and generalized penalty functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagurney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramanujam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Alternating direction method with self-adaptive penalty parameters for monotone variational inequalities</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="337" to="356" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Snapvx: A network-based convex optimization solver</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">An ADMM algorithm for a class of total variation regularized estimation problems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annergren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1203.1828</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Revisiting Frank-Wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="427" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">A modified Frank-Wolfe algorithm for solving the traffic assignment problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="169" to="177" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">The Traffic Assignment Problem: Models and Methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Patriksson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Clarkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Algorithms</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="63" to="96" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">SPAMS: A sparse modeling software, version 2.3</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<ptr target="http://spams-devel.gforge.inria.fr/downloads.html" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A stochastic quasi-Newton method for online convex optimization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="436" to="443" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">A stochastic quasi-method for large-scale optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1008" to="1031" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A linearly-convergent stochastic L-BFGS algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="249" to="258" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Methods of conjugate gradients for solving linear systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Hestenes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stiefel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952">1952</date>
			<publisher>NBS</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">An introduction to the conjugate gradient method without the agonizing pain</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Shewchuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Nonlinear Programming: Analysis and Methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Avriel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A damped-Newton method for the linear complementarity problem</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Harker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lectures in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="265" to="284" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">A combined method for determining reaction paths, minima, and transition state geometries</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Ayala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Schlegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="375" to="384" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">The barzilai and borwein gradient method for the large scale unconstrained minimization problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raydan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="26" to="33" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Variable metric method for minimization</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Davidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">A rapidly convergent descent method for minimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="163" to="168" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">The convergence of a class of double-rank minimization algorithms: The new algorithm</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Broyden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA Journal of Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="222" to="231" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A new approach to variable metric algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="317" to="322" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A family of variable-metric methods derived by variational means</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="23" to="26" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Updating quasi-Newton matrices with limited storage</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="773" to="782" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical programming</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Optimization theory and methods: nonlinear programming</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A multi-batch L-BFGS method for machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Berahas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takác</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1055" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">The tradeoffs of large scale learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Optimization methods for large-scale machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics Review</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="223" to="311" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Res: Regularized stochastic BFGS algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="6089" to="6104" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Global convergence of online limited memory BFGS</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3151" to="3181" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Stochastic block BFGS: Squeezing more curvature out of data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1869" to="1878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">On the use of stochastic Hessian information in optimization methods for machine learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Neveitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="977" to="995" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">New insights and perspectives on the natural gradient method</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1193</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Scaling up natural gradient by sparsely factorizing the inverse fisher matrix</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with Kronecker-factored approximate curvature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2408" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">A trust region method based on interior point techniques for nonlinear programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="149" to="185" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">Practical techniques for nonlinear optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Northwestern University, America</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">A brief description of the levenberg-marquardt algorithm implemented by levmar</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Lourakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundation of Research and Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Introduction to Derivative-Free Optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Conn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Vicente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Audet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kokkolaras</surname></persName>
		</author>
		<title level="m">Blackbox and Derivative-Free Optimization: Theory, Algorithms and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Derivative-free optimization: a review of algorithms and comparison of software implementations</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Sahinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1247" to="1293" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">An Introduction to Genetic Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Ant Colony Optimization and Swarm Intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Birattari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stützle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Winfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">Nonlinear Programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
			<affiliation>
				<orgName type="collaboration">Athena Scientific Belmont</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Iteration complexity of randomized blockcoordinate descent methods for minimizing a composite function</title>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Adaptive coordinate descent</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schoenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Genetic and Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="885" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Approximate sparsity patterns for the inverse of a matrix and preconditioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huckle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="291" to="303" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Preconditioning techniques for large linear systems: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Benzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="418" to="477" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">CVX: Matlab software for disciplined convex programming, version 2.1</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<ptr target="http://cvxr.com/cvx" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Cvxpy: A python-embedded modeling language for convex optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2909" to="2913" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Cvxopt: A python package for convex optimization, version 1.1.6</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<ptr target="https://cvxopt.org/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Nonlinear modeling, estimation and predictive control in apmonitor</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hedengren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Shishavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Edgar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Chemical Engineering</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="133" to="148" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for largescale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Incorporating nesterov momentum into adam</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Fixing weight decay regularization in Adam</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Normalized direction-preserving Adam</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04546</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Recent advances in recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salehinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barfett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Valaee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01078</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8624" to="8628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Training deep and recurrent networks with Hessian-free optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="479" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Fast curvature matrix-vector products for secondorder gradient descent</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1723" to="1738" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Learning recurrent neural networks with Hessian-free optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Training the random neural network using quasi-Newton methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stafylopatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="331" to="339" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Limited-memory bfgs optimization of recurrent neural network language models for speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6114" to="6118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">On-line Q-learning using connectionist systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Cambridge University Engineering Department, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning: An overview</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07274</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Natural actor-critic algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2471" to="2482" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<monogr>
		<title level="m" type="main">Learning to Learn</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Large scale online learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m" type="main">Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Spall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Gaussian processes for big data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intellegence</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="282" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Scalable variational gaussian process classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G D G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Hybrid monte carlo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roweth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters B</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="page" from="216" to="222" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">MCMC using Hamiltonian dynamics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of Markov Chain Monte Carlo</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="113" to="162" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Riemann manifold langevin and hamiltonian monte carlo methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calderhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="123" to="214" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">The fundamental incompatibility of scalable Hamiltonian monte carlo and naive data subsampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="533" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Bayesian posterior sampling via stochastic gradient fisher scoring</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1591" to="1598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">The No-U-turn sampler: adaptively setting path lengths in Hamiltonian monte carlo</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1593" to="1623" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Primal-dual subgradient methods for convex problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="221" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">A tutorial on adaptive MCMC</title>
		<author>
			<persName><forename type="first">C</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thoms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="343" to="373" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Stan: A probabilistic programming language</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Riddell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">MCMC methods for functions: modifying old algorithms to make them faster</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">O</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="424" to="446" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient Langevin dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Bayesian sampling using stochastic gradient thermostats</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babbush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Skeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3203" to="3211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Non-convex optimization for machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="142" to="336" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">A global optimization method, αbb, for general twice-differentiable constrained NLPs-I. theoretical advances</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Adjiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dallwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Chemical Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1137" to="1158" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Mixed-integer nonlinear optimization in process synthesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Adjiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schweiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Floudas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of combinatorial optimization</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">A convex relaxation approach for computing minimal partitions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="810" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Unsupervised and semi-supervised multiclass support vector machines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<monogr>
		<title level="m" type="main">Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.03025</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<title level="m" type="main">Provable non-convex projected gradient descent for a class of constrained matrix optimization problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrillidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01316</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Low-rank matrix completion using alternating minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Annual Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Understanding alternating minimization for matrix completion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="651" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Fast matrix completion without the condition number</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wootters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="638" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Statistical guarantees for the em algorithm: From population to sample-based analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="77" to="120" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<monogr>
		<title level="m" type="main">High dimensional expectationmaximization algorithm: Statistical optimization and asymptotic normality</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.8729</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b223">
	<monogr>
		<title level="m" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04836</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b225">
	<monogr>
		<title level="m" type="main">Second-Order Optimization For Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>University of Toronto (Canada</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Conjugate directions for stochastic gradient descent</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1351" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">SGD-QN: Careful quasi-Newton stochastic gradient descent</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1737" to="1754" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Stochastic conjugate gradient algorithm with variance reduction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
