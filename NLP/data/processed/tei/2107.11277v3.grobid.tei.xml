<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine Learning with a Reject Option: A survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-02-21">21 Feb 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hendrickx</forename><surname>Kilian</surname></persName>
							<email>kilian.hendrickx@cs.kuleuven.be</email>
						</author>
						<author>
							<persName><forename type="first">Perini</forename><surname>Lorenzo</surname></persName>
							<email>lorenzo.perini@cs.kuleuven.be</email>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Van Der Plas Dries</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Meert</forename><surname>Wannes</surname></persName>
							<email>wannes.meert@cs.kuleuven.be</email>
						</author>
						<author>
							<persName><forename type="first">Davis</forename><surname>Jesse</surname></persName>
							<email>jesse.davis@cs.kuleuven.be</email>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Perini</surname></persName>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Van Der Plas</surname></persName>
							<email>dries.vanderplas@cs.kuleuven.be</email>
						</author>
						<author>
							<persName><forename type="first">W</forename><surname>Meert</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Davis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kilian</forename><surname>Hendrickx</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Siemens Digital Industries Software</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">OSG bv</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Natus Medical</orgName>
								<address>
									<settlement>Kontich</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Antwerp</orgName>
								<address>
									<settlement>Antwerp</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">KU Leuven; Leuven.AI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">KU Leuven; Leuven.AI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Machine Learning with a Reject Option: A survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-21">21 Feb 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">69D59153C9D2A6973440E4DB6CF302BF</idno>
					<idno type="arXiv">arXiv:2107.11277v3[cs.LG]</idno>
					<note type="submission">Received: 23 July 2021 / Revised: 27 July 2023 &amp; 15 February 2024</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake.</p><p>This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>application domains and show how machine learning with rejection relates to other machine learning research areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The canonical task in machine learning is to learn a predictive model that captures the relationship between a set of input variables and a target variable on the basis of training data. Machine-learned models are powerful because, after training, they offer the ability to make accurate predictions about future examples. Since this enables automating a number of tasks that are difficult and/or time-consuming, such models are ubiquitously deployed.</p><p>However, their key functionality of always returning a prediction for a given novel input is also a drawback. While the model may produce accurate predictions in general, in certain circumstances this may not be the case. For instance, there could be certain regions of the feature space where the model struggles to differentiate among the different classes. Or the current test example could be highly dissimilar to the data used to train the model. In certain application domains, such as medical diagnostics <ref type="bibr" target="#b119">(Kotropoulos and Arce 2009)</ref> and engineering <ref type="bibr" target="#b228">(Zou et al. 2011)</ref>, mispredictions can have serious consequences. Therefore, it would be beneficial for a model to be cautious in situations where it is uncertain about its predictions. The prediction task could be deferred to a human expert in these situations.</p><p>One way to accomplish this is to use machine learning models with rejection. Such models assess their confidence in each prediction and have the option to abstain from making a prediction when they are likely to make a mistake. This ability to abstain from making a prediction has several benefits. First, by only making predictions when it is confident, it can result in improved performance for the retained examples <ref type="bibr" target="#b171">(Pudil et al. 1992)</ref>. Second, avoiding mispredictions can increase a user's trust in the system (El-Yaniv and Wiener 2010). Third, it can still result in time savings by only requiring human interventions to make decisions in a small number of cases. Fourth, avoiding strongly biased predictions helps build a more fair model <ref type="bibr" target="#b129">(Lee et al. 2021;</ref><ref type="bibr" target="#b176">Ruggieri et al. 2023)</ref>. This machine learning subfield was already studied in 1970 by <ref type="bibr" target="#b22">Chow (1970)</ref> and <ref type="bibr" target="#b99">Hellman (1970)</ref>. However, the proliferation of applications has resulted in renewed interest in this area.</p><p>This survey aims to provide an overview of the subfield of machine learning with rejection, which we structure around eight key research questions.</p><p>Q1. How can we formalize the conditions for which a model should abstain from making a prediction? Q2. How can we evaluate the performance of a model with rejection? Q3. What architectures are possible for operationalizing (i.e., putting this into practice) the ability to abstain from making a prediction? Q4. How do we learn models with rejection? Q5. What are the main pros and cons of using a specific architecture? Q6. How can we combine multiple rejectors? Q7. Where does the need for machine learning with rejection methods arise in real-world applications? Q8. How does machine learning with rejection relate to other research areas?</p><p>In addition to the individual contributions of addressing each of these research questions, our major contribution is that we identify the main characteristics of machine learning models with rejection, allowing us to structure the methods in this research field. By providing an overview of the research field as well as deeper insights into the various techniques, we aid in further advance this research area, as well as its adaptation to real-world applications.</p><p>The remainder of this paper is structured as follows. In Section 2, we formalize the setting in which machine learning with rejection operates and identify the two main motivations to abstain from making a prediction (Q1). Section 3 introduces the means to evaluate the performance of models with rejection (Q2). Sections 4, 5, and 6 provide a structured overview of the actionable techniques to reject based on the relevant literature. In these sections, we focus on describing the architecture (Q3), the rejector's learning (Q4), and the key pros and cons (Q5). In Section 7 we explore how to combine multiple rejectors to allow different types of rejection (Q6) Section 8 discusses the main application fields (Q7), while Section 9 explores the relation of machine learning with rejection with other research areas (Q8). Finally, Section 10 summarizes our conclusions and lists the main open research questions.</p><p>2 The learning with reject problem setting</p><p>In the standard supervised setting, a learner has access to a training set D = {(x 1 , y 1 ), . . . , (xn, yn)}, where each x i is a d dimensional vector and y i is the target. The training data is assumed to be independent and identically distributed <ref type="bibr">(i.i.d.)</ref> according to some unknown probability measure P (with density p(X, Y )). More generally, we denote the feature space as X and the target space as Y, which could be discrete Y = {1, 2, . . . , K}, continuous Y = R, or even probabilistic Y = [0, 1].</p><p>The assumption is that there is an unknown, non-deterministic function f : X → Y that maps the examples to their target value. Given a hypothesis space H of functions h : X → Y, the goal of a learner is to find a good approximation to f . Typically, this can be done by finding a model h ∈ H with a small expected risk R which is usually approximated using the training data R(h) := X ×Y L(h(x), y)dP (x, y)</p><formula xml:id="formula_0">≈ n i=1 L(h(x i ), y i ) n ,<label>(1)</label></formula><p>where L is a suitable loss function such as the squared or zero-one loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Models with a reject option</head><p>In learning with rejection, the output space of the model is extended to include a new value ® <ref type="bibr" target="#b38">(Cortes et al. 2016a</ref><ref type="bibr" target="#b37">(Cortes et al. , 2018;;</ref><ref type="bibr" target="#b79">Gamelas Sousa et al. 2014a)</ref>. This new symbol means that the model abstains from making a prediction. When performing classification with rejection, which is also called cautious classification, this new output ® can be seen as an additional class <ref type="bibr">(Ferri and Hernández-Orallo 2004)</ref>. Conceptually, since h only approximates the true underlying model, there are likely regions of X where h systematically differs from f . Specifically, discrepancies between h and f can be due to inconsistent data (e.g., classes overlapping), insufficient data (e.g., unexplored regions), or even incorrect model assumptions (e.g., h must be linear while f is not). Therefore, the goal of rejection is to determine such regions in order to abstain from making likely inaccurate predictions.</p><p>Formally, a model with rejection m : X → Y ∪ {®} is represented by a pair (h, r), where h : X → Y is the predictor and r : R → R is the rejector. Note that the rejector may use a variety of different inputs such as examples (R = X ), confidence or probability values (R = [0, 1]), or even both (R = X × [0, 1]). At prediction time, m outputs the symbol ® and abstains from making predictions when the rejector r determines that the predictor is at a heightened risk of making a misprediction and otherwise returns the predictor's output:</p><formula xml:id="formula_1">m(x) = ® if the prediction is rejected ; h(x) if the prediction is accepted .</formula><p>(2)</p><p>At prediction time, the key design decision for a model with rejection is how to structure the relationship between the predictor and rejector. Based on our analysis of the literature, we have identified three common architectural principles.</p><p>Separated rejector architecture. The rejector operates independently from the predictor. The most typical operationalization of this architecture lets the rejector serve as a filter that decides whether to pass a test example to the predictor. Figure <ref type="figure">1</ref> shows the test time data flow of a separated rejector. Dependent rejector architecture. Here, the rejector bases its decision on the output of the predictor. For instance, the dependent rejector can look at how close a prediction is to the predictor's decision boundary and abstain if it is too close. Figure <ref type="figure" target="#fig_0">2</ref> shows the data flow for the dependent rejector. Integrated rejector architecture. This principle involves integrating the rejector and predictor into a single model m by treating rejection as an additional class that can be returned by the model m. Thus, it is impossible to distinguish between the role of the predictor h and the rejector r. Figure <ref type="figure" target="#fig_1">3</ref> illustrates this scenario.</p><p>Fig. <ref type="figure">1</ref>: Data flow in a separated rejector, in which both the predictor and the rejector are stand-alone models. The rejector is a filter and only passes accepted examples to the predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Types of rejection</head><p>At a high-level, a learned predictor can exhibit (high) uncertainty in its predictions for four reasons:</p><p>R1. There can be cases where a vector x i ∈ X is associated with multiple values from the target space Y. This can arise in situations such as when classes overlap in classification tasks.  R2. Some instances in the training data are incorrect (e.g., the values of certain features were recorded incorrectly, labeling errors). R3. P (X, Y ) might differ between the training phase and deployment (e.g., concept drift). Consequently, the training data is no longer representative. R4. Some examples x could simply not be acquired due to their inherent rarity (anomalies, out-of-distribution).</p><p>Based on this intuition, two types of rejections can be performed:</p><p>Ambiguity Rejection: occurs if x falls in a region where the target y is ambiguous (R1 and R2). This often occurs in regions that are close to the decision boundary in classification tasks; Novelty Rejection: occurs if x falls in a region where there was little <ref type="bibr">(or no)</ref> training data. Hence, the predictor may struggle to make accurate predictions because it did not see enough data to accurately model the relationship between X and Y (R3 and R4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Ambiguity Rejection</head><p>Ambiguity rejection allows a model to abstain from making a prediction for an example x in regions where, despite having access to some training examples, the model h fails to capture the correct relationship f between X and Y <ref type="bibr" target="#b63">(Flores 1958;</ref><ref type="bibr" target="#b99">Hellman 1970;</ref><ref type="bibr" target="#b71">Fukunaga and Kessell 1972)</ref>. This can happen for two reasons. First, the observed relationship between X and Y is not deterministic. This can arise due to the intrinsic probabilistic nature of Y |X, which a deterministic predictor h cannot handle (e.g., coin toss), or the training data containing too many errors (e.g., incorrectly labeled examples), which would make it difficult for h to approximate f . In classification, this can arise due to classes overlapping in certain regions of the instance-space (Figure <ref type="figure">4a</ref>), while in regression, it leads to high variance in the target variable in certain regions (Figure <ref type="figure">5a</ref>). One way to interpret this issue is that the chosen feature space does not allow for accurately determining the target value (e.g., missing features might cause examples with different predictions to be projected onto the same example <ref type="bibr" target="#b201">(Van Craenendonck et al. 2018</ref>, Figure <ref type="figure" target="#fig_0">2</ref>)).</p><p>Second, a poor choice of the predictor's hypothesis space makes it impossible to learn the relationship between X and Y . This occurs when the chosen hypothesis space H does not include f . Figure <ref type="figure">4b</ref> illustrates this error in binary classification, where only linear models (e.g., perceptron) are considered for H while the true target concept f is non-linear. Similarly, Figure <ref type="figure">5b</ref> shows a regression problem with H as the family of quadratic models, while f is non-quadratic. In both cases, the expected prediction error is large for certain examples because f ̸ ∈ H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Novelty Rejection</head><p>Many machine learning models struggle when forced to extrapolate to regions of the feature space that were not (sufficiently) present in the training data <ref type="bibr" target="#b35">(Cordella et al. 1995a;</ref><ref type="bibr" target="#b177">Sambu Seo et al. 2000;</ref><ref type="bibr" target="#b200">Vailaya and Jain 2000)</ref>.</p><p>Novelty rejection allows a model to abstain from making predictions for examples that are sufficiently different from the training data <ref type="bibr" target="#b49">(Dubuisson and Masson 1993)</ref>. For such examples, the predictor h is likely to make mistakes because the absence of training data similar to x prevents h from learning the correct target y.</p><p>In practice, this arises when one of the following assumptions of the training procedure is violated. First, the sampling distribution differs from the true distribution. Thus, parts of the feature space are not represented in the data (e.g., the data does not contain any examples of patients suffering from a particular rare disease) ( <ref type="bibr" target="#b202">Van der Plas et al. 2021)</ref> or one cannot generate data for all imaginable situations (e.g., a sensor can break in many different ways) <ref type="bibr" target="#b100">(Hendrickx et al. 2022;</ref><ref type="bibr" target="#b199">Urahama and Furukawa 1995;</ref><ref type="bibr" target="#b106">Hsu et al. 2020)</ref>. Second, the skew in the class distribution is too large that the model ignores parts of the feature space. Thus, the predictor may choose to ignore these examples in its objective to optimize the accuracy model-complexity trade-off. Third, a new distribution appears after training and these new examples are out-of-distribution. For instance, this can arise due to drift that leads to new classes <ref type="bibr" target="#b126">(Landgrebe et al. 2004</ref>) or an adversarial agent that deliberately tries to mislead <ref type="bibr" target="#b213">(Wang and Yiu 2020;</ref><ref type="bibr" target="#b32">Corbière et al. 2022)</ref>.</p><p>Figures <ref type="figure">4c</ref> and <ref type="figure">5c</ref> illustrate this rejection type for a classification and a regression problem. In both cases, the three black stars represent test examples that are far away from the training examples. The learned model h may be inaccurate in these regions due to the lack of training data, which increases the chance of making a misprediction for these test examples.</p><p>The importance of novelty rejection often becomes particularly apparent in medical applications since in many of these applications, it is challenging or expensive to get an exhaustive training set. Therefore, the training set might only include patients from specific age groups or with particular medical conditions. For instance, <ref type="bibr">Van</ref>  Accept Ambiguity Reject Novelty Reject Rejector Fig. 5: Illustration of a regression scenario where rejection can be applied. The dash-dotted line represents the predictor h, while the solid line indicates the true function f . In a), examples in between the dotted lines are rejected (cross mark) due to the high variance of Y . In b), examples indicated with the cross mark are rejected due to the incorrect model bias. In c), star-marked examples are rejected because they differ from the training data (novelties).</p><p>encountered during training such as children or people suffering from extremely rare disorders. However, this information and the assumptions made during data collection might not be available to the user of the model. Therefore, adding a novelty rejector is crucial to avoid poor prediction performance on these patients. In this case, a rejector based on a LOF outlier detector can reject the predictions from children as they have a different morphology than the adults in the training set (Figure <ref type="figure" target="#fig_3">6</ref>). This mitigates the risk of incorrect predictions. The predictor is trained only on adults. The accuracy on children (66.8%) is much lower than the accuracy on adults (77.7%). Introducing the reject option mitigates the risk of making incorrect predictions as it ends up rejecting the predictions for most children.</p><p>3 Evaluating models with rejection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rejection Prediction</head><p>No Yes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct</head><p>True accept (TA) False reject (FR) Incorrect False accept (FA) True reject (TR) Table <ref type="table">1</ref>: Confusion matrix for learning-to-reject models. The columns show whether an example is rejected, the rows show whether h's prediction is correct.</p><p>Conceptually, a model with a reject option involves evaluating the outputs of both the predictor and the rejector. Thus, its performance can be viewed through the prism of the confusion matrix shown in Table <ref type="table">1</ref>, where the columns represent the rejector's decision and the rows whether the predictor's output is correct or not. Intuitively, the learning-to-reject model is "correct" if a correct prediction is returned to the user (a true accept) or an example is rejected when the model's prediction was wrong (a true reject). It is considered to have made a "mistake" if the model provides the user with an incorrect prediction (false accept) or rejects an example for which the model's prediction was correct.</p><p>Viewed through this lens, a model with rejection has two goals. On the one hand, it wants to have high accuracy A on examples for which it makes a prediction:</p><formula xml:id="formula_2">A = T A T A + F A .</formula><p>This makes the model reliable as practitioners can trust its outputs. On the other hand, it wants to have high coverage:</p><formula xml:id="formula_3">ϕ = T A + F A T A + F A + F R + T R .</formula><p>That is, it should make a prediction for as many test examples as possible <ref type="bibr" target="#b43">(De Stefano et al. 2000;</ref><ref type="bibr" target="#b51">El-Yaniv and Wiener 2010;</ref><ref type="bibr" target="#b130">Lei 2014)</ref>. This can alternatively be viewed as having a low rejection rate, defined as 1-ϕ. This makes the model useful in practice as its predictions can be utilized for decision-making. Unfortunately, these two goals are competing as the accuracy can be increased by limiting the predictions to the most confident cases, i.e., reducing the coverage <ref type="bibr" target="#b97">(Hansen et al. 1997;</ref><ref type="bibr" target="#b104">Homenda et al. 2016</ref>). As a result, metrics specifically tailored to learning to reject must capture this trade-off <ref type="bibr" target="#b33">(Cordelia et al. 1998)</ref>.</p><p>Broadly speaking, three categories of metrics exist that evaluate different aspects of a model with rejection:</p><p>Evaluating models for a given rejection rate: This entails having a fixed rejection rate provided by the user. In this case, one only needs to evaluate performance on the non-rejected examples and it is possible to use the standard evaluations (e.g., accuracy <ref type="bibr" target="#b88">(Golfarelli et al. 1997</ref>)); Evaluating the overall model performance/rejection trade-off : One can plot the rejection rate on the x-axis and the predictor's performance obtained on a representative test set on the y-axis, similar to Receiver Operating Characteristic (ROC) analysis; Evaluating models through a cost function: This case only requires knowing the model's output and the costs for (mis)predictions and rejections.</p><p>3.1 Evaluating models with a fixed rejection rate.</p><p>Given a dataset and a fixed rejection rate, <ref type="bibr" target="#b30">Condessa et al. (2017)</ref> argue that a good evaluation metric should meet four main criteria: given a fixed predictor, such metric should: (p1) depend on the model's rejection rate; (p2) be able to compare two models with different rejectors for a given rejection rate (and for the same predictor); (p3) be able to compare two models with different rejectors with different rejection rates when one clearly outperforms the other; (p4) reach its maximum value for a perfect rejector (i.e., a rejector that rejects all misclassified examples) and its minimum value for a rejector that rejects all accurate predictions. In addition, Condessa et al. (2017) propose three types of evaluation metrics that meet the required conditions: a) Prediction quality. The model's prediction quality (PQ) measures the predictor's performance on the non-rejected examples. For instance, one can use classical evaluation metrics on the accepted examples such as the accuracy</p><formula xml:id="formula_4">P Qacc = T A T A + F A ,</formula><p>the F-scores <ref type="bibr" target="#b165">(Pillai et al. 2011;</ref><ref type="bibr" target="#b145">Mesquita et al. 2016)</ref> or any other evaluation metric, including fairness metrics <ref type="bibr" target="#b140">(Madras et al. 2018)</ref>. While this allows comparing models with different rejectors, looking only at the prediction quality will tend to favor the model with the highest rejection rate. By being more conservative (i.e., having a lower coverage), the model tends only to offer predictions for the subset of examples for which it is most confident.</p><p>b) Rejection quality. The rejection quality (RQ) indicates the rejector's ability to reject misclassified examples. One way to do this is by comparing the ratio of misclassified examples on the rejected subset ( T R F R ) to the ratio on the complete dataset ( F A+T R T A+F R ), i.e.</p><formula xml:id="formula_5">RQratio = T R F R F A + T R T A + F R .</formula><p>Looking only at the rejection quality will favor models with the lowest rejection rate. The lower the rejection rate is, the more likely the rejector is to abstain only on those few examples for which it is most confident that the predictor will make a mistake. c) Combined quality. The combined quality (CQ) evaluates the model with a reject option as a whole. One way to accomplish this is by combining the predictor's performance on the non-rejected examples (prediction quality) with the rejector's performance on the misclassified examples (rejection quality). For instance, using P Qacc and RQratio yields to</p><formula xml:id="formula_6">CQacc-ratio = T A + T R T A + F A + F R + T R .</formula><p>Overall, the combined quality offers a more holistic assessment of the model's overall performance as it measures both the predictor's and the rejector's quality <ref type="bibr" target="#b133">(Lin et al. 2018</ref>). The downside is that aggregating the two metrics yields a less fine-grained characterization of the model performance. Specifically, in case a model has low CQ, it is hard to ascertain which component, the predictor or rejector, is contributing the most to the model's poor performance.</p><p>Pros and Cons. The main advantage of this category is that the metrics clearly measure the fine-grained model performance in the given setting. However, using one of these types of metrics may be limiting in some cases. For instance, theoretical research may not care about evaluating the model with rejection for a specific rejection rate, as it is usually specified based on domain knowledge. Moreover, given a rejection rate, not all performance metrics can be always used, as some may suffer from task-related issues. For instance, rejecting a whole class would not allow utilizing metrics like F1-score and AUC for the prediction quality as they need both classes' labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluating the model performance/rejection trade-off.</head><p>To assess the performance-rejection trade-off, a common approach is to evaluate the prediction quality (for non-rejected examples) by varying the rejection rate from 0% to 100%, which is known as the Accuracy-Reject Curve (ARC) <ref type="bibr" target="#b148">(Nadeem et al. 2010)</ref>. This involves plotting the rejection rate on the x-axis and the prediction quality (e.g., accuracy) on the y-axis <ref type="bibr" target="#b96">(Hanczar and Sebag 2014)</ref>. Higher curves indicate better performance. Alternatively, risk-based metrics like mean squared error can be plotted, with lower curves indicating better performance <ref type="bibr" target="#b177">(Sambu Seo et al. 2000)</ref>. Sometimes the predictor's performance on the rejected examples is also shown with the intuition that it should be worse on this subset of the data than on the accepted ones <ref type="bibr" target="#b228">(Zou et al. 2011;</ref><ref type="bibr">Condessa et al. 2015c,b)</ref>.  <ref type="formula">2023</ref>) compare two of their proposed models with a novelty reject option (blue) with two models that were used as baseline (black). The proposed models outperform the baselines but the light blue model only does for rejection rates lower than 0.01.</p><p>When comparing two models using the ARC method, two scenarios arise. First, one model clearly outperforms the other in terms of prediction quality for all rejection rates. Second, the models show varying prediction quality across different rejection rates. To illustrate this, consider Figure <ref type="figure" target="#fig_4">7</ref> where the light blue curve only outperforms the black curves if the rejection rate is lower than 0.01. When it is not clear which model performs best, the overall performance can be assessed using the Area Under the ARC Curve (AURC), similar to the AUC in standard machine learning <ref type="bibr">(Vanderlooy et al. 2006a,b;</ref><ref type="bibr" target="#b125">Landgrebe et al. 2006)</ref>. In this example, the AURC of the light blue curve will be higher than the AURC of the black curves, indicating that it performs better overall than the black curves.</p><p>Pros and Cons. The main advantage of this category is that any prediction quality metric can be used on the y-axis. Moreover, they provide a high-level overview of how the model works for different rejection rates. However, generating these curves can be challenging for two reasons. First, some rejectors do not allow directly setting the rejection rate <ref type="bibr" target="#b216">(Wu et al. 2007;</ref><ref type="bibr" target="#b103">Homenda et al. 2014)</ref>, but they might have other hyperparameters (e.g., rejection threshold). Mapping these to rejection rates may be challenging and may not be possible to achieve all possible rejection rates. Second, altering the rejection rate of a model with rejection can require completely retraining the whole model. This may be too computationally demanding to perform a fine-grained analysis <ref type="bibr" target="#b30">(Condessa et al. 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluating models through a cost function.</head><p>For classification tasks, one can ask the user to specify the costs for (mis)predictions as well as for rejection and evaluate a model by its (expected) total cost at test time. Although the costs can be designed as a continuous function of the examples <ref type="bibr" target="#b146">(Mozannar and Sontag 2020;</ref><ref type="bibr" target="#b42">De et al. 2021)</ref>, the cost function typically accounts only for three constant costs: the cost of correct prediction Cc, the cost of a prediction error Ce, and the cost of rejection Cr such that Cc &lt; Cr &lt; Ce <ref type="bibr" target="#b43">(De Stefano et al. 2000;</ref><ref type="bibr" target="#b7">Balsubramani 2016;</ref><ref type="bibr" target="#b26">Condessa et al. 2013)</ref>. Without loss of generality, usually one assumes normalized costs, i.e., Cc = 0, Ce = 1 and Cr ∈ [0, 1] in which the normalized value for Cr can be obtained from the initial values as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cr-Cc</head><p>Ce-Cc . Although the costs need to be set based on some domain knowledge, there are two constraints for setting the rejection cost properly. First, it must be lower than a random predictor's average cost for a classification task with K classes, i.e. Cr ≤ 1 K <ref type="bibr" target="#b36">(Cordella et al. 1995b;</ref><ref type="bibr" target="#b102">Herbei and Wegkamp 2006)</ref>. Otherwise, the expected cost of always making a random prediction would be lower than the rejection cost, which nullifies the task of rejection. Second, one should account for possible imbalance classes by setting Cr ≤ 1 -max k≤K P (Y = k), where P (Y = k) is the class frequency <ref type="bibr">(Perini et al. 2023</ref>). In fact, for higher Cr, a naive model that always predicts the most frequent class k would obtain a cost equal to 1-P (Y = k) and rejecting examples would not be worth it for higher rejection costs.</p><p>Pros and Cons. The main benefit of this category is its high interpretability: given a final cost, we can easily go back to the causes that yield such a cost. Moreover, one can use the same cost function to optimize the model parameters during the learning phase. This ensures coherence between learning the optimal model at training time and measuring its performance at test time. However, this category has the key drawback that the user must set the cost function based on domain knowledge, which is not always easy to obtain. Setting different costs changes the quality of the models, which may end up ranking several compared models differently. A way to alleviate this would be to make the Cost-Reject plot <ref type="bibr" target="#b94">(Hanczar 2019)</ref>, where, similar to the ARC, the x-axis and y-axis represent respectively the normalized rejection cost and the normalized prediction cost <ref type="bibr" target="#b69">(Friedel et al. 2006;</ref><ref type="bibr" target="#b0">Abbas et al. 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Separated rejector</head><p>Separated rejectors operate by filtering out unlikely examples. They are typically used for novelty rejection though there are some examples of using them for ambiguity rejection <ref type="bibr" target="#b6">(Asif and Minhas 2020)</ref>. Because the rejector does not use the predictor's output in any way, it is typically a function of the examples: r : X → R. Formally, the separated architecture yields the following model:</p><formula xml:id="formula_7">m(x) = ® if r(x) &lt; τ h(x) otherwise.</formula><p>(3)</p><p>If the rejector r outputs a value less than τ , then the model m rejects the example (®). Otherwise, m uses the predictor h to make a prediction. The separation between predictor and rejector means that the rejector is learned independently of the predictor. The learning task involves learning the rejector itself as well as setting the threshold τ . To align with its goal of identifying unlikely or unexpected examples, a common choice is to use anomaly/outlier, out-ofdistribution, or novelty detection algorithms. Three categories of methods can be used for this goal: models that 1) estimate p(X), 2) are one-class classifiers, and 3) quantify the degree of novelty using a data-driven score function.</p><p>Learning a separated rejector. A first option to learn a separated rejector is to use a probabilistic model that estimates the marginal density p(X) and to reject a test example x if p(x) &lt; τ . These probabilistic models often make assumptions about the distribution of the examples and are trained to maximize the likelihood of the training dataset <ref type="bibr" target="#b209">(Vasconcelos et al. 1993)</ref>. For instance, <ref type="bibr" target="#b126">Landgrebe et al. (2004)</ref> proposes a locally normal distribution assumption and uses a Gaussian Mixture Model (GMM) to estimate p(X) with a specified number of components whereas others have considered Variational Autoencoders <ref type="bibr" target="#b213">(Wang and Yiu 2020)</ref> and Normalizing Flows <ref type="bibr" target="#b149">(Nalisnick et al. 2019)</ref>.</p><p>Another option to learn a separated rejector is by employing a one-class classification model. Generally, they enclose the dataset into a specific surface and flag any example that falls outside such region as novelty. For instance, a typical approach is to use a One-Class Support Vector Machine (OCSVM) to encapsulate the training data through a hypersphere <ref type="bibr" target="#b24">(Coenen et al. 2020;</ref><ref type="bibr" target="#b103">Homenda et al. 2014)</ref>. By adjusting the size of the hypersphere, the proportion of non-rejected examples can be increased <ref type="bibr" target="#b216">(Wu et al. 2007)</ref>.</p><p>Alternatively, some models assign scores that represent the degree of novelty of each example (i.e., the higher the more novel), such as LOF (Van der Plas et al. 2023) or Neural Networks <ref type="bibr" target="#b106">(Hsu et al. 2020</ref>). When dealing with these methods, one often initially transforms the scores into novelty probabilities using heuristic functions, such as sigmoid and squashing <ref type="bibr" target="#b211">(Vercruyssen et al. 2018)</ref>, or Gaussian Processes <ref type="bibr" target="#b143">(Martens et al. 2023)</ref>. Then, the rejection threshold can be set to reject examples with high novelty probability.</p><p>Learning the rejection threshold τ . The rejection threshold τ is a crucial parameter that determines whether an example is rejected or not. In many cases, the threshold is set based on domain knowledge. For instance, one can introduce adversarial examples and set a threshold to reject them all <ref type="bibr" target="#b105">(Hosseini et al. 2017)</ref>. In case the number of novelties is unknown, one can use existing methods to estimate the contamination factor, i.e. the proportion of novelties, and set the threshold accordingly <ref type="bibr">(Perini et al. 2022b,a)</ref>. Otherwise, heuristics can be employed, such as rejecting examples falling within the first or second percentiles of correctly classified training examples <ref type="bibr" target="#b213">(Wang and Yiu 2020)</ref>.</p><p>Benefits and drawbacks of a separated rejector. Using a separated rejector has several benefits. First, the rejector is predictor agnostic. Hence, it can be combined with any type of predictor. Second, because the rejector can be trained independently of the predictor, it is possible to augment an existing predictor with a reject option using this architecture. Third, by serving as a filter, the predictor makes fewer predictions. This is particularly advantageous when there is a high computational cost associated with using the predictor. Finally, this architecture is generally simpler to operationalize compared to rejectors that interact with the predictor. However, there are two evident drawbacks. First, not sharing information between the predictor and the rejector results often in sub-optimal rejection performance <ref type="bibr" target="#b103">(Homenda et al. 2014</ref>). Second, this architecture is typically only used for novelty rejection because it is naturally related to assessing whether x is rare or not while ambiguity rejection requires information on p(Y |X), which is often estimated through the predictor's output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dependent rejector</head><p>Dependent rejectors analyze the predictor's output to identify examples that the predictor is likely to mispredict. The rejector is typically represented as a confidence function c h : X → [0, 1] that measures how likely the predictor is to make a correct prediction. Formally, the model for a dependent architecture has the form:</p><formula xml:id="formula_8">m(x) = ® if r(x; h) &lt; τ ; h(x) otherwise. (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>where r may depend on the feature vector x, and on the predictor h through the confidence function c h . Similar to the separated rejector, the model rejects the example (®) only if the rejector outputs a value lower than τ . Without loss of generality, we assume the confidence values c h (x) ∈ [0, 1], as one can always transform a score function into this range.</p><p>The learning task for a dependent rejector usually entails (1) selecting a confidence function that measures how confident the predictor is in its predictions, and (2) setting the rejection threshold τ .</p><p>Learning a dependent rejector: types of confidence function c h . The form of the confidence function depends on the desired rejection type. For ambiguity rejection, the metric should indicate the variability of the target variable or the potential predictor bias. For novelty rejection, the confidence should capture the example's similarity to the training data. We distinguish among four ways to derive the confidence scores: i) estimating the conditional probability P (Y |X), ii) estimating the class conditional density p(X|Y ), iii) performing a sensitivity analysis, and iv) exploiting the predictor's properties.</p><p>The conditional probability approaches allow for ambiguity rejections by using the maximum of the class conditional probability as confidence function <ref type="bibr" target="#b156">(Pazzani et al. 1994;</ref><ref type="bibr">Fumera et al. 2004</ref>; Lam and Suen 1995)</p><formula xml:id="formula_10">c h (x) = max k∈Y P (Y = k|X = x) (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>where k is either the true target value or the predictor's output. Low P (Y |X) values for two or more targets k 1 , k 2 ∈ Y indicate high randomness of the data or proximity to the decision boundary <ref type="bibr" target="#b5">(Arlandis et al. 2002)</ref>. Deriving the conditional probabilities from the predictor's outputs can be done by post-processing the predictor's output using techniques such as sigmoid calibration for binary classification tasks <ref type="bibr" target="#b34">(Cordella et al. 2014;</ref><ref type="bibr" target="#b15">Brinkrolf and Hammer 2017</ref>)</p><formula xml:id="formula_12">P (Y = 1|X = x) ≈ 1 1 + exp(Ah(x) + B) ,</formula><p>with parameters A and B learned during the training <ref type="bibr" target="#b16">(Brinkrolf and Hammer 2018)</ref>, softmax transformation for multi-class classification tasks <ref type="bibr" target="#b123">(Kwok 1999</ref>)</p><formula xml:id="formula_13">P (Y = k|X = x) ≈ exp (h k (x))</formula><p>j∈Y exp (h j (x))</p><p>, where h j (x) is the predictor's output for x related to class j, or by fitting Gaussian Processes for regression tasks (Sambu <ref type="bibr" target="#b177">Seo et al. 2000)</ref>. In case of multiple predictors h 1 , . . . , h V , one can also measure the ensemble agreement as the conditional probability <ref type="bibr" target="#b87">(Glodek et al. 2012;</ref><ref type="bibr" target="#b219">Zhang 2013</ref>)</p><formula xml:id="formula_14">P (Y = k|X = x) ≈ V v=1 1[hv(x) = k] V .</formula><p>On the other hand, the class conditional density approaches perform novelty rejection putting <ref type="bibr" target="#b49">(Dubuisson and Masson 1993;</ref><ref type="bibr" target="#b50">Dübuisson et al. 1985</ref>)</p><formula xml:id="formula_15">c h (x) = max k∈Y p(X = x|Y = k).<label>(6)</label></formula><p>Intuitively, a low density p(X|Y ) expresses that a sample is rare <ref type="bibr" target="#b28">(Condessa et al. 2015b)</ref>. Common methods to estimate the confidence in Eq. 6 employ generative predictors that directly measure the data density such as Gaussian Mixture Models (GMMs) <ref type="bibr" target="#b200">(Vailaya and Jain 2000)</ref>. It is also possible to employ heuristic approaches such as normalizing the class distance between the example x and its v-th nearest neighbor x ′ <ref type="bibr" target="#b31">(Conte et al. 2012;</ref><ref type="bibr" target="#b212">Villmann et al. 2015;</ref><ref type="bibr" target="#b61">Fischer et al. 2014b)</ref>, i.e.</p><formula xml:id="formula_16">p(X = x|Y = k) ≈ d(x, x ′ ) {(x * ,y * )∈D : y * =k} d(x * , x ′ * )</formula><p>and computing the proportion of neighbors within a specified radius R <ref type="bibr" target="#b10">(Berlemont et al. 2015)</ref> as</p><formula xml:id="formula_17">p(X = x|Y = k) ≈ |{x ′ : d(x, x ′ ) ≤ R, (x ′ , y ′ ) ∈ D, y ′ = k}| (x ′ ,y ′ )∈D |{x * : d(x ′ , x * ) ≤ R, (x * , y * ) ∈ D, y * = k}| .</formula><p>The sensitivity analysis line allows only ambiguity rejection, as it measures the robustness of the predictor under perturbation of either (a) its parameters or (b) the examples <ref type="bibr" target="#b131">(Lewicke et al. 2008;</ref><ref type="bibr" target="#b99">Hellman 1970)</ref>. Intuitively, slightly perturbing the predictor's parameters has major effects on the predictions only for examples that fall in the proximity of the decision boundary: slight variations of the parameters yield slight changes in the decision boundary, which, in turn, may end up flipping the predictions for some examples. Examples of employed perturbations involve adding some noise to the model's parameter values (e.g., adding a random sample from a normal distribution with null mean and small variance to the weights of a neural network), employing neural networks with a dropout layer (Geifman and El-Yaniv 2017), or using a Bayesian simulation <ref type="bibr" target="#b161">(Perini et al. 2020b</ref>). In the case of constructing multiple predictors, a common and simple confidence metric is</p><formula xml:id="formula_18">c {h1,...,h M } (x) = 1 -Var{h 1 (x), . . . , h M (x)}</formula><p>where h 1 , . . . , h M are the M predictors constructed by perturbing the parameters, and the variance is scaled to be in [0, 1]. In some cases, one can directly employ an ensemble of M similar predictors and measure the variance of their predictions <ref type="bibr">(Fumera and Roli 2004;</ref><ref type="bibr" target="#b109">Jiang et al. 2020</ref>).</p><p>Alternatively, one can perturb the test example x to be x + ε, where ε is a random noise such that ∥ε∥ is small. Intuitively, we want a predictor's output to remain the same when the example is only slightly perturbed. Thus, the confidence metric should reflect the robustness of h <ref type="bibr" target="#b144">(Mena et al. 2020;</ref><ref type="bibr" target="#b44">Denis and Hebiri 2020;</ref><ref type="bibr" target="#b121">Kühne et al. 2021)</ref>, such as</p><formula xml:id="formula_19">c h (x) = P (h(x + ε) = h(x)),</formula><p>which measures how likely it is that the prediction does not change when the example is perturbed. More generally, we can apply transformations such as rotations and symmetries to the examples <ref type="bibr" target="#b21">(Chen et al. 2018)</ref>. Finally, because choosing a specific value for ε is hard, one can use existing approaches to find each example's minimum ε that will alter its predicted label <ref type="bibr" target="#b45">(Devos et al. 2021</ref>) and derive a confidence metric as a function of the training ε.</p><p>The property-based methods consist of learning the confidence based on some of the predictor's properties, such as using the leaf configurations of a tree ensemble or the neural network's weight of specific neurons. This line allows both ambiguity and novelty rejection, depending on the utilized property. These methods tend to exploit heuristic and data-driven intuitions and there are no overarching themes that connect these intuitions. For instance, <ref type="bibr" target="#b46">Devos et al. (2023)</ref> present a method to detect evasion attacks in tree ensembles. By enumerating the leaves of each tree as o i , they map each example x to the configuration o = (o 1 , . . . , o V ) ∈ N V of the V activated leaves (one per tree) when passing x as input to the ensemble. In such output configuration space, they quantify the proximity to the decision boundary by measuring the Hamming distance between the configuration o of a test example with ensemble's prediction y and the closest training example's configuration o ′ with flipped prediction ŷ ̸ = y:</p><formula xml:id="formula_20">OC-score(x) = min o ′ ∈Rŷ V v=1 1 ov ̸ = o ′ v</formula><p>where R ŷ is the set of training configurations with flipped predictions. One can derive a confidence metric c h by, for instance, min-max normalizing the OC-score. On the other hand, confidence values can also be derived from the weight vectors of a Self-Organizing Map (SOM) (Gamelas <ref type="bibr" target="#b80">Sousa et al. 2014b</ref><ref type="bibr" target="#b78">Sousa et al. , 2015))</ref>. Because SOM's can approximate the input data density, they approximate p(X = x|Y = k) with p(w|Y = k, X = x), where w are the weights of the neural network, using standard statistical techniques, such as the Parzen Windows <ref type="bibr" target="#b3">(Alhoniemi et al. 1999</ref>). Moreover, El-Yaniv and Wiener (2011) propose a disbelief principle, which computes the confidence function by measuring how much the predictor h deteriorates if retrained with the constraint to predict a specific example x differently (i.e., hx)</p><formula xml:id="formula_21">c h (x) = 1 R(hx) -R(h) ,</formula><p>where R(hx) &gt; R(h) &gt; 0. Finally, the literature presents additional ad-hoc confidence metrics for k-NN and Random Forest <ref type="bibr" target="#b89">(Göpfert et al. 2018;</ref><ref type="bibr" target="#b41">Dalitz 2009)</ref>.</p><p>Learning the rejection threshold τ . Setting an appropriate rejection threshold τ is crucial for having an accurate dependent rejector. At a high level, the threshold is set in three main ways: using domain knowledge, adhering to user-provided constraints, or tuning it empirically based on some objective function.</p><p>In some situations, users possess domain knowledge that enables setting τ to achieve a desired rejection rate ρ (Le Capitaine and Frélicot 2012; <ref type="bibr" target="#b154">Pang et al. 2021)</ref>.</p><p>Setting τ in this situation entails (1) ranking training examples based on their confidence level, and (2) setting τ such that the desired percentage of predictions are rejected <ref type="bibr" target="#b190">(Sotgiu et al. 2020)</ref>, that is, at</p><formula xml:id="formula_22">P X (c h (x) &lt; τ ) = ρ.</formula><p>Note that this approach is also used when evaluating the model performance/rejection trade-off, which needs to measure the model performance for a fixed rejection rate (see Sec. 3.2) <ref type="bibr" target="#b139">(Ma et al. 2001;</ref><ref type="bibr" target="#b101">Heo et al. 2018;</ref><ref type="bibr" target="#b72">Fumera et al. 2003)</ref>.</p><p>In other cases, users provide knowledge as specific constraints that should be satisfied <ref type="bibr" target="#b163">(Pietraszek 2005</ref>). On the one hand, the user may provide an upper bound R for the rejection rate and aim to limit the number of rejections. This results in learning the appropriate τ by minimizing the model misclassification risk while adhering to the rejection rate constraint <ref type="bibr" target="#b224">(Zhou et al. 2022;</ref><ref type="bibr">Pugnana and</ref><ref type="bibr">Ruggieri 2022, 2023)</ref> </p><formula xml:id="formula_23">τ = arg min t∈[0,1] P XY (h(x) ̸ = y, c h (x) ≥ t) subject to P X (c h (x) &lt; t) ≤ R.</formula><p>On the other hand, the user may provide an upper bound M for the proportion of mispredictions, and aim to control the allowable error <ref type="bibr" target="#b208">(Varshney 2011;</ref><ref type="bibr" target="#b180">Sayedi et al. 2010</ref>). Thus, one needs to learn τ by setting up the complementary problem as before, namely by minimizing the model rejection rate while satisfying the constraints on error <ref type="bibr" target="#b132">(Li and Sethi 2006;</ref><ref type="bibr" target="#b64">Franc and Prusa 2019;</ref><ref type="bibr" target="#b65">Franc et al. 2021</ref>)</p><formula xml:id="formula_24">τ = arg min t∈[0,1] P X (c h (x) &lt; t) subject to P XY (h(x) ̸ = y, c h (x) ≥ t) ≤ M.</formula><p>Moreover, one can generalize this problem by finding the threshold τ such that the predictor's misclassification risk at test time is guaranteed to be bounded with high probability <ref type="bibr" target="#b85">(Geifman and El-Yaniv 2017)</ref>.</p><p>Finally, it is possible to set τ empirically according to some objective function. The most common approach is to set a single global threshold τ , which makes the rejection both simple and transparent, yet usually effective <ref type="bibr" target="#b71">(Fukunaga and Kessell 1972)</ref>. This is the case for Chow's rule <ref type="bibr" target="#b22">(Chow 1970</ref>) which involves learning the optimal τ by minimizing the risk function that includes the expected error rate and the rejection rate</p><formula xml:id="formula_25">τ = arg min t∈[0,1]      {x∈X : c h (x)≥t} (1 -c h (x)) p(x) dx Error rate + t {x∈X : c h (x)&lt;t} p(x) dx Rejection rate      . (7)</formula><p>However, in real-world scenarios, obtaining complete knowledge of class distributions is challenging, limiting the applicability of Chow's rule <ref type="bibr" target="#b184">(Shekhar et al. 2019)</ref>. Thus, in a binary classification case, <ref type="bibr" target="#b196">Tortorella (2000)</ref> proposes to use two rejection thresholds τ 1 and τ 2 such that</p><formula xml:id="formula_26">h(x) =      0 if c h (x) &lt; τ 1 ; 1 if c h (x) &gt; τ 2 ; ® if τ 1 ≤ c h (x) ≤ τ 2 (8)</formula><p>with c h (x) = P (Y = 1|X = x). He proposes to learn τ 1 , τ 2 by optimizing a cost function that is identical to finding the intersection between the cost function and the convex hull of the ROC curve <ref type="bibr">(10)</ref> where C f n , C f p , C tn , and C tp are the costs for false negatives, false positives, true negatives, and true positives, while F N R(t), T N R(t), T P R(t) and F P R(t) are the false negative, false positive, true negative and true positive rates obtained by evaluating the models with the thresholds set to t. This approach is theoretically equivalent to Chow's rule under the Bayesian optimality assumption (Santos-Pereira and Pires 2005; <ref type="bibr" target="#b47">Du et al. 2010</ref>). However, when estimating posterior probabilities, Chow's rule is not suitable, and τ should be learned using a cost-based approach <ref type="bibr" target="#b142">(Marrocco et al. 2007;</ref><ref type="bibr" target="#b119">Kotropoulos and Arce 2009)</ref>. Different approaches have extended Tortorella's method to address other scenarios <ref type="bibr" target="#b178">(Sansone et al. 2001)</ref>, such as stable formulations of ROC curves for small datasets <ref type="bibr" target="#b110">(Jigang et al. 2006)</ref>, robust and fast-to-retrain rejections for cost-sensitive situations <ref type="bibr" target="#b48">(Dubos et al. 2016;</ref><ref type="bibr" target="#b59">Fischer et al. 2015b)</ref>, and tailored solutions for learning metaclassifiers or handling multiple classes <ref type="bibr" target="#b164">(Pietraszek 2007;</ref><ref type="bibr" target="#b19">Cecotti and Vajda 2013)</ref>.</p><formula xml:id="formula_27">τ 1 = arg min t∈[0,1] P (Y = 1)(C f n -Cr)F N R(t) + P (Y = 0)(C tn -Cr)T N R(t) (9) τ 2 = arg min t∈[0,1] P (Y = 1)(C tp -Cr)T P R(t) + P (Y = 0)(C f p -Cr)F P R(t)</formula><p>For tasks requiring a more fine-grained rejection capability, considering multiple local thresholds τ 1 , τ 2 , . . . (up to a finite number) may be beneficial <ref type="bibr" target="#b147">(Muzzolini et al. 1998;</ref><ref type="bibr" target="#b122">Kummert et al. 2016;</ref><ref type="bibr" target="#b120">Krawczyk and Cano 2018)</ref>. Normally, setting local thresholds requires dividing the feature space into J regions J i and setting a (local) threshold in each region. For instance, one can design regions and thresholds by constructing one region for each class, i.e. J i = {x * | y * = i}, which means that the number of regions J equals the number of classes K <ref type="bibr" target="#b76">(Fumera et al. 2000)</ref>; then, one often finds the local threshold by using for each J i the same approach as for global thresholds; -using the Voronoi-cell decomposition, which requires J prototypes w i to have present a greedy optimization method to adaptively determine local thresholds using a heuristic principle; -setting up an optimization problem that finds the optimal thresholds by assigning different class rejection costs; for instance, in binary classification, <ref type="bibr" target="#b223">Zheng et al. (2011)</ref> proposes to find</p><formula xml:id="formula_28">τ 1 , τ 2 = arg min 0≤t1,t2≤1 P XY (c h (x) &lt; t 1 |y = 0)C r,0 + P XY (c h (x) &lt; t 2 |y = 1)C r,1 + P XY (m(x) ̸ = y)Ce</formula><p>where C r,0 and C r,1 are the costs for rejecting examples from the negative and positive classes; -optimizing an objective function that accounts for different user-specified class misclassification risks M 1 , . . . , M K ; <ref type="bibr" target="#b134">Lin et al. (2022)</ref> treat each class indepen-dently, setting</p><formula xml:id="formula_29">L(t k ) = Â(t k ) + K j λ j ( Mj -M j ) 2</formula><p>where Â(t k ) is any ambiguity metric, λ j is a penalization term that needs to be set to high values to penalize high differences between the model's misclassification risk M and the user-specified target.</p><p>Although multiple thresholds give more fine-grained control over a rejector's performance <ref type="bibr" target="#b127">(Laroui et al. 2021;</ref><ref type="bibr" target="#b82">Gangrade et al. 2021a)</ref>, this is usually more computationally expensive. However, <ref type="bibr">Fischer et al. (2016)</ref> propose efficient schemes for optimizing local thresholds and show that the computation time can be reduced to polynomial <ref type="bibr" target="#b12">(Boulegane et al. 2019)</ref>.</p><p>Benefits and drawbacks of a dependent rejector. Designing a dependent rejector has several benefits. First, the interaction between the predictor and rejector enables both types of rejection, because the rejector learns from the predictor's output the regions of the feature space where examples are mispredicted or unlikely to fall. Second, a dependent rejector can extend an existing predictor (including black-box) by simply setting a proper threshold on a confidence measure. Third, it allows the reuse of previously learned models, eliminating the need for costly retraining <ref type="bibr" target="#b228">(Zou et al. 2011;</ref><ref type="bibr" target="#b192">Tang and Sazonov 2014)</ref>. Fourth, a confidence-based rejection could be improved by considering multiple confidence metrics where each one captures different aspects of the underlying uncertainty <ref type="bibr" target="#b193">(Tax and Duin 2008)</ref>.</p><p>However, this architecture has potential drawbacks as well. First, the quality of the dependent rejector is highly influenced by the quality of the confidence metric, which is usually hard to evaluate. Second, typically a dependent rejector does not affect the predictor's learning phase. This results in possible sub-optimal predictions of the model with a reject option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Integrated rejector</head><p>The integrated rejector combines the rejector and predictor into a single model where it is impossible to distinguish between the role of the h and the r. Formally, the model with integrated reject option acts as</p><formula xml:id="formula_30">m(x) ∈ Y ∪ {®}. (11)</formula><p>Conceptually, this model simply includes ® as an additional output.</p><p>This architecture usually needs a unique algorithm for learning predictor and rejector in tandem <ref type="bibr">(Cortes et al. 2016a,b)</ref>. There are two distinct approaches to learning an integrated rejector. The first approach is model-agnostic and involves designing an objective function that penalizes (mis)predictions as well as rejections. The second approach is model-specific and entails integrating a rejector into an existing predictor, where rejection becomes part of the decision-making process.</p><p>Learning a model-agnostic integrated rejector. Typically, learning a model that simultaneously makes accurate predictions and rejects the examples that will be otherwise mispredicted can be done by simply designing a specialized objective function <ref type="bibr" target="#b146">(Mozannar and Sontag 2020)</ref>. Then, such a function can be minimized using potentially any existing optimizer, which makes it model-agnostic. For instance, for classification, a simple cost-based objective for any hypothesis m can be expressed as</p><formula xml:id="formula_31">L = E p(X,Y ) Cr1 m(x)=® (x) + 1 m(x)̸ ∈{y,®} (x)</formula><p>with Cr ∈ (0, 1/2]. For tasks other than classification, ad-hoc loss functions are used, such as those for multilabel classification <ref type="bibr" target="#b166">(Pillai et al. 2013;</ref><ref type="bibr">Nguyen and</ref><ref type="bibr">Hüllermeier 2020, 2021)</ref>, regression <ref type="bibr" target="#b6">(Asif and Minhas 2020;</ref><ref type="bibr" target="#b111">Kalai and Kanade 2021)</ref>, online learning <ref type="bibr" target="#b37">(Cortes et al. 2018;</ref><ref type="bibr" target="#b115">Kocak et al. 2020)</ref>, and multi-instance learning <ref type="bibr" target="#b222">(Zhang and Metaxas 2006)</ref>. However, in many cases, surrogate losses are employed to enable efficient optimization, as learning from discrete losses is computationally impractical <ref type="bibr" target="#b214">(Wegkamp 2007;</ref><ref type="bibr" target="#b90">Grandvalet et al. 2009;</ref><ref type="bibr" target="#b17">Cao et al. 2022)</ref>. Consequently, the original loss L is converted into a convex loss by utilizing surrogate functions ψ : R → R, such as the logistic and hinge functions <ref type="bibr" target="#b175">(Ramaswamy et al. 2018;</ref><ref type="bibr" target="#b221">Zhang et al. 2018;</ref><ref type="bibr" target="#b9">Bartlett and Wegkamp 2008)</ref>:</p><formula xml:id="formula_32">Logistic: ψ(L) = 1 1 + exp(L) , Hinge: ψ(L) =      1 -1-Cr Cr L if L &lt; 0 1 -L if 0 ≤ L &lt; 1 0 otherwise</formula><p>For the hinge loss to be convex, it is required that Cr ≤ 1/2 which is the case for classification as otherwise the cost of rejection would be higher than the cost of random guessing. Numerous studies in the literature have explored the properties of surrogate loss functions <ref type="bibr" target="#b218">(Yuan and Wegkamp 2010)</ref>, including calibration effects <ref type="bibr" target="#b153">(Ni et al. 2019;</ref><ref type="bibr" target="#b20">Charoenphakdee et al. 2021)</ref>, estimates of bounds for misclassification risk <ref type="bibr" target="#b184">(Shekhar et al. 2019;</ref><ref type="bibr" target="#b113">Kato et al. 2020)</ref>, penalization effects in high-dimensional spaces <ref type="bibr" target="#b215">(Wegkamp and Yuan 2012)</ref>, proximity to the optimal Bayes solution <ref type="bibr" target="#b13">(Bounsiar et al. 2008;</ref><ref type="bibr">Shen et al. 2020a)</ref>, and convergence rate analysis <ref type="bibr" target="#b44">(Denis and Hebiri 2020)</ref>. Lastly, one can allocate an extra class K + 1 (commonly known as the reject class) for rejection and assign a specific penalization cost Cr for predicting such a class. With this setting, there are two main alternatives. In the first case, there are no actual examples belonging to this class. Thus, these approaches design loss functions to enable the classifier to assign on its own a positive score to ambiguous examples <ref type="bibr" target="#b107">(Huang et al. 2020;</ref><ref type="bibr" target="#b53">Feng et al. 2022)</ref>. For instance, <ref type="bibr" target="#b226">Ziyin et al. (2019)</ref> propose to measure the expected loss as</p><formula xml:id="formula_33">L = E p(X,Y ) log(s k (x) + 1 Cr s K+1 (x)) ,</formula><p>where s k (x) and s K+1 (x) are probabilities, respectively, for the class y = k and K + 1 (rejection). At a high level, decreasing the rejection cost Cr results in higher chances of rejection. In the second case, one artificially generates examples x n+1 , . . . , x N (e.g., adversarial examples) and assigns them to the rejection class K + 1. By training a predictor with K + 1 classes, the reject option is naturally incorporated as output, and any (multi-class) predictor can be used for novelty <ref type="bibr" target="#b210">(Vasconcelos et al. 1995;</ref><ref type="bibr" target="#b189">Singh and Markou 2004;</ref><ref type="bibr" target="#b199">Urahama and Furukawa 1995)</ref> or ambiguity rejection <ref type="bibr" target="#b195">(Thulasidasan et al. 2019;</ref><ref type="bibr" target="#b155">Pang et al. 2022</ref>).</p><p>Learning a model-specific integrated rejector. In many practical use cases, one may already know that a specific class of models works well within the given context, such as SVM models in medical applications <ref type="bibr" target="#b95">(Hanczar and Dougherty 2008;</ref><ref type="bibr" target="#b93">Hamid et al. 2017)</ref>. Given a specific predictor, its learning algorithm can be slightly adapted to include the reject option. For instance, integrated SVMs set two (or more) hyperplanes on the feature space and reject all the examples located in between them <ref type="bibr" target="#b165">(Pillai et al. 2011;</ref><ref type="bibr" target="#b133">Lin et al. 2018;</ref><ref type="bibr" target="#b225">Zidelmal et al. 2012)</ref>. Figure <ref type="figure" target="#fig_6">8</ref> shows two common cases to learn the hyperplanes. First, one can parametrize the hyperplane as w • x + b ± ε = 0, with ε ≥ 0, which results in parallel and equidistant hyperplanes from the decision boundary, where ε indicates the distance <ref type="bibr" target="#b74">(Fumera and Roli 2002)</ref>. Learning such hyperplanes requires minimizing the empirical loss</p><formula xml:id="formula_34">L = 1 2 w • w + C n i=1 l(ξ i , ε) - n i=1 α i [y i (w • x i + b) -1 + ξ i ]</formula><p>where w is the weight vector, b is the intercept of the hyperplane, C is a (large) hyperparameter that regulates the importance of the performance/rejection tradeoff expressed inside the function l(ξ i , ε), and α i are the Lagrangian multipliers.</p><p>Second, the two hyperplanes can be parametrized as</p><formula xml:id="formula_35">w ′ • x + b ′ = 0 and w ′′ • x + b ′′ = 0.</formula><p>By formulating two distinct optimization problems, one can learn the parameters of these hyperplanes. In this approach, one hyperplane is highly penalized for mispredicting the positive class, while the other one is for the negative class. Essentially, this technique yields two SVMs that have few mispredictions on either class, and examples falling in between the hyperplanes can be naturally rejected <ref type="bibr" target="#b207">(Varshney 2006)</ref>. With the same approach, one can also learn a OCSVM for each class to reject test examples that lie outside any learned hypersphere (novelty) or within two overlapping hyperspheres (ambiguity) <ref type="bibr" target="#b137">(Lotte et al. 2008;</ref><ref type="bibr" target="#b136">Loeffel et al. 2015;</ref><ref type="bibr" target="#b216">Wu et al. 2007</ref>). Finally, Gamelas <ref type="bibr" target="#b79">Sousa et al. (2014a)</ref> shows that limiting the number of support vectors reduces the computational cost, while still ensuring high performance in most cases.</p><p>However, in several cases, more than two SVMs are used. For instance, in multi-label classification one can exploit as many SVMs as the number of labels and fit each hyperplane to discriminate between one class and all the others <ref type="bibr" target="#b165">(Pillai et al. 2011)</ref>. This raises the issue of defining rejection in the regions of intersection between some, but not all, of the hyperplanes. To address this, a natural solution is to utilize a data-replication method (Gamelas <ref type="bibr" target="#b81">Sousa et al. 2009</ref>). This approach involves replicating the complete dataset for each class k ∈ Y, adding a new dimension z with the class number, changing the target variable of each replica to a discrete one-vs-all label, and discriminating class k from the other classes <ref type="bibr" target="#b18">(Cardoso and Pinto Da Costa 2007;</ref><ref type="bibr" target="#b40">da Rocha Neto et al. 2011)</ref>.</p><p>Finally, Neural Network models allow integrating the rejector and predictor into the same structure by modifying their output layers <ref type="bibr" target="#b83">(Gangrade et al. 2021b;</ref><ref type="bibr" target="#b226">Ziyin et al. 2019)</ref>. <ref type="bibr" target="#b85">Geifman and El-Yaniv (2019)</ref> propose to introduce an additional head mr into the network that is dedicated to rejection. Specifically, mr is set as a sigmoid function and used such that</p><formula xml:id="formula_36">m(x) = ® if mr(x) &lt; 0.5.</formula><p>Similar to the SVM case, <ref type="bibr" target="#b84">Gasca A. et al. (2011)</ref> and <ref type="bibr" target="#b145">Mesquita et al. (2016)</ref> measure the disagreement of two Neural Networks trained to prioritize the classes differently. Specifically, they assume a binary classification task and use the output of two neural networks h 1 , h 2 to predict the positive class if h 1 (x), h 2 (x) ≥ 0, the negative class if h 1 (x), h 2 (x) &lt; 0 and rejection if they disagree on the sign. For this task, they use two weighted Extreme Learning Machines (wELM) <ref type="bibr" target="#b227">(Zong et al. 2013)</ref>, namely two neural networks with Q hidden neurons that output</p><formula xml:id="formula_37">h * (x) = Q q=1 wyβqg(aq • x + bq)</formula><p>where wy is the cost related to the example x that belongs to the class y, aq is the weight vector connecting the q-th hidden node and the input nodes, bq is the bias of the q-th hidden node and g is the activation function. By setting the class misprediction costs, learning the parameters β = (β 1 , . . . , β Q ) requires using the traditional weighted least square formulation min ∥Hβ -Y ∥ 2 so that</p><formula xml:id="formula_38">β = (H T W H) -1 H T W Y</formula><p>where H is the n × Q matrix of activation functions h iq = g(aq • x i + bq), W is the n×1 matrix of class costs (one per example) and Y is the target vector. Thus, each network limits one class mispredictions and the region of disagreement is designed to be the rejection region.</p><p>Benefits and drawbacks of an integrated rejector. This architecture has two key benefits. First, integrating the predictor and rejector means that both aspects of the model are optimized toward the task at hand. This can improve the performance of the model with rejection when compared to using other architectures because the predictor's and the rejector's components can affect each other. Second, because it is a unique model, the bias introduced by the model with rejection is potentially less than in the scenario where the predictor and rejector are two different models. However, this architecture has potential drawbacks, as designing such a rejector might not be trivial. First, it requires extensive knowledge about the predictor in order to integrate the reject option. Second, it may require developing a novel algorithm to learn the model with a reject option from data. Finally, it is computationally more expensive than the other architectures, as any changes to the rejector require retraining the entire model, which can be timeconsuming <ref type="bibr" target="#b23">(Clertant et al. 2020;</ref><ref type="bibr" target="#b188">Shpakova and Sokolovska 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Combining multiple rejectors</head><p>Most rejectors are tailored towards a single rejection type. However, by combining multiple rejectors one can enable multiple rejections, such as performing both ambiguity and novelty rejection. We distinguish between two types of combinations of rejectors based on whether the rejectors' rejection regions overlap or not because examples in overlapping regions require deeper analysis (e.g., to specify the underlying rejection type).</p><p>First, when rejectors do not overlap (or when we are not interested in the example's rejection type), one can simply combine the rejection sets by a logical or-rule: reject the example if any of the rejectors rejects it and assign such rejection type <ref type="bibr" target="#b66">(Frélicot 1997;</ref><ref type="bibr" target="#b191">Suutala et al. 2004</ref>). For instance, given Z rejectors r 1 , r 2 , . . . , r Z with thresholds τ 1 , τ 2 , . . . , τ Z , one can combine them into m as</p><formula xml:id="formula_39">m(x) = ® if ∃ i ≤ Z : r i (x; h) &lt; τ i h(x) otherwise</formula><p>Second, when rejectors overlap in some regions a simple or-rule can be insufficient to determine the reason for rejection because each rejector may decide to abstain for a different reason. Typically, existing works carefully select the order to evaluate the rejectors. This is usually done in a multi-step architecture: either by stacking only the rejectors <ref type="bibr" target="#b67">(Frélicot 1998;</ref><ref type="bibr" target="#b68">Frélicot and Mascarilla 2002)</ref>, or even using multiple models with rejection <ref type="bibr" target="#b171">(Pudil et al. 1992;</ref><ref type="bibr" target="#b8">Barandas et al. 2022)</ref>. For instance, given Z rejectors r 1 , r 2 , . . . , r Z with thresholds τ 1 , τ 2 , . . . , τ Z , one can order rejectors by importance and combine them as</p><formula xml:id="formula_40">m(x) =            ® 1 if r 1 (x; h) &lt; τ 1 ; ® 2 if r 1 (x; h) ≥ τ 1 and r 2 (x; h) &lt; τ 2 ; . . . h(x) if r i (x; h) ≥ τ i ∀ i ≤ Z;</formula><p>where ® i indicates the rejector r i 's type of rejection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Applications of machine learning models with rejection</head><p>In safety-sensitive domains, making the wrong decision can have serious consequences such as fatal accidents with self-driving cars, major breakdowns in industrial settings or incorrect diagnoses in medical applications. In these domains, rejection can be used to make cautious predictions. However, the number of papers discussing machine learning with rejection in practical applications is still limited. In this section, an overview of these papers is given.</p><p>Biomedical applications. Machine learning with rejection is primarily explored in medical applications due to the potential consequences of incorrect decisions <ref type="bibr" target="#b135">(Liu et al. 2022)</ref>. The main focus is on ambiguity rejection for medical diagnosing, specifically the detection and classification of diseases. If the model is confident enough, the detection results are automatically translated into a diagnosis. Otherwise, a medical expert verifies the detection <ref type="bibr" target="#b116">(Kompa et al. 2021)</ref>. For instance, vocal pathologies are detected using voice recording data, where a linear classifier is trained and uncertain predictions are rejected based on a threshold of the derived posterior probability <ref type="bibr" target="#b119">(Kotropoulos and Arce 2009)</ref>. Spine disease diagnosis employs the data-replication method, which predicts only when two biased classifiers agree <ref type="bibr" target="#b81">(Gamelas Sousa et al. 2009)</ref>. Cancer detection, particularly breast tumor detection, benefits from a reject option implemented with an SVM classifier using confidence-based rejection <ref type="bibr" target="#b92">(Guan et al. 2020)</ref>. The rejection thresholds are chosen to limit the rejection rate and reduce manual effort.</p><p>Other biomedical applications have also adopted the use of a reject option. <ref type="bibr" target="#b137">Lotte et al. (2008)</ref> conduct an experiment on distinguishing hand movements using brain activity. They employ a separate novelty rejector trained in a supervised manner to discard brain activity associated with other activities. <ref type="bibr" target="#b131">Lewicke et al. (2008)</ref> explore sleep stage scoring with both types of rejection, utilizing confidence metrics derived from a Neural Network classifier's neural activities. Another sleep stage scoring application utilizes a separate rejector based on Local Outlier Factor (LOF) anomaly scores for novelty rejection, identifying patients who deviate from the training data (Van der <ref type="bibr" target="#b202">Plas et al. 2021)</ref>. Some papers compare the performance of multiple models with rejection to determine the optimal approach for specific biomedical applications. For instance, <ref type="bibr" target="#b112">Kang et al. (2017)</ref> predict the effectiveness of a diabetes drug for individual patients, while <ref type="bibr" target="#b192">Tang and Sazonov (2014)</ref> investigate the classification of body positions using sensors placed in patients' shoes. Lastly, a medical application focuses on the analysis of tissue examples, aiming to classify each pixel of tissue images into categories such as bone, fat, or muscle <ref type="bibr" target="#b26">(Condessa et al. 2013</ref><ref type="bibr" target="#b27">(Condessa et al. , 2015a))</ref>.</p><p>Engineering applications. Applications in engineering can also benefit from a reject option. For instance, in the chemical identification of gases, time-series data is processed by two classifiers to classify the observed gas. Classification occurs only when there is agreement between the classifiers, and ambiguous predictions are rejected until consensus is reached <ref type="bibr" target="#b98">(Hatami and Chira 2013)</ref>. A similar ambiguity rejection technique, rejecting when two classifiers disagree, is employed in defect detection in software applications <ref type="bibr" target="#b145">(Mesquita et al. 2016)</ref>. Fault detection in steam generators utilizes a set of one against all SVM classifiers, and rejection is based on the distance to the decision boundary of these classifiers, allowing for both rejection types <ref type="bibr" target="#b228">(Zou et al. 2011</ref><ref type="bibr">). Finally, Hendrickx et al. (2022)</ref> employs a separated novelty rejector for vehicle usage profiling.</p><p>Economics applications. In the domain of economics, two applications of machine learning with rejection have been proposed, both focused on ambiguity rejection and novelty rejection. The first application uses a Learning Vector Quantization (LVQ) to classify dollar bills by value <ref type="bibr" target="#b2">(Ahmadi et al. 2004</ref>). Confidence metrics are obtained from the classifier for both types of rejection. The second application investigates a few rejection techniques on top of a predictor to decide whether to grant a loan <ref type="bibr" target="#b24">(Coenen et al. 2020)</ref> Image recognition applications. Reject options are usually available for analyzing text styles and reading handwritten numbers <ref type="bibr">(Fumera and Roli 2004)</ref>, used for both ambiguity rejection <ref type="bibr" target="#b217">(Xu et al. 1992;</ref><ref type="bibr" target="#b108">Huang and Suen 1995;</ref><ref type="bibr" target="#b174">Rahman and Fairhurst 1998)</ref> and novelty rejection <ref type="bibr" target="#b138">(Lou et al. 1999;</ref><ref type="bibr" target="#b5">Arlandis et al. 2002)</ref>. These methods employ a confidence-based dependent rejector. Additionally, there is a paper focused on identifying walkers based on their footprints <ref type="bibr" target="#b191">(Suutala et al. 2004</ref>). Initially, each footprint is individually predicted or rejected, and then the information from three consecutive footprints is combined for the final decision.</p><p>9 Link to other research areas This section briefly discusses the fields related to learning with rejection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Uncertainty quantification</head><p>The field of uncertainty quantification (UQ) aims to measure how uncertain a learned model's predictions are <ref type="bibr" target="#b77">(Gal et al. 2017)</ref>. It distinguishes between two types of uncertainties: aleatoric uncertainty, which is the randomness in the data, and epistemic uncertainty, which is the lack of knowledge. Aleatoric uncertainty arises from non-deterministic relations between features and the target, while epistemic uncertainty can be caused by a small training set or incorrect model bias. For instance, when predicting the outcome of tossing an unfair coin, initially we lack historical data, resulting in high data-epistemic uncertainty. As we observe more coin tosses, data-epistemic uncertainty decreases, but aleatoric uncertainty remains due to the stochastic nature of the coin flip <ref type="bibr" target="#b181">(Senge et al. 2014)</ref>.</p><p>These uncertainties are inherently related to rejection. Rejecting examples due to high aleatoric uncertainty falls into the ambiguity rejection scenario. On the other hand, high epistemic uncertainty due to the lack of data may cause either ambiguity or novelty rejection. That is, if an example is similar to the training set but its prediction strongly depends on the choice of the dataset (e.g., close to the predictor's decision boundary for classification tasks), then this gives rise to an ambiguity rejection. Alternatively, if an example is dissimilar to any of the training examples, this leads to a novelty rejection.</p><p>Methods for UQ can be applied within learning with rejection. UQ focuses on obtaining (calibrated) estimates that meaningfully convey the level of uncertainty <ref type="bibr" target="#b118">(Kotelevskii et al. 2022)</ref>, which learning with rejection can leverage to allow the model to abstain when the uncertainty is high <ref type="bibr" target="#b157">(Perello-Nieto et al. 2017;</ref><ref type="bibr" target="#b116">Kompa et al. 2021)</ref>. While calibrated uncertainty estimates are not always necessary for learning with rejection, they can be important. For instance, calibrated uncertainty estimates enable setting an optimal threshold that minimizes the empirical risks <ref type="bibr" target="#b22">(Chow 1970)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Anomaly detection</head><p>Anomaly detection <ref type="bibr" target="#b169">(Prasad et al. 2009</ref>) is a Data Mining task aimed at identifying examples that deviate from expected behavior in a dataset. It is closely linked to novelty rejection because anomalies, being rare and substantially different from the training data, fall under the category of novelties <ref type="bibr" target="#b198">(Ulmer and Cinà 2020;</ref><ref type="bibr" target="#b167">Pimentel et al. 2014;</ref><ref type="bibr" target="#b141">Markou and Singh 2003)</ref>. Anomaly detectors are often utilized for novelty rejection within a separate rejector architecture.</p><p>Adding a reject option to anomaly detectors allows them to abstain from processing examples when a clear decision cannot be made <ref type="bibr" target="#b161">(Perini et al. 2020b</ref><ref type="bibr">(Perini et al. , 2023))</ref>. However, enabling this option in unsupervised anomaly detection poses two challenges. First, most confidence metrics assume a supervised setting, relying on measuring the distance to a decision surface. However, in anomaly detection, a hard decision surface may not always exist, necessitating specialized metrics that consider the model bias of the detector <ref type="bibr" target="#b161">(Perini et al. 2020b)</ref>. Second, the lack of labeled data makes it difficult to train a rejector using standard performance metrics. Instead, unsupervised techniques are employed, leveraging performance metrics that measure the stability of the anomaly detector itself <ref type="bibr">(Perini et al. 2020a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Active learning</head><p>Active learning <ref type="bibr" target="#b182">(Settles 2009;</ref><ref type="bibr" target="#b70">Fu et al. 2013;</ref><ref type="bibr" target="#b220">Zhang and Chaudhuri 2014;</ref><ref type="bibr" target="#b150">Nguyen et al. 2019)</ref> involves the interaction between a learning algorithm and an oracle who provides feedback to guide the learner. Its purpose is to reduce the need for labeling large amounts of data while still achieving high predictive performance. The algorithms focus on identifying the examples that would be most beneficial for the learner to label, thus minimizing the associated labeling costs.</p><p>Active learning and learning with rejection share the focus on uncertain examples <ref type="bibr" target="#b4">(Amin et al. 2021</ref>). However, they differ in their motivations for addressing uncertainty. In active learning, uncertainty is crucial during training to improve efficiency by minimizing the amount of labeled data required for an accurate model. In contrast, learning with rejection aims to capture uncertainty at test time to prevent mispredictions. Its focus is on avoiding unreliable predictions based on uncertain examples.</p><p>Another difference is that outliers are not always considered. For instance, methods based on discriminative learning cannot express low-density regions. <ref type="bibr" target="#b183">Sharma and Bilgic (2017)</ref> determine uncertain examples based on evidence measures that support the positive (E +1 ) or negative class (E -1 ) in binary classification. An example x has an uncertain class if E +1 (x) ≈ E -1 (x). Two cases are distinguished based on the magnitude of the evidence: if both E +1 and E -1 are large, the model is uncertain because of strong, but conflicting evidence for both classes, while if both E +1 and E -1 are small, the model is uncertain because of insufficient evidence for either class. Because both cases assume that a model is uncertain if P (Y |X) ≈ 0.5 when using a uniform prior, they both correspond to our ambiguity rejection scenario.</p><p>Combining active learning with machine learning with rejection could be of great use <ref type="bibr" target="#b117">(Korycki et al. 2019;</ref><ref type="bibr" target="#b170">Puchkin and Zhivotovskiy 2022;</ref><ref type="bibr" target="#b185">Shekhar et al. 2020)</ref>. When the interaction with an oracle is possible, it may be of interest to query the rejected test examples. New data types could be identified by novelty rejection, while ambiguity rejection may fine-tune the decision boundary. 9.4 Class-incremental / incomplete learning Typically, learned models assume knowledge of all possible classes during training. However, class-incremental learning focuses on models that adapt during deployment to detect and predict novel classes that were not seen during training.</p><p>Novelty rejection and class-incremental learning both operate under an openworld assumption and aim to detect novel examples compared to the training set. However, there are two key differences.</p><p>First, class-incremental learning specifically targets examples belonging to novel classes, distinguishing them from outliers. In contrast, novelty rejection techniques do not prioritize this distinction. Second, class-incremental learning involves detecting novel class examples and retraining the model to recognize them. Novelty rejection techniques can be considered in class-incremental learning. Moreover, both techniques can be combined into a single pipeline, by adapting incremental models with novelty rejected examples. For instance, such examples can be used as prototypes in a k-Nearest Neighbors (k-NN) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">Delegating classifiers</head><p>Similar to learning with rejection, the approach of delegation involves the use of a classifier, which only classifies examples with high confidence and delegates the prediction for the remaining examples <ref type="bibr" target="#b194">(Temanni and Nadeem 2007;</ref><ref type="bibr" target="#b114">Khodra 2016)</ref>.</p><p>The delegated examples are given to another, more specialized, classifier which makes a prediction <ref type="bibr">(Ferri et al. 2004;</ref><ref type="bibr" target="#b168">Prasad and Sowmya 2008)</ref>. In contrast, learning with rejection usually assumes that the user will inspect any rejected examples. Furthermore, delegation can be developed as a chain, where the next classifier makes a prediction for the examples for which the previous model was too uncertain (Giraud-Carrier 2022).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6">Meta-learning</head><p>Meta-learning, also known as "learning to learn", explores methods and techniques for automatically learning the characteristics, behaviors, and performance of machine learning models <ref type="bibr" target="#b11">(Bock 1988;</ref><ref type="bibr" target="#b206">Vanschoren 2018</ref>). It aims to develop higher-level knowledge that guides the learning process itself <ref type="bibr" target="#b14">(Brazdil et al. 2009;</ref><ref type="bibr" target="#b91">Gridin 2022)</ref>.</p><p>Despite having different goals and levels of abstraction, meta-learning can provide valuable insights and approaches for the context of learning with rejection. For instance, meta-learning algorithms analyze the behavior and performance of classifiers on different datasets to derive general knowledge about their strengths, weaknesses, and limitations <ref type="bibr" target="#b1">(Abbasi et al. 2012;</ref><ref type="bibr" target="#b197">Tremmel et al. 2022;</ref><ref type="bibr" target="#b25">Cohen et al. 2022)</ref>. This knowledge can then be used to make informed decisions about when to reject predictions. Moreover, meta-learning algorithms can identify relevant features or attributes that are informative for determining when to reject predictions <ref type="bibr" target="#b56">(Filchenkov and Pendryak 2016;</ref><ref type="bibr">Shen et al. 2020b)</ref>. By focusing on important features, rejectors can make more accurate decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusions and perspectives</head><p>We have studied the subfield of machine learning with rejection and provided a higher-level overview of existing research. To conclude, we revisit our key research questions and point to new directions that future research might take.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Research questions revisited</head><p>This survey paper is built around eight key research questions, introduced in the introduction. In this section, we revisit each of these questions and briefly summarize our findings.</p><p>How can we formalize the conditions for which a model should abstain from making a prediction? In Section 2, we identify two types of rejections: ambiguity rejection and novelty rejection. Ambiguity rejection abstains from making a prediction an example falls in a region where the target value is ambiguous (e.g., close to the decision boundary in classification tasks). This could be due to a non-deterministic true relation between the features and target variable, or due to a hypothesis space that is not able to capture the true relation. Novelty rejection abstains from making a prediction on examples that are rare with respect to the given training set. For such an example, there is no guarantee that the model correctly extrapolates to this untrained region, making it likely that the model mispredicts the example.</p><p>How can we evaluate the performance of a model with rejection? Standard machine learning evaluation is focused on a model's predictive quality. However, in machine learning with rejection, there exists a trade-off between the predictive quality and the proportion of rejected examples.</p><p>In Section 3, we provide an overview of techniques evaluating both the prediction and rejection quality of models with rejection. We identify three categories: metrics evaluating models with a given rejection rate, metrics evaluating the overall model performance/rejection trade-off, and metrics evaluating models through a cost function.</p><p>What architectures are possible for operationalizing (i.e., putting this into practice) the ability to abstain from making a prediction? We categorize machine learning methodologies with rejection in three different architectures, depending on the relationship between the predictor and the rejector: separated, dependent and integrated rejector. These categories are introduced and mapped to the existing literature in Sections 4, 5 and 6.</p><p>How do we learn models with rejection? For each architecture, we discuss the main techniques to learn a model with rejection and related these to the existing literature in Sections 4, 5 and 6. First, the separated rejector is usually learned independently of the predictor. Second, learning the dependent rejector entails learning for which examples the predictor is likely to mispredict using a confidence function. Both architectures need setting a rejection threshold. Third, integrated rejector needs a unique algorithm for learning predictor and rejector in tandem. Usually, this architecture relies on designing an objective function.</p><p>What are the main pros and cons of using a specific architecture? Each architecture, discussed in Sections 4,5, and 6, offers distinct benefits and drawbacks. Separated rejectors show broad applicability, as they can be combined with any predictor. However, they often yield sub-optimal rejection performance since they do not learn from the predictor's mispredictions. On the other hand, dependent rejectors have reduced, yet still high, applicability, relying on a specific confidence function learned from the predictor's output, but they can enhance the rejection quality by leveraging the predictor's mispredictions. Finally, integrated rejectors necessitate joint design with the predictor, but learning a single model for prediction and rejection improves the overall performance for both prediction and rejection tasks.</p><p>How can we combine multiple rejectors? We discuss the combination of multiple rejectors for enabling various types of rejections within a unique model. There are two approaches for combining rejectors. First, when rejectors do not overlap, a logical "or" rule is applied, rejecting an example if any of the rejectors rejects it. Second, when rejectors overlap in some regions and disagree on the type of rejection, a multi-step architecture is used, ordering rejectors by importance to make decisions based on the most relevant rejector.</p><p>Where does the need for machine learning with rejection methods arise in real-world applications? On a high level, machine learning with rejection is typically used in applications where incorrect decisions can have severe consequences, both financially and safety-related. These consequences motivate the need for robust and trustworthy machine learning. In Section 8, we provide an overview of application areas in which machine learning with rejection is already used.</p><p>How does machine learning with rejection relate to other research areas? Section 9 shows that machine learning with rejection is closely related to several other subfields of machine learning. This relation sometimes leads to terminology and techniques overlapping or inspired by these other domains. In contrast, other cases show machine learning with rejection from a broader perspective. In this survey, we related machine learning with rejection to uncertainty quantification, anomaly detection, active learning, class-incremental learning, delegating classifiers, and meta-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Future directions</head><p>Given its significance for the usage of machine learning in real-world problems and the growing attention for trustworthy AI, we expect machine learning with rejection to remain an active research field. In this section, we briefly discuss three key research directions for which we see a strong need.</p><p>Standard settings to compare different models with rejection. A large number of machine learning models with rejection already exist. However, these are typically evaluated on custom or even proprietary data. This makes it difficult to benchmark and compare the different approaches. While some papers use publicly available datasets, there is no standard benchmark set for machine learning with rejection. Additionally, applying multiple strategies to evaluate the rejector offers a better view of an algorithm's performance and improves comparability.</p><p>Partial rejection for machine learning models. A promising avenue for further exploration is the concept of partial abstention. Nowadays, machine learning problems often involve seeking elaborated predictions rather than simple scalar or class values as in classification and regression tasks. For instance, in multi-label classification, a prediction for an instance is a subset of possible class labels. In such cases, the idea of abstaining from a complete prediction can be extended to partial abstention, where the learner delivers predictions on some but not necessarily all class labels, according to its level of certainty <ref type="bibr" target="#b151">(Nguyen and Hüllermeier 2020)</ref>. This has the key benefit of providing a middle ground between making predictions for the entire structure and completely abstaining from making any predictions.</p><p>Algorithms enabling models with rejection in domains other than classification. Most papers on machine learning with rejection study supervised classification problems. Modern machine learning tackles numerous other tasks such as regression, forecasting, and clustering, or even semi-supervised and self-supervised feedback loops. We believe that the rejection can also be of use in such areas. However, since only a handful of relevant studies exist, this requires more attention from the research community. Future research can also focus on integrating rejectionrelated variables into statistical frameworks utilized in educational measurement, such as Item Response Theory (IRT). This integration has the potential to improve the precision of item calibration, trait estimation, and the interpretation of test scores. Furthermore, it can provide valuable insights into the psychological aspects of learning, enabling the development of more precise instructional strategies and interventions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Data flow in a dependent rejector. First, the predictor processes the example at hand. Next, the rejector assesses the confidence in the prediction based on the predictor's representation of the example.</figDesc><graphic coords="6,106.57,89.45,276.56,61.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Data flow in an integrated rejector, in which the predictor and rejector are one model. This model embeds the reject and predict functions and directly outputs a prediction or a rejection.</figDesc><graphic coords="6,106.57,207.63,276.57,67.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Fig. 4: Illustration of a classification scenario where the two classes overlap in a region. The dotted lines represent the rejector, the dash-dotted line the fitted predictor h, and the solid line the ground truth relation f . a) shows ambiguity rejection due to a non-deterministic relation between X and Y , b) introduces ambiguity rejection due to the model bias, and c) illustrates an example of novelty rejection. While in the first two plots the rejection region is inside the two dotted lines (examples with cross marks are rejected), in the third figure the rejected novel examples (stars) are outside the dotted line. a) b) c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Results of a novelty reject option in a sleep stage scoring application (Van der Plas et al. 2021).The predictor is trained only on adults. The accuracy on children (66.8%) is much lower than the accuracy on adults (77.7%). Introducing the reject option mitigates the risk of making incorrect predictions as it ends up rejecting the predictions for most children.</figDesc><graphic coords="9,72.00,89.45,345.70,115.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Example of an accuracy-reject curve in which Van der Plas et al. (2023) compare two of their proposed models with a novelty reject option (blue) with two models that were used as baseline (black). The proposed models outperform the baselines but the light blue model only does for rejection rates lower than 0.01.</figDesc><graphic coords="12,59.10,74.40,371.50,123.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>J i = {x * | d(x * , w j ) ≤ d(x * , w k ) ∀k ̸ = i}, for i ≤ J<ref type="bibr" target="#b212">(Villmann et al. 2015;</ref>  Fischer et al. 2015a,b;<ref type="bibr" target="#b62">Fischer and Villmann 2016);</ref><ref type="bibr" target="#b57">then, Fischer et al. (2014a)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Two cases of Integrated SVMs: (a) on the left side, the two hyperplanes are parallel and equidistant to the decision boundary (dashed line); (b) on the right side, each SVM gives higher priority to one class by limiting mispredictions.</figDesc><graphic coords="22,72.00,89.45,345.69,67.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FundingKilian</head><figDesc>Hendrickx and Dries Van der Plas received funding from VLAIO (Flemish Innovation &amp; Entrepreneurship) through the Baekeland PhD mandates [HBC.2017.0226] (KH) and [HBC.2019.2615] (DV). Lorenzo Perini received funding from FWO-Vlaanderen, aspirant grant 1166222N. Jesse Davis is partially supported by the KU Leuven research funds [C14/17/070]. Lorenzo Perini, Jesse Davis and Wannes Meert received funding from the Flemish Government under the "Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen" programme.</figDesc></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Not applicable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>Not applicable</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>The authors declare that they have no conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics approval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Not applicable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent to participate</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Not applicable</head><p>Authors' contributions Concept: JD, WM; Literature Study: KH, LP, DVdP; Categorization: KH, LP, DVdP, WM, JD; Writing -original draft preparation: KH, LP, DVdP; Writingreview and editing: WM, JD; Writing -revision: LP, WM, JD, DVdP, KH; Funding acquisition: WM, JD; Supervision: WM, JD.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accuracy Rejection Normalized-Cost Curves (ARNCCs): A Novel 3-Dimensional Framework for Robust Classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S A</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaheen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alshdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alharbey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-O</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aziz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="160125" to="160143" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Metafraud: A meta-learning framework for detecting financial fraud</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly: Management Information Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1293" to="1327" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improvement of reliability in banknote classification using reject option and local PCA</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Omatu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fujinaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kosaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="277" to="293" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic measures for responses of self-organizing map units</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alhoniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Himberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vesanto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ICSC Congress on Computational Intelligence Methods and Applications</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Massad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Haefke</surname></persName>
		</editor>
		<meeting>the International ICSC Congress on Computational Intelligence Methods and Applications<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>ICSC Academic Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="286" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning with Labeling Induced Abstentions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="12576" to="12586" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rejection strategies and confidence measures for a k-NN classifier in an OCR task</title>
		<author>
			<persName><forename type="first">J</forename><surname>Arlandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Perez-Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="576" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized Neural Framework for Learning with Rejection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">U A A</forename><surname>Minhas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning to Abstain from Binary Prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Balsubramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08151</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uncertainty-Based Rejection in Machine Learning: Implications for Model Development and Interpretability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barandas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Folgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Simão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gamboa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics (Switzerland)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification with a reject option using a hinge loss</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Wegkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1823" to="1840" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Siamese neural network based similarity metric for inertial gesture classification and rejection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berlemont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A perspective on artificial intelligence: Learning to learn</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="52" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Arbitrated Dynamic Ensemble with Abstaining for Time-Series Forecasting on Data Streams</title>
		<author>
			<persName><forename type="first">D</forename><surname>Boulegane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Madhusudan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1040" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">General solution and learning method for binary classification with performance constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bounsiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Beauseroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grall-Maës</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1455" to="1465" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vilalta</surname></persName>
		</author>
		<title level="m">Metalearning -Applications to Data Mining</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic extension and reject options for pairwise LVQ</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brinkrolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th International Workshop on Self-Organizing Maps and Learning Vector Quantization, Clustering and Data Visualization (WSOM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpretable machine learning with reject option</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brinkrolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatisierungstechnik</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="290" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generalizing Consistent Multi-Class Classification with Rejection to be Compatible with Arbitrary Losses</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to classify ordinal data: The data replication method</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Pinto Da Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1393" to="1429" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rejection Schemes in Multi-class Classification -Application to Handwritten Character Recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cecotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="445" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classification with Rejection Based on Cost-sensitive Classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Charoenphakdee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1507" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rotation-blended CNNs on a new open dataset for tropical cyclone image-to-intensity regression</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On optimum recognition error and reject tradeoff</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="46" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Clertant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sokolovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chevaleyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hanczar</surname></persName>
		</author>
		<title level="m">Interpretable cascade classifiers with abstention. AISTATS 2019 -22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probability of default estimation, with a reject option</title>
		<author>
			<persName><forename type="first">L</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian Active Meta-Learning for Reliable and Efficient AI-Based Demodulation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simeone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="5366" to="5380" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classification with reject option using contextual information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Condessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ozolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovacevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 10th International Symposium on Biomedical Imaging</title>
		<meeting><address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1340" to="1343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Condessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ozolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovačević</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01287</idno>
		<title level="m">Image Classification with Rejection using Contextual Information</title>
		<imprint>
			<date type="published" when="2015">2015a</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust hyperspectral image classification with rejection fields</title>
		<author>
			<persName><forename type="first">F</forename><surname>Condessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovacevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Hyperspectral Image and Signal Processing</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">2015b. June</date>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised hyperspectral image classification with rejection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Condessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovacevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2321" to="2332" />
			<date type="published" when="2015">2015c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Performance measures for classification systems with rejection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Condessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovačević</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="437" to="450" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An ensemble of rejecting classifiers for anomaly detection of audio events</title>
		<author>
			<persName><forename type="first">D</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Percannella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saggese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -2012 IEEE 9th International Conference on Advanced Video and Signal-Based Surveillance</title>
		<meeting>-2012 IEEE 9th International Conference on Advanced Video and Signal-Based Surveillance</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="76" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Confidence Estimation via Auxiliary Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Corbière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saporta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6043" to="6055" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimizing the error/reject trade-off for a multi-expert system using the Bayesian combining rule</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Cordelia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tortorella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1451</biblScope>
			<biblScope unit="page" from="717" to="725" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Random forest for reliable pre-classification of handwritten characters</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fontanella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scotto</forename><surname>Di Freca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -International Conference on Pattern Recognition</title>
		<meeting>-International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1319" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An adaptive reject option for LVQ classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">974</biblScope>
			<biblScope unit="page" from="68" to="73" />
			<date type="published" when="1995">1995a</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A method for improving classification reliability of multilayer perceptrons</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tortorella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1140" to="1147" />
			<date type="published" when="1995">1995b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online Learning with Abstention</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Stockholmsmässan</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1059" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Boosting with abstention</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="1668" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning with Rejection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 27th International Conference on Algorithmic Learning Theory (ALT 2016)</title>
		<meeting>The 27th International Conference on Algorithmic Learning Theory (ALT 2016)<address><addrLine>Bari, Italy; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="volume">9925</biblScope>
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Diagnostic of Pathology on the Vertebral Column with Embedded Reject Option</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Da Rocha Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IbPRIA 2011: Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6669</biblScope>
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reject options and confidence measures for kNN classifiers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dalitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Schriftenreihe des Fachbereichs Elektrotechnik und Informatik Hochschule Niederrhein</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="16" to="38" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Okati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zarezade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rodriguez</surname></persName>
		</author>
		<title level="m">Classification Under Human Assistance. 35th AAAI Conference on Artificial Intelligence, AAAI 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5905" to="5913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">To reject or not to reject: that is the question-an answer in case of neural classifiers</title>
		<author>
			<persName><forename type="first">C</forename><surname>De Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="94" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Consistency of plug-in confidence sets for classification in semi-supervised learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonparametric Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="72" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Devos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<title level="m">Versatile Verification of Tree Ensembles. Proceedings of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2654" to="2664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detecting Evasion Attacks in Deployed Tree Ensembles</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Perini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="227" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adapting cost-sensitive learning for reject option</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management, Proceedings</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1865" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ROC-based cost-sensitive classification with a reject option</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dubos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3320" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A statistical decision rule with incomplete knowledge about classes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Masson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="165" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Computer aided system diagnostic with an incomplete learning set</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dübuisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Usai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Malvache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Nuclear Energy</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="875" to="880" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On the foundations of noise-free selective classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1605" to="1641" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Agnostic selective classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards Better Selective Classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajimirsadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning and representation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<title level="m">Delegating classifiers. Proceedings, Twenty-First International Conference on Machine Learning, ICML 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<title level="m">Cautious Classifiers. Proceedings of ROC Analysis in Artificial Intelligence, 1st International Workshop (ROCAI-2004)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Datasets meta-feature description for recommending feature selection algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Filchenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pendryak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference</title>
		<meeting>Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference</meeting>
		<imprint>
			<date type="published" when="2015">2016. 2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Local Rejection Strategies for Learning Vector Quantization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wersing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">8681</biblScope>
			<biblScope unit="page" from="563" to="570" />
			<date type="published" when="2014">2014a</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient rejection strategies for prototypebased classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wersing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="334" to="342" />
			<date type="published" when="2015">2015a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wersing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.06549</idno>
		<title level="m">Optimum Reject Options for Prototypebased Classification</title>
		<imprint>
			<date type="published" when="2015">2015b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Optimal local rejection for classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wersing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="445" to="457" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rejection Strategies for Learning Vector Quantization -A Comparison of Probabilistic and Deterministic Approaches</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Villmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wersing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014b</date>
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
	<note>ESANN 2014 -Proceedings</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Villmann</surname></persName>
		</author>
		<ptr target="http://www.techfak.uni-bielefeld.de/˜fschleif/mlr/mlr.html" />
		<title level="m">A Probabilistic Classifier Model with Adaptive Rejection Option</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An Optimum Character Recognition System Using Decision Functions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Flores</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Electronic Computers</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">180</biblScope>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
	<note>EC</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On discriminative learning of prediction uncertainty</title>
		<author>
			<persName><forename type="first">V</forename><surname>Franc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prusa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th International Conference on Machine Learning, ICML 2019</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3465" to="3480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Optimal strategies for reject option classifiers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Franc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Voracek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning rejection thresholds for a class of fuzzy classifiers from possibilistic clustered noisy data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Frélicot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Fuzzy Systems Association World Congress</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On unifying probabilistic/fuzzy and possibilistic rejection-based classifiers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Frélicot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSPR /SPR 1998: Advances in Pattern Recognition</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="736" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Reject strategies driven combination of pattern classifiers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Frélicot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mascarilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="243" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Cost Curves for Abstaining Classifiers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Friedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2006 -3th workshop ROC Analysis in ML</title>
		<meeting><address><addrLine>Pittsburgh, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A survey on instance selection for active learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="283" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Application of optimum error-reject functions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kessell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="814" to="817" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Classification with reject option in text categorisation systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="582" to="587" />
		</imprint>
	</monogr>
	<note>Proceedings.</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A two-stage classifier with reject option for text categorisation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3138</biblScope>
			<biblScope unit="page" from="771" to="779" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Support Vector Machines with Embedded Reject Option</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition with Support Vector Machines</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="68" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Analysis of error-reject trade-off in linearly combined multiple classifiers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1265" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Multiple reject thresholds for improving classification reliability</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science, volume 1876 LNCS</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="863" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Concrete Dropout</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3581" to="3590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Robust classification with reject option using the self-organizing map</title>
		<author>
			<persName><forename type="first">Gamelas</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Da Rocha Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1603" to="1619" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Reject option paradigm for the reduction of support vectors. 22nd European Symposium on Artificial Neural Networks</title>
		<author>
			<persName><forename type="first">Gamelas</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Da Rocha Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Coimbra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014a</date>
			<biblScope unit="page" from="331" to="336" />
		</imprint>
	</monogr>
	<note>ESANN 2014 -Proceedings</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Classification with Reject Option Using the Self-Organizing Map</title>
		<author>
			<persName><forename type="first">Gamelas</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Da Rocha Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS, chapter Artificial</title>
		<imprint>
			<biblScope unit="volume">8681</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2014">2014b</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An ordinal data method for the classification with reject option</title>
		<author>
			<persName><forename type="first">Gamelas</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Machine Learning and Applications</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="746" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Online Selective Classification with Limited Feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gangrade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cutkosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="14529" to="14541" />
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Selective Classification via One-Sided Prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gangrade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 24th International Conference on Artificial Intelligence and Statistics (AISTATS)<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="volume">130</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A Rejection Option for the Multilayer Perceptron Using Hyperplanes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saldaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Velásquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rendón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Abundez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Valdovinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Cruz</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">6593</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">SelectiveNet: A Deep Neural Network with an Integrated Reject Option</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc. Geifman</publisher>
			<date type="published" when="2017">2017. 2019. 2019. 2019. June</date>
			<biblScope unit="page" from="3768" to="3776" />
		</imprint>
	</monogr>
	<note>Selective Classification for Deep Neural Networks International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Combining Base-Learners into Ensembles</title>
		<author>
			<persName><forename type="first">Giraud-Carrier</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Multiple classifier combination using reject options and markov fusion networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glodek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Palm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI&apos;12 -Proceedings of the ACM International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">On the error-reject trade-off in biometric verification systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Golfarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="786" to="796" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Mitigating concept drift via rejection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Göpfert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wersing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science, volume 11139 LNCS</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="456" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Support Vector Machines with a Reject Option</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Gridin</surname></persName>
		</author>
		<title level="m">Hyperparameter Optimization</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Bounded-abstaining classification for breast tumors in imbalanced ultrasound images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Mathematics and Computer Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="325" to="336" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Machine Learning with Abstention for Automated Liver Disease Diagnosis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sabih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Afsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Frontiers of Information Technology (FIT), volume 2017-Janua</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="356" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Performance visualization spaces for classification with rejection option</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hanczar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106984</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Classification with reject option in gene expression data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hanczar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="1889" to="1895" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Combination of one-class support vector machines for classification with reject option</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hanczar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases -Part I</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="547" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The Error-Reject Tradeoff</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liisberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Open Systems &amp; Information Dynamics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="184" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Classifiers with a reject option for early time-series classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hatami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Symposium on Computational Intelligence and Ensemble Learning, CIEL 2013 -2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013</title>
		<meeting>the 2013 IEEE Symposium on Computational Intelligence and Ensemble Learning, CIEL 2013 -2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The Nearest Neighbor Classification Rule with a Reject Option</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems Science and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="179" to="185" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Know Your Limits: Machine Learning with Rejection for Vehicle Engineering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">13087</biblScope>
			<biblScope unit="page" from="273" to="288" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Uncertainty-Aware Attention for Reliable Interpretation and Prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Classification with reject option</title>
		<author>
			<persName><forename type="first">R</forename><surname>Herbei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Wegkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="709" to="721" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Classification with rejection based on various SVM techniques</title>
		<author>
			<persName><forename type="first">W</forename><surname>Homenda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3480" to="3487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Classification with rejection: Concepts and evaluations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Homenda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Systems and Computing</title>
		<imprint>
			<date type="published" when="2016-12">2016. December</date>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="page" from="413" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poovendran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04318</idno>
		<title level="m">Blocking Transferability of Adversarial Examples in Black-Box Learning Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Generalized ODIN: Detecting Out-of-Distribution Image without Learning from Out-of-Distribution Data</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10948" to="10957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Self-adaptive training: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, 2020-Decem</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A method of combining multiple experts for the recognition of unconstrained handwritten numerals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="94" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Risk-Controlled Selective Prediction for Regression Deep Neural Network Models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Bootstrap methods for reject rules of fisher LDA</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jigang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhengding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -International Conference on Pattern Recognition</title>
		<meeting>-International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="425" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Towards optimally abstaining from prediction with OOD test examples</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="12774" to="12785" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Reliable prediction of anti-diabetic drug failure using a reject option</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="883" to="891" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">ATRO: Adversarial Training with a Rejection Option</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fukuhara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2020. 2019</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Delegating Classifiers for Automatic Text Categorization Delegating Classifiers for Automatic Text Categorization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Khodra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06">2016. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kocak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erkip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Second opinion needed: communicating uncertainty in medical machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kompa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Beam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Active Learning with Abstaining Classifiers for Imbalanced Drifting Data Streams</title>
		<author>
			<persName><forename type="first">L</forename><surname>Korycki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krawczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Kotelevskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Artemenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fedyanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Noskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fishkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shelmanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vazhentsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petiushko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Panov</surname></persName>
		</author>
		<title level="m">Nonparametric Uncertainty Quantification for Single Deterministic Neural Network</title>
		<imprint>
			<date type="published" when="2022">2022. NeurIPS 2022</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Linear classifier with reject option for the detection of vocal fold paralysis and vocal fold edema</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurasip Journal on Advances in Signal Processing</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Online ensemble learning with abstaining classifiers for drifting and noisy data streams</title>
		<author>
			<persName><forename type="first">B</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="677" to="692" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Securing Deep Learning Models with Autoencoder based Anomaly Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kühne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>März</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gühmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference of the PHM Society 2021</title>
		<meeting>the European Conference of the PHM Society 2021</meeting>
		<imprint>
			<publisher>PHM Society</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="221" to="233" />
		</imprint>
	</monogr>
	<note>Virtual event</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Local reject option for deterministic multi-class SVM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kummert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gopfert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning-ICANN 2016: 25th International Conference on Artificial Neural Networks</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-09-06">2016. September 6-9, 2016</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Moderating the outputs of support vector machine classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1018" to="1031" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Optimal combinations of pattern classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="945" to="954" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">The interaction between classification and reject performance for distance-based reject-option classifiers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Landgrebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paclík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="908" to="917" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A combining strategy for ill-defined problems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Landgrebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paclík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th Annual Symposium of the Pattern Recognition Association of South Africa</title>
		<meeting>of the 15th Annual Symposium of the Pattern Recognition Association of South Africa</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">How to define a rejection class based on model learning?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laroui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vernay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Villiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Debreuve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laroui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vernay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Villiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laroui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recogniton</title>
		<meeting><address><addrLine>Milano</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">A family of measures for best top-n class-selective decision rules</title>
		<author>
			<persName><forename type="first">H</forename><surname>Le Capitaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frélicot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="552" to="562" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Wornell</surname></persName>
		</author>
		<title level="m">Fair Selective Classification Via Sufficiency. Proceedings of the 38th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="6076" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Classification with confidence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="755" to="769" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Sleep Versus Wake Classification From Heart Rate Variability Using Computational Intelligence: Consideration of Rejection in Classification Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lewicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Sazonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Corwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schuckers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="118" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Confidence-based classifier design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1230" to="1240" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Twin SVM with a reject option through ROC curve</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1710" to="1732" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">SCRIB: Set-Classifier with Class-Specific Risk Bounds for Blackbox Models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Westover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="7497" to="7505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Incorporating uncertainty in learning to defer algorithms for safe computer-aided diagnosis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barbieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Classification with a reject option under Concept Drift: The Droplets algorithm</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">X</forename><surname>Loeffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Detyniecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Data Science and Advanced Analytics</title>
		<meeting>the 2015 IEEE International Conference on Data Science and Advanced Analytics</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Pattern rejection strategies for the design of self-paced EEG-based brain-computer interfaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mouchère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lécuyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -International Conference on Pattern Recognition</title>
		<meeting>-International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Rejection Criteria and Pairwise Discrimination of Handwritten Numerals Based on Structural Features</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="228" to="238" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A support vector machines-based rejection technique for speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Randolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Drish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="381" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Predict responsibly: Improving fairness and accuracy by learning to defer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, 2018-Decem</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6147" to="6157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Novelty detection: a review-part 1: statistical approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Markou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2497" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">An empirical comparison of ideal and empirical ROC-based reject rules</title>
		<author>
			<persName><forename type="first">C</forename><surname>Marrocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Molinara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tortorella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4571</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Semi-Supervised Learning from Active Noisy Soft Labels for Anomaly Detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Perini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Uncertainty-Based Rejection Wrappers for Black-Box Classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="101721" to="101746" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Classification with reject option for software defect prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P P</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Da Rocha Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing Journal</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1085" to="1093" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Mozannar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01862</idno>
		<title level="m">Consistent Estimators for Learning to Defer to an Expert</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Classifier design with incomplete knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Muzzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pierson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="345" to="369" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Accuracy-Rejection Curves (ARCs) for Comparing Classification Methods with a Reject Option</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S A</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hanczar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning in Systems Biology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="65" to="81" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Whye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balaji</surname></persName>
		</author>
		<title level="m">Hybrid Models with Deep and Invertible Features (ICML</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Epistemic Uncertainty Sampling</title>
		<author>
			<persName><forename type="first">V.-L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Destercke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference On Discovery Science DS 2019</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11828</biblScope>
			<biblScope unit="page" from="72" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Reliable multilabel classification: Prediction with partial abstention</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020 -34th AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5264" to="5271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Multilabel classification with partial Abstention: Bayes-optimal prediction under label independence</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="613" to="665" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Charoenphakdee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10655</idno>
		<title level="m">On the Calibration of Multiclass Classification with Rejection</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="31" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14785</idno>
		<title level="m">Adversarial Training with Rectified Rejection</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022-06">2022. June</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="15202" to="15212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Tradingoff coverage for accuracy in forecasts: Applications to clinical data analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schulenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Symposium on AI in Medicine</title>
		<meeting>the AAAI Symposium on AI in Medicine</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="100" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Background check: A general technique to build more reliable and versatile classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perello-Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Filho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -IEEE International Conference on Data Mining, ICDM</title>
		<meeting>-IEEE International Conference on Data Mining, ICDM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1143" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Estimating the Contamination Factor&apos;s Distribution in Unsupervised Anomaly Detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Perini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Buerkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022a. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">A Ranking Stability Measure for Quantifying the Robustness of Anomaly Detection Methods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Perini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Galvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vercruyssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Computer and Information Science</title>
		<imprint>
			<biblScope unit="volume">1323</biblScope>
			<biblScope unit="page" from="397" to="408" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">How to Allocate your Label Budget? Choosing between Active Learning and Learning to Reject in Anomaly Detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Perini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Giannuzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Quantifying the Confidence of Anomaly Detectors in Their Example-Wise Predictions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Perini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vercruyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</title>
		<meeting>European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases<address><addrLine>Ghent, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note>ECML-PKDD</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Transferring the Contamination Factor between Anomaly Detection Domains by Shape Similarity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Perini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vercruyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022</title>
		<meeting>the 36th AAAI Conference on Artificial Intelligence, AAAI 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022b</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="4128" to="4136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Optimizing abstaining classifiers using ROC analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pietraszek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning -ICML &apos;05</title>
		<meeting>the 22nd international conference on Machine learning -ICML &apos;05<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="665" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">On the use of ROC analysis for the optimization of abstaining classifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pietraszek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="169" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">A Classification Approach with a Reject Option for Multi-label Problems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">6978</biblScope>
			<biblScope unit="page" from="98" to="107" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Multi-label classification with a reject option</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2256" to="2266" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">A review of novelty detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="215" to="249" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Multi-level classification of emphysema in HRCT lung images using delegated classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">5241</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="66" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Almanza-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Anomaly detection. Computers, Materials and Continua</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Exponential Savings in Agnostic Active Learning Through Abstention</title>
		<author>
			<persName><forename type="first">N</forename><surname>Puchkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhivotovskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4651" to="4665" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Multistage pattern recognition with reject option</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovičova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bláha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -International Conference on Pattern Recognition</title>
		<meeting>-International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="92" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">AUC-based Selective Classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pugnana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page">206</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Pugnana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<title level="m">A Model-Agnostic Heuristics for Selective Classification. Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="9461" to="9469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">An evaluation of multi-expert configurations for the recognition of handwritten numerals</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fairhurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1255" to="1273" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Consistent algorithms for multiclass classification with an abstain option</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="530" to="554" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pugnana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>State</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<title level="m">Can We Trust Fair-AI ? AAAI 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Gaussian process regression: active data selection and test point rejection</title>
		<author>
			<persName><forename type="first">Sambu</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium</title>
		<meeting>the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="241" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">A classification reliability driven reject rule for multi-expert systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tortorella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">06</biblScope>
			<biblScope unit="page" from="885" to="904" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">On optimal reject rules and ROC curves</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Santos-Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Pires</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="943" to="952" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Trading off mistakes and don&apos;t-know predictions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sayedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zadimoghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010, NIPS 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Reliable classification: Learning classifiers that distinguish aleatoric and epistemic uncertainty</title>
		<author>
			<persName><forename type="first">R</forename><surname>Senge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bösner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dembczyński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haasenritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Donner-Banzhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">255</biblScope>
			<biblScope unit="page" from="16" to="29" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Active Learning Literature Survey</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Evidence-based uncertainty sampling for active learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="202" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09561</idno>
		<title level="m">Binary Classification with Bounded Abstention Rate</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Active Learning for Classification with Abstention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Information Theory -Proceedings</title>
		<imprint>
			<date type="published" when="2020-06-02">2020. 2020-June(2</date>
			<biblScope unit="page" from="2801" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">AUC optimization with a reject option</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020 -34th AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5686" to="5691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">A novel meta learning framework for feature selection using data synthesis and fuzzy similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Garibaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Fuzzy Systems</title>
		<imprint>
			<date type="published" when="2020-07">2020. 2020-July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Probabilistic personalised cascade with abstention</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shpakova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sokolovska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="8" to="15" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">An Approach to Novelty Detection Applied to the Classification of Image Regions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Markou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="396" to="407" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Deep neural rejection against adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sotgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Information Security</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Reject-Optional LVQ-Based Two-Level Classifier to Improve Reliability in Footstep Identification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Suutala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pirttikangas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riekki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Röning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3001</biblScope>
			<biblScope unit="page" from="182" to="187" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Highly Accurate Recognition of Human Postures and Activities Through Classification With Rejection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Sazonov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="309" to="315" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Growing a multi-class classifier with a reject option</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1565" to="1570" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Aggregating Abstaining and Delegating Classifiers For Improving Classification performance : An application to lung cancer survival prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Temanni</surname></persName>
		</author>
		<author>
			<persName><surname>-R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Nadeem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Camda</title>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<date type="published" when="2007-01">2007. January 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Combating Label Noise in Deep Learning Using Abstention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mohd-Yusof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36 th International Conference on Machine Learning</title>
		<meeting>the 36 th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">An Optimal Reject Rule for Binary Classifiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tortorella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science, volume 1876 LNCS</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="611" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">A meta-learning BCI for estimating decision confidence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tremmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandez-Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Citi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neural Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Ulmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cinà</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05329</idno>
		<title level="m">Know Your Limits: Uncertainty Estimation with ReLU Classifiers Fails at Reliable OOD Detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Gradient descent learning of nearest neighbor classifiers with outlier rejection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Urahama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="761" to="768" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Reject option for VQ-based Bayesian classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vailaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 15th International Conference on Pattern Recognition. ICPR-2000</title>
		<meeting>15th International Conference on Pattern Recognition. ICPR-2000</meeting>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="48" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">COBRASTS: A New Approach to Semi-supervised Clustering of Time Series</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Craenendonck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumančić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<idno>11198 LNAI</idno>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="179" to="193" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">A reject option for automated sleep stage scoring</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbraecken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Interpretable Machine Learning in Healthcare</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Virtual event</note>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">A novel reject option applied to sleep stage scoring</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Der Plas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbraecken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2023 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Reliable classifiers in ROC space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vanderlooy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sprinkhuizen-Kuyper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Smirnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th BENELEARN Machine Learning Conference</title>
		<meeting>the 15th BENELEARN Machine Learning Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006a</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">An analysis of reliable classifiers through ROC isometrics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vanderlooy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Sprinkhuizen-Kuyper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Smirnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ROC Analysis in Machine Learning</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2006">2006b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<title level="m">Meta-Learning: A Survey</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">A Kernel Based Rejection Method for Supervised Classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="312" to="321" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">A risk bound for ensemble classification with a reject option</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Statistical Signal Processing Workshop (SSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="769" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Enhanced reliability of multilayer perceptron networks through controlled pattern rejection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fairhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bisset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">261</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Investigating feedforward neural networks with respect to the rejection of spurious patterns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fairhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bisset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Semi-Supervised Anomaly Detection with an Application to Water Analytics</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vercruyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Verbruggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -IEEE International Conference on Data Mining, ICDM, 2018-Novem</title>
		<meeting>-IEEE International Conference on Data Mining, ICDM, 2018-Novem</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Learning Vector Quantization with Adaptive Cost-Based Outlier-Rejection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Villmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Biehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">9257</biblScope>
			<biblScope unit="page" from="772" to="782" />
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Classification with Rejection: Scaling Generative Classifiers with Supervised Deep Infomax</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Yiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2980" to="2986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Lasso type classifiers with a reject option</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Wegkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="155" to="168" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Support vector machines with a reject option</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Wegkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1368" to="1385" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">A Novel Classification-Rejection Sphere SVMs for Multiclass Classification Problems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Natural Computation (ICNC 2007)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="34" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Methods of Combining Multiple Classifiers and Their Applications to Handwriting Recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krzyzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y C</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krzyżak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y C</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="418" to="435" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Classification methods with reject option based on convex risk minimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Wegkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="111" to="130" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Reliable classification of vehicle types based on cascade classifier ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="322" to="332" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Beyond disagreement-based agnostic active learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="442" to="450" />
			<date type="published" when="2014-01">2014. January</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">On Reject and Refine Options in Multicategory Classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">522</biblScope>
			<biblScope unit="page" from="730" to="745" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">RO-SVM: Support Vector Machine with Reject Option for Image Categorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procedings of the British Machine Vision Conference</title>
		<meeting>edings of the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>British Machine Vision Association</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="1" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Cost-sensitive SVM with Error Cost and Class-dependent Reject Cost</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>-H</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Theory and Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="130" to="135" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Reject Before You Run: Small Assessors Anticipate Big Language Models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Martínez-Plumed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3169</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Heartbeat classifcation using support vector machines (SVMs) with an embedded reject option</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zidelmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amirou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belouchrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page">1250001</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Deep gamblers: Learning to abstain with portfolio theory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ziyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Weighted extreme learning machine for imbalance learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="229" to="242" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Cost-sensitive Multi-class SVM with Reject Option: A Method for Steam Turbine Generator Fault Diagnosis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Theory and Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="83" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
