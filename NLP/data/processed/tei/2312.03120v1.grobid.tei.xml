<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning</title>
				<funder ref="#_BPzSEYY">
					<orgName type="full">Battelle for the U.S. Department of Energy</orgName>
				</funder>
				<funder ref="#_KBVjGXb">
					<orgName type="full">U.S. DOE Office of Science, Office of Advanced Scientific Computing Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-12-05">5 Dec 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Omer</forename><surname>Subasi</surname></persName>
							<email>omer.subasi@pnnl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">High Performance Computing Group</orgName>
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<addrLine>902 Battelle Blvd</addrLine>
									<postCode>99354</postCode>
									<settlement>Richland</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oceane</forename><surname>Bel</surname></persName>
							<email>obel@pnnl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">High Performance Computing Group</orgName>
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<addrLine>902 Battelle Blvd</addrLine>
									<postCode>99354</postCode>
									<settlement>Richland</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Manzano</surname></persName>
							<email>joseph.manzano@pnnl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">High Performance Computing Group</orgName>
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<addrLine>902 Battelle Blvd</addrLine>
									<postCode>99354</postCode>
									<settlement>Richland</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Barker</surname></persName>
							<email>kevin.barker@pnnl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">High Performance Computing Group</orgName>
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<addrLine>902 Battelle Blvd</addrLine>
									<postCode>99354</postCode>
									<settlement>Richland</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-12-05">5 Dec 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">0D84904A3A69DF9418DA62BD5969FD69</idno>
					<idno type="arXiv">arXiv:2312.03120v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Distributed Machine Learning</term>
					<term>Deep Learning</term>
					<term>Federated Learning</term>
					<term>Parallel and Distributed Computing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cuttingedge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the last decade, Machine Learning (ML) has been applied to ever increasing immense amount of data that is becoming available as more people become daily users of internet, mobile and wireless networks. Coupled with the significant advances in deep learning (DL), ML has found more complex applications: from medical to machine translation and speech recognition, to intelligent object recognition, and to smart cities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Modern parallel and heterogeneous computing systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> have enabled such applications by supporting highly parallel training. These large-scale and distributed systems therefore have become the backbone of modern ML <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Federated Learning (FL), as a sub-field of DL, has emerged as a distributed learning solution to provide data privacy <ref type="bibr" target="#b8">[9]</ref>. Ever since its inception <ref type="bibr" target="#b8">[9]</ref>, FL has been studied extensively and adapted widely <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>In this study, we review the current landscape of modern ML systems and applications, and offer an overview as a self-contained text. While there are many surveys on large-scale <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>, distributed ML <ref type="bibr" target="#b6">[7]</ref>, DL <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14]</ref>, and FL <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, we instead provide a high-level joint view of modern parallel and distributed ML and FL. In this way, our work differentiates itself from the existing literature. In brief, our study</p><p>• presents the concepts and methods of ML and DL. • discusses the parallelism and scaling approaches of large-scale distributed ML. Moreover, it explores the communication aspects, such as costs, topologies, and networking, of parallel and distributed training and inference. • introduces FL, its applications and aggregation methods. It then elaborates on the security and privacy aspects as well as the existing platforms and datasets. • summarizes open research questions in the modern landscape of parallel and distributed ML, DL and FL.</p><p>Figure <ref type="figure">1</ref> outlines and summarizes our study.</p><p>Our study is organized as follows: Section 2 overviews the related work on large-scale and distributed ML. Section 3 provides the background on ML. Section 4 discusses distributed ML. Section 5 presents FL. Section 6 summarizes the existing open challenges. Finally, Section 7 concludes our review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Surveys pertaining to parallel, distributed and large scale ML have been very numerous in the literature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Our work is different and unique because it provides an introductory review of the latest joint landscape of ML, DL and FL.</p><p>Different than the general surveys such as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, some surveys offer in depth cost and comparisons of algorithms and methods both theoretically and empirically <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Many studies focus on distributed DL. Some of them are <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref>. Moreover, there exists a significant number of surveys that focus on specific types of models such as <ref type="bibr" target="#b20">[21]</ref> for graph neural networks (GNNs), <ref type="bibr" target="#b21">[22]</ref> for Internet-of-Things (IoTs), <ref type="bibr" target="#b22">[23]</ref> for wireless networks, <ref type="bibr" target="#b23">[24]</ref> for mobile and 5G networks or for specific target environments such as <ref type="bibr" target="#b24">[25]</ref> for unmanned aerial vehicles (UAV).</p><p>FL literature unsurprisingly offers many surveys. Some of the latest surveys are <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Among studies having specific topics, <ref type="bibr" target="#b25">[26]</ref> surveys privacy and security methods for FL, <ref type="bibr" target="#b26">[27]</ref> discusses block chain-based FL. <ref type="bibr" target="#b27">[28]</ref> presents differential privacy for FL. <ref type="bibr" target="#b12">[13]</ref> offers a survey of FL for IoT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction to ML</head><p>ML is the process of learning from data to perform complex tasks for which there is no known deterministic and algorithmic solution, or building such a solution is not practical. For instance, developing a deterministic algorithm based on rules to detect spam emails is highly impractical. It is not possible to know the exact list of the detection rules. In addition, these rules most often change over time. Since the list of the rules may be ever-increasing and even contradictory, the maintenance of such algorithms would require constant labor.</p><p>The ML process is mainly two-fold: Training and prediction (inference). In the training phase, the parameters of a learning model are optimized based on data. In the prediction phase, the trained model is deployed to perform predictions on new data. While in most cases the training and prediction phases are mutually exclusive, in incremental learning cases, they are coupled together. The models in these cases are continuously trained and make predictions. Figure <ref type="figure">2</ref> visualizes the training and prediction phases.</p><p>The main goal of ML is to generalize such that it performs well with unseen data. However, this goal contradicts its optimization goal in which ML tries to minimize the training loss with the training data. As a result, the well-known bias-variance problem emerges. If an ML model over-fits the training data, that is, having high variance, it performs poorly with the unseen data. On the other hand, if the model under-fits, that is, having high bias, it does not learn important patterns or regularities in the data. Over-fitting typically happens when a model is too complex for the underlying problem. In contrast, under-fitting happens when the model is too simple. Figure <ref type="figure" target="#fig_6">3</ref> depicts the bias-variance trade-off.</p><p>In the following, we present different types of ML tasks. After that, we look into different problems that ML can solve. Then, we review widely used ML algorithms and methods. Finally, we survey the existing ML platforms that are not supported with specialized hardware and not suited for DL or FL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>§3: Machine Learning</head><formula xml:id="formula_0">• §3.B: Algorithms • Feedback based • Target problem based • Algo. approach based • §3.C: Frameworks • Scikit-Learn • Weka • XGBoost • Shogun • LibSVM • Cloud based • §4.B: Parallelism Types • Data • Model • Pipeline • §4.C: Vertical Optimization • Model Simplification • Optimization Approximation • Communication Optimization • §4.D: Comm.</formula><p>Topologies • Centralized • Hierarchical • Fully Distributed §5: Federated Learning §6: Open Questions &amp; Challenges • §6.A: Parallel &amp; Distributed ML • §6.B: Federated Learning Our Review • §4.E: Sync Models • Bulk Synchronous • Stale Synchronous • Approximate Synchronous • Asynchronous • §4.F: Distro. ML Frameworks • Tensorflow • PyTorch • … §4: Distributed Learning • §5.B: Aggregation Algorithms ML Algorithm Trained Model Prediction Training Phase Prediction Phase (Inference) Training data New data Hyperparameters Fig. 2: ML phases: Training and prediction (inference).</p><formula xml:id="formula_1">• FedAvg • FedProx, • SCAFFOLD • FedSGD, • FedOpt • … • §5.C: Security &amp; Privacy • Attacks • Defenses • §5.D: Frameworks • Tensorflow Federated • IBM Federated • NVIDIA FLARE • FedML • FATE • PySyft • OpenFL • §5.E: Datasets</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ML Algorithms</head><p>ML algorithms can be categorized by the format and requirements of data (external feedback), by the type of problems they are designed for (target problem), and by the techniques they use (algorithmic approaches).</p><p>It is worth noting that there is another way of categorizing ML: online and offline. In offline learning, the entire training data is available prior to training. This is the most common application of ML. In online learning <ref type="bibr" target="#b28">[29]</ref>, either the entire data is not available beforehand or it is computationally infeasible to perform training over the entire data at once. An example of the former is sequential training such as time series analysis in financial markets. An example of the latter is learning with a very large dataset which does not fit into the memory and consequently, training becomes prohibitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Complexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>Fig. <ref type="figure" target="#fig_6">3</ref>: Bias-variance trade-off. Model complexity with respect to bias and variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">External feedback</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML algorithms can be classified based on the external feedback as follows:</head><p>Supervised Learning: Learning is performed by feeding labeled input data so that a model's parameters are optimized. Labeled data can be desired classes, categories, or numerical outputs corresponding to the training instances. During training, the optimization is achieved by minimizing a predetermined cost function. After training, the trained model is deployed to predict the outputs of new instances. An example supervised learning is to classify newly seen handwritten digits by training with the labeled digits.</p><p>Unsupervised Learning: The goal of unsupervised learning is to find structures and patterns Fig. <ref type="figure">4</ref>: An artificial neural network example. in unlabeled data. This means that in unsupervised learning, the data does not possess desired outputs. As an example of unsupervised learning, clustering aims to find similar groups (clusters) in given data. Dimensionality reduction is another example of unsupervised learning where the goal is to find a subset of key features that describes the data well.</p><p>Semi-supervised Learning: In semi-supervised learning, the amount of labeled data is small while the amount of unlabeled data is large. Clustering algorithms are typically used to propagate existing labels to the unlabeled data. An assumption of semi-supervised learning is that similar data shares the same label.</p><p>Reinforcement Learning: Reinforcement learning is applied when an agent interacts with an environment. Based on the observations it makes, the agent takes actions. The actions are rewarded or penalized according to a reward function. Applications of reinforcement learning lie in the fields such as game theory, robotics and industrial automation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Target Problem</head><p>Under this categorization, ML algorithms are grouped according to the kind of problems they are designed to solve.</p><p>In classification problems, the aim is to correctly categorize data instances into the known classes.</p><p>In regression problems, the goal is to estimate the value of a variable based on other variables (features).</p><p>Clustering finds the distinct groups of similar data instances based on a selected similarity metric.</p><p>Anomaly and novelty detection is used to find data instances that are significantly different than others. These instances are called outliers. In anomaly detection, training data consists of both outliers and regular (expected) data instances. In novelty detection, on the other hand, the goal is used to detect unseen data where training data is free of outliers.</p><p>Dimensionality reduction is used to reduce the number features of the training data. In dimensionality reduction, if a subset of the original set of the features is selected, it is called feature selection. In contrast, if features are combined into new ones, it is called feature extraction. Dimensionality reduction can also be used to decrease computational costs of training. Furthermore, it can also be used to prevent over-fitting. The problem of over-fitting with high-dimensional data is famously known as the curse of dimensionality. The curse of dimensionality arises due to data sparsity in high dimensional spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Algorithmic Approaches</head><p>ML algorithms can be categorized based on algorithmic approaches that they employ.</p><p>Stochastic Gradient Decent (SGD) based algorithms are optimized based on a loss function of the outputs of the model parameters in the opposite direction of the gradient. Because at each training step a random subset of data is used, this optimization method is called stochastic. Many common ML algorithms are optimized with SGD such as artificial neural networks. Support Vector Machines (SVMs) <ref type="bibr" target="#b29">[30]</ref> are typically used when the input data is not linearly separable in its original space. They map the input data to high dimensional spaces where it becomes linearly separable. SVMs can be used for classification, regression, and novelty detection.</p><p>Artificial Neural Networks (ANNs) are constructed by multiple layers of nodes (neurons) that have inputs, outputs, corresponding feature weights, and an activation function. Layers can be input, hidden, and output layers. ANNs have recently been very successful in tasks such as image classification, object detection, and natural language processing. Figure <ref type="figure">4</ref> depicts an example of an ANN. Some well-known types of ANNs include:</p><p>• Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b30">[31]</ref> are deep neural networks that incorporate convolutions and pooling. While convolutions help with learning local data, pooling help with learning abstract features. CNNs have been extremely successful in tasks such as image classification, object detection, and image segmentation. • Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b31">[32]</ref> maintains a temporal state of sequence data. The temporal state may hold short-term or longterm memory. RNNs are used in tasks such as time series forecasting, natural language processing, and anomaly detection. • Autoencoders <ref type="bibr" target="#b32">[33]</ref> are ANNs that learn latent representations of input data with no supervision. They are used for dimensionality reduction and visualization of high dimensional data. • Generative Adversarial Networks (GANs) <ref type="bibr" target="#b33">[34]</ref> are (originally unsupervised) neural networks used to generate data based on a game between a generator and discriminator network. They have been successfully applied in supervised and semi-supervised learning. • Graph Neural Networks (GNNs) <ref type="bibr" target="#b34">[35]</ref> are a type of ANNs designed to perform learning and prediction on data described by graphs. GNNs provide an easy way to do node, edge, and graph level ML tasks. • Self-Organizing Maps (SOMs) <ref type="bibr" target="#b35">[36]</ref> are neural networks which produce a low dimensional representation of high dimensional data. SOMs are used for visualization, clustering, and classification. The training is unsupervised where after random initialization, neurons compete against each other. • Boltzmann Machines [37, 38] are fully connected ANNs which, unlike other ANNs, have probabilistic activation functions. Neurons output 1 or 0 based on Boltzmann distribution. Boltzmann Machines can be used for classifying, denoising, or completing images. • Deep Belief Networks <ref type="bibr" target="#b38">[39]</ref> are stacked Boltzmann Machines designed to tackle larger and more complex learning challenges. They are used for semi-supervised learning. • Hopfield Networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> are fully connected networks that are used for tasks such as character recognition.</p><p>Transformers <ref type="bibr" target="#b41">[42]</ref> are a class of DL models that has shown extraordinary success in many ML fields including natural language processing and computer vision. Transformers were first introduced by a landmark paper from Google <ref type="bibr" target="#b42">[43]</ref> which were based on a novel mechanism called Attention. At its core, a transformer is an encoderdecoder model. The success of Transformers has become a regular news-headliner such as the release of GPT-4 <ref type="bibr" target="#b43">[44]</ref> and ChatGPT <ref type="bibr" target="#b44">[45]</ref>.</p><p>Rule-based algorithms <ref type="bibr" target="#b45">[46]</ref> use a set of rules to learn patterns from the input data. They are typically easier to interpret than other ML algorithms. Decision trees are the most well-known rule-based algorithms.</p><p>Evolutionary algorithms <ref type="bibr" target="#b46">[47]</ref> use ideas from biological evolution. In evolutionary algorithms, the target problem is represented by a set of properties. The performance metric is called fitness function. Based on fitness scores, the set of properties is mutated and crossed over. These algorithms iterate until accurate estimates are obtained. Evolutionary algorithms can also be used to create other algorithms such as neural networks.</p><p>Semantic and Topic algorithms <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref> are used to learn specific semantic patterns and distinct relationships in the input data. An example application of these algorithms is to find the topics and relate them to each other in a given set of documents.</p><p>Ensemble algorithms combine other algorithms to obtain a solution that performs better than the individual algorithms. Different ways to build ensembles are:</p><p>• Bagging combines multiple classifiers and uses voting to determine the final output. • Boosting is a technique that trains the subsequent models with the data instances misclassified by the preceding models in the chain. • Stacking is the process where a model trains with the outputs of the preceding models in a chain of several models. Stacking typically reduces the classification variance. • Random Forests combine multiple decision trees and output an (weighted) average of the outputs of the individual trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Existing ML Frameworks</head><p>In this section, we present the existing ML platforms that are not supported with specialized hardware and typically not suited for DL or FL.</p><p>We then briefly mention the popular ML services in the cloud. Scikit-Learn <ref type="bibr" target="#b49">[50]</ref> is the most popular opensource Python library that offers an extensive suite of ML algorithms. The library is very well maintained and provides a comprehensive set of algorithms, methods, pre-processing, pipelining, model selection and hyper-parameter search capabilities. It provides interfaces to work with NumPy and SciPy packages.</p><p>Weka <ref type="bibr" target="#b50">[51]</ref> is a general-purpose and popular Java ML library. It provides a large collection of algorithms and visualization tools. Weka supports numerous tasks such as pre-processing, classification, regression, clustering and visualization.</p><p>XGBoost <ref type="bibr" target="#b51">[52]</ref> is a scalable and distributed gradient boosting library based on decision trees. It implements parallel ML algorithms for classification, regression and ranking tasks.</p><p>Shogun <ref type="bibr" target="#b52">[53]</ref> is a research-oriented open-source ML library. It offers a large number of ML algorithms and cross-platform support by providing bindings with other languages and environments such as Python, Octave, R, Java. Shogun's core library is implemented in C++.</p><p>LibSVM <ref type="bibr" target="#b53">[54]</ref> is a specialized C/C++ library for SVMs. It provides interfaces for Python, R, MATLAB and many others.</p><p>Many companies offer standard ML and distributed ML services. Moreover, these services often include the support for GPUs and other ML specific hardware. Popular cloud ML services are Google's Cloud <ref type="bibr" target="#b54">[55]</ref>, Microsoft Azure <ref type="bibr" target="#b55">[56]</ref>, Amazon's SageMaker <ref type="bibr" target="#b56">[57]</ref> and the IBM Watson Cloud <ref type="bibr" target="#b57">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distributed Machine Learning</head><p>In this section, we introduce large-scale distributed ML. We then explore different types of parallelisms used in distributed training. Next, we dive into vertical scaling techniques. After that, we present the optimizations for communications in distributed ML. Then we continue with the communication topologies and synchronization models. Finally, we conclude this section by the discussion of the existing distributed ML frameworks.</p><p>As a side note, we use client and participant interchangeably in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Introduction to Distributed ML</head><p>Distributed ML is proposed to utilize distributed and heterogeneous computing systems to solve large and complex problems where a solution cannot be obtained by a single standalone homogeneous computing device. Distributed ML offers two different approaches. The first is to use heterogeneous resources available in a single computing system such as Graphical Processing Units (GPUs). This is called vertical scaling. The second is to use multiple machines to solve larger problems and to support fault-tolerance. This is called horizontal scaling.</p><p>GPUs have been the most common mean of vertical scaling. Given sufficient parallelism, it has been shown that GPUs significantly accelerate training <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>. For instance, NVIDIA GPUs have been popular in accelerating ML <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b60">61]</ref>. Vendors such as Google have implemented their own specific hardware accelerators. Tensor Processing Units (TPUs) <ref type="bibr" target="#b59">[60]</ref> are designed specifically for this purpose. Others such as Graphcore <ref type="bibr" target="#b61">[62]</ref> and SambaNova <ref type="bibr" target="#b62">[63]</ref> have followed this trend with sophisticated dataflow-based hardware designs and powerful system software tool-chains.</p><p>In contrast to vertical scaling, horizontal scaling corresponds to distributed training and inference across multiple machines. Horizontal scaling enables ML solutions to handle applications and data that do not fit in the resources of a single machine. Additionally, the usage of multiple machines typically accelerate training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parallelisms in Distributed</head><p>Training and Inference</p><p>There are three types of parallelisms used in distributed training. These are data, model and pipeline parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Data Parallelism</head><p>In data parallelism, the same ML model is trained with different subsets of the data in parallel at different computing resources. Once all computing resources finish the assigned training, the models are accumulated and an average model is obtained. Then, this average model is distributed back to each computing resource for the subsequent rounds of training. Figure <ref type="figure" target="#fig_1">5</ref> depicts data parallelism with two parallel resources.</p><p>The main advantage of data parallelism is that it is applicable to any distributed ML model without requiring expert/domain knowledge. It is also very scalable for compute-intensive models, such as CNNs. One disadvantage of data parallelism is that model synchronization may become a bottleneck. Another disadvantage occurs when the model does not fit in the memory of a single device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Model Parallelism</head><p>In model parallelism, the model is partitioned and distributed to different computing resources. The data is distributed as well according to the model distribution. When there is a dependency among the computing resources, synchronization is needed for the parameters (weights) to be shared consistently. Figure <ref type="figure" target="#fig_2">6</ref> shows model parallelism where two resources are used. An important note is that in the figure, every time that a dashed line crosses a resource boundary, at least one synchronization event must take place to ensure data consistency.</p><p>The main advantage of model parallelism is that models take less memory in each single resource (device). Its main disadvantage is that the model partitioning is often nontrivial. Another disadvantage is the potential intensive communications among the resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Pipeline Parallelism</head><p>Pipeline parallelism combines model and data parallelisms. It distributes the model and data in a such a way that there is a pipeline among the computing resources in which each resource has a different part of the model. Pipeline parallelism maintains the advantages of model parallelism while increasing the resource utilization. Figure <ref type="figure" target="#fig_3">7</ref> illustrates pipeline parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Vertical Optimization Approaches</head><p>We have discussed the types of parallelisms used in distributed ML above. Now, we explore three vertical optimization approaches. They are model simplification, optimization approximation, and communication optimization approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Model Simplification</head><p>Model simplification refers to the reformulation of a target model to decrease its computational complexity as a way of achieving efficiency. Model simplification can be further divided into categories based on the type of the ML models. These models can be based on kernels, trees, graphs and deep neural networks. Table <ref type="table" target="#tab_3">1</ref> summarizes the model simplification techniques. Simplifications for kernel-based models are made by sampling-based or projection-based approximations. While sampling-based methods <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref> approximate kernel matrices by random samples, projection-based methods <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref> use Gaussian or sparse random projections to map the data features to low dimensional sub-spaces.</p><p>Performance and scalability improvements for tree-based models, such as decision trees and random forests, are commonly based on rule <ref type="bibr" target="#b67">[68]</ref> or feature sampling <ref type="bibr" target="#b51">[52]</ref>  <ref type="bibr" target="#b68">[69]</ref>.</p><p>Graph-based simplifications are developed for graph-based models where nodes represent the data instances and edges represent the similarity between the instances. In these models, the cost of training comes from two main sources: graph construction and the label matrix inversion. For sparse graphs, graph construction constitutes the main cost of training. This is because when label  propagation is used, it lowers the cost of the inversion of the label matrix and it becomes less costly than graph construction. As a result, graph construction dominates the main computational cost. To construct sparse graphs <ref type="bibr" target="#b69">[70]</ref>, hashing methods <ref type="bibr" target="#b70">[71]</ref> [72] are often used. Different than sparse graph models, there are also graph models that are built by anchor graphs <ref type="bibr" target="#b72">[73]</ref>. An anchor graph is a hierarchical representation of a target graph. It is built with a small subset of the instances. This small subset is used to retain the similarities between all instances. In such a representation, the label matrix inversion is the main cost of training. To reduce the cost of the matrix inversion, the pruning of anchors' adjacency <ref type="bibr" target="#b73">[74]</ref> is a common technique.</p><p>Performance improvements for deep neural networks can be achieved in two different ways. First, activation functions, such as Rectified Linear Unit (ReLU) <ref type="bibr" target="#b74">[75]</ref> and its variants <ref type="bibr" target="#b75">[76]</ref> [77], can be employed instead of the expensive functions, such as sigmoid and tanh, which use the exponential function. Other techniques, specifically for CNNs, involve depth-wise filter factorization <ref type="bibr" target="#b77">[78]</ref> and group-wise convolutions <ref type="bibr" target="#b78">[79]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Optimization Approximation</head><p>Optimization approximation is a family of techniques that are used to reduce the cost of the optimization related computations, i.e., gradient computations, for training. It is generally realized by computing the gradients with a small number  of instances or parameters instead of all instances or parameters. Care has to be taken since such approximations can lead to longer convergence times, local extrema, or even non-convergence. Optimization approximation can be categorized based on the specific optimization algorithm that is being used: Mini-batch gradient descent, coordinate descent, and numerical integration based on Markov chain Monte Carlo. Table <ref type="table">2</ref> shows the existing techniques.</p><p>Techniques that are used for mini-batch gradient descent approximations are adaptive sampling of mini-batches, adaptive learning rates, and the improvements in gradient approximations. Adaptive sampling <ref type="bibr" target="#b79">[80]</ref> [81] for mini-batches takes the data distribution and gradient contributions into account rather than just using random batches of samples or making a gradual increase in the batch size <ref type="bibr" target="#b81">[82]</ref>. Learning rates are also crucial in terms of achieving fast convergence <ref type="bibr" target="#b82">[83]</ref>. Adaptive learning rates can boost the speed and quality of convergence <ref type="bibr" target="#b83">[84]</ref>. Further adaptive adjustments are shown to be effective <ref type="bibr">[85] [86]</ref>. Complementary to adaptive sampling or adaptive learning rates, reducing the variance of gradients and computing more accurate gradients are shown to be effective and efficient in achieving fast convergence. Such methods use average gradients or look-ahead corrections of gradients <ref type="bibr" target="#b86">[87]</ref>  <ref type="bibr" target="#b87">[88]</ref>. In addition to the accurate first-order gradients, higher-order gradients may be needed due to ill-conditioning <ref type="bibr" target="#b88">[89]</ref>  <ref type="bibr" target="#b89">[90]</ref>. Hessian matrices are estimated by the high-order gradients to make convergence possible <ref type="bibr" target="#b88">[89]</ref>.</p><p>Coordinate gradient descent are targeted at the problems where the instances are high dimensional, such as recommender systems <ref type="bibr" target="#b90">[91]</ref> and natural language processing <ref type="bibr" target="#b91">[92]</ref> Table 2: Optimization approximation based techniques.</p><p>optimizations performed by coordinate gradient descent, a small number of parameters can be selected at each iteration. Random selection of parameters has shown to be effective <ref type="bibr">[93] [94]</ref>.</p><p>Parameter selection can also be based on the first and/or second-order gradients information <ref type="bibr">[95] [96]</ref>. Another approach for speedup is to use extrapolation steps during the optimization phase <ref type="bibr" target="#b94">[94]</ref>. If the optimization problem is non-convex, then studies such as <ref type="bibr" target="#b97">[97]</ref> [98] present specific solutions. For instance, Li and Lin <ref type="bibr" target="#b97">[97]</ref> propose an extended variant of accelerated proximal gradient method.</p><p>Finally, Bayesian optimization methods are commonly based on Markov chain Monte Carlo <ref type="bibr" target="#b99">[99]</ref>  <ref type="bibr" target="#b100">[100]</ref>. Such methods employ stochastic minibatches due to the high cost of the acceptance tests <ref type="bibr" target="#b101">[101]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Communication Optimization Approaches</head><p>Optimizations to reduce communication costs constitute another option to those for computation.</p><p>In these optimizations, compression of gradients is one of the two main ideas. Some studies compress each gradient component to just 1 bit <ref type="bibr" target="#b102">[102]</ref>. Others map gradients to a discrete set of values <ref type="bibr" target="#b103">[103]</ref> or sketch gradients into buckets and then encode them <ref type="bibr" target="#b104">[104]</ref>. Some proposals only communicate gradients that are bigger than a certain threshold <ref type="bibr" target="#b105">[105]</ref>. A combination of gradient compression and low-precision learning has been shown to further reduce the communication costs <ref type="bibr" target="#b106">[106]</ref>. The other main idea for the optimization of communication is gradient delaying <ref type="bibr" target="#b107">[107]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Communication Topology</head><p>In a distributed ML system, the computing resources (clusters) can be structured in different ways. The types of topologies that the resources use can be categorized into three: centralized, hierarchical, and fully distributed (decentralized). Figure <ref type="figure" target="#fig_4">8</ref> depicts these topologies. Table <ref type="table" target="#tab_6">4</ref> summarizes our discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Centralized Topology</head><p>In this topology, the computation of the global model parameters, gradient averaging and communications with the distributed nodes/clients are performed at a central server. Every distributed client directly communicates with the central server and works with its local data only. A major disadvantage of a centralized topology is that the central server constitutes a single point of failure and a computational bottleneck. Advantages of a centralized topology are the ease of its implementation and inspection. Figure <ref type="figure" target="#fig_4">8</ref> (a) presents an example of this topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Hierarchical Topology</head><p>The computations and aggregation of the global model parameters are performed in a stage-wise and hierarchical way. Each child node only communicates with its parent. These topologies offer higher scalability than the centralized counterparts and easier manageability than the distributed counterparts. Figure <ref type="figure" target="#fig_4">8</ref> (b) depicts a hierarchical topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Fully Distributed Topology</head><p>Every participant maintains a local copy of the global model in a fully distributed topology. Participants directly communicate with each other. Compared to the centralized and hierarchical topologies, scalability is much higher and the single points of failure are eliminated. However, the implementation of these topologies is relatively more complex. Figure <ref type="figure" target="#fig_4">8</ref> (c) shows this topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Synchronization Models</head><p>Synchronization models are techniques to guide and perform synchronization between parallel computations and communications. These models seek to establish the best trade-off between fast updates and accurate models. To do fast updates, lower levels of synchronization are required. In comparison, to obtain accurate models, higher levels of synchronization are needed.</p><p>As far as ML is concerned, stochastic gradient descent is one of the most popular algorithms for the optimization during the training phase. As discussed below, variants of stochastic gradient descent have been implemented in accordance with the underlying synchronization model. Therefore, those variants constitute practical examples for the corresponding synchronization model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Bulk Synchronous Parallel</head><p>It is a synchronization model <ref type="bibr" target="#b111">[111]</ref> where synchronization happens between each computation and communication phase. Since this model is serializable by construction, the final output is guaranteed to be correct. However, when there are discrepancy between the progress of parallel workers, the faster workers have to wait for the slower ones. This can result in significant synchronization overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Stale Synchronous Parallel</head><p>This synchronization model <ref type="bibr" target="#b107">[107]</ref> allows the faster workers continue with their version of data for an additional but limited number of iterations to reduce the synchronization overheads due to the wait on the slower workers. While this can help reduce the overheads, data consistency and model convergence may become difficult to establish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Approximate Synchronous Parallel</head><p>In this model, synchronization is sometimes omitted or delayed to reduce the overheads. However, the accuracy and consistency of a model may deteriorate if care is not taken. An advantage of approximate synchronicity is that when a parameter update is insignificant, the server can delay synchronization as much as possible. A disadvantage is that selecting which updates are significant or not is typically difficult to do. As an example of the application of this model, Gaia <ref type="bibr" target="#b112">[112]</ref> is an approximate synchronous parallel ML system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Asynchronous Parallel</head><p>This synchronization model omits all synchronizations among the workers. While these omissions may significantly reduce the computation time and communication overhead, asynchronous communications may cause ML models to produce incorrect outputs. To give an example application, HOGWILD algorithms <ref type="bibr" target="#b113">[113]</ref> are developed based on asynchronous communications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Existing Distributed Learning Frameworks</head><p>There are many ML frameworks that provide distributed ML algorithms and utilities. The most popular distributed implementations are Tensorflow <ref type="bibr" target="#b114">[114,</ref><ref type="bibr" target="#b115">115,</ref><ref type="bibr" target="#b116">116]</ref>, PyTorch <ref type="bibr" target="#b117">[117,</ref><ref type="bibr" target="#b118">118]</ref>, MXNet <ref type="bibr" target="#b119">[119,</ref><ref type="bibr" target="#b120">120]</ref>, Horovod <ref type="bibr" target="#b121">[121]</ref>, Baidu <ref type="bibr" target="#b122">[122]</ref>, Dianne <ref type="bibr" target="#b123">[123]</ref>, CNTK <ref type="bibr" target="#b124">[124]</ref> and Theano <ref type="bibr" target="#b125">[125]</ref>. Table <ref type="table" target="#tab_7">5</ref> summarizes these frameworks. Other than the ML frameworks above, some general-purpose distributed computing libraries, such as Apache Spark <ref type="bibr" target="#b127">[126]</ref> and Hadoop [127], also support distributed ML.</p><p>Tensorflow <ref type="bibr" target="#b114">[114]</ref> is a free and open-source software library developed for ML and DL by Google. In fact, Tensorflow is the most popular library among the DL libraries. It supports distributed learning with several distribution strategies, such as mirrored, multi-worker and parameter server, that are either data or model parallel <ref type="bibr" target="#b115">[115,</ref><ref type="bibr" target="#b116">116]</ref>. The library provides efficient and scalable ML implementations for CPUs, multi-GPUs and mobile devices.</p><p>PyTorch <ref type="bibr" target="#b117">[117]</ref> is another free and open-source framework based on the Torch Library developed by Meta. It is a popular framework for scientific research and provides automatic differentiation and dynamic computation graphs. It supports distributed learning mainly in two ways with torch.distributed package <ref type="bibr" target="#b118">[118]</ref>. First, same as the Tensorflow mirrored strategy, PyTorch offers distributed data-parallel training which is based on the single-program and multiple-data paradigm. Second, for the cases that do not fit into data parallelism, PyTorch provides Remote Procedure Call (RPC) based distributed training. Examples of these types of distributed training are parameter server, pipeline parallelism, and reinforcement learning with multiple agents and observers.</p><p>MXNet <ref type="bibr" target="#b119">[119]</ref> is an open-source DL framework for research prototyping and production. It offers data-parallel distributed learning with parameter servers. MXNet allows mixing both symbolic and imperative programming for computational efficiency and scalability. MXNet supports many programming languages such as C++, Python, R and Julia.</p><p>Horovod <ref type="bibr" target="#b121">[121]</ref> is a distributed wrapper DL framework for TensorFlow, Keras, PyTorch, and Apache MXNet. Horovod is often easy to use because it only requires an addition of a small number of library calls to the source code. Horovod supports data, model and pipeline parallelisms.</p><p>Baidu <ref type="bibr" target="#b122">[122]</ref> was started as an easy-to-use, efficient distributed DL platform. It supports largescale ML and can train hundreds of machines in parallel with GPUs. Baidu offers various commercial solutions, such as machine translation, recommender systems, image classification and segmentation.</p><p>Dianne <ref type="bibr" target="#b123">[123]</ref> is a distributed and ANNsfocused software framework based on OSGi which is a dynamic module system for Java. Dianne supports both model and data parallelisms and offers UI-based functionality. Theano <ref type="bibr" target="#b125">[125]</ref> was a popular open-source Python library to define, optimize and evaluate mathematical expressions. It has support for efficient multi-dimensional arrays. Developed by Universite de Montreal, it is no longer used widely. Theano supports data-parallel distributed learning by both synchronous and asynchronous training. It also supports multi-GPU multi-machine distributed training.</p><p>General-purpose distributed frameworks that are based on MapReduce programming model <ref type="bibr" target="#b128">[128]</ref>, such as Apache Spark <ref type="bibr" target="#b127">[126]</ref> and Apache Hadoop [127], supports distributed ML algorithms, applications and utilities. Apache Spark is one of the most popular implementations of MapReduce. It includes MLlib <ref type="bibr" target="#b129">[129]</ref> which is an open-source scalable distributed ML library. MLlib consists of widely-used ML algorithms and utilities for classification, regression, clustering, and dimensionality reduction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Federated Learning (FL)</head><p>In this section, we first introduce FL. We then present the existing aggregation algorithms in detail. After that, we discuss the security and privacy aspects of FL. We conclude this section by the available FL platforms and datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Introduction to FL</head><formula xml:id="formula_2">W 1 W 2 W n W 1 , W 2 , …, W n -&gt; W</formula><p>Fig. <ref type="figure" target="#fig_5">9</ref>: Federated Learning Overview.</p><p>FL can be categorized based on how data partitioning is done. Horizontal FL <ref type="bibr" target="#b130">[130]</ref> refers to the case where the clients share the same feature space but have different sample spaces. This is similar to data parallelism. An example of horizontal FL is wake-up voice recognition on smartphones. Users with different types of voices (different sample spaces) speak the same wake-up command (same feature space).</p><p>Vertical FL <ref type="bibr" target="#b130">[130]</ref> takes place where the clients share the same sample space but have different feature spaces. As an example, the common customers (same sample space) of a bank and an e-commerce company (different feature spaces) join the training of an FL model for optimizing personal loans.</p><p>Finally, Federated Transfer Learning Distributed machine learning and FL have some fundamental differences. These are:</p><p>• While distributed machine learning's main goal is to minimize the computational costs and achieve high scalability, FL's main goal is to provide privacy and security for the user/client data. As a result, FL is designed such that user/client data is never shared. • Distributed learning assumes that the user data is independent and identically distributed (i. for epochs e from 1 to E do the following is done in parallel 20:</p><formula xml:id="formula_3">w k t+1 ← U pdateClientW eight(k, w k t ) 21:</formula><p>end for 22:</p><formula xml:id="formula_4">w t+1 ← K k=1 w k t+1 K 23:</formula><p>end function 24: end if</p><p>We now discuss the very first FL algorithm, FedAvg, proposed by McMahan et. al. <ref type="bibr" target="#b8">[9]</ref>. Algorithm 1 describes FedAvg. It shows the action taken by the server and clients during a round of FL. The clients train the model with their data. Once trained, the weights are sent to the server as described by the U pdateClientW eight function on line 7. Once the server receives the weights from the clients, which is done in parallel, it averages out all the weights and sends the average weights back to each clients, as seen in the ServerU pdateW eight function on line 16. Training is repeated if the data changes. This is to keep the weights updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">FL Applications</head><p>FL has a wide range of applications across different domains and settings. Some of them are:</p><p>• Smartphones: FL has been used to develop ML applications for smartphones such as next-word prediction, and face and voice recognition. • Healthcare: FL has been applied successfully for research problems in medical studies such as drug discovery and brain tumor segmentation. • The internet of things (IoT): IoT is a network of digital or mechanical computing objects that have sensors, software, and other computing technologies. IoT exchanges data with other devices and systems over the internet to perform specific learning tasks. Applications of FL in IoT include autonomous driving and intrusion and anomaly detection. • Finance: FL has been adopted to detect/identify financial crimes such as fraudulent loans and money laundering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">FL Aggregation Algorithms</head><p>In FL, due to data parallelism and horizontal FL, aggregation algorithms are needed to aggregate the models or gradients between the participants. As stated above, the very first aggregation algorithm, called Federated Averaging (FedAvg), was introduced by McMahan et. al. <ref type="bibr" target="#b8">[9]</ref> who essentially kick-started FL itself. FedAvg computes the global model parameters by averaging the parameter updates of the participants. Once the global parameters are computed and updated, these parameters are communicated back to the participants. FedAvg is a straightforward algorithm however, it is biased toward the participants who have favorable network conditions. Aggregation algorithms have been studied extensively for centralized topologies <ref type="bibr" target="#b132">[132,</ref><ref type="bibr" target="#b133">133]</ref>. To decrease the communication overheads, Liu et. al. propose the Federated Stochastic Block Coordinate Descent (FedBCD) algorithm <ref type="bibr" target="#b132">[132]</ref> in which each participant makes multiple local updates before synchronizing with other participants. Differently, FedOpt <ref type="bibr" target="#b133">[133]</ref> uses gradient compression to reduce communication overhead while sacrificing accuracy. Furthermore, for edge devices where computational resources are limited, algorithms such as FedGKT <ref type="bibr" target="#b134">[134]</ref> are developed.</p><p>A significant objective in FL is to provide fairness. Fairness means that the clients equally contribute to the global model with respect to certain metrics. Researchers have proposed algorithms such as Stochastic Agnostic Federated Learning (SAFL) <ref type="bibr">[135]</ref> and FedMGDA+ <ref type="bibr" target="#b135">[136]</ref> to achieve fairness.</p><p>Adaptive FL and its impact on convergence and accuracy have been explored in various recent works. ADAGRAD <ref type="bibr" target="#b136">[137]</ref> offers an adaptive approach to ML optimization compared to FedAvg. ADAGRAD and its variants dynamically choose server and client learning rates and momentum parameters during training. Mime Lite <ref type="bibr" target="#b137">[138]</ref> is a closely related study where adaptive learning rates and momenta are reported to improve accuracy. Some recent aggregation algorithms support heterogeneity of participant data. FedProx <ref type="bibr" target="#b139">[139]</ref> is such an algorithm used for FL over heterogeneous data and resources. SCAFFOLD <ref type="bibr" target="#b140">[140]</ref> is another algorithm that accounts for heterogeneous data while reducing the number of rounds to converge. FedAtt <ref type="bibr" target="#b141">[141]</ref> accounts for the client contributions by attending to the importance of their model updates. The attention is quantified by the similarity between the server model and the client model in a layer-wise manner. FedNova <ref type="bibr" target="#b142">[142]</ref> proposes a normalized averaging method as a way to avoid objective inconsistencies and to achieve fast convergence for highly heterogeneous clients.</p><p>Personalization is another important consideration in FL. There has been extensive research on personalized FL <ref type="bibr">[143] [144]</ref>. Tan et. al. <ref type="bibr" target="#b145">[145]</ref> offer a survey of the latest personalization techniques.</p><p>When the topology of the clients in FL is hierarchical, an aggregation algorithm needs to take the hierarchy into account. Numerous hierarchical aggregation algorithms have been proposed for such settings <ref type="bibr" target="#b146">[146,</ref><ref type="bibr" target="#b147">147]</ref>. Among hierarchical solutions, SPAHM <ref type="bibr" target="#b146">[146]</ref> and PFNM <ref type="bibr" target="#b147">[147]</ref> are Bayesian FL methods. Similarly, for decentralized topologies, decentralized algorithms have been developed <ref type="bibr" target="#b148">[148,</ref><ref type="bibr" target="#b149">149]</ref>.</p><p>Considering fault-tolerance in FL, Krum <ref type="bibr" target="#b150">[150]</ref> is an aggregation scheme that is reportedly resilient to Byzantine failures <ref type="bibr" target="#b151">[151]</ref> where computing processes fail arbitrarily and failure symptoms are different for different observers. For these types of failures, more fault-tolerant FL studies are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Security and Privacy in FL</head><p>The security of FL entails ensuring the triad of confidentiality, integrity and availability of its data and models, and particularly, data privacy. Privacy is defined as the protection of the raw data against the information leakage. In this section, we first summarize the attack types and then the defensive actions and methods existing in the FL literature <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Attacks</head><p>There are numerous attack types in FL. Poisoning attacks aim to tamper with and/or alter the data or the model. Data poisoning <ref type="bibr" target="#b156">[156,</ref><ref type="bibr" target="#b157">157,</ref><ref type="bibr" target="#b158">158]</ref> refers to altering the features in the training data or generating false data to degrade the performance of a model on the unseen data. Model poisoning <ref type="bibr" target="#b159">[159,</ref><ref type="bibr" target="#b160">160,</ref><ref type="bibr" target="#b161">161]</ref> refers to the modification of the model parameters and/or the fabrication of false weights that are communicated between the participants and the servers.</p><p>Backdoor attacks <ref type="bibr" target="#b177">[177,</ref><ref type="bibr" target="#b178">178]</ref> inject malicious instructions into the models while not impacting their expected performance. These attacks are non-transparent and notoriously difficult to detect.</p><p>Inference attacks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b162">162,</ref><ref type="bibr" target="#b163">163]</ref> involve gaining knowledge of the sensitive information of the participants, the training data or the model through the communications occurring during training or inference. Membership inference attacks aim to learn if a sample has been used as a training instance. Property inference attacks aim to learn the meta-characteristics of the training data. Class representative inference attacks aim to learn representative samples of a target class.</p><p>Generative Adversarial Networks (GANs) based attacks <ref type="bibr" target="#b166">[166,</ref><ref type="bibr" target="#b167">167,</ref><ref type="bibr" target="#b168">168]</ref> are used to launch poisoning attacks where GANs generate the altered or false data and/or model parameters.</p><p>There are many other attack types <ref type="bibr" target="#b179">[179,</ref><ref type="bibr" target="#b180">180]</ref>, such free-riders <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b163">163]</ref> and Eavesdropping <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b163">163]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Defenses</head><p>The most commonly used attack defense mechanisms can be categorized by the usage of trusted execution environments <ref type="bibr" target="#b169">[169,</ref><ref type="bibr" target="#b170">170]</ref>, homomorphic encryption <ref type="bibr" target="#b164">[164,</ref><ref type="bibr" target="#b165">165]</ref>, differential privacy <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b154">154,</ref><ref type="bibr" target="#b155">155]</ref>, and possibly some combinations of them. There are many other techniques which are based on GANs <ref type="bibr" target="#b181">[181]</ref>, anomaly detection <ref type="bibr" target="#b176">[176]</ref>, secure multi-party computation <ref type="bibr" target="#b171">[171]</ref>, data anonymization <ref type="bibr" target="#b175">[175]</ref>, and blockchains <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b172">172]</ref>.</p><p>A trusted execution environment <ref type="bibr" target="#b169">[169,</ref><ref type="bibr" target="#b170">170]</ref> is an (hardware/software) architecture where the program execution is secured and information leakage is not possible. Such architectures use specialized designs to prevent unauthorized accesses as well as privacy violations in FL <ref type="bibr" target="#b169">[169,</ref><ref type="bibr" target="#b170">170]</ref>.</p><p>Homomorphic encryption <ref type="bibr" target="#b182">[182]</ref> is a certain type of encryption in which the decryption of the results of the computations performed on the encrypted data is the same as the result of the same computations performed on the unencrypted data. Homomorphic encryption has various levels depending on the whether addition and/or multiplication is supported. It has been adapted for data privacy in FL <ref type="bibr" target="#b164">[164,</ref><ref type="bibr" target="#b165">165]</ref>.</p><p>Differential privacy <ref type="bibr" target="#b152">[152,</ref><ref type="bibr" target="#b153">153]</ref> is a technique for achieving data privacy by adding noise to raw data. It is commonly used in FL <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b154">154,</ref><ref type="bibr" target="#b155">155]</ref>.</p><p>Table <ref type="table">6</ref> reviews attacks and defenses in FL. In addition, Table <ref type="table">7</ref> compares the defense mechanisms in terms of the strength of the protection, the computational and communication efficiency, robustness, scalability, and generalizability of a mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Existing FL Frameworks</head><p>The most widely used FL frameworks are Ten-sorFlow Federated <ref type="bibr" target="#b183">[183,</ref><ref type="bibr" target="#b184">184]</ref>, IBM Federated Learning <ref type="bibr" target="#b185">[185]</ref>, NVIDIA FLARE <ref type="bibr" target="#b186">[186]</ref> NVIDIA FLARE (Federated Learning Application Runtime Environment) <ref type="bibr" target="#b186">[186]</ref> is a modular open-source software development kit (SDK) for FL which offers secure and privacy-preserving distributed learning. FLARE provides FL algorithms such as FedAvg, FedProx and SCAFFOLD. It offers differential privacy and homomorphic encryption. FLARE SDK has several components, such as a simulator for prototyping, secure management tools for provisioning and deployment and an API for extensions.</p><p>FedML <ref type="bibr" target="#b187">[187]</ref> framework offers a wide-range of cross-platform FL capabilities including natural language processing, computer vision, and GNNs. FedAvg, FedOpt, FedNova and FedGKT are the supported FL algorithms. FedML offers defense mechanisms such as differential privacy, cryptography routines, and several coding methods. It supports data and model parallel distributed learning. FedML models can be trained and deployed at the edge or on the cloud. FATE <ref type="bibr" target="#b188">[188]</ref> is an open-source platform initiated by WeBank, a bank based in Shenzhen, China. It provides a diverse set of FL algorithms, such as tree-based algorithms, DL, and transfer learning. It offers a set of modules consisting of an ML algorithms library, a high-performance serving system, an end-to-end pipeline system, a multi-party communication network system, and a module for cloud technologies. FATE provides homomorphic encryption and RSA for secure and privacy preserving training. FATE supports data and pipeline parallelisms.</p><p>PySyft <ref type="bibr" target="#b189">[189]</ref> is an open-source multi-language library that provides secure and private DL and FL in Python for frameworks such as PyTorch, Tensorflow and Keras. It supports differential privacy and homomorphic encryption. FedAvg, FedProx and FedSGD are among the available aggregation algorithms. Training can be data or model parallel.</p><p>OpenFL <ref type="bibr" target="#b190">[190]</ref> is an open-source Python framework originally developed by Intel Labs. It provides a set of workflows for the researchers to experiment with FL. FedAvg and ADAGRAD algorithms are built-in. OpenFL's capabilities include trusted execution environments, RSA, differential privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">FL Datasets</head><p>As FL research progresses, new datasets are being built. One of the most well-known datasets for FL is the LEAF <ref type="bibr" target="#b191">[191]</ref>. It is a suite of open-source federated datasets. There are a total of six different datasets. One of the datasets, called FEMNIST, is built for image classification. Sentiment140, which consists of Tweets, is a dataset for sentiment analysis. Shakespeare is a text dataset of Shakespeare Dialogues which is used for next character prediction. Celeba is an image classification dataset of celebrity images. There is a synthetic classification dataset which is generated for the FL models that are device-dependant. Lastly, the Reddit comments dataset is used for next word prediction.</p><p>TensorFlow Federated <ref type="bibr" target="#b192">[192]</ref> offers several datasets to support FL simulations. While some of its datasets are the same as those of LEAF, there are also different datasets, such as the federated CIFAR-100 dataset, the FLAIR dataset, and the federated Google Landmark v2 dataset.</p><p>Street Dataset <ref type="bibr" target="#b193">[193]</ref> is a real-world image dataset. It contains images generated from street cameras. A total of seven object categories annotated with bounding boxes. This dataset is built for object detection tasks.</p><p>CC-19 <ref type="bibr" target="#b194">[194]</ref> is a new dataset related to the latest family of coronavirus (COVID-19). It contains the Computed Tomography (CT) scan of subjects and is built for image classification.</p><p>FedTADBench <ref type="bibr" target="#b195">[195]</ref> offers three different datasets to evaluate time series anomaly detection algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Open Questions and Challenges</head><p>In this section, we summarize the challenges that ML and FL face. We only present major problems. This is because there is a large number of open problems, and we choose to keep our presentation concise and focused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Challenges for Parallel and Distributed ML</head><p>The major challenges with parallel and distributed ML are related to performance, fault-tolerance, security and privacy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b196">196,</ref><ref type="bibr" target="#b197">197]</ref>. Typically, in distributed and parallel training, additional resources are used to decrease wallclock time <ref type="bibr" target="#b198">[198]</ref>. Such additional resources can be multiple machines, multiple GPUs and highend communication networks. As a result, the decrease in wall-clock time may not compensate for the additional resources or their energy consumption. Therefore, research studies, such as <ref type="bibr" target="#b15">[16]</ref>, are needed to investigate this trade-off with different applications and system architectures.</p><p>Distributed and parallel ML platforms, especially those executed on high-performance computing systems, often consider fault-tolerance as a second-class concern. However, given the sizes of the latest large-scale computing systems, failures are common; not rare <ref type="bibr" target="#b199">[199]</ref>. As a result, efficient checkpointing and/or replication solutions <ref type="bibr" target="#b199">[199]</ref> are needed to recover from errors and to limit the amount of lost computation due to a failure.</p><p>Ensuring security and privacy for distributed and parallel ML has consistently been a serious concern <ref type="bibr" target="#b196">[196]</ref>. While FL was devised for the privacy of user data, there have been many novel types of attacks <ref type="bibr" target="#b179">[179,</ref><ref type="bibr" target="#b180">180]</ref>. These attacks include adversarial <ref type="bibr" target="#b168">[168]</ref>, poisoning, evasion, backdoor, and integrity attacks <ref type="bibr" target="#b180">[180,</ref><ref type="bibr" target="#b197">197]</ref>. As such attacks get sophisticated, so must their defenses. Moreover, the systematic deployment of the defenses to the physical systems as well as the evaluation of these deployments have not studied well <ref type="bibr" target="#b197">[197]</ref>. Furthermore, there is a lack of the rigorous efficiency and efficacy studies of attack defense mechanisms. As a result of these issues, security and privacy for ML remain an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Challenges for FL</head><p>The main challenges in FL are two-fold <ref type="bibr" target="#b200">[200]</ref>: explainability and interpretability, and federated GNNs. Explainability and interpretability refer to the understanding of the contributions of the clients or the data features. For instance, Shapley values are proposed <ref type="bibr" target="#b201">[201]</ref> to quantify the impact of the features on the model output. Zheng et. al. propose a quantified ranking of features <ref type="bibr" target="#b202">[202]</ref>. Similarly, there are studies <ref type="bibr" target="#b203">[203]</ref> targeting vertical FL. Several works introduce tailored measures of interpretability such as <ref type="bibr" target="#b204">[204]</ref> defining a measure based on the gradients. However, in general, the problem of explainability and interpretability remains open because i) ensuring privacy while building explainable models is not trivial, ii) the aggregation of the local parameters obscures interpretability, iii) there is a lack of datasets that are not composed of images or text, and iv) there is a lack of a general framework for explainable federated models.</p><p>Research for FL with GNNs <ref type="bibr" target="#b205">[205,</ref><ref type="bibr" target="#b206">206,</ref><ref type="bibr" target="#b207">207]</ref> has recently started. For instance, FedGraphNN <ref type="bibr" target="#b205">[205]</ref> provides an FL benchmark system to evaluate various graph models, algorithms and datasets.</p><p>Another example is GraphFL <ref type="bibr" target="#b207">[207]</ref> which is designed to classify nodes on graphs. However, many questions are still waiting to be solved, such as the protection against malicious attacks, interpretability, lack of modern graph neural frameworks for FL <ref type="bibr" target="#b208">[208]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work, we provided a review of modern large-scale, parallel and distributed ML: the state-of-the-art algorithms, optimization methods, types of parallelisms, communication topologies, synchronization models, and the existing frameworks. Moreover, we reviewed FL. We discussed various aggregation algorithms in FL. In addition, we reviewed the security and privacy aspects including various types of attacks and defense mechanisms. Moreover, we explored the existing FL frameworks and datasets. We concluded our study with the open research problems and challenges in large-scale distributed ML and FL. The major challenges are typically related to performance, security, privacy, explainability, portability, and fault-tolerance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :Fig. 2 :</head><label>12</label><figDesc>Fig. 1: The outline of our review.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Data parallelism for a deep neural network.</figDesc><graphic coords="8,142.87,73.70,317.49,221.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Model parallelism for a deep neural network.</figDesc><graphic coords="8,142.87,335.30,317.48,112.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Pipeline parallelism for a deep neural network.</figDesc><graphic coords="9,134.93,73.70,317.49,171.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Different topologies of distributed ML.</figDesc><graphic coords="9,89.57,285.50,136.06,123.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FL [ 9 ]</head><label>9</label><figDesc>is a variant of ML where training a model is done by distributed clients that individually train local models. Once local models are trained, all local model parameters are sent to a central server which then calculates the average of the parameters (weights) to compute an average model. This average model is then communicated back to the clients for subsequent local training. FL performs distributed training without sharing private client data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 -</head><label>3</label><figDesc>Global model aggregation and update 2-Local model training and W i upload</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>[131] refers to the case where both the sample and the feature spaces are different. Federated transfer learning transfers features from different feature spaces to the same representation to train a model with the data of different clients. An example is disease diagnosis by many different collaborating countries with multiple hospitals which have different patients (different sample spaces) with different medication tests (different feature spaces).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 1 7 :</head><label>17</label><figDesc>i.d). On the other hand, FL assumes noni.i.d because users typically have different data distributions and types. • Distributed learning is performed based on aggregating client data, which is then distributed to different clients for training and inference. Contrarily, FL utilizes decentralized data. The client data is never shared and is never aggregated on a central server. Federated learning: client and server functions 1: i ← isClient or isServer 2: E ← totalEpochs 3: B ← totalN umBatches 4: η ← learningRate 5: w ← initialW eights 6: if i = isClient then function UpdateClientWeight(w,k) 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>9 : 18 :</head><label>918</label><figDesc>for batchs b from 1 to B do 10: w ← w -η∇l(w, b) for k in sub batch of K clients do 19:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>. To speed up the Model simplifications for different ML models.</figDesc><table><row><cell>Model Type</cell><cell>Techniques</cell><cell>Existing Work</cell></row><row><cell>Kernel-based Models</cell><cell>Sampling-based Projection-based</cell><cell>[64, 65, 66, 67]</cell></row><row><cell>Tree-based Models</cell><cell>Rule sampling Feature sampling</cell><cell>[68, 69, 52]</cell></row><row><cell>Graph-based Models</cell><cell>Sparse graph construction Anchor graph based optimization</cell><cell>[70, 71, 72, 73, 74]</cell></row><row><cell>Deep Neural Network Models</cell><cell>Efficient activation functions Filter factorization and grouping</cell><cell>[75, 76, 77, 78, 79]</cell></row><row><cell>Categories</cell><cell>Techniques</cell><cell>Existing Work</cell></row><row><cell></cell><cell></cell><cell>[80], [81], [82]</cell></row><row><cell></cell><cell>Adaptive sampling</cell><cell>[83] [84]</cell></row><row><cell>Mini-batch gradient descent</cell><cell>Adaptive learning rates</cell><cell>[85] [86]</cell></row><row><cell></cell><cell>Gradient corrections</cell><cell>[87], [88]</cell></row><row><cell></cell><cell></cell><cell>[89], [90]</cell></row><row><cell>Coordinate gradient descent</cell><cell>Rule sampling Feature sampling</cell><cell>[91] [92] [93] [94] [95] [96] [94] [97] [98]</cell></row><row><cell>Bayesian optimization</cell><cell>Sparse graph construction Anchor graph based optimization</cell><cell>[99], [100], [101]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>. Ho et. al. explore the usage of gradient delays for stale synchronous parallel communications. Zheng et. al.<ref type="bibr" target="#b108">[108]</ref> on the other hand compute approximate second-order gradients and overlap these computations with the delays to enhance the communication efficiency.Communication optimization approaches.</figDesc><table><row><cell>Zhang, Choromanska, and LeCun [109] define an</cell></row><row><cell>elastic relationship between the local and global</cell></row><row><cell>model to avoid local minima as gradient transfers</cell></row><row><cell>are delayed. Different than these studies, McMa-</cell></row><row><cell>han and Streeter [110] introduce communication</cell></row><row><cell>optimizations for online learning.</cell></row><row><cell>Table 3 summarizes these techniques.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different communication topologies.</figDesc><table><row><cell>Topology</cell><cell cols="5">Complexity Scalability Manageability Single Point Failures Latency</cell></row><row><cell>Centralized</cell><cell>Low</cell><cell>Low</cell><cell>High</cell><cell>Yes</cell><cell>Low</cell></row><row><cell>Hierarchical</cell><cell>Medium</cell><cell>Medium</cell><cell>Medium</cell><cell>Yes</cell><cell>Medium</cell></row><row><cell>Fully Distributed</cell><cell>High</cell><cell>High</cell><cell>Low</cell><cell>No</cell><cell>High</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Existing distributed learning platforms.</figDesc><table><row><cell cols="2">Frameworks Pros</cell><cell>Cons</cell><cell>Parallelism</cell></row><row><cell></cell><cell>Most popular.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Strong support by Google.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Efficient and scalable CPU,</cell><cell></cell><cell></cell></row><row><cell>Tensorflow</cell><cell>multi-GPU,</cell><cell>Difficult to use API</cell><cell>Data, Model</cell></row><row><cell></cell><cell>mobile implementations.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Various training strategies:</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Multi-worker, Parameter server...</cell><cell></cell><cell></cell></row><row><cell>PyTorch</cell><cell>Dynamic computation graph Automatic differentiation Support of remote procedure calls</cell><cell>No support for mobile</cell><cell>Data, Model, Pipeline</cell></row><row><cell></cell><cell>High scalability</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Support of many languages:</cell><cell></cell><cell></cell></row><row><cell>MXNet</cell><cell>C++, Python, Julia, R</cell><cell>Difficult to use API</cell><cell>Data</cell></row><row><cell></cell><cell>Usage of symbolic</cell><cell></cell><cell></cell></row><row><cell></cell><cell>and imperative programming</cell><cell></cell><cell></cell></row><row><cell>Horovod</cell><cell>Easy to use Supports Tensorflow, Keras, PyTorch, and MXNet</cell><cell>Lacks fault tolerance</cell><cell>Data Model Pipeline</cell></row><row><cell>Baidu</cell><cell>Commercial ML and DL solutions</cell><cell>Limited scalability No support for fault-tolerance</cell><cell>Data, Pipeline</cell></row><row><cell>Dianne</cell><cell>Java based development platform</cell><cell>No other languages</cell><cell>Data, Model</cell></row><row><cell>CNTK</cell><cell>Open-source Efficient and high-performing</cell><cell>No longer actively developed Limited mobile support</cell><cell>Data, Model</cell></row><row><cell>Theano</cell><cell>Open-source and cross-platform Powerful numerical library</cell><cell>Discontinued</cell><cell>Data</cell></row><row><cell cols="2">The Microsoft Cognitive Toolkit (CNTK) [124]</cell><cell></cell><cell></cell></row><row><cell cols="2">is open-source software for commercial-grade DL.</cell><cell></cell><cell></cell></row><row><cell cols="2">However, it is no longer actively developed. It</cell><cell></cell><cell></cell></row><row><cell cols="2">supports distributed learning through parallel</cell><cell></cell><cell></cell></row><row><cell cols="2">Stochastic Gradient Descent (SGD) algorithms.</cell><cell></cell><cell></cell></row><row><cell cols="2">CNTK implements the following four parallel</cell><cell></cell><cell></cell></row><row><cell cols="2">SGD algorithms: Data-parallel, block momentum,</cell><cell></cell><cell></cell></row><row><cell cols="2">model averaging, and asynchronous data-parallel</cell><cell></cell><cell></cell></row><row><cell>SGD.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Existing FL platforms.</figDesc><table><row><cell>, FedML</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Machine Learning (ML)In this section, we first overview ML in terms of concepts and goals. Then we review various ML algorithms. Finally, we discuss the existing modern ML frameworks.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was supported by the <rs type="funder">U.S. DOE Office of Science, Office of Advanced Scientific Computing Research</rs>, under award 66150: "<rs type="projectName">CENATE -Center for Advanced Architecture Evaluation</rs>" project. The <rs type="institution">Pacific Northwest National Laboratory</rs> is operated by <rs type="funder">Battelle for the U.S. Department of Energy</rs> under contract <rs type="grantNumber">DE-AC05-76RL01830</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_KBVjGXb">
					<orgName type="project" subtype="full">CENATE -Center for Advanced Architecture Evaluation</orgName>
				</org>
				<org type="funding" xml:id="_BPzSEYY">
					<idno type="grant-number">DE-AC05-76RL01830</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defense Type Addressed Attacks Potential Negative Effects</head><p>Differential privacy <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b152">152,</ref><ref type="bibr" target="#b153">153,</ref><ref type="bibr" target="#b154">154,</ref><ref type="bibr" target="#b155">155]</ref> Data Poisoning <ref type="bibr" target="#b156">[156,</ref><ref type="bibr" target="#b157">157,</ref><ref type="bibr" target="#b158">158]</ref> Model Poisoning <ref type="bibr" target="#b159">[159,</ref><ref type="bibr" target="#b160">160,</ref><ref type="bibr" target="#b161">161]</ref> Inference attacks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b162">162,</ref><ref type="bibr" target="#b163">163]</ref> Decreased model utility Homomorphic encryption <ref type="bibr" target="#b164">[164,</ref><ref type="bibr" target="#b165">165]</ref> Inference attacks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b162">162,</ref><ref type="bibr" target="#b163">163]</ref> GAN-based attacks <ref type="bibr" target="#b166">[166,</ref><ref type="bibr" target="#b167">167,</ref><ref type="bibr" target="#b168">168]</ref> High computational costs Trusted execution environments <ref type="bibr" target="#b169">[169,</ref><ref type="bibr" target="#b170">170]</ref> Inference attacks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b162">162,</ref><ref type="bibr" target="#b163">163]</ref> Model Poisoning <ref type="bibr" target="#b159">[159,</ref><ref type="bibr" target="#b160">160,</ref><ref type="bibr" target="#b161">161]</ref> Specialized hardware Secure Multi-party Computation <ref type="bibr" target="#b171">[171]</ref> GAN-based attacks <ref type="bibr" target="#b166">[166,</ref><ref type="bibr" target="#b167">167,</ref><ref type="bibr" target="#b168">168]</ref> Inference attacks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b162">162,</ref><ref type="bibr" target="#b163">163]</ref> Eavesdropping <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b163">163]</ref> High computational costs Blockchain <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b172">172]</ref> Blockchain attacks <ref type="bibr" target="#b173">[173,</ref><ref type="bibr" target="#b174">174]</ref> High resource costs Data anonymization <ref type="bibr" target="#b175">[175]</ref> GAN-based attacks <ref type="bibr" target="#b166">[166,</ref><ref type="bibr" target="#b167">167,</ref><ref type="bibr" target="#b168">168]</ref> Inference attacks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b162">162,</ref><ref type="bibr" target="#b163">163]</ref> Decreased data usability</p><p>Anomaly detection <ref type="bibr" target="#b176">[176]</ref> Data Poisoning <ref type="bibr" target="#b156">[156,</ref><ref type="bibr" target="#b157">157,</ref><ref type="bibr" target="#b158">158]</ref> Model Poisoning <ref type="bibr" target="#b159">[159,</ref><ref type="bibr" target="#b160">160,</ref><ref type="bibr" target="#b161">161]</ref> Free-riders <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b163">163]</ref> Detection latency  <ref type="bibr" target="#b175">[175]</ref> Medium Medium Low High High Anomaly detection <ref type="bibr" target="#b176">[176]</ref> Medium High Medium High Low Table <ref type="table">7</ref>: Comparison of the defense mechanisms in terms of the strength of the protection, the computational and communication efficiency, robustness, scalability, and generalizability of a mechanism.</p><p>[187], Federated AI Technology Enabler (FATE) <ref type="bibr" target="#b188">[188]</ref>, PySyft <ref type="bibr" target="#b189">[189]</ref>, and Open Federated Learning (OpenFL) <ref type="bibr" target="#b190">[190]</ref>. Table <ref type="table">8</ref> summarizes the existing FL frameworks.</p><p>TensorFlow Federated <ref type="bibr" target="#b183">[183]</ref> (and Keras Federated <ref type="bibr" target="#b184">[184]</ref>) is an open-source framework for FL by Google. It enables researchers to simulate FL algorithms. FedAvg, FedProx, FedSGD, and Mime Lite are some of the FL aggregation algorithms that are readily available. TensorFlow Federated supports data and model parallelisms. It provides differential privacy as a privacy measure. Tensor-Flow Federated has two main APIs. FL API offers built-in algorithms. FL Core API offers a set of lower-level functionalities for new algorithms to be implemented.</p><p>IBM Federated Learning <ref type="bibr" target="#b185">[185]</ref> provides support for FL and DL models written in Keras, PyTorch and TensorFlow. FedAvg, SPAHM, PFNM, and Krum are among the available aggregation algorithms. IBM FL supports data and model parallelism. In addition, differential privacy, secure multi-party computation and homomorphic encryption are available defenses for ensuring privacy and security. IBM FL also offers the implementations of several topologies and communication protocols.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on deep learning and its applications</title>
		<author>
			<persName><forename type="first">Shi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khushnood</forename><surname>Abbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">100379</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning: a comprehensive overview on techniques, taxonomy, applications and research directions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><surname>Sarker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SN Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">420</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Survey of machine learning accelerators</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Reuther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Michaleas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Gadepally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Samsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Kepner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE High Performance Extreme Computing Conference (HPEC)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Survey and benchmarking of machine learning accelerators</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Reuther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Michaleas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Gadepally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Samsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Kepner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE High Performance Extreme Computing Conference (HPEC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on machine learning accelerators and evolutionary hardware platforms</title>
		<author>
			<persName><forename type="first">Sathwika</forename><surname>Bavikadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijitt</forename><surname>Dhavlle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amlan</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Haridass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hagar</forename><surname>Hendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cory</forename><surname>Merkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Purab</forename><surname>Ranjan Sutradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Manoj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pudukotai</forename><surname>Dinakarrao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Design &amp; Test</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="91" to="116" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on largescale machine learning</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2574" to="2594" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey on distributed machine learning</title>
		<author>
			<persName><forename type="first">Joost</forename><surname>Verbraeken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Wolting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Katzy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeroen</forename><surname>Kloppenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Verbelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">S</forename><surname>Rellermeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020-03">mar 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions</title>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Ellen Lwakatare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiswarya</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivica</forename><surname>Crnkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helena</forename><surname>Holmström Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Software Technology</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page">106368</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eider</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Agüera Y Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaojin (jerry)</forename><surname>Zhu</surname></persName>
		</editor>
		<meeting>the 20th International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-04-22">2017, 20-22 April 2017. 2017</date>
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
	<note>ume 54 of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From distributed machine learning to federated learning: A survey</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="885" to="917" />
			<date type="published" when="2022-04">apr 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on federated learning</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page">106775</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on federated learning systems: Vision, hype and reality for data privacy and protection</title>
		<author>
			<persName><forename type="first">Qinbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaomin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sixu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3347" to="3366" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Federated learning for internet of things: A comprehensive survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pubudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aruna</forename><surname>Pathirana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1622" to="1658" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of deep learning and its applications: a new paradigm to machine learning</title>
		<author>
			<persName><forename type="first">Munish</forename><surname>Shaveta Dargan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maruthi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gulshan</forename><surname>Rohit Ayyagari</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Computational Methods in Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1071" to="1092" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Machine learning: Algorithms, real-world applications and research directions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><surname>Sarker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SN computer science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">160</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Demystifying parallel and distributed deep learning: An in-depth concurrency analysis</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019-08">aug 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed deep learning on data systems: A comparative analysis of approaches</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcquillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandish</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekta</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Kislal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domino</forename><surname>Valdano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2021-06">jun 2021</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1769" to="1782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed training of deep learning models: A taxonomic perspective</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenny</forename><surname>Rahayu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2802" to="2818" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Arno</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools</title>
		<imprint>
			<date type="published" when="2020-02">feb 2020</date>
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Federated machine learning: Survey, multi-level classification, desirable criteria and future directions in communication and networking systems</title>
		<author>
			<persName><forename type="first">Abdel</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azzam</forename><surname>Wahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Mourad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarik</forename><surname>Otrok</surname></persName>
		</author>
		<author>
			<persName><surname>Taleb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1342" to="1397" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Distributed graph neural network training: A survey</title>
		<author>
			<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comprehensive survey on training acceleration for large machine learning models in iot</title>
		<author>
			<persName><forename type="first">Haozhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haobo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="939" to="963" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed machine learning for wireless communication networks: Techniques, architectures, and applications</title>
		<author>
			<persName><forename type="first">Shuyan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekram</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1458" to="1493" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey: Distributed machine learning for 5g and beyond</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Nassef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakimeh</forename><surname>Purmehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mallik</forename><surname>Tatipamula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toktam</forename><surname>Mahmoodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Netw</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<date type="published" when="2022-04">apr 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distributed machine learning for uav swarms: Computing, sensing, and semantics</title>
		<author>
			<persName><forename type="first">Yahao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc-Viet</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shikh-Bahaei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey on security and privacy of federated learning</title>
		<author>
			<persName><forename type="first">Viraaji</forename><surname>Mothukuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><forename type="middle">M</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyedamin</forename><surname>Pouriyeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Dehghantanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="619" to="640" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Blockchain-enabled federated learning: A survey</title>
		<author>
			<persName><forename type="first">Youyang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Palash Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenquan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longxiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Yearwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Differential privacy for deep and federated learning: A survey</title>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">El</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ouadrhiri</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abdelhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="22359" to="22380" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online learning: A comprehensive survey</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doyen</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">459</biblScope>
			<biblScope unit="page" from="249" to="289" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<title level="m">Support vector machines. IEEE Intelligent Systems and their applications</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="18" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Umme Zahoora, and Aqsa Saeed Qureshi. A survey of the recent architectures of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Asifullah</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anabia</forename><surname>Sohail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5455" to="5516" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A review of recurrent neural networks: Lstm cells and network architectures</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosheng</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1235" to="1270" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dor</forename><surname>Bank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05991</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Self-organizing maps. Handbook of natural computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><surname>Van Hulle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="585" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Boltzmann machines: Constraint satisfaction networks that learn</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon University, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning and relearning in boltzmann machines. Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep belief networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Scholarpedia</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5947</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1977">1977. 2007</date>
		</imprint>
	</monogr>
	<note>Hopfield network</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hopfield networks is all you need</title>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schäfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milena</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geir</forename><surname>Kjetil Sandve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02217</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A survey of transformers</title>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>AI Open</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<biblScope unit="page" from="3" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><surname>Chatgpt</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rulebased machine learning methods for functional prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sholom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><surname>Indurkhya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="383" to="403" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Evolutionary machine learning: A survey</title>
		<author>
			<persName><forename type="first">Akbar</forename><surname>Telikani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirhessam</forename><surname>Tahmassebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021-10">oct 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A survey of topic modeling in text mining</title>
		<author>
			<persName><forename type="first">Rubayyi</forename><surname>Alghamdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Alfalqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Adv. Comput. Sci. Appl.(IJACSA)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Topic modeling: A comprehensive review</title>
		<author>
			<persName><forename type="first">Pooja</forename><surname>Kherwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Poonam</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EAI Endorsed Transactions on Scalable Information Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Édouard</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">85</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<title level="m">Data Mining, Fourth Edition: Practical Machine Learning Tools and Techniques</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>4th edition</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Christian Gehl, and Vojtech Franc. The shogun machine learning toolbox</title>
		<author>
			<persName><forename type="first">Sören</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Widmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Behr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>De Bona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">60</biblScope>
			<biblScope unit="page" from="1799" to="1802" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on intelligent systems and technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Google cloud</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Microsoft azure</title>
		<imprint>
			<publisher>Microsoft</publisher>
			<biblScope unit="page" from="3" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Machine learning on aws</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Ibm watson assistant</title>
		<author>
			<persName><surname>Ibm Watson</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">cudnn: Efficient primitives for deep learning</title>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno>CoRR, abs/1410.0759</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Robert Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diemthu</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyuan</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><surname>Hyun Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Salek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emad</forename><surname>Samadiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chris</forename><surname>Severn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gregory</forename><surname>Sizikov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matthew</forename><surname>Snelham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jed</forename><surname>Souter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Steinberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andy</forename><surname>Swing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mercedes</forename><surname>Tan</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017-06">jun 2017</date>
			<publisher>Gregory Thorson</publisher>
			<pubPlace>Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross; Bo Tian, Horia Toma, Erick Tuttle,</pubPlace>
		</imprint>
	</monogr>
	<note>In-datacenter performance analysis of a tensor processing unit</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep learning at scale on nvidia v100 accelerators</title>
		<author>
			<persName><forename type="first">Rengan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quy</forename><surname>Ta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Performance Modeling</title>
		<imprint>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
		<respStmt>
			<orgName>Benchmarking and Simulation of High Performance Computer Systems (PMBS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Dissecting the graphcore IPU architecture via microbenchmarking</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><forename type="middle">Paolo</forename><surname>Scarpazza</surname></persName>
		</author>
		<idno>CoRR, abs/1912.03413</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Accelerating scientific applications with sambanova reconfigurable dataflow architecture</title>
		<author>
			<persName><forename type="first">Murali</forename><surname>Emani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatram</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Papka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Florescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumti</forename><surname>Jairath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Nama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Sujeeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="114" to="119" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sampling methods for the nyström method</title>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="981" to="1006" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sampling with minimum sum of squared similarities for nystrom-based large scale spectral clustering</title>
		<author>
			<persName><forename type="first">Djallel</forename><surname>Bouneffouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inanc</forename><surname>Birol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2313" to="2319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A randomized algorithm for the decomposition of matrices</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Per-Gunnar Martinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Rokhlin</surname></persName>
		</author>
		<author>
			<persName><surname>Tygert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="68" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fast and balanced: Efficient label tree learning for large scale object recognition</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A streaming parallel decision tree algorithm</title>
		<author>
			<persName><forename type="first">Yael</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Haim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Tom-Tov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Large graph construction for scalable semisupervised learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="679" to="686" />
		</imprint>
	</monogr>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fast k nn graph construction with locality sensitive hashing</title>
		<author>
			<persName><forename type="first">Yan-Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanggang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">September 23-27, 2013. 2013</date>
			<biblScope unit="page" from="660" to="674" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 13</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A survey on learning to hash</title>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><surname>Heng Tao Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="769" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning on big graph: Label inference and regularization with anchor hierarchy</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengchang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1101" to="1114" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Scalable semisupervised learning by efficient anchor graph regularization</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1864" to="1877" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Awni Y Hannun</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Bounded activation functions for enhanced training stability of deep neural networks on visual pattern recognition problems</title>
		<author>
			<persName><forename type="first">Sung</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rabia</forename><surname>Khalil-Hani</surname></persName>
		</author>
		<author>
			<persName><surname>Bakhteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page" from="718" to="734" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Adaptive sampling for sgd by exploiting side information</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Gopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="364" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Variance reduction in sgd by distributed importance sampling</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06481</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks: Tricks of the Trade: Second Edition</title>
		<imprint>
			<biblScope unit="page" from="437" to="478" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<title level="m">On the convergence of adam and beyond</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName><surname>Ning Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Gradient methods for minimizing composite functions</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="161" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A survey of optimization methods from a machine learning perspective</title>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehui</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3668" to="3681" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">On optimization methods for deep learning</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Prochnow</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A generic coordinate descent framework for learning from implicit feedback</title>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargav</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Recent advances of large-scale linear classification</title>
		<author>
			<persName><forename type="first">Guo-Xun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Hua</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2584" to="2603" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Coordinate descent converges faster with the gauss-southwell rule than random selection</title>
		<author>
			<persName><forename type="first">Julie</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issam</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Friedlander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoyt</forename><surname>Koepke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1632" to="1641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Efficiency of coordinate descent methods on huge-scale optimization problems</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="362" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">A primer on coordinate descent algorithms</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenyinying</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00040</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A coordinate gradient descent method for l1regularized convex minimization</title>
		<author>
			<persName><forename type="first">Sangwoon</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim-Chuan</forename><surname>Toh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="307" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Accelerated proximal gradient methods for nonconvex programming</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Understanding the metropolishastings algorithm. The american statistician</title>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Chib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Greenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="327" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">suppl 1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Bayesian posterior sampling via stochastic gradient fisher scoring</title>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Sungjin Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6380</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Qsgd: Communication-efficient sgd via gradient quantization and encoding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demjan</forename><surname>Grubic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Vojnovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Terngrad: Ternary gradients to reduce communication in distributed deep learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01887</idno>
		<title level="m">Deep gradient compression: Reducing the communication bandwidth for distributed training</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Zipml: Training linear models with end-to-end low precision, and a little bit of deep learning</title>
		<author>
			<persName><forename type="first">Hantian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaan</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4035" to="4043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">More effective distributed ml via a stale synchronous parallel parameter server</title>
		<author>
			<persName><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cipar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Kyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garth</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Asynchronous stochastic gradient descent with delay compensation</title>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Ming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4120" to="4129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Deep learning with elastic averaging sgd. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">E</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Delay-tolerant algorithms for asynchronous distributed online learning</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Streeter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A bridging model for parallel computation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="103" to="111" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Gaia: Geo-distributed machine learning approaching lan speeds</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Konomis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="629" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Taming the wild: A unified analysis of hogwild!-style algorithms</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Distributed training with tensorflow</title>
		<author>
			<persName><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/guide/distributedtraining" />
		<imprint/>
	</monogr>
	<note>Last accessed 09.01.2022</note>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Distributed training with keras</title>
		<author>
			<persName><surname>Keras</surname></persName>
		</author>
		<idno>09.01.2022</idno>
		<ptr target="https://www.tensorflow.org/tutorials/distribute/keras" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/tutorials/beginner/distoverview.html" />
		<imprint/>
	</monogr>
	<note>Last accessed 09.01.2022</note>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Mxnet-mpi: Embedding mpi parallelism in parameter server task model for scaling deep learning</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Amith R Mamidala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fausto</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><surname>Artico</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in tensorflow</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balso</forename></persName>
		</author>
		<idno>CoRR, abs/1802.05799</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Bringing hpc techniques to deep learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Baidu Research</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Dianne: a modular framework for designing, training and deploying deep neural networks on heterogeneous distributed infrastructure</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coninck</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bohez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Verbelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Vankeirsbilck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Simoens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Dhoedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="52" to="65" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Cntk: Microsoft&apos;s open-source deep-learning toolkit</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16, page 2135</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16, page 2135<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<imprint>
			<pubPlace>Joseph Turian, David</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Theano: A cpu and gpu math compiler in python</title>
		<author>
			<persName><forename type="first">Warde-Farley</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Python in Science Conference</title>
		<meeting>the 9th Python in Science Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Apache spark: a unified engine for big data processing</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tathagata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><surname>Michael J Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Mapreduce: simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Mllib: Machine learning in apache spark</title>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burak</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davies</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Amde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doris</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Federated machine learning: Concept and applications</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">A secure federated transfer learning framework</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoping</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="70" to="82" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Fedbcd: A communication-efficient collaborative learning framework for distributed features</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="4277" to="4290" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Fedopt: Towards communication efficiency and privacy preservation in federated learning</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Moustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takayuki</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2864</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Group knowledge transfer: Federated learning of large cnns at the edge</title>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Avestimehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gary</forename><surname>Sivek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ananda</forename><surname>Theertha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Suresh</forename></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14068" to="14080" />
			<date type="published" when="2019">2020. 2019</date>
		</imprint>
	</monogr>
	<note>Agnostic federated learning</note>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Federated learning meets multi-objective optimization</title>
		<author>
			<persName><forename type="first">Zeou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiarash</forename><surname>Shaloudegi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00295</idno>
		<title level="m">Adaptive federated optimization</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karimireddy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Breaking the centralized barrier for cross-device federated learning</title>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Theertha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28663" to="28676" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Federated optimization in heterogeneous networks</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">SCAF-FOLD: Stochastic controlled averaging for federated learning</title>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Karimireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Theertha</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Learning private neural language modeling with attentive aggregation</title>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Tackling the objective inconsistency problem in heterogeneous federated optimization</title>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauri</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vincent Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7611" to="7623" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Adaptive personalized federated learning</title>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mahdi Kamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13461</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Personalized federated learning using hypernetworks</title>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Shamsian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Navon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9489" to="9502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Towards personalized federated learning</title>
		<author>
			<persName><forename type="first">Alysa</forename><surname>Ziying Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Statistical model aggregation via parameter matching</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristjan</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nghia</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric federated learning of neural networks</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristjan</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nghia</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Khazaeni</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7252" to="7261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Decentralized learning works: An empirical comparison of gossip learning and federated learning</title>
		<author>
			<persName><forename type="first">István</forename><surname>Hegedűs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Danner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Márk</forename><surname>Jelasity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="109" to="124" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Decentralized federated learning with unreliable communications</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">Ye</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="487" to="500" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Machine learning with adversaries: Byzantine tolerant gradient descent</title>
		<author>
			<persName><forename type="first">Peva</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><forename type="middle">El</forename><surname>El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachid</forename><surname>Mhamdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName><surname>Stainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">The byzantine generals problem</title>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Lamport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Shostak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshall</forename><surname>Pease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Concurrency: the works of leslie lamport</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="203" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Differential privacy</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">July 10-14, 2006. 2006</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Federated learning with differential privacy: Algorithms and performance analysis</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Farokhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">Qs</forename><surname>Quek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vincent Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3454" to="3469" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Ldpfed: Federated learning with local differential privacy</title>
		<author>
			<persName><forename type="first">Stacey</forename><surname>Truex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ka-Ho</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><forename type="middle">Emre</forename><surname>Gursoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking</title>
		<meeting>the Third ACM International Workshop on Edge Systems, Analytics and Networking</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="61" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Data poisoning attacks against federated learning systems</title>
		<author>
			<persName><forename type="first">Vale</forename><surname>Tolpegin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stacey</forename><surname>Truex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><forename type="middle">Emre</forename><surname>Gursoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Security-ESORICS 2020: 25th European Symposium on Research in Computer Security, ESORICS 2020</title>
		<meeting><address><addrLine>Guildford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">September 14-18, 2020. 2020</date>
			<biblScope unit="page" from="480" to="501" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 25</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Data poisoning in sequential and parallel federated learning</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Nuding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Mayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM on International Workshop on Security and Privacy Analytics, IWSPA &apos;22</title>
		<meeting>the 2022 ACM on International Workshop on Security and Privacy Analytics, IWSPA &apos;22</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Data poisoning attacks on federated machine learning</title>
		<author>
			<persName><forename type="first">Gan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="11365" to="11375" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Sparsefed: Mitigating model poisoning attacks in federated learning with sparsification</title>
		<author>
			<persName><forename type="first">Ashwinee</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Mahloujifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Supriyo</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7587" to="7624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Mpaf: Model poisoning attacks to federated learning based on fake clients</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3396" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Shieldfl: Mitigating model poisoning attacks in privacy-preserving federated learning</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinbin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingjiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">H</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1639" to="1654" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Houmansadr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE symposium on security and privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="739" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives</title>
		<author>
			<persName><forename type="first">Pengrui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybersecurity</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Batchcrypt: Efficient homomorphic encryption for crosssilo federated learning</title>
		<author>
			<persName><forename type="first">Chengliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhe</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference</title>
		<meeting>the 2020 USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>USENIX ATC 2020</note>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilko</forename><surname>Henecka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivey-Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Thorne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10677</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Svm-based generative adverserial networks for federated learning and edge computing attack model and outpoising</title>
		<author>
			<persName><forename type="first">Poongodi</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjan</forename><surname>Walia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celestine</forename><surname>Iwendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahamed</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName><surname>Ahanger</surname></persName>
		</author>
		<author>
			<persName><surname>St Suganthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Mm Kamruzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wajdi</forename><surname>Bourouis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounir</forename><surname>Alhakami</surname></persName>
		</author>
		<author>
			<persName><surname>Hamdi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Expert Systems, page e13072</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Poisongan: Generative poisoning attacks against federated learning in edge computing systems</title>
		<author>
			<persName><forename type="first">Jiale</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thi</forename><forename type="middle">Thanh</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shui</forename><surname>Binh</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3310" to="3322" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Adversarial machine learning attacks and defense methods in the cyber security domain</title>
		<author>
			<persName><forename type="first">Ishai</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Shabtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Elovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Ppfl: privacy-preserving federated learning with trusted execution environments</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Haddadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kleomenis</forename><surname>Katevas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Perino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Kourtellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th annual international conference on mobile systems, applications, and services</title>
		<meeting>the 19th annual international conference on mobile systems, applications, and services</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">A trainingintegrity privacy-preserving federated learning scheme with trusted execution environment</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">522</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Secure multi-party computation for federated learning</title>
		<author>
			<persName><forename type="first">Antigoni</forename><surname>Vaikkunth Mugunthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Polychroniadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tucker</forename><forename type="middle">Hybinette</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><surname>Balch</surname></persName>
		</author>
		<author>
			<persName><surname>Smpai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NeurIPS 2019 Workshop on Robust AI in Financial Services</title>
		<meeting>the NeurIPS 2019 Workshop on Robust AI in Financial Services</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Blockchain and federated learning for privacy-preserved data sharing in industrial iot</title>
		<author>
			<persName><forename type="first">Yunlong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueyue</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabita</forename><surname>Maharjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4177" to="4186" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Exploring the attack surface of blockchain: A comprehensive survey</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Spaulding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Njilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Kamhoua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daehun</forename><surname>Nyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mohaisen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1977" to="2008" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">A survey on blockchain systems: Attacks, defenses, and privacy preservation</title>
		<author>
			<persName><forename type="first">Yourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhuri</forename><surname>Siddula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Confidence Computing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">100048</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">Anonymizing data for privacy-preserving federated learning</title>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aris</forename><surname>Gkoulalas-Divanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Salonidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issa</forename><surname>Sylla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09096</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Abnormal client behavior detection in federated learning</title>
		<author>
			<persName><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09933</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">How to backdoor federated learning</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2938" to="2948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">Backdoor attacks and defenses in federated learning: State-of-theart, taxonomy, and future directions</title>
		<author>
			<persName><forename type="first">Xueluan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihan</forename><surname>Kong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>IEEE Wireless Communications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">A survey of privacy attacks in machine learning</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Rigaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Garcia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07646</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses</title>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chulin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1563" to="1580" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Pdgan: A novel poisoning defense method in federated learning using generative adversarial network</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiale</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shui</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithms and Architectures for Parallel Processing: 19th International Conference, ICA3PP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="595" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">A survey of homomorphic encryption for nonspecialists</title>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Fontaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Galand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Information Security</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<title level="m" type="main">Federated learning</title>
		<author>
			<persName><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/federated/federatedlearning" />
		<imprint/>
	</monogr>
	<note>Last accessed 09.01.2022</note>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Federated learning for image classification</title>
		<author>
			<persName><surname>Keras</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/federated/tutorials/" />
		<imprint/>
	</monogr>
	<note>federated learning for image classification Last accessed 09.01.2022</note>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gegi</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Rajamoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Yuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayaram</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Purcell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tran</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoise</forename><surname>Ngoc Minh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Supriyo</forename><surname>Holohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalisha</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Witherspoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Steuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hifaz</forename><surname>Wynter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Laguna</surname></persName>
		</author>
		<author>
			<persName><surname>Yurochkin</surname></persName>
		</author>
		<title level="m">Mayank Agarwal, Ebube Chuba, and Annie Abay</title>
		<imprint/>
	</monogr>
	<note>IBM federated learning: an enterprise framework white paper V0.1. CoRR, abs/2007.10987, 2020</note>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Ting</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristopher</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Harouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abood</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chester</forename><surname>Quraini</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<editor>Daguang Xu, Nic Ma, Prerna Dogra, Mona Flores, and Andrew Feng</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Nvidia flare: Federated learning from simulation to real-world</note>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Fedml: A research library and benchmark for federated machine learning</title>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhyun</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Vepakomma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Avestimehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Best Paper Award at Federate Learning Workshop</note>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Fate: An industrial grade platform for collaborative learning with data protection</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022-07">jul 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ziller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Trask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Lopardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Szymkow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bluemke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Mickael</forename><surname>Nounahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Passerat-Palmbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kritika</forename><surname>Prakash</surname></persName>
		</author>
		<title level="m">Théo Ryffel, Zarreen Naowal Reza, and Georgios Kaissis. PySyft: A Library for Easy Federated Learning</title>
		<meeting><address><addrLine>Nick Rose; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="111" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Openfl: the open federated learning library</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><forename type="middle">J</forename><surname>Sheller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Riviera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansi</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakash</forename><surname>Narayana Moorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Shi-Han Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parsa</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Mirhaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyridon</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics in Medicine &amp; Biology</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Leaf: A benchmark for federated settings</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Caldas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Meher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Duddu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">Datasets for running tensorflow federated simulations</title>
		<author>
			<persName><surname>Tensorflow</surname></persName>
		</author>
		<idno>02.21.2022</idno>
		<ptr target="https://www.tensorflow.org/federated/apidocs/python/tff/simulation/datasets" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m" type="main">Real-world image datasets for federated learning</title>
		<author>
			<persName><forename type="first">Jiahuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Blockchain-federatedlearning and deep learning models for covid-19 detection using ct imaging</title>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><forename type="middle">Aman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noorbakhsh</forename><surname>Amiri Golilarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="16301" to="16314" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">Fedtadbench: Federated timeseries anomaly detection benchmark</title>
		<author>
			<persName><forename type="first">Fanxing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Eleftherios Anastasiadis, and George Loukas. A taxonomy and survey of attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pitropakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Panaousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanassis</forename><surname>Giannetsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">100199</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Machine learning security: Threats, countermeasures, and evaluations</title>
		<author>
			<persName><forename type="first">Mingfu</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="74720" to="74742" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">Performance analysis and comparison of distributed machine learning systems</title>
		<author>
			<persName><forename type="first">Salem</forename><surname>Alqahtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Demirbas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02061</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Fault tolerance in iterative-convergent machine learning</title>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingjing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5220" to="5230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">Advances and open problems in federated learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Peter Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kallista</forename><forename type="middle">A</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><forename type="middle">El</forename><surname>D'oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rouayheb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badih</forename><surname>Gascón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaïd</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lie</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouyuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauri</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farinaz</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koushanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tancrède</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lepoint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayfer</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Özgür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Raykova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziteng</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Vepakomma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno>CoRR, abs/1912.04977</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Interpret federated learning with shapley values</title>
		<author>
			<persName><forename type="first">Guan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04519</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">A vertical federated learning method for interpretable scorecard and its application in credit scoring</title>
		<author>
			<persName><forename type="first">Fanglan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojia</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06218</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Fed-eini: An efficient and interpretable inference framework for decision tree ensembles in vertical federated learning</title>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongji</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE international conference on big data (big data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1242" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Balancing privacy protection and interpretability in federated learning</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichen</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajie</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.08044</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">Fedgraphnn: A federated learning system and benchmark for graph neural networks</title>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emir</forename><surname>Ceyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07145</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">Spreadgnn: Serverless multi-task federated learning for graph neural networks</title>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emir</forename><surname>Ceyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Avestimehr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02743</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Graphfl: A federated learning framework for semi-supervised node classification on graphs</title>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="498" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m" type="main">Federated graph neural networks: Overview, techniques and challenges</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07256</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
