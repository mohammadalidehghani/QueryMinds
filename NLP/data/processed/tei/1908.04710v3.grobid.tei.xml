<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">metric-learn: Metric Learning Algorithms in Python</title>
				<funder>
					<orgName type="full">Inria Parietal team</orgName>
				</funder>
				<funder>
					<orgName type="full">INRIA, France</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-07-27">27 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">William</forename><surname>De Vazelhes</surname></persName>
							<email>wdevazelhes@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nathalie</forename><surname>Vauquier</surname></persName>
							<email>nathalie.vauquier@inria.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Paris Research Center</orgName>
								<orgName type="institution" key="instit2">Huawei Technologies</orgName>
								<address>
									<postCode>92100</postCode>
									<settlement>Boulogne-Billancourt</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Ant Group</orgName>
								<orgName type="institution">Google LLC</orgName>
								<address>
									<addrLine>111 8th Ave 525 Almanor Ave</addrLine>
									<postCode>10011 94085</postCode>
									<settlement>New York Sunnyvale</settlement>
									<region>NY CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Magnet Team</orgName>
								<orgName type="institution" key="instit2">INRIA Lille -Nord</orgName>
								<address>
									<postCode>59650</postCode>
									<settlement>Europe Villeneuve d&apos;Ascq</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">metric-learn: Metric Learning Algorithms in Python</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-27">27 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">E448D407297F05D7935BDD79CA2D4BC1</idno>
					<idno type="arXiv">arXiv:1908.04710v3[cs.LG]</idno>
					<note type="submission">Submitted 8/19; Revised 7/20;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>machine learning</term>
					<term>python</term>
					<term>metric learning</term>
					<term>scikit-learn</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>metric-learn is an open source Python package implementing supervised and weaklysupervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT license.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many approaches in machine learning require a measure of distance between data points. Traditionally, practitioners would choose a standard distance metric (Euclidean, City-Block, Cosine, etc.) using a priori knowledge of the domain. However, it is often difficult to design metrics that are well-suited to the particular data and task of interest. Distance metric learning, or simply metric learning <ref type="bibr" target="#b0">(Bellet et al., 2015)</ref>, aims at automatically constructing task-specific distance metrics from data. A key advantage of metric learning is that it can be applied beyond the standard supervised learning setting (data points associated with labels), in situations where only weaker forms of supervision are available (e.g., pairs of points that should be similar/dissimilar). The learned distance metric can be used to perform retrieval tasks such as finding elements (images, documents) of a database that are semantically closest to a query element. It can also be plugged into other machine learning algorithms, for instance to improve the accuracy of nearest neighbors models (for classification, regression, anomaly detection...) or to bias the clusters found by clustering  <ref type="bibr" target="#b3">(Huang et al., 2012)</ref>.</p><p>algorithms towards the intended semantics. Finally, metric learning can be used to perform dimensionality reduction. These use-cases highlight the importance of integrating metric learning with the rest of the machine learning pipeline and tools. metric-learn is an open source package for metric learning in Python, which implements many popular metric-learning algorithms with different levels of supervision through a unified interface. Its API is compatible with scikit-learn <ref type="bibr" target="#b8">(Pedregosa et al., 2011)</ref>, a prominent machine learning library in Python. This allows for streamlined model selection, evaluation, and pipelining with other estimators.</p><p>Positioning with respect to other packages. Many metric learning algorithms were originally implemented by their authors in Matlab without a common API convention. 1 In R, the package dml <ref type="bibr" target="#b14">(Tang et al., 2018)</ref> implements several metric learning algorithms with a unified interface but is not tightly integrated with any general-purpose machine learning library. In Python, pyDML <ref type="bibr" target="#b13">(Suárez et al., 2020)</ref> contains mainly fully supervised and unsupervised algorithms, while pytorch-metric-learning 2 focuses on deep metric learning using the pytorch framework <ref type="bibr" target="#b7">(Paszke et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background on Metric Learning</head><p>Metric learning is generally formulated as an optimization problem where one seeks to find the parameters of a distance function that minimize some objective function over the input data. All algorithms currently implemented in metric-learn learn so-called Mahalanobis distances. Given a real-valued parameter matrix L of shape (n components, n features) where n features is the number of features describing the data, the associated Mahalanobis distance between two points x and x is defined as</p><formula xml:id="formula_0">D L (x, x ) = (Lx -Lx ) (Lx -Lx ).</formula><p>This is equivalent to Euclidean distance after linear transformation of the feature space defined by L. Thus, if L is the identity matrix, standard Euclidean distance is recovered. Mahalanobis distance metric learning can thus be seen as learning a new embedding space, with potentially reduced dimension n components. Note that D L can also be written as</p><formula xml:id="formula_1">D L (x, x ) = (x -x ) M (x -x )</formula><p>, where we refer to M = L L as the Mahalanobis matrix.</p><p>Metric learning algorithms can be categorized according to the form of data supervision they require to learn a metric. metric-learn currently implements algorithms that fall into the following categories. Supervised learners learn from a data set with one label per training example, aiming to bring together points from the same class while spreading points from different classes. For instance, data points could be face images and the class could be the identity of the person (see Figure <ref type="figure" target="#fig_0">1a</ref>). Pair learners require a set of pairs of points, with each pair labeled to indicate whether the two points are similar or not. These methods aim to learn a metric that brings pairs of similar points closer together and pushes pairs of dissimilar points further away from each other. Such supervision is often simpler to collect than class labels in applications when there are many labels. For instance, a human annotator can often quickly decide whether two face images correspond to the same person (Figure <ref type="figure" target="#fig_0">1b</ref>) while matching a face to its identity among many possible people may be difficult. Triplet learners consider 3-tuples of points and learn a metric that brings the first (anchor ) point of each triplet closer to the second point than to the third one. Finally, quadruplet learners consider 4-tuples of points and aim to learn a metric that brings the two first points of each quadruplet closer than the two last points. Both triplet and quadruplets learners can be used to learn a metric space where closer points are more similar with respect to an attribute of interest, in particular when this attribute is continuous and/or difficult to annotate accurately (e.g., the hair color of a person on an image, see Figure <ref type="figure" target="#fig_0">1c</ref>, or the age of a person, see Figure <ref type="figure" target="#fig_0">1d</ref>). Triplet and quadruplet supervision can also be used in problems with a class hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of the Package</head><p>The current release of metric-learn (v0.6.2) can be installed from the Python Package Index (PyPI) and conda-forge, for Python 3.6 or later. 3 The source code is available on GitHub at <ref type="url" target="http://github.com/scikit-learn-contrib/metric-learn">http://github.com/scikit-learn-contrib/metric-learn</ref> and is free to use, provided under the MIT license. metric-learn depends on core libraries from the SciPy ecosystem: numpy, scipy, and scikit-learn. Detailed documentation (including installation guidelines, the description of the algorithms and the API, as well as examples) is available at <ref type="url" target="http://contrib.scikit-learn.org/metric-learn">http://contrib.scikit-learn.org/metric-learn</ref>. The development is collaborative and open to all contributors through the usual GitHub workflow of issues and pull requests. Community interest for the package has been demonstrated by its recent inclusion in the scikit-learn-contrib organization which hosts high-quality scikit-learncompatible projects, 4 and by its more than 1000 stars and 200 forks on GitHub at the time of writing. The quality of the code is ensured by a thorough test coverage (97% as of June 2020). Every new contribution is automatically checked by a continuous integration platform to enforce sufficient test coverage as well as syntax formatting with flake8.</p><p>Currently, metric-learn implements 10 popular metric learning algorithms. Supervised learners include Neighborhood Components Analysis (NCA, <ref type="bibr" target="#b2">Goldberger et al., 2004)</ref>, Large Margin Nearest Neighbors (LMNN, <ref type="bibr" target="#b15">Weinberger and Saul, 2009)</ref>, Relative Components Analysis (RCA, <ref type="bibr" target="#b10">Shental et al., 2002)</ref>, 5 Local Fisher Discriminant Analysis (LFDA, <ref type="bibr" target="#b12">Sugiyama, 2007)</ref> and Metric Learning for Kernel Regression (MLKR, <ref type="bibr" target="#b16">Weinberger and Tesauro, 2007)</ref>. The latter is designed for regression problems with continuous labels. Pair learners include Mahalanobis Metric for Clustering (MMC, <ref type="bibr" target="#b17">Xing et al., 2002)</ref>, Information Theoretic Metric Learning (ITML, <ref type="bibr" target="#b1">Davis et al., 2007)</ref> and Sparse High-Dimensional Metric 3. Support for Python 2.7 and 3.5 was dropped in v0.6.0. 4. <ref type="url" target="https://github.com/scikit-learn-contrib/scikit-learn-contrib">https://github.com/scikit-learn-contrib/scikit-learn-contrib</ref> 5. RCA takes as input slightly weaker supervision in the form of chunklets (groups of points of same class).</p><p>Learning <ref type="bibr">(SDML, Qi et al., 2009)</ref>. Finally, the package implements one triplet learner and one quadruplet learner: Sparse Compositional Metric Learning (SCML, <ref type="bibr" target="#b11">Shi et al., 2014)</ref> and Metric Learning from Relative Comparisons by Minimizing Squared Residual (LSML, <ref type="bibr" target="#b4">Liu et al., 2012)</ref>. Detailed descriptions of these algorithms can be found in the package documentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Software Architecture and API</head><p>metric-learn provides a unified interface to all metric learning algorithms. It is designed to be fully compatible with the functionality of scikit-learn. All metric learners inherit from an abstract BaseMetricLearner class, which itself inherits from scikit-learn's BaseEstimator. All classes inheriting from BaseMetricLearner should implement two methods: get metric (returning a function that computes the distance, which can be plugged into scikit-learn estimators like KMeansClustering) and score pairs (returning the distances between a set of pairs of points passed as a 3D array). Mahalanobis distance learning algorithms also inherit from a MahalanobisMixin interface, which has an attribute components corresponding to the transformation matrix L of the Mahalanobis distance. MahalanobisMixin implements get metric and score pairs accordingly as well as a few additional methods. In particular, transform allows to transform data using components , and get mahalanobis matrix returns the Mahalanobis matrix M = L T L.</p><p>Supervised metric learners inherit from scikit-learn's base class TransformerMixin, the same base class used by sklearn.LinearDiscriminantAnalysis and others. As such, they are compatible for pipelining with other estimators via sklearn.pipeline.Pipeline. To illustrate, the following code snippet trains a Pipeline composed of LMNN followed by a k-nearest neighbors classifier on the UCI Wine data set, with the hyperparameters selected with a grid-search. Any other supervised metric learner can be used in place of LMNN.</p><p>from sklearn.datasets import load_wine from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.pipeline import Pipeline from metric_learn import LMNN X_train, X_test, y_train, y_test = train_test_split(*load_wine(return_X_y=True)) lmnn_knn = Pipeline(steps=[('lmnn', LMNN()), ('knn', KNeighborsClassifier())]) parameters = {'lmnn__k':[1, 2], 'knn__n_neighbors':[1, 2]} grid_lmnn_knn = GridSearchCV(lmnn_knn, parameters, cv=3, n_jobs=-1, verbose=True) grid_lmnn_knn.fit(X_train, y_train) grid_lmnn_knn.score <ref type="bibr">(X_test, y_test)</ref> Weakly supervised algorithms (pair, triplet and quadruplet learners) fit and predict on a set of tuples passed as a 3-dimensional array. Tuples can be pairs, triplets, or quadruplets depending on the algorithm. Pair learners take as input an array-like pairs of shape (n pairs, 2, n features), as well as an array-like y pairs of shape (n pairs,) giving labels (similar or dissimilar) for each pair. In order to predict the labels of new pairs, one needs to set a threshold on the distance value. This threshold can be set manually or automatically calibrated (at fit time or afterwards on a validation set) to optimize a given score such as accuracy or F1-score using the method calibrate threshold. Triplet learners work on array-like of shape (n triplets, 3, n features), where for each triplet we want the first element to be closer to the second than to the third one. Quadruplet learners work on array-like of shape (n quadruplets, 4, n features), where for each quadruplet we want the two first elements to be closer together than the two last ones. Both triplet and quadruplet learners can naturally predict whether a new triplet/quadruplet is in the right order by comparing the two pairwise distances. To illustrate the weakly-supervised learning API, the following code snippet computes cross validation scores for MMC on pairs from Labeled Faces in the Wild <ref type="bibr" target="#b3">(Huang et al., 2012)</ref>. Thanks to our unified interface, MMC can be switched for another pair learner without changing the rest of the code below.</p><p>from sklearn.datasets import fetch_lfw_pairs from sklearn.model_selection import cross_validate, train_test_split from metric_learn import MMC ds = fetch_lfw_pairs() pairs = ds.pairs.reshape(*ds.pairs.shape[:2], -1) # we transform 2D images into 1D vectors y_pairs = 2 * ds.target -1 # we need the labels to be in {+1, -1} pairs, _, y_pairs, _ = train_test_split(pairs, y_pairs) cross_validate(MMC(diagonal=True), pairs, y_pairs, scoring='roc_auc <ref type="bibr">',</ref><ref type="bibr">return_train_score=True,</ref><ref type="bibr">cv=3,</ref><ref type="bibr">verbose=True)</ref> 5. Future Work metric-learn is under active development. We list here some promising directions to further improve the package. To scale to large data sets, we would like to implement stochastic solvers (SGD and its variants), forming batches of tuples on the fly to avoid loading all data in memory at once. We also plan to incorporate recent algorithms that provide added value to the package, such as those that can deal with multi-label <ref type="bibr" target="#b6">(Liu and Tsang, 2015)</ref> and high-dimensional problems <ref type="bibr" target="#b5">(Liu and Bellet, 2019)</ref>, or learn other forms of metrics like bilinear similarities, nonlinear and local metrics (see <ref type="bibr" target="#b0">Bellet et al., 2015</ref>, for a survey).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Different types of supervision for metric learning illustrated on face image data taken from the Labeled Faces in the Wild data set (Huang et al., 2012).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We are thankful to Inria for funding 2 years of development. We also thank scikit-learn developers from the <rs type="funder">Inria Parietal team</rs> (in particular <rs type="person">Gaël Varoquaux</rs>, <rs type="person">Alexandre Gramfort</rs> and <rs type="person">Olivier Grisel</rs>) for fruitful discussions on the design of the API and funding to attend SciPy 2019, as well as scikit-learn-contrib reviewers for their valuable feedback.</p></div>
			</div>
			<div type="funding">
<div><p>Most of the work was carried out while the author was affiliated with <rs type="funder">INRIA, France</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Metric Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Information-Theoretic Metric Learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neighbourhood Components Analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to Align from Scratch</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Metric Learning from Relative Comparisons by Minimizing Squared Residual</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Escaping the Curse of Dimensionality in Similarity Learning: Efficient Frank-Wolfe Algorithm and Generalization Bounds</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="185" to="199" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large Margin Metric Learning for Multi-Label Prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Efficient Sparse Metric Learning in High-dimensional Space via L1-penalized Log-determinant Regularization</title>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adjustment Learning and Relevant Component Analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shental</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparse Compositional Metric Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1027" to="1061" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">pyDML: A Python Library for Distance Metric Learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">dml: Distance metric learning in R</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page">1036</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Metric Learning for Kernel Regression</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distance Metric Learning with Application to Clustering with Side-Information</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
