<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000</title>
				<funder>
					<orgName type="full">IEEE TRANS-ACTIONS ON IMAGE PROCESSING</orgName>
				</funder>
				<funder ref="#_3YKSXbd">
					<orgName type="full">Program for New Century Excellent Talents in University</orgName>
				</funder>
				<funder>
					<orgName type="full">New Century Excellent Talents in University of China</orgName>
				</funder>
				<funder ref="#_CrTNnMP #_2hpuHpb">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_RFAZbny">
					<orgName type="full">Foundation for the Author of National Excellent Doctoral Dissertation of China</orgName>
					<orgName type="abbreviated">FANEDD</orgName>
				</funder>
				<funder>
					<orgName type="full">IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING</orgName>
				</funder>
				<funder ref="#_pwhSE6T">
					<orgName type="full">IEEE</orgName>
				</funder>
				<funder>
					<orgName type="full">National Key Laboratory of Science and Technology on ATR, National University of Defense Technology</orgName>
				</funder>
				<funder>
					<orgName type="full">National University of Defense Technology</orgName>
				</funder>
				<funder ref="#_h6aVZn4">
					<orgName type="full">National Excellent Doctoral Dissertation Award of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-05-15">15 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Gong</surname></persName>
							<email>gongzhiqiang13@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronic Science and Technology</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on ATR</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Zhong</surname></persName>
							<email>zhongping@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronic Science and Technology</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on ATR</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">College of Electronic Science and Technology</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on ATR</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>SENIOR MEMBER, IEEE</roleName><forename type="first">Weidong</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronic Science and Technology</orgName>
								<orgName type="laboratory">National Key Laboratory of Science and Technology on ATR</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-15">15 May 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">C820E0EF1C7DF8BF6DE52616843DAF53</idno>
					<idno type="DOI">10.1109/ACCESS.2017.DOI</idno>
					<idno type="arXiv">arXiv:1807.01477v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>INDEX TERMS Diversity</term>
					<term>Training Data</term>
					<term>Model Learning</term>
					<term>Inference</term>
					<term>Supervised Learning</term>
					<term>Active Learning</term>
					<term>Unsupervised Learning</term>
					<term>Posterior Regularization Diversity in Machine Learning Diversity in Machine Learning (Section III-V) Data Diversification (Section III) Model Diversification (Section IV) Inference Diversification (Section V) D-model (Section IV-A) D-models (Section IV-B) Data Model Inference Machine Learning (Section II) Remote Sensing Imaging Task Camera Relocalization Machine Translation Information Retrieval Extensive Applications (Section VI)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a total good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though the diversity plays an important role in machine learning process, there is no systematical analysis of the diversification in machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process, respectively. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed, including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work. Our analysis provides a deeper understanding of the diversity technology in machine learning tasks, and hence can help design and learn more effective models for real-world applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Traditionally, machine learning methods can learn model's parameters automatically with the training samples and thus it can provide models with good performances which can satisfy the special requirements of various applications. Actually, it has achieved great success in tackling many realworld artificial intelligence and data mining problems <ref type="bibr" target="#b0">[1]</ref>, such as object detection <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, natural image processing <ref type="bibr" target="#b4">[5]</ref>, autonomous car driving <ref type="bibr" target="#b5">[6]</ref>, urban scene understanding <ref type="bibr" target="#b6">[7]</ref>, machine translation <ref type="bibr" target="#b3">[4]</ref>, and web search/information retrieval <ref type="bibr" target="#b7">[8]</ref>, and others. A success machine learning system often requires plentiful training data which can provide enough information to train the model, a good model learning process which can better model the data, and an accurate inference to discriminate different objects. However, in real-world applications, limited number of labelled training data are available. Besides, there exist large amounts of parameters in the machine learning model. These would make the "over-fitting" phenomenon in the machine learning process. Therefore, obtaining an accurate inference from the machine learning model tends to be a difficult task. Many factors can help to improve the performance of the machine learning process, among which the diversity in machine learning plays an important role.</p><p>Diversity shows different concepts depending on context and application <ref type="bibr" target="#b38">[39]</ref>. Generally, a diversified system contains more information and can better fit for various environments. It has already become an important property in many social fields, such as biological system, culture, products and so on. Particularly, the diversity property also has significant effects on the learning process of the machine learning system. Therefore, we wrote this survey mainly for two reasons. First, while the topic of diversity in machine learning methods has received attention for many years, there is no framework of diversity technology on general machine learning models. Although <ref type="bibr">Kulesza et al.</ref> discussed the determinantal point processes (DPP) in machine learning which is only one of the measurements for diversity <ref type="bibr" target="#b38">[39]</ref>. <ref type="bibr" target="#b10">[11]</ref> mainly summarized the diversity-promoting methods for obtaining multiple diversified search results in the inference phase. Besides, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b67">[68]</ref> analyzed several methods on classifier ensembles, which represents only a specific form of ensemble learning. All these works do not provide a full survey of the topic, nor do they focus on machine learning with general forms. Our main aim is to provide such a survey, hoping to induce diversity in general machine learning process. As a second motivation, this survey is also useful to researchers working on designing effective learning process.</p><p>Here, the diversity in machine learning works mainly on decreasing the redundancy between the data or the model and providing informative data or representative model in the machine learning process. This work will discuss the diversity property from different components of the machine learning process, including the training data, the learned model, and the inference. The diversity in machine learning tries to decrease the redundancy in the training data, the learned model as well as the inference and provide more information for machine learning process. It can improve the performance of the model and has played an important role in machine learning process. In this work, we summarize the diversification of machine learning into three categories: the diversity in training data (data diversification), the diversity of the model/models (model diversification) and the diversity of the inference (inference diversification).</p><p>Data diversification can provide samples with enough information to train the machine learning model. The diversity in training data aims to maximize the information contained in the data. Therefore, the model can learn more information from the data via the learning process and the learned model can be better fit for the data. Many prior works have imposed the diversity on the construction of each training batch for the machine learning process to train the model more effectively <ref type="bibr" target="#b8">[9]</ref>. In addition, diversity in active learning can also make the labelled training data contain the most information <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and thus the learned model can achieve good performance with limited training samples. Moreover, in special unsupervised learning method by <ref type="bibr" target="#b37">[38]</ref>, diversity of the pseudo classes can encourage the classes to repulse from each other and thus the learned model can provide more discriminative features from the objects.</p><p>Model diversification comes from the diversity in human visual system. <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref> have shown that the human visual system represents decorrelation and sparseness, namely diversity. This makes different neurons in the human learning respond to different stimuli and generates little redundancy in the learning process which ensures the high effectiveness of the human learning. However, general machine learning methods usually perform the redundancy in the learned model where different factors model the similar features <ref type="bibr" target="#b18">[19]</ref>. Therefore, diversity between the parameters of the model (Dmodel) could significantly improve the performance of the machine learning systems. The D-model tries to encourage different parameters in each model to be diversified and each parameter can model unique information <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. As a result, the performance of each model can be significantly improved <ref type="bibr" target="#b21">[22]</ref>. However, general machine learning model usually provides a local optimal representation of the data with limited training data. Therefore, ensemble learning, which can learn multiple models simultaneously, becomes another hot machine learning methods to provide multiple choices and has been widely applied in many real-world applications, such as the speech recognition <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and image segmentation <ref type="bibr" target="#b26">[27]</ref>. However, general ensemble learning usually makes the learned multiple base models converge to the same or similar local optima. Thus, diversity among multiple base models by ensemble learning (D-models) , which tries to repulse different base models and encourages each base model to provide choice reflecting multi-modal belief <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, can provide multiple diversified choices and significantly improve the performance.</p><p>Instead of learning multiple models with D-models, one can also obtain multiple choices in the inference phase, which is generally called multiple choice learning (MCL). However, the obtained choices from usual machine learning systems presents similarity between each other where the next choice will be one-pixel shifted versions of others <ref type="bibr" target="#b27">[28]</ref>. Therefore, to overcome this problem, diversity-promoting prior can be imposed over the obtained multiple choices from the inference. Under the inference diversification, the model can provide choices/representations with more complement information <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b31">[32]</ref>. This could further improve the performance of the machine learning process and provide multiple discriminative choices of the objects.</p><p>This work systematically covers the literature on diversitypromoting methods over data diversification, model diversification, and inference diversification in machine learning tasks. In particular, three main questions from the analysis of diversity technology in machine learning have arisen.</p><p>• How to measure the diversity of the training data, the learned model/models, and the inference and enhance these diversity in machine learning system, respectively? How do these methods work on the diversification of the machine learning system? • Is there any difference between the diversification of the model and models? Furthermore, is there any similarity between the diversity in the training data, the learned model/models, and the inference? • Which real-world applications can the diversity be applied in to improve the performance of the machine learning models? How do the diversification methods work on these applications? Although all of the three problems are important, none of them has been thoroughly answered. Diversity in machine learning can balance the training data, encourage the learned parameters to be diversified, and diversify the multiple choices from the inference. Through enforcing diversity in the machine learning system, the machine learning model can present a better performance. Following the framework, the three questions above have been answered with both the theoretical analysis and the real-world applications.</p><p>The remainder of this paper is organized as Fig. <ref type="figure" target="#fig_0">1</ref> shows. Section II discusses the general forms of the supervised learning and the active learning as well as a special form of unsupervised learning in machine learning model. Besides, as Fig. <ref type="figure" target="#fig_0">1</ref> shows, Sections III, IV and V introduce the diversity methods in machine learning models. Section III outlines some of the prior works on diversification in training data. Section IV reviews the strategies for model diversification, including the D-model, and the D-models. The prior works for inference diversification are summarized in Section V. Finally, section VI introduces some applications of the diversity-promoting methods in prior works, and then we do some discussions, conclude the paper and point out some future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. GENERAL MACHINE LEARNING MODELS</head><p>Traditionally, machine learning consists of supervised learning, active learning, unsupervised learning, and reinforcement learning. For reinforcement learning, training data is given only as the feedback to the program's actions in a dynamic environment, and it does not require accurate input/output pairs and the sub-optimal actions need not to be explicitly correct. However, the diversity technologies mainly work on the model itself to improve the model's performance. Therefore, this work will ignore the reinforcement learning and mainly discuss the machine learning model as Fig. <ref type="figure">2</ref> shows. In the following, we'll introduce the general forms of supervised learning and a representative form of active learning as well as a special form of unsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SUPERVISED LEARNING</head><p>We consider the task of general supervised machine learning models, which are commonly used in real-word machine learning tasks. Fig. <ref type="figure">2</ref> shows the flowchart of general machine learning methods in this work. As Fig. <ref type="figure">2</ref> shows, the supervised machine learning model consists of data preprocessing, training (modeling), and inference. All of the steps can affect the performance of the machine learning process.</p><p>Let X = {x 1 , x 2 , • • • , x N1 } denote the set of training samples and y i is the corresponding label of x i , where</p><formula xml:id="formula_0">y i ∈ Ω = {cl 1 , cl 2 , • • • , cl n } (Ω</formula><p>is the set of class labels, n is the number of the classes, and N 1 is the number of the labelled training samples). Traditionally, the machine learning task can be formulated as the following optimization problem <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_1">max W L(W |X) s.t. g(W ) ≥ 0 (1)</formula><p>where L(W |X) represents the loss function and W is the parameters of the machine learning model. Besides, g(W ) ≥ 0 is the constraint of the parameters of the model. Then, the Lagrange multiplier of the optimization can be reformulated as follows.</p><formula xml:id="formula_2">L 0 = L(W |X) + ηg(W )<label>(2)</label></formula><p>where η is a positive value. Therefore, the machine learning problem can be seen as the minimization of L 0 . Figs. <ref type="figure" target="#fig_1">3</ref> and <ref type="figure">4</ref> show the flowchart of two special forms of supervised learning models, which are generally used in realworld applications. Among them, Fig. <ref type="figure" target="#fig_1">3</ref> shows the flowchart of a special form of supervised machine learning with a single model. Generally, in the data-preprocessing stage, the more diversification and balance each training batch has, the more effectiveness the training process is. In addition, it should be noted that the factors in the same layer of the model can be diversified to improve the representational ability of the model (which is called D-model in this paper). Moreover, when we obtain multiple choices from the model in the inference, the obtained choices are desired to provide more complement information. Therefore, some works focus on the diversification of multiple choices (which we call inference diversification). Fig. <ref type="figure">4</ref> shows the flowchart of supervised machine learning with multiple parallel base models. We can find that a best strategy to diversify the training set for different base models can improve the performance of the whole ensemble (which is called D-models). Furthermore, we can diversify these base models directly to enforce each base model to provide more complement information for further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ACTIVE LEARNING</head><p>Since labelling is always cost and time consuming, it usually cannot provide enough labelled samples for training in real world applications. Therefore, active learning, which can reduce the label cost and keep the training set in a moderate size, plays an important role in the machine learning model <ref type="bibr" target="#b34">[35]</ref>. It can make use of the most informative samples and</p><p>VOLUME 4, 2016 Data Preprocessing Training Modeling Inference Active Learning Supervised Learning Unsupervised Learning labelled data unlabelled data Candidate Data FIGURE 2: Flowchart for training process of general machine learning (including the active learning, supervised learning and unsupervised learning). We can find that when the training data is labelled, the training process is supervised. In contrast, the training process is unsupervised. Besides, it should be noted that when both the labelled and unlabelled data are used for training, the training process is semi-supervised.</p><p>Training Data Model Latent Parameter Factors Layer 1 Layer s Diversification Inference Choice 1 Choice 2 Choice M Data Preprocessing samples  Training batch  Data augmentation In addition, it should be noted that the factors in the same layer of the model can be diversified to improve the representational ability of the model (which is called D-model in this paper). Moreover, when we obtain multiple choices from the model, the obtained choices are desired to provide more complement information. Therefore, some works focus on the diversification of multiple choices (which we call inference diversification).</p><p>provide a higher performance with less labelled training samples.</p><p>Through active learning, we can choose the most informative samples for labelling to train the model. This paper will take the Convex Transductive Experimental Design (CTED) as a representative of the active learning methods <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p><formula xml:id="formula_3">Denote U = {u i } N2</formula><p>i=1 as the candidate unlabelled samples for active learning, where N 2 represents the number of the candidate unlabelled samples. Then, the active learning problem can be formulated as the following optimization problem <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_4">A * , b * = arg min A,b U -U A 2 F + N2 i=1 N2 j=1 a 2 ij b i + α b 1 s.t. b i ≥ 0, i = 1, 2, • • • , N 2<label>(3</label></formula><p>) where a ij is the (i, j)-th entry of A, and α is a positive tradeoff parameter.</p><p>• F represents the Frobenius norm (F-norm) which calculates the root of the quadratic sum of the items in a matrix. As is shown, CTED utilizes a data reconstruction framework to select the most informative</p><p>Training Data Subset 1 Subset 2 Subset M Choice 1 Choice 2 Choice M Model 1 Model 2 Model M Diversification FIGURE 4: Flowchart of supervised machine learning with multiple parallel models. We can find that a best strategy to diversify the training set for different models can improve the performance of multiple models. Furthermore, we can diversify different models directly to enforce different model to provide more complement information for further analysis.</p><p>samples for labelling. The matrix A contains reconstruction coefficients and b is the sample selection vector. The L 1 -norm makes the learned b to be sparse. Then, the obtained b * is used to select samples for labelling and finally the training set is constructed with the selected training samples. However, the selected samples from CTED usually make similarity from each other, which leads to the redundancy of the training samples. Therefore, diversity property is also required in the active learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. UNSUPERVISED LEARNING</head><p>As discussed in former subsection, limited number of the training samples will limit the performance of the machine learning process. Instead of the active learning, to solve the problem, unsupervised learning methods provide another way to train the machine learning model without the labelled training samples. This work will mainly discuss a special unsupervised learning process developed by <ref type="bibr" target="#b37">[38]</ref>, which is an end-to-end self-supervised method. Denote c i (i = 1, 2, • • • , Λ) as the center points which is used to formulate the pseudo classes in the training process where Λ represents the number of the pseudo classes. Just as subsection II-B, U = {u 1 , u 2 , • • • , u N2 } represents the unlabelled training samples and N 2 denotes the number of the unsupervised samples. Besides, denote ϕ(u i ) as the features of u i extracted from the machine learning model. Then, the pseudo label z i of the data u i can be defined as</p><formula xml:id="formula_5">z i = min k∈{1,2,••• ,Λ} c k -ϕ(u i ) ,<label>(4)</label></formula><p>Then, the problem can be transformed to a supervised one with the pseudo classes. As shown in subsection II-A, the machine learning task can be formulated as the following optimization <ref type="bibr" target="#b37">[38]</ref> max</p><formula xml:id="formula_6">W,ci L(W |U, z i ) + ηg(W ) + N2 k=1 c z k -ϕ(u k )<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">L(W |U, z i (i = 1, 2, • • • , N 2 )</formula><p>) denotes the optimization term and</p><formula xml:id="formula_8">N2 k=1 c z k -ϕ(u k</formula><p>) is used to minimize the intra-class variance of the constructed pseudo-classes. g(W ) demonstrates the constraints in Eq. 2. With the iteratively learning of Eq. 4 and Eq. 5, the machine learning model can be trained unsupervisedly.</p><p>Since the center points play an important role in the construction of the pseudo classes, diversifying these center points and repulsing the points from each other can better discriminate these pseudo classes. This would show positive effects on improving the effectiveness of the unsupervised learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ANALYSIS</head><p>As former subsections show, diversity can improve the performance of the machine learning process. In the following, this work will summarize the diversification in machine learning from three aspects: data diversification, model diversification, and inference diversification.</p><p>To be concluded, diversification can be used in supervised learning, active learning, and unsupervised learning to improve the model's performance. According to the models in II-A and II-B, the diversification technology in machine learning model has been divided into three parts: data diversification (Section III), model diversification (Section IV), and inference diversification (Section V). Since the diversification in training batch (Fig. <ref type="figure" target="#fig_1">3</ref>) and the diversification in active learning and unsupervised learning mainly consider the diversification in training data, we summarize the prior works in these diversification as data diversification in section III. Besides, the diversification of the model in Fig. <ref type="figure" target="#fig_1">3</ref> and the multiple base models in Fig. <ref type="figure">4</ref> mainly focus on the diversification in the machine learning model directly, and thus we summarize these works as model diversification in section IV. Finally, the inference diversification in Fig. <ref type="figure" target="#fig_1">3</ref> will be summarized in section V. In the following section, we'll first introduce the data diversification in machine learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATA DIVERSIFICATION</head><p>Obviously, the training data plays an important role in the training process of the machine learning models. For supervised learning in subsection II-A, the training data provides more plentiful information for the learning of the parameters. While for active learning in subsection II-B, the learning process would select the most informative and less redundant samples for labelling to obtain a better performance. Besides, for unsupervised learning in subsection II-C, the pseudo classes can be encouraged to repulse from each other and the model can provide more discriminative features unsupervisedly. The following will introduce the methods for these data diversification in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DIVERSIFICATION IN SUPERVISED LEARNING</head><p>General supervised learning model is usually trained with mini-batches to accurately estimate the training model. Most of the former works generate the mini-batches randomly. However, due to the imbalance of the training samples under random selection, redundancy may occur in the generated mini-batches which shows negative effects on the effectiveness of the machine learning process. Different from classical stochastic gradient descent (SGD) method which relies on the uniformly sampling data points to form a mini-batch, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> proposes a non-uniformly sampling scheme based on the determinantal point process (DPP) measurement.</p><p>A DPP is a distribution over subsets of a fixed ground set, which prefers a diverse set of data other than a redundant one <ref type="bibr" target="#b38">[39]</ref>. Let Θ denote a continuous space and the data x i ∈ Θ(i = 1, 2, • • • , N 1 ). Then, the DPP denotes a positive semidefinite kernel function on Θ,</p><formula xml:id="formula_9">φ : Θ × Θ → R P (X ∈ Θ) = det(φ(X)) det(φ + I)<label>(6)</label></formula><p>where φ(X) denotes the kernel matrix and the pairwise φ(x i , x j ) is the pairwise correlation between the data x i and x j . det(•) denotes the determinant of matrix. I is an identity matrix. Since the space Θ is constant, det(φ + I) is a constant value. Therefore, the corresponding diversity prior of transition parameter matrix modeled by DPP can be formulated as P (X) ∝ det(φ(X))</p><p>In general, the kernel can be divided into the correlation and the prior part. Therefore, the kernel can be reformulated as</p><formula xml:id="formula_11">φ(x i , x j ) = R(x i , x j ) π(x i )π(x j )<label>(8)</label></formula><p>where π(x i ) is the prior for the data x i and R(X) denotes the correlation of these data. These kernels would always induce repulsion between different points and thus a diverse set of points tends to have higher probability. Generally, the vectors are supposed to be uniformly distributed variables.</p><p>Therefore, the prior π(x i ) is a constant value, and then, the kernel φ(x i , x j ) = R(x i , x j ).</p><p>The DPPs provide a probability measure over every configuration of subsets on data points. Based on a similarity matrix over the data and a determinant operator, the DPP assigns higher probabilities to those subsets with dissimilar items. Therefore, it can give lower probabilities to mini-batches which contain the redundant data, and higher probabilities to mini-batches with more diverse data <ref type="bibr" target="#b8">[9]</ref>. This simultaneously balances the data and generates the stochastic gradients with lower variance. Moreover, <ref type="bibr" target="#b9">[10]</ref> further regularizes the DPP (R-DPP) with an arbitrary fixed positive semi-definite matrix inside of the determinant to accelerate the training process.</p><p>Besides, <ref type="bibr" target="#b11">[12]</ref> generalizes the diversification of the minibatch sampling to arbitrary repulsive point processes, such as the Stationary Poisson Disk Sampling (PDS). The PDS is one type of repulsive point process. It can provide point arrangements similar to DPP but with much more efficiency. The PDS indicates that the smallest distance between each pair of sample points should be at least r with respect to some distance measurement D(x i , x j ) <ref type="bibr" target="#b11">[12]</ref>, such as the Euclidean distance and the heat kernel. The measurement can be formulated as Euclidean distance:</p><formula xml:id="formula_13">D(x i , x j ) = x i -x j 2<label>(10)</label></formula><p>Heat kernel:</p><formula xml:id="formula_14">D(x i , x j ) = e x i -x j 2 σ (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>where σ is a positive value. Given a new mini-batch B, and the algorithm of PDS can work as follows in each iteration.</p><p>• Randomly select a data point x new .</p><p>• If D(x new , x i ) ≤ r(∀x i ∈ B), throw out the point; otherwise add x new in batch B. The computational complexity of PDS is much lower than that of the DPP.</p><p>Under these diversification prior, such as the DPP and the PDS, each mini-batch consists of the training samples with more diversity and information, which can train the model more effectively, and thus the learned model can exact more discriminative features from the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DIVERSIFICATION IN ACTIVE LEARNING</head><p>As section II-B shows, active learning can obtain a good performance with less labelled training samples. However, some selected samples with CTED are similar to each other and contain the overlapping and redundant information. The highly similar samples make the redundancy of the training samples, and this further decreases the training efficiency, which requires more training samples for a comparable performance.</p><p>To select more informative and complement samples with the active learning method, some prior works introduce the diversity in the selected samples obtained from CTED (Eq.</p><p>3) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. To promote diversity between the selected samples, <ref type="bibr" target="#b13">[14]</ref> enhances CTED with a diversity regularizer</p><formula xml:id="formula_16">min A,b U -U A 2 F + N2 i=1 N2 1 a 2 ij b i + α b 1 + γb T Sb s.t. b i ≥ 0, i = 1, 2, • • • , N 2 (12) where A = [a 1 , • • • , a N2 ],</formula><p>• represents the F-norm, and the similarity matrix S ∈ R N2×N2 is used to model the pairwise similarities among all the samples, such that larger value of s ij demonstrates the higher similarity between the i-th sample and the j-th one. Particularly, <ref type="bibr" target="#b13">[14]</ref> chooses the cosine similarity measurement to formulate the diversity term. And the diversity term can be formulated as</p><formula xml:id="formula_17">s ij = a i (a j ) T a i a j .<label>(13)</label></formula><p>As <ref type="bibr" target="#b21">[22]</ref> introduces, s ij tends to be zero when a i and a j tends to be uncorrelated.</p><p>Similarly, <ref type="bibr" target="#b12">[13]</ref> denotes the diversity term in active learning with the angular of the cosine similarity to obtain a diverse set of training samples. The diversity term can be formulated as</p><formula xml:id="formula_18">s ij = π 2 -arccos( a i (a j ) T a i a j ).<label>(14)</label></formula><p>Obviously speaking, when the two vectors become vertical, the vectors tend to be uncorrelated. Therefore, under the diversification, the selected samples would be more informative.</p><p>Besides, <ref type="bibr" target="#b14">[15]</ref> takes advantage of the well-known RBF kernel to measure the diversity of the selected samples, the diversity term can be calculated by</p><formula xml:id="formula_19">s ij = a i -a j 2 σ 2<label>(15)</label></formula><p>where σ is a positive value. Different from Eqs. 13 and Eq. 14 which measure the diversity from the angular view, Eq. 15 calculates the diversity from the distance view. Generally, given two data, if they are similar to each other, the term will have a large value. Through adding diversity regularization over the selected samples by active learning, samples with more information and less redundancy would be chosen for labelling and then used for training. Therefore, the machine learning process can obtain comparable or even better performance with limited training samples than that with plentiful training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DIVERSIFICATION IN UNSUPERVISED LEARNING</head><p>As subsection II-C shows, the unsupervised learning in <ref type="bibr" target="#b37">[38]</ref> is based on the construction of the pseudo classes with the center points. By repulsing the center points from each other, the pseudo classes would be further enforced to be away from one another. If we encourage the center points to be diversified and repulse from each other, the learned features from different classes can be more discriminative.</p><p>Generally, the Euclidean distance can be used to calculate the diversification of the center points. The pseudo label of x i is also calculated by Eq. 4. Then, the unsupervised learning method with the diversity-promoting prior can be formulated as</p><formula xml:id="formula_20">max W,ci L(W |U, z i )+ηg(W )+ N2 k=1 c z k -u k +γ j =k c j -c k (16)</formula><p>where γ is a positive value which denotes the tradeoff between the optimization term and the diversity term. Under the diversification term, in the training process, the center points would be encouraged to repulse from each other. This makes the unsupervised learning process be more effective to obtain discriminative features from samples in different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODEL DIVERSIFICATION</head><p>In addition to the data diversification to improve the performance with more informative and less redundant samples, we can also diversify the model to improve the representational ability of the model directly. As introduction shows, the machine learning methods aim to learn parameters by the machine itself with the training samples. However, due to the limited and imbalanced training samples, highly similar parameters would be learned by general machine learning process. This would lead to the redundancy of the learned model and negatively affect the model's representational ability.</p><p>Therefore, in addition to the data diversification, one can also diversify the learned parameters in the training process and further improve the representational ability of the model (D-model). Under the diversification prior, each parameter factor can model unique information and the whole factors model a larger proportional of information <ref type="bibr" target="#b21">[22]</ref>. Another method is to obtain diversified multiple models (D-models) through machine learning. Traditionally, if we train the multiple models separately, the obtained representations from different models would be similar and this would lead to the redundancy between different representations. Through regularizing the multiple base models with the diversification prior, different models would be enforced to repulse from each other and each base model can provide choices reflecting multi-modal belief <ref type="bibr" target="#b26">[27]</ref>. In the following subsections, we'll introduce the diversity methods for D-model and Dmodels in detail separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. D-MODEL</head><p>The first method tries to diversify the parameters of the model in the training process to directly improve the representational ability of the model. Fig. <ref type="figure" target="#fig_3">5</ref> shows the effects of D-model on improving the performance of the machine learning model. As Fig. <ref type="figure" target="#fig_3">5</ref> shows, under the D-model, each factor would model unique information and the whole factors model a larger proportional of information and then the information will be further improved. Traditionally, Bayesian method and posterior regularization method can be used to</p><p>VOLUME 4, 2016 Model Diversification Machine learning model Parameters of the machine learning model Feature space of the data Model Diversification impose diversity over the parameters of the model. Different diversity-promoting priors have been developed in prior works to measure the diversity between the learned parameter factors according to the special requirements of different tasks. This subsection will mainly introduce the methods which can enforce the diversity of the model and summarize these methods occurred in prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Bayesian Method</head><p>Traditionally, diversity-promoting priors can be used to measure the diversification of the model. The parameters of the model can be calculated by the Bayesian method as</p><formula xml:id="formula_21">W ∝ P (W |X) = P (X|W ) × P (W )<label>(17)</label></formula><p>where</p><formula xml:id="formula_22">W = [w 1 , w 2 , • • • , w K ]</formula><p>denotes the parameters in the machine learning model, K is the number of the parameters, P (X|W ) represents the likelihood of the training set on the constructed model and P (W ) stands for the prior knowledge of the learned model. For the machine learning task at hand, P (W ) describes the diversity-promoting prior. Then, the machine learning task can be written as</p><formula xml:id="formula_23">W * = arg max W P (W |X) = arg max W P (X|W ) × P (W )<label>(18)</label></formula><p>The log-likelihood of the optimization can be formulated as</p><formula xml:id="formula_24">W * = arg max W (log P (X|W ) + log P (W ))<label>(19)</label></formula><p>Then, Eq. 19 can be written as the following optimization</p><formula xml:id="formula_25">max W log P (X|W ) + log P (W )<label>(20)</label></formula><p>where log P (X|W ) represents the optimization objective of the model, which can be formulated as L 0 in subsection II-A.</p><p>TABLE 1: Overview of most frequently used diversification method in D-model and the papers in which example measurements can be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measurements Papers</head><p>Cosine Similarity <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b123">[125]</ref>, <ref type="bibr" target="#b132">[134]</ref>- <ref type="bibr" target="#b135">[137]</ref> Determinantal Point Process <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b85">[87]</ref>, <ref type="bibr" target="#b90">[92]</ref>, <ref type="bibr" target="#b112">[114]</ref>, <ref type="bibr" target="#b119">[121]</ref>- <ref type="bibr" target="#b121">[123]</ref>, <ref type="bibr" target="#b125">[127]</ref>, <ref type="bibr" target="#b127">[129]</ref>-[132], [145]-[149] Submodular Spectral Diversity [54] Inner Product [43], [51] Euclidean Distance [40]-[42] Heat Kernel [2], [3], [143] Divergence [40] Uncorrelation and Evenness [55] L 2,1</p><p>[56]- <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b126">[128]</ref>, <ref type="bibr" target="#b131">[133]</ref>, <ref type="bibr" target="#b137">[139]</ref>- <ref type="bibr" target="#b140">[142]</ref> the diversity-promoting prior log P (W ) aims to encourage the learned factors to be diversified. With Eq. 20, the diversity prior can be imposed over the parameters of the learned model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Posterior Regularization Method</head><p>In addition to the former Bayesian method, posterior regularization methods can be also used to impose the diversity property over the learned model <ref type="bibr" target="#b136">[138]</ref>. Generally, the regularization method can add side information into the parameter estimation and thus it can encourage the learned factors to possess a specific property. We can also use the posterior regularization to enforce the learned model to be diversified. The diversity regularized optimization problem can be formulated as max</p><formula xml:id="formula_26">W L 0 + γf (W )<label>(21)</label></formula><p>where f (W ) stands for the diversity regularization which measures the diversity of the factors in the learned model. L 0 represents the optimization term of the model which can be seen in subsection II-A. γ demonstrates the tradeoff between the optimization and the diversification term. From Eqs. 20 and 21, we can find that the posterior regularization has the similar form as the Bayesian method. In general, the optimization (20) can be transformed into the form <ref type="bibr" target="#b20">(21)</ref>. Many methods can be applied to measure the diversity property of the learned parameters. In the following, we will introduce different diversity priors to realize the Dmodel in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Diversity Regularization</head><p>As Fig. <ref type="figure" target="#fig_3">5</ref> shows, the diversity regularization encourages the factors to repulse from each other or to be uncorrelated. The key problem with the diversity regularization is the way to calculate the diversification of the factors in the model. Prior works mainly impose the diversity property into the machine learning process from six aspects, namely the distance, the angular, the eigenvalue, the divergence, the L 2,1 , and the DPP. The following will introduce the measurements and further discuss the advantages and disadvantages of these measurements.</p><p>Distance-based measurements. The simplest way to formulate the diversity between different factors is the Euclidean distance. Generally, enlarging the distances between different factors can decrease the similarity between these factors. Therefore, the redundancy between the factors can be decreased and the factors can be diversified. <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref> have applied the Euclidean distance as the measurements to encourage the latent factors in machine learning to be diversified.</p><p>In general, the larger of the Euclidean distance two vectors have, the more difference the vectors are. Therefore, we can diversify different vectors through enlarging the pairwise Euclidean distances between these vectors. Then, the diversity regularization by Euclidean distance from Eq. 21 can be formulated as</p><formula xml:id="formula_27">f (W ) = K i =j w i -w j 2 (<label>22</label></formula><formula xml:id="formula_28">)</formula><p>where K is the number of the factors which we intend to diversify in the machine learning model. Since the Euclidean distance uses the distance between different factors to measure the similarity of these factors , generally the regularizer in Eq. 22 is variant to scale due to the characteristics of the distance. This may decrease the effectiveness of the diversity measurement and cannot fit for some special models with large scale range.</p><p>Another commonly used distance-based method to encourage diversity in the machine learning is the heat kernel <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b141">[143]</ref>. The correlation between different factors is formulated through Gaussian function and it can be calculated as</p><formula xml:id="formula_29">f (w i , w j ) = -e -w i -w j 2 σ (<label>23</label></formula><formula xml:id="formula_30">)</formula><p>where σ is a positive value. The term measures the correlation between different factors and we can find that when w i and w j are dissimilar, f (w i , w j ) tends to zero. Then, the diversity-promoting prior by the heat kernel from Eq. 20 can be formulated as</p><formula xml:id="formula_31">P (W ) = e -γ K i =j e - w i -w j 2 σ (<label>24</label></formula><formula xml:id="formula_32">)</formula><p>The corresponding diversity regularization form can be formulated as</p><formula xml:id="formula_33">f (W ) = - K i =j e -w i -w j 2 σ (<label>25</label></formula><formula xml:id="formula_34">)</formula><p>where σ is a positive value. Heat kernel takes advantage of the distance between the factors to encourage the diversity of the model. It can be noted that the heat kernel has the form of Gaussian function and the weight of the diversity penalization is affected by the distance. Thus, the heat kernel presents more variance with the penalization and shows better performance than general Euclidean distance.</p><p>All the former distance-based methods encourage the diversity of the model by enforcing the factors away from each other and thus these factors would show more difference. However, it should be noted that the distance-based measurements can be significantly affected by scaling which can limit the performance of the diversity prior over the machine learning.</p><p>Angular-based measurements. To make the diversity measurement be invariant to scale, some works take advantage of the angular to encourage the diversity of the model. Among these works, the cosine similarity measurement is the most commonly used <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Obviously, the cosine similarity can measure the similarity between different vectors. In machine learning tasks, it can be used to measure the redundancy between different latent parameter factors <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. The aim of cosine similarity prior is to encourage different latent factors to be uncorrelated, such that each factor in the learned model can model unique features from the samples.</p><p>The cosine similarity between different factors w i and w j can be calculated as <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> </p><formula xml:id="formula_35">c ij = &lt; w i , w j &gt; w i w j , i = j, 1 ≤ i, j = K<label>(26)</label></formula><p>Then, the diversity-promoting prior of generalized cosine similarity measurement from Eq. 20 can be written as</p><formula xml:id="formula_36">P (W ) ∝ e -γ( i =j c p ij ) 1 p<label>(27)</label></formula><p>It should be noted that when p is set to 1, the diversitypromoting prior over different vectors w i (i = 1, 2, • • • , K) by cosine similarity from Eq. 20 can be formulated as</p><formula xml:id="formula_37">P (W ) ∝ e -γ i =j cij (<label>28</label></formula><formula xml:id="formula_38">)</formula><p>where γ is a positive value. It can be noted that under the diversity-promoting prior in Eq. 28, the c ij is encouraged to be 0. Then, w i and w j tend to be orthogonal and different factors are encouraged to be uncorrelated and diversified. Besides, the diversity regularization form by the cosine similarity measurement from Eq. 21 can be formulated as</p><formula xml:id="formula_39">f (W ) = - K i =j &lt; w i , w j &gt; w i w j<label>(29)</label></formula><p>However, there exist some defects in the former measurement where the measurement is variant to orientation. To overcome this problem, many works use the angular of cosine similarity to measure the diversity between different factors <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b46">[47]</ref>.</p><p>Since the angular between different factors is invariant to translation, rotation, orientation and scale, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b46">[47]</ref> develops the angular-based diversifying method for Restricted Boltzmann Machine (RBM). These works use the variance and mean value of the angular between different factors to formulate the diversity of the model to overcome the problem occurred in cosine similarity. The angular between different factors can be formulated as</p><formula xml:id="formula_40">Γ ij = arccos &lt; w i , w j &gt; w i w j (30)</formula><p>Since we do not care about the orientation of the vectors just as <ref type="bibr" target="#b20">[21]</ref>, we prefer the angular to be acute or right. From the mathematical view, two factors would tend to be uncorrelated when the angular between the factors enlarges. Then, the diversity function can be defined as <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b47">[48]</ref>- <ref type="bibr" target="#b49">[50]</ref> f</p><formula xml:id="formula_41">(W ) = Ψ(W ) -Π(W )<label>(31)</label></formula><p>where</p><formula xml:id="formula_42">Ψ(W ) = 1 K 2 i =j Γ ij , Π(W ) = 1 K 2 i =j (Γ ij -Ψ(W )) 2 .</formula><p>In other words, Ψ(W ) denotes the mean of the angular between different factors and Π(W ) represents the variance of the angular. Generally, a larger f (W ) indicates that the weight vectors in W are more diverse. Then, the diversity promoting prior by the angular of cosine similarity measurement can be formulated as</p><formula xml:id="formula_43">P (W ) ∝ e γf (W )<label>(32)</label></formula><p>The prior in Eq. 32 encourages the angular between different factors to approach π 2 , and thus these factors are enforced to be diversified under the diversification prior. Moreover, the measurement is invariant to scale, translation, rotation, and orientation.</p><p>Another form of the angular-based measurements is to calculate the diversity with the inner product <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Different vectors present more diversity when they tend to be more orthogonal. The inner product can measure the orthogonality between different vectors and therefore it can be applied in machine learning models for more diversity. The general form of diversity-promoting prior by inner product measurement can be written as <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b50">[51]</ref> </p><formula xml:id="formula_44">P (W ) = e -γ K i =j &lt;wi,wj &gt; . (<label>33</label></formula><formula xml:id="formula_45">)</formula><p>Besides, <ref type="bibr" target="#b62">[63]</ref> uses the special form of the inner product measurement, which is called exclusivity. The exclusivity between two vectors w i and w j is defined as</p><formula xml:id="formula_46">χ(w i , w j ) = w i w j 0 = m k=1 w i (k) • w j (k)<label>(34)</label></formula><p>where denotes the Hadamard product, and • 0 denotes the L 0 norm. Therefore, the diversity-promoting prior can be written as</p><formula xml:id="formula_47">P (W ) = e -γ K i =j wi wj 0<label>(35)</label></formula><p>Due to the non-convexity and discontinuity of L 0 norm, the relaxed exclusivity is calculated as <ref type="bibr" target="#b62">[63]</ref> χ r (w i , w j ) =</p><formula xml:id="formula_48">w i w j 1 = m k=1 |w i (k)| • |w j (k)| (36)</formula><p>where • 1 denotes the L 1 norm. Then, the diversitypromoting prior based on relaxed exclusivity can be calculated as</p><formula xml:id="formula_49">P (W ) = e -γ K i =j wi wj 1<label>(37)</label></formula><p>The inner product measurement takes advantage of the characteristics among the vectors and tries to encourage different factors to be orthogonal to enforce the learned factors to be diversified. It should be noted that the measurement can be seen as a special form of cosine similarity measurement.</p><p>Even though the inner product measurement is variant to scale and orientation, in many real-world applications, it is usually considered first to diversify the model since it is easier to implement than other measurements. Instead of the distance-based and angular-based measurements, the eigenvalues of the kernel matrix can also be used to encourage different factors to be orthogonal and diversified. Recall that, for an orthogonal matrix, all the eigenvalues of the kernel matrix are equal to 1. Here, we denote κ(W ) = W W T as the kernel matrix of W . Therefore, when we constrain the eigenvalues to 1, the obtained vectors would tend to be orthogonal <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Three ways are generally used to encourage the eigenvalues to approach constant 1, including the submodular spectral diversity (SSD) measurement, the uncorrelation and evenness measurement, and the log-determinant divergence (LDD). In the following, the two form of the eigenvalue-based measurements will be introduced in detail.</p><p>Eigenvalue-based measurements. As the former denotes, κ(W ) = W W T stands for the kernel matrix of the latent factors. Two commonly used methods to promote diversity in the machine learning process based on the kernel matrix would be introduced. The first method is the submodular spectral diversity (SSD), which is based on the eigenvalues of the kernel matrix. <ref type="bibr" target="#b53">[54]</ref> introduces the SSD measurement in the process of feature selection, which aims to select a diverse set of features. Feature selection is a key component in many machine learning settings. The process involves choosing a small subset of features in order to build a model to approximate the target concept well.</p><p>The SSD measurement uses the square distance to encourage the eigenvalues to approach 1 directly. Define (λ 1 , λ 2 , • • • , λ K ) as the eigenvalues of the kernel matrix. Then, the diversity-promoting prior by SSD from Eq. 20 can be formulated as <ref type="bibr" target="#b53">[54]</ref> </p><formula xml:id="formula_50">P (W ) = e -γ K i=1 (λi(κ(W ))-1) 2 (38)</formula><p>where γ is also a positive value. From Eq. 21, the diversity regularization f (W ) can be formulated as</p><formula xml:id="formula_51">f (W ) = - K i=1 (λ i (κ(W )) -1) 2<label>(39)</label></formula><p>This measurement regularizes the variance of the eigenvalues of the matrix. Since all the eigenvalues are enforced to approach 1, the obtained factors tend to be more orthogonal and thus the model can present more diversity.</p><p>Another diversity measurement based on the kernel matrix is the uncorrelation and evenness <ref type="bibr" target="#b54">[55]</ref>. This measurement encourages the learned factors to be uncorrelated and to play equally important roles in modeling data. Formally, this amounts to encouraging the kernel matrix of the vectors to have more uniform eigenvalues. The basic idea is to normalize the eigenvalues into a probability simplex and encourage the discrete distribution parameterized by the normalized eigenvalues to have small Kullback-Leibler (KL) divergence with the uniform distribution <ref type="bibr" target="#b54">[55]</ref>. Then, the diversity-promoting prior by uniform eigenvalues from Eq. 20 is formulated as</p><formula xml:id="formula_52">P (W ) = e -γ( tr(( 1 d κ(W )) log( 1 d κ(W ))) tr( 1 d κ(W )) -log tr( 1 d κ(W )))<label>(40)</label></formula><p>subject to κ(W ) 0 ( κ(W ) is positive definite matrix) and W 1 = 0, where κ(W ) is the kernel matrix. Besides, the diversity-promoting uniform eigenvalue regularizer (UER) from Eq. 21 is formulated as</p><formula xml:id="formula_53">f (W ) = -[ tr(( 1 d κ(W )) log( 1 d κ(W ))) tr( 1 d κ(W )) -log tr( 1 d κ(W ))]<label>(41)</label></formula><p>where d is the dimension of each factor.</p><p>Besides, <ref type="bibr" target="#b52">[53]</ref> takes advantage of the log-determinant divergence (LDD) to measure the similarity between different factors. The diversity-promoting prior in <ref type="bibr" target="#b52">[53]</ref> combines the orthogonality-promoting LDD regularizer with the sparsitypromoting L 1 regularizer. Then, the diversity-promoting prior from Eq. 20 can be formulated as</p><formula xml:id="formula_54">P (W ) = e -γ(tr(κ(W ))-log det(κ(W ))+τ |W |1)<label>(42)</label></formula><p>where tr(•) denotes the matrix trace. Then, the corresponding regularizer from Eq. 21 is formulated as</p><formula xml:id="formula_55">f (W ) = -(tr(κ(W )) -log det(κ(W )) + τ |W | 1 )). (<label>43</label></formula><formula xml:id="formula_56">)</formula><p>The LDD-based regularizer can effectively promote nonoverlap <ref type="bibr" target="#b52">[53]</ref>. Under the regularizer, the factors would be sparse and orthogonal simultaneously. These eigenvalue-based measurements calculate the diversity of the factors from the kernel matrix view. They not only consider the pairwise correlation between the factors, but also take the multiple correlation into consideration. Therefore, they generally present better performance than the distance-based and angular-based methods which only consider the pairwise correlation. However, the eigenvaluebased measurements would cost more computational sources in the implementation. Moreover, the gradient of the diversity term which is used for back propagation would be complex to compute and usually requires special processing methods, such as projected gradient descent algorithm <ref type="bibr" target="#b54">[55]</ref> for the uncorrelation and evenness.</p><p>DPP measurement. Instead of the eigenvalue-based measurements, another measurement which takes the multiple correlation into consideration is the determinantal point process (DPP) measurement. As subsection III-A shows, the DPP on the parameter factors W has the form as</p><formula xml:id="formula_57">P (W ) ∝ det(φ(W )).<label>(44)</label></formula><p>Generally, it can encourage the learned factors to repulse from each other. Therefore, the DPP-based diversifying prior can obtain machine learning models with a diverse set of the learned factors other than a redundant one. Some works have shown that the DPP prior is usually not arbitrarily strong for some special case when applied into machine learning models <ref type="bibr" target="#b59">[60]</ref>. To encourage the DPP prior strong enough for all the training data, the DPP prior is augmented by an additional positive parameter γ. Therefore, just as section III-A, the DPP prior can be reformulated as</p><formula xml:id="formula_58">P (W ) ∝ det(φ(W )) γ<label>(45)</label></formula><p>where φ(W ) denotes the kernel matrix and φ(w i , w j ) demonstrates the pairwise correlation between w i and w j . The learned factors are usually normalized, and thus the optimization for machine learning can be written as</p><formula xml:id="formula_59">max W log P (X|W ) + γ log(det(φ(W )))<label>(46)</label></formula><p>where f (W ) = log(det(φ(W ))) represents the diversity term for machine learning. It should be noted that different kernels can be selected according to the special requirements of different machine learning tasks <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. For example, in <ref type="bibr" target="#b61">[62]</ref>, the similarity kernel is adopted for the DPP prior which can be formulated as</p><formula xml:id="formula_60">φ(w i , w j ) = &lt; w i , w j &gt; w i w j . (<label>47</label></formula><formula xml:id="formula_61">)</formula><p>When we set the cosine similarity as the correlation kernel φ, from geometric interpretation, the DPP prior P (W ) ∝ det(φ(W )) can be seen as the volume of the parallelepiped spanned by the columns of W <ref type="bibr" target="#b38">[39]</ref>. Therefore, diverse sets are more probable because their feature vectors are more orthogonal, and hence span larger volumes. It should be noted that most of the diversity measurements consider the pairwise correlation between the factors and ignore the multiple correlation between three or more factors. While the DPP measurement takes advantage of the merits of the DPP to make use of the multiple correlation by calculating the similarity between multiple factors. L 2,1 measurement. While all the former measurements promote the diversity of the model from the pairwise or multiple correlation view, many prior works prefer to use the L 2,1 for diversity since L 2,1 can take advantage of the group-wise correlation and obtain a group-wise sparse representation of the latent factors W [56]- <ref type="bibr" target="#b58">[59]</ref>.</p><p>It is well known that the L 2,1 -norm leads to the group-wise sparse representation of W . L 2,1 can also be used to measure the correlation between different parameter factors and diversify the learned factors to improve the representational ability of the model. Then, the L 2,1 prior from Eq. 20 can be calculated as</p><formula xml:id="formula_62">P (W ) = e -γ K i ( n j |wi(j)|) 2<label>(48)</label></formula><p>where w i (j) means the j-th entry of w i . The internal L 1 norm encourages different factors to be sparse, while the external L 2 norm is used to control the complexity of entire model. Besides, the diversity term based on f (W ) from Eq. 21 can be formulated as</p><formula xml:id="formula_63">f (W ) = - K i ( n j |w i (j)|) 2 (<label>49</label></formula><formula xml:id="formula_64">)</formula><p>where n is the dimension of each factor w i . The internal L 1 -norm encourages different factors to be sparse, while the external L 2 -norm is used to control the complexity of entire model.</p><p>In most of the machine learning models, the parameters of the model can be looked as the vectors and diversity of these factors can be calculated from the mathematical view just as these former measurements. When the norm of the vectors are constrained to constant 1, we can also take these factors as the probability distribution. Then, the diversity between the factors can be also measured from the Bayesian view.</p><p>Divergence measurement. Traditionally, divergence, which is generally used Bayesian method to measure the difference between different distributions, can be used to promote diversity of the learned model <ref type="bibr" target="#b39">[40]</ref>.</p><p>Each factor is processed as a probability distribution firstly. Then, the divergence between factors w i and w j can be calculated as</p><formula xml:id="formula_65">D(w i w j ) = n k=1 (w i (k) log w i (k) w j (k) -w i (k) + w j (k))<label>(50)</label></formula><p>subject to w i = 1.</p><p>The divergence can measure the dissimilarity between the learned factors, such that the diversity-promoting regularization by divergence from Eq. 21 can be formulated as <ref type="bibr" target="#b39">[40]</ref> </p><formula xml:id="formula_66">f (W ) = K i =j D(w i w j ) = K i =j n k=1 (w i (k) log w i (k) w j (k) -w i (k) + w j (k))<label>(51)</label></formula><p>The measurement takes advantage of the characteristics of the divergence to measure the dissimilarity between different distributions. However, the norm of the learned factors need to satisfy w i = 1 which limits the application field of the diversity measurement.</p><p>In conclusion, there are numerous approaches to diversify the learned factors in machine learning models. A summary of the most frequently encountered diversity methods is shown in Table <ref type="table">1</ref>. Although most papers use slightly different specifications for their diversification of the learned model, the fundamental representation of the diversification is similar. It should also be noted that the thing in common among the studied diversity methods is that the diversity enforced in a pairwise form between members strikes a good balance between complexity and effectiveness <ref type="bibr" target="#b62">[63]</ref>. In addition, different applications should choose the proper diversity measurements according to the specific requirements of different machine learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Analysis</head><p>These diversity measurements can calculate the similarity between different vectors and thus encourage the diversity of the machine learning model. However, there exists the difference between these measurements. The details of these diversity measurements can be seen in Table <ref type="table" target="#tab_5">2</ref>. It can be noted from the table that all these methods take advantage of the pairwise correlation except L 2,1 which uses the groupwise correlation between different factors. Moreover, the determinantal point process, submodular spectral diversity, and uncorrelation and evenness can also take advantage of correlation among three or more factors.</p><p>Another property of these diversity measurement is scale invariant. Scale invariant can make the diversity of the model be invariant w.r.t. the norm of these factors. The cosine similarity measurement calculates the diversity via the angular between different vectors. As a special case for DPP, the cosine similarity can be used as the correlation term R(w i , w j ) in DPP and thus the DPP measurement is scale invariant. Besides, for divergence measurement, since the factors are constrained with w i = 1, the measurement is scale invariant.</p><p>These measurements can encourage diversity within different vectors. Generally, the machine learning models can be looked as the set of latent parameter factors, which can be represented as the vectors. These factors can be learned and used to represent the objects. In the following, we'll mainly summarize the methods to diversify the ensemble learning (D-models) for better performance of machine learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. D-MODELS</head><p>The former subsection introduces the way to diversify the parameters in single model and improve the representational ability of the model directly. Much efforts have been done to obtain the highest probability configuration of the machine learning models in prior works. However, even when the training samples are sufficient, the maximum a posteriori (MAP) solution could also be sub-optimal. In many situations, one could benefit from additional representations with multiple models. As Fig. <ref type="figure">4</ref> shows, ensemble learning (the way for training multiple models) has already occurred in many prior works. However, traditional ensemble learning methods to train multiple models may provide representations that tend to be similar while the representations obtained from different models are desired to provide complement information. Recently, many diversifying methods have been proposed to overcome this problem. As Fig. <ref type="figure" target="#fig_4">6</ref> shows, under the model diversification, each base model of the en-  semble can produce different outputs reflecting multi-modal belief. Therefore, the whole performance of the machine learning model can be improved. Especially, the D-models play an important role in structured prediction problems with multiple reasonable interpretations, of which only one is the groundtruth <ref type="bibr" target="#b26">[27]</ref>. Denote W i (i = 1, 2, • • • , s) and P (W i ) as the parameters and the inference from the ith model where s is the number of the parallel base models. Then, the optimization of the machine learning to obtain multiple models can be written as</p><formula xml:id="formula_67">max W1,W2,••• ,Ws s i=1 L(W i |X i )<label>(52)</label></formula><p>where L(W i |X i ) represents the optimization term of the ith model and X i denotes the training samples of the ith model. Traditionally, the training samples are randomly divided into multiple subsets and each subset trains a corresponding model. However, selecting subsets randomly may lead to the redundancy between different representations. Therefore, the first way to obtain multiple diversified models is to diversify these training samples over different base models, which we call sample-based methods.</p><p>Another way to encourage the diversification between different models is to measure the similarity between different base models with a special similarity measurement and encourage different base models to be diversified in the training process, which is summarized as the optimizationbased methods. The optimization of these methods can be written as</p><formula xml:id="formula_68">max W1,W2,••• ,Ws s i=1 L(W i |X) + γΓ(W 1 , W 2 , • • • , W s ) (53)</formula><p>where Γ(W 1 , W 2 , • • • , W s ) measures the diversification between different base models. These methods are similar to the methods for D-model in former subsection.</p><p>Finally, some other methods try to obtain large amounts of models and select the top-L models as the final ensemble, which is called the ranking-based methods in this work. In the following, we'll summarize different methods for diversifying multiple models from the three aspects in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Optimization-Based Methods</head><p>Optimization-based methods are one of the most commonly used methods to diversify multiple models. These methods try to obtain multiple diversified models by optimizing a given objective function as Eq. 53 shows, which includes VOLUME 4, 2016 a diversity measurement. Just as the diversity of D-model in prior subsection, the main problem of these methods is to define diversity measurements which can calculate the difference between different models. Many prior works <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b63">[64]</ref>- <ref type="bibr" target="#b67">[68]</ref> have summarized some pairwise diversity measurements, such as Q-statistics measure <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b68">[69]</ref>, correlation coefficient measure <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b68">[69]</ref>, disagreement measure <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b125">[127]</ref>, double-fault measure <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b125">[127]</ref>, k statistic measure <ref type="bibr" target="#b71">[72]</ref>, Kohavi-Wolpert variance <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b125">[127]</ref>, inter-rater agreement <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b125">[127]</ref>, the generalized diversity <ref type="bibr" target="#b64">[65]</ref> and the measure of "Difficult" <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b125">[127]</ref>. Recently, some more measurements have also been developed, including not only the pairwise diversity measurement <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b68">[69]</ref> but also the measurements which calculate the multiple correlation and others <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b72">[73]</ref>- <ref type="bibr" target="#b74">[75]</ref>. This subsection will summarize these methods systematically.</p><p>Bayesian-based measurements. Similar to D-model, Bayesian methods can also be applied in D-models. Among these Bayesian methods, divergence is the most commonly used one. As former subsection shows, the divergence can measure the difference between different distributions. The way to formulate the diversity-promoting term by the divergence method over the ensemble learning is to calculate the divergence between different distributions from the inference of different models, respectively <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b68">[69]</ref>. The diversitypromoting term by divergence from Eq. 53 can be formulated as</p><formula xml:id="formula_69">Γ(W 1 , W 2 , • • • ,W s ) = s i,j n k=1 (P (W i (k)) log P (W i (k)) P (W j (k)) -P (W i (k)) + P (W j (k)))<label>(54)</label></formula><p>where W i (k) represents the k-th entry in W i . P (w i ) denotes the distributions of the inference from the i-th model. The former diversity term can increase the difference between the inference obtained from different models and would encourage the learned multiple models to be diversified.</p><p>In addition to the divergence measurements, Renyientropy which measures the kernelized distances between the images of samples and the center of ensemble in the highdimensional feature space can also be used to encourage the diversity of the learned multiple models <ref type="bibr" target="#b75">[76]</ref>. The Renyientropy is calculated based on the Gaussian kernel function and the diversity-promoting term from Eq. 53 can be formulated as</p><formula xml:id="formula_70">Γ(W 1 , W 2 , • • • , W s ) = -log[ 1 s 2 s i=1 s j=1 G(P (W i ) -P (W j ), 2σ 2 )] (<label>55</label></formula><formula xml:id="formula_71">)</formula><p>where σ is a positive value and G(•) represents the Gaussian kernel function, which can be calculated as</p><formula xml:id="formula_72">G(W i -W j ,2σ 2 ) = 1 (2π) d 2 σ d exp{- (P (W i ) -P (W j )) T (P (W i ) -P (W j )) 2σ 2 }</formula><p>(56) where d denotes the dimension of W i . Compared with the divergence measurement, the Renyi-entropy measurement can be more fit for the machine learning model since the difference can be adapted for different models with different value σ. However, the Renyi-entropy would cost more computational sources and the update of the ensemble would be more complex.</p><p>Another measurement which is based on the Bayesian method is the cross entropy measurement <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b122">[124]</ref>. The cross entropy measurement uses the cross entropy between pairwise distributions to encourage two distributions to be dissimilar and then different base models could provide more complement information. Therefore, the cross-entropy between different base models can be calculated as</p><formula xml:id="formula_73">Γ(w i , w j ) = 1 n n k=1 (P k (w i ) log P k (w j ) + (1 -P k (w i )) log(1 -P k (w j )))<label>(57)</label></formula><p>where P (W i ) is the inference of the i-th model and P k (W i ) is the probability of the sample belonging to the kth class.</p><p>According to the characteristics of the cross entropy and the requirement of the diversity regularization, the diversitypromoting regularization of the cross entropy from Eq. 53 can be formulated as</p><formula xml:id="formula_74">Γ(w 1 , w 2 , • • • , w K ) = 1 n s i,j n k=1 (P k (w i ) log P k (w j ) + (1 -P k (w i )) log(1 -P k (w j )))<label>(58)</label></formula><p>We all know that the larger the cross entropy is, the more difference the distributions are. Therefore, under the cross entropy measurement, different models can be diversified and provide more complement information. Most of the former Bayesian methods promote the diversity in the learned multiple base models by calculating the pairwise difference between these base models. However, these methods ignore the correlation among three or more base models.</p><p>To overcome this problem, <ref type="bibr" target="#b74">[75]</ref> proposes a hierarchical pair competition-based parallel genetic algorithm (HFC-PGA) to increase the diversity among the component neural networks. The HFC-PGA takes advantage of the average of all the distributions from the ensemble to calculate the difference of each base model. The diversity term by HFC-PGA from Eq. 53 can be formulated as</p><formula xml:id="formula_75">Γ(W 1 , W 2 , • • • , W s ) = s j=1 ( 1 s s i=1 P (W i )-P (W j )) 2<label>(59)</label></formula><p>It should be noted that the HFC-PGA takes advantage of multiple correlation between the multiple models. However, the HFC-PGA method uses the fix weight to calculate the mean of the distributions and further calculate the covariance of the multiple models which usually cannot fit for different tasks. This would limit the performance of the diversity promoting prior.</p><p>To deal with the shortcomings of the HFC-PGA, negative correlation learning (NCL) tries to reduce the covariance among all the models while the variance and bias terms are not increased <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b80">[82]</ref>, <ref type="bibr" target="#b81">[83]</ref>. The NCL trains the base models simultaneously in a cooperative manner that decorrelates individual errors. The penalty term can be designed in different ways depending on whether the models are trained sequentially or parallelly. <ref type="bibr" target="#b80">[82]</ref> uses the penalty to decorrelate the current learning model with all previously learned models</p><formula xml:id="formula_76">Γ(W 1 , W 2 , • • • , W s ) = s k=1 (P (W k ) -l) k-1 j=1 (P (W j ) -l) (60)</formula><p>where l represents the target function which is a desired output scalar vector. Besides, define P = s i=1 α i P (W i ) where s i=1 α i = 1. Then, the penalty term can also be defined to reduce the correlation mutually among all the learned models by using the actual distribution P obtained from each model instead of the target function l <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>.</p><formula xml:id="formula_77">Γ(W 1 , W 2 , • • • , W s ) = s k=1 (P (W k ) -P ) k-1 j=1 (P (W j ) -P ) (61)</formula><p>This measurement uses the covariance of the inference results obtained from the multiple models to reduce the correlation mutually among the learned models. Therefore, the learned multiple models can be diversified. In addition, <ref type="bibr" target="#b82">[84]</ref> further combines the NCL with sparsity. The sparsity is purely pursued by the L 1 norm regularization without considering the complementary characteristics of the available base models.</p><p>Most of the Bayesian methods promote diversity in ensemble learning mainly by increasing the difference between the probability distributions of the inference of different base models. There exist other methods which can promote diversity over the parameters of each base model directly.</p><p>Cosine similarity measurement. Different from the Bayesian methods which promote diversity from the distribution view, <ref type="bibr" target="#b65">[66]</ref> introduces the cosine similarity measurements to calculate the difference between different models from geometric view. Generally, the diversity-promoting term from Eq. 53 can be written as</p><formula xml:id="formula_78">Γ(W 1 , W 2 , • • • , W s ) = - s i =j &lt; W i , W j &gt; W i W j .<label>(62)</label></formula><p>In addition, as a special form of angular-based measurement, a special form of inner product measurement, termed as exclusivity, has been proposed by <ref type="bibr" target="#b62">[63]</ref> to obtain diversified models. It can jointly suppress the training error of ensemble and enhance the diversity between bases. The diversitypromoting term by exclusivity (see Eq. 37 for details) from Eq. 53 can be written as</p><formula xml:id="formula_79">Γ(W 1 , W 2 , • • • , W s ) = - s i =j W i W j 1<label>(63)</label></formula><p>These measurements try to encourage the pairwise models to be uncorrelated such that each base model can provide more complement information. L 2,1 measurement. Just as the former subsection, L 2,1 norm can also be used as the diversification of multiple models <ref type="bibr" target="#b57">[58]</ref>. the diversity-promoting regularization by L 2,1 from Eq. 53 can be formulated as</p><formula xml:id="formula_80">Γ(W 1 , W 2 , • • • , W s ) = - s i ( K j |W i (j)|) 2<label>(64)</label></formula><p>The L 2,1 measurement uses the group-wise correlation between different base models and favors selecting diverse models residing in more groups. Some other diversity measurements have been proposed for deep ensemble. <ref type="bibr" target="#b83">[85]</ref> reveals that it may be better to ensemble many instead of all of the neural networks at hand. The paper develops an approach named Genetic Algorithm based Selective Ensemble (GASEN) to obtain different weights of each neural network. Then, based on the obtained weights, the deep ensemble can be formulated. Moreover, <ref type="bibr" target="#b84">[86]</ref> also encourages the diversity of the deep ensemble by defining a pair-wise similarity between different terms.</p><p>These optimization-based methods utilize the correlation between different models and try to repulse these models from one another. The aim is to enforce these representations which are obtained from different models to be diversified and thus these base models can provide outputs reflecting multi-modal belief.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Sample-Based Methods</head><p>In addition to diversify the ensemble learning from the optimization view, we can also diversify the models from the sample view. Generally, we randomly divide the training set into multiple subsets where each base model corresponds to a specific subset which is used as the training samples. However, there exists the overlapping between the representations of different base models. This may cause the redundancy and even decrease the performance of the ensemble learning due to the reduction of the training samples over each model by the division of the whole training set.</p><p>To overcome this problem and provide more complement information from different models, <ref type="bibr" target="#b26">[27]</ref> develops a novel method by dividing the training samples into multiple subsets by assigning the different training samples into the specified subset where the corresponding learned model shows the lowest predict error. Therefore, each base model would focus on modeling the features from specific classes. Besides, clustering is another popular method to divide the training samples for different models <ref type="bibr" target="#b85">[87]</ref>. Although diversifying <ref type="bibr">VOLUME 4, 2016</ref> the obtained multiple subsets can make the multiple models provide more complement information, the less of training samples by dividing the whole training set will show negative effects over the performance.</p><p>To overcome this problem, another way to enforce different models to be diversified is to assign each sample with a specified weight <ref type="bibr" target="#b86">[88]</ref>. By training different base models with different weights of samples, each base model can focus on complement information from the samples. The detailed steps in <ref type="bibr" target="#b86">[88]</ref> are as follows:</p><p>• Define the weights over each training sample randomly, and train the model with the given weights; • Revise the weights over each training sample based on the final loss from the obtained model, and train the second model with the updated weights; • Train M models with the aforementioned strategies. The former methods take advantage of the labelled training samples to enforce the diversity of multiple models. There exists another method, namely Unlabeled Data to Enhance Ensemble (UDEED) <ref type="bibr" target="#b87">[89]</ref>, which focuses on the unlabelled samples to promote diversity of the model. Unlike the existing semi-supervised ensemble methods where error-prone pseudo-labels are estimated for unlabelled data to enlarge the labelled data to improve accuracy. UDEED works by maximizing accuracies of base models on labelled data while maximizing diversity among them on unlabelled data. Besides, <ref type="bibr" target="#b88">[90]</ref> combines the different initializations, different training sets and different feature subsets to encourage the diversity of the multiple models.</p><p>The methods in this subsection process on the training sets to diversify different models. By training different models with different training samples or samples with different weights, these models would provide different information and thus the whole models could provide a larger proportional of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Ranking-Based Methods</head><p>Another kind of methods to promote diversity in the obtained multiple models is ranking-based methods. All the models is first ranked according to some criterion, and then the top-L are selected to form the final ensemble. Here, <ref type="bibr" target="#b89">[91]</ref> focuses on pruning techniques based on forward/backward selection, since they allow a direct comparison with the simple estimation of accuracy from different models.</p><p>Cluster can be also used as ranking-based method to enforce diversity of the multiple models <ref type="bibr" target="#b90">[92]</ref>. In <ref type="bibr" target="#b90">[92]</ref>, each model is first clustered based on the similarity of their predictions, and then each cluster is then pruned to remove redundant models, and finally the remaining models in each cluster are finally combined as the base models.</p><p>In addition to the former mentioned methods, <ref type="bibr" target="#b22">[23]</ref> provides multiple diversified models by selecting different sets of multiple features. Through multi-scale or other tricks, each sample will provide large amounts of features, and then choose top-L multiple features from the all the features as the base features (see <ref type="bibr" target="#b22">[23]</ref> for details). Then, each base feature from the samples is used to train a specific model, and the final inference can be obtained through the combination of these models.</p><p>In summary, this paper summarizes the diversification methods for D-models from three aspects: optimizationbased methods, sample-based methods, and ranking-based methods. The details of the most frequently encountered diversity methods is shown in Table <ref type="table" target="#tab_6">3</ref>. Optimization-based methods encourage the multiple models to be diversified by imposing diversity regularization between different base models while optimizing these models. In contrary, samplebased methods mainly obtain diversified models by training different models with specific training sets. Most of the prior works focus on diversifying the ensemble learning from the two aspects. While the ranking-based methods try to obtain the multiple diversified models by choosing the top-L models. The researchers can choose the specific method for D-models based on the special requirements of the machine learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. INFERENCE DIVERSIFICATION</head><p>The former section summarizes the methods to diversify different parameters in the model or multiple base models. The D-model focuses on the diversification of parameters in the model and improves the representational ability of the model itself while D-models tries to obtain multiple diversified base models, each of which focus on modeling different features from the samples. These works improve the performance of the machine learning process in the modeling stage (see Fig. <ref type="figure">2</ref> for details). In addition to these methods, there exist some other works focusing on obtaining multiple choices in the inference of the machine learning model. This section will summarize these diversifying methods in the inference stage. To introduce the methods for inference diversification in detail, we choose the graph model as the representation of the machine learning models.</p><p>We consider a set of discrete random variables</p><formula xml:id="formula_81">X = {x i |i ∈ {1, 2, • • • , N }}, each taking value y i in a finite label set L v . Let G = (V, E)(V = {1, 2, • • • , N }, E = V 2</formula><p>) describe a graph defined over these variable. The set L χ = v∈χ L v denotes a Cartesian product of sets of labels corresponding to the subset χ ∈ V of variables. Besides, denote θ A : X A → R, (∀A ∈ V ∪ E) as the functions which define the energy at each node and edge for the labelling of variables in scope. The goal of the MAP inference is to find the labelling y = {y 1 , y 2 , • • • , y N } of the variables that minimizes this real-valued energy function:</p><formula xml:id="formula_82">y * = arg min y E(y) = arg min y A∈V ∪E θ A (y)<label>(65)</label></formula><p>However, y * usually converges to the sub-optimal results due to the limited representational ability of the model and the limited training samples. Therefore, multiple choices, which can provide complement information, are desired from the model for the specialist. Traditional methods to obtain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine learning model Inference Diversification</head><p>Diverse Choices FIGURE 7: Effects of inference diversification for improving the performance of the machine learning model. The results come from prior work <ref type="bibr" target="#b30">[31]</ref>. Through inference diversification, multiple diversified choices can be obtained. Then, under the help of other methods, such as re-ranking in <ref type="bibr" target="#b30">[31]</ref>, the final solution can be obtained. multiple choices y 1 , y 2 , • • • , y M try to solve the following optimization:</p><formula xml:id="formula_83">y m = arg min y E(y) = arg min y A∈V E θ A (y) s.t. y m = y i , i = 1, 2, • • • , m -1<label>(66)</label></formula><p>However, the obtained second-best choice will typically be one-pixel shifted versions of the best <ref type="bibr" target="#b27">[28]</ref>. In other words, the next best choices will almost certainly be located on the upper slope of the peak corresponding with the most confident detection, while other peaks may be ignored entirely.</p><p>To overcome this problem, many methods, such as diversified multiple choice learning (D-MCL), submodular, M-Modes, M-NMS, have been developed for inference diversification in prior works. These methods try to diversify the obtained choices (do not overlap under a user-defined criteria) while obtaining high score on the optimization term. Fig. <ref type="figure">7</ref> shows some results of image segmentation from <ref type="bibr" target="#b30">[31]</ref>. Under the inference diversification, we can obtain multiple diversified choices, which represent the different optima of the data. Actually, there also exist many methods which focus on providing multiple diversified choices in the inference phase. In this work, we summarize the diversification in these works as inference diversification. The following subsections will introduce these works in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DIVERSITY-PROMOTING MULTIPLE CHOICE LEARNING (D-MCL)</head><p>The D-MCL tries to find a diverse set of highly probable solutions under a discrete probabilistic model. Given a dissimilarity function measuring similarity between the pairwise choices, our formulation involves maximizing a linear combination of the probability and the dissimilarity to the previous choices. Even if the MAP solution alone is of poor TABLE 4: Overview of most frequently used inference diversification methods and the papers in which example measurements can be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measurements Papers D-MCL [29], [31], [93]-[96], [144] Submodular for Diversification</head><p>[97]- <ref type="bibr" target="#b99">[101]</ref>, <ref type="bibr" target="#b104">[106]</ref>, [107] M-modes</p><p>[30] M-NMS <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b106">[108]</ref>- <ref type="bibr" target="#b109">[111]</ref>, [120] DPP <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b149">[151]</ref>, <ref type="bibr" target="#b152">[154]</ref> quality, a diverse set of highly probable hypotheses might still enable accurate predictions. The goal of D-MCL is to produce a diverse set of low-energy solutions.</p><p>The first method is to approach the problem with a greedy algorithm, where the next choice is defined as the lowest energy state with at least some minimum dissimilarity from the previously chosen choices. To do so, a dissimilarity function ∆(y, y i ) is defined first. In order to find the M diverse, low energy, labellings y 1 , y 2 , • • • , y M , the method proceeds by solving a sequence of problems of the form <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b93">[95]</ref>, <ref type="bibr" target="#b94">[96]</ref>, <ref type="bibr" target="#b142">[144]</ref> </p><formula xml:id="formula_84">y m = arg min y (E(y) -γ m-1 i=1 ∆(y, y i ))<label>(67)</label></formula><p>for m = 1, 2, • • • , M , where γ &gt; 0 determines a trade-off between diversity and energy, y 1 is the MAP-solution and the function ∆ : L V × L V → R defines the diversity of two labels. In other words, ∆(y, y i ) takes a large value if y and y i are diverse, and a small value otherwise. For special case, the M-Best MAP is obtained when ∆ is a 0-1 dissimilarity (i.e. ∆(y, y i ) = I(y = y i )). The method considers the pairwise dissimilarity between the obtained choices. More importantly, it is easy to understand and implement. However, under the greedy strategy, each new labelling is obtained based on the previously found solutions, and ignores the upcoming labellings <ref type="bibr" target="#b92">[94]</ref>. Contrary to the former form, the second method formulate the M -best diverse problem in form of a single energy minimization problem <ref type="bibr" target="#b92">[94]</ref>. Instead of the greedy sequential procedure in <ref type="bibr" target="#b66">(67)</ref>, this method suggests to infer all M labellings jointly, by minimizing</p><formula xml:id="formula_85">E M (y 1 , y 2 , • • • , y M ) = M i=1 E(y i )-γ∆ M (y 1 , y 2 , • • • , y M ) (68)</formula><p>where ∆ M defines the total diversity of any M labellings. To achieve this, let us first create M copies of the initial model. Three specific different diversity measures are introduced. The split-diversity measure is written as the sum of pairwise diversities, i.e. those penalizing pairs of labellings <ref type="bibr" target="#b92">[94]</ref> </p><formula xml:id="formula_86">∆ M (y 1 , y 2 , • • • , y M ) = M i=2 i-1 j=1 ∆(y i , y j )<label>(69)</label></formula><p>The node-diversity measure is defined as <ref type="bibr" target="#b92">[94]</ref> ∆</p><formula xml:id="formula_87">M (y 1 , y 2 , • • • , y M ) = v∈V ∆ v (y 1 v , y 2 v , • • • , y M v ) (70)</formula><p>Finally, the special case of the split-diversity and nodediversity measures is the node-split-diversity measure [94]</p><formula xml:id="formula_88">∆ M (y 1 , y 2 , • • • , y M ) = v∈V M i=2 i-1 j=1 ∆ v (y i v , y j v )<label>(71)</label></formula><p>The D-MCL methods try to find multiple choices with a dissimilarity function. This can help the machine learning model to provide choices with more difference and show more diversity. However, the obtained choices may not be the local extrema and there may exist other choices which could better represent the objects than the obtained ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SUBMODULAR FOR DIVERSIFICATION</head><p>The problem of searching for a diverse but high-quality subset of items in a ground set V of N items has been studied in information retrieval <ref type="bibr" target="#b97">[99]</ref>, web search <ref type="bibr" target="#b96">[98]</ref>, social networks <ref type="bibr" target="#b101">[103]</ref>, sensor placement <ref type="bibr" target="#b102">[104]</ref>, observation selection problem <ref type="bibr" target="#b100">[102]</ref>, set cover problem <ref type="bibr" target="#b103">[105]</ref>, document summarization <ref type="bibr" target="#b98">[100]</ref>, <ref type="bibr" target="#b99">[101]</ref>, and others. In many of these works, an effective, theoretically-grounded and practical tool for measuring the diversity of a set S ⊆ V are submodular set functions. Submodularity is a property that comes from marginal gains. A set function F : 2 V → R is submodular when its marginal gains F (a|S) ≡ F (S ∪ a) -F (S) are decreasing: F (a|S) ≥ F (a|T ) for all S ⊆ T and a / ∈ T . In addition, if F is monotone, i.e. F (S) ≤ F (T ) whenever S ⊆ T , then a simple greedy algorithm that iteratively picks the element with the largest marginal gain F (a|S) to add to the current set S, achieves the best possible approximation bound of</p><formula xml:id="formula_89">(1 - 1 e</formula><p>) <ref type="bibr" target="#b105">[107]</ref>. This result has presented significant practical impact. Unfortunately, if the number N = |V | of items is exponentially large, then even a single linear scan for greedy augmentation is simply infeasible. The diversity is measured by a monotone, nondecreasing and normalized submodular function f :</p><formula xml:id="formula_90">2 V → R + .</formula><p>Denote S as the set of choices. The diversification is measured by a monotone, nondecreasing and normalized submodular function D : 2 V → R + . Then, the problem can be transformed to find a maximizing configurations for the combined score <ref type="bibr" target="#b95">[97]</ref>- <ref type="bibr" target="#b99">[101]</ref> </p><formula xml:id="formula_91">F (S) = E(S) + γD(S)<label>(72)</label></formula><p>The optimization can be solved by the greedy algorithm that starts out with S 0 = ∅, and iteratively adds the best term:</p><formula xml:id="formula_92">y m = arg max y F (y|S m-1 ) = arg max y {E(y) + γD(y|S m-1 )}<label>(73)</label></formula><p>where S m = y m S m-1 . The selected choice S m is within a factor of 1 -1 e of the optimal solution S * :</p><formula xml:id="formula_93">F (S m ) ≥ (1 - 1 e )F (S * ).<label>(74)</label></formula><p>The submodular takes advantage of the maximization of marginal gains to find multiple choices which can provide the maximum of complement information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. M-NMS</head><p>Another way to obtain multiple diversified choices is the nonmaximum suppression (M-NMS) <ref type="bibr" target="#b109">[111]</ref>, <ref type="bibr" target="#b148">[150]</ref>. The M-NMS is typically defined in an algorithmic way: starting from the MAP prediction one goes through all labellings according to an increasing order of the energy. A labelling becomes part of the predicted set if and only if it is more than ρ away from the ones chosen before, where ρ is the threshold defined by user to judge whether two labellings are similar. The M-NMS guarantee the choices to be apart from each other. The M-NMS is typically implemented by greedy algorithm <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b108">[110]</ref>, <ref type="bibr" target="#b109">[111]</ref>, <ref type="bibr" target="#b118">[120]</ref>.</p><p>A simple greedy algorithm for instantiating multiple choices are used: Search over the exponentially large space of choices for the maximally scoring choice, instantiate it, remove all choices with overlapping, and repeat. The process is repeated until the score for the next-best choice is below a threshold or M choices have been instantiated. However, the general implementation of such an algorithm would take exponential time.</p><p>The M-NMS method tries to find M-best choices by throwing away the similar choices from the candidate set. To be concluded, the D-MCL, submodular, and M-NMS have the similar idea. All of them tries to find the M-best choices under a dissimilarity function or the ones which can provide the most complement information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. M-MODES</head><p>Even though the former three methods guarantee the obtained multiple choices to be apart from each other, the choices are typically not local extrema of the probability distribution. To further guarantee both the local extrema and the diversification of the obtained multiple choices simultaneously, the problem can be transformed to the M-modes. The Mmodes have multiple possible applications, because they are intrinsically diverse.</p><p>For a non-negative integer δ, define the δ-neighborhood of a labelling y to be N δ = {y|d(y, y ) ≤ δ} as the set of labellings whose distances from y is no more than δ, where d(•) measures the distance between two labellings, such as the Hamming distance. Then, a labelling y is defined as a local maximum of the energy function E(•), iff E(y) ≥ E(y ), ∀y ∈ N (y).</p><p>Given δ, the set of modes is denoted by M δ , formally, [30]</p><formula xml:id="formula_94">M δ = {y|E(y ) ≥ E(y), ∀y ∈ N δ (y)}<label>(75)</label></formula><p>As δ increases from zero to infinity, the δ-neighborhood of y monotonically grows and the set of modes M δ monotonically decreases. Therefore, the M δ can form a nested sequence, [30]</p><formula xml:id="formula_95">M 0 ⊇ M 1 ⊇ • • • ⊇ M ∞ = {MAP}<label>(76)</label></formula><p>Here, the M-modes can be defined as computing the M labellings with minimal energies in M δ . Then, the problem has been transformed to M-modes: Compute the M labellings with minimal energies in M δ .</p><p>Besides, <ref type="bibr" target="#b29">[30]</ref> has already validated that a labelling is a mode if and only if it behaves like a "local mode" everywhere, and thus a new chain has been constructed and Mmodes problem is reduced into the M best problem of the new chain.</p><p>Furthermore, it also validates the one-to-one cost preserving correspondence between consistent configurations α and the set of modes M δ . Therefore, the problem of computing the M best modes are transferred to the problem of computing the M best configurations in the new chain.</p><p>Different from the former three methods, M-modes can obtain M choices which are the local extrema of the optimization and this can provide M choices which contains the most complement information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. DPP</head><p>General M-NMS and M-Modes try to select choices with the highest scores. Since these methods give a priority to scores of the choices, it might finally end up in selecting the overlapping choices and miss the best possible set of nonoverlapping ones with acceptable scores <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b149">[151]</ref>, <ref type="bibr" target="#b152">[154]</ref>. To address this problem, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b149">[151]</ref>, <ref type="bibr" target="#b152">[154]</ref> attempt to use the DPP to select a set of diverse and informative choices with enriched representations.</p><p>The definition of DPP have been introduced in detail in subsection III-A. It can be noted that the DPP is a distribution over the subsets of a fixed ground set, which prefers a diverse set of points. The selected subset of items by DPP can be representative and cover significant amount of information from the whole set. Besides, the selection would be diverse and non-repetitive <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b152">[154]</ref>. To make the inference diversification, <ref type="bibr" target="#b4">[5]</ref> calculates the probability of inclusion of each choice depends on the determinant of a kernel matrix. The kernel matrix is defined such that it captures all spatial and contextual information between choices all at once. To apply the DPP, the quality and diversity terms need to be defined. The quality term (unary score) defines the optimization term, such as the E(y) in Eq. 66. The diversity term defines the pairwise correlation of the obtained choices.</p><p>Similar to Eq. 68, the model is first transformed into the Mbest diverse problem in form of a single energy minimization problem. The optimization problem based on the DPP can be formulated as</p><formula xml:id="formula_96">E M (y 1 , y 2 , • • • , y M ) = M i=1 E(y i )-γ det(L(y 1 , y 2 , • • • , y M ))<label>(77)</label></formula><p>The kernel matrix in DPP is defined as</p><formula xml:id="formula_97">L(y 1 , y 2 , • • • , y M ) = [L ij ] i,j=1,2,••• ,M<label>(78)</label></formula><p>where L ij = E(y i )E(y j )S ij . S ij is the similarity term to computer the dissimilarity between different choices. A DPP tends to pick uncorrelated choices in these cases. On the other hand, higher quality items increase the determinant, and thus a DPP tends to pick high-quality and diverse set of choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. ANALYSIS</head><p>Even though all the methods in former subsections can be used for inference diversity, there exists some difference between these methods. These methods in prior works are summarized in Table <ref type="table">4</ref>. It can be noted from the former subsections that the D-MCL is the easiest one to implement. One only needs to calculate the MAP choice and obtain other choices by constraining the optimization with a dissimilarity function. In contrary, the M-NMS neglects the choices which are in the neighbors of the former choice and obtain other choices from the remainder. The D-MCL and M-NMS obtain choices by solving the optimization with the user-defined similarity while the submodular method tries to obtain the choices which can provide the maximal marginal and complement information. The former three methods may provide choices which is not local optima while the local optimal choices usually contain more information than others. Therefore, different from the former three methods, the Mmodes tries to obtain multiple diversified choices which are also local optima. All these methods before can be used in traditional machine learning methods. The DPP method for inference diversification which is developed by <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b149">[151]</ref> is mainly applied in object detection tasks. The methods in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b149">[151]</ref> take advantage of the merits of DPP to obtain high quality choices with less overlapping which could present better performance than M-NMS and M-modes. From the introduction of data diversification, D-model, D-models, and inference diversification, one can choose the proper method for diversification of machine learning in various computer vision tasks. In the following, we'll introduce some applications of diversity technology in machine learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPLICATIONS</head><p>Diversity technology in machine learning can significantly improve the representational ability of the model in many computer vision tasks, including the remote sensing imaging tasks <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b110">[112]</ref>, camera relocalization <ref type="bibr" target="#b85">[87]</ref>, <ref type="bibr" target="#b86">[88]</ref>, natural image segmentation <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b93">[95]</ref>, object detection <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b107">[109]</ref>, machine translation <ref type="bibr" target="#b94">[96]</ref>, <ref type="bibr" target="#b111">[113]</ref>, information retrieval <ref type="bibr" target="#b97">[99]</ref>, <ref type="bibr" target="#b112">[114]</ref>, <ref type="bibr" target="#b156">[158]</ref>- <ref type="bibr" target="#b158">[160]</ref>, social network analysis <ref type="bibr" target="#b97">[99]</ref>, <ref type="bibr" target="#b153">[155]</ref>, <ref type="bibr" target="#b155">[157]</ref>, document summarization <ref type="bibr" target="#b98">[100]</ref>, <ref type="bibr" target="#b99">[101]</ref>, <ref type="bibr" target="#b160">[162]</ref>, web search <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b154">[156]</ref>, <ref type="bibr" target="#b162">[164]</ref>, and others. The diversity priors, which can decrease the redundancy in the learned model or diversify the obtained multiple choices, can provide more informative features and show powerful ability in real-world application, especially for the machine learning tasks with limited training samples and complex structures in the training samples. In the following, we'll introduce some of the applications of the diversity technology in machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. REMOTE SENSING IMAGING TASKS</head><p>Remote sensing images, such as the hyperspectral images and the multi-spectral images, have played a more and more important role in the past two decades <ref type="bibr" target="#b113">[115]</ref>. However, there exist some typical difficulties in remote sensing imaging tasks. First, limited number of training samples in remote sensing imaging tasks usually make it difficult to describe the images. Since labelling is usually time-consuming and cost, it usually cannot provide enough training samples to train the model. Besides, remote sensing images usually have large intra-class variance and low inter-class variance, which make it difficult to extract discriminative features from the images. Therefore, proper feature extraction models are required for the representations of the remote sensing images.</p><p>Recently, deep models have demonstrated their impressive performance in extracting features from the remote sensing images <ref type="bibr" target="#b150">[152]</ref>. However, the deep models usually consist of large amounts of parameters while the limited training samples would make the learned deep model be sub-optimal. This would limit the performance of the deep models for the representation of the remote sensing images.</p><p>To overcome these problems, some works have applied the diversity-promoting prior to diversify the model for better performance <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b151">[153]</ref>. <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b19">[20]</ref> attempt to diversify the learned model with the independence prior, which is based on the cosine similarity in the former section for remote sensing images. <ref type="bibr" target="#b21">[22]</ref> develops a special diversitypromoting deep structural metric learning method for scene classification in remote sensing while <ref type="bibr" target="#b19">[20]</ref> imposes the independence prior on deep belief network (DBN) for hyperspectral image classification. If we denote W = [w 1 , w 2 , • • • , w s ] as the metric parameter factors in <ref type="bibr" target="#b21">[22]</ref> and the latent factors of RBM in <ref type="bibr" target="#b19">[20]</ref>, then the diversity term by the independence prior in the two papers can be formulated as</p><formula xml:id="formula_98">f (W ) = K i =j &lt; w i , w j &gt; w i w j (<label>79</label></formula><formula xml:id="formula_99">)</formula><p>where K is the number of the factors. The diversity term f (W ) encourages the factors to be diversified, so as to improve the representational ability of the model for the images. As introduced in subsection IV-A3, to make use of the multiple information, <ref type="bibr" target="#b61">[62]</ref> have applied the DPP prior in the learning process of deep model for hyperspectral image classification. The DPP prior can be formulated as</p><formula xml:id="formula_100">P (W ) = (det(ψ(W ))) γ<label>(80)</label></formula><p>The diversity regularization f (W ) in Eq. 21 can be written as</p><formula xml:id="formula_101">f (W ) = -log[(det(ψ(W )))]<label>(81)</label></formula><p>Besides, <ref type="bibr" target="#b61">[62]</ref> also provides the way to process the diversity regularization by the back propagation,</p><formula xml:id="formula_102">∂f (W ) ∂W = - ∂ log det(ψ(W )) ∂W = -2W (W W T ) -1 . (<label>82</label></formula><formula xml:id="formula_103">)</formula><p>With the developed diversified model for hyperspectral image representation, the classification performance can be significantly improved <ref type="bibr" target="#b61">[62]</ref>. These prior works mainly improve the representational ability of the model for better performance from the D-model view.</p><p>Besides, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b151">[153]</ref> tries to improve the representational ability from the data diversification way. <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b151">[153]</ref> create the pseudo classes with the center points. In <ref type="bibr" target="#b151">[153]</ref>, the pseudo classes are used to decrease the intra-class variance. Furthermore, the diversity-promoting prior, which is created based on the Euclidean distance to repulse different pseudo classes from each other, is used to improve the effectiveness of the developed training process for remote sensing scenes. In <ref type="bibr" target="#b37">[38]</ref>, the pseudo classes are used for unsupervised learning of remote sensing scene representation. The pseudo classes are used to allocate pseudo labels and training the model under the supervised way. Similar to <ref type="bibr" target="#b151">[153]</ref>, the diversity-promoting prior is also used to repulse different pseudo classes from each other to improve the effectiveness of the unsupervised learning process.</p><p>Furthermore, some other works focus on the diversification of multiple models for remote sensing images <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b110">[112]</ref>. <ref type="bibr" target="#b76">[77]</ref> has applied cross entropy measurement to diversify the obtained multiple models and then the obtained multiple models could provide more complement information (See subsection IV-B1 for details). Different from <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b110">[112]</ref> divides the training samples into several subsets for different models, separately. Then, each model focuses on the representation of different classes and the whole representation of these models could be improved (See subsection IV-B2 for details).</p><p>To further describe the effectiveness of the diversitypromoting methods in machine learning, we listed some comparison results between the general model and the diversified model over different datasets in Table <ref type="table" target="#tab_8">5</ref>. The results come from the prior works. This work choose the Ucmerced Land use dataset, Pavia University dataset, and the Indian Pines as representatives.</p><p>Ucmerced Land Use dataset <ref type="bibr" target="#b78">[79]</ref> was manually extracted from orthoimagery. It is multi-class land-use scenes in the visible spectrum which contains 2100 aerial scene images divided into 21 challenging scene categories, including agricultural, airplane, baseball diamond, beach, buildings, chaparral, dense residential, forest, freeway, golf course, harbor, intersection, medium density residential, mobile home park, overpass, parking lot, river, runway, sparse residential, storage tanks, and tennis court. Each scene has 256 × 256 pixels with a resolution of one foot per pixel. For the experiments in Table <ref type="table" target="#tab_8">5</ref>, 80% scenes of each class are used for training and the remainder are for testing.</p><p>20 VOLUME 4, 2016 From the comparisons in Table <ref type="table" target="#tab_8">5</ref>, we can find that the diversity technology can improve the representational ability of the machine learning model and thus significantly improve the classification performance of the learned machine learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. IMAGE SEGMENTATION</head><p>In computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels). The goal of segmentation is to simplify and change the representation of an image into something that is more meaningful and easier to analyze. More precisely, image segmentation is the process of assigning a label to each pixel in an image such that pixels with the same label share certain characteristics. Since a semantic segmentation algorithm deals with tremendous amount of uncertainty from inter and intra object occlusion and varying appearance, lighting and pose, obtaining multiple best choices from all possible segmentations tends to be one of the possible way to solve the problem. Therefore, the image segmentation problem can be transformed into the M-best problem. However, as traditional problem in M-best problem, the obtained multiple choices are usually similar and the information provided to the user tends to be redundant. As section V shows, the obtained multiple choices will usually be only one-pixel shifted versions to each other.</p><p>The way to solve this problem is to introduce diversity into the training process to encourage the multiple choices to be diverse. Under inference diversification, the model is desired to provide a diverse set of low-energy solutions which represent different local optimal results from the data. Fig. <ref type="figure">7</ref> shows the examples of inference diversification over the task in prior work <ref type="bibr" target="#b30">[31]</ref>. Many works <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b91">[93]</ref>- <ref type="bibr" target="#b93">[95]</ref>, <ref type="bibr" target="#b95">[97]</ref>, <ref type="bibr" target="#b115">[117]</ref>- <ref type="bibr" target="#b117">[119]</ref> have introduced inference diversity into the image segmentation tasks via different ways. <ref type="bibr" target="#b28">[29]</ref> first introduces the D-MCL in subsection V-A for image segmentation. The work developed the diversification method as Eq. 67 shows. To further improve the performance in work <ref type="bibr" target="#b28">[29]</ref>, prior works <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b91">[93]</ref> combine the D-MCL with reranking which provide a way to obtain multiple diversified choices and select the proper one from the multiple choices.</p><p>As discussed in subsection V-A, the greedy nature of original D-MCL makes the obtained labelling only be influenced by previously found labellings while ignore the upcoming labellings. To overcome this problem, <ref type="bibr" target="#b92">[94]</ref> develops a novel D-MCL which has the form as Eq. 68. Besides, <ref type="bibr" target="#b95">[97]</ref>, <ref type="bibr" target="#b117">[119]</ref> use the submodular to measure the diversification between multiple choices (see details in subsection V-B). <ref type="bibr" target="#b116">[118]</ref> combines the NMS (see details in subsection V-C) and the sliding window to obtain multiple choices. Instead of inference diversification for better performance of image segmentation, prior work <ref type="bibr" target="#b26">[27]</ref> tries to obtain multiple diversified models for image segmentation task. The method proposed by <ref type="bibr" target="#b26">[27]</ref> is to divide the training samples into several subsets where each base model is trained with a specific one. Through allocating each training sample to the model with lowest predict error, each model tends to model different classes from others.</p><p>Under the inference diversification and D-models for image segmentation tasks, the obtained multiple choices would be diversified as Fig. <ref type="figure">7</ref> shows and the performance of the model would also be significantly improved. Just as the remote sensing imaging tasks, we list some comparison results in prior works to show the effectiveness of the diversity technology in machine learning. The comparison results are listed in Table <ref type="table" target="#tab_10">6</ref>. Generally, the experimental results on the PASCAL Visual Object Classes Challenge (PASCAL VOC) dataset are chosen to show the effectiveness of the diversity in machine learning for image segmentation. Also, From the table 6, we can find that the inference diversification can significantly improve the performance for segmentation.</p><p>VOLUME 4, 2016 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CAMERA RELOCALIZATION</head><p>Camera relocalization is to estimate the pose of a camera relative to a known 3D scene from a single RGB-D frame <ref type="bibr" target="#b85">[87]</ref>. It can be formulated as the inversion of the generative rendering procedure, which is to find the camera pose corresponding to a rendering of the 3D scene model that is most similar to the observed input. Since the problem is a nonconvex optimization problem which has many local optima, one of the methods to solve the problem is to find a set of M predictors which generate M camera pose hypotheses and then infers the best pose from the multiple pose hypotheses. Similar to traditional M-best problems, the obtained M predictors is usually similar.</p><p>To overcome this problem and obtain hypotheses that are different from each other, <ref type="bibr" target="#b86">[88]</ref> tries to learn 'marginally relevant' predictors, which can make complementary predictions, and compare their performance when used with different selection procedures. In <ref type="bibr" target="#b86">[88]</ref>, greedy algorithm is used to obtain multiple diversified models. Different weights are defined on each training samples, and the weights is updated with the training loss from the former learned model. Finally, multiple diversified models can be obtained for camera relocalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. OBJECT DETECTION</head><p>Object detection is one of the computer vision tasks which deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Similar to image segmentation tasks, great uncertainty is contained in the object detection algorithms. Therefore, obtaining multiple diversified choices is also an important way to solve the problem. Some prior works <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b107">[109]</ref> have made great effects to obtain multiple diversified choices by M-NMS (see V-C for details). <ref type="bibr" target="#b31">[32]</ref> use the greedy procedure for eliminating repeated detections via NMS. Besides, <ref type="bibr" target="#b107">[109]</ref> demonstrates that the energies resulting from M-NMS lead to the maximization of submodular function, and then through branch-and-bound strategy <ref type="bibr" target="#b124">[126]</ref>, all the image can be explored and diversified multiple detections can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. MACHINE TRANSLATION</head><p>Machine translation (MT) task is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another. Recently, machine translation systems have been developed and widely used in real-world application. Commercial machine translation services, such as Google translator, Microsoft translator, and Baidu translator, have made great success. From the perspective of the user interaction, the ideal machine translator is an agent that reads documents in one language and provides accurate, high quality translations in another. This interaction ideal has been implicit in machine translation (MT) research since the field's inception. Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Therefore, to overcome this problem, providing the M-best translations instead of a single best one is necessary <ref type="bibr" target="#b114">[116]</ref>.</p><p>However, in MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. The implicit goal behind these technologies is to better explore the output space by introducing diversity into the surrogate set. Some prior works have introduced diversity into the obtained multiple choices and obtained better performance <ref type="bibr" target="#b111">[113]</ref>. <ref type="bibr" target="#b94">[96]</ref> develops the method to diversify multiple choices which is introduced in subsection V-A. In the works, a novel dissimilarity function has been defined on different translations to increase the diversity between the obtained translations. It can be formulated as <ref type="bibr" target="#b94">[96]</ref> ∆ n (y, y ) = -</p><formula xml:id="formula_104">|y|-q i=1 |y |-q j=1 [[y i:i+q = y j:j+q ]]<label>(83)</label></formula><p>where [[•]] is the Iverson bracket (1 if input condition is true, 0 otherwise) and y i:j is the subsequence of y from word i to word j (inclusive). The advantage of this dissimilarity function is its simplicity. Besides, the diversity-promoting can ensure the machine learning system obtain multiple diversified translation for the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. INFORMATION RETRIEVAL 1) Natural Language Processing</head><p>In machine learning and natural language processing, a topic model is a statistical model for discovering the abstract "topics" that occur in a collection of documents. Probabilistic topic models such as Latent Dirichlet Allocation (LDA) and Restricted Boltzmann Machine (RBM) can provide a useful and elegant tool for discovering hidden structure within large data sets of discrete data, such as corpuses of text. However, LDA implicitly discovers topics along only a single dimension while RBM tends to learn multiple redundant hidden units to best represent dominant topics and ignore those in the long-tail region <ref type="bibr" target="#b20">[21]</ref>. To overcome this problem, diversifying over the learned model (D-model) can be applied over the learning process of the model.</p><p>Recent research on multi-dimensional topic modeling aims to devise techniques that can discover multiple groups of topics, where each group models some different dimension or aspect of the data. Therefore, prior work <ref type="bibr" target="#b112">[114]</ref> presents a new multi-dimensional topic model that uses a determinantal point process prior (see details in subsection V-A) to encourage different groups of topics to model different dimensions of the data. Determinantal point processes are probabilistic models of repulsive phenomena which originated in statistical physics but have recently seen interest from the machine learning community. Besides, <ref type="bibr" target="#b20">[21]</ref> introduces the RBM for topic modeling to utilize hidden units to discover the latent topics and learn compact semantic representations for documents. Furthermore, to reduce the redundancy of the learned RBM, <ref type="bibr" target="#b20">[21]</ref> utilizes the angular-based diversification method which has the form of Eq. 31 to diversify the learned hidden units in RBM. Under the diversification, the RBM can learn much more powerful latent document representations that boost the performance of topic modeling greatly <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Web Search</head><p>The problem of result diversification has been studied in various tasks, but the most robust literature on result diversification exists in web search <ref type="bibr" target="#b162">[164]</ref>. Web search has become the prodominant method for people to fulfill their information needs. In web search, it is general to provide different search results with different interpretations of a query. To satisfy the requirement of multiple distinct user type, the web search system is desired to provide a diverse set of results.</p><p>The increasing requirements for easily accessible information via web-based services have attracted a lot of attentions on the studies of obtaining diverse search results <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b154">[156]</ref>, <ref type="bibr" target="#b162">[164]</ref>. The objective is to achieve large coverage on a few features but very small coverage on the remaining features, which satisfies the submodularity. Therefore, <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b162">[164]</ref> take advantage of the submodular for result diversification (see Subsection V-B for details). Besides, <ref type="bibr" target="#b154">[156]</ref> uses the distance-based measurement to formulate the dissimilarity function in Subsection V-A and <ref type="bibr" target="#b10">[11]</ref> has further summarized the diversification methods for result diversification which has the form as D-MCL (See Subsection V-A for details). These search diversification methods can provide multiple diversified choices for the users to satisfy the requirements of specific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. SOCIAL NETWORK ANALYSIS</head><p>Social network analysis is a process of investigating social structures through the use of networks and graph theory. It characterizes networked structures in terms of nodes and the ties, edges, or links (relationships or interactions) that connect them. Ranking nodes on graphs is a fundamental task in social network analysis and it can be applied to measure the centrality in social networks <ref type="bibr" target="#b97">[99]</ref>. However, many nodes in top-K ranking list obtained general methods are general similar since it only takes the relevance of the nodes into consideration <ref type="bibr" target="#b153">[155]</ref>.</p><p>To improve the effectiveness of the ranking process, many prior works have incorporated the diversity into the top-K ranking results <ref type="bibr" target="#b97">[99]</ref>, <ref type="bibr" target="#b153">[155]</ref>, <ref type="bibr" target="#b155">[157]</ref>. To enforce diversity in the top-K ranking results, the way to measure the similarity tends to be the key problem. <ref type="bibr" target="#b97">[99]</ref> has formulated the diversified ranking problem as a submodular set function maximization and processes the problem as Subsection V-B shows. Besides, <ref type="bibr" target="#b153">[155]</ref> takes advantage of the Heat kernel to formulate the weights on the social graph (see Subsection IV-A3 for details) where larger weights would be if points are closer. Then through the random walks in an absorbing Markov chain, diversified ranking can be obtained <ref type="bibr" target="#b153">[155]</ref>. Furthermore, according to the special characteristics, <ref type="bibr" target="#b155">[157]</ref> develops a novel goodness measure to balance both the relevant and the diversity. For simplicity, the goodness measure would not be shown in this work and more details can be found in <ref type="bibr" target="#b155">[157]</ref> for interested readers. It should be noted that the optimization problem in <ref type="bibr" target="#b155">[157]</ref> is equal to the D-MCL in subsection V-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. DOCUMENT SUMMARIZATION</head><p>Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Generally, a good summary should coverage elements from distinct parts of data to be representative of the corpus and does not contain elements that are too similar to each other. Therefore, coverage and diversity are usually essential for the multi-document summarization <ref type="bibr" target="#b159">[161]</ref>.</p><p>Since coverage and diversity could sometimes be conflicting requirements <ref type="bibr" target="#b159">[161]</ref>, some prior works try to find a tradeoff between the coverage and the diversity. As Subsection V-B shows, since there exist efficient algorithms (with near-optimal solutions) for a diverse set of constraints when the submodular function is monotone in the document summarization task, the submodular optimization methods have been applied in the diversification of the obtained summary of the multiple documents <ref type="bibr" target="#b98">[100]</ref>, <ref type="bibr" target="#b99">[101]</ref>, <ref type="bibr" target="#b159">[161]</ref>- <ref type="bibr" target="#b161">[163]</ref>. <ref type="bibr" target="#b98">[100]</ref> defines a class of submodular functions meant for document summarization and <ref type="bibr" target="#b99">[101]</ref> treats the document summarization problem as maximizing a submodular function with a budget constraint. <ref type="bibr" target="#b159">[161]</ref> further investigates the personalized data summarization by the submodular function subject to multiple constraints. Under the diversification by submodular, the obtained elements can be diversified and the multiple documents can be better summarized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSIONS</head><p>This article surveyed the available works on diversity technology in general machine learning model, by systematically categorizing the diversity in training samples, D-model, Dmodels, and inference diversity in the model. We first summarize the main results and identify the challenges encountered throughout the article.</p><p>Recently, due to the excellent performance of the machine learning model for feature extraction, machine learning methods have been widely applied in real-world applications.</p><p>However, the limited number and imbalance of training samples in real-world applications usually make the learned machine learning models be sub-optimal, sometimes even lead to the "over-fitting" in the training process. This would limit the performance of the machine learning models. Therefore, this work summarizes the diversity technology in prior works which can work on the machine learning model as one of the methods to improve the model's representational ability. We want to emphasize that the diversity technology is not decisive. The diversity can only be considered as an additional technology to improve the performance of the machine learning process.</p><p>Through the introduction of the diversity in machine learning, the three questions proposed in the introduction can be easily answered. The detailed descriptions of data diversification, model diversification, and inference diversification are introduced in sections III, IV, and V. With these methods, the machine learning model can be diversified and the performance can be improved. Besides, the diversification of the model (D-model) tries to improve the representational ability of the machine learning model directly (see Fig. <ref type="figure" target="#fig_3">5</ref> for details) while the diversification of the models (D-models) aims to obtain multiple diversified choices under the diversification of the ensemble learning (see Fig. <ref type="figure" target="#fig_4">6</ref> for details). It should also be noted that the diversification measurements for data diversification, model diversification, and the inference diversification show some similarity. As introduced, the diversity aims to decrease the redundancy between the data or the factors. The key problem for diversification is the way to measure the similarity between the data or the factors. However, in the machine learning process, the data and the factors are processes as the vectors. Therefore, there exist overlaps between the data diversification, model diversification as well as the inference diversification, such as the DPP measurement. More importantly, we should also note that the diversification in different steps of machine learning models presents its special characteristics. The details for the applications of diversity methods are shown in section VI. This work only lists the most common applications in real-world. The readers should consider whether the diversity methods are necessary according to the specific task they face with.</p><p>Advice for implementation. We expect this article is useful to researchers who want to improve the representational ability of machine learning models for computer vision tasks. For a given computer vision task, the proper machine learning model should be chosen first. Then, we advise to consider adding diversity-promoting priors to improve the performance of the model and further what type of diversity measurement is desired. When one desires obtain multiple models or multiple choices, then one can consider diversifying multiple models or the obtained multiple choices and section IV-B and V would be relevant and helpful. We advise the reader to first consider whether the multiple models or multiple choices can be helpful for the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>The training of machine learning models requires large amounts of labelled samples. However, the limited training samples constrain the performance of machine learning models. Therefore, effective diversity technology, which can encourage the model to be diversified and improve the representational ability of the model, is expected to be an active area of research in machine learning tasks. This paper summarizes the diversity technology for machine learning in previous works. We introduce diversity technology in data pre-processing, model training, inference, respectively. Other researchers can judge whether diversity technology is needed and choose the proper diversity method for the special requirements according to the introductions in former sections.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 :</head><label>1</label><figDesc>FIGURE 1: The basic framework of this paper. The main body of this paper consists of three parts: General Machine Learning Models in Section II, Diversity in Machine Learning in Section III-V, and Extensive Applications in Section VI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 3 :</head><label>3</label><figDesc>FIGURE 3: Flowchart of a special form of supervised machine learning with single model. Since diversity mainly occurs in the training batch in the data-preprocessing, this work will mainly discuss the diversity of samples in the training batch for data diversification. Generally, the more diversification and balance each training batch is, the more effectiveness the training process is. In addition, it should be noted that the factors in the same layer of the model can be diversified to improve the representational ability of the model (which is called D-model in this paper). Moreover, when we obtain multiple choices from the model, the obtained choices are desired to provide more complement information. Therefore, some works focus on the diversification of multiple choices (which we call inference diversification).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 5 :</head><label>5</label><figDesc>FIGURE 5: Effects of D-model on improving the performance of the machine learning model. Under the model diversification, each parameter factor of the machine learning model tends to model unique information and the whole machine learning model can model more useful information from the objects. Thus, the representational ability can be improved. The figure shows the results from the image segmentation task in [31]. As showed in the figure, the extracted features from the model can better discriminate different objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 6 :</head><label>6</label><figDesc>FIGURE 6: Effects of D-models for improving the performance of the machine learning model. The figure shows the image segmentation task from the prior work [27]. The single model often produce solutions with low expected loss and step into the sub-optimal results. Besides, general ensemble learning usually provide multiple choices with great similarity. Therefore, this work summarizes the methods which can diversify the ensemble learning (D-models). As the figure shows, under the model diversification, each model of the ensemble can produce different outputs reflecting multi-modal belief [27].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc>Comparisons of Different Measurements.represents that the measurement possess the property while × means the measurement does not possess the property.</figDesc><table><row><cell>Measurements</cell><cell cols="4">Pairwise Correlation Multiple Correlation Group-wise Correlation</cell><cell>Scale Invariant</cell></row><row><cell>Cosine Similarity</cell><cell></cell><cell></cell><cell>×</cell><cell>×</cell></row><row><cell>Determinantal Point Process</cell><cell></cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>Submodular Spectral Diversity</cell><cell></cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>Euclidean Distance</cell><cell></cell><cell></cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>Heat Kernel</cell><cell></cell><cell></cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>Divergence</cell><cell></cell><cell></cell><cell>×</cell><cell>×</cell></row><row><cell>Uncorrelation and Evenness</cell><cell></cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>L 2,1</cell><cell></cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>Inner Product</cell><cell></cell><cell></cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>D-models</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Inference</cell><cell>Horse Cow</cell><cell></cell></row><row><cell></cell><cell>Inference</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Inference</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>Overview of most frequently used diversification method in D-models and the papers in which example measurements can be found.</figDesc><table><row><cell>Methods</cell><cell>Measurements</cell><cell>Papers</cell></row><row><cell></cell><cell>Divergence</cell><cell>[26], [69]</cell></row><row><cell>Optimization-based</cell><cell>Renyi-entropy Cross Entropy</cell><cell>[76] [77], [78]</cell></row><row><cell></cell><cell>Cosine Similarity</cell><cell>[63], [66]</cell></row><row><cell></cell><cell>L 2,1</cell><cell>[58]</cell></row></table><note><p>NCL</p><p>[73],<ref type="bibr" target="#b73">[74]</ref></p><p>,<ref type="bibr" target="#b80">[82]</ref></p><p>,<ref type="bibr" target="#b81">[83]</ref> </p><p>Others [64]-[66], [69]-[72] Sample-based -[27], [87]-[90] Ranking-based -[</p><p>23</p><p>],<ref type="bibr" target="#b89">[91]</ref></p><p>,<ref type="bibr" target="#b90">[92]</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 :</head><label>5</label><figDesc>Some comparison results between the general model and the diversified model for remote sensing imaging tasks.) over the city of Pavia, Italy. The image consists of 610 × 340 pixels with 115 spectral bands. The image is divided into 9 classes with a total of 42, 776 labelled samples, including the asphalt, meadows, gravel, trees, metal sheet, bare soil, bitumen, brick, and shadow. For the experiments, 200 samples of each class are used for training and the remainder are for testing. Indian Pines dataset [81] was taken by AVIRIS sensor in northwestern Indiana. The image has 145 × 145 pixels with 224 spectral channels where 24 channels are removed due to the noise. The image is divided into 8 classes with a total of 8, 598 labelled samples, including the Corn no_till, Corn min_till, Grass pasture, hay windrowed, Soybeans no_till, Soy beans min, Soybeans clean, and woods. For the experiments, 200 samples of each class are used for training and the remainder are for testing.</figDesc><table><row><cell>Dataset</cell><cell>Reference</cell><cell>Methods</cell><cell>Accuracy (%)</cell><cell>Diversified Method</cell><cell>Accuracy (%)</cell></row><row><cell>Ucmerced Land Use dataset</cell><cell>[22] [77]</cell><cell>DSML CaffeNet</cell><cell>95.95 ± 0.24 95.48</cell><cell>D-DSML Diversified MCL</cell><cell>96.76 ± 0.36 97.05 ± 0.55</cell></row><row><cell></cell><cell>[20]</cell><cell>DBN</cell><cell>91.18 ± 0.08</cell><cell>D-DBN-PF</cell><cell>93.11 ± 0.06</cell></row><row><cell>Pavia University</cell><cell>[62]</cell><cell cols="3">DML-MS-CNN 99.03 ± 0.25 DPP-DML-MS-CNN</cell><cell>99.46 ± 0.03</cell></row><row><cell></cell><cell>[112]</cell><cell>DBN</cell><cell>90.61 ± 1.15</cell><cell>M-DBN</cell><cell>92.55 ± 0.74</cell></row><row><cell>Indian Pines</cell><cell>[20] [62]</cell><cell cols="3">DBN DML-MS-CNN 98.87 ± 0.21 DPP-DML-MS-CNN 88.25 ± 0.17 D-DBN</cell><cell>91.03 ± 0.12 99.08 ± 0.23</cell></row><row><cell cols="3">Pavia University dataset [80] was gathered by a sensor</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">known as the reflective optics system imaging spectrometer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(ROSIS-3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 6 :</head><label>6</label><figDesc>Some comparison results between the general model and the diversified model for image segmentation.</figDesc><table><row><cell>Dataset</cell><cell>Reference</cell><cell>Methods</cell><cell>Accuracy (%)</cell><cell>Diversified Method</cell><cell>Accuracy (%)</cell></row><row><cell>PASCAL VOC 2010 dataset</cell><cell>[29]</cell><cell>MAP</cell><cell>91.54</cell><cell>DivMBEST</cell><cell>95.16</cell></row><row><cell>PASCAL VOC 2011 dataset</cell><cell>[27]</cell><cell>MCL</cell><cell>about 66</cell><cell>sMCL</cell><cell>about 71</cell></row><row><cell>PASCAL VOC 2012 dataset</cell><cell>[31]</cell><cell>Second Order Pooling (O 2 P )-MAP</cell><cell>46.5</cell><cell>DivMBEST+Ranking</cell><cell>48.1</cell></row><row><cell>PASCAL VOC 2012 dataset</cell><cell>[97]</cell><cell>MAP</cell><cell>43.43</cell><cell>submodular-MCL</cell><cell>55.32</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>  VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1"><p>  VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_2"><p>  VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_3"><p>  VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_4"><p>  VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_6"><p>  VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_7"><p>  VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_8"><p>  VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_9"><p>  VOLUME 4, 2016   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>ACTIONS <rs type="institution">ON IMAGE PROCESSING</rs>, the <rs type="funder">IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING</rs>, the <rs type="institution" subtype="infrastructure">IEEE JOURNAL OF SE-LECTED TOPICS IN SIGNAL PROCESSING</rs>, and the <rs type="institution" subtype="infrastructure">IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING</rs>. His current research interests include computer vision, machine learning, and pattern recognition. <rs type="person">Dr. Zhong</rs> is a Referee of the <rs type="institution" subtype="infrastructure">IEEE TRANSACTIONS ON NEU-RAL NETWORKS AND LEARNING SYSTEMS</rs>, the <rs type="funder">IEEE TRANS-ACTIONS ON IMAGE PROCESSING</rs>, the <rs type="funder">IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING</rs>, the <rs type="institution" subtype="infrastructure">IEEE JOURNAL OF SE-LECTED TOPICS IN SIGNAL PROCESSING</rs>, and the <rs type="institution" subtype="infrastructure">IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING</rs>, and the <rs type="funder">IEEE</rs> <rs type="programName">GEOSCIENCE AND REMOTE SENS-ING LETTERS</rs>. He was the recipient of the <rs type="funder">National Excellent Doctoral Dissertation Award of China</rs> (<rs type="grantNumber">2011</rs>) and the <rs type="funder">New Century Excellent Talents in University of China</rs> (2013).</p><p>WEIDONG HU was born in September, 1967. He received the B.S. degree in microwave technology, the M.S. degree and Ph.D. degree in communication and electronic system from the <rs type="funder">National University of Defense Technology</rs>, Changsha, P. R. China in 1990China in  , 1994China in   and 1997, respectively.  , respectively.   He is currently a full professor with the <rs type="funder">National Key Laboratory of Science and Technology on ATR, National University of Defense Technology</rs>. His research interests include radar signal and data processing.</p></div>
			</div>
			<div type="funding">
<div><p>This work was supported in part by the <rs type="funder">Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">61671456</rs> and <rs type="grantNumber">61271439</rs>, in part by the <rs type="funder">Foundation for the Author of National Excellent Doctoral Dissertation of China (FANEDD)</rs> under Grant <rs type="grantNumber">201243</rs>, and in part by the <rs type="funder">Program for New Century Excellent Talents in University</rs> under Grant <rs type="grantNumber">NECT-13-0164</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pwhSE6T">
					<orgName type="program" subtype="full">GEOSCIENCE AND REMOTE SENS-ING LETTERS</orgName>
				</org>
				<org type="funding" xml:id="_h6aVZn4">
					<idno type="grant-number">2011</idno>
				</org>
				<org type="funding" xml:id="_CrTNnMP">
					<idno type="grant-number">61671456</idno>
				</org>
				<org type="funding" xml:id="_2hpuHpb">
					<idno type="grant-number">61271439</idno>
				</org>
				<org type="funding" xml:id="_RFAZbny">
					<idno type="grant-number">201243</idno>
				</org>
				<org type="funding" xml:id="_3YKSXbd">
					<idno type="grant-number">NECT-13-0164</idno>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">IEEE JOURNAL OF SE-LECTED TOPICS IN SIGNAL PROCESSING</orgName>
				</org>
				<org type="infrastructure">					<orgName type="extracted">IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING</orgName>
				</org>
				<org type="infrastructure">					<orgName type="extracted">IEEE TRANSACTIONS ON NEU-RAL NETWORKS AND LEARNING SYSTEMS</orgName>
				</org>
				<org type="infrastructure">					<orgName type="extracted">IEEE JOURNAL OF SE-LECTED TOPICS IN SIGNAL PROCESSING</orgName>
				</org>
				<org type="infrastructure">					<orgName type="extracted">IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diversity induced matrix decomposition model for salient object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="253" to="267" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Salient object detection via structured matrix decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="818" to="832" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<title level="m">A Formal Model of Ambiguity and its Applications in Machine Translation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Detection with Diverse Proposals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7369" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-supervised relative depth learning for urban scene understanding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fusion-based methods for result diversification in web search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Determinantal Point Processes for Mini-Batch Diversification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kjellstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast determinantal point processes via distortion-free intermediate sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Derezinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03717</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Search Result Diversification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drosou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitoura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD Record</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Active mini-batch sampling using repulsive point processes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Salvi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02772</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diverse expected gradient active learning for relative attributes</title>
		<author>
			<persName><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3203" to="3217" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diversifying Convex Transductive Experimental Design for Active Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1997" to="2003" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-class active learning by uncertainty sampling with diversity maximization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Compution Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse coding and decorrelation in primary visual cortex during natural vision</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Vinje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="issue">5456</biblScope>
			<biblScope unit="page" from="1273" to="1276" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6583</biblScope>
			<biblScope unit="page" from="607" to="607" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of V1 simple cell receptive fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zylberberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Deweese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1002250" to="e1002250" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Latent variable modeling with diversity-inducing mutual angular regularization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.07336</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to diversify Deep Belief Networks for Hyperspectral Image Classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Schãűnlieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3516" to="3530" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diversifying Restricted Boltzmann Machine for Document Modeling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1315" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Diversity-Promoting Deep Structural Metric Learning for Remote Sensing Scene Classification</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="371" to="390" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deep ensemble learning method for monaural speech separation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="967" to="977" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximum entropy discrimination Markov networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2531" to="2569" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic multiple choice learning for training diverse deep ensembles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P S</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2119" to="2127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">N-best maximal decoders for part models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2627" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Diverse m-best solutions in markov random fields</title>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Computing the M most probable modes of a graphical model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="161" to="169" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative reranking of diverse segmentations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1923" to="1930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Introduction to machine learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural Networks and Learning Machines</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Batch-Mode Regularized Multimetric Active Learning Framework for Classification of Hyperspectral Images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Crawford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6594" to="6609" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Active learning via transductive experimental design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-greedy active learning for text categorization using convex transductive experimental design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="635" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An End-to-End Joint Unsupervised Learning of Deep Model and Pseudo-Classes for Remote Sensing Representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07224</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Determinantal point processes for machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="123" to="286" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Inductive multi-task learning with multiple view data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="543" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Correlating Filter Diversity with Convolutional Neural Network Accuracy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ellen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint diversity regularization and graph regularization for multiple kernel k-means clustering via latent variables</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="page" from="154" to="163" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Diversity priors for learning early visual features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rodriguez-Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Piater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving the Generalization Performance of Multi-class SVM via Angular Regularization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2131" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep compact similarity metric for kinship verification from face images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="84" to="94" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">On the generalization error bounds of neural networks under diversity-inducing mutual angular regularization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07110</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="181" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diversity regularized metric learning for person re-identification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4264" to="4268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Compact and Discriminative Stacked Autoencoder for Hyperspectral Image Classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multiple Kernel k-Means Clustering with Matrix-Induced Regularization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1888" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unified locally linear classifiers with diversity-promoting anchor points</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2339" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nonoverlap-promoting Variable Selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Selecting Diverse Features via Spectral Regularization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1583" to="1591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Uncorrelation and Evenness: a New Diversity-Promoting Regularizer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3811" to="3820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2078" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Single and multiple object tracking using a multi-feature joint sparse representation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="816" to="833" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An l2/l1 regularization framework for diverse learning tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="206" to="211" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep super-class learning for long-tail distributed image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="118" to="128" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Determinantal point process models and statistical inference</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lavancier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rubak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="853" to="877" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning the parameters of Determinantal Point Process Kernels</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Affandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1224" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A CNN with multiscale convolution and diversified metric for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Exclusivity regularized machine: a new ensemble SVM classifier</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1739" to="1745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Learning to diversify via weighted kernels for classifier ensemble</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Hao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1167</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An analysis of diversity measures</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="247" to="271" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Diversity regularized machine</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1603" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ensemble diversity measures and their application to thinning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Banfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Diversity creation methods: a survey and categorisation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Limits on majority vote accuracy in classifier fusion</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The random subspace method for constructing decision forests</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="832" to="844" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Design of effective neural network ensembles for image classification purposes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9-10</biblScope>
			<biblScope unit="page" from="699" to="707" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, Boosting, and Randomization</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="139" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Negatively correlated neural networks can produce best ensembles</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="176" to="185" />
		</imprint>
	</monogr>
	<note>Australian journal of intelligent information processing systems</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fast decorrelated neural network ensembles with random weights</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alhamdoosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="104" to="117" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A new selective neural network ensemble with negative correlation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="488" to="498" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Selective ensemble of SVDDs with Renyi entropy based diversity measure</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="185" to="196" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Diversifying deep multiple choices for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IGARSS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Why M heads are better than one: Training a diverse ensemble of deep networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06314</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGSPATIAL Int. Conf. Adv. Geograph</title>
		<meeting>the ACM SIGSPATIAL Int. Conf. Adv. Geograph</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Indian</forename><surname>Pines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dataset</forename></persName>
		</author>
		<ptr target="http://www.ehu.ews/ccwintoco/index.php?title=Hypespectral_Remote_Sensing_Scenes" />
		<imprint>
			<date type="published" when="2019">Apr. 25. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Ensemble learning using decorrelated neural networks</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="373" to="384" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Crowd Counting with deep negative correaltion learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5382" to="5390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Convex ensemble learning with sparsity and diversity</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="49" to="59" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Ensembling neural networks: many could be better than all</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="263" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Sensitivity and similarity regularization in dynamic selection of ensembles of neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Keshavarz-Hedayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Dimopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3953" to="3958" />
		</imprint>
		<respStmt>
			<orgName>IJCNN</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in RGB-D images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multi-output learning for camera relocalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1114" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Exploiting unlabeled data to enhance ensemble diversity</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="129" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">An ensemble diversity approach to supervised binary hashing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinã Ąn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raziperchikolaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">An empirical investigation on the use of diversity for creation of classifier ensembles</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Didaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Multiple Classifier Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="206" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Clustered Support Vector Machines</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Multiple choice learning: Learning to produce multiple structured outputs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Inferring M-best diverse labelings in a single one</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1814" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Efficiently enforcing diversity in multi-output structured prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rutenbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="284" to="292" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A systematic exploration of diversity in machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Naatural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Naatural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1100" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Submodular Maximization and Diversity in Structured Output Spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Diversifying Search Results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM Internatiuonal Conference on Web Search and Data Mining</title>
		<meeting>the Second ACM Internatiuonal Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Scalable diversified ranking on large graphs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2133" to="2146" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A Class of Submodular Functions for Document Summarization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 49th Ann. Meeting of the Assoc. for Computational Linguistics: Human Language Technologies (ACL)</title>
		<meeting>49th Ann. Meeting of the Assoc. for Computational Linguistics: Human Language Technologies (ACL)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="510" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Multi-Document Summarization via Budgeted Maximization of Submodular Functions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Language Technologies: Ann. Conf. North Am. Chapter of the Assoc. for Computational Linguistics (HLT-NAACL)</title>
		<meeting>Human Language Technologies: Ann. Conf. North Am. Chapter of the Assoc. for Computational Linguistics (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="912" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Near-Optimal Observation Selection Using Submodular Functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Nat&apos;l Conf. Artificial Intelligence (AAAI)</title>
		<meeting>22nd Nat&apos;l Conf. Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Maximizing the Spread of Influence through a Social Network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth ACM SIGKDD Int&apos;l Conf. Knowlege Discovery and Data Mining (KDD)</title>
		<meeting>Ninth ACM SIGKDD Int&apos;l Conf. Knowlege Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="235" to="284" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Approximation Algorithms</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shekhovtsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Wolsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming</title>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="265" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Statistical thermodynamics of natural images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tkacik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18701" to="018701" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Branch and bound strategies for non-maximal suppression in object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="385" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">On detection of multiple object instances using hough transforms</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1773" to="1784" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Discriminative models for multi-class object layout</title>
		<author>
			<persName><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A diversified deep ensemble for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WHISPERS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Mutual information and diverse decoding improve neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00372</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Multi-dimensional Topic Modeling with Determinantal Point Processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Independent Work Report Fall</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Hyperspectral remote sensing data analysis and future challenges</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scheunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6" to="36" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Wider pipelines: N-best alignments and parses in MT training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zollmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMTA</title>
		<meeting>AMTA</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Mode-Marginals: Expressing Uncertainty via M-Best Solutions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Perturbations, Optimization, and Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Submodboxes: Near-optimal search for a set of diverse object proposals</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1378" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2645" to="2653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Maximizing Diversity for Multimodal Optimization</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">O D</forename><surname>Franca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2539</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Priors for Diversity in Generative Latent Variable Models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2996" to="3004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Expectationmaximization for learning determinantal point processes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3149" to="3157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Fast determinantal point process sampling with application to clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2319" to="2327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">The use of entropy to measure structural diversity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Masisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nelwamondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Marwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Learning compact and effective distance metrics with diversity regularization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Database</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="610" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Efficient subwindow search: A branch and bound framework for object localization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2129" to="2142" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Block-wise map inference for determinantal point processes with application to change-point detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Signal Processing Workshop (SSP)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Learning cross-media joint representation with sparse and semi-supervised regularization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="965" to="978" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Inference for determinantal point processes without spectral knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T R</forename><surname>Aueb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3393" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Deep Determinantal Point Process for Large-Scale Multi-Label Classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Diversified Hidden Markov Models for Sequential Labeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Daxu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2947" to="2960" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Diversified Dictionaries for Multi-Instance Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="407" to="416" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Saliency detection by multitask sparsity pursuit</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1327" to="1338" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Learning latent space models with angular constraints</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3799" to="3810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Diversity-promoting Bayesian learning of latent variable models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Towards sparsity and selectivity: Bayesian learning of restricted Boltzmann machine for early visual features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez-Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Piater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">ELM based multiple kernel k-means with diversity-induced regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJANN)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2699" to="2705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Saliency Detection via Diversity-Induced Multi-view Matrix Decomposition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision, pp</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="137" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Semi-supervised crossmedia feature learning with unified patch graph regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="583" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Feature Selection with L2, 1 -2 Regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4967" to="4982" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Robust structured subspace learning for data representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2085" to="2098" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Diversified texture synthesis with feed-forward networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Near-optimal map inference for determinantal point processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2735" to="2743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Structured determinantal point processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Fixed-point algorithms for learning determinantal point processes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mariet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2389" to="2397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Improving sequential determinantal point processes for supervised video summarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="517" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Diverse sequential subset selection for supervised video summarization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2069" to="2077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">End-to-End integration of a convolution network, deformable parts model and non-maximum suppression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="851" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Individualness and determinantal point processes for pedestrian detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="330" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Land use classification in remote sensing images by convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Castelluccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.00092</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Joint Learning of the Center Points and Deep Metrics for Land-Use Classification in Remtoe Sensing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">76</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Tagging like Humans: Diverse and Distinct Image Annotation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7967" to="7975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Improving Diversity in Ranking Using Absorbing Random Walks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Gael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andrzejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Language Technologies: The Ann. conf. North Am. Chapter of the Assoc. for Computational Linguistics (HLT-NAACL &apos;07)</title>
		<meeting>Human Language Technologies: The Ann. conf. North Am. Chapter of the Assoc. for Computational Linguistics (HLT-NAACL &apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">A Unified Framework for Recommending Diverse and Relevant Queries</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Int&apos;l Conf. World Wide Web (WWW &apos;11)</title>
		<meeting>20th Int&apos;l Conf. World Wide Web (WWW &apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Diversified Ranking on Large Graphs: An Optimization Viewpoint</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Konuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int&apos;l Conf. Knowledge Discovery and Data Mining (KDD &apos;11)</title>
		<meeting>17th Int&apos;l Conf. Knowledge Discovery and Data Mining (KDD &apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Improving Recommendation Lists through Topic Diversification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mcnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th int&apos;l conf. World Wide Web (WWW&apos;05)</title>
		<meeting>14th int&apos;l conf. World Wide Web (WWW&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Novelty and Diversity in Information Retrieval Evaluation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Buttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31th Ann. Int&apos;l ACM SIGIR Conf. Research and Development in Information Retrieval (SIGIR &apos;08)</title>
		<meeting>31th Ann. Int&apos;l ACM SIGIR Conf. Research and Development in Information Retrieval (SIGIR &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Diversifying Query Suggestion Results</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Nat&apos;l Conf. Artificial Intelligence (AAAI)</title>
		<meeting>Nat&apos;l Conf. Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Fast Constrained Submodular Maximization: Personalized Data Summarization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Badanidiyuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karbasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conferencee on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1358" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Beyond keyword search: discovering relevant scientific literature</title>
		<author>
			<persName><forename type="first">K</forename><surname>El-Arini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="439" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Temporal corpus summarization using submodular word coverage</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sipos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shivaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Information and Knowledge management</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge management</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">ZHIQIANG GONG received the Bachelor&apos;s degree in applied mathematics from Shanghai Jiaotong University (SJTU)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Panigrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Das</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PING ZHONG (M&apos;09-SM&apos;18) received the M.S. degree in applied mathematics and the Ph.D. degree in information and communication engineering from the National University of Defense Technology (NUDT)</title>
		<meeting><address><addrLine>Shanghai, China; Changsha, China; Changsha, China; Changsha, China; Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2012. 2015. 2003 and 2008. March 2015</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
		<respStmt>
			<orgName>D degree at National Key Laboratory of Science and Technology on ATR, National University of Defense Technology ; University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Online selection of diverse results Proceedings of the fifth ACM International Conference on Web Search and Data Mining He is currently pursuing the Ph His research interests are computer vision and image analysis 2013, and the Mater degree in applied mathematics from National University of Defense Technology (NUDT) respectively He is currently an Associate Professor with the National Key Laboratory of Science and Technology on ATR to February 2016, he was a Visiting Scholar in the Department of Applied Mathematics and Theory Physics He has authored or coauthored more than 30 peer reviewed papers in international journals such as the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, the IEEE TRANS-</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
