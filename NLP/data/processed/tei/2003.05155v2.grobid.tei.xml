<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology</title>
				<funder ref="#_uhvFVRP">
					<orgName type="full">Deutsche Forschungsgesellschaft (DFG)</orgName>
				</funder>
				<funder ref="#_GdWeawu #_P9tYvPR #_jXV9BZD #_w9ZgttH">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_DxgsBRc">
					<orgName type="full">Technology Promotion (IITP)</orgName>
				</funder>
				<funder ref="#_H5GNnfC">
					<orgName type="full">BMBF</orgName>
				</funder>
				<funder ref="#_evQTJ4C">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder ref="#_Akv6e7e #_vJzxzJU">
					<orgName type="full">Korea government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Studer</surname></persName>
							<email>stefan.studer@daimler.com</email>
							<idno type="ORCID">0000-0003-1598-6899</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Group Research</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Research</orgName>
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<postCode>71059</postCode>
									<settlement>Sindelfingen</settlement>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thanh</forename><surname>Binh Bui</surname></persName>
							<idno type="ORCID">0000-0003-1598-6899</idno>
							<affiliation key="aff1">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<postCode>10587</postCode>
									<settlement>Berlin</settlement>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Drescher</surname></persName>
							<email>christian.d.drescher@daimler.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Group Research</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Research</orgName>
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<postCode>71059</postCode>
									<settlement>Sindelfingen</settlement>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Hanuschkin</surname></persName>
							<email>alexander.hanuschkin@daimler.com</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Group Research</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Research</orgName>
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<settlement>Sindelfingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ludwig</forename><surname>Winkler</surname></persName>
							<email>winkler@tu-berlin.de</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<postCode>10587</postCode>
									<settlement>Berlin</settlement>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Peters</surname></persName>
							<email>steven.peters@daimler.com</email>
							<idno type="ORCID">0000-0003-3131-1664</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Group Research</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Research</orgName>
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<postCode>71059</postCode>
									<settlement>Sindelfingen</settlement>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
							<idno type="ORCID">0000-0002-3861-7685</idno>
							<affiliation key="aff3">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution">Esslingen University of Applied Sciences</orgName>
								<address>
									<country>Germany;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<postCode>10587</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Dept. of Artificial Intelligence</orgName>
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>Brain team</addrLine>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Max-Planck-Institut für Informatik</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<postCode>136-713 66123</postCode>
									<settlement>Seoul Saarbrücken</settlement>
									<country>South Korea Germany;</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">39B4FC501B9051289B829EF7C50ECD88</idno>
					<note type="submission">Received: 1 March 2021 Accepted:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning Applications</term>
					<term>Quality Assurance Methodology</term>
					<term>Process Model</term>
					<term>Automotive Industry and Academia</term>
					<term>Best Practices</term>
					<term>Guidelines</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning is an established and frequently used technique in industry and academia but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners have a need for guidance throughout the life cycle of a machine learning application to meet business expectations. We therefore propose a process model for the development of machine learning applications, that covers six phases from defining the scope to maintaining the deployed machine learning application. The first phase combines business and data understanding as data availability oftentimes affects the feasibility of the project. The sixth phase covers state-of-the-art approaches for monitoring and maintenance of a machine learning applications, as the risk of model degradation in a changing environment is eminent. With each task of the process, we propose quality assurance methodology that is suitable to adress challenges in machine learning development that we identify in form of risks. The methodology is drawn from practical experience and scientific literature and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support but lacks to address machine learning specific tasks. Our work proposes an industry and application neutral process model tailored for machine learning applications with focus on technical tasks for quality assurance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many industries, such as manufacturing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, personal transportation <ref type="bibr" target="#b2">[3]</ref> and healthcare <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> are currently undergoing a process of digital transformation, challenging established processes with machine learning-driven approaches. The expanding demand is highlighted by the Gartner report <ref type="bibr" target="#b5">[6]</ref>, claiming that organizations expect to double the number of machine learning (ML) projects within a year.</p><p>However, 75 to 85 percent of practical ML projects currently do not match their sponsors' expectations, according to surveys of leading technology companies <ref type="bibr" target="#b6">[7]</ref>. Fischer et al. <ref type="bibr" target="#b7">[8]</ref> name data and software quality among others as the key challenges in the machine learning life cycle. Another reason is the lack of guidance through standards and development process models arXiv:2003.05155v2 [cs.LG] 24 Feb 2021 specific to ML applications. Industrial organizations, in particular, rely heavily on standards to guarantee a consistent quality of their products or services. A Japanese industry Consortium (QA4AI) was founded to address those needs <ref type="bibr" target="#b8">[9]</ref>.</p><p>Due to the lack of a process model for ML applications, many project organizations rely on alternative models that are closely related to ML, such as, the Cross-Industry Standard Process model for Data Mining (CRISP-DM) <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. It is grounded on industrial data mining experience <ref type="bibr" target="#b11">[12]</ref> and is considered most suitable for industrial projects amongst related process models <ref type="bibr" target="#b12">[13]</ref>. In fact, CRISP-DM has become the de-facto industry standard <ref type="bibr" target="#b13">[14]</ref> process model for data mining, with an expanding number of applications <ref type="bibr" target="#b14">[15]</ref>, e.g., in quality diagnostics <ref type="bibr" target="#b15">[16]</ref>, marketing <ref type="bibr" target="#b16">[17]</ref>, and warranty <ref type="bibr" target="#b17">[18]</ref>.</p><p>However, we have identified two major shortcomings of CRISP-DM: First, CRISP-DM focuses on data mining and does not cover the application scenario of ML models inferring real-time decisions over a long period of time (see fig. <ref type="figure">1</ref>). The ML model has to be adaptable to a changing environment or the model's performance will degrade over time, such that, a permanent monitoring and maintaining of the ML model is required after the deployment.</p><p>Second, and more worrying, CRISP-DM lacks guidance on quality assurance methodology. This oversight is particularly evident in comparison to standards in the area of information technology <ref type="bibr" target="#b18">[19]</ref> but also apparent in alternative process models for data mining <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. In our definition, quality is not only defined by the product's fitness for its purpose <ref type="bibr" target="#b13">[14]</ref>, but the quality of the task executions in any phase during the development of a ML application. This ensures that errors are caught as early as possible to minimize costs in the later stages during the development. The initial effort and cost to perform the quality assurance methodology is expected to outbalance the risk of fixing errors in a later state, that are typically more expensive due to increased project complexity <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Our process model follows the principles of CRISP-DM, in particular by keeping the model industry and application neutral, but is modified to the particular requirements of ML applications and proposes quality assurance methodology that became industry best practice. Our contributions focus primarily on the technical tasks needed to produce evidence that every step in the development process is of sufficient quality to warrant the adoption into business processes.</p><p>The following second section describes the related work and ongoing research in the development of process models for machine learning applications. In the third chapter, the tasks and quality assurance methodology are introduced for each process phase. Finally, a conclusion and an outlook are given in the fourth chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CRISP-DM defines a reference framework for carrying out data mining projects and sets out activities to be performed to complete a product or service. The activities are organized in six phases (see table <ref type="table">1</ref>). The successful completion of a phase initiates the execution of the subsequent activity. CRISP-DM includes iterations of revisiting previous steps until success or completion criteria are met. It can be therefore characterized as a waterfall life cycle with backtracking <ref type="bibr" target="#b19">[20]</ref>. During the development of applications, processes and tasks to be performed can be derived from the standardized process model. Methodology instantiates these tasks, i.e. stipulates how to do a task (or how it should be done).</p><p>For each activity, CRISP-DM defines a set of (generic) tasks that are stable and general. Hereby, tasks are called stable when they are designed to keep the process model up to date with new modeling techniques to come and general when they are intended to cover many possible project scenarios. CRISP-DM has been specialized, e.g., to incorporate temporal data mining (CRISP-TDM; <ref type="bibr" target="#b23">[24]</ref>), null-hypothesis driven confirmatory data mining (CRISP-DM0; Difference between data mining processes and machine learning applications. A) In the data mining process information is directly extracted from data to find pattern und gain knowledge. B) A machine learning application consists of two steps. A machine learning model on data is trained and applied to perform inference on new data. Note that the model itself can be studied to gain insight within a knowledge discovery process. <ref type="bibr" target="#b24">[25]</ref>), evidence mining (CRISP-EM; <ref type="bibr" target="#b25">[26]</ref>), and data mining in the healthcare (CRISP-MED-DM; <ref type="bibr" target="#b26">[27]</ref>).</p><p>Complementary to CRISP-DM, process models for ML applications have been proposed <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> (see Table <ref type="table">1</ref>). Amershi et al. <ref type="bibr" target="#b27">[28]</ref> conducted an internal study at Microsoft on challenges of ML projects and derived a process model with nine different phases. However, their process model lacks quality assurance methodology and does not cover the business needs. Breck et al. <ref type="bibr" target="#b28">[29]</ref> proposed 28 specific tests to quantify issues in the ML pipeline to reduce the technical debt <ref type="bibr" target="#b29">[30]</ref> of ML applications. These tests estimate the production readiness of a ML application, i.e., the quality of the application in our context. However, their tests do not completely cover all project phases, e.g., excluding the business understanding activity. Practical experiences reveal that business understanding is a necessary first step that defines the success criteria and the feasibility for the subsequent tasks. Without considering the business needs, the ML objectives might be defined orthogonal to the business objectives and causes to spend a great deal of effort producing the rights answers to the wrong questions.</p><p>To our knowledge, Marbán et al. <ref type="bibr" target="#b19">[20]</ref> were the first to consider quality in the context of process models for data mining. Borrowing ideas from software development, their work suggests creating traceability, test procedures, and test data for challenging the product's fitness for its purpose during the evaluation phase.</p><p>We address these issues by devising a process model for the development of practical ML applications. In addition, we will provide a curated list of references for an in-depth analysis on the specific tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Quality Assurance in Machine Learning Projects</head><p>We propose a process model that we call CRoss-Industry Standard Process model for the development of Machine Learning applications with Quality assurance methodology (CRISP-ML(Q)) to highlight its compatibility to CRISP-DM. It is designed for the development of machine applications i.e. application scenarios where a ML model is deployed and maintained as part of a product or service (see fig. <ref type="figure">1</ref>).</p><p>As a first contribution, quality assurance methodology is introduced in each phase and task of the process model (see fig. <ref type="figure" target="#fig_2">2</ref>). The quality methodology serves to mitigate risks that affect the success and efficiency of the machine learning application. As a second contribution, CRISP-ML(Q) covers a monitoring and maintenance phase to address risks of model degradation in a changing environment. This extends the scope of the process model as compared to CRISP-DM, see Table <ref type="table">1</ref>. Moreover, business and data understanding are merged into a single phase because industry practice has taught us that these two activities, which are separate in CRISP-DM, are strongly intertwined, since business objectives can be derived or changed based on available data (see Table <ref type="table">1</ref>). A similar approach has been outlined in the W-Model <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRISP-ML(Q)</head><p>CRISP-DM Amershi et al. <ref type="bibr">[</ref>28] Breck et al. [29] Business &amp; Data Understanding Business Understanding Requirements -Data Understanding Collection Data Data Preparation Data Preparation Cleaning Infrastructure Labeling Feature Engineering Modeling Modeling Training Model Evaluation Evaluation Evaluation -Deployment Deployment Deployment -Monitoring &amp; Maintenance -Monitoring Monitoring Table 1: Comparing different process models for DM and ML projects. Business and data understanding phases are merged in CRISP-ML(Q) and a separate maintenance phase is introduced in comparison to CRISP-DM. Amershi et al. [28] and Breck et al. [29] lack the business understanding phase. Deep red color highlight data and petrol blue color model related phases.</p><p>In what follows, we describe selected tasks from CRISP-ML(Q) and propose quality assurance methodology to determine whether these tasks were performed according to current standards from industry best practice and academic literature, which have proven to be general and stable and are suitable to mitigate the task specific risks. The selection reflects tasks and methods that we consider the most important.</p><p>The flow chart in fig. <ref type="figure" target="#fig_2">2</ref> explains the CRISP-ML(Q) approach for quality assurance. Requirements and constraints define the objectives of a generic phase, instantiate specific steps and tasks and identify risks, that can affect the efficiency and success of the ML application. If risks aren't feasible, appropriate quality assurance methods are chosen to mitigate risks in an iterative approach using guidelines and checklists. While general risk management has diverse disciplines <ref type="bibr" target="#b18">[19]</ref>, this approach focuses on risks that affect the efficiency and success of the ML application and require technical tasks for risk mitigation.</p><p>Note that the processes and quality measures in this document are not designed for safety-critical systems. Safety-critical systems might require different or additional processes and quality measures.</p><p>yes Instantiate step and task Identify risks Define requirements and constraints Choose quality assurance method Start CRISP-ML(Q) phase Mitigate risks no risks feasable ? Phase finished ? no yes Start next phase </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Business and Data Understanding</head><p>The initial phase is concerned with tasks to define the business objectives and translate it to ML objectives, to collect and verify the data quality and to finaly assess the project feasibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Define the Scope of the ML Application</head><p>CRISP-DM names the data scientist responsible to define the scope of the project. However, in daily business, the separation of domain experts and data scientists carries the risk, that the application will not satisfy the business needs. Moreover, the availability of training samples will to a large extent influence the feasibility of the data-based application <ref type="bibr" target="#b27">[28]</ref>. It is, therefore, best practice to merge the requirements of the business unit with ML requirements while keeping in mind data related constraints in a joint step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Success Criteria</head><p>We propose to measure the success criteria of a ML project on three different levels: the business success criteria, the ML success criteria and the economic success criteria. According to the IEEE standard for developing software life cycle processes <ref type="bibr" target="#b18">[19]</ref>, the requirement measurable is one of the essential principles of quality assurance methodology. In addition, each success criterion has to be defined in alignment to each other and with respect to the overall system requirements <ref type="bibr" target="#b31">[32]</ref> to prevent contradictory objectives.</p><p>Business Success Criteria: Define the purpose and the success criteria of the ML application from a business point of view. For example, if an ML application is planned for a quality check in production and is supposed to outperform the current manual failure rate of 3%, the business success criterion could be derived as e.g. "failure rate less than 3%".</p><p>ML Success Criteria: Translate the business objective into ML success criteria (see table <ref type="table">2</ref>). It is advised to define a minimum acceptable level of performance to meet the business goals (e.g. for a Minimal Viable Product (MVP)) In the mentioned example, the minimal success criterion is defined as "accuracy greater 97%", but data scientists might optimize further, for example, the true-positive rate to miss items with quality issues.</p><p>Economic Success Criteria: It is best practice to add an economic success criterion in the form of a Key Performance Indicator (KPI) to the project. A KPI is an economical measure for the relevance of the ML application. In the mentioned example, a KPI can be defined as "cost savings with automated quality check per part".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Feasibility</head><p>Checking the feasibility before setting up the project is considered best practice for the overall success of the ML approach <ref type="bibr" target="#b32">[33]</ref> and can minimize the risk of premature failures due to false expectations. A feasibility test of the ML application should assess the situation and whether further development should be pursued. It is crucial, that the assessment includes the availability, size and quality of the training sample set. In practice, a major source of project delays is the lack of data availability (see section 3.1.4). A small sample size carries the risk of low performance on out-of-sample data <ref type="bibr" target="#b33">[34]</ref>. The risk might be mitigated by e.g. adding domain knowledge or increasing data quality. However, if the sample size is not sufficient the ML project should be terminated or put on hold at this stage.</p><p>Applicability of ML technology: Literature search for either similar applications on a similar domain or similar methodological approaches on a different domain could assess the applicability of the ML technology. It is common to demonstrate the feasibility of a ML application with a proof of concept (PoC) when the ML algorithm is used for the first time in a specific domain. If a PoC already exists, setting up a software project that focuses on the deployment directly is more efficient, e.g. in case of yet another price estimation of used cars <ref type="bibr" target="#b34">[35]</ref>.</p><p>Legal constraints: It is beyond the scope of this paper to discuss legal issues but they are essential for any business application <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Legal constraints are frequently augmented by ethical and social considerations like fairness and trust <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>.</p><p>Requirements on the application: The success criteria that have been defined in section 3.1.2 have to be augmented with requirements that arise from running the application in the target domain or if not accessible an assumed target domain <ref type="bibr" target="#b31">[32]</ref>. The requirements include robustness, scalability, explainability and resource demand and are used for the development and verification in later phases (see section 3.3). The challenge during the development is to optimize the success criteria while not violating the requirements and constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Data Collection</head><p>Costs and time is needed to collect a sufficient amount of consistent data by preparing and merging data from different sources and different formats (see section 3.2). A ML project might be delayed until the data is collected or could even be stopped if the collection of data of sufficient quality (see section 3.1.5) is not feasible.</p><p>Data version control: Collecting data is not a static task but rather an iterative task. Modification on the data set (see section 3.2) should be documented to mitigate the risk of obtaining irreproducible or wrong results. Version control on the data is one of the essential tools to assure reproducibility and quality as it allows to track errors and unfavorable modifications during the development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5.">Data Quality Verification</head><p>The following three tasks examine whether the business and ML objectives can be achieved with the given quality of the available data. A ML project is doomed to fail if the data quality is poor. The lack of a certain data quality will trigger the previous data collection task (see section 3.1.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data description:</head><p>The data description forms the basis for the data quality verification. A description and an exploration of the data is performed to gain insight about the underlying data generation process. The data should be described on a meta-level and by their statistical properties. Furthermore, a technically well funded visualization of the data should help to understand the data generating process <ref type="bibr" target="#b40">[41]</ref>. Information about format, units and description of the input signals is expanded by domain knowledge.</p><p>Data requirements: The data requirements can be defined either on the meta-level or directly in the data and should state the expected conditions of the data, i.e. whether a certain sample is plausible. The requirements can be, e.g., the expected feature values (a range for continuous features or a list for discrete features), the format of the data and the maximum number of missing values. The bounds of the requirements has to be defined carefully to include all possible real world values but discard non-plausible data. Data that does not satisfy the expected conditions could be treated as anomalies and have to be evaluated manually or excluded automatically. To mitigate the risk of anchoring bias in the definition phase discussing the requirements with a domain expert is advised <ref type="bibr" target="#b28">[29]</ref>. Documentation of the data requirements could be done in the form of a schema <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Data verification: The initial data, added data but also the production data has to be checked according to the requirements (see section 3.6). In cases where the requirements are not met, the data will be discarded and stored for further manual analysis. This helps to reduce the risk of decreasing the performance of the ML application through adding low-quality data and helps to detect varying data distributions or unstable inputs. To mitigate the risk of insufficient representation of extreme cases, it is best practice to use data exploration techniques to investigate the sample distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6.">Review of Output Documents</head><p>The Business &amp; Data Understanding phase delivers the scope for the development (section 3.1.3), the success criteria (section 3.1.2) of a ML application and a data quality verification report (section 3.1.5) to approve the feasibility of the project. The output documents need to be reviewed to rank the risks and define the next tasks. If certain quality criteria are not met, re-iterations of previous tasks are possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Preparation</head><p>Building on the experience from the preceding data understanding phase, data preparation serves the purpose of producing a data set for the subsequent modeling phase. However, data preparation is not a static phase and backtracking circles from later phases are necessary if, for example, the modeling phase or the deployment phase reveal erroneous data. To path the way towards ML lifecycle in a later phase, methods for data preparation that are suitable for automation as demonstrated by Fischer et al. <ref type="bibr" target="#b7">[8]</ref> are preferable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Select Data</head><p>Feature selection: Selecting a good data representation based on the available measurements is one of the challenges to assure the quality of the ML application. It is best practice to discard underutilized features as they provide little to none modeling benefit but offer possible loopholes for errors i.e. instability of the feature during the operation of the ML application <ref type="bibr" target="#b29">[30]</ref>. In addition, the more features are selected the more samples are necessary. Intuitively an exponentially increasing number of samples for an increasing number of features is required to prevent the data from becoming sparse in the feature space. This is termed as the curse of dimensionality <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. Thus, it is best practice to select just necessary features. A checklist for the feature selection task is given in <ref type="bibr" target="#b45">[46]</ref>. Note that data often forms a manifold of lower dimensions in the feature space and models have to learn this respectively <ref type="bibr" target="#b46">[47]</ref>. Feature selection methods can be separated into three categories: 1) filter methods select features from data without considering the model, 2) wrapper methods use a learning model to evaluate the significance of the features and 3) embedded methods combines the feature selection and the classifier construction steps. A detailed explanation and in-depth analysis on the feature selection problem are given in <ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref>. Feature selection could carry the risk of selection bias but could be reduced when the feature selection is performed within the cross-validation of the model (see section 3.3) to account for all possible combinations <ref type="bibr" target="#b52">[52]</ref>.</p><p>However, the selection of the features should not be relied purely on the validation and test error but should be analyzed by a domain expert as potential biases might occur due to spurious correlation in the data. Lapuschkin et al. <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b54">54]</ref> showed that classifiers could exploit spurious correlations, here the copyright tag on the horse class, to obtain a remarkable test performance and, thus, fakes a false sense of generalization. In such cases, explanation methods <ref type="bibr" target="#b55">[55]</ref> could be used to highlight the significance of features (see section 3.4) and analyzed from a human's perspective.</p><p>Data selection: Discarding samples should be well documented and strictly based on objective quality criteria. However, certain samples might not satisfy the necessary quality i.e. doesn't satisfy the requirements defined in section 3.1.5 and are not plausible and, thus, should be removed from the data set.</p><p>Unbalanced Classes: In cases of unbalanced classes, where the number of samples per class is skewed, different sampling strategies could improve the results. Over-sampling of the minority class and/or under-sampling of the majority class <ref type="bibr" target="#b56">[56]</ref><ref type="bibr" target="#b57">[57]</ref><ref type="bibr" target="#b58">[58]</ref><ref type="bibr" target="#b59">[59]</ref> have been used. Over-sampling increases the importance of the minority class but could result in overfitting on the minority class. Under-Sampling by removing data points from the majority class has to be done carefully to keep the characteristics of the data and reduce the chance of introducing biases. However, removing points close to the decision boundary or multiple data points from the same cluster should be avoided. Comparing the results of different sampling techniques' reduces the risk of introducing bias to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Clean Data</head><p>Noise reduction: The gathered data often includes, besides the predictive signal, noise and unwanted signals from other sources. Signal processing filters could be used to remove the irrelevant signals from the data and improve the signal-to-noise ratio <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b61">61]</ref>. However, filtering the data should be documented and evaluated because of the risk that an erroneous filter could remove important parts of the signal in the data.</p><p>Data imputation: To get a complete data set, missing, NAN and special values could be imputed with a model readable value. Depending on the data and ML task the values are imputed by mean or median values, interpolated, replaced by a special value symbol <ref type="bibr" target="#b62">[62]</ref> (as the pattern of the values could be informative), substituted by model predictions <ref type="bibr" target="#b63">[63]</ref>, matrix factorization <ref type="bibr" target="#b64">[64]</ref> or multiple imputations <ref type="bibr" target="#b65">[65]</ref><ref type="bibr" target="#b66">[66]</ref><ref type="bibr" target="#b67">[67]</ref> or imputed based on a convex optimization problem <ref type="bibr" target="#b68">[68]</ref>. To reduce the risk of introducing substitution artifacts, the performance of the model should be compared between different imputation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Construct Data</head><p>Feature engineering: New features could be derived from existing ones based on domain knowledge. This could be, for example, the transformation of the features from the time domain into the frequency domain, discretization of continuous features into bins or augmenting the features with additional features based on the existing ones. In addition, there are several generic feature construction methods, such as clustering <ref type="bibr" target="#b69">[69]</ref>, dimensional reduction methods such as Kernel-PCA <ref type="bibr" target="#b70">[70]</ref> or auto-encoders <ref type="bibr" target="#b71">[71]</ref>. Nominal features and labels should be transformed into a one-hot encoding while ordinal features and labels are transformed into numerical values. However, the engineered features should be compared against a baseline to assess the utility of the feature. Underutilized features should be removed. Models that construct the feature representation as part of the learning process, e.g. neural networks, avoid the feature engineering steps <ref type="bibr" target="#b72">[72]</ref>.</p><p>Data augmentation: Data augmentation utilizes known invariances in the data to perform a label preserving transformation to construct new data. The transformations could either be performed in the feature space <ref type="bibr" target="#b57">[57]</ref> or input space, such as applying rotation, elastic deformation or Gaussian noise to an image <ref type="bibr" target="#b73">[73]</ref>. Data could also be augmented on a meta-level, such as switching the scenery from a sunny day to a rainy day. This expands the data set with additional samples and allows the model to capture those invariances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Standardize Data</head><p>File format: Some ML tools require specific variable or input types (data syntax). Indeed in practice, the comma separated values (CSV) format is the most generic standard (RFC 4180). ISO 8000 recommends the use of SI units according to the International System of Quantities. Defining a fix set of standards and units, helps to avoid the risks of errors in the merging process and further in detecting erroneous data (see section 3.1.5).</p><p>Normalization: Without proper normalization, the features could be defined on different scales and might lead to strong bias to features on larger scales. In addition, normalized features lead to faster convergence rates in neural networks than without <ref type="bibr" target="#b74">[74,</ref><ref type="bibr" target="#b75">75]</ref>. Note that the normalization, applied to the training set has to be applied also to the test set using the same normalization parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Modeling</head><p>The choice of modeling techniques depends on the ML and the business objectives, the data and the boundary conditions of the project the ML application is contributing to. The requirements and constraints that have been defined in section 3.1 are used as inputs to guide the model selection to a subset of appropriate models. The goal of the modeling phase is to craft one or multiple models that satisfy the given constraints and requirements. An outline of the modeling phase is depicted in fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Universe of all models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset of appropriate models</head><p>Performance Robustness</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalability Explainability Model Complexity</head><p>Resource Demand Ra nk ing Model Figure <ref type="figure">3</ref>. An outline of the modeling phase. Only a subset of models fulfill the constraints and requirements defined in section 3.1 and section 3.1.3 and have to be evaluated using quality measures (see table <ref type="table">2</ref>)</p><p>. Literature research on similar problems: It is best practice to screen the literature (e.g. publications, patents, internal reports) for a comprehensive overview on similar ML tasks, since ML has become an established tool for a wide number of applications. New models can be based on published insights and previous results can serve as performance baselines.</p><p>Define quality measures of the model: The modeling strategy has to have multiple objectives in mind <ref type="bibr" target="#b76">[76]</ref>. We suggest to evaluate the models on at least six complementary properties (see fig. <ref type="figure">3</ref>). Besides a performance metric, soft measures such as robustness, explainability, scalability, resource demand and model complexity have to be evaluated (see table <ref type="table">2</ref>). The measures can be weighted differently depending on the application. In practical application, explainability <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b78">78]</ref> or robustness might be valued more than accuracy. Additionally, the model's fairness <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> or trust might have to be assessed and mitigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance</head><p>The model's performance on unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness</head><p>The model's resiliency to inconsistent inputs and to failures in the execution environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalability</head><p>The model's ability to scale to high data volume in the production system.</p><p>Explainability The model's direct or post-hoc explainability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Complexity</head><p>The model's capacity should suit the data complexity.</p><p>Resource Demand The model's resource demand for deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2: Quality measure of machine learning models</head><p>Model Selection: There are plenty of ML models and introductory books on classical methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b79">79]</ref> and Deep Learning <ref type="bibr" target="#b72">[72]</ref> can be used to compare and understand their characteristics. The model selection depends on the data and has to be tailored to the problem.</p><p>There is no such model that performs the best on all problem classes (No Free Lunch Theorem for ML <ref type="bibr" target="#b80">[80]</ref>). It is best practice to start with models of lower capacity, which can serve as baseline, and gradually increase the capacity. Validating each step assures its benefit and avoid unnecessary complexity of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporate domain knowledge:</head><p>In practice, a specialized model for a specific task performs better than a general model for all possible tasks. However, adapting the model to a specific problem involves the risk of incorporating false assumption and could reduce the solution space to a non-optimal subset. Therefore, it is best practice to validate the incorporated domain knowledge in isolation against a baseline. Adding domain knowledge should always increase the quality of the model, otherwise, it should be removed to avoid false bias.</p><p>Model training: The trained model depends on the learning problem and as such are tightly coupled. The learning problem contains an objective, optimizer, regularization and cross-validation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b72">72]</ref>. The objective of the learning problem depends on the application. Different applications value different aspects and have to be tweaked in alignment with the business success criteria. The objective is a proxy to evaluate the performance of the model. The optimizer defines the learning strategy and how to adapt the parameters of the model to improve the objective. Regularization which can be incorporated in the objective, optimizer and in the model itself is needed to reduce the risk of overfitting and can help to find unique solutions. Cross-validation is performed for feature selection, to optimize the hyper-parameters of the model and to test its generalization property to unseen data <ref type="bibr" target="#b81">[81]</ref>. Cross-validation <ref type="bibr" target="#b44">[45]</ref> is based on a splitting of historical data in training, validation and testing data, where the latter is used as a proxy for the target environment <ref type="bibr" target="#b82">[82]</ref>. Frameworks such as Auto-ML <ref type="bibr" target="#b83">[83,</ref><ref type="bibr" target="#b84">84]</ref> or Neural Architecture Search <ref type="bibr" target="#b85">[85]</ref> enables to partly automatize the hyper-parameters optimization and the architecture search.</p><p>Using unlabeled data and pre-trained models: Labeling data could be very expensive and might limit the available data set size. Unlabeled data might be exploited in the training process, e.g. by performing unsupervised pre-training <ref type="bibr" target="#b86">[86,</ref><ref type="bibr" target="#b87">87]</ref> and semi-supervised learning algorithms <ref type="bibr" target="#b88">[88,</ref><ref type="bibr" target="#b89">89]</ref>. Complementary, transfer learning could be used to pre-train the network on a proxy data (e.g. from simulations) that resembles the original data to extract common features <ref type="bibr" target="#b90">[90]</ref>.</p><p>Model Compression: Compression or pruning methods could be used to obtain a more compact model. In kernel methods low rank approximations of the kernel matrix is an essential tool to tackle large scale learning problems <ref type="bibr" target="#b91">[91,</ref><ref type="bibr" target="#b92">92]</ref>. Neural Networks use a different approach <ref type="bibr" target="#b93">[93]</ref> by either pruning the network weights <ref type="bibr" target="#b94">[94]</ref> or applying a compression scheme on the network weights <ref type="bibr" target="#b95">[95]</ref>.</p><p>Ensemble methods: Ensemble methods train multiple models to perform the decision based on the aggregate decisions of the individual models. The models could be of different types or multiple instantiations of one type. This results in a more fault-tolerant system as the error of one model could be absorbed by the other models. Boosting, Bagging or Mixture of Experts are mature techniques to aggregate the decision of multiple models <ref type="bibr" target="#b96">[96]</ref><ref type="bibr" target="#b97">[97]</ref><ref type="bibr" target="#b98">[98]</ref>. In addition, ensemble models are used to compute uncertainty estimates and can highlight areas of low confidence <ref type="bibr" target="#b99">[99,</ref><ref type="bibr" target="#b100">100]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Assure reproducibility</head><p>A major principle of scientific methods and the characteristics of robust ML applications is reproducibility. However, ML models are difficult to reproduce due to the mostly non-convex and stochastic training procedures and randomized data splits. It has been proposed to distinguish reproducibility on two different levels. First, one has to assure that the method itself is reproducible and secondly its results <ref type="bibr" target="#b101">[101]</ref>.</p><p>Method reproducibility: This task aims at reproducing the model from an extensive description or sharing of the used algorithm, data set, hyper-parameters and runtime environment (e.g. software versions, hardware and random seeds <ref type="bibr" target="#b102">[102]</ref>). The algorithm should be described in detail i.e. with (pseudo) code and on the meta-level including the assumptions.</p><p>Result reproducibility: It is best practice to validate the mean performance and assess the variance of the model on different random seeds <ref type="bibr" target="#b103">[103,</ref><ref type="bibr" target="#b104">104]</ref>. Reporting only the top performance of the model <ref type="bibr" target="#b103">[103,</ref><ref type="bibr" target="#b105">105]</ref> is common but dubious practice. Large performance variances indicate the sensitivity of the algorithm and question the robustness of the model.</p><p>Experimental Documentation: Keeping track of the changed model's performance and its causes by precedent model modifications allows model comprehension by addressing which modifications were beneficial and improve the overall model quality. The documentation should contain the listed properties in the method reproducibility task. Tool-based approach on version control and meta-data handling while experimenting on ML models and hyper-parameters exist <ref type="bibr" target="#b106">[106]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation</head><p>Validate performance: A risk occurs when information from a test set leak into the validation or even training set. Hence, it is best practice to hold back an additional test set, which is disjoint from the validation and training set, stored only for a final evaluation and never shipped to any partner to be able to measure the performance metrics (blind-test). The test set should be assembled and curated with caution and ideally by a team of experts that are capable to analyze the correctness and ability to represent real cases. In general, the test set should cover the whole input distribution and consider all invariances, e.g. transformations of the input that do not change the label, in the data. Another major risk is that the test set cannot cover all possible inputs due to the large input dimensionality or rare corner cases, i.e. inputs with low probability of occuring <ref type="bibr" target="#b107">[107]</ref><ref type="bibr" target="#b108">[108]</ref><ref type="bibr" target="#b109">[109]</ref>. Extensive testing reduces this risk <ref type="bibr" target="#b82">[82]</ref>. It is recommended to separate the teams and the procedures collecting the training and the test data to erase dependencies and avoid methodology dependence. Additionally, it is recommended to perform a sliced performance analysis to highlight weak performance on certain classes or time slices.</p><p>Determine robustness: A major risk occurs if ML applications are not robust to perturbed, e.g. noisy or wrong, or even designed adversarial input data as show by Chan-Hon-Tong <ref type="bibr" target="#b110">[110]</ref>. This requires methods to statistically estimate the model's local and global robustness. One approach is adding different kinds of noisy or falsified input to the data or varying the hyperparameters to characterize the model's generalization ability. Formal verification approaches <ref type="bibr" target="#b31">[32]</ref> and robustness validation methods using cross-validation techniques on historical data <ref type="bibr" target="#b82">[82]</ref> exist.</p><p>The model's robustness should match the quality claims made in table <ref type="table">2</ref>.</p><p>Increase explainability for ML practitioner &amp; end user: Explainability of a model helps to find errors and allows strategies, e.g. by enriching the data set, to improve the overall performance <ref type="bibr" target="#b111">[111]</ref>. In practice, inherently interpretable models are not necessary inferior to complex models in case of structured input data with meaningful features <ref type="bibr" target="#b77">[77]</ref>. Thrun et al. <ref type="bibr" target="#b112">[112]</ref> show the advantages of the glass box model Explainable Boosting Machine, that visualizes feature-wise contributions to the predictions at comparable performance to common models. To achieve explainability and gain a deeper understanding of what a model has already learned and to avoid spurious correlations <ref type="bibr" target="#b54">[54]</ref>, it is best practice to carefully observe the features which impact the model's prediction the most and check whether they are plausible from a domain experts' point of view <ref type="bibr" target="#b113">[113]</ref><ref type="bibr" target="#b114">[114]</ref><ref type="bibr" target="#b115">[115]</ref>. Moreover, case studies have shown that explainability helps to increase trust and users' acceptance <ref type="bibr" target="#b116">[116]</ref> and could guide humans in ML assisted decisions <ref type="bibr" target="#b78">[78]</ref>. Unified frameworks to explore model explainabilty are available (e.g. <ref type="bibr" target="#b117">[117,</ref><ref type="bibr" target="#b118">118]</ref>).</p><p>Compare results with defined success criteria: Finally, domain and ML experts have to decide if the model can be deployed. Therefore, it is best practice to document the results of the evaluation phase and compare them to the business and ML success criteria defined in section 3.1.2. If the success criteria are not met, one might backtrack to earlier phases (modeling or even data preparation) or stop the project. Identified limitations of robustness and explainability during evaluation might require an update of the risk assessment (e.g. Failure Mode and Effects Analysis (FMEA)) and might also lead to backtracking to the modeling phase or stopping the project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Deployment</head><p>The deployment phase of a ML model is characterized by its practical use in the designated field of application.</p><p>Define inference hardware: Choose the hardware based on the requirements defined in section 3.1.3 or align with an existing hardware. While cloud services offer scalable computation resources, embedded system have hard constraints. ML specific options are e.g. to optimize towards the target hardware <ref type="bibr" target="#b119">[119]</ref> regarding CPU and GPU availability, to optimize towards the target operation system (demonstrated for Android and iOS by Sehgal and Kehtarnavaz <ref type="bibr" target="#b120">[120]</ref>) or to optimize the ML workload for a specific platform <ref type="bibr" target="#b121">[121]</ref>. Monitoring and maintenance (see section 3.6) have to be considered in the overall architecture.</p><p>Model evaluation under production condition: The risk persists that the production data does not resemble the training data. Previous assumptions on the training data might not hold in production and the hardware that gathered the data might differ. Therefore it is best practice to evaluate the performance of the model under incrementally increasing production conditions by iteratively running the tasks in section 3.4. On each incremental step, the model has to be calibrated to the deployed hardware and the test environment. This allows identifying wrong assumptions on the deployed environment and the causes of model degradation. Domain adaptation techniques can be applied <ref type="bibr" target="#b122">[122,</ref><ref type="bibr" target="#b123">123]</ref> to enhance the generalization ability of the model. This step will also give a first indication whether the business and economic success criteria, which was defined in section 3.1.2, could be met.</p><p>Assure user acceptance and usability: Even after passing all evaluation steps, there might be the risk that the user acceptance and the usability of the model is underwhelming. The model might be incomprehensible and or does not cover corner cases. It is best practice to build a prototype and run an field test with end users <ref type="bibr" target="#b82">[82]</ref>. Examine the acceptance, usage rate and the user experience. A user guide and disclaimer shall be provided to the end users to explain the system's functionality and limits.</p><p>Minimize the risks of unforeseen errors: The risks of unforeseen errors and outage times could cause system shutdowns and a temporary suspension of services. This could lead to user complaints and the declining of user numbers and could reduce the revenue. A fall-back plan, that is activated in case of e.g. erroneous model updates or detected bugs, can help to tackle the problem. Options are to roll back to a previous version, a pre-defined baseline or a rule-based system. A second option to counteract unforeseen errors is to implement software safety cages that control and limit the outputs of the ML application <ref type="bibr" target="#b124">[124]</ref> or even learn safe regions in the state space <ref type="bibr" target="#b125">[125]</ref>.</p><p>Deployment strategy: Even though the model is evaluated rigorously during each previous step, there is the risk that errors might be undetected through the process. Before rolling out a model, it is best practice to setup an e.g. incremental deployment strategy that includes a pipeline for models and data <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b126">126]</ref>. When cloud architectures are used, strategies can often be aligned on general deployment strategies for cloud software applications <ref type="bibr" target="#b127">[127]</ref>. The impact of such erroneous deployments and the cost of fixing errors should be minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Monitoring and Maintenance</head><p>With the expansion from knowledge discovery to data-driven applications to infer real-time decisions, ML models are used over a long period and have a life cycle which has to be managed. The risk of not maintaining the model is the degradation of the performance over time which leads to false predictions and could cause errors in subsequent systems. The main reason for a model to become impaired over time is rooted in the violation of the assumption that the training data and the input data for inference come from the same distribution. The causes of the violations are: The hardware that the model is deployed on and the sensor hardware will age over time. Wear parts in a system will age and friction characteristics of the system might change. Sensors get noisier or fail over time. This will shift the domain of the system and has to be adapted by the model or by retraining it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>System updates: Updates on the software or hardware of the system can cause a shift in the environment. For example, the units of a signal got changed during an update. Without notifications, the model would use this scaled input to infer false predictions.</p><p>After the underlying problem is known, we can formulate the necessary methods to circumvent stale models and assure the quality. We propose two sequential tasks in the maintenance phase to assure or improve the quality of the model. In the monitor task, the staleness of the model is evaluated and returns whether the model has to be updated or not. Afterward, the model is updated and evaluated to gauge whether the update was successful.</p><p>Monitor: Baylor et al. <ref type="bibr" target="#b76">[76]</ref> proposes to monitor all input signals and notify when an update has occurred. Therefore, statistics of the incoming data and the predicted labels can be compared to the statistics of the training data. Complementary, the schema defined in section 3.1.5 can be used to validate the correctness of the incoming data. Inputs that do not satisfy the schema can be treated as anomalies and denied by the model <ref type="bibr" target="#b76">[76]</ref>. Libraries exist to help implementing an automatic data validation system <ref type="bibr" target="#b42">[43]</ref>. If the labels of the incoming data are known e.g. in forecasting tasks, the performance of the model can be directly monitored and recorded. An equal approach can be applied to the outputs of the model that underlie a certain distribution if environment conditions are stable and can give an estimate on the number of actions performed when interacting with an environment <ref type="bibr" target="#b29">[30]</ref>. The monitoring phase also includes a comparison of the performance with the defined success criteria. Based on the monitoring results, it can be decided upon whether the model should be updated e.g. if input signals change significantly, the number of anomalies reaches a certain threshold or the performance has reached a lower bound. The decision whether the model has to be updated should consider the costs of updating the model and the costs resulting from erroneous predictions due to stale models.</p><p>Update: In the updating step, new data is collected to re-train the model under the changed data distribution. Consider that new data has to be labeled which could be very expensive. Instead of training a completely new model from scratch, it is advised to fine-tune the existing model to new data. It might be necessary to perform some of the modeling steps in section 3.3 to cope with the changing data distribution. Every update step has to undergo a new evaluation (section 3.4) before it can be deployed. The performance of the updated model should be compared against the previous versions and could give insights on the time scale of model degradation. It should be noted, that ML systems might influence their own behavior during updates due to direct, e.g. by influencing its future training data selection, or indirect, e.g. via interaction through the world, feedback loops <ref type="bibr" target="#b29">[30]</ref>. The risk of positive feedback loops causing system instability has to be addressed e.g. by not only monitoring but limiting the actions of the model.</p><p>In addition, as part of the deployment strategy, a module is needed that tracks the application usage and performance and handles several deployment strategies like A/B testing <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b126">126]</ref>. The module can e.g. be set up in form of a microservice <ref type="bibr" target="#b22">[23]</ref> or a directed graph <ref type="bibr" target="#b128">[128]</ref>. To reduce the risk of serving erroneous models, an automatic or human controlled fallback to a previous model needs to be implemented. The automation of the update strategy can be boosted up to a continuous training and continuous deployment of the ML application <ref type="bibr" target="#b76">[76]</ref> while covering the defined quality assurance methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>We have introduced CRISP-ML(Q), a process model for ML applications with quality assurance methodology, that helps organizations to increase efficiency and the success rate in their ML projects. It guides ML practitioners through the entire ML development life-cycle, stepping into the phases and tasks of the iterative process including maintenance and monitoring. Whenever tasks specific risks can be identified, we provide quality-oriented methods to mitigate those risks. All methods provided are considered best practices in ML projects in industry and academia.</p><p>Our survey is indicative of the existence of specialist literature, but its contributions are not covered in ML textbooks and are not part of the academic curriculum. Hence, novices to industry practice often lack a profound state-of-the-art knowledge to mitigate risks and ensure project success. Stressing quality assurance methodology is particularly important because many ML practitioners focus solely on improving the predictive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>An important future step on the basis of our and related work is the standardization of a process model. This would contribute to more successful ML projects and thus would have a major impact on the ML community <ref type="bibr" target="#b13">[14]</ref>.</p><p>Note that the process and quality measures in this work are not designed for safety-relevant systems. Their study and and the discussion of legal constrains are left to future work.</p><p>We encourage industry from automotive and other domains to implement CRISP-ML(Q) in their machine learning applications and contribute their knowledge to establish a CRoss-Industry Standard Process model for the development of machine learning applications with Quality assurance methodology in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure1. Difference between data mining processes and machine learning applications. A) In the data mining process information is directly extracted from data to find pattern und gain knowledge. B) A machine learning application consists of two steps. A machine learning model on data is trained and applied to perform inference on new data. Note that the model itself can be studied to gain insight within a knowledge discovery process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of the CRISP-ML(Q) approach for quality assurance. The flow chart shows the instantiation of one specific tasks in a development phase, and the dedicated steps to identify and mitigate risks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><figDesc>Non-stationary data distribution: Data distributions change over time and result in a stale training set and, thus, the characteristics of the data distribution are represented incorrectly by the training data. Either a shift in the features and/or in the labels are possible. This degrades the performance of the model over time. The frequency of the changes depends on the domain. Data of the stock market are very volatile whereas the visual properties of elephants won't change much over the next years. • Degradation of hardware:</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments: Special thanks to the internal Daimler AI community. We would like to thank <rs type="person">Miriam Hägele</rs>, <rs type="person">Lorenz Linhardt</rs>, <rs type="person">Simon Letzgus</rs>, <rs type="person">Danny Panknin</rs> and <rs type="person">Andreas Ziehe</rs> for proofreading the manuscript and the in-depth discussions.</p></div>
			</div>
			<div type="funding">
<div><p>Funding: This research was funded by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> for funding the project <rs type="projectName">AIAx -Machine Learning-driven Engineering</rs> (Nr. <rs type="grantNumber">01IS18048</rs>). K.-R. M. acknowledges partial financial support by the <rs type="funder">BMBF</rs> under Grants <rs type="grantNumber">01IS14013A-E</rs>, <rs type="grantNumber">01IS18025A</rs>, <rs type="grantNumber">01IS18037A</rs>, <rs type="grantNumber">01GQ1115</rs> and <rs type="grantNumber">01GQ0850</rs>; <rs type="funder">Deutsche Forschungsgesellschaft (DFG)</rs> under Grant <rs type="projectName">Math+</rs>, <rs type="grantNumber">EXC 2046/1</rs>, Project <rs type="projectName">ID</rs> <rs type="grantNumber">390685689</rs> and by the <rs type="funder">Technology Promotion (IITP)</rs> grant funded by the <rs type="funder">Korea government</rs> (No. <rs type="grantNumber">2017-0-00451</rs>, No. <rs type="grantNumber">2017-0-01779</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_evQTJ4C">
					<idno type="grant-number">01IS18048</idno>
					<orgName type="project" subtype="full">AIAx -Machine Learning-driven Engineering</orgName>
				</org>
				<org type="funding" xml:id="_H5GNnfC">
					<idno type="grant-number">01IS14013A-E</idno>
				</org>
				<org type="funding" xml:id="_GdWeawu">
					<idno type="grant-number">01IS18025A</idno>
				</org>
				<org type="funding" xml:id="_P9tYvPR">
					<idno type="grant-number">01IS18037A</idno>
				</org>
				<org type="funding" xml:id="_jXV9BZD">
					<idno type="grant-number">01GQ1115</idno>
				</org>
				<org type="funded-project" xml:id="_uhvFVRP">
					<idno type="grant-number">01GQ0850</idno>
					<orgName type="project" subtype="full">Math+</orgName>
				</org>
				<org type="funded-project" xml:id="_w9ZgttH">
					<idno type="grant-number">EXC 2046/1</idno>
					<orgName type="project" subtype="full">ID</orgName>
				</org>
				<org type="funding" xml:id="_DxgsBRc">
					<idno type="grant-number">390685689</idno>
				</org>
				<org type="funding" xml:id="_Akv6e7e">
					<idno type="grant-number">2017-0-00451</idno>
				</org>
				<org type="funding" xml:id="_vJzxzJU">
					<idno type="grant-number">2017-0-01779</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest. The funders had no role in the writing of the manuscript, or in the decision to publish the results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A cyber-physical systems architecture for industry 4.0-based manufacturing systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manufacturing letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="18" to="23" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How virtualization, decentralization and network building change the manufacturing landscape: An Industry 4.0 Perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brettel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friederichsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of mechanical, industrial science and engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="37" to="44" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autonomous driving in the real world: Experiences with Tesla autopilot and summon</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Burns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on automotive user interfaces and interactive vehicular applications</title>
		<meeting>the 8th international conference on automotive user interfaces and interactive vehicular applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Machine learning applications in cancer prognosis and prediction. Computational and structural biotechnology journal</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kourou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Exarchos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Exarchos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Karamouzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Fotiadis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="8" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<title level="m">Survey Analysis: AI and ML Development Strategies, Motivators and Adoption Challenges</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Artificial Intelligence: Localization Winners, Losers, Heroes, Spectators, and You</title>
		<author>
			<persName><forename type="first">Nimdzi</forename><surname>Insights</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Pactera EDGE</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AI System Engineering-Key Challenges and Lessons Learned</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ehrlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sobiezky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moser</surname></persName>
		</author>
		<idno type="DOI">10.3390/make3010004</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning and Knowledge Extraction</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="56" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Guidelines for quality assurance of machine learning-based artificial intelligence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matsuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SEKE2020: the 32nd International Conference on Software Engineering &amp; Knowledge Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="335" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">CRISP-DM 1.0 Step-by-step data mining guide</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khabaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reinartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shearer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wirth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>The CRISP-DM consortium</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CRISP-DM: Towards a standard process model for data mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on the Practical Application of Knowledge Discovery and Data Mining</title>
		<meeting>the Fourth International Conference on the Practical Application of Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The CRISP-DM Model: The New Blueprint for Data Mining</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shearer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Data Warehousing</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey of Knowledge Discovery and Data Mining process models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kurgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Musilek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of data mining and knowledge discovery process models and methodologies</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mariscal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Marbán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fernández</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0269888910000032</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge Eng. Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="137" to="166" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Future trends in data mining</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pryakhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="87" to="97" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quality Diagnostic Models for Packaging Manufacturing: An Industrial Data Mining Case Study</title>
		<author>
			<persName><forename type="first">N</forename><surname>De Abajo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Cuesta</surname></persName>
		</author>
		<author>
			<persName><surname>Ann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predictive modeling in automotive direct marketing: tools, experiences and open issues</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arndt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="398" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analysing Warranty Claims of Automobiles; An Application Description following the CRISP-DM Data Mining Process</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lindner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Computer Science Conference</title>
		<meeting>the Fifth International Computer Science Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Std 1074-1997</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Standard for Developing Software Life Cycle Processes</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Toward data mining engineering: A software engineering approach</title>
		<author>
			<persName><forename type="first">O</forename><surname>Marbán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Segovia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Menasalvas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fernández-Baizán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="87" to="107" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">SEMMA Data Mining Methodology</title>
		<author>
			<persName><surname>Sas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>SAS Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Implementation of Six Sigma to reduce cost of quality: a case study of automobile sector</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Surange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Failure Analysis and Prevention</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="282" to="294" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards Enterprise-Ready AI Deployments Minimizing the Risk of Consuming AI Models in Business Applications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Muthusamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slominski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ishakian</surname></persName>
		</author>
		<idno type="DOI">10.1109/AI4I.2018.8665685</idno>
	</analytic>
	<monogr>
		<title level="m">First International Conference on Artificial Intelligence for Industries (AI4I)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="108" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Extending CRISP-DM to incorporate temporal data mining of multidimensional medical data streams: A neonatal intensive care unit case study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Catley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tracy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">CRISP-DM0: A method to extend CRISP-DM to support null hypothesis driven confirmatory data mining. 1st Advances in Health Informatics Conference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcgregor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="96" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Specializing CRISP-DM for evidence mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Venter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Waal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Willers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP International Conference on Digital Forensics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="303" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CRISP Data Mining Methodology Extension for Medical Domain</title>
		<author>
			<persName><forename type="first">O</forename><surname>Niaksu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Baltic Journal of Modern Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="92" to="109" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Begel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<title level="m">Software Engineering for Machine Learning: A Case Study. International Conference on Software Engineering (ICSE 2019) -Software Engineering in Practice track</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The ML test score: A rubric for ML production readiness and technical debt reduction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1123" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hidden technical debt in machine learning systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Crespo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dennison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="2503" to="2511" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Learning in Automotive Software</title>
		<author>
			<persName><forename type="first">F</forename><surname>Falcini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitidieri Costanza</surname></persName>
		</author>
		<idno type="DOI">10.1109/MS.2017.79</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="56" to="63" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Open Problems in Engineering and Quality Assurance of Safety Critical Machine Learning Systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuwajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yasuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakae</surname></persName>
		</author>
		<idno>1812.03057</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Preliminary Systematic Literature Review of Machine Learning System Development Process</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Washizaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fukazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno>arXiv:cs.LG/1910.05528</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Secrets of Machine Learning: Ten Things You Wish You Had Known Earlier to be More Effective at Data Analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carlson</surname></persName>
		</author>
		<idno>arXiv:cs.LG/1906.01998</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Predicting the price of used cars using machine learning techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pudaruth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Inf. Comput. Technol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="753" to="764" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Autonomy and Accountability: legal liability for machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><surname>Responsibility</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Queen Mary School of Law Legal Studies Research Paper</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Legal requirements on explainability in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lognoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Streel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frénay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Binns</surname></persName>
		</author>
		<title level="m">Fairness in Machine Learning: Lessons from Political Philosophy. Proceedings of the 1st Conference on Fairness, Accountability and Transparency</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Wilson</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<idno>arXiv:cs.CY/1808.00023</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Barredo Arrieta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gil-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benjamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chatila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<idno>arXiv:cs.AI/1910.10045</idno>
	</analytic>
	<monogr>
		<title level="m">Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI 2019</title>
		<imprint>
			<biblScope unit="volume">XAI</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Megaman: scalable manifold learning in python</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcqueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meilă</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="5176" to="5180" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data management challenges in production machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data</title>
		<meeting>the 2017 ACM International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1723" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unit Testing Data with Deequ</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Biessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rukat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seufert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brunelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taptunov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Management of Data</title>
		<meeting>the 2019 International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1993" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Curse of Dimensionality</title>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4899-7687-1_192</idno>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning and Data Mining</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Sammut</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer US: Boston</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="314" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning, 5th Edition; Information science and statistics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An Introduction to Variable and Feature Selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On relevant dimensions in kernel feature spaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1875" to="1908" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A review of feature selection and feature extraction methods applied on microarray data</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Hira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Gillies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in bioinformatics</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A Review of Feature Selection Techniques in Bioinformatics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Inza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larrañaga</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btm344</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2507" to="2517" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Chandrashekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sahin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compeleceng.2013.11.024</idno>
	</analytic>
	<monogr>
		<title level="j">A Survey on Feature Selection Methods. Comput. Electr. Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="16" to="28" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feature Extraction: Foundations and Applications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikravesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Fuzziness and Soft Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer-Verlag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Selection bias in gene extraction on the basis of microarray gene-expression data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="6562" to="6566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Analyzing classifiers: Fisher vectors and deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2912" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unmasking Clever Hans predictors and assessing what machines really learn</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wäldchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1096</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Explainable AI: interpreting, explaining and visualizing deep learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer Nature</publisher>
			<biblScope unit="volume">11700</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural network classification and prior class probabilities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="299" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Synthetic Minority Over-sampling Technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
		<author>
			<persName><surname>Smote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Monard</surname></persName>
		</author>
		<idno type="DOI">10.1145/1007730.1007735</idno>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lemaître</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Aridas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="559" to="563" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A primer on wavelets and their scientific applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Lyons</surname></persName>
		</author>
		<title level="m">Understanding Digital Signal Processing</title>
		<meeting><address><addrLine>NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall PTR: Upper Saddle River</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Nd Edition</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep Learning for Missing Value Imputationin Tables with Non-Numerical Data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Biessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/MC.2009.263</idno>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multiple imputation: a review of practical and theoretical findings</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="142" to="159" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multiple imputation using chained equations: issues and guidance for practice</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Royston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="377" to="399" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multiple imputation by chained equations: what is it and how does it work?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Azur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frangakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Leaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of methods in psychiatric research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="40" to="49" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">From Predictive Methods to Missing Data Imputation: An Optimization Approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning feature representations with k-means</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="561" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Kernel principal component analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stamatescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
		</author>
		<title level="m">Understanding data augmentation for classification: when to warp? 2016 international conference on digital image computing: techniques and applications (DICTA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">TFX: A TensorFlow-based production-scale machine learning platform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haykal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1387" to="1395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Biessmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08558</idno>
		<title level="m">Quantifying Interpretability and Trust in Machine Learning Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">The Lack of a Priori Distinctions Between Learning Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1996.8.7.1341</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1341" to="1390" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">An introduction to kernel-based learning algorithms</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="181" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Machine Learning Testing: Survey, Landscapes and Horizons</title>
		<imprint>
			<date type="published" when="1906">2019, abs/1906.10742. 1906.10742</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-05318-5</idno>
		<title level="m">Machine Learning -Methods, Systems, Challenges; The Springer Series on Challenges in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Efficient and Robust Automated Machine Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Cortes, C</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Why Does Unsupervised Pre-training Help Deep Learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deep Feature Learning of In-Cylinder Flow Fields to Analyze CCVs in an SI-Engine</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dreher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zündorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dreizler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanuschkin</surname></persName>
		</author>
		<idno type="DOI">10.1177/1468087420974148</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engine Research</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Semi-supervised Learning with Deep Generative Models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning, 1st ed</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Using the Nyström Method to Speed Up Kernel Machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13; Leen</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Tresp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2153" to="2175" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">A Survey of Model Compression and Acceleration for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno>1710.09282</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">DeepCABAC: A Universal Compression Algorithm for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kirchhoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matlage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marbán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Marinc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno>abs/1907.11900</idno>
		<imprint>
			<date type="published" when="1907">2019. 1907.11900</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Ensemble-based classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Ensembling neural networks: many could be better than all</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="239" to="263" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Popular ensemble methods: An empirical study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="169" to="198" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="6402" to="6413" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">The Machine Learning Reproducibility Checklist</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2019" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">A Practical Taxonomy of Reproducibility for</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tatman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<title level="m">Deep reinforcement learning that matters. Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Winner&apos;s Curse?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>On Pace, Progress, and Empirical Rigor</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Unreproducible Research is Reproducible</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR: Long Beach</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="725" to="734" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Vartak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Subramanyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Husnoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><surname>Modeldb</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939502.2939516</idno>
		<title level="m">A System for Machine Learning Model Management. Proceedings of the Workshop on Human-In-the-Loop Data Analytics</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Metamorphic Testing of Driverless Cars</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1145/3241979</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="61" to="67" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Automated Testing of Deep-neural-network-driven Autonomous Cars</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Deeptest</surname></persName>
		</author>
		<idno type="DOI">10.1145/3180155.3180220</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Software Engineering</title>
		<meeting>the 40th International Conference on Software Engineering<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ICSE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Automated Whitebox Testing of Deep Learning Systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName><surname>Deepxplore</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132747.3132785</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles</title>
		<meeting>the 26th Symposium on Operating Systems Principles<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">An Algorithm for Generating Invisible Data Poisoning Using Adversarial Noise That Breaks Image Classification Deep Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chan-Hon-Tong</surname></persName>
		</author>
		<idno type="DOI">10.3390/make1010011</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning and Knowledge Extraction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="192" to="204" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Debugging Machine Learning Tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chakarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vijaykeerthy</surname></persName>
		</author>
		<idno>abs/1603.07292</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Explainable AI Framework for Multivariate Hydrochemical Time Series</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ultsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Breuer</surname></persName>
		</author>
		<idno type="DOI">10.3390/make3010009</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Extraction 2021</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="170" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">How to explain individual classification decisions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1803" to="1831" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">What is relevant in a text document?&quot;: An interpretable machine learning approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">181142</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">How to Achieve Explainability and Transparency in Human AI Interaction. HCI International</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Theofanou-Fuelbier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Junk</surname></persName>
		</author>
		<editor>Stephanidis, C.</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Springer International Publishing: Cham</publisher>
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seegerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hägele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dähne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kindermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<idno>arXiv:cs.LG/1909.09223</idno>
		<title level="m">InterpretML: A Unified Framework for Machine Learning Interpretability</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Machine learning at Facebook: Understanding inference at the edge</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="331" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Guidelines and Benchmarks for Deployment of Deep Learning Models on Smartphones as Real-Time Apps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sehgal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<idno type="DOI">10.3390/make1010027</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning and Knowledge Extraction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="450" to="465" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Serving Machine Learning Workloads in Resource Constrained Environments: a Serverless Deployment Example</title>
		<author>
			<persName><forename type="first">A</forename><surname>Christidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moschoyiannis</surname></persName>
		</author>
		<idno type="DOI">10.1109/SOCA.2019.00016</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th Conference on Service-Oriented Computing and Applications</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Covariate shift adaptation by importance weighted cross validation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krauledat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="985" to="1005" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Heckemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gesell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Berns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trapp</surname></persName>
		</author>
		<title level="m">Safe automotive software. International Conference on Knowledge-Based and Intelligent Information and Engineering Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Safe learning of regions of attraction for uncertain, nonlinear systems with gaussian processes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Berkenkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moriconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Schoellig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 55th Conference on Decision and Control</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="4661" to="4666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Derakhshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mahdiraji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rabl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
		<title level="m">Continuous Deployment of Machine Learning Pipelines. EDBT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="397" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Fehling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Leymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Retter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schupeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbitter</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-7091-1568-8</idno>
		<title level="m">Cloud Computing Patterns: Fundamentals to Design, Build, and Manage Cloud Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Interpretability and Reproducability in Production Machine Learning Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundararaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khermosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Talagala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="658" to="664" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
