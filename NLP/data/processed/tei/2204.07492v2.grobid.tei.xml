<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Machine Learning Tutorial for Operational Meteorology, Part I: Traditional Machine Learning</title>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Department of Commerce</orgName>
				</funder>
				<funder>
					<orgName type="full">Numpy</orgName>
				</funder>
				<funder>
					<orgName type="full">U.S. Department of Commerce</orgName>
				</funder>
				<funder ref="#_D3AJnp7 #_C7ARwkU">
					<orgName type="full">NOAA/Office of Oceanic and Atmospheric Research under NOAA-University of Oklahoma</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-06-07">7 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Oklahoma</orgName>
								<address>
									<settlement>Norman</settlement>
									<region>OK</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Meteorology</orgName>
								<orgName type="institution">University of Oklahoma</orgName>
								<address>
									<settlement>Norman</settlement>
									<region>OK</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">NSF AI Institute for Research on Trustworthy AI in Weather, Climate, and Coastal Oceanography</orgName>
								<orgName type="institution" key="instit2">University of Oklahoma</orgName>
								<address>
									<settlement>Norman</settlement>
									<region>OK</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Cooperative Institute for Severe and High-Impact Weather Research and Operations</orgName>
								<orgName type="institution">University of Oklahoma</orgName>
								<address>
									<settlement>Norman</settlement>
									<region>OK</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">NOAA/NWS/Storm Prediction Center</orgName>
								<address>
									<settlement>Norman</settlement>
									<region>Oklahoma</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Marine</orgName>
								<orgName type="institution">and Atmospheric Sciences</orgName>
								<address>
									<settlement>Earth</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<settlement>Raleigh</settlement>
									<region>North Carolina</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Machine Learning Tutorial for Operational Meteorology, Part I: Traditional Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-07">7 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">56DC27B8025713076C1408ABB7ABF425</idno>
					<idno type="arXiv">arXiv:2204.07492v2[physics.ao-ph]</idno>
					<note type="submission">This work has been submitted for publication. Copyright in this work may be transferred without further notice, and this version may no longer be accessible.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naÃ¯ve Bayes; and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The mention and use of machine learning (ML) within meteorological journal articles is accelerating (Fig. <ref type="figure">1</ref>; e.g., <ref type="bibr" target="#b9">Burke et al. 2020;</ref><ref type="bibr" target="#b34">Hill et al. 2020;</ref><ref type="bibr" target="#b50">Lagerquist et al. 2020;</ref><ref type="bibr" target="#b57">Li et al. 2020;</ref><ref type="bibr" target="#b58">Loken et al. 2020;</ref><ref type="bibr" target="#b61">Mao and Sorteberg 2020;</ref><ref type="bibr" target="#b71">MuÃ±oz-Esparza et al. 2020;</ref><ref type="bibr" target="#b88">Wang et al. 2020;</ref><ref type="bibr" target="#b6">Bonavita et al. 2021;</ref><ref type="bibr">Cui et al. 2021;</ref><ref type="bibr" target="#b20">Flora et al. 2021;</ref><ref type="bibr" target="#b36">Hill and Schumacher 2021;</ref><ref type="bibr" target="#b80">Schumacher et al. 2021;</ref><ref type="bibr" target="#b94">Yang et al. 2021;</ref><ref type="bibr" target="#b96">Zhang et al. 2021)</ref>. With a growing number of published meteorological studies using ML methods, it is increasingly important for meteorologists to be well-versed in ML. However, the availability of meteorology specific resources about ML terms and methods is scarce. Thus, this series of papers (total of 2) aim to reduce the scarcity of meteorology specific ML resources.</p><p>While many ML methods are generally not new (i.e., published before 2002), there is a concern from ML developers that end users (i.e., non-ML specialists) may be hesitant or are concerned about trusting ML. However, early work in this space suggests that non-technical explanations may be an important part of how end users perceive the trustworthiness of ML guidance (e.g., <ref type="bibr" target="#b10">Cains et al. 2022</ref>). Thus, an additional goal of these papers is to enhance trustworthiness of ML methods through plain language discussions and meteorological examples.</p><p>Corresponding author: Randy J. Chase, randychase@ou.edu In practice, ML models are often viewed as a black box which could also be contributing to user hesitancy. These mystified feelings towards ML methods can lead to an inherent distrust with ML methods, despite their potential. Furthermore, the seemingly opaque nature of ML methods prevents ML forecasts from meeting one of the three requirements of a good forecast outlined by <ref type="bibr" target="#b70">Murphy (1993)</ref>: consistency. In short, <ref type="bibr" target="#b70">Murphy (1993)</ref> explains that in order for a forecast to be good, the forecast must (1) be consistent with the user's prior knowledge, (2) have good quality (i.e., accuracy) and (3) be valuable (i.e., provide benefit). Plenty of technical papers demonstrate how ML forecasts can meet requirements 2 and 3, but as noted above if the ML methods are confusing and enigmatic, then it is difficult for ML forecasts to be consistent with a meteorologists prior knowledge. This series of papers will serve as a reference for meteorologists in order to make the black box of ML more transparent and enhance user trust in ML.</p><p>This paper is organized as follows. Section 2 provides an introduction to all ML methods discussed in this paper and will define common ML terms. Section 3 discusses the general ML methods in context of a simple meteorological example, while also describing the end-to-end ML pipeline. Then, Section 4 summarizes this paper and also discusses the topics of the next paper in the series. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Machine Learning Methods and Common Terms</head><p>This section will describe a handful of the most common ML methods. Before that, it is helpful to define some terminology used within ML. First, we define ML as any empirical method where parameters are fit (i.e., learned) on a training dataset in order to optimize (e.g., minimize or maximize) a predefined loss (i.e., cost) function. Within this general framework, ML has two categories: supervised and unsupervised learning. Supervised learning are ML methods that are trained with prescribed input features and output labels. For example, predicting tomorrow's high temperature at a specific location where we have measurements (i.e., labels). Meanwhile, unsupervised methods do not have a predefined output label (e.g., self-organizing maps; <ref type="bibr" target="#b73">Nowotarski and Jensen 2013)</ref>. An example of an unsupervised ML task would be clustering all 500 mb geopotential height maps to look for unspecified patterns in the weather. This paper focuses on supervised learning.</p><p>The input features for supervised learning, also referred to as input data, predictors or variables, can be written mathematically as the vector (matrix) ğ‘‹. The desired output of the ML model is usually called the target, predictand or label, and is mathematically written as the scalar (vector) ğ‘¦. Drawing on the meteorological example of predicting tomorrow's high temperature, the input feature would be tomorrow's forecasted temperature from a numerical weather model (e.g., GFS) and the label would be tomorrow's observed temperature.</p><p>By empirical we mean any method that uses data as opposed to physics Supervised ML methods can be further broken into two sub-categories: regression and classification. Regression tasks are ML methods that output a continuous range of values, like the forecast of tomorrow's high temperature (e.g., 75.0 â€¢ F). Meanwhile classification tasks are characteristic of ML methods that classify data (e.g., will it rain or snow tomorrow). Reposing tomorrow's high temperature forecast as a classification task would be: "Will tomorrow be warmer than today?". This paper will cover both regression and classification methods. In fact, many ML methods can be used for both tasks.</p><p>All ML methods described here will have one thing in common: the ML method quantitatively uses the training data to optimize a set of weights (i.e., thresholds) that enable the prediction. These weights are determined either by minimizing the error of the ML prediction or maximizing a probability of a class label. The two different methods coincide with the regression and classification respectively. Alternative names for error that readers might encounter in the literature are loss or cost.</p><p>Now that some of the common ML terms has been discussed, the following subsections will describe the ML methods. It will start with the simplest methods (e.g., linear regression) and move to more complex methods (e.g., support vector machines) as the sections proceed. Please note that the following subsections aim to provide an introduction and the intuition behind each method. An example of the methods being applied and helpful application discussion can be found in Section 3. F . 2. A visual example of linear regression with a single input predictor. The x-axis is a synthetic input feature, the y-axis is a synthetic output label. The solid black line is the regression fit, and the red dashed lines are the residuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a. Linear Regression</head><p>An important concept in ML is when choosing to use ML for a task, one should start with the simpler ML models first. Occam's razor tells us to prefer the simplest solution that can solve the task or represent the data. While this doesn't always mean the simplest ML model available, it does mean that simpler models should be tried before more complicated ones <ref type="bibr" target="#b38">(Holte 1993)</ref>. Thus, the first ML method discussed is linear regression which has a long history in meteorology (e.g. <ref type="bibr" target="#b60">Malone 1955</ref>) and forms the heart of the model output statistics product (i.e., MOS; Glahn and Lowry 1972) that many meteorologists are familiar with. Linear regression is popular because it is a simple method that is also computationally efficient. At its simplest form, linear regression approximates the value you would like to predict ( Å·) by fitting weight terms (ğ‘¤ ğ‘– ) in the following equation,</p><formula xml:id="formula_0">Å· = ğ‘–=ğ· âˆ‘ï¸ ğ‘–=0 ğ‘¤ ğ‘– ğ‘¥ ğ‘– .<label>(1)</label></formula><p>The first predictor (ğ‘¥ 0 ) is always 1 so that ğ‘¤ 0 is a bias term, allowing the function to move from the origin as needed.</p><p>ğ· is the number of features for the task.</p><p>As noted before, with ML, the objective is to find ğ‘¤ ğ‘– such that a user-specified loss function (i.e., error function) is minimized. The most common loss function for traditional linear regression is the residual summed squared error (RSS):</p><formula xml:id="formula_1">ğ‘…ğ‘†ğ‘† = ğ‘ âˆ‘ï¸ ğ‘—=1 (ğ‘¦ ğ‘— -Å· ğ‘— ) 2</formula><p>(2)</p><p><ref type="url" target="https://en.wikipedia.org/wiki/Occam%27s_razor">https://en.wikipedia.org/wiki/Occam%27s_razor</ref> </p><p>where ğ‘¦ ğ‘— is a true data point, Å· ğ‘— is the predicted data point and ğ‘ is the total number of data points in the training dataset. A graphical example of a linear regression and its residuals is shown in Fig. <ref type="figure">2</ref>. Linear regression using residual summed squared error can work very well and is a fast learning algorithm, so we suggest it as a baseline method before choosing more complicated methods.</p><p>The exact minimization method is beyond the scope of this paper, but know that the minimization uses the slope (i.e., derivative) of the loss function to determine how to adjust the trainable weights. If this sounds familiar, that is because it is the same minimization technique learned in most first year college calculus classes and is a similar technique to what is used in data assimilation for numerical weather prediction (c.f., Chapter 5 and Section 10.5 in <ref type="bibr" target="#b42">Kalnay 2002;</ref><ref type="bibr" target="#b49">Lackmann 2011)</ref>. The concept of using the derivative to find the minimum is repeated throughout most ML methods given there is often a minimization (or maximization) objective.</p><p>Occasionally datasets can contain irrelevant or noisy predictors which can cause instabilities in the learning. One approach to address this is to use a modified version of linear regression known as ridge regression <ref type="bibr" target="#b37">(Hoerl and Kennard 1970)</ref>, which minimizes both the summed squared error (like before) and the sum of the squared weights called an ğ¿ 2 penalty. Mathematically, the new loss function can be described as</p><formula xml:id="formula_2">ğ‘…ğ‘†ğ‘† ğ‘Ÿ ğ‘–ğ‘‘ğ‘”ğ‘’ = ğ‘ âˆ‘ï¸ ğ‘—=1 (ğ‘¦ ğ‘— -Å· ğ‘— ) 2 + ğœ† ğ· âˆ‘ï¸ ğ‘–=0 ğ‘¤ 2 ğ‘– (3)</formula><p>Here, ğœ† (which is â‰¥ 0) is a user-defined parameter that controls the weight of the penalty. Likewise, another modified version of linear regression is lasso regression <ref type="bibr" target="#b84">(Tibshirani 1996)</ref> which minimizes the sum of the absolute value of the weights. This penalty to learning is also termed an ğ¿ 1 penalty. The lasso loss function mathematically is</p><formula xml:id="formula_3">ğ‘…ğ‘†ğ‘† ğ‘™ğ‘ğ‘ ğ‘ ğ‘œ = ğ‘ âˆ‘ï¸ ğ‘—=1 (ğ‘¦ ğ‘— -Å· ğ‘— ) 2 + ğœ† ğ· âˆ‘ï¸ ğ‘–=0 |ğ‘¤ ğ‘– | (4)</formula><p>Both lasso and ridge encourage the learned weights to be small but in different ways. The two penalties are often combined to create the elastic-net penalty <ref type="bibr" target="#b97">(Zou and Hastie 2005</ref>)</p><formula xml:id="formula_4">ğ‘…ğ‘†ğ‘† ğ‘’ğ‘™ğ‘ğ‘ ğ‘¡ğ‘–ğ‘ = ğ‘ âˆ‘ï¸ ğ‘—=1 (ğ‘¦ ğ‘— -Å· ğ‘— ) 2 + ğœ† ğ· âˆ‘ï¸ ğ‘–=0 (ğ›¼ğ‘¤ 2 ğ‘– + (1 -ğ›¼)|ğ‘¤ ğ‘– |).</formula><p>(5) In general, the addition of components to the loss function, like described in Eq. 3-5, is known as regularization and is found in other ML methods. Some recent examples of papers using linear regression include subseasonal prediction of tropical cyclone parameters <ref type="bibr" target="#b55">(Lee et al. 2020</ref>), relating F . 3. A graphical depiction of the sigmoid function (Eq. 6). The x-axis is the predicted label value, while the y-axis is the now scaled value. mesocyclone characteristics to tornado intensity <ref type="bibr" target="#b81">(Sessa and Trapp 2020</ref>) and short term forecasting of tropical cyclone intensity <ref type="bibr" target="#b39">(Hu et al. 2020</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. Logistic Regression</head><p>As a complement to linear regression, the first classification method discussed here is logistic regression. Logistic regression is an extension from linear regression in that it uses the same functional form of Eq. 1. The differences lie in how the weights for Eq. 1 are determined and a minor adjustment to the output of Eq. 1. More specifically, logistic regression applies the sigmoid function (Fig. <ref type="figure">3</ref>) to the output of Eq. 1 defined as:</p><formula xml:id="formula_5">ğ‘†( Å·) = 1 1 + ğ‘’ -Å· (6)</formula><p>Large positive values into the sigmoid results in a value of 1 while large negative values result in a value of 0. Effectively, the sigmoid scales the output of Eq. 1 to a range of 0 to 1, which then can be interpreted like a probability. For the simplest case of classification involving just two classes (e.g., rain or snow), the output of the sigmoid can be interpreted as a probability of either class (e.g., rain or snow). The output probability then allows for the classification to be formulated as the ğ‘¤ ğ‘– that maximizes the probability of a desired class. Mathematically, the classification loss function for logistic regression can be described as</p><formula xml:id="formula_6">loss = ğ‘–=ğ· âˆ‘ï¸ ğ‘–=0 -ğ‘¦ ğ‘– log(ğ‘†( Å·)) + (1 -ğ‘¦ ğ‘– ) log(1 -ğ‘†( Å·)). (7)</formula><p>Like before for linear regression, the expression in Eq. 7 is minimized using derivatives. If the reader is interested in more information on the mathematical techniques of minimization they can find more information in Chapter 5 of <ref type="bibr" target="#b42">Kalnay (2002)</ref>.</p><p>Logistic regression has been used for a long time within meteorology. One of the earliest papers using logistic regression showed skill in predicting the probability of hail greater than 1.9 cm <ref type="bibr" target="#b3">(Billet et al. 1997)</ref>, while more recent papers have used logistic regression to identify storm mode <ref type="bibr" target="#b41">(Jergensen et al. 2020)</ref>, subseasonal prediction of surface temperature <ref type="bibr" target="#b87">(Vigaud et al. 2019</ref>) and predict the transition of tropical cyclones to extratropical cyclones <ref type="bibr" target="#b2">(Bieli et al. 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c. NaÃ¯ve Bayes</head><p>An additional method to do classification is known as naÃ¯ve Bayes <ref type="bibr" target="#b46">(Kuncheva 2006)</ref>, which is named for its use of Bayes's theorem and can be written as the following:</p><formula xml:id="formula_7">ğ‘ƒ(ğ‘¦|ğ‘¥) = ğ‘ƒ(ğ‘¦)ğ‘ƒ(ğ‘¥|ğ‘¦) ğ‘ƒ(ğ‘¥) .<label>(8)</label></formula><p>In words, Eq. 8 is looking for the probability of some label ğ‘¦ (e.g., snow), given a set of input features ğ‘¥ (ğ‘ƒ(ğ‘¦|ğ‘¥); e.g., temperature). This probability can be calculated from knowing the probability of the label ğ‘¦ occurring in the dataset (ğ‘ƒ(ğ‘¦); e.g., how frequent it snows) times the probability of the input features given it belongs to the class ğ‘¦ (ğ‘ƒ(ğ‘¥|ğ‘¦); e.g., how frequently is it 32 o F when it's snowing), divided by the probability of the input features (ğ‘ƒ(ğ‘¥)).</p><p>The naÃ¯ve part of the naÃ¯ve Bayes algorithm comes from assuming that all input features ğ‘¥, are independent of one another and the term ğ‘ƒ(ğ‘¥|ğ‘¦) can be modeled by an assumed distribution (e.g., normal distribution) with parameters determined from the training data. While these assumptions are often not true, the naÃ¯ve Bayes classifier can be skillful in practice. A few simplification steps results in the following Å· = argmax(log(ğ‘ƒ(ğ‘¦))</p><formula xml:id="formula_8">+ ğ‘ âˆ‘ï¸ ğ‘–=0 log(ğ‘ƒ(ğ‘¥ ğ‘– |ğ‘¦))). (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>Again in words, the predicted class ( Å·) from naÃ¯ve Bayes is the classification label (ğ‘¦) such that the sum of the log of the probability of that classification (ğ‘ƒ(ğ‘¦)) and the sum of log of all the probabilities of the specific inputs given the classification (ğ‘ƒ(ğ‘¥ ğ‘– |ğ‘¦)) is maximized. In order to help visualize the quantity ğ‘ƒ(ğ‘¥ ğ‘– |ğ‘¦), a graphical example is shown in Fig. <ref type="figure">4</ref>. This example uses surface weather measurements from a station near Marquette, Michigan where data were compiled when it was raining and snowing. Fig. <ref type="figure">4</ref> shows distribution of air temperature (i.e., an input feature) given the two classes (i.e., rain vs snow). In order to get ğ‘ƒ(ğ‘¥ ğ‘– |ğ‘¦), we need to assume an underlying distribution function. The common assumed distribution with naÃ¯ve Bayes is the normal distribution</p><formula xml:id="formula_10">ğ‘“ (ğ‘¥; ğœ‡, ğœ) = 1 ğœ âˆš 2ğœ‹ ğ‘’ -1 2 ( ğ‘¥-ğœ‡ ğœ ) (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where ğœ‡ is the mean and ğœ is the standard deviation of the training data. While the normal distribution assumption for the temperature distribution in Fig. <ref type="figure">4</ref> is questionable due to thermodynamic constraints that lock the temperature at 32 o F (i.e., latent cool/heating), naÃ¯ve bayes can still have skill. Initially, it might not seem like any sort of weights/biases are being fit like the previously mentioned methods (e.g., logistic regression), but ğœ‡ and ğœ are being learned fron the training data. If performance from the normal distribution is poor, other distributions can be assumed, like a multinomial or a Bernoulli distribution.</p><p>A popular use of naÃ¯ve Bayes classification in the meteorological literature has been the implementation of Prob-Severe (e.g., <ref type="bibr" target="#b13">Cintineo et al. 2014</ref><ref type="bibr" target="#b14">Cintineo et al. , 2018</ref><ref type="bibr" target="#b15">Cintineo et al. , 2020) )</ref> which uses various severe storm parameters and observations to classify the likelihood of any storm becoming severe in the next 60 minutes. Additional examples of naÃ¯ve Bayes classifiers in meteorology have been used for identifying tropical cyclone secondary eyewall formation from microwave imagery <ref type="bibr" target="#b45">(Kossin and Sitkowski 2009)</ref>, identifying anomalous propagation in radar data <ref type="bibr" target="#b75">(Peter et al. 2013</ref>) and precipitation type (e.g., Convective/Stratiform) retrievals from geostationary satellites <ref type="bibr" target="#b29">(Grams et al. 2016</ref>). F . 5. A visual representation of the two functions that can be used in decision trees for classification, Entropy (blue) and Gini impurity (red).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d. Trees and Forests</head><p>Decision trees are based on a decision making method that humans have been using for years: flow charts, where the quantitative decision points within the flow chart are learned automatically from the data. Early use of decision trees in meteorology (e.g., <ref type="bibr" target="#b12">Chisholm et al. 1968</ref>) actually pre-dated the formal description of the decision tree algorithm <ref type="bibr" target="#b7">(Breiman 1984;</ref><ref type="bibr" target="#b76">Quinlan 1993;</ref><ref type="bibr" target="#b8">Breiman 2001)</ref>. Since then, tree-based methods have grown in popularity and have been demonstrated to predict a variety of complex meteorological phenomena. Topics include: aviation applications (e.g., <ref type="bibr">Williams et al. 2008a,b;</ref><ref type="bibr" target="#b91">Williams 2014;</ref><ref type="bibr" target="#b71">MuÃ±oz-Esparza et al. 2020)</ref>; severe weather (e.g., <ref type="bibr" target="#b23">Gagne et al. 2009</ref><ref type="bibr" target="#b24">Gagne et al. , 2013;;</ref><ref type="bibr" target="#b65">McGovern et al. 2014;</ref><ref type="bibr" target="#b67">Mecikalski et al. 2015;</ref><ref type="bibr" target="#b51">Lagerquist et al. 2017;</ref><ref type="bibr">Gagne et al. 2017;</ref><ref type="bibr" target="#b18">Czernecki et al. 2019;</ref><ref type="bibr" target="#b9">Burke et al. 2020;</ref><ref type="bibr" target="#b34">Hill et al. 2020;</ref><ref type="bibr" target="#b58">Loken et al. 2020;</ref><ref type="bibr" target="#b26">Gensini et al. 2021;</ref><ref type="bibr" target="#b20">Flora et al. 2021;</ref><ref type="bibr" target="#b59">Loken et al. 2022)</ref>; solar power (e.g., <ref type="bibr" target="#b64">McGovern et al. 2015)</ref>; precipitation (e.g., <ref type="bibr" target="#b19">Elmore and Grams 2016;</ref><ref type="bibr">Herman and Schumacher 2018b,a;</ref><ref type="bibr" target="#b83">Taillardat et al. 2019;</ref><ref type="bibr" target="#b58">Loken et al. 2020;</ref><ref type="bibr" target="#b88">Wang et al. 2020;</ref><ref type="bibr" target="#b61">Mao and Sorteberg 2020;</ref><ref type="bibr" target="#b57">Li et al. 2020;</ref><ref type="bibr" target="#b36">Hill and Schumacher 2021;</ref><ref type="bibr" target="#b80">Schumacher et al. 2021)</ref>; satellite and radar retrievals (e.g., <ref type="bibr" target="#b48">KÃ¼hnlein et al. 2014;</ref><ref type="bibr" target="#b16">Conrick et al. 2020;</ref><ref type="bibr" target="#b94">Yang et al. 2021;</ref><ref type="bibr" target="#b96">Zhang et al. 2021</ref>) and climate related topics (e.g., <ref type="bibr">Cui et al. 2021)</ref>.</p><p>To start, we will describe decision trees in context of a classification problem. The decision tree creates splits in the data (i.e., decisions) that are chosen such that either the Gini Impurity value or the Entropy value decreases after the split. Gini Impurity is defined as</p><formula xml:id="formula_12">Gini = ğ‘–=ğ‘˜ âˆ‘ï¸ ğ‘–=0 ğ‘ ğ‘– (1 -ğ‘ ğ‘– ) (11)</formula><p>where ğ‘ ğ‘– is the probability of class i (i.e., the number of data points labeled class i divided by the total number of data points). While Entropy is defined as</p><formula xml:id="formula_13">Entropy = ğ‘–=ğ‘˜ âˆ‘ï¸ ğ‘–=0 ğ‘ ğ‘– log 2 ( ğ‘ ğ‘– ).<label>(12)</label></formula><p>Both functions effectively measure how similar the data point labels are in each one of the groupings of the tree after some split in the data. Envision the flow chart as a tree. The decision is where the tree branches into two directions, resulting in two separate leaves. The goal of a decision tree is to choose the branch that results in a leaf having a minimum of Gini or Entropy. In other words, the data split would ideally result in two sub-groups of data where all the labels are the same within each sub-group. Fig. <ref type="figure">5</ref> shows both the Gini impurity and entropy for a two class problem. Consider the example of classifying winter precipitation as rain or snow. From some example surface temperature dataset the likely decision threshold would be near 32 â€¢ F, which would result in the subsequent two groupings of data point labels (i.e., snow/rain) having a dominant class label (i.e., fraction of class k is near 0 or 1) and thus having a minimum of Entropy or Gini (i.e., near 0). The actual output of this tree could be either the majority class label, or the ratio of the major class (i.e., a probabilistic output).</p><p>While it is helpful to consider a decision tree with a single decision, also known as a tree with a depth of 1, the prediction power of a single decision is limited. A step toward more complexity is to include increasing depth (i.e., more decisions/branches). To continue with the rain/snow example from the previous paragraph, we could include a second decision based on measured wet bulb temperature. A tree with depth two will likely have better performance, but the prediction power is still somewhat limited.</p><p>An additional step to increase the complexity of decision trees, beyond including more predictors, is a commonly used method in meteorology: ensembles. While it might not be clear here, decision trees become over-fit (i.e., work really well for training data, but perform poorly on new data) as the depth of the tree increases. An alternative approach is to use an ensemble of trees (i.e., a forest). Using an ensemble of trees forms the basis of two additional tree based methods: random forests <ref type="bibr" target="#b8">(Breiman 2001</ref>) and gradient boosted decision trees <ref type="bibr" target="#b21">(Friedman 2001)</ref>.</p><p>Random forests are a collection of decision trees that are trained on random subsets of data and random subsets of input variables from the initial training dataset. In other words, the mathematics are exactly the same for each tree, the decisions still aim to minimize the loss (e.g., Entropy), but each tree is given a different random subset of data sampled from the original dataset with replacement. Gradient boosted decision trees are an ensemble of trees that instead of training multiple trees on random subsets (i.e., random forest), each tree in the ensemble is successively trained on the remaining error from the previous trees. To put it another way, rather than minimizing the total error on random trees, the reduced error from the first decision tree is now minimized on the second tree, and the reduced error from trees one and two is then minimized on the third tree and so on. In order to come up with a single prediction out of the ensemble of trees, the predictions can be combined through a voting procedure (i.e., count up the predicted classes of each tree) or by taking the average probabilistic output from each tree. Random forests can use either method, while gradient boosted trees are limited to the voting procedure.</p><p>While the discussion here has been centered on classification for the tree-based methods, they can be used for regression as well. The main alteration to the decision tree method to convert to a regression-based problem is the substitution of the loss function (i.e., Eq. 11-12). For example a common loss function for random forest for regression and gradient boosted regression is the same loss function as linear regression described in the previous section (e.g., Eq. 2), the residual summed squared error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e. Support Vector Machines</head><p>A support vector machine (commonly referred to as SVM <ref type="bibr">;</ref><ref type="bibr" target="#b85">Vapnik 1963</ref>) is an ML method similar to linear and logistic regression. The idea is that a support vector machine uses a linear boundary to do its predictions, which has a similar mathematical form but written differently to account to vector notation. The equation is</p><formula xml:id="formula_14">Å· = w ğ‘‡ x + ğ‘ (13)</formula><p>where w is a vector of weights, x is a vector of input features, b is a bias term and Å· is the regression prediction.</p><p>In the case of classification, only sign of the right side of Eq. 13 is used. This linear boundary can be generalized beyond two-dimensional problems (i.e., two input features) to three-dimensions where the decision boundary is called a plane, or any higher order space where the boundary is called a hyperplane. The main difference between linear methods discussed in Sections 2a-2b and support vector machines is that support vector machines include margins to the linear boundary. Formally, the margin is the area between the linear boundary and the closest training datapoint for each class label (e.g., closest rain data point and closest snow datapoint). This is shown schematically with a synthetic dataset in Fig. <ref type="figure">6a</ref>. While this is an ideal case, usually classes overlap (Fig. <ref type="figure">6b</ref>), but support vector machines can still handle splitting the classes. The optimization task for support vector machines is stated as the following: Find w ğ‘‡ such that the margin is maximized.</p><p>In other words, support vector machines aim to maximize a) b) the distance between the two closest observations on either side of the hyperplane. Mathematically, the margin distance is described as</p><formula xml:id="formula_15">margin = 1 w ğ‘‡ w . (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>Like before, the maximization is handled by numerical techniques to optimize the problem but the resulting solution will be the hyperplane with the largest separation between the classes. A powerful attribute of the support vector machine method is that it can be extended to additional mathematical formulations for the boundary, for example a quadratic function. Thus the person using support vector machines can decide which function would work best for their data. Recent applications of support vector machines in meteorology include the classification of storm mode <ref type="bibr" target="#b41">(Jergensen et al. 2020)</ref>, hindcasts of tropical cyclones <ref type="bibr" target="#b72">(Neetu et al. 2020</ref>) and evaluating errors with quantitative precipitation retrievals in the United States <ref type="bibr" target="#b47">(Kurdzo et al. 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Machine Learning Application and Discussion</head><p>This section will discuss the use of all ML methods with a familiar use-case: thunderstorms. Specifically, this section will show two ML applications derived from popular meteorological datasets: radar and satellite. The particular data used are from the Storm EVent ImageRy dataset (SEVIR; <ref type="bibr" target="#b86">Veillette et al. 2020)</ref>, which contains over 10,000 storm events from between 2017 and 2019. Each event spans four hours and includes measurements from both GOES-16 and NEXRAD. An example storm event and the 5 measured variables: Red channel visible reflectance (0.64ğœ‡m; Channel 2), midtropospheric water vapor brightness temperature (6.9 ğœ‡m; Channel 9), clean infrared window brightness temperature (10.7 ğœ‡m; Channel 13), Vertically Integrated Liquid (VIL; from NEXRAD) and Geostationary Lightning Mapper (GLM) measured lightning flashes are found in Fig. <ref type="figure" target="#fig_3">7</ref>. In addition to discussing ML in context of the SEVIR dataset, this section will follow the general steps to using ML and contain helpful discussions of the best practices as well as the most common pitfalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a. Problem Statements</head><p>The SEVIR data will be applied to two tasks: (1) Does this image contain a thunderstorm? and (2) How many lightning flashes are in this image? To be explicit, we assume the GLM observations are unavailable and we need to use the other measurements (e.g., infrared brightness temperature) as features to estimate if there are lightning flashes (i.e., classification), and how many of them are there (i.e., regression). While both of these tasks might be considered redundant since we have GLM, the goal of this paper is to provide discussion on how to use ML as well as discussion on the ML methods themselves. That being said, a potential useful application of the trained models herein would be to use them on satellite sensors that do not have lightning measurements. For example, all generations of GOES prior to GOES-16 did not have a lightning sensor co-located with the main sensor. Thus, we could poten- tially use the ML models trained here to estimate GLM measurements prior to GOES-16 (i.e., November 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. Data</head><p>The first step of any ML project is to obtain data. Here, the data are from a public archive hosted on the Amazon Web Service. For information of how to obtain the SEVIR data as well as the code associated with this manuscript see the Data Availability Statement. One major question at this juncture is, "How much data is needed to do machine learning?". While there does not exist a generic number that can apply to all datasets, the idea is to obtain enough data such that one's training data are diverse. A diverse dataset is desired because any bias found within the training data would be encoded in the ML method <ref type="bibr">(McGovern et al. 2021)</ref>. For example, if a ML model was trained on only images where thunderstorms were present, then the ML model would likely not know what a non-lightning producing storm would look like and be biased. Diversity in the SEVIR dataset is created by including random images (i.e., no storms) from all around the United States (c.f. Figure <ref type="figure">2</ref> in <ref type="bibr" target="#b86">Veillette et al. 2020)</ref>.</p><p>After obtaining the data, it is vital to remove as much spurious data as possible before training because the ML model will not know how to differentiate between spurious data and high quality data. A common anecdote when using ML models is garbage in, garbage out. The SEVIR dataset has already gone through rigorous quality control, but this is often not the case with raw meteorological datasets. Two examples of quality issues that would likely be found in satellite and radar datasets are satellite artifacts (e.g., GOES-17 heat pipe; <ref type="bibr" target="#b62">McCorkel et al. 2019</ref>) and radar ground clutter (e.g., <ref type="bibr" target="#b40">Hubbert et al. 2009</ref>). Cleaning and manipulating the dataset to get it ready for ML often takes a researcher 50% -80% of their time . Thus, do not be discouraged if cleaning one's datasets is taking a large amount of time because a high-quality dataset will be best for having a successful ML model.</p><p>Subsequent to cleaning the data, the next step is to engineer the inputs (i.e., features) and outputs (i.e., labels). One avenue to create features is to use every single pixel in the image as a predictor. While this could work, given the number of pixels in the SEVIR images (589,824 total pixels for one visible image) it is computationally impractical to train a ML model with all pixels. Thus, we are looking for a set of statistics than can be extracted from each image. For the generation of features, domain knowledge is criti- <ref type="url" target="https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html">https://www.nytimes.com/2014/08/18/technology/  for-big-data-scientists-hurdle-to-insights-is-janitor-work.  html</ref> cal because choosing meteorologically relevant quantities will ultimately determine the ML models skill. For the ML tasks presented in Section 3a, information about the storm characteristics (e.g., strength) in the image would be beneficial features. For example, a more intense storm is often associated with more lightning. Proxies for estimating storm strength would be: the magnitude of reflectance in the visible channel; how cold brightness temperatures in the water vapor and clean infrared channel are; and how much vertically integrated water there is. Thus, to characterize these statistics, we extract the following percentiles from each image and variable: 0, <ref type="bibr">1,10,25,50,75,90,99,100.</ref> To create the labels the number of lightning flashes in the image are summed. For Problem Statement 1, an image is classified as containing a thunderstorm if the image has at least one flash in the last five minutes. For Problem Statement 2, the sum of all lightning flashes in the past five minutes within the image are used for the regression target. Now that the data have been quality controlled and our features and labels have been extracted, the next step is to split that dataset into three independent sub-categories named the training, validation and testing sets. The reason for these three sub-categories is because of the relative ease at which ML methods can "memorize" the training data. This occurs because ML models can contain numerous (e.g., hundreds, thousands, or even millions) learnable parameters, thus the ML model can learn to perform well on the training data but not generalize to other non-training data, which is called over-fitting. In order to assess how over-fit a ML model is, it is important to evaluate a trained ML model on data outside of its training data (i.e., validation and testing sets).</p><p>The training dataset is the largest subset of the total amount of data. The reason the training set is the largest is because the aforementioned desired outcome of most ML models is to generalize on wide variety of examples. Typically, the amount of training data is between 70 -85% of the total amount of data available. The validation dataset, regularly 5-15% of the total dataset, is a subset of data used to assess if a ML model is over-fit and is also used for evaluating best model configurations (e.g., the depth of a decision tree). These model configurations are also known as hyperparameters. Machine learning models have numerous configurations and permutations that can be varied and could impact the skill of any one trained ML model. Thus, common practice is to systematically vary the available hyperparameter choices, also called a grid search, and then evaluate the different trained models based on the validation dataset. Hyperparamters will be discussed in more detail later. The test dataset is the last grouping that is set aside to the very end of the ML process. The test dataset is often of similar size to the validation dataset, but the key difference is that the test dataset is used after all hyperparameter variations have been concluded. The reason for this last dataset is because when doing the systematic varying of the hyperparameters the ML practitioner is inadvertently tuning a ML model to the validation dataset. One will often choose specific hyperparameters in such a way to achieve the best performance on the validation dataset. Thus, to provide a truly unbiased assessment of the trained ML model skill for unseen data, the test dataset is set aside and not used until after training all ML models.</p><p>It is common practice outside of meteorology (i.e., data science) to randomly split the total dataset into the three subsets. However, it is important to strive for independence of the various subsets. A data point in the training set should not be highly correlated to a data point in the test set. In meteorology this level of independence is often challenging given the frequent spatial and temporal autocorrelations in meteorologic data. Consider the SEVIR dataset. Each storm event has four hours of data broken into five minute time steps. For one storm event, there is a large correlation between adjacent five minute samples. Thus, randomly splitting the data would likely provide a biased assessment of the true skill of the ML model. In order to reduce the number of correlated data points across subsets, time is often used to split the dataset. For our example, we choose to split the SEVIR data up by training on 01 Jan 2017 -01 Jun 2019 and split every other week in the rest of 2019 into the validating and testing sets. This equates to a 72%, 13% and 15% split for the training, validation and test sets respectively. In the event that the total dataset is small and splitting the data into smaller subsets creates less robust statistics, a resampling method known as k-fold cross-validation (e.g., <ref type="bibr" target="#b4">Bischl et al. 2012;</ref><ref type="bibr" target="#b28">Goodfellow et al. 2016</ref>) can be used. The SEVIR dataset was sufficiently large that we chose not to do k-fold crossvalidation, but a meteorological example using it can be found in <ref type="bibr" target="#b82">Shield and Houston (2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c. Training and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) C</head><p>As stated in Section 3.a, task ( <ref type="formula" target="#formula_0">1</ref>) is to classify if an image contains a thunderstorm. Thus, the classification methods available to do this task are: logistic regression, naÃ¯ve Bayes, decision trees, random forest, gradient boosted trees and support vector machines. In order to find an optimal ML model, it is often best to try all methods available. While this might seem like a considerable amount of additional effort, the ML package used in this tutorial (i.e., scikit learn ) uses the same syntax for all methods (e.g., method.fit(ğ‘‹,ğ‘¦), method.predict(ğ‘‹ ğ‘£ ğ‘ğ‘™ )),. Thus, fitting all available methods does not require substantially more effort from the ML practitioner and will likely result in finding a best performing model.</p><p>To start off, all methods are initially trained using their default hyperparameters in scikit-learn and just one input feature, the minimum infrared brightness temperature (ğ‘‡ ğ‘ ). We choose to use ğ‘‡ ğ‘ because meteorologically it is a proxy for the depth of the storms in the domain, which is correlated to lightning formation <ref type="bibr" target="#b95">(Yoshida et al. 2009</ref>). To assess the predictive power of this variable, the distributions of ğ‘‡ ğ‘ for thunderstorms and no thunderstorms are shown in Fig. <ref type="figure">8</ref>. As expected, ğ‘‡ ğ‘ for thunderstorms show more frequent lower temperatures than non-thunderstorm images. Training all methods using ğ‘‡ ğ‘ achieves an accuracy of 80% on the validation dataset. While accuracy is a common and easy to understand metric, it is best to always use more than one metric when evaluating ML methods.</p><p>Another common performance metric for classification tasks is the Area Under the Curve (AUC). More specifically the common area metric is associated with the Receiver Operating Characteristics curve (ROC). The ROC curve is calculated from the relationship between the Probability of False Detection (POFD) and the Probability of Detection (POD). Both POFD and POD parameters are calculated from determining parameters within a contingency table which are the true positives (both the ML prediction and label say thunderstorm), false positives (ML prediction predicts thunderstorm, label has no thunderstorm), false negatives (ML prediction is no thunderstorm, label shows there is a thunderstorm) and true negatives (ML says no thunderstorm, label says no thunderstorm). The POFD and POD are defined by</p><formula xml:id="formula_17">POFD = FalsePositive TruePositive + FalsePositive . (<label>15</label></formula><formula xml:id="formula_18">) POD = TruePositive TruePositive + FalseNegative . (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>All of the ML models, except support vector machines (as coded in sklearn), can provide a probabilistic estimation of the classification (e.g., this image is 95% likely to have lightning in it). When calculating the accuracy before, we assumed a threshold of 50% to designate what the ML prediction was. In order to get the ROC curve, the threshold probability is instead varied from 0% to 100%. The resulting ROC curves for all of the ML methods except support vector machines are shown in Fig. <ref type="figure">9a</ref>. We see that for this simple one feature model, all methods are still very similar and have AUCs near 0.9 (Fig. <ref type="figure">9a</ref>), which is generally considered good performance .</p><p>An additional method for evaluating the performance of classification method is called a performance diagram (Figure <ref type="figure">9b</ref>; <ref type="bibr" target="#b78">Roebber 2009)</ref>. The performance diagram is also calculated from the contingency table, using the POD again for the y-axis, but this time the x-axis is the success ratio (SR) which is defined as</p><formula xml:id="formula_20">SR = TruePositive TruePositive + FalsePositive . (<label>17</label></formula><formula xml:id="formula_21">)</formula><p>From this diagram, several things can be gleaned about the models performance. In general, the top right corner is where 'best' performing models are found. This area is characterized by models that capture nearly all events (i.e., thunderstorms), while not predicting a lot of false alarms (i.e., false positives). This corner is also associated with high values of critical success index (CSI; filled contours Fig. <ref type="figure">9b</ref>), defined as</p><formula xml:id="formula_22">CSI = TruePositive TruePositive + FalsePositive + FalseNegative . (18)</formula><p>which is a metric that shows a model's performance without considering the true negatives. Not considering the true negatives is important because true negatives can dominate ML tasks in meteorology given the often rare nature of events with large impacts (e.g., floods, hail, tornadoes). The last set of lines on this diagram are the frequency bias contours (dashed grey lines Fig. <ref type="figure">9b</ref>). These contours indicate if a model is over-forecasting or under-forecasting.</p><p>For the simple ML models trained, even though most of them have a similar accuracy and AUC, the performance diagram suggests their performance is indeed different. Consider the tree based methods (green box; Fig. <ref type="figure">9b</ref>). They are all effectively at the same location with a POD of about 0.9 and a SR of about 0.75, which is a region that has a frequency bias of almost 1.5. Meanwhile the logistic regression, support vector machines and naÃ¯ve Bayes methods are much closer to the frequency bias line of 1, while having a similar CSI as the tree based methods. Thus, after considering overall accuracy, AUC and the performance diagram, the best performing model would be either the logistic regression, support vector machines or naÃ¯ve Bayes. At this junction, the practitioner has the option No formal peer reviewed journal states this, it is more of a rule of thumb in machine learning practice to consider if they want a slightly over-forecasting system or a slightly under-forecasting system. For the thunderstorm, no-thunderstorm task, there are not many implications for over-forecasting or under-forecasting. However, developers of a tornado prediction model may prefer a system that produces more false positives (over-forecasting; storm warned, no tornado) than false negatives (underforecasting; storm not warned, tornado) as missed events could have significant impact to life and property. It should be clear that without going beyond a single metric, this differentiation between the ML methods would not be possible.</p><p>While the previous example was simple by design, we as humans could have used a simple threshold at the intersection of the two histograms in Fig. <ref type="figure">8</ref> to achieve similar accuracy (e.g., 81%; not shown). The next logical step with the classification task would be to use all available features. One important thing to mention at this step is that it is good practice to normalize input features. Some of the ML methods (e.g., random forest) can handle inputs of different magnitudes (e.g., CAPE is on the order of 100s to 1000s, but Lifted Index is on the order of 1s to 10s), but others (e.g., logistic regression) will be unintentionally biased towards larger magnitude features if you do not scale your input features. Common scaling methods include min-max scaling and scaling your input features to have mean 0 and standard deviation of 1 (i.e., standard anomaly) which are defined mathematically as</p><formula xml:id="formula_23">minmax = ğ‘¥ -ğ‘¥ ğ‘šğ‘–ğ‘› ğ‘¥ ğ‘šğ‘ğ‘¥ -ğ‘¥ ğ‘šğ‘–ğ‘› .<label>(19)</label></formula><p>and</p><formula xml:id="formula_24">standard anom. = ğ‘¥ -ğœ‡ ğœ<label>(20)</label></formula><p>respectively. In Eq. 19, ğ‘¥ ğ‘šğ‘–ğ‘› is the minimum value within the training dataset for some input feature ğ‘¥ while ğ‘¥ ğ‘šğ‘ğ‘¥ is the maximum value in the training dataset. In Eq. 20, ğœ‡ is the mean of feature ğ‘¥ in the training dataset and ğœ is the standard deviation. For this paper, the standard anomaly is used. Using all available input features yields an accuracy of 90%, 84%, 86%, 91%, 90%, 89% for logistic regression, naÃ¯ve Bayes, decision tree, random forest, gradient boosted trees and support vector machines respectively. Beyond the relatively good accuracy, the ROC curves are shown in Fig. <ref type="figure">10a</ref>. This time there are generally two sets of curves, one better performing group (logistic regression, random forest, gradient boosted trees and support vector machines) with AUCs of 0.97 and a worse performing group (naÃ¯ve Bayes and decision tree) AUCs around 0.87. This separation coincides with the flexibility of the classification methods. The better performing groups are better set to deal with many features and non-linear interactions of the features, while the worse performing group is a bit more restricted in how it combines many features. Considering F . 10. As in Figure <ref type="figure">9</ref>, but now trained with all available predictors. The annotations from Fig. <ref type="figure">9</ref> have been removed.</p><p>the performance diagram (Fig. <ref type="figure">10b</ref>), the same grouping of high AUC performing models have higher CSI scores (&gt; 0.8) and have little to no frequency bias. Meanwhile the lower AUC performing models have lower CSI (0.75) and NB has a slight overforecasting bias. Overall, the ML performance on classifying if an image has a thunderstorm is doing well with all predictors. While a good performing model is a desired outcome of ML, at this point we do not know how the ML is making its predictions. This is part of the 'black-box' issue of ML and does not lend itself to being consistent with the ML user's prior knowledge (see note in introduction on consistency; <ref type="bibr" target="#b70">Murphy 1993)</ref>.</p><p>In order to alleviate some of opaqueness of the ML black-box, one can interrogate the trained ML models by asking: "What input features are most important to the decision?" and "Are the patterns the ML models learned physical (e.g., follow meteorological expectation)?". The techniques named permutation importance <ref type="bibr" target="#b8">(Breiman 2001;</ref><ref type="bibr" target="#b54">Lakshmanan et al. 2015)</ref> and accumulated local effects (ALE; <ref type="bibr" target="#b1">Apley and Zhu 2020)</ref> are used to answer these two questions respectively. Permutation importance is a method in which the relative importance of a input feature is quantified by considering the change in evaluation metric (e.g., AUC) when that input variable is shuffled (i.e., randomized). The intuition is that the most important variables when shuffled will cause the largest change to the evaluation metric. There are two main flavors of permutation importance, named single-pass and multi-pass. Single-pass permutation importance goes through each input variable and shuffles them one by one, calculating the change in the evaluation metrics. Multi-pass permutation importance uses the result of the single-pass, but progres-sively permutes features. In other words, features are successively permuted in the order that they were determined as important (most important then second most important etc) from the single pass but are now left shuffled. The specific name for the method we have been describing is the backward multi-pass permutation importance. The backward name comes from the direction of shuffling, starting will all variables unshuffled and shuffling more and more of them. There is the opposite direction, named forward multi-pass permutation importance, where the starting point is that all features are shuffled to start. Then each feature is unshuffled in order of their importance from the single-pass permutation importance. For visual learners, see the animations (for the backward direction; Fig. <ref type="figure">ES4</ref> and Fig. <ref type="figure">ES5</ref>) in the supplement of <ref type="bibr" target="#b66">McGovern et al. (2019)</ref>. The reason for doing multi-pass permutation importance is because correlated features could result in falsely identifying non-important variables using the single pass permutation importance. The best analysis of the permutation test is to use both the single pass and multi-pass tests in conjunction.</p><p>The top five most important features for the better performing models (i.e., logistic regression, random forest and gradient boosted trees) as determined by permutation importance are shown in Fig. <ref type="figure" target="#fig_0">11</ref>. For all ML methods both the single and multi-pass test show that the maximum vertically integrated liquid is the most important feature, while the minimum brightness temperature from the clean infrared and midtropospheric water vapor channels are found within the top 5 predictors (except multi-pass test for logistic regression). In general, the way to interpret these are to take the consensus over all models which features are F . 11. Backward permutation importance test for the best performing classification ML models. Single pass results are in the top row, while multi-pass forward results are for the bottom row. Each column corresponds to a different ML method: logistic regression (a,d), random forest (b,e) and gradient boosted trees (c,f). Bars are colored by their source, yellow for the vertically integrated liquid (VIL), red for the infrared (IR), blue for water vapor (WV) and black for visible (VIS). Number subscripts correspond to the percentile of that variable. The dashed black line is the original AUC value when all features are not shuffled. important. At this point it time to consider if the most important predictors make meteorological sense. Vertically integrated liquid has been shown to have a relationship to lightning (e.g., <ref type="bibr" target="#b89">Watson et al. 1995)</ref> and is thus plausible to be the most important predictor. Similarly, the minimum brightness temperature at the water vapor and clean infrared channels also makes physical sense because lower temperatures are generally associated with taller storms. We could also reconcile the maximum infrared brightness temperature (Fig <ref type="figure" target="#fig_0">11a</ref>) as a proxy for the surface temperature which correlates to buoyancy, but note that the relative change in AUC with this feature is quite small. Conversely, any important predictors that don't align with traditional meteorological knowledge may require further exploration to determine why the model is placing such weight on those variables. Does the predictor have some statistical correlation with the meteorological event that is unexplained by past literature, or are there nonphysical characteristics of the data that may be influencing the model during training?</p><p>In the latter case, it is possible that your model might be getting the right answer for the wrong reasons.</p><p>Meanwhile minimum brightness temperature at both the water vapor and clean infrared channels also make physical sense since lower temperatures are related with taller storms. We could also reconcile the max infrared brightness temperature as a proxy for the surface temperature, which correlates to buoyancy, but not that the relative change in AUC with this feature is quite small. If any the top predictors don't make sense meteorologically, then your model might be getting the right answer for the wrong reasons.</p><p>Accumulated local effects are where small changes to input features and their associated change on the output of the model are quantified. The goal behind ALE is to investigate the relationship between an input feature and the output. ALE is performed by binning the data based on the feature of interest. Then for each example in each bin, the feature value is replaced by the edges of the bin. The mean difference in the model output from the replaced feature F . 12. Accumulated local effects (ALE) for (a) the maximum Vertically Integrated Liquid (VIL max ), (b) the minimum brightness temperature from infrared (IR min ) and (c) the minimum brightness temperature from the water vapor channel (WV min ). Lines correspond to all the ML methods trained (except support vector machines) and colors match Fig. <ref type="figure">9</ref>. Grey histograms in the background are the counts of points in each bin.</p><p>value is then used as the ALE for that bin. This process is repeated for all bins which result in a curve. For example, the ALE for some of the top predictors of the permutation test are shown in in Fig. <ref type="figure">12</ref>. At this step, the ALEs can be mainly used to see if the ML models have learned physically plausible trends with input features. For the vertically integrated liquid, all models show that as the max vertically integrated liquid increases from about 2 ğ‘˜ğ‘” ğ‘š 2 to 30 ğ‘˜ğ‘” ğ‘š 2 the average output probability of the model will increase, but values larger than 30 ğ‘˜ğ‘” ğ‘š 2 generally all have the same local effect on the prediction (Fig. <ref type="figure">12a</ref>). As for the minimum clean infrared brightness temperature, the magnitude of the average change is considerably different across the different models, but generally all have the same pattern. As the minimum temperature increases from -88 â€¢ C to -55 â€¢ C, the mean output probability decreases: temperatures larger than -17 â€¢ C have no change (Fig. <ref type="figure">12b</ref>). Lastly, all but the logistic regression shows a similar pattern with the minimum water vapor brightness temperature, but notice the magnitude of the y-axis (Fig. <ref type="figure">12c</ref>). Much less change occurs with this feature. For interested readers, additional interpretation techniques and examples can be found in <ref type="bibr" target="#b69">Molnar (2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) R</head><p>As stated in Section 3.a, task (2) is to predict the number of lightning flashes inside an image. Thus, the regression methods available to do this task are: linear regression, decision tree, random forest, gradient boosted trees and support vector machines. Similar to task (1) a simple scenario is considered first, using ğ‘‡ ğ‘ as the lone predictor. Figure <ref type="figure" target="#fig_7">13</ref> shows the general relationship between ğ‘‡ ğ‘ and the number of flashes in the image. For ğ‘‡ ğ‘ &gt; -25 â€¢ ğ¶, most images do not have any lightning, while ğ‘‡ ğ‘ &lt; -25 â€¢ ğ¶ shows a general increase of lightning flashes. Given there are a lot of images with 0 flashes (approximately 50% of the total dataset; black points in Fig. <ref type="figure" target="#fig_7">13</ref>), the linear methods will likely struggle to capture a skillful prediction. One way to improve performance would be to only predict the number of flashes on images where there is non-zero flashes. While this might not seem like a viable way forward since non-lightning cases would be useful to predict, in practice we could leverage the very good performance of the classification model from Section 3.c.1, and then use the trained regression on images that are confident to have at least one flash in them. An example of this done in the literature is <ref type="bibr">Gagne et al. (2017)</ref> where hail size predictions were only made if the classification model said there was hail.</p><p>As before, all methods are fit on the training data initially using the default hyperparameters. A common way to compare regression model performance is to create a oneto-one plot, which has the predicted number of flashes on the x-axis and the true measured number of flashes on the y-axis. A perfect model will show all points tightly centered along the diagonal of the plot. This is often the quickest qualitative assessment of how a regression model is performing. While ğ‘‡ ğ‘ was well suited for the classification of thunderstorm/no-thunderstorm, it is clear that fitting a linear model to the data in Fig. <ref type="figure" target="#fig_7">13</ref> did not do well (Fig. <ref type="figure" target="#fig_8">14a</ref>,<ref type="figure">e</ref>), leading to a strong over-prediction of the number of lightning flashes in an images with less than 100 flashes, while under-predicting the number of flashes for images with more than 100 flashes. The tree based methods tend to do better, but there is still a large amount of scatter and an over estimation of storms with less than 100 flashes.</p><p>In order to tie quantitative metrics to the performance of each model the following are common metrics calculated: Mean Bias, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE) and coefficient of determination (ğ‘… 2 ). Their mathematical representations are the following:</p><formula xml:id="formula_25">Bias = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘—=1 (ğ‘¦ ğ‘— -Å· ğ‘— ),<label>(21)</label></formula><formula xml:id="formula_26">MAE = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘—=1 |ğ‘¦ ğ‘— -Å· ğ‘— | (22) RMSE = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘—=1 (ğ‘¦ ğ‘— -Å· ğ‘— ) 2 (23) R 2 = 1 - ğ‘ ğ‘—=1 (ğ‘¦ ğ‘— -Å· ğ‘— ) 2 ğ‘ ğ‘—=1 (ğ‘¦ ğ‘— -È³) 2<label>(24)</label></formula><p>All of these metrics are shown in Fig. <ref type="figure">15</ref>. In general, the metrics give a more quantitative perspective to the oneto-one plots. The poor performance of the linear methods shows, with the two worst performances being the support vector machines and linear regression with biases of 71 and 6 flashes respectively. While no method provides remarkable performance, the random forest and gradient boosted trees perform better with this single feature model (show better metrics holistically).</p><p>As before, the next logical step is to use all available features to predict the number of flashes: those results are found in Fig. <ref type="figure">16</ref> and 17. As expected, the model performance increases. Now all models show a general correspondence between the predicted number of flashes and the true number of flashes in the one-to-one plot (Fig. <ref type="figure">16</ref>). Meanwhile the scatter for random forest and gradient boosted trees has reduced considerably when comparing to the single input models (Fig. <ref type="figure">16c</ref>,<ref type="figure">d</ref>). While comparing the bias of the models trained with all predictors is relatively similar, the other metrics are much improved, showing large reductions in MAE, RMSE and increases in ğ‘… 2 (Fig. <ref type="figure" target="#fig_3">17</ref>) for all methods except decision trees. This reinforces that fact that similar to the classification example, it is always good to compare more than one metric.</p><p>Since the initial fitting of the ML models used the default parameters, there might be room for tuning the models to have better performance. Here we will show an example of some hyperparameter tuning of a random forest. The common parameters that can be altered in a random forest include, the maximum depth of the trees (i.e., number of decisions in a tree) and the number of trees in the forest. The formal hyperparameter search will use the full training dataset, and systematically vary the depth of the trees from 1 to 10 (in increments of 1) as well as the number of trees from 1 to 100 <ref type="bibr">(1,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">25,</ref><ref type="bibr">50,</ref><ref type="bibr">100)</ref>. This results in 60 total models that are trained.</p><p>In order to evaluate which is the best configuration, the same metrics as before are shown in Fig. <ref type="figure">18</ref> as a function of the depth of the trees. The random forest quickly gains skill with added depth beyond one, with all metrics improving for both the training (dashed lines) and validation datasets (solid lines). Beyond a depth of four, the bias, MAE and RMSE all stagnate, but the ğ‘… 2 value increases until a depth of eight where the training data continue to increase. There does not seem to be that large of an effect of increasing the number of trees beyond 10 (color change of lines). The characteristic of increasing training metric skills but no increase (or a decrease) to validation data skill is the overfitting signal we discussed in Section 3.b. Thus, the best random forest model choice for predicting lightning flashes is a random forest with a max depth of eight and a total of 10 trees. The reason we choose 10 trees, is because in general choosing a simpler model is less computationally expensive to use as well as a more interpretable than a model with 1000 trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d. Testing</head><p>As mentioned before, the test dataset is the dataset you hold out until the end when all hyperparameter tuning has finished so that there is no unintentional tuning of the final model configuration to a dataset. Thus, now that we have evaluated the performance of all our models on the validation dataset it is time to run the same evaluations as in Section 3.c.1 and Section 3.c.2. These test results are the end performance metrics that should be interpreted as the expected ML performance on new data (e.g., the ML applied in practice). For the ML models here the metrics </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary and Future Work</head><p>This manuscript was the first of two Machine Learning (ML) tutorial papers designed for the operational meteorology community. This paper supplied a survey of some of the most common ML methods. All ML methods described here are considered supervised methods, meaning the data the models are trained from include pre-labeled truth data. The specific methods covered included linear regression, logistic regression, decision trees, random forests, gradient boosted decision trees, naÃ¯ve Bayes and support vector machines. The overarching goal of the paper was to introduce the ML methods in such a way that ML methods are more familiar to readers as they encounter them in the operational community and within the general meteorological literature. Moreover, this manuscript provided ample references of published meteorological examples as well as open-source code to act as catalysts for readers to adapt and try ML on their own datasets and in their workflows.</p><p>Additionally, this manuscript provided a tutorial example of how to apply ML to a couple meteorological tasks using the Storm EVent ImageRy dataset (SEVIR; Veillette et al. 2020) dataset. We:</p><p>1. Discussed the various steps of preparing data for ML (i.e., removing artifacts; engineering features, train/val/test splits; Section 3.b)</p><p>2. Conducted a classification task to predict if satellite images had lightning within them. This section included discussions of training, evaluation and interrogation of the trained ML models (Section 3.c.1)</p><p>3. Exhibited a regression task to predict the number of lightning flashes in a satellite image. This section also contained discussions of training/evaluation as well as an example of hyperparameter tuning (Section 3.c.2) 4. Released python code to conduct all steps and examples in this manuscript (see Data Availability Statement)</p><p>The follow on paper in this series will discuss a more complex, yet potentially more powerful, grouping of ML F . 15. Validation dataset metrics for all ML models. Colors are the same as in Fig. <ref type="figure" target="#fig_8">14</ref>. Exact numerical value is reported on top of each bar.</p><p>methods: neural networks and deep learning. Like a lot of the ML methods described in this paper, neural networks aren't necessarily new <ref type="bibr" target="#b79">(Rumelhart et al. 1986</ref>) and were first applied to meteorology topics decades ago (e.g., <ref type="bibr" target="#b43">Key et al. 1989;</ref><ref type="bibr" target="#b56">Lee et al. 1990</ref>). Although, given the exponential growth of computing resources and dataset sizes, research using neural networks and deep learning in meteorology has been accelerating (e.g., Fig <ref type="figure">1c</ref>; <ref type="bibr" target="#b22">Gagne et al. 2019;</ref><ref type="bibr" target="#b50">Lagerquist et al. 2020;</ref><ref type="bibr" target="#b15">Cintineo et al. 2020;</ref><ref type="bibr">Chase et al. 2021;</ref><ref type="bibr">Hilburn et al. 2021;</ref><ref type="bibr" target="#b52">Lagerquist et al. 2021;</ref><ref type="bibr">Molina et al. 2021;</ref><ref type="bibr" target="#b77">Ravuri et al. 2021</ref>). Thus, it is important that operational meteorologists also understand the basics of neural networks and deep learning. F . 16. As in Fig. <ref type="figure" target="#fig_8">14</ref>, but now the x-axis is provided from the ML models trained with all available input features. F . 17. As in Fig. <ref type="figure">15</ref>, but for ML models trained with all available input features. F . A1. As in Figure <ref type="figure">9</ref>, but now for the test dataset</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 F . 1 .</head><label>11</label><figDesc>Search results for the Meteorology and Atmospheric Science category when searching abstracts for machine learning methods and severe weather. Machine learning keywords searched were: linear regression, logistic regression, decision trees, random forest, gradient boosted trees, support vector machines, k-means, k-nearest, empirical orthogonal functions, principal component analysis, self organizing maps, neural networks, convolutional neural networks and unets. Severe weather keywords searched were: tornadoes, hail, hurricanes and tropical cyclones. (a) Counts of publications per year for all papers in the Meteorology and Atmospheric Science category (black line; reduced by one order of magnitude), machine learning topics (blue line) and severe weather topics (red line). (b) Same as (a), but with the two subtopics normalized by the total number of Meteorology and Atmospheric Science papers. (c) Number of neural network papers (including convolutional and unets) published in Meteorology and Atmospheric sciences. All data are derived from Clarivate Web of Science.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Visualizing the probability of a input feature given the class label. This example is created from five minute weather station observations from near Marquette, Michigan (years included: 2005 -2020). Precipitation phase was determined by the present weather sensor. The histogram is the normalized number of observations in that temperature bin, while the smooth curves are the normal distribution fit to the data. Red are raining instances, blue are snowing instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>Support vector machine classification examples. (a) ideal (synthetic) data where the x and y axis are both input features, while the color designates what class each point belongs to. The decision boundary learned by the support vector machine is the solid black line, while the margin is shown by the dashed lines. (b) a real world example using NAM18Z forecasts of U and V wind and tipping bucket measurements of precipitation. Blue plus markers are raining instances and the red minus signs are non-raining instances. Black lines are the decision boundary and margins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F . 7 .</head><label>7</label><figDesc>An example storm image from the Storm EVent ImageRy dataset. This event is from 06 August 2018. (a) the visible reflectance (b) the mid-tropospheric water vapor brightness temperature (c) the clean infrared brightness temperatures (d) the vertically integrated liquid retrieved from NEXRAD and (e) gridded GLM number of flashes. (a) also has annotated locations of representative percentiles that were engineered features used for the ML models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>https://scikit-learn.org/stable/ F . 8. The normalized distributions of minimum brightness temperature (ğ‘‡ ğ‘ ) from the clean infrared channel for thunderstorm images (blue;T-storm) and non-thunderstorm images (red; No T-storm).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>Performance metrics from the simple classification (only using ğ‘‡ ğ‘ ). (a) Receiver Operating Characteristic (ROC) curves for each ML model (except support vector machines), logistic regression (LgR; blue), naÃ¯ve Bayes (NB; red), decision tree (DT; geen), random forest (RF; yellow) and gradient boosted trees (GBT; light green). The area under the ROC curve is reported in the legend. (b) Performance Diagram for all ML models (same colors as a). Color fill is the corresponding CSI value for each Success Ratio-Probability of Detection (SR, POD) pair. Dashed contours are the frequency bias.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>F . 13 .</head><label>13</label><figDesc>The training data relationship between the minimum brightness temperature from infrared (ğ‘‡ ğ‘ ) and the number of flashes detected by GLM. All non-thunderstorm images (number of flashes equal to 0) are in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>F . 14 .</head><label>14</label><figDesc>The one-to-one relationship between the predicted number of lightning flashes from the ML learning models trained on only ğ‘‡ ğ‘ (x-axis; Å·) and the number of measure flashes from GLM (y-axis; ğ‘¦). Each marker is one observation. Meanwhile areas with more than 100 points in close proximity are shown in the colored boxes. The lighter the shade of the color, the higher density of points. (a) linear regression (LnR; reds), (b) decision tree (DT; blues), (c) random forest (RF; oranges), (d) gradient boosted trees (GBT; purples) and (e) linear support vector machines (SVM; greys). are very similar as the validation set. For brevity the extra figures are included in the appendix (Fig A1-A3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>F</head><figDesc>. 18. Hyperparameter tuning of a random forest for predicting the number of lightning flashes. All input features are used.Solid lines are the validation dataset while the dashed lines are the training data. The vertical dotted line is the depth of trees where over-fitting begins. a) b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="2,70.20,81.08,432.00,143.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="13,70.20,81.07,432.03,321.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="21,53.07,241.53,432.02,288.01" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>F . A3. As in Fig.15, but for the test dataset</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This material is based upon work supported by the <rs type="funder">National Science Foundation</rs> under Grant No. ICER-2019758, supporting authors RJC, AM and AB. Author DRH was provided support by <rs type="funder">NOAA/Office of Oceanic and Atmospheric Research under NOAA-University of Oklahoma</rs> Cooperative Agreements number <rs type="grantNumber">NA16OAR4320115</rs> and number <rs type="grantNumber">NA21OAR4320204</rs>, <rs type="funder">U.S. Department of Commerce</rs>. The scientific results and conclusions, as well as any views or opinions expressed herein, are those of the authors and do not necessarily reflect the views of NOAA or the <rs type="funder">Department of Commerce</rs>.</p><p>We want to acknowledge the work put forth by the authors of the SEVIR dataset (<rs type="person">Mark S. Veillette</rs>, <rs type="person">Siddharth Samsi</rs> and <rs type="person">Christopher J. Mattioli</rs>) for making a high-quality free dataset. We would also like to acknowledge the open-source python community for providing their tools for free. Specifically, we acknowledge <rs type="person">Google Colab</rs> (<rs type="affiliation">Bisong 2019), Anaconda (Anaconda 2020</rs>), scikit-learn <ref type="bibr" target="#b74">(Pedregosa et al. 2011</ref>), Pandas (Wes McKinney 2010), <rs type="funder">Numpy</rs> <ref type="bibr" target="#b30">(Harris et al. 2020)</ref> and Jupyter <ref type="bibr" target="#b44">(Kluyver et al. 2016)</ref>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_D3AJnp7">
					<idno type="grant-number">NA16OAR4320115</idno>
				</org>
				<org type="funding" xml:id="_C7ARwkU">
					<idno type="grant-number">NA21OAR4320204</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability statement.</head><p>As an effort to catalyse the use and trust of machine learning within meteorology we have supplied a github repository with a code tutorial of a lot of the same things discussed in this paper. The latest version of github repository can be located here: <ref type="url" target="https://github.com/ai2es/WAF_ML_Tutorial_Part1">https:  //github.com/ai2es/WAF_ML_Tutorial_Part1</ref>. If you are interested in the version of the repository that was available at time of publication please see the zendo archive of version 1 here: URL. The original github repo for SEVIR is located here: <ref type="url" target="https://github.com/MIT-AI-Accelerator/neurips-2020-sevir">https://github.com/  MIT-AI-Accelerator/neurips-2020-sevir</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing dataset figures</head><p>This appendix contains the test dataset evaluations for both the classification task (Fig. <ref type="figure">A1</ref>) and the regression task (Fig. <ref type="figure">A2-A3</ref>). Results are largely the same as the validation set, so to save space they were included here.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><surname>Anaconda</surname></persName>
		</author>
		<ptr target="https://docs.anaconda.com/" />
		<title level="m">Anaconda software distribution. Anaconda Inc</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visualizing the effects of predictor variables in black box supervised learning models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Apley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1111/rssb.12377</idno>
		<ptr target="https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12377" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1059" to="1086" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A statistical model to predict the extratropical transition of tropical cyclones</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Camargo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tippett</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-19-0045.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/35/2/waf-d-19-0045.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="451" to="466" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Use of regression techniques to predict hail size and the probability of large hail</title>
		<author>
			<persName><forename type="first">J</forename><surname>Billet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="154" to="164" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Resampling methods for meta-model validation with recommendations for evolutionary computation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>O. Mersmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName><surname>Weihs</surname></persName>
		</author>
		<idno type="DOI">10.1162/EVCO_a_00069</idno>
		<ptr target="https://doi.org/10.1162/EVCO_a_00069" />
	</analytic>
	<monogr>
		<title level="j">Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="275" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Google Colaboratory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bisong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4842-4470-8_7</idno>
		<ptr target="https://doi.org/10.1007/978-1-4842-4470-8_7" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Apress</publisher>
			<biblScope unit="page" from="59" to="64" />
			<pubPlace>Berkeley, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine learning for earth system observation and prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bonavita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coauthors</forename></persName>
		</author>
		<idno type="DOI">10.1175/BAMS-D-20-0307.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/bams/102/4/BAMS-D-20-0307.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="710" to="E716" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<title level="m">Classification and Regression Trees</title>
		<meeting><address><addrLine>New York, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<ptr target="https://doi.org/10.1023/A:1010933404324" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Calibration of Machine Learning-Based Probabilistic Hail Predictions for Operational Forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccorkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-19-0105.1</idno>
		<ptr target="https://doi.org/10.1175/WAF-D-19-0105.1" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="168" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nws forecasters&apos; perceptions and potential uses of trustworthy ai/ml for hazardous weather risks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Cains</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coauthors</forename></persName>
		</author>
		<ptr target="https://ams.confex.com/ams/102ANNUAL/meetingapp.cgi/Paper/393121" />
	</analytic>
	<monogr>
		<title level="m">annual meeting</title>
		<imprint>
			<publisher>american meteorological society</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">102</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">2021: A dualfrequency radar retrieval of two parameters of the snowfall particle size distribution using a neural network</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Nesbitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Mcfarquhar</surname></persName>
		</author>
		<idno type="DOI">10.1175/JAMC-D-20-0177.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/apme/60/3/JAMC-D-20-0177.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology and Climatology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="341" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The diagnosis of upper-level humidity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="613" to="619" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An empirical model for assessing the severe weather potential of developing convection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cintineo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavolonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sieglaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="639" to="653" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical model for assessing the severe weather potential of developing convection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cintineo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coauthors</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="331" to="345" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Noaa probsevere v2.0-probhail, probwind, and probtor</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Cintineo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pavolonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Sieglaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cronce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brunner</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-19-0242.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/35/4/wafD190242.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1523" to="1543" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual-polarization radar retrievals of coastal pacific northwest raindrop size distribution parameters using random forest regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Conrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Zagrodnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Mass</surname></persName>
		</author>
		<idno type="DOI">10.1175/JTECH-D-19-0107.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/atot/37/2/jtech-d-19-0107.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Atmospheric and Oceanic Technology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="229" to="242" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2021: Climatology of linear mesoscale convective system morphology in the united states based on the random-forests method</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1175/JCLI-D-20-0862.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/clim/34/17/JCLI-D-20-0862.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Climate</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="7257" to="7276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Application of machine learning to large hail prediction -the importance of radar reflectivity, lightning occurrence and convective parameters derived from era5</title>
		<author>
			<persName><forename type="first">B</forename><surname>Czernecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taszarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>PÃ³Å‚rolniczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kolendowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wyszogrodzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Szturc</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.atmosres.2019.05.010</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0169809519300900" />
	</analytic>
	<monogr>
		<title level="j">Atmospheric Research</title>
		<imprint>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page" from="249" to="262" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Using mping data to generate random forests for precipitation type forecasts. 14th Conference on Artificial and Computational Intelligence</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>and its Applications to the Environmental Sciences, 4.2.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using machine learning to generate storm-scale probabilistic guidance of severe weather hazards in the warn-onforecast system</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Flora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Potvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Handler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mc-Govern</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-20-0194.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/mwre/149/5/MWR-D-20-0194.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1535" to="1557" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Machine learning for stochastic parameterization: Generative adversarial networks in the lorenz &apos;96 model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monahan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.04711" />
	</analytic>
	<monogr>
		<title level="j">Journal of Advances in Modeling Earth Systems, Conditionally Accepted</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Classification of convective areas using decision trees</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brotzge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Atmospheric and Oceanic Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1341" to="1353" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Severe hail prediction within a spatiotemporal relational data mining framework</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brotzge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
		<meeting><address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">2017: Storm-based probabilistic hail forecasting with machine learning applied to convection-allowing ensembles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sobash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1819" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Machine learning classification of significant tornadoes and hail in the united states using era5 proximity soundings</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Gensini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Converse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Ashley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taszarek</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-21-0056.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/36/6/WAF-D-21-0056.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2143" to="2160" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Use of Model Output Statistics (MOS) in Objective Weather Forecasting</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Glahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Meteor</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1203" to="1211" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">NaÃ¯ve bayesian precipitation type retrieval from satellite using a cloud-top and ground-radar matched climatology</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Grams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Kirstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gourley</surname></persName>
		</author>
		<idno type="DOI">10.1175/JHM-D-16-0058.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/hydr/17/10/jhm-d-16-0058_1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrometeorology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2649" to="2665" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coauthors</forename></persName>
		</author>
		<idno type="DOI">10.1038/s41586-020-2649-2</idno>
		<ptr target="https://doi.org/10.1038/s41586-020-2649-2" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="issue">7825</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dendrology in numerical weather prediction: What random forests and logistic regression tell us about forecasting. Mon</title>
		<author>
			<persName><forename type="first">G</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schumacher</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-17-0307.1</idno>
		<ptr target="https://doi.org/10.1175/MWR-D-17-0307.1" />
	</analytic>
	<monogr>
		<title level="j">Wea. Rev</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="1785" to="1812" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Money doesn&apos;t grow on trees, but forecasts do: Forecasting extreme precipitation with random forests. Mon</title>
		<author>
			<persName><forename type="first">G</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schumacher</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-17-0250.1</idno>
		<ptr target="https://doi.org/10.1175/MWR-D-17-0250.1" />
	</analytic>
	<monogr>
		<title level="j">Wea. Rev</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="1571" to="1600" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">2021: Development and interpretation of a neural-network-based synthetic radar reflectivity estimator using goes-r satellite observations</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Hilburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ebert-Uphoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<idno>1175/JAMC-D-20-0084.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/apme/60/1/jamc-d-20-0084.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology and Climatology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Forecasting severe weather with random forests</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Schumacher</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-19-0344</idno>
		<ptr target="https://doi.org/10.1175/MWR-D-19-0344" />
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2135" to="2161" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<ptr target="https://journals.ametsoc.org/view/journals/mwre/148/5/mwr-d-19-0344.1.xml" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Forecasting excessive rainfall with random forests and a deterministic convection-allowing model</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Schumacher</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-21-0026.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/36/5/WAF-D-21-0026.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1693" to="1711" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ridge regression: Biased estimation for nonorthogonal problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
		<idno type="DOI">10.1080/00401706.1970.10488634</idno>
		<ptr target="https://www.tandfonline.com/doi/pdf/10.1080/00401706.1970.10488634" />
		<imprint>
			<date type="published" when="1970">1970. 1970</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="55" to="67" />
		</imprint>
	</monogr>
	<note type="report_type">Technometrics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very simple classification rules perform well on most commonly used datasets</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1022631118932</idno>
		<ptr target="https://doi.org/10.1023/A:1022631118932" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="63" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Short-term tropical cyclone intensity forecasting from satellite imagery based on the deviation angle variance technique</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Tyo</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-19-0102.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/35/1/waf-d-19-0102.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="285" to="298" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weather radar ground clutter. part i: Identification, modeling, and simulation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hubbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meymaris</surname></persName>
		</author>
		<idno type="DOI">10.1175/2009JTECHA1159.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/atot/26/7/2009jtecha1159_1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Atmospheric and Oceanic Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1165" to="1180" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Classifying convective storms using machine learning</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Jergensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lagerquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-19-0170.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/35/2/waf-d-19-0170.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="537" to="559" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kalnay</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511802270</idno>
		<ptr target="https://doi.org/10.1017/CBO9780511802270" />
		<title level="m">Atmospheric Modeling, Data Assimilation and Predictability</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Classification of merged AVHRR and SMMR Arctic data with neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Key</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maslanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schweiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1331" to="1338" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Jupyter notebooks -a publishing format for reproducible computational workflows. Positioning and Power in Academic Publishing: Players, Agents and Agendas</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kluyver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coauthors</forename></persName>
		</author>
		<editor>F. Loizides, and B. Schmidt</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IOS Press</publisher>
			<biblScope unit="page" from="87" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An objective model for identifying secondary eyewall formation in hurricanes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Kossin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sitkowski</surname></persName>
		</author>
		<idno type="DOI">10.1175/2008MWR2701.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/mwre/137/3/2008mwr2701.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="876" to="892" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the optimality of naÃ¯ve bayes with dependent binary features</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2005.12.001</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0167865505003582" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="830" to="837" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Geospatial qpe accuracy dependence on weather radar network configurations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kurdzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Joback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Kirstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y N</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.1175/JAMC-D-19-0164.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/apme/59/11/JAMC-D-19-0164.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology and Climatology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1773" to="1792" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Precipitation estimates from msg seviri daytime, nighttime, and twilight data with random forests</title>
		<author>
			<persName><forename type="first">M</forename><surname>KÃ¼hnlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Appelhans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>NauÃŸ</surname></persName>
		</author>
		<idno type="DOI">10.1175/JAMC-D-14-0082.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/apme/53/11/jamc-d-14-0082.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology and Climatology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2457" to="2480" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Numerical Weather Prediction/Data Assimilation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lackmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>American Meteorological Society</publisher>
			<biblScope unit="page" from="274" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep learning on three-dimensional multiscale data for nexthour tornado prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lagerquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Homeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J G</forename><surname>Ii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-19-0372.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/mwre/148/7/mwrD190372.xml" />
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2837" to="2861" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Machine learning for real-time prediction of damaging straight-line convective wind</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lagerquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Weather and Forecasting</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2175" to="2193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Using deep learning to nowcast the spatial coverage of convection from himawari-8 satellite data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lagerquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ebert-Uphoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kumler</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-21-0096</idno>
		<ptr target="https://doi.org/10.1175/MWR-D-21-0096" />
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3897" to="3921" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<ptr target="https://journals.ametsoc.org/view/journals/mwre/149/12/MWR-D-21-0096.1.xml" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Which polarimetric variables are important for weather/no-weather discrimination?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lakshmanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Karstens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ryzhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berkseth</surname></persName>
		</author>
		<idno type="DOI">10.1175/JTECH-D-13-00205.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/atot/32/6/jtech-d-13-00205_1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Atmospheric and Oceanic Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1209" to="1223" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Subseasonal predictions of tropical cyclone occurrence and ace in the s2s dataset. Weather and Forecasting</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Camargo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vitart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tippett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-19-0217.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/35/3/waf-d-19-0217.1.xml" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="921" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A neural network approach to cloud classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Welch</surname></persName>
		</author>
		<idno type="DOI">10.1109/36.58972</idno>
		<ptr target="https://doi.org/10.1109/36.58972" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="846" to="855" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A causal inference model based on random forests to identify the effect of soil moisture on precipitation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coauthors</forename></persName>
		</author>
		<idno type="DOI">10.1175/JHM-D-19-0209.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/hydr/21/5/jhm-d-19-0209.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrometeorology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1115" to="1131" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Loken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Karstens</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-19-0258.1/4951271/wafd190258.pdf</idno>
		<ptr target="https://journals.ametsoc.org/waf/article-pdf/doi/10.1175/WAF-D-19-0258.1/4951271/wafd190258.pdf" />
		<title level="m">Generating Probabilistic Next-Day Severe Weather Forecasts from Convection-Allowing Ensembles Using Random Forests. Weather and Forecasting</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Comparing and interpreting differently-designed random forests for next-day severe weather hazard prediction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Loken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-21-0138.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/aop/WAF-D-21-0138.1/WAF-D-21-0138.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Application of statistical methods in weather prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Malone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="806" to="815" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Improving radar-based precipitation nowcasts with machine learning using an approach based on random forest. Weather and Forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorteberg</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-20-0080.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/35/6/waf-d-20-0080.1.xml" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2461" to="2478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Goes-17 advanced baseline imager performance recovery summary</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccorkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Naarden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Efremova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coakley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krimchansky</surname></persName>
		</author>
		<idno type="DOI">10.1109/IGARSS40859.2019.9044466</idno>
		<ptr target="https://doi.org/10.1109/IGARSS40859.2019.9044466" />
	</analytic>
	<monogr>
		<title level="m">IGARSS 2019 -2019 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">2021: The need for ethical, responsible, and trustworthy artificial intelligence for environmental sciences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ebert-Uphoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J G I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bostrom</surname></persName>
		</author>
		<idno>2112.08453</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Solar energy prediction: An international contest to initiate interdisciplinary research on compelling meteorological problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Basara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hamill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Margolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1395" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Enhancing understanding and improving prediction of severe weather through spatiotemporal relational learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Basara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="50" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Making the black box more transparent: Understanding the physical implications of machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lagerquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jergensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Homeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>early online release</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Probabilistic 0-1-h convective initiation nowcasts that combine geostationary satellite observations and numerical weather prediction model data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mecikalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jewett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>AhÄ³evych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology and Climatology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1039" to="1059" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">2021: A benchmark to test generalization capabilities of deep learning methods to classify severe convective storms in a changing climate</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Prein</surname></persName>
		</author>
		<idno type="DOI">10.1029/2020EA001490</idno>
		<idno>EA001490 2020EA001490</idno>
		<ptr target="https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020EA001490" />
	</analytic>
	<monogr>
		<title level="j">Earth and Space Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">490</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="https://christophm.github.io/interpretable-ml-book" />
		<title level="m">Interpretable Machine Learning. 2nd</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">What is a good forecast? an essay on the nature of goodness in weather forecasting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1175/1520-0434(1993)008&lt;0281:WIAGFA&gt;2.0.CO;2</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/8/2/1520-0434_1993_008_0281_wiagfa_2_0_co_2.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="281" to="293" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Aviation turbulence forecasting at upper levels with machine learning techniques based on regression trees</title>
		<author>
			<persName><forename type="first">D</forename><surname>MuÃ±oz-Esparza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Sharman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deierling</surname></persName>
		</author>
		<idno type="DOI">10.1175/JAMC-D-20-0116.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/apme/59/11/JAMC-D-20-0116.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology and Climatology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1883" to="1899" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Quantifying the benefits of nonlinear methods for global statistical hindcasts of tropical cyclones intensity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Neetu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lengaigne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vialard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mangeas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Menkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leloup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Knaff</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-19-0163.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/35/3/waf-d-19-0163.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="807" to="820" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Classifying proximity soundings with self-organizing maps toward improving supercell and tornado forecasting</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Nowotarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-12-00125.1</idno>
		<ptr target="https://doi.org/10.1175/WAF-D-12-00125.1" />
	</analytic>
	<monogr>
		<title level="j">Wea. Forecasting</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="783" to="801" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coauthors</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Application of a bayesian classifier of anomalous propagation to singlepolarization radar reflectivity data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Steinle</surname></persName>
		</author>
		<idno type="DOI">10.1175/JTECH-D-12-00082.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/atot/30/9/jtech-d-12-00082_1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Atmospheric and Oceanic Technology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1985" to="2005" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<meeting><address><addrLine>San Mateo, California</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Skilful precipitation nowcasting using deep generative models of radar</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coauthors</forename></persName>
		</author>
		<idno type="DOI">10.1038/s41586-021-03854-z</idno>
		<ptr target="https://doi.org/10.1038/s41586-021-03854-z" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">597</biblScope>
			<biblScope unit="issue">7878</biblScope>
			<biblScope unit="page" from="672" to="677" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Visualizing multiple measures of forecast quality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Roebber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="601" to="608" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1038/323533a0</idno>
		<ptr target="https://doi.org/10.1038/323533a0" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">From random forests to flood forecasts: A research to operations success story</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Trojniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Herman</surname></persName>
		</author>
		<idno type="DOI">10.1175/BAMS-D-20-0186.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/bams/102/9/BAMS-D-20-0186.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1742" to="E1755" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Observed relationship between tornado intensity and pretornadic mesocyclone characteristics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Trapp</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-19-0099.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/35/4/wafD190099.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1243" to="1261" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Diagnosing supercell environments: A machine learning approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Houston</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-21-0098.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/aop/WAF-D-21-0098.1/WAF-D-21-0098.1.xml" />
	</analytic>
	<monogr>
		<title level="m">Weather and Forecasting</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Forestbased and semiparametric methods for the postprocessing of rainfall ensemble forecasting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Taillardat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>FougÃ¨res</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Naveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mestre</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-18-0149.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/34/3/waf-d-18-0149_1.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="617" to="634" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2346178" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Pattern recognition using generalized portrait method. Automation and Remote Control</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<ptr target="https://ci.nii.ac.jp/naid/10020952249/en/" />
		<imprint>
			<date type="published" when="1963">1963</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="774" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Sevir : A storm event imagery dataset for deep learning applications in radar and satellite meteorology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Veillette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mattioli</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/" />
		<editor>Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9" to="22" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems fa78a16157fed00d7a80515818432169-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Probabilistic skill of subseasonal surface temperature forecasts over north america</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vigaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tippett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Acharya</surname></persName>
		</author>
		<idno type="DOI">10.1175/WAF-D-19-0117.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/34/6/waf-d-19-0117_1.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1789" to="1806" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Nowcasting multicell short-term intense precipitation using graph models and random forests</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-20-0050.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/mwre/148/11/MWR-D-20-0050.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4453" to="4466" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Lightning from two national detection networks related to vertically integrated liquid and echo-top information from wsr-88d radar</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Holle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>LÃ³pez</surname></persName>
		</author>
		<idno type="DOI">10.1175/1520-0434(1995)010&lt;0592:LFTNDN&gt;2.0.CO;2</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/wefo/10/3/1520-0434_1995_010_0592_lftndn_2_0_co_2.xml" />
	</analytic>
	<monogr>
		<title level="j">Weather and Forecasting</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="592" to="605" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">Wes</forename><surname>Mckinney</surname></persName>
		</author>
		<idno type="DOI">10.25080/Majora-92bf1922-00a</idno>
		<ptr target="https://doi.org/10.25080/Majora-92bf1922-00a" />
		<title level="m">Data Structures for Statistical Computing in Python. Proceedings of the 9th Python in Science Conference, StÃ©fan van der Walt, and Jarrod Millman</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="56" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Using random forests to diagnose aviation turbulence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="51" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Combining observations and model data for short-term storm forecasting. Remote Sensing Applications for Aviation Weather Hazard Detection and Decision Support</title>
		<author>
			<persName><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>AhÄ³evych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dettling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>International Society for Optics and Photonics</publisher>
			<pubPlace>San Diego, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Remote detection and diagnosis of thunderstorm turbulence. Remote Sensing Applications for Aviation Weather Hazard Detection and Decision Support</title>
		<author>
			<persName><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blackburn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>International Society for Optics and Photonics</publisher>
			<pubPlace>San Diego, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Estimating pm2.5 concentrations in contiguous eastern coastal zone of china using modis aod and a two-stage random forest model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1175/JTECH-D-20-0214.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/atot/aop/JTECH-D-20-0214.1/JTECH-D-20-0214.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Atmospheric and Oceanic Technology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A fifth-power relationship for lightning activity from tropical rainfall measuring mission satellite observations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ushio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kawasaki</surname></persName>
		</author>
		<idno type="DOI">10.1029/2008JD010370</idno>
		<ptr target="https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2008JD010370" />
	</analytic>
	<monogr>
		<title level="j">Journal of Geophysical Research: Atmospheres</title>
		<imprint>
			<biblScope unit="page" from="114" to="D119" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Machine learning approaches for improving near-real-time imerg rainfall estimates by integrating cloud properties from noaa cdr patmos-x</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1175/JHM-D-21-0019.1</idno>
		<ptr target="https://journals.ametsoc.org/view/journals/hydr/22/10/JHM-D-21-0019.1.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrometeorology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2767" to="2781" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/3647580" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
