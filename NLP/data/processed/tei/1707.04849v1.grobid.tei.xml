<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Minimax deviation strategies for machine learning and recognition with short learning samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-08-31">August 31, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Minimax deviation strategies for machine learning and recognition with short learning samples</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-08-31">August 31, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">E93A45F7261C7155A1C1E651B1378D08</idno>
					<idno type="arXiv">arXiv:1707.04849v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-25T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The small learning sample problem has been around in machine learning under different names during its whole life. The learning sample is used to compensate for the lack of knowledge about the recognized object when its statistical model is not completely known. Naturally, the longer the learning sample is, the better is the subsequent recognition. However, when the learning sample becomes too small (2, 3, 5 elements) an effect of small samples becomes evident. In spite of the fact that any learning sample (even a very small one) provides some additional information about the object, it may be better to ignore the learning sample than to utilize it with the commonly used methods.</p><p>Example 1. Let us consider an object that can be in one of two random states y = 1 and y = 2 with equal probabilities. In each state the object generates two independent Gaussian random signals x 1 and x 2 with variances equal 1. Mean values of signals depend on the state as it is shown on Fig.</p><p>1. In the first state the mean value is (2, 0). In the second state the mean value depends on an unknown parameter θ and is (0, θ). Even if no learning sample is given a minimax strategy can be used to make a decision about the state y. The minimax strategy ignores the second signal and makes decision y * = 1 when x 1 &gt; 1 and decision y * = 2 when x 1 ≤ 1. Now let us assume that there is a sample of signals generated by an object in the second state but with higher variance 16. A maximum likelihood strategy estimates the unknown parameter θ and then makes a decision about y as if the estimated value of the parameter is its true value. Fig. <ref type="figure">2</ref> shows how the probability of a wrong decision (called the risk) depends on parameter θ for different sizes of the learning sample. If the learning sample is sufficiently long, the risk of maximum likelihood strategy may become arbitrarily close to the minimum possible risk. Naturally, when the length of the sample decreases the risk becomes worse and worse. Furthermore, when it becomes as small as 3 or 2 elements the risk of the maximum likelihood strategy becomes worse than the risk of the minimax strategy that uses neither the learning sample nor the signal x 2 at all. Hence, it is better to ignore available additional data about the recognized object than to try to make use of it in a conventional way. It demonstrates a serious theoretical flaw of commonly used methods, and definitely not that short samples are useless. Any learning sample, no mater how long or short it is, provides some, may be not a lot information about the recognized object and a reasonable method has to use it.</p><p>θ -6 -3 -0 3 6 R(q, θ)</p><formula xml:id="formula_0">min q R(q, θ) R(q min max , θ) R(q M L , θ) θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) n = 1 n = 2 θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) n = 3 n = 10</formula><p>Figure <ref type="figure">2</ref>: Example 1. Probability of a wrong decision (risk) for different sizes n of the learning sample. The curve R(q M L , θ) is the risk of a maximum likelihood strategy. The curve R(q minmax , θ) is the risk of a minimax strategy. The curve min q R(q, θ) is the minimum possible risk for each model.</p><p>Example 2. This is a simple example that has been used by H.Robbins in his seminal article <ref type="bibr" target="#b4">[5]</ref> where he initiated empirical Bayessian approach and explaned its main idea. An object can be in one of two possible states y = 1 and y = 2. In each state the object generates a univariate Gaussian signal x with variance 1. The mean value of the generated signal depends on the state y so that</p><formula xml:id="formula_1">p(x|y = 1) = 1 √ 2π e -(x+1) 2 2 , p(x|y = 2) = 1 √ 2π e -(x-1) 2 2 .</formula><p>Only a priori probabilities of states are unknown and θ is the probability of the first state so that p(y = 1) = θ and p(y = 2) = 1θ. A minimax strategy for such incomplete statistical model makes decision y * based on the sign of the observed signal and ensures probability of correct recognition 0.84 independently of a priori probabilities of states.</p><p>Let not only a single object, but a collection of mutually independant objects be available for recognition. Each object is in its own hidden state and is presented with its own signal. Let us also assume that the decision about</p><formula xml:id="formula_2">x y * = 2 y * = 1 p(x|y = 1) p(x|y = 2) 1 -1 Figure 3: Example 2. x ∈ R -signal, y ∈ {1, 2} -state.</formula><p>each object's state does not have to be made immediately when the object is observed and can be postponed until the whole collection is observed. In this case maximum likelihood estimations of a priori probabilities of states can be computed and then each object of the collection is recognized as if the estimated values of probabilities were the true values. When the presented collection is sufficiently long the probability of a wrong decision can be made as close to the minimum as possible (Fig. <ref type="figure">4</ref>). However, when the collection is too short, the probability of a wrong decision can be much worse than that of the minimax strategy.</p><p>The considered examples lead to a difficult and up to now an unanswered question. What should be done when a fixed sample of 2-3 elements is given and no additional elements can be obtained? Is it really the best way to ignore these data or is it possible to make use of them? We want to fill up this gap between maximum likelihood and minimax strategies and develop a strategy that covers teh whole range of learning samples lengths including zero length. However, this gap, and it is infact a gap, shows a theoretical imperfection of the commonly used learning procedures, namely, of maximum likelihood learning. The short sample problem in whole follows from the fact that maximum likelihood learning as well as many other learning procedures have not been deduced from any explicit risk-oriented requirement to the quality of post-learning recognition. We will formulate such risk-oriented requirements a priori and will see what type of learning procedures follow.</p><p>2 Basic definitions Definition 1. An object is represented with a tuple</p><formula xml:id="formula_3">X, Y, Θ, p XY : X × Y × Θ → R θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 n = 1 n = 2 θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 n = 5 n = 10</formula><p>Figure <ref type="figure">4</ref>: Example 2. Probability of a wrong decision (risk) for different sizes n of the learning sample. The curve R(q M L , θ) shows the risk of a maximum likelihood strategy, R(q minmax , θ) is the risk of a minimax strategy, min q R(q, θ) is the minimal possible risk.</p><p>where X is a finite set of signal values x ∈ X; Y is a finite set of states y ∈ Y ; Θ is a finite set of models θ ∈ Θ; p XY (x, y; θ) is a probability of a pair (x ∈ X, y ∈ Y ) for a model θ ∈ Θ.</p><p>A signal x is an observable parameter of recognized object whereas a state y is its hidden parameter. A pair (x, y) is random and for each pair (x ∈ X, y ∈ Y ) its probability p XY (x, y; θ) exists. However, this probability is not known because it depends on an unknown model θ. As for the model θ it is not random, it takes a fixed but unknown value. Only the set Θ is known that the value θ belongs to.</p><p>Let z be some random data that depend on a model θ and take values from a finite set Z. The data is specified with a tuple Z, p Z : Z × Θ → R where p Z (z; θ) is a probability of data z ∈ Z for model θ ∈ Θ. Definition 2. A random data Z, p Z : Z ×Θ → R that depends on a model is called a learning data for an object X, Y, Θ,</p><formula xml:id="formula_4">p XY : X × Y × Θ → R if p XY Z (x, y, z; θ) = p XY (x, y; θ) • p Z (z; θ) for all x ∈ X, y ∈ Y, z ∈ Z, θ ∈ Θ.</formula><p>A learning sample ((x i , y i )|i = 1, 2, . . . , n) used for supervised learning is a special cases of learning data when</p><formula xml:id="formula_5">Z = (X × Y ) n and p Z (z; θ) = n i=1 p XY (x i , y i ; θ).</formula><p>A learning sample (x i |i = 1, 2, . . . , n) for unsupervised learning is another special case of learning data when</p><formula xml:id="formula_6">Z = X n and p Z (z; θ) = n i=1 y∈Y p XY (x i , y; θ).</formula><p>Any expert knowledge about the true model is also learning data. One can even consider the case when |Z| = 1 and therefore p Z (z; θ) = 1, which is equivalent to the absence of any learning data at all. We do not restrict the learning data in any way except that for any fixed model the learning data z depend neither on the current signal x nor on the current state y so that</p><formula xml:id="formula_7">p XY Z (x, y, z; θ) = p XY (x, y; θ) • p Z (z; θ) for all x ∈ X, y ∈ Y, z ∈ Z, θ ∈ Θ. Definition 3. A non-negative function q : X ×Y ×Z → R is called a strategy if y∈Y q(y |x, z) = 1 for all x ∈ X, z ∈ Z.</formula><p>Value q(y |x, z) of a strategy q : X × Y × Z → R is a probability of a randomized decision that the current state of an object is y, given the current observed signal x and the available learning data z. The set of all strategies</p><formula xml:id="formula_8">q : X × Y × Z → R is denoted Q.</formula><p>Let ω : Y × Y be a loss function whose value ω(y, y ′ ) is the loss of a decision y ′ when the true state is y. Definition 4. Risk R(q, θ) of a strategy q on a model θ is expected loss</p><formula xml:id="formula_9">R(q, θ) = z∈Z x∈X y∈Y p XY (x, y; θ)p Z (z; θ) y ′ ∈Y q(y ′ |x, z)ω(y, y ′ ).</formula><p>Let us be reminded that throughout the paper the sets X, Y , Z and Θ are assumed to be finite. This allows a much more transparent formulation of main results. Allowing some of the sets to be infinite would require finer mathematical tools and the results might be obscured by unnecessary technical details.</p><p>3 Improper and Bayesian strategies.</p><p>One can see that the risk of a strategy depends not only on the strategy itself but also on the model that the strategy is applied to. Therefore, in a general case it is not possible to prefer some strategy q 1 to another strategy q 2 . The risk of q 1 may be better than the risk of q 2 on some models and worse on the others. However, it is possible to prefer strategy q 2 to strategy q 1 if the risk of q 1 is greater than the risk of q 2 on all models. In this case we will say that q 2 dominates q 1 and q 1 is dominated by q 2 . Definition 5. A strategy q 0 is called improper if a strategy q * exists such that R(q 0 , θ) &gt; R(q * , θ) for all θ ∈ Θ.</p><p>We want to exclude all improper from consideration strategies and derive a common form of all the rest. Let T denote the set of all non-negative functions τ : Θ → R such that θ∈Θ τ (θ) = 1. Functions of such type will be refferred to as weight functions. Definition 6. A strategy q * is called Bayesian if there exists a weight function τ ∈ T such that</p><formula xml:id="formula_10">q * = arg min q∈Q θ∈Θ τ (θ)R(q, θ).</formula><p>Theorem 1. Each strategy q 0 ∈ Q is either Bayesian or improper, but never both.</p><p>Proof. For a given strategy q 0 let us define a function</p><formula xml:id="formula_11">F : T × Q → R, F (τ, q) = θ∈Θ τ (θ) R(q, θ) -R(q 0 , θ) .</formula><p>According to Definition 4, for any fixed θ the risk R(q, θ) is a linear function of probabilities q(y |x, z). Consequently, for any fixed τ the function F is also a linear function of probabilities q(y |x, z). Similarly, function F is a linear function of weights τ (θ) for any fixed strategy q. The set Q of strategies and the set T of weight functions are both closed convex sets. Consequently, due to the known duality theorem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref> function</p><formula xml:id="formula_12">F has a saddle point (τ * ∈ T, q * ∈ Q) such that max τ ∈T min q∈Q F (τ, q) = F (τ * , q * ) = min q∈Q max τ ∈T F (τ, q),</formula><p>where</p><formula xml:id="formula_13">q * = argmin q∈Q max τ ∈T F (τ, q), τ * = argmax τ ∈T min q∈Q F (τ, q).</formula><p>It is obvious that F (τ, q 0 ) = 0 for any τ ∈ T . Therefore, the inequality min q∈Q F (τ, q) ≤ 0 holds for every τ ∈ T and, consequently,</p><formula xml:id="formula_14">max τ ∈T min q∈Q F (τ, q) = F (τ * , q * ) ≤ 0.</formula><p>Therefore, there are two mutually exclusive cases: either F (τ * , q * ) &lt; 0 or F (τ * , q * ) = 0. In such way the proof of the theorem is reduced to proving the following four propositions: Proposition 1. If the strategy q 0 is Bayessian then F (τ * , q * ) = 0. Proposition 2. If F (τ * , q * ) = 0 then the strategy q 0 is Bayessian. Proposition 3. If the strategy q 0 is improper then F (τ * , q * ) &lt; 0. Proposition 4. If F (τ * , q * ) &lt; 0 then the strategy q 0 is improper.</p><p>Proof of Proposition 1. If the strategy q 0 is Bayessian then according to Definition 6 a weight function τ 0 exists such that inequality</p><formula xml:id="formula_15">θ∈Θ τ 0 (θ)R(q, θ) ≥ θ∈Θ τ 0 (θ)R(q 0 , θ)</formula><p>is valid for all q ∈ Q. Consequently, for all q ∈ Q the chain</p><formula xml:id="formula_16">0 ≤ θ∈Θ τ 0 (θ)[R(q, θ) -R(q 0 , θ)] = F (τ 0 , q) ≤ max τ ∈T F (τ, q)</formula><p>is also valid. Since all numbers max τ ∈T F (τ, q), q ∈ Q, are not negative the least of them is also not negative and</p><formula xml:id="formula_17">min q∈Q max τ ∈T F (τ, q) = F (τ * , q * ) ≥ 0</formula><p>From this inequality it follows that F (τ * , q * ) = 0 because a case F (τ * , q * ) &gt; 0 is impossible.</p><p>Proof of Proposition 2. Let F (τ * , q * ) = 0 then</p><formula xml:id="formula_18">0 = F (τ * , q * ) = max τ ∈T min q∈Q F (τ, q) = min q∈Q F (τ * , q) = = min q∈Q θ∈Θ τ * (θ) R(q, θ) -R(q 0 , θ) = = min q∈Q θ∈Θ τ * (θ)R(q, θ) - θ∈Θ τ * (θ)R(q 0 , θ).</formula><p>It implies the equality</p><formula xml:id="formula_19">min q∈Q θ∈Θ τ * (θ)R(q, θ) = θ∈Θ τ * (θ)R(q 0 , θ)</formula><p>and therefore, q 0 = arg min q∈Q θ∈Θ τ * (θ)R(q, θ), which means that q 0 is Bayesian according to Definition 6.</p><p>Proof of Proposition 3. If the strategy q 0 is improper then according to Definition 5 a strategy q 1 exists such that inequality R(q 1 , θ) &lt; R(q 0 , θ) holds for all θ. The set of models is finite and therefore, a value ε &lt; 0 exists such that for any θ inequality R(q 1 , θ) -R(q 0 , θ) ≤ ε holds and a chain</p><formula xml:id="formula_20">0 &gt; ε ≥ θ∈Θ τ (θ)[R(q 1 , θ) -R(q 0 , θ)] = F (τ, q 1 ) ≥ min q∈Q F (τ, q)</formula><p>is valid for any τ ∈ T . Since all numbers min q∈Q F (τ, q), τ ∈ T , are not greater then ε the greatest of them is also not greater then ε and max</p><formula xml:id="formula_21">τ ∈T min q∈Q F (τ, q) = F (τ * , q * ) ≤ ε &lt; 0.</formula><p>Proof of Proposition 4. Let F (τ * , q * ) &lt; 0 then</p><formula xml:id="formula_22">F (τ * , q * ) = min q∈Q max τ ∈T F (τ, q) = max τ ∈T F (τ, q * ) = = max τ ∈T θ∈Θ τ (θ) R(q * , θ) -R(q 0 , θ) = max θ∈Θ R(q * , θ) -R(q 0 , θ)</formula><p>and therefore max θ∈Θ R(q * , θ) -R(q 0 , θ) &lt; 0.</p><p>Consequently, the inequality R(q * , θ) &lt; R(q 0 , θ) holds for all models θ ∈ Θ and q 0 is improper according to Definition 5.</p><p>The theorem gives good reasons to reappraise lot of well-known methods that are commonly used as something self-evident. Let us illustrate this criticism with two simple examples. The first example considers a certain method of recognition without learning and the second relates to maximum likelihood learning. In both examples the loss function is</p><formula xml:id="formula_23">ω(y, y ′ ) = 0, if y = y ′ , 1, if y = y ′ .</formula><p>Example 3. Let x be an image of a letter, y be its name and θ be a position of the letter in a field of vision. Let the function p XY : X × Y × Θ → R be constructively defined so that probability p XY (x, y; θ) can be calculated for each triple x, y, θ. In this case when an image x with an unknown position θ is observed the decision y * (x) about the name of the letter has to be of the form</p><formula xml:id="formula_24">y * (x) = argmax y∈Y θ∈Θ τ (θ)p XY (x, y; θ).<label>(1)</label></formula><p>Theorem 1 reveals a certain weakness of the commonly used form</p><formula xml:id="formula_25">argmax y∈Y max θ∈Θ p XY (x, y; θ).<label>(2)</label></formula><p>The strategy (2) could be represented in the form (1) if the weights τ (θ) in (1) could be chosen individually for each observation x ∈ X. However, each Bayessian strategy is specified with its own weight function τ : Θ → R so that weights are assigned to elements of the set Θ, not of the set Θ × X.</p><p>As a rule, the strategy (2) cannot be represented in the form (1) with fixed weights τ (θ) that do not depend on x. It means that the strategy (2) is not Bayessian and is dominated by some other strategy that for each position of the letter recognizes its name better then strategy (2).</p><p>Example 4. Let the sets X, Y and Θ be specified for the recognized object as well as a function p XY : X × Y × Θ → R. Let the learning information be a random learning sample z = ((</p><formula xml:id="formula_26">x i , y i )|i = 1, 2, . . . , n) such that p Z (z; θ) = n i=1 p XY (x i , y i ; θ).</formula><p>Then the decision y * about the current state y 0 based on the current signal x 0 and available learning sample z has to be of the form</p><formula xml:id="formula_27">y * = arg max y 0 ∈Y θ∈Θ τ (θ) n i=0 p(x i , y i ; θ)<label>(3)</label></formula><p>for some fixed τ that does not depend on z. One can see that the commonly used maximum likelihood strategy</p><formula xml:id="formula_28">y * = arg max y 0 p(x 0 , y 0 ; θ M L (z)),<label>(4)</label></formula><formula xml:id="formula_29">θ M L (z) = arg max θ∈Θ n i=1 p(x i , y i ; θ)</formula><p>can almost never be represented in the form (3) with constant weights and therefore is not Bayessian. It means that some other strategy exists that makes a decision about the current state based both on current signal and learning information and for each model makes it better than strategy (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A gap between maximum likelihood and minimax strategies.</head><p>We consider maximum likelihood and minimax strategies and specify a gap between them.</p><p>Let us define for each θ ∈ Θ a strategy q opt (θ) = argmin q∈Q R(q, θ) that assigns a probability q opt (y |x, z; θ) for each triple (x, y, z). The strategy q opt (θ) is the best possible strategy that should be used if a true model were known. Since the model is known no learning data are needed. For any fixed model θ a strategy q(θ) : X × Y × Z → R can be replaced with a strategy q X (θ) : X × Y → R with the same risk. Probabilities q(y |x, z; θ) have to be transformed into probabilities q X (y |x; θ) according to expression q X (y |x; θ) = z∈Z p Z (z; θ)q(y |x, z; θ) and so the chain</p><formula xml:id="formula_30">R(q, θ) = z∈Z x∈X y∈Y p XY (x, y; θ)p Z (z; θ) y ′ ∈Y q(y ′ |x, z; θ)ω(y, y ′ ) = = x∈X y∈Y p XY (x, y; θ) y ′ ∈Y ω(y, y ′ ) z∈Z p Z (z; θ)q(y ′ |x, z; θ) = = x∈X y∈Y p XY (x, y; θ) y ′ ∈Y q X (y ′ |x; θ)ω(y, y ′ ) = R(q X , θ).</formula><p>is valid for each model θ. Consequently, for each θ the equality min q∈Q R(q, θ) = min</p><formula xml:id="formula_31">q X ∈Q X R(q X , θ)<label>(5)</label></formula><p>is valid. The symbol Q X in (5) designates a set of all strategies of the form q X : X × Y → R that do not use the learning data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 7.</head><p>A strategy q M L : X × Y × Z → R is called a maximum likelihood strategy if for each triple (x, y, z) it specifies a probability</p><formula xml:id="formula_32">q M L (y |x, z) = q opt X (x|y; θ M L (z)),</formula><p>where q opt X (θ) = argmin</p><formula xml:id="formula_33">q X ∈Q X R(q X , θ) and θ M L (z) = argmax θ∈Θ p Z (z; θ).</formula><p>In other words, maximum likelihood strategies use the learning data z to estimate a model θ and make a decision that minimizes the expected loss with an assumption that the estimated model is the true model.</p><p>As it has been quoted for Examples 3 and 4, as a rule, maximum likelihood strategies cannot be represented in a form of a Bayessian strategy</p><formula xml:id="formula_34">q B = argmin q∈Q θ∈Θ τ (θ)R(q, θ)</formula><p>with fixed weights τ (θ) that do not depend on the learning data. In such cases the maximum likelihood strategy q M L may be dominated with another strategy of the form X × Y × Z → R. Minimax strategies are free of this flaw.</p><p>Definition 8. Strategy argmin q∈Q max θ∈Θ R(q, θ) is called a minimax strategy.</p><p>Theorem 2. No minimax strategy is improper.</p><p>Proof. Let us prove an equivalent statement that any improper strategy q 0 is not minimax. Indeed, as far as q 0 is improper another strategy q 1 exists such that R(q 1 , θ) &lt; R(q 0 , θ) for all θ. Therefore, max θ R(q 1 , θ) &lt; max θ R(q 0 , θ) and min q max θ R(q, θ) &lt; max θ R(q 0 , θ) and q 0 is not argmin q max θ R(q 0 , θ).</p><p>Though maximum likelihood strategy may be improper whereas minimax strategy is never improper the first one has an essential advantage over the second. There is a rather wide class of learning data such that the maximum likelihood strategy is in a sense consistent for any recognized object whereas there is a rather wide class of recognized objects such that the minimax strategy is not consistent for any learning data. Let us exactly formulate these statements and prove them.</p><p>Let z ∈ Z be a random variable that depends on model θ and let for each z ∈ Z and θ ∈ Θ a probabillity p Z (z; θ) be given. We will say that this dependence is essential if for each two different models</p><formula xml:id="formula_35">θ 1 = θ 2 a value z * exists such that p Z (z * ; θ 1 ) = p Z (z * ; θ 2 ). Let z n = (z i |i = 1, 2, . . . , n) ∈ Z n be a learning sample, p Z n (z n ; θ * ) = n i=1 p Z (z i ; θ *</formula><p>) be a probability of a sample and θ M L (z n ) = argmax θ p Z n (z n ; θ) be a maximum likelihood estimation of the model.</p><p>Consistency is a generally known property of maximum likelihood estimate. In the considered case this property may be formulated in a simple way that the probability of inequality θ M L (z n ) = θ * converges to zero when n increases or, formally,</p><formula xml:id="formula_36">lim n→∞ z n ∈Z n err n i=1 p Z (z i ; θ * ) = 0 (<label>6</label></formula><formula xml:id="formula_37">)</formula><p>where</p><formula xml:id="formula_38">Z n err = {z n ∈ Z n |θ M L (z n ) = θ * }. (<label>7</label></formula><formula xml:id="formula_39">)</formula><p>The consistency of a maximum likelihood estimations is a base for a proof of the following theorem about consistency of maximum likelihood strategy.</p><p>Theorem 3. Let z be random variable that takes values from a set Z according to probability distribution p Z (z; θ) that essentially depends on θ;</p><p>let n be a positive integer and</p><formula xml:id="formula_40">z n = (z i |i = 1, 2, . . . , n) ∈ Z n be a random learning sample with probability distribution p Z n (z n ; θ) = n i=1 p Z (z i ; θ); let q M L n : X × Y × Z n → R be a maximum likelihood strategy for an object X, Y, Θ, p XY : X × Y × Θ → R and learning data Z n , p Z n : Z n × Θ → R . Then lim n→∞ max θ∈Θ R(q M L n , θ) -min q∈Q R(q, θ) = 0.</formula><p>Proof. As far as a set Θ is finite the proof of the theorem is reduced to proof of the equality lim</p><formula xml:id="formula_41">n→∞ R(q M L n , θ) -min q∈Q R(q, θ) = 0 (8)</formula><p>for any θ. The subsequent proof is based on equality (5), on equalities ( <ref type="formula" target="#formula_36">6</ref>) and ( <ref type="formula" target="#formula_38">7</ref>) that express consistency of maximum likelihood estimates and on equality</p><formula xml:id="formula_42">R(q M L n , θ) = z n ∈Z n p Z n (z n ; θ) min q X ∈Q X R(q X , θ M L (z n )),</formula><p>where</p><formula xml:id="formula_43">θ M L (z n ) = argmax θ∈Θ p Z n (z n ; θ),</formula><p>that follows from Definition 7. The following chain is valid:</p><formula xml:id="formula_44">lim n→∞ [R(q M L n , θ) -min q∈Q R(q, θ)] = lim n→∞ [R(q M L n , θ) -min q X ∈Q X R(q X , θ)] = = lim n→∞ [ zn∈Z n p Z n (z n ; θ) min q X ∈Q X R(q X , θ M L (z n )) -min q X ∈Q X R(q X , θ)] = lim n→∞ zn∈Z n p Z n (z n ; θ)[ min q X ∈Q X R(q X , θ M L (z n )) -min q X ∈Q X R(q X , θ)] = lim n→∞ z n ∈Z n err p Z n (z n ; θ)[ min q X ∈Q X R(q X , θ M L (z n )) -min q X ∈Q X R X (q X , θ)] ≤ lim n→∞ z n ∈Z n err p Z n (z n ; θ)[max y∈Y max y ′ ∈Y w(y, y ′ ) -min y∈Y min y ′ ∈Y w(y, y ′ )] = lim n→∞ {[max y∈Y max y ′ ∈Y w(y, y ′ ) -min y∈Y min y ′ ∈Y w(y, y ′ )] z n ∈Z n err p Z n (z n ; θ)} = [max y∈Y max y ′ ∈Y w(y, y ′ ) -min y∈Y min y ′ ∈Y w(y, y ′ )] lim n→∞ z n ∈Z n err p Z n (z n ; θ) = 0.</formula><p>It follows from a chain that for any θ an inequality lim n→∞ R(q M L n , θ) -min q∈Q R(q, θ) ≤ 0 holds. The difference R(q M L n , θ) -min q∈Q R(q, θ) is never negative and so (8) is proved.</p><p>So, with the increasing length of learning sample the risk function of maximum likelihood strategy becomes arbitrarily close to the minimum possible risk function. Minimax strategy has not this property. Moreover, for certain class of objects minimax strategies simply ignore the learning sample, no matter how long it is.</p><formula xml:id="formula_45">Theorem 4. Let for an object X, Y, Θ, p XY : X × Y × Θ → R a pair (θ * , q * X ) exists such that q * X = argmin q X ∈Q X R(q X , θ * ), θ * = argmax θ∈Θ R(q * X , θ).</formula><p>Then the inequality</p><formula xml:id="formula_46">max θ∈Θ R(q, θ) ≥ max θ∈Θ R(q * X , θ)<label>(9)</label></formula><p>is valid for any learning data Z, p Z : Z × Θ → R and any strategy q : X × Y × Z → R.</p><p>Proof. For any strategy q ∈ Q we have the chain</p><formula xml:id="formula_47">max θ∈Θ R(q, θ) ≥ R(q, θ * ) ≥ min q∈Q R(q, θ * ) = = min q X ∈Q X R(q X , θ * ) = R(q * X , θ * ) = max θ∈Θ R(q * X , θ).</formula><p>The theorem shows that for some objects the minimax approach is particularly inappropriate because it enforces to ignore any learning data. There is nothing unusual in conditions of the Theorem 4. Examples 1 and 2 in Introduction show just the cases when these conditions are satisfied. So, there is a following gap between maximum likelihood and minimax strategies. Maximum likelihood strategy may be dominated with other strategy. In this case it can be improved and, consequently, it is not optimal from any point of view. However, for wide class of learning data maximum likelihood strategies are consistent and so their chortage does not become apparent when learning sample of an arbitrary size may be obtained. Cases of learning samples of fixed sizes, especially, short samples form an area of improper application of maximum likelihood strategies. This area is not covered with minimax strategies. Though minimax strategies are dominated with no strategy, for rather wide class of objects minimax requirement enforces to ignore any learning sample, no matter how long it is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Minimax deviation strategies.</head><p>This section is aimed at developing a Bayesian consistent strategy that has to fill a gap between maximum likelihood and minimax strategies.</p><formula xml:id="formula_48">Definition 9. A strategy argmin q∈Q max θ∈Θ R(q, θ) -min q ′ ∈Q R(q ′ , θ) is called mini- max deviation strategy.</formula><p>Minimax deviation strategies do not have the drawback of the minimax strategies. A theorem that is similar to Theorem 3 for maximum likelihood strategies is also valid for minimax deviation strategies.</p><p>Theorem 5. Let z be random variable that takes values from a set Z according to probability distribution p Z (z; θ) that essentially depends on θ;</p><p>let n be a positive integer and z n = (z i |i = 1, 2, . . . , n) ∈ Z n is a random learning sample with probability distribution p Z n (z n ; θ) = n i=1 p Z (z i ; θ);</p><formula xml:id="formula_49">let q * n : X × Y × Z n → R be a minimax deviation strategy for an object X, Y, Θ, p XY : X × Y × Θ → R and learning data Z n , p Z n : Z n × Θ → R . Then lim n→∞ max θ∈Θ R(q * n , θ) -min q∈Q R(q, θ) = 0. (<label>10</label></formula><formula xml:id="formula_50">)</formula><p>Proof. The Theorem is a straighforward consequence of Definition 9 and the Theorem 3. Let q M L n be a maximum likelihood strategy for an object X, Y, Θ,</p><formula xml:id="formula_51">p XY : X × Y × Θ → R and learning data Z n , p Z n : Z n × Θ → R . It follows from Definition 9 that max θ∈Θ R(q * n , θ) -min q∈Q R(q, θ) ≤ max θ∈Θ R(q M L n , θ) -min q∈Q R(q, θ)</formula><p>for any n. It follows from Theorem 3 that</p><formula xml:id="formula_52">lim n→∞ max θ∈Θ R(q * n , θ) -min q∈Q R(q, θ) ≤ ≤ lim n→∞ max θ∈Θ R(q M L n , θ) -min q∈Q R(q, θ) = 0.</formula><p>As far as the difference [R(q * n , θ) -min q∈Q R(q, θ) is negative for no model the equality (10) is proved.</p><p>Let us note that the proof of the Theorem 10 shows not only a consistency of minimax deviation strategy. It shows also that minimax deviation strategy converges to desired result not slower than maximum likelihood strategy. Similarly, one can show that this advantage of minimax deviation strategy holds as compared with any consistent strategy and from this point of view it is the best of all consistent strategies. Following theorem states that minimax deviation strategies are also inappropriate for recognition of certain type of objects. Theorem 6. Let for an object X, Y, Θ, p : X × Y × Θ → R a model θ * and a strategy q * X exist such that q * X = argmin</p><formula xml:id="formula_53">q X ∈Q X [R X (q X , θ * ) -min q ′ X ∈Q X R X (q ′ X , θ * )],<label>(11)</label></formula><formula xml:id="formula_54">θ * = argmax θ∈Θ [R X (q * X , θ) -min q ′ X ∈Q X R X (q ′ X , θ)].<label>(12)</label></formula><p>Then the inequality</p><formula xml:id="formula_55">max θ∈Θ [R(q, θ) -min q X ∈Q X R(q X , θ)] ≥ max θ∈Θ [R(q * X , θ) -min q X ∈Q X R(q X , θ)]</formula><p>holds for any learning data Z, p Z : Z × Θ → R and any strategy q ∈ Q.</p><p>Proof. In fact, proof of the theorem does not differ from the proof of the Theorem 4.</p><p>However, the consequences of this theorem for minimax deviation strategies are not so destructive as those of Theorem 4 for minimax strategies. In fact, conditions (11) and (12) imply that a strategy q * X ∈ Q X exists that does not use learning information and assures minimal possible risk for each model, R(q * X , θ) = min q X ∈Q X R(q X , θ) for all θ ∈ Θ.</p><p>In this case, any learning data are needless and has to be omitted by any strategy.</p><p>Evidently, minimax deviation strategy is not improper and, consequently, is Bayessian. The following theorem shows how the corresponding weight function has to be obtained.</p><p>Theorem 7. Minimax deviation strategy</p><formula xml:id="formula_56">q * = argmin q∈Q max θ∈Θ R(q, θ) -min q X ∈Q X R(q X , θ)</formula><p>is a Bayesian strategy argmin q∈Q θ∈Θ τ * (θ)R(q, θ) with respect to weight function</p><formula xml:id="formula_57">τ * = arg max τ ∈T min q∈Q θ∈Θ τ (θ)R(q, θ) - θ∈Θ τ (θ) min q X ∈Q X R(q X , θ) .<label>(13)</label></formula><p>Proof. Let us define a function</p><formula xml:id="formula_58">F : T × Q → R, F (τ, q) = θ∈Θ τ (θ)R(q, θ) - θ∈Θ τ (θ) min q X ∈Q X R(q X , θ)</formula><p>and express q * and τ * in terms of F ,</p><formula xml:id="formula_59">q * = argmin q∈Q max θ∈Θ R(q, θ) -min q X ∈Q X R(q X , θ) = argmin q∈Q max τ ∈T θ∈Θ τ (θ) R(q, θ) -min q X ∈Q X R(q X , θ) = argmin q∈Q max τ ∈T F (τ, q), τ * = arg max τ ∈T min q∈Q F (τ, q).</formula><p>The function F is a linear function of q for fixed τ and a linear function of τ for fixed q and is defined on a Cartesian product of two closed convex sets T and Q. In such case a pair (τ * , q * ) is a saddle point <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>,</p><formula xml:id="formula_60">min q∈Q max τ ∈T F (τ, q) = F (τ * , q * ) = max τ ∈T min q∈Q F (τ, q),</formula><p>that implies F (τ * , q * ) = min q∈Q F (τ * , q) and</p><formula xml:id="formula_61">q * = arg min q∈Q F (τ * , q) = = arg min q∈Q θ∈Θ τ * (θ)R(q, θ) - θ∈Θ τ * (θ) min q X ∈Q X R(q X , θ) = = arg min q∈Q θ∈Θ τ * (θ)R(q, θ).</formula><p>In such way developing minimax deviation strategy is reduced to calculating weights τ (θ) of models that maximize concave function (13). In described below experiments general purpose methods of non-smooth optimization <ref type="bibr" target="#b5">[6]</ref> were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Minimax deviation strategies have been built for objects considered in Introduction in Examples 1 and 2. Minimax deviation strategies have been compared with maximum likelihood and minimax strategies. Results are presented on Figures <ref type="figure">5</ref> and <ref type="figure">6</ref> that show risk R(q, θ) of the strategies as a function of a model for several learning sample sizes. Figure <ref type="figure">5</ref> relates to Example 1 and Figure <ref type="figure">6</ref> to Example 2.</p><formula xml:id="formula_62">θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) n = 1 n = 2 θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) θ -6 -3 -0 3 6 R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) n = 3 n = 10</formula><p>Figure <ref type="figure">5</ref>: Example 1. Probability of making a wrong decision for different sizes n of the learning sample. The dashed line shows the risk of a minimax deviation strategy. The curve R(q M L , θ) is the risk of a maximum likelihood strategy. The curve R(q minmax , θ) is the risk of a minimax strategy. The curve min q R(q, θ) is the minimum possible risk for each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The paper analyzes the problem when for given object that differs from (14). Moreover, any decision that differs from (15) can be replaced with a decision of the form (15) with the better recognition quality.</p><p>There is nothing in decision (15) that could be treated as a selecting some best model of the model set and so no question stands what estimator θ est : Z → Θ has to be used. No model has to be selected, on the contrary, all models have to take part in decision with their weights. It is essential that the weights do not depend on learning data, they are determined by requirement to searched strategy for concrete applied situation. The paper shows a way for computing these weights for minimax deviation strategy that is appropriate for learning samples of any length and in such way fills a gap between maximum likelihood and minimax startegies.</p><p>Minimax deviation strategy is not at all a single strategy that is reasonable in such or other application. Many other strategies are appropriate too, for example, a strategies of the form argmin q∈Q max θ∈Θ R(q, θ)α(θ) β(θ)</p><p>with predefined numbers α(θ) and β(θ) &gt; 0. Minimax strategy is a special case of (16) when α(θ) = 0, β(θ) = 1, minimax deviation strategy is a case when α(θ) = min q∈Q R(q, θ), β(θ) = 1. A reasonable modification of minimax deviation strategy is a case when α(θ) = 0, β(θ) = min q∈Q R(q, θ). The numbers α(θ) may be risks of some already developed strategy and this is a case when the developer wants to check whether the better strategy is possible. At last, numbers α(θ) may be simply desired values of risks in concrete applied situation.</p><p>Requirements of the form (16) together with various loss functions determine various applied situations and obtained results show the way to cope with all them. It has become quite clear now that each strategy of the form (16) may be represented in the form (15) because, obviously, no of them is improper. Obtained results imply unexpected conclusion that learning data take part in a decision (15) in a unified form that depends neither on applied situation nor on recognized object. So, no question stands more how the learning data have to influence the decision about current state when the current signal is observed. Learning data influence the decision via and only via probabilities p Z ; (z; θ), not via choise of some best model of the model set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example 1. (x 1 , x 2 ) ∈ R 2 -signal, y ∈ {1, 2} -state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>X</head><figDesc>, Y, Θ, p XY : X × Y × Θ → R , loss function w : Y × Y → R, learning data source Z, p Z : Z × Θ → R , observed current signal x and available learning data z a decision y * about to the quality of post-learning recognition implies the decision of the form y * = argmin y ′ ∈Y θ∈Θ τ (θ)p Z (z; θ) y∈Y p XY (x, y; θ)w(y, y ′ ) (15)</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) 0 0.5 1 θ R(q, θ) min q R(q, θ) R(q min max , θ) R(q M L , θ) The curve R(q M L , θ) is the risk of a maximum likelihood strategy. The curve R(q minmax , θ) is the risk of a minimax strategy. The curve min q R(q, θ) is the minimum possible risk for each model.</p><p>current hidden state y has to be made. The wide class of commonly used strategies make the decision of a form</p><p>where θ est : Z → Θ is a reasonable estimating a model θ based on learning data z. It means that the learning data are used to choose a single best model and the objects are recognized as if this best model equals the true model. The approach is acceptable if learning data are arbitrarily long learning samples and estimator θ est : Z → Θ is consistent. If the learning information has a fixed format, for example, is a learning sample of limited size then the approach gives no guarantee for subsequent recognition. Indeed, the approach is not deduced from any risk-oriented requirement. Reasonable requirement</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Borwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<title level="m">Convex Analysis and Nonlinear Optimization</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambrige University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fundamentals of Convex Analysis</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Hiriart-Urruty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Asymptotically Subminimax Solutions of Compound Statistical Decision Problems</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability</title>
		<editor>
			<persName><forename type="first">Jerzy</forename><surname>Neyman</surname></persName>
		</editor>
		<meeting>the Second Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="131" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Nondifferentiable Optimization and Polynomial Problems. Nonconvex Optimization and Its Applications</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Shor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Webb</surname></persName>
		</author>
		<title level="m">Statistical Pattern Recognition</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
