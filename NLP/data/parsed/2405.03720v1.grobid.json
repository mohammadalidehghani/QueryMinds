{
  "title": "",
  "abstract": "",
  "introduction": "Spatial data is ubiquitous, encompassing a wide range of applications from environmental observations and biological measurements to more recent fields like computer vision. A critical challenge in the analysis of spatial data is spatial prediction, which involves estimating unobserved values based on nearby observations under the assumption of certain correlations. Among parametric algorithms, Kriging is particularly notable ((Matheron (1963) )). Described as the best linear unbiased estimator (BLUE), Kriging employs a weighted average of nearby observations, with weights determined by a covariance function typically presumed to be stationary. However, this assumption does not hold in many real-world scenarios, such as data from satellites, monitoring stations, and urban streets, which tend to exhibit nonstationarity (Katzfuss (2013) ). Moreover, Kriging faces computational challenges with large datasets, requiring the inversion of the covariance matrix, an operation with a computational complexity of O(N 3 ) (Chen et al. (2020) ). A variety of algorithms have been proposed to address the issues highlighted above. Mao et al. (2022) introduced a model-free method for handling non-stationary spatial data, presenting a significant advancement in the field. Another popular alternative to Kriging is the Vecchia approximation, which simplifies the full Gaussian distribution by conditioning on neighboring values (Vecchia (1988) ). Drawing on the concept of neighbor-based approximation, Wang et al. (2019) developed a nearest neighbor neural network specifically for spatial prediction. Building on these ideas, Chen et al. (2020) introduced the innovative Deep Kriging framework, which combines spatial basis functions with deep neural networks to model any spatial processes effectively. While deep learning approaches have demonstrated significant promise in approximating spatial surfaces, they typically require a substantial amount of training data. Transfer learning emerges as an effective solution to this challenge. For instance, He et al. (2019) employed transfer learning to leverage a neural network pre-trained on the ImageNet dataset for Hyperspectral Image Classification (HSI). Similarly, Zhang and Liu (2012) utilized transfer learning to adapt to a common latent representation for writer adaptation. Both the HSI and writer adaptation scenarios, which are limited by small target sample sizes, benefit from the integration of external information from larger datasets. Spatial applications often grapple with the challenge of small sample sizes. For instance, in estimating PM 2.5 concentrations, only about 70 high-quality monitoring stations are available throughout North Carolina, and approximately 200 in California (Yang et al. (2023) ). Despite this, a large volume of relatively low-quality data exists in these states. Since the spatial distribution of PM 2.5 tends to be stable, integrating data from these abundant but lower-quality stations could significantly enhance estimation accuracy. Drawing inspiration from the works of He et al. (2019) and Chen et al. (2020) , this paper proposes a neural network-based transfer learning method that utilizes external information to improve spatial predictions in datasets with limited target data.",
  "body": "Spatial data is ubiquitous, encompassing a wide range of applications from environmental observations and biological measurements to more recent fields like computer vision. A critical challenge in the analysis of spatial data is spatial prediction, which involves estimating unobserved values based on nearby observations under the assumption of certain correlations. Among parametric algorithms, Kriging is particularly notable ((Matheron (1963) )). Described as the best linear unbiased estimator (BLUE), Kriging employs a weighted average of nearby observations, with weights determined by a covariance function typically presumed to be stationary. However, this assumption does not hold in many real-world scenarios, such as data from satellites, monitoring stations, and urban streets, which tend to exhibit nonstationarity (Katzfuss (2013) ). Moreover, Kriging faces computational challenges with large datasets, requiring the inversion of the covariance matrix, an operation with a computational complexity of O(N 3 ) (Chen et al. (2020) ). A variety of algorithms have been proposed to address the issues highlighted above. Mao et al. (2022) introduced a model-free method for handling non-stationary spatial data, presenting a significant advancement in the field. Another popular alternative to Kriging is the Vecchia approximation, which simplifies the full Gaussian distribution by conditioning on neighboring values (Vecchia (1988) ). Drawing on the concept of neighbor-based approximation, Wang et al. (2019) developed a nearest neighbor neural network specifically for spatial prediction. Building on these ideas, Chen et al. (2020) introduced the innovative Deep Kriging framework, which combines spatial basis functions with deep neural networks to model any spatial processes effectively. While deep learning approaches have demonstrated significant promise in approximating spatial surfaces, they typically require a substantial amount of training data. Transfer learning emerges as an effective solution to this challenge. For instance, He et al. (2019) employed transfer learning to leverage a neural network pre-trained on the ImageNet dataset for Hyperspectral Image Classification (HSI). Similarly, Zhang and Liu (2012) utilized transfer learning to adapt to a common latent representation for writer adaptation. Both the HSI and writer adaptation scenarios, which are limited by small target sample sizes, benefit from the integration of external information from larger datasets. Spatial applications often grapple with the challenge of small sample sizes. For instance, in estimating PM 2.5 concentrations, only about 70 high-quality monitoring stations are available throughout North Carolina, and approximately 200 in California (Yang et al. (2023) ). Despite this, a large volume of relatively low-quality data exists in these states. Since the spatial distribution of PM 2.5 tends to be stable, integrating data from these abundant but lower-quality stations could significantly enhance estimation accuracy. Drawing inspiration from the works of He et al. (2019) and Chen et al. (2020) , this paper proposes a neural network-based transfer learning method that utilizes external information to improve spatial predictions in datasets with limited target data. Method and Theoretical Properties Let Y i be the observation at spatial location s i = (s i1 , s i2 ) for i ∈ {1, ..., n}. This paper assumes Y i = f (s i ; θ) + ε i , (1) where f is a spatial process that depends on parameters θ and ε i iid ∼ Normal(0, τ 2 ) is error. The spatial process is modelled using a feed-forward neural network (FFNN) with input s i . The FFNN has two stages: in the first stage we deform the spatial coordinates using Radial Basis Function (RBF), and in the second stage the weights are applied to capture the underlying spatial strcture. Below, we describe the model with a single hidden layer in the first stage, and with seven hidden layers of 100 neurons in the second stage. Following Chen et al. (2020) , we first use an embedding layer expanding the spatial location into p known basis functions K 1 (s), ..., K p (s). In particular, we use the Wendland basis function ϕ(d) =        (1 -d) 6 (35d 2 + 18d + 3)/3, d ∈ [0, 1] 0, otherwise. where d is the Euclidean distance between observations and knots. Adopting the idea in Nychka et al. (2015) , we use a multi-resolution with the knots arranged on a rectangular grid. In particular, we used four level of resolutions, and at each level, let u i , i = 1, 2, 3, 4 be a rectangular grid of points. The basis function is defined as ϕ * (s) = ϕ(d) = ϕ(||s -u i ||). After the first stage, we have a basis function representation of x ∈ R 139 . The second stage of the neural network is defined as follows: h 1 = ReLU(W 1 x + b 1 ) . . . h 7 = ReLU(W 7 h 6 + b 7 ) y = W 8 h 3 + b 8 where: • W 1 ∈ R 100×139 , b 1 ∈ R 100 are the weights and biases of the first hidden layer. • W 2 . . . W 7 ∈ R 100×100 , b 2 . . . b 7 ∈ R 100 are the weights and biases of the second and third hidden layers. • W 8 ∈ R 1×100 , b 8 ∈ R are the weights and bias of the output layer. • ReLU(•) is the Rectified Linear Unit activation function. • y ∈ R is the output of the network. A visual illustration of the architecture of the two-stage neural network are provided in the supplementary materials. The neural network above is first trained on a large external data, and all parameters are transferred to the target data set, since we assume the distribution of the covariates X are the same. Many recent advancement of transfer learning in NLP and Computer Vision area shows that the method proposed could improve the estimation performance. For example, Segment Anything Model (Kirillov et al. (2023) ) learns from a millions of images and is the stateof-the-art in segmentation task. The basic idea behind both Segment Anything Model and the proposed approach is that the neural network is learning latent representation of the training feature, and can decode the learned feature and adapt to new tasks quickly. Simulation To evaluate the performance of the proposed method above, a simulation study is carried out using both stationary and non-stationary data on a unit square [0, 1] 2 . The stationary data is a Matern spatial process as defined by Lindgren et al. (2011) , where the correlation between two spatial location s i , s j is C(s i , s j ) = σ 2 2 ν-1 Γ(ν) (κ||s i -s j ||) ν K ν (κ||s i -s j ||) where K ν is the modified Bessel function of the second kind with smoothness factor ν = 1, and κ = √ 8 ρ where ρ = 0.2 is the spatial range, and σ 2 = 1 is the spatial range. The nugget error ϵ i = 0.01 is the i.i.d noise. The non-stationary process is defined in Chen et al. (2020) with Y i = sin{30( si -0.9) 4 }cos{2( si -0.9)}+( si -0.9)/2, where s i = (s i1 , s i2 ) and si = s i1 +s i2 2 . To distinguish these two processes, in this paper we define the stationary process as Y iS and the non-stationary process as Y iN . An independent nugget variance of ν 2 = 1e -6 is added to the non-stationary process. Figure 1 shows the spatial surface of sample realizations. For each process, the source data set each consists of N = 4900 observations. We evaluate the performance on the target data set of N = 25, 64, 100, 225 observations. Figure 2 displays the MSE comparison of 1). source data pre-trained MSE on target data 2). target data set only, and 3). Kriging result on stationary data. Figure 3 compares the results on nonstationary data. During the pre-training stage on external data, a total of 1500 epochs are used with a learning rate of 0.001. A trace plot with validation set is monitored to make sure the neural network has converged. During the tuning stage on target data, all parameters from the pre-training stage are updated. A total of 1000 epochs with a learning rate of 0.001 is used. Figure 3: Non-stationary process MSE Conclusion As we can see from the figures above, for stationary data, the proposed approach outperforms both target only setting and the traditional Kriging approach. As target set gets larger, the proposed method and target only neural network converges. For the non-stationary scenario, the proposed method significantly outperforms target only approach when the target sample size is less than 100, and similar to the stationary case, these two neural network converges when the sample size gets larger. The proposed transfer learning approach is a first step towards spatial transfer learning. Following He et al. (2019) , we tune all parameters in the model. One possible future extension is to add additional layers to the neural network, fix the first seven layers in the network, and only tune additional layers. Also, the fully connected neural network may not capture the spatial dependence well. It is possible to combine the 4N network proposed by Wang et al. (2019) or the graph neural network Klemmer et al. (2023) . Figure 1 : 1 Figure 1: Simulation data Figure 2 : 2 Figure 2: Stationary process MSE"
}