{
  "title": "A Declarative Query Language for Scientific Machine Learning",
  "abstract": "The popularity of data science as a discipline and its importance in the emerging economy and industrial progress dictate that machine learning be democratized for the masses. This also means that the current practice of workforce training using machine learning tools, which requires low-level statistical and algorithmic details, is a barrier that needs to be addressed. Similar to data management languages such as SQL, machine learning needs to be practiced at a conceptual level to help make it a staple tool for general users. In particular, the technical sophistication demanded by existing machine learning frameworks is prohibitive for many scientists who are not computationally savvy or well versed in machine learning techniques. The learning curve to use the needed machine learning tools is also too high for them to take advantage of these powerful platforms to rapidly advance science. In this paper, we introduce a new declarative machine learning query language, called MQL, for naive users. We discuss its merit and possible ways of implementing it over a traditional relational database system. We discuss two materials science experiments implemented using MQL on a materials science workflow system called MatFlow.",
  "introduction": "can it perform the most complex analysis a modern ML algorithm can? The current state of ML is not accessible to most of potential users of data science [29] , scientists in particular, and we concur with many researchers who believe that a significant barrier exists towards exploiting ML without a declarative platform [47] . In the absence of a language similar to SQL, it is extremely difficult and unlikely for naive users and scientists alike to comprehend, let alone devise, a simple regression analysis code fragment easily executable on a machine. For example, the process to perform a clustering analysis [55] (or classification [46] ) on the Boston housing dataset on Kaggle [50] is by no means an easy task even for a good computational scientist, without adequate proficiency in regression analysis. It requires exploratory data analysis, principal component analysis, and more to get a sense of the data and to make a decision about the number of clusters that are appropriate, most of which can also be performed by a smart algorithm. Then there is the issue of accuracy and selection of the best model for the analysis [5, 30, 66] . The natural question then is, are all these details necessary, at least most of the time? Could these analysis algorithms be chosen by a query processor from an abstract request for prediction, clustering or classification based on the properties of the data sets the same way relational database engines select join algorithms, aggregate function algorithms or procedures for OLAP functions? Could optimization be possible and decided by query processors in ways analogous to SQL engines? While we do not currently have all the answers, we believe that the starting point should be the development of a suitable declarative query language for ML that will be simple in spirit and expressive enough to be able to support most, if not all, ML analysis needs on tabular data. To that end, our goal in this paper is to introduce an ML query language, called MQL (stands for Machine learning Query Language), capable of supporting three basic classes of ML tasks -prediction, classification and clustering. We stagger the language constructs in three tiers -data preparation (or wrangling), model construction, and ML analysis. These language constructs have distinct semantics and no inherent inter-dependencies. Finally, as data science becomes mainstream [10] and scientists increasingly rely on ML tools almost in every areas of science and engineering (e.g., [6, 44, 53, 71] ), efforts are needed to lower the entry barriers to ML tools for scientists. As various user aids emerge (e.g., visual interfaces [42, 58] , natural language interfaces [56] , ML tools [11, 12, 51] , arresting ML application development costs [21, 28, 41] are becoming imperative. New ways must be found to reduce access and application development costs involving AI and ML. Declarative query languages reduce the access barrier, and thus costs of data analysis by allowing minimally trained experts to use ML. We believe, declarative languages also are more amenable to automated code synthesis, e.g., natural language interfaces to scientific applications that can be constructed fully autonomously from software specifications, again expressed in natural language. In the sections to follow, we first present MQL's syntax and semantics, and then discuss its merit over the contemporary declarative languages. Using two materials science experiments in our lab, we highlight how easy cost-inspiring it is to use MQL for scientific application using tabular data and traditional ML. While more research is needed to make MQL more expressive and powerful to support more sophisticated ML frameworks such as deep learning, the current edition of MQL paves the way for further extensions.",
  "body": "can it perform the most complex analysis a modern ML algorithm can? The current state of ML is not accessible to most of potential users of data science [29] , scientists in particular, and we concur with many researchers who believe that a significant barrier exists towards exploiting ML without a declarative platform [47] . In the absence of a language similar to SQL, it is extremely difficult and unlikely for naive users and scientists alike to comprehend, let alone devise, a simple regression analysis code fragment easily executable on a machine. For example, the process to perform a clustering analysis [55] (or classification [46] ) on the Boston housing dataset on Kaggle [50] is by no means an easy task even for a good computational scientist, without adequate proficiency in regression analysis. It requires exploratory data analysis, principal component analysis, and more to get a sense of the data and to make a decision about the number of clusters that are appropriate, most of which can also be performed by a smart algorithm. Then there is the issue of accuracy and selection of the best model for the analysis [5, 30, 66] . The natural question then is, are all these details necessary, at least most of the time? Could these analysis algorithms be chosen by a query processor from an abstract request for prediction, clustering or classification based on the properties of the data sets the same way relational database engines select join algorithms, aggregate function algorithms or procedures for OLAP functions? Could optimization be possible and decided by query processors in ways analogous to SQL engines? While we do not currently have all the answers, we believe that the starting point should be the development of a suitable declarative query language for ML that will be simple in spirit and expressive enough to be able to support most, if not all, ML analysis needs on tabular data. To that end, our goal in this paper is to introduce an ML query language, called MQL (stands for Machine learning Query Language), capable of supporting three basic classes of ML tasks -prediction, classification and clustering. We stagger the language constructs in three tiers -data preparation (or wrangling), model construction, and ML analysis. These language constructs have distinct semantics and no inherent inter-dependencies. Finally, as data science becomes mainstream [10] and scientists increasingly rely on ML tools almost in every areas of science and engineering (e.g., [6, 44, 53, 71] ), efforts are needed to lower the entry barriers to ML tools for scientists. As various user aids emerge (e.g., visual interfaces [42, 58] , natural language interfaces [56] , ML tools [11, 12, 51] , arresting ML application development costs [21, 28, 41] are becoming imperative. New ways must be found to reduce access and application development costs involving AI and ML. Declarative query languages reduce the access barrier, and thus costs of data analysis by allowing minimally trained experts to use ML. We believe, declarative languages also are more amenable to automated code synthesis, e.g., natural language interfaces to scientific applications that can be constructed fully autonomously from software specifications, again expressed in natural language. In the sections to follow, we first present MQL's syntax and semantics, and then discuss its merit over the contemporary declarative languages. Using two materials science experiments in our lab, we highlight how easy cost-inspiring it is to use MQL for scientific application using tabular data and traditional ML. While more research is needed to make MQL more expressive and powerful to support more sophisticated ML frameworks such as deep learning, the current edition of MQL paves the way for further extensions. RELATED WORK The main purpose of a declarative language to reduce the humanmachine interfacing barriers by making machine instructions simple and easy to conceptualize. An all time great example of declarative languages is SQL. While this definition of declarativity is subjected to interpretation, the essence should remain. From this standpoint, a simple language for ML has to be highly abstract, and should support the so called naive users' use of ML tasks having only conceptual and rudimentary knowledge of this technology while the machines assume the bulk of the technical underpinnings and efficiency concerns [70] . Given that ML tasks are complex, involved, and require subject expertise, meeting such levels of abstraction requirements in a human-machine interfacing language, or query language, is undoubtedly a tall order. Nonetheless, several attempts were made to simplify the use of ML technologies for the masses. Among them, AutoML [59] maybe the most prominent effort of all. While challenges remain [23] , the emergence of large language models appear to address many of these challenges to some extent [69] toward democratizing ML. AutoML, or Automated Machine Learning, is a set of techniques aimed at automating the process of building ML models. The basic idea behind AutoML is to make ML more accessible to users with limited ML expertise by automating some of the complex and timeconsuming tasks, such as data preprocessing, feature engineering, model selection, and hyperparameter optimization, involved in model development. By automating these tasks, AutoML aims to reduce the amount of manual effort required to build and deploy ML models, making it easier for non-experts to leverage the power of ML in their applications. Variants of the ideas behind AutoML are also being investigated. Among them, MLBase [33] attempted to help automating the pipeline by proposing a declarative language and an optimizer to lend a hand in balancing the efficiency aspects of declarativity that usually delegates this responsibility to the system. Despite the design goal, the language they support appears to retail procedural features still and is not abstract enough compared to languages such as SQL to have a wider appeal. An early effort to develop a simpler ML front-end was a natural language interpreter called WOLFE [65] . In approaches such as WOLFE, query understanding and mapping the intent into some form of executable code is employed, in WOLFE the code is written in TensorFlow. A similar translational approach is used in languages such as sql4ml [45] , ML2SQL/MLearn [60, 61] , P6 [38] , MLog [39] , Datalog [73] , and Dyna [72] . The popularity of translational implementation of declarative languages to ML frameworks such as TensorFlow, PyTorch or SciKit Learn is not by accident. Rather it is convenience and a desire to leverage the community investments in powerful and a large body of algorithms for ML over a few decades. While a more powerful end-to-end ML systems are developed and mature, such as SystemML [16] , SystemDS [15] , End-ToEndML [52] , Merlion [13] , VeML [36] , and Relax [34] , we believe these translation grounded systems will continue to play a major role in democratizing ML. In sql4ml, an SQL like ML instruction is mapped to TensorFlow script. However, the CREATE VIEW abstraction conceived can do little to hide the subject expertise users need to state the analysis needs defeating the purpose of a declarative language for ML. The MLearn system [60] also do so using its ML2SQL mapping approach [61] . The operator creation based approach is tedious and rests significant domain knowledge burden on the users. Dyna and P6 systems bring optimization and visualization capabilities into the declarative ML landscape. A more systematic investigation to deal with performance of AutoML engines show that ML pipelines efficiency can be improved using a predictive model [76] . And in cases where automation is difficult, a human-in-the-loop approach may also help [74] . In MQL, however, we adopt a SQL-Centric approach, as opposed to DSL-Centric or UDF-Centric approaches [17] and propose an entirely new declarative language for ML in the spirit of SQL even though we too rely on a translational approach to its implementation. While we present a mapping to SciKit Learn for its implementation as a proof of concept, we note that more needs to be done to make MQL a viable system for serious ML platform. In sections 5 and 6, we will present MQL's capabilities in scientific computing, and possible improvements respectively to elaborate further. MACHINE LEARNING QUERY LANGUAGE With the intent to stay close to an SQL-like language, called MQL, we propose a syntax similar to SQL and design lower level operational procedures to assign a semantics to the declarative statements of MQL. MQL retains part of SQL flavor to leverage the community knowledge of SQL and reduce cognitive overload. Syntax of MQL Similar to SQL, MQL supports two basic statements -the GEN-ERATE statement for querying tables and CONSTRUCT statement for creating a ML model. While the GENERATE statement is able to exploit an existing model, it can also operate without one by generating its own model. 3.1.1 The GENERATE Statement. GENERATE statement stands at the level of SQL's SELECT statement and is the main workhorse of MQL. It operates on tabular data to make predictions, categorize objects and group sets of objects into bins. It has five basic clausesan ML class selection (one of PREDICTION, CLASSIFICATION and CLUSTER), optional object labeling, feature selection, a data set, a filter condition over the data set, and an input table of unknown cases (test set). GENERATE [DISPLAY OF] PREDICTION v [OVER s] | CLASSIFICATION INTO L1, L2, ..., Lp [OVER s] | CLUSTER OF k [USING MODEL ModelName | ALGORITHM AlgorithmName] [WITH MODEL ACCURACY P] [LABEL B1, B2, ..., Bm] [FEATURES A1, A2, ..., An FROM r1, r2, ..., rq WHERE c] In the above statement, ğ‘Ÿ ğ‘™ is a table over the scheme ğ‘… ğ‘™ , ğ‘ is a Boolean condition, ğ´ ğ‘– âˆˆ âˆª ğ‘™ ğ‘… ğ‘™ , ğ‘  is a table over the scheme âˆª ğ‘— ğµ ğ‘— âˆª ğ‘– ğ´ ğ‘– , k is an integer, and ğ‘£ âˆˆ âˆª ğ‘™ ğ‘… ğ‘™ , ğ¿ ğ‘˜ âˆˆ ğ‘‘ğ‘œğ‘š(ğ‘‹ ) 1 . In this statement and in all the MQL statements, the vertical bar (|) means exclusive OR, and the square bracket ([]) means optional. ğ‘£ in the PREDICTION clause is the target variable, and ğ´ ğ‘– s are the features. The optional LABEL clause identifies attributes ğµ ğ‘— as the object identifiers for all the three ML tasks. The CLASSIFICATION clause classifies each object âˆª ğ‘— ğµ ğ‘— into one of ğ¿ ğ‘˜ categories. The ğ‘˜ in CLUSTER clause is an integer expression that can include SQL aggregate functions over the tables ğ‘Ÿ ğ‘™ . Finally, the optional USING clause is meant to either use an existing model (MODEL option) generated using the CONSTRUCT clause (discussed next), or a specific ML algorithm (ALGORITHM option) for the generation of the model. As in SQL, WHERE is an optional clause, but unlike SQL, FROM is required. The OVER clause supplies the unknown test dataset over the scheme ğ´ ğ‘– âˆª ğµ ğ‘— . The ACCURACY option accepts a threshold within the interval (0,1). 3.1.2 CONSTRUCT Statement. To create an explicit model, MQL uses the CONSTRUCT statement below. It stands at a level similar to SQL's CREATE TABLE statement, but is at the data level and more functional. It is able to generate a default model for prediction, classification or clustering, optionally using a specific algorithm for supervised or unsupervised learning. The TRAIN ON parameter N (similarly TEST ON) is an integer value less than the cardinality of the table ğ‘Ÿ , and can be expressed as an expression, possibly using SQL aggregate functions. While the expression for ğ‘€ should be such that ğ‘ + ğ‘€ â‰¤ |ğ‘Ÿ | (where |ğ‘Ÿ | = |ğ‘Ÿ 1 | Ã— |ğ‘Ÿ 2 | Ã— . . . Ã— |ğ‘Ÿ ğ‘› |), MQL will not object if the condition ğ‘ + ğ‘€ â‰¤ |ğ‘Ÿ | is not met and will assign the eventual semantics entailed by these two expressions. In this statement, ğ´ ğ‘– is the feature vector over which the model is created. CONSTRUCT ModelName [AS SUPERVISED | UNSUPERVISED] FOR PREDICTION v | CLASSIFICATION INTO L1, L2, ..., Lp | CLUSTER OF k [USING AlgorithmName] [WITH MODEL ACCURACY P] TRAIN ON N TEST ON M FEATURES A1, A2, ..., An FROM r1, r2, ..., rn WHERE c 3.1.3 The INSPECT Statement. The INSPECT statement is similar to the UPDATE statement of SQL and helps editing or wrangling the tables. For attributes ğ´ ğ‘– , it allows the values in these columns to be categorized, missing values predicted, convert categories to continuous values and eliminate duplicate rows. This statement affords MQL the power to manipulate a table to make it suitable 1 ğ‘‘ğ‘œğ‘š (ğ‘‹ ) is the set of elements in the domain of the column ğ‘‹ , and ğ‘‹ âˆˆ âˆª ğ‘– ğ´ ğ‘– . for a potential learning task fully autonomously by a smart preprocessing engine. INSPECT returns a table with a scheme of a relation reflective of the resulting table in the FROM clause. INSPECT A1 [CATEGORIZE INTO L1, L2, ..., Lx | IMPUTE | NUMERIZE AS E | DEDUPLICATE], A2 [CATEGORIZE INTO L1, L2, ..., Lx | IMPUTE | NUMERIZE AS E | DEDUPLICATE], ..., An [CATEGORIZE INTO L1, L2, ..., Lx | IMPUTE | NUMERIZE AS E | DEDUPLICATE] FROM r1, r2, ..., rn WHERE c Semantics of MQL The semantics we assign to each of these statements are system and implementation specific. By that we mean that each system implementing these statements will play a major role in their meaning, efficiency, accuracy and performance. For example, if they are implemented in TensorFlow, as opposed to PyTorch or R, they will demonstrate different characteristics, i.e., the predictions made by underlying TensorFlow algorithms could be different from the Pytorch or SciKit-Learn based algorithms, and the prediction accuracies may vary. We consider this aspect of MQL as somewhat similar to SQL's optimization strategies. Only difference is in the case of MQL it is more about the quality of the predictions or the semantic interpretations of data. In this paper, we do not address these issues and only focus on generic semantics we expect from each of these statements. For illustrative purposes, we use Kaggle's Boston housing market dataset [50] as our example. This data has 506 observations with 13 continuous and 1 binary attributes stored as the file bostonHomes with the following interpretations (partial list, full list in [50] ): (1) CRIM -per capita crime rate by town (2) ZN -proportion of residential land zoned for lots over 25,000 sq.ft. (3) NOX -nitric oxides concentration (parts per 10 million) (4) DIS -weighted distances to five Boston employment centres (5) TAX -full-value property-tax rate per $10,000 (6) PTRATIO -pupil-teacher ratio by town (7) MEDV -Median value of owner-occupied homes in $1000's Many distinct analyses for this dataset by a large number of researchers point to how a smart query processor and optimizer could exploit them to develop processing strategies to meet user needs. Our goal, however, is not to delve into processing strategies or optimization opportunities except to offer these passing comments for interested readers. Instead, we refer to the Python code segment in Fig 2 that implements a linear regression model assuming a Pandas DataFrame \"df\" with columns MEDV, CRIM, ZN, NOX, DIS, TAX, PTRATIO. It splits the data into training and testing sets, standardizes the features, builds a linear regression model using SciKit-Learn, trains the model on the training set, evaluates it on the test set, and makes predictions. The number of epochs and other hyperparameters can be adjusted by a query processor for the dataset, as needed, to meet any user specified performance threshold. Similar code segments can be generated to implement the CONSTRUCT and INSPECT statements. Query Processing. Compared to SQL databases, ML databases and query processing are likely more nuanced, complex, and require more user involvement in library and algorithm selection, or code customization. Query processing for MQL currently needs additional instructions beyond the Python scripts similar to the one in Fig 2 , and are not explained further. A file handler has been implemented to bring data to the MQL store and link with the query processor. The directory path for the Boston housing data in CSV format can be included in the Python code segment or copied into the directory where the code is running. The MQL query for the prediction of home values using the Boston housing data in Fig 1 can be submitted in command line mode in the MQL engine for execution. In this query, the median home value is being predicted for homes in the file homesNew given a subset of features in CRIM, ZN, NOX, DIS, TAX, PTRATIO. In the plot, the HomeNo in the file homesNew is used as label.  In this program, we first extract the features ('CRIM', 'ZN', 'NOX', 'DIS', 'TAX', 'PTRATIO') and the target ('MEDV') from the DataFrame. We then split the data into training and testing sets using train_test_split. Next, we create a Linear Regression model and train it on the training data using fit. We then make predictions on the test set using predict, and evaluate the model using mean squared error (mean_squared_error). On execution over the slightly sparse dataset in GENERATE DISPLAY OF PREDICTION MEDV OVER homesNew LABEL HomeNo FEATURES CRIM, ZN, NOX, DIS, TAX, PTRATIO FROM bostonHomes IMPLEMENTATION STRATEGY We have implemented the MQL statements using SciKit-Learn over CSV datasets. Currently, we only support one table in CSV format in the FROM clause and no WHERE clause condition is allowed 2 . Note that these restrictions are not a limitation of the language and does not affect its expressive power. While the current implementation is directly using Python over CSV files on Panda, a more serious implementation in PostgreSQL using User Defined Functions (UDFs) written in SQL and PL/Python [57, 62] is underway. Once completed, we should be able to compare performance of the current file based and the PostgreSQL based approaches to implementation and comment more on how these choices influence various ML query processing parameters in ways import pandas as pd df = pd.read_csv(\"bostonHomes.csv\") from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error # Assuming you have a DataFrame 'df' with columns 'MEDV', 'CRIM', 'ZN', 'NOX', 'DIS', 'TAX', 'PTRATIO' # Extracting features (CRIM, ZN, NOX, DIS, TAX, PTRATIO) and target (MEDV) X = df[['CRIM', 'ZN', 'NOX', 'DIS', 'TAX', 'PTRATIO']] y = df['MEDV'] # Splitting the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Creating a linear regression model model = LinearRegression() # Training the model model.fit(X_train, y_train) # Making predictions on the test set y_pred = model.predict(X_test) # Evaluating the model mse = mean_squared_error(y_test, y_pred) print(\"Mean Squared Error:\", mse) # Printing the coefficients of the model print(\"Coefficients:\", model.coef_) # Printing the intercept of the model print(\"Intercept:\", model.intercept_) # Test set prediction from sklearn.impute import SimpleImputer test_samples = pd.DataFrame({ 'CRIM': [0.00632 , 0.50031, np.nan, 0.02731], 'ZN': [18, 7, 12, 0], 'NOX': [0.538, np.nan, np.nan, 0.469], 'DIS': [4.09, 3.20, 2.78, np.nan], 'TAX': [296, 107, 148, 242], 'PTRATIO': [15.3, 3.5, 11.6, np.nan]}) imputer = SimpleImputer(strategy='median') imputer.fit(X_train) X_test_imputed = imputer.transform(test_samples) X_test_imputed_df = pd.DataFrame(X_test_imputed, columns=['CRIM', 'ZN', 'NOX', 'DIS', 'TAX', 'PTRATIO']) predictions = model.predict(X_test_imputed_df) # Printing the predictions print(\"Predictions for the 4 test samples:\", predictions) Figure 2: SciKit-Learn Python code for MQL query in Fig 1. HomeNo CRIM ZN NOX DIS TAX PTRATIO 1 0.00632 18 0.538 4.09 296 15.3 2 0.50031 7 -3.20 107 3.5 3 -12 -2.78 148 11.6 4 0.02731 0 0.469 -242 -Figure 3: Test data input table homesNew.  similar to P2D [27] that also takes a similar translational approach. Opportunities also exist to decide system defaults for DISPLAY OF, data wrangling for test data (e.g., assuming zero va;ue imputation), and so on. Translational Semantics of MQL One of the most convenient and effective implementation strategies for novel languages is to map it to a fully functional and well known language. Among the popular ML frameworks such as PyTorch, Keras, TensorFlow, XGBoost, MXNet and so, SciKit Learn probably is one of the most widely used. We choose SciKit Learn for its excellent support for tabular data analysis using traditional ML tasks, and the ease of use. In this section, we develop an algorithm ğœ to assign a translational semantics to all MQL programs P by mapping it to a SciKit Learn program S, i.e., ğœ (P) = ğ‘†, such that ğœ‡ (P) â‰¡ ğœ‡ (ğ‘†), where ğœ‡ is a meaning function. An MQL program essentially is a sequence of one of three MQL statements -GENERATE, CONSTRUCT or INSPECT. Therefore, the meaning of a program P is the intended meaning of each of the statements in the sequence they are stated. As MQL is a contextindependent language, it is easy to see that if every MQL statement ğ‘ âˆˆ P could be translated into a SciKit Learn program S ğ‘ , i.e., ğœ (ğ‘) = S ğ‘ , such that ğœ‡ (ğ‘) â‰¡ S ğ‘ , then the relationship ğœ‡ (P) â‰¡ ğœ‡ (S) will hold. We therefore, design our translation function ğœ to map P on a case by case basis. The operational model of MQL shown in  It should be noted that there is an inherent dependency â†¼ among the MQL statements as follows. Although the semantics of the CONSTRUCT statements depends on the appropriate table representation conforming to the data types that can be adhered to by data wrangling operations using the INSPECT statements, MQL mandates that such an operation should be initiated by the user's program. In other words, invoking an INSPECT operation is not automatic even when CONSTRUCT â†¼ INSPECT holds, and MQL expects the program to FAIL if CONSTRUCT is not executable due to datatype errors and not corrective step using INSPECT precedes it. On the other hand, the dependency GENERATE â†¼ CONSTRUCT is fully automatic. The dependency of the GENERATE statement on CONSTRUCT is manifested in two principal ways. First, when the ALGORITHM option is used and a new model generation is required as follows. GENERATE [DISPLAY OF] PREDICTION v [OVER s] | CLASSIFICATION INTO L1, L2, ..., Lp [OVER s] | CLUSTER OF k ALGORITHM AlgorithmName [WITH MODEL ACCURACY P] [LABEL B1, B2, ..., Bm] FEATURES A1, A2, ..., An FROM r1, r2, ..., rq WHERE c Or when none of the options USING or ALGORITHMS is used in the GENERATE statement indicating a default model must be generated as follows. GENERATE [DISPLAY OF] PREDICTION v [OVER s] | CLASSIFICATION INTO L1, L2, ..., Lp [OVER s] | CLUSTER OF k [WITH MODEL ACCURACY P] [LABEL B1, B2, ..., Bm] FEATURES A1, A2, ..., An FROM r1, r2, ..., rq WHERE c In these cases, an appropriate CONSTRUCT execution is invoked and a model is generated for use and destroyed once GENERATE ends. Note that in both cases, FEATURES option is mandatory as GENERATE needs to know which features to use. The second way GENERATE indirectly depends on CONSTRUCT is when USING is used as follows. In this case, FEATURES need be used as a model is already built and deployed. In this case, dependency is explicitly captured using an explicit CONSTRUCT statement ahead of the GENERATE statement in the program (or executed separately and the generated model archived in the MQL system). GENERATE [DISPLAY OF] PREDICTION v [OVER s] | CLASSIFICATION INTO L1, L2, ..., Lp [OVER s] | CLUSTER OF k USING MODEL ModelName [WITH MODEL ACCURACY P] [LABEL B1, B2, ..., Bm] Mapping Algorithm ğœ We breakdown the mapping algorithm ğœ into three component algorithms driven by a main driver algorithm where the translation takes off (see Alg 1). In the Algs 2 through 4, we use a function generate(p) that returns a set of descriptive properties of a syntactically correct MQL statement ğ‘. generate retruns the following descriptions: -StType: returns the class of statement type -one of gen, con or ins. -Model: in GENERATE, stored when USING MODEL, custom when ALGORITHM and default when none. -MLtype: in GENERATE or CONSTRUCT, pred when PREDIC-TION, class when CLASSIFICATION and clus when CLUS-TER. -ModName: in GENERATE, ModelName when USING MODEL, NULL otherwise. -Features: in GENERATE and CONSTRUCT, ğ´ 1 , ğ´ 2 , . . . , ğ´ ğ‘› when FEATURES, NULL otherwise. -Display: in GENERATE, yes when DISPLAY OFF, no otherwise. -Label: in GENERATE, yes when LABEL, no otherwise. -AlgName: in GENERATE and CONSTRUCT, AlgorithmName when ALGORITHM, NULL otherwise. In the Alg 1, we call generate and assign all these descriptive features of a statement ğ‘ into a class Î”, and pass it to the other algorithms as a decision making tool. Algorithm 1: Translator ğœ Data: an MQL program P Result: A SciKit Learn Script S ğ‘ 1 begin 2 for all ğ‘ âˆˆ P do 3 Î” â† gather(ğ‘); 4 switch Î”.StType do 5 case gen do 6 call GENERATE(ğ‘, Î”) 7 case cons do 8 call CONSTRUCT(ğ‘, Î”) 9 case ins do 10 call INSPECT(ğ‘, Î”) 11 return The Alg 2 for translating a GENERATE statement, uses a stored model when USING MODEL is used, otherwise it calls Alg 3 to use the functionalities of CONSTRUCT even though a CONSTRUCT statement is not explicitly requested. This is because when a stored model is not used in it, either a specific or custom model is requested using the ALGORITHM option, or none at all (default), which in both cases must be constructed, used and discarded. Note that, MQL has a default algorithm for each class of analysis, and it is not query or data dependent. This also means that use of a default algorithm is not always appropriate though results will be generated. To ensure analysis quality, WITH MODEL ACCURACY option can be used with default so that the system is able to find the best model for the intended analysis. Alg 4 implements a quicker and shortcut instruction for SQL's UPDATE statement. It supports convenient operations not directly available in UPDATE. For example, IMPUTE or DEDUPLICATE have no UPDATE counterparts. EXPERIMENTAL RESULTS In this section, we discuss two scientific ML applications that were recently modeled using MQL in our materials design system Mat-Flow as shown in Fig 6. Our first application is a quantum dye Algorithm 2: GENERATE Data: a GENERATE Statement ğ‘, Î” Result: A SciKit Learn Script S ğ‘ 1 begin 2 switch Î”.Model do 3 case stored do 4 switch Î”.MLType do 5 case pred do 6 create S ğ‘ for Î”.ModName for prediction of ğ‘£ using test set ğ‘ ; 7 case class do 8 create S ğ‘ for Î”.ModName for classification of ğ‘£ using test set ğ‘  into classes ğ¿ 1 , ğ¿ 2 , . . . , ğ¿ ğ‘ ; 9 case clus do 10 create S ğ‘ for Î”.ModName to create ğ‘˜ clusters using test set ğ‘ ; 11 if Î”.Display=yes then 12 include visualization instructions 13 if Î”.Label=yes then 14 include instructions for labels ğµ 1 , ğµ 2 , . . . , ğµ ğ‘š 15 return 16 case custom or default or best do 17 call CONSTRUCT(ğ‘, Î”) 18 return design experiment in which we aim to discover a new quantum dye molecule with a target extinction coefficient (ğœ€) higher than 250,000 ğ‘€ -1 ğ‘ğ‘š -1 using inverse ML [54] . The experiment involves two steps. In the first step, we use experimental data to learn the most relevant features ğ¹ that are significant contributors to high extinction coefficient of dyes. Then in the second step, we estimate the values of the features ğ¹ as a directional feature vector as candidates for our target extinction coefficient. The idea is to use these candidate features to design a novel molecule using a system such as GenUI [64] . Our validation process involved finding Cyanine-5 (Cy5) as a possible quantum dye with an extinction coefficient ğœ€ = 250, 000 ğ‘€ -1 ğ‘ğ‘š -1 . The second experiment is about prediction of bending modulus ğœ… of lipid bilayer membranes using experimental data. In this experiment, a meticulously curated dataset is used representing a large number of lipid bilayer membrane properties. We analyzed this data set using a graph convolutional neural network to generate a model for estimating the bending modulus of new experimental lipids with more than 78% accuracy, which is much higher than existing methods. Quantum Dye Design Quantum dyes or dots are widely used in biomedical applications such as cancer detection, medical imaging, and also in studying transport mechanisms in cells, functional heterogeneity of cells, diffusion movements of membrane transport proteins, and many Algorithm 3: CONSTRUCT Data: a CONSTRUCT Statement ğ‘, Î” Result: A SciKit Learn Script S ğ‘ 1 begin 2 create instructions to generate a table ğ‘‡ (Î”.Features) from the FROM clause tables ; /* includes ğ‘£ in the feature set if Î”.MLType=pred */ 3 include instructions to divide table ğ‘‡ into ğ‘ training and ğ‘€ test sets; 4 switch Î”.Model do 5 case others do 6 choose the default algorithm ğ´ for Î”.MLType; 7 switch Î”.MLType do 8 case pred do 9 if Î”.Model==custom then 10 create S ğ‘ for prediction of ğ‘£ using Î”.AlgName 11 else 12 create S ğ‘ for prediction of ğ‘£ of objects in ğ‘‡ using ğ´ 13 case class do 14 if Î”.Model==custom then 15 create S ğ‘ for classification of objects in ğ‘‡ using using Î”.AlgName and labels ğ¿ 1 , ğ¿ 2 , . . . , ğ¿ ğ‘ 16 else 17 create S ğ‘ for classification of objects in ğ‘‡ using ğ´ and labels ğ¿ 1 , ğ¿ 2 , . . . , ğ¿ ğ‘ 18 case clus do 19 if Î”.Model==custom then 20 create S ğ‘ for classification of objects in ğ‘‡ using Î”.AlgName for ğ‘˜ classes 21 else 22 create S ğ‘ for classification of objects in ğ‘‡ using ğ´ for ğ‘˜ classes 23 return 24 case best do 25 for each algorithm ğ´ of Î”.MLType do 26 create S ğ‘ for Î”.MLType of objects in ğ‘‡ using Î”.AlgName; 27 include instructions to choose the best model with accuracy â‰¥ ğ‘ƒ 28 return similar health research. Quantum dots also play a major role in solar cells, semiconductors, light emitting devices, etc. [22] . Our partner materials science lab is interested in discovering new quantum dyes that have molar extinction coefficient (ğœ€) higher than or equal to 250,000 ğ‘€ -1 ğ‘ğ‘š -1 [14] . Algorithm 4: INSPECT Data: an INSPECT Statement ğ‘, Î” Result: A SciKit Learn Script S ğ‘ 1 begin 2 create instructions to generate a table ğ‘‡ (ğ‘…) from the FROM clause tables ; /* Following an SQL interpretation of the relations in the FROM clause */ 3 for every attribute ğ´ ğ‘– and for every row in ğ‘‡ do 4 generate instructions for categorize, impute, numerize or deduplicate 5 return However, ğœ€ is not the only property of quantum dyes that are of interest. Their optical properties including high quantum yield, high brightness, high stability against photobleaching, and intermittent fluorescence signals are too in general [67] , making dye discovery a multi-dimensional design problem. It is actually known that the specific characteristics of quantum dyes can vary depending on the material system (such as CdSe, PbS, or InP) and the synthesis techniques employed. Quantum dyes can be engineered with different properties by modifying their size, shape, composition, and surface functionalization, offering a wide range of possibilities for tailoring their characteristics to specific applications. Thus discovering the fact that Cy5 has ğœ€ = 250, 000 from a Google search is not sufficient [3] . In fact, many Alexa Fluor family of dyes have members with even higher ğœ€ [1] . In our experiment, we use nanoHUB [43] quantum dye design data and set out to first determine which ML model ğ‘€ predicts the extinction coefficient ğœ€ most accurately, and learn a very small dominant set of predictive features ğ¹ , the latent space ğ‘§, that control ğœ€. Our goal is to discover dyes that are absent in nanoHUB dataset that have ğœ€ â‰¥ 150, 000, such as Cyanine-5 (Cy5). We then query scientific literature, and scientific repositories such as PubChem [32] , ChemSpider [49] or ChemDB [19] to extract the feature values ğ¹ of dye molecules, and compare their predicted ğœ€ by our chosen model ğ‘€, and with experimental and literature reports, and validate the accuracy. For our experiment, we generated a dataset, called DyeData, from nanoHUB over a 249 column feature space consisting of 8,802 dye objects. The feature space attributes represent important dye characteristics that are significant determinants of dipole moments (ğœ‡) and are used to optimize coupling within dye aggregates, especially in materials research [14] . We have integrated and explored Deep4Chem [31] , PhotoChem CAD 3 [68] , and Dyomics [2] , in order to prepare our feature space. In addition, we utilized RDKit [4] to extract various molecular descriptors as physical properties. The context ğ‘ in this set up is the extinction coefficient ğœ€. The following INSPECT and CONSTRUCT statements are representative of a sequence operations we have performed in MatFlow. We created two models -a Random Forest model and a Linear Regression Model, from which the Random Forest model was chosen for its higher model accuracy. The discovered latent feature space ğ¹ selected only 23 features from a total of 249 columns. A GEN-ERATE statement is issued to predict the extinction coefficients of unknown dyes using the Random Forest Model. The results of this analysis can be found in [54] . The INSPECT statement below involves two CSV files -Chromophore.csv(Tag, MolarCoeff, . . .) and High_Extinction.csv(Tag, ShouldBe) with a join column named \"Tag\". MQL supports all SQL statements and MQL statements can be used anywhere a table is expected, and vice versa. The INSPECT and SQL statements below show a sequence of data wrangling operations during the generation of the DyeData dataset. We have used about 80% (equal to 7,040) of the total observations (8, 802) in the DyeData dataset as the training data in the GENERATE statement below, and about 20% (equal to 1,760) as the test data. INSPECT ShouldBe NUMERIZE AS log(ShouldBe) FROM High_Extinction.csv; CREATE VIEW Temp.csv as SELECT Tag, CASE WHEN ShouldBe=NULL THEN ShouldBe=MolarCoeff END AS Epsilon FROM FROM Chromophore.csv LEFT OUTER JOIN High_Extinction.csv; ALTER TABLE Chromophore.csv DROP COLUMN MolarCoeff; CREATE VIEW DyeData.csv as SELECT * FROM Chromophore.csv LEFT OUTER JOIN Temp.csv; CONSTRUCT epsilonPred FOR PREDICTION epsilon USING RandomForest TRAIN ON 7040 TEST ON 1760 FEATURES * FROM DyeData; GENERATE DISPLAY OF PREDICTION epsilon OVER TestData USING ALGORITHM LinearRegression WITH MODEL ACCURACY 80 FEATURES * FROM DyeData; GENERATE DISPLAY OF PREDICTION epsilon OVER TestData USING MODEL RandonForest; Membrane Bending Modulus Prediction Lipid bilayer membranes play an important role in the functional architecture of living cells, facilitating essential processes including the transmission of signals between and within cells and the transport of substances across the cellular barrier [7] . These bilayers are dynamic structures composed of molecules with hydrophilic heads and hydrophobic tails, resulting in considerable variations in both their composition and physical properties [63] . Several factors, including molecular composition, environmental conditions, and physical state of lipids have a wide range of interactions in bilayers. These interactions make it difficult to understand and forecast the combined effects on the bilayer's properties [25] . Prediction of bending modulus of lipid membranes is important in various fields such as biophysics, biochemistry, and materials science. The bending modulus is a measure of the membrane's resistance to bending or deformation and is a crucial parameter in understanding the mechanical properties and behavior of lipid membranes and has applications in lipid design, drug discovery, nanotechnology, material science and other biomedical applications. However, this application requires the use of graph neural networks which the current edition of MQL does not support. We thus implemented a PyTorch translation for the GENERATE statement so that we are able to use a GNN model created using PyTorch through the invocation of the following GENERATE statement as a one off demonstration of the versatility of MQL and that developing a multi-framework implementation of MQL is feasible. GENERATE DISPLAY OF PREDICTION Kappa OVER LipidTestData USING MODEL LipidGnn; DISCUSSION In our view, there are not too many declarative ML languages that stand at the same level as MQL. By that we mean, a language that does not require users to express analysis needs using a language more akin to procedural codes. It should be readily noticed that the languages such as Dyna, ML2SQL, sql4ml, and P6 [38] though possibly are more powerful and customizable, they are closer to procedural languages such as Python or C++, and thus give an appearance and flavor of imperative programming. The CREATE FUNCTION or the CREATE OPERATOR statements and the elaborate codes in Python or C++ is a significant barrier. In contrast, we hide all procedurality and offer a flavor of SQL like semantics. As discussed in Sec 2, Dyna and P6 are both focused on optimization and visualization respectively, and thus declarativity is not their main focus. They are successful in code optimization and developing conceptual codes for easy visual analytics specification. They too are not truly comparable to MQL. We actually agree with Gleeson [26] and believe that declarativity should be SQL like, even for ML. Gleeson, however, encoded several ML tasks directly in PostgreSQL using ML features supported in it. For example, regression has been coded as follows where the objective is to \"learn\" the parameters ğ‘š and ğ‘ of a linear equation of the form ğ‘¦ = ğ‘šğ‘¥ + ğ‘ from the training data. WITH regression AS (SELECT regr_slope(y, x) AS gradient, regr_intercept(y, x) AS intercept FROM linear_regression WHERE y IS NOT NULL) SELECT x, (x * gradient) + intercept AS prediction FROM linear_regression CROSS JOIN regression WHERE y IS NULL; In this code segment, the regr_slope() and regr_intercept() functions are used to estimate the gradient and intercept terms, respectively. These correspond to the parameters ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ and ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘’ğ‘ğ‘¡ in the equation ğ‘¦ = ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ Ã— ğ‘¥ + ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘’ğ‘ğ‘¡. However, this is possible only in PostgreSQL and other database engines will not recognize this code segment. In MQL, we will express the same functionality as follows, which we are able to execute on any database engine using a front-end. GENERATE DISPLAY OF PREDICTION y OVER unknown_xs FEATURES x, gradient, intercept FROM linear_regression In the above query, the table unknown_xs contains the values ğ‘¥ for which ğ‘¦ needs to be predicted. The DISPLAY OF option plots a graph to show the ğ‘¦ values against each ğ‘¥ in unknown_xs. Without the DISPLAY OF option, MQL will just compute a table with the columns ğ‘¥ and ğ‘¦. The difference obviously is, in MQL, users think in a more abstract manner and at a very conceptual level. Our current implementation has a few drawbacks that we plan to address in MQL's future editions. The first drawback was a design choice for the first edition of MQL. In this edition, we did not include an option to generate visualization primitives for the CONSTRUCT and INSPECT statements, only GENERATE supports data visualization. But, it is necessary to allow visualization of various relationships during model building, feature selection and data wrangling. We are in the process of designing an extended set of suitable features to support data visualization. The second limitation of MQL is related to the quality of analysis and query processing performance. There are numerous ML frameworks and a large number of ML algorithms that are suitable for applications on a case by case basis. Therefore, it is imperative that an MQL query optimizer be developed to identify candidate algorithms most relevant to a specific ML task, data set and analytic options. In absence of such an optimizer, MQL is in risk to compromise quality of analysis or performance, or both. We hope to address this limitation soon. CONCLUSION The MQL language we have introduced has several basic strengths and advantages over other similar languages. First, its basic structure is simple and easy to understand. For example, the most basic construct for a prediction analysis is GENERATE PREDICTION f OVER inputData FEATURES f1, f2, ..., fk [LABEL objectName] FROM dataSet where ğ‘“ is a feature outside the feature vector âŸ¨ğ‘“ 1 , ğ‘“ 2 , . . . , ğ‘“ ğ‘˜ âŸ© which must be included in the scheme of the table dataSet, and so must be objectName if LABEL option is used. The table inputData must have a scheme that includes the feature vector âŸ¨ğ‘“ 1 , ğ‘“ 2 , . . . , ğ‘“ ğ‘˜ âŸ© plus objectName. The statement basically requests predictions of ğ‘“ for the objects labeled objectName with features {ğ‘“ 1 , ğ‘“ 2 , . . . , ğ‘“ ğ‘˜ }. Similar comments apply to classification and clustering requests in MQL. The algorithmic and procedural separation of MQL and its declarative semantics also offers the opportunity for selecting implementation strategies, optimization and system level customization not offered by most contemporary declarative ML languages (the few that we are aware of). Better opportunities for using large language models now emerge to map natural language queries into MQL in ways similar to SQL for a more streamlined execution, instead of mapping to archaic Python codes. While we are contemplating a PostgreSQL implementation of MQL and explore optimization strategies, its current implementation on a file based store serves as a proof of concept and demonstrates its merits. A more detailed description of MQL's implementation will be published elsewhere. Before concluding, we would like to note that one of the goals behind designing a declarative language for ML is to explore the possibility of a conversational ML agent that is capable of understanding analysis needs and autonomously construct ML models by selecting appropriate data sets and algorithms to be able to generate responses to scientific inquiries [9, 40] . Developing an SQL like query language for ML such as MQL makes it easier to leverage decades of experience of mapping natural language to SQL [24, 78] . A growing research interest is supporting exploration of ML models [18] , and conversational systems based on ML [35] . We believe that natural language to ML mapping could also help with explainable AI [8, 48] . Recent interest in using large language models (LLM) as an interface to database applications [37, 75] opens up opportunities for LLM interfaces to scientific applications requiring ML [20, 77] . Our future research is aimed at developing a conversational interface for scientific inquiries in the areas of materials science and computational biology using an LLM as a front-end and a ML system at the back-end for data analysis. Figure 1 : 1 Figure 1: MQL query for median home value prediction. 3. 2 . 2 22 Results. We assign translational semantics to the query in Fig 1 by mapping it to the SciKit-Learn program in Fig 2 for execution. Fig 3, it produced the plot in Fig 4(b) in which we assumed zero for missing values as shown in Fig 4(a). If imputed values are used for missing values as shown in the code in Fig 2, predictions will be slightly different. The predicted versus actual plot is shown in Fig 4(c). (a) homesNew assumed table.(b) Bar plot. (c) Predicted v actual plot. Figure 4 : 4 Figure 4: Predicted home median values. Fig 5 5 follows this simple implementation strategy. Figure 5 : 5 Figure 5: MQL operational model. Figure 6 : 6 Figure 6: MatFlow architecture. This means that if data has to come from multiple tables, users will need to pre-process and create a single table."
}