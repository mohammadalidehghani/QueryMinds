{
  "title": "On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation",
  "abstract": "We study a new technique for understanding convergence of learning agents under small modifications of data. We show that such convergence can be understood via an analogue of Fatou's lemma which yields Γ-convergence. We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning.",
  "introduction": "Introduction Recent progress in the use of optimal transportation techniques for machine learning in domain adaptation [3] and development of Wasserstein Generative adversarial networks [4] have helped our understanding of potential learning derived from theoretic properties of the underlying data. The topic of optimal transportation has grown significantly in recent years (see [8] , [9] , [11] and references therein). Machine learning models aim to solve a task (to prescribed accuracy) using only the information of known data (training set). In this context it is preferred to have non-parametric models over parametric statistical families. In this document we explore an idea that we call measure pre-conditioning the training data which consists in modifying the statistical model in order to improve performance of algorithms while preserving the limiting model. One can argue that measure pre-conditioning implicitly imposes unjustified structure to a problem but the idea is that measure pre-conditioning will simplify computations and ensure convergence to the original model. For example measure pre-conditioning one of the measures may allow using optimal transportation techniques to adapt a domain which would otherwise be very costly, this would yield a desired training in a task with little information. We use the terminology \"measure pre-conditioning\" as the technique reminds us of pre-conditioning matrices from linear algebra and optimization. 1.1 Organization of this document",
  "body": "1 Introduction 1.1 Organization of this document . . . . . . . . . . . . . . . . . . 1.2 Relation to literature . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Necessity of non-parametric measure pre-conditioning techniques 2 Measure pre-conditionings 3 A mathematical framework admitting pre-conditioning 3.1 Formulation of the problem . . . . . . . . . . . . . . . . . . . . 3.2 Convergence of the learning problem . . . . . . . . . . . . . . . 3.3 The main question . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Main Theorem . . . . . . . . . . . . . . . . . . . . . . . 3.4 A version of the envelope Theorem . . . . . . . . . . . . . . . . 3.4.1 No Empirical Probability Measure can Converge in the Total Variation Sense for all Distributions . . . . . . . . Introduction Recent progress in the use of optimal transportation techniques for machine learning in domain adaptation [3] and development of Wasserstein Generative adversarial networks [4] have helped our understanding of potential learning derived from theoretic properties of the underlying data. The topic of optimal transportation has grown significantly in recent years (see [8] , [9] , [11] and references therein). Machine learning models aim to solve a task (to prescribed accuracy) using only the information of known data (training set). In this context it is preferred to have non-parametric models over parametric statistical families. In this document we explore an idea that we call measure pre-conditioning the training data which consists in modifying the statistical model in order to improve performance of algorithms while preserving the limiting model. One can argue that measure pre-conditioning implicitly imposes unjustified structure to a problem but the idea is that measure pre-conditioning will simplify computations and ensure convergence to the original model. For example measure pre-conditioning one of the measures may allow using optimal transportation techniques to adapt a domain which would otherwise be very costly, this would yield a desired training in a task with little information. We use the terminology \"measure pre-conditioning\" as the technique reminds us of pre-conditioning matrices from linear algebra and optimization. 1.1 Organization of this document Relation to literature The authots of [3] develop the idea of optimal transport domain adaptation on which a linear approximation of the transport map is used to infer labels on target domain and [5] developed CO-OT, a technique on which optimal transport is not only done between source and target domains of data but in the space of data and labels. Recently [12] developed the META-optimal transport technique which by presolving an optimization problem improves on the optimal transport efficiency. In this work, the idea is similar: can we modify training sets to ensure properties of learning? The modifications considered in this document, differ from the ones on [12] as we only consider measure pre-conditioning data without establishing a minimization purpose beforehand. These techniques should remind the reader of the concept of preconditioning in optimization, on which one modifies a matrix via a correct scaling to benefit the algorithm computations. In the same fashion, here one modifies the measure associated to a training set to benefit statistical properties of the learning agent. Necessity of non-parametric measure pre-conditioning techniques The need for non-parametric measure pre-conditioning techniques arises from the modeller's attempt to not intervene in the learning while improving it's computational performance. Measure pre-conditioning is posed in this document as a general technique and it is the modeller's task to determine which pre-conditioning is useful for their own goal. In section 4.1 we give several examples with different goals in mind. 2 Measure pre-conditionings In this section we introduce the main concept and discuss several possible \"measure pre-conditionings\". In this context a measure pre-conditioning will be a technique to manipulate data in order to obtain a \"nicer\" measure. For example, we can regularize our problem to obtain a measure that is absolutely continuous with respect to Lebesgue or a measure that has a different type of support. Measure pre-conditioning is also similar to parameter fitting for curves. In the case of real variable one attempts to infer information from isolated data points by first creating a continuous (typically smooth) curve joining the points. Pre-conditioning between points in R has drawbacks (overfitting, highvariation, etc) and so will measure pre-conditioning (see section 8). Measure pre-conditionining will have the advantage of enabling stronger techniques to infer learning as we will see throughout the paper. We start by defining several possible measure pre-conditioning techniques and analyzing their properties. Problem 1 (General measure pre-conditioning problem for independent identically distributed data) Let (X 1 , X 2 , . . . , Xn) be a sample, that is {X i } n i=1 is a set of independent identically distributed data such that X 1 ∼ µ. Suppose that the sample will be used to train a machine learning model, the measure pre-conditioning problem is to find a good way to obtain a measure μn from the sample such that μn improves performance of the model or the computational cost of the algorithms while keeping the most relevant features of the problem intact. As such, this measure pre-conditioning problem is not mathematically well posed, as we haven't defined what \"improves performance of the model\" or \" keeping the most relevant features\" mean. Performance improvement can be done in several ways: simplification of algorithms, computational cost, control on domain adaptation or even yielding mathematical properties for the learning agent. All of these type of improvements are valid and impactful in machine learning research. The aim of this paper is to analyze how different measure pre-conditionings impact model performance. A mathematical framework admitting pre-conditioning Let us start with a basic framework from Machine Learning models in order to be able to define measure-preconditioning and show it's relevance. The simplest case is the minimization over all fitting functions f within a class of fitters C minimizing the expected value of the loss function L measuring the loss of fitting the random variable Y with the variable X via f (X). Formulation of the problem Now assume we don't know the full model π but we have a training sample, i.e. we have (X 1 , Y 1 ), . . . , (Xn, Yn) ∼ π, statistically we know the values on the sample but not the full distribution. Assume we approximate π using the sample via a probability measure πn, the associated C-model for L under πn reads arg min f ∈C Eπ n [L(f (x), y)] . (2) This formulation immediately give rise to the following questions i If L and C are fixed, what conditions on π n ensure that the minimizer in (2) approaches (1) ? In what topology? ii What properties could (2) have that (1) may lack? iii Given a choosing of π n 's, could we find sequences L n 's and C n so that the computations on the C n -problem with loss function L n associated to π n converge to (1)? Could these problems improve the algorithmic performance? Idea 1 (Measure pre-conditioning) A measure pre-condition is a way to define πn from the sample (X 1 , Y 1 ), . . . , (Xn, Yn) such that the associated C-problem with loss function L has improved performance in any way while preserving the convergence of minimizers of (2) to that of (1). Convergence of the learning problem Our main focus will be answering: when do minimizers of (2) converge to minimizers of 1 and in which way?. We first notice that in many situations it is possible to obtain the same total loss under convergence of the measures (without necessarilly having convergence of minimizers), this situation is rather general and known and is not the main question in the ML community but it gives a good starting point for the techniques used in this document. For many applications it is enough to know convergence of the total loss and so we exemplify conditions that yield such convergence. Proposition 1 (Standard convergence results on total loss (not minimizers)) 1. If ||L||∞ < ∞ or if spt(µ) is compact, |Eπ n [L(f (X), Y )] -Eπ[L(f (X), Y )]| ≤ ||πn -π|| T V . 2. Given f ∈ C if (x, y) → L(f (x), y) is Lipschitz, then |Eπ n [L(f (X), Y )] -Eπ[L(f (X), Y )]| ≤ d 1 (πn, π). 3. If L is C 2 and || ∂L ∂1 || < ∞ then |Eπ n [L(f (X), Y )] -Eπ[L(f (X), Y )]| ≲ ||πn -π|| T V + sup x∈Ω d(f * (x), f * n (x)) 4. If C is a compact class on C(Y), and πn → π in d 1 then along a subsequence n k Eπ n k [L(f * n k (X), Y )] → Eπ[L(f * (X), Y )] where f * n k is the C-optimizing argument for πn and f * is the C-optimizing argument for π. The proof of proposition 1 is direct and hence omitted. The main question Measure preconditioning modifies the minimization problem at level n, i.e. it changes the structure of the measure used to evaluate loss with a sample of size n. If the model was unchanged we would expect convergence of the learning agent trained with the sample of size n, i.e. f * n to the best fit with respect to the loss for the parametric distribution f * . If measure pre-conditioning modifies the measure at level n, the true question is when and in which ways does f * n → f * ?. To answer the convergence of minimizers, as it is usual in functional analysis and economics, we introduce Γ-convergence. Main Theorem We present an informal version of the main theorem of the work. This informal version corresponds to the rigorous statements answered in Theorem 5, Proposition 6 and section 5 Theorem 2 Full learner recovery system is a concept that allows us to show convergence of learning agents to the ideal parametric agent in cases not covered previously in the literature. This concept allows us to generalize stability arguments for less regular losses and a bigger class of classification/regression problems. Full learner recovery systems are general enough to be applied to several settings in Machine-Learning, including Domain Adaptation transfer learning. These systems explain many phenomena in ML-research where convergence is improved. Full learner recovery systems give a guideline on how and when to modify training data without disturbing the original problem. The formulation of Theorem 2 is not mathematically precise, we dedicate this work to make the Theorem rigorous and prove it in the subsequent sections. We start with the introduction of the main mathematical tool. A version of the envelope Theorem Definition 1 (Γ-convergence on a metric space) Let (X, d) be a metric space and let F j , F : X → R ∪ {±∞},we say Fn Γ-converges to F , denoted Fn Γ -→ F if and only if the following two conditions hold I For all sequences {x j } such that x j d -→ x we have lim inf j→∞ F j (x j ) ≥ F (x). II For every x ∈ X there exists a sequence x j d -→ x such that F (x) ≥ lim sup j→∞ F j (x j ). Remark 1 The most general definition for Γ-convergence is one where X is assumed to be a topological space and not necessarily metric. The definition presented above (Definition 1) is the sequential-definition. We have chosen the sequential definition as it simplifies the theory significantly, knowing that some important examples that we have in mind are only topological spaces on which the Γ-limit is defined via Γ -lim n→∞ Fn(x) = sup U ∈N (x) lim inf n→∞ inf y∈U fn(y). (3) In some of the examples below the underlying convergence will not correspond to a metric space, on which one must think of (3) instead of (I) and (II). The motivation behind the definition of Γ-convergence is that minimizers converge to minimizers, the content of the following theorem from [13] : Theorem 3 (Γ-convergence and minima) Let (X, d) and F j , F be as in Definition 1, then 1. If I from definition 1 is satisfied for all x ∈ X and K is a compact subset of X then inf K F ≤ lim inf j→∞ inf K F j (4) 2. Similarly, if II from definition 1 is satisfied and U is an open subset of X then lim sup j→∞ inf U F j ≤ inf U F (5) This Theorem can be found as [13, Proposition 1.18] . Finally we recall one more Theorem from [13] . We say that a sequence {F j } of functions on a metric space (X, d) is equi-mildly coercive if there exists a non-empty compact set K such that inf X F j = inf K F k for all j. Theorem 4 (Minimizers and Γ-limits) In a metric space (X, d) if {F j } is equi-mildly coercive and Fn Γ -→ F then min X F = lim j→∞ inf K F j (6) Furthermore, every limit point of a sequence of minimizers of (6) is a minimizer of F . For a proof see [13, Theorem 1.21] . With the theory in hand we take a general approach to answer the questions (i) and (ii). Instead of a constructive proof to find the optimal topologies (on C and P(X × Y )) we reformulate the convergence problem for it to satisfy the hypothesis of Theorem 1. This way we can relate to classical problems by looking at the given topologies of each framework and verifying the hypothesis. Going back to the framework of Problems (1) and 2, we want to be able to recover minimizers from our measure conditioning. We note the interaction of the class of fitters C, the loss function L and the mode of convergence of the conditioners that we choose to evaluate, henceforth it is logical to check conditions for them as a collective, rather than separately. This motivates the following definition. -→ f , we have lim inf n→∞ Eπ n [L(fn(X), Y )] ≥ Eπ[L(f (X), Y )]. (7) 2. If π j m -→ π and for every f ∈ C there exists a sequence f j ∈ C, such that f j d -→ f and Eπ[L(f (X), Y )] ≥ lim sup j→∞ Eπ j [L(f j (X), Y )] (8) Remark 2 In analytical terms, these conditions ensure 2-sided Fatou-Lemmas for integration with respect to L on the first coordinate. Γ-convergence can be also used to address the existence of minimizers of the parametric model but that is not the approach of this work, we assume existence of minimizers of the limiting problem and study recovery sequences, from now on we assume the existence of a unique minimizers for (2). (C , d ) is a compact metric space, assume the limiting problem from 1 has a solution f ∈ C, then there exists a sub-sequence {fn k } of {fn} ∈ C such that fn k ∈ arg min f ∈C Eπ n k [L(f (X), Y )] such that as k → ∞, fn k d -→ f and Eπ n k [L(fn k (X), Y )] → Eπ[L(f (X), Y ))]. (9) Proof The definition of (C, d, L, m -→) forming a full learner recovery system is such that E L πn Γ -→ E L π , i.e. by taking the functional Fn(f ) : C → R, defined via Fn(f ) := Eπ n [L(f (X), Y )], the definition 2 is equivalent to Fn Γ -→ F . By compactness of C we get the hypothesis for Theorem 4 so we get the thesis. □ In many cases C is not necessarily compact. The assumption of compactness simplifies the arguments but the argument above can be obtained without compactness of C if instead one assumes equi-mild-coercivity of {F n }, that is there exists a compact set K for which all F n 's satisfy inf C F n = inf K F n . See [13, Theorem 1.21], we instead assume compactness of C to avoid this subtlety. Remark 3 Evidently the statement of Theorem 5 is useless unless we explore examples and explain the ideas and how to use it. So far, we have just re-written the problem so that we can conclude (subsequential) convergence of learned agents by checking a modified version of Fatou's Lemma. This rewriting allows us to cover different cases at the same time, as we do in the following examples. The goal of this list is not to be exhaustive but to show the many different formulations that can be included in Definition 2. Notice that checking Definition 2 involves only studying a two sided version of Fatou's Lemma that can be corroborated in every particular case. Once one establishes that the given ML problem of the form (2) and (1) are indeed a full recovery system with {π n }, π, C, L one has ensured convergence of minimizers (which amounts to perfect approximation of the model). In the following proposition we show the wide range of options one has for full recovery systems, although the d-convergence in some items of the following proposition are not necessarily with respect to a metric, we have in mind Remark 1. Proposition 6 The following are full learner recovery systems 1. Let K ⊂ R p be compact, C a compact subset of {f : R p → R s.t. (x, y) → L(f (x), y) ∈ L 1 (π) } with respect to d, where d denotes point-wise convergence, L : R p × R → R be any positive, bounded, continuous function and let m -→ denote set-wise convergence i.e µn(A) → µ(A) for every Borel set A, where µn, µ ∈ P(K). 2. m -→:=⇀ (weak convergence of measures), C be compact such that {L(f (x), y)} f ∈C uniformly integrable with respect to {πn} and there exists g such that L(g(x), y) ∈ L 1 π such that fn(x) ≤ g(x) holds π-a.e.  Proof In all of the cases above we only need to ensure a Fatou-like lemma (Definition 2). 3. 1. Here Γ-convergence must be thought as in Remark 1. This is a direct consequence of Fatou's lemma for varying measures (found in [14] or [29, Theorem 1.1]). 2. See [29, Theorem 2.2]. The uniform Lipschitz condition gives L(f j (x), y)dπn -dπ(x) + L(f j (x), y) -L(f (x), y)dπ(x, y) ≤ d 1 (πn, π) + L(f j (x), y) -L(f (x), y)dπ(x, y) where the first term comes from Kantorovich-Rubinstein [9, Particular Case 5.16] and the second one vanishes by dominated convergece. 4. In this case we don't only have the inequalities of definition 2 but the limits coincide: L(fn(x), y)dπn(x, y) -L(f (x), y)dπ(x, y) ≤ L(fn(x), y)d(πn -π) + L(fn(x), y) -L(f (x), y)dπ ≤ M ||πn -π|| T V + L(fn(x), y) -L(f (x), y)dπ where the first one goes to zero by the assumption πn T V --→ π and the second one by the assumed d-continuity and dominated convergence. □ The goal of this list is not to be exhaustive but to show the many different formulations that can be included in Definition 2. Notice that checking Definition 2 involves only studying a two sided version of Fatou's Lemma that can be corroborated in every particular case. Once one establishes that the given ML problem of the form ( 2 ) and ( 1 ) are indeed a full recovery system with {π n }, π, C, L one has ensured convergence of minimizers (which amounts to perfect approximation of the model). Remark 4 Observe that the conditions imposed for C and L on Proposition 6 case 4 are less restrictive than the ones on 6 case 2. This is intuitively obvious as the total variation convergence is stronger than weak convergence. This means that ensuring a stronger convergence in measure is a degree of improvement for the ML-problem associated to fixed C and L. It is also evident that regularity conditions usually assumed in ML-theory (like Lipschitz properties of L) yield strong approximations in most types of convergence m -→, making this framework not only inclusive but rather general. One of the main advantages of measure pre-conditioning is the ability to change the training sample. It is common to use the empirical measure in nonparametric statistics, nevertheless the next section shows that the empirical measure is in general, not the best formulation for (2) as it may happen that the conditions for convergence hold for a different sequence of measures and not the sequence of empirical measures. We will see this is the case of Proposition 6 case 4, where the sequence of empirical measures would not ensure subsequential convergence but a different sequence does, justifying completely the use of measure pre-conditioning as it improves the likelihood that the algorithm gives a reasonable final learnt agent. Remark 5 (Compactness) Stronger conditions like compactness of the underlying sets yield a more elegant theory. Many of the modes of convergence are equivalent under the assumption on compactness (see [26] or [8, Chapter 7] ). The assumption of compactness simplifyies most theorems as it will automatically bound sequences and so Definition 2 is much easier to satisfy and verify which automatically yields: Proposition 7 If C ⊆ C(Y ), (x, y) → L(x, y) is continuous and sup f ∈C sup (x,y) |L(f (x), y)| < ∞ then (2) → (1) in the C uniform topology, i.e. arg min f ∈C Eπ [L(f (x), y)] C -→ arg min f ∈C Eπ [L(f (x), y)] . No Empirical Probability Measure can Converge in the Total Variation Sense for all Distributions Towards studying when to measure pre-condition we realize that it is important to know what types of empirical measures converge and in which cases. In the seminal work [15] , the authors proved the following theorem: Theorem 8 (No Empirical Probability Measure can Converge in the Total Variation Sense for all Distributions) Let {πn} be a sequence of empirical distributions and δ > 0, then there exists a proability measure π such that inf n sup A |πn(A) -π(A)| > 1 2 -δ a.s. For a proof see [15] . Theorem 8 tells us that the class of measures approximated in total variation norm by the empirical measure is not all measures. For different measures, other probability measures formed from data can converge in total variation but the empirical measure does not converge to all measures. Remark 6 In [15] it is shown that the standard empirical measure does not converge in total variation sense to absolutely continuous limits. Hence, Theorem 5 does not apply with Proposition 6 case 4 if we use the standard empirical measure. Nevertheless, as shown in [16] , the kernel-empirical measure given by πn = 1 hn K(f /h) does converge in total variation (see Definition 5 below). Hence, Theorem 5 via Proposition 6 case 4 applies to the sequence {πn} but not the sequence of standard empirical measures. This shows that the model solution for ML-program 2 will converge to the best parametric C-model. This argumentation explains why standard techniques in Machine-Learning, such as shifting and adding noise give better results in practice, as convergence is ensured by this system. Example: Linear regression Let us consider π ∈ P ac (R 2 ), we consider the linear regression problem with square-loss function with respect to target measure π: min (a,b)∈R 2 E π [(Y -aX + b) 2 ]. (TargetLR) By differentiating with respect to a, b from first order conditions we know that the solutions to (TargetLR) are a = y • xdπ(x, y) -ydπ(x, y) xdπ(x, y) x 2 dπ(x, y) - xdπ(x, y) 2 (10) b = ydπ(x, y) -      y • xdπ(x, y) -ydπ(x, y) xdπ(x, y) x 2 dπ(x, y) - xdπ(x, y) 2      xdπ(x, y) (11) If we consider a sequence of measures π n , obtained using the sample (X 1 , Y 1 ), . . . , (X n , Y n ) then the linear regression problem with square-loss function with respect to approximating measure π n is min (a,b)∈R 2 E πn [(Y -aX + b) 2 ]. (AppxLR) The solution (a πn , b πn ) to (AppxLR) is given by a πn = y • xdπ n (x, y) -ydπ n (x, y) xdπ n (x, y) x 2 dπ n (x, y) - xdπ n (x, y) 2 (12) b πn = ydπ n (x, y) -      y • xdπ n (x, y) -ydπ n (x, y) xdπ n (x, y) x 2 dπ n (x, y) - xdπ n (x, y) 2      xdπ n (x, y) (13) If π n corresponds to the empirical measure, then rate of convergence of a πn and b πn have been widely studied. See [27, Chapter 3] for example. We also know by Theorem 8 that Section 2] we can find a sequence of measures (Parzen windows) {π n } such that πn π n ̸ T V --→ π. By [16, T V --→ π. For simplicity, assume that xdπ n (x, y) = 0, x 2 dπ n (x, y) = 1, xdπ(x, y) = 0 and x 2 dπ(x, y) = 1. With this assumption we immediately obtain the following bound: |a πn -a π | ≤ sup (x,y)∈spt(πn)∪spt(π) |x • y| ||π n -π|| T V . (14) Which in the case where {π n }, π are uniformly compactly supported yields |a πn -b πn | ≲ ||π n -π|| T V . (15) Equation ( 15 ) is a bound on the order of convergence on the coefficient of linear regression of (AppxLR) to that of (TargetLR) which is not available in the case of the empirical measure, as indicated by Theorem 8. The bound (15) different to the usual order of convergence bounds for linear regression exemplifies the impact of measure pre-conditioning. Equation ( 15 ) shows (uniform) stability of learning agents corresponding to the measure pre-conditioned problem, allowing us to use more tools than the standard ones. Measure pre-conditioning approaches Measure pre-conditioning approaches impose certain structures to the original data. The idea is to analyze how does this structure impacts final outcomes of the modelling. In some way, this process resembles plain statistical inference. Background and Notation Let Ω ⊂ R n be fixed. We denote by P p (Ω) to be the set of probability measures with p-th finite moment. That is P p (Ω) = {µ ∈ P(Ω) : Ω |x -x 0 | p dµ < ∞, for some x 0 ∈ Ω}. We define the Wasserstein p distance between µ, ν ∈ P p (Ω) d p (µ, ν) = inf π∈Γ(µ,ν) Ω×Ω |x -y| p dπ(x, y) 1/p where Γ(µ, ν) denotes the set of probability measures on Ω × Ω having first marginal µ and second marginal ν. We say a map T : Ω 1 → Ω 2 is a Monge map with respect to the cost function c : Ω 1 × Ω 2 → R, between Borel measures µ and ν whenever T ∈ arg min T #µ=ν Ω1 c(x, T (x))dµ(x) (16) where T #µ means that for every Borel set A, ν(A) = µ(T -1 (A)). Empirical measures and non-parametric estimation In this section we discuss common non-parametric estimates and their relations to the structure of the ML-problems ( 2 ) and (1) . We aim to explain how each measure can be used to pre-condition and the pros and cons coming with their use. Non-exhausting list of non-parametric estimation techniques Definition 3 (Empirical measure) Given X 1 , . . . , Xn we define the standard empirical measure as the number of successes on the n occurrences: µn(A) = 1 n n k=1 δ X k (A). Definition 4 (Histogram) Given X 1 , . . . , Xn we define the histogram measure associated to the sets B 1 , . . . , Bm µn(A) = 1 n n k=1 m l=1 1 ρ(B l ) δ X k (A ∩ B l ). where ρ is a probability measure (usually taken to be normalized Lebesgue). Definition 5 (Kernel estimation via Parzen windows) Given X 1 , . . . , Xn, we define the n-th density estimation with kernel K via fπ n (x) = 1 nHn n i=1 K x -X i Hn where K is fixed and {Hn} is any sequence of random variables, that (may) depend on the sample X 1 , . . . , Xn that satisfy that Hn → 0 almost surely and nHn → ∞ almost surely. The idea of this formulation of the kernel estimation comes from [19] and [17] and it is fully justified by Theorem 14. Wasserstein 2-Barycenter Definition 6 (Wasserstein Barycenter) Given a sample X 1 , X 2 , . . . , Xn random variables in R p we define the 2-Wasserstein Barycenter of the sample (also called Frechet mean) as any probability measure satisfying µ * ∈ arg min ρ∈P 2 (R p ) n k=1 d 2 (ρ, δ X k ) 2 (17) where δ X k denotes the unit mass at X k . Remark 7 Note that ρ → d 2 (•, ν) 2 is lower-semicontinuous for all ν and so Wasserstein Barycenters exist. In general, Wassertein barycenters with respect to random Dirac measures are not unique. If instead, one of the deltas is replaced by an absolutely continuous measure, uniqueness can be shown. We don't do this replacement in this document, instead we study the entropic regularization of the minimization problem in Definition 8. The theory of Wasserstein Barycenters has recently received attention from several fields of applied mathematics, see for example [18] for a more complete theory. Remark 8 The barycenter can be defined given any distance function d : P(R p ) × P(R p ) → R and a sample (X 1 , . . . , Xn) the d-barycenter is any probability measure µ satisfying µ * ∈ arg min ρ 1 n n k=1 d(ρ, δ X k ) ( 18 ) where the infimum is taken over all probability measures on R p .. We have only chosen the Wasserstein 2-distance as we aim to focus on Domain Adaptation. Remark 9 It is important to notice that efficient algorithms to compute Wasserstein Barycenters have recently been developed (see [2] ) in the case of empirical measures. This efficient computability is essential for the applications we have in mind. Uniform convex hull Definition 7 (Convex Hull) The convex hull of a set B ⊆ R p is defined to be the smallest convex set on which B is contained, equivalently Conv(B) = C convex B⊆C C. We define the uniform convex hull of the sample (X 1 , X 2 , . . . Xn) to be the uniform measure on the convex hull of {X 1 , X 2 , . . . , Xn}, i.e. µconv = L p |c L p (Conv({X 1 , X 2 , . . . , Xn})) (19) where L p denotes the Lebesgue measure in R p . Remark 10 Note that µconv is the restriction of the Lebesgue measure to the convex hull of the sample so it's support is automatically convex. This particular property could be significant for future applications as the theory of convex optimization unlocks several numerical techniques. Evidently, it's support also includes all points of the sample. Note that Definition 7 always gives a well defined measure. Entropically regularized barycenter Definition 8 Given a sample X 1 , X 2 , . . . , Xn random variables in R p and a reference probability measure ν we define the ν-entropically regularized 2-Wasserstein Barycenter of the sample as any probability measure satisfying µ * ∈ arg min ρ∈P 2 (R p ) 1 n n k=1 d 2 (ρ, δ X k ) 2 + Ent(ρ | ν) (20) where δ X k denotes the unit mass at X k and Ent(µ | ν) denotes the relative entropy of ρ with respect to ν given by Ent(ρ | ν) = log dρ dν dν (21) whenever ρ ≪ ν and Ent(ρ | ν) = ∞ otherwise. Remark 11 If ν ≪ L p , the functional to minimize is lower semi-continuous and with the addition of entropy a unique absolutely continuous minimizer of (20). Class-regularized barycenter Motivated from the work of [3] we can also think of measure pre-conditioning in terms of pre-established class based groups. The idea behind the next definition is that elements in the same class may be very similar while elements from different classes could be very different from each other. Definition 9 (Class barycenter) Given a sample X 1 , X 2 , . . . , Xn random variables in R p suppose that each X i belongs to one and only one of a finite collection of classes {C l } m l=1 , then we can define the class-based barycenter to be any measure µ satisfying µ * ∈ arg min µ∈P 2 (R p ) 1 m m k=1 d 2 (ρ, ν k ) 2 + Ent(ρ | ν) (22) where ν k is a measure determined only from class C k . For example, one would obtain a barycenter of barycenters if one were to choose ν k to be the 2-Wasserstein barycenter of {X i : X i ∈ C k }. MMD-regularized Conditional measures Definition 10 Given a characteristic kernel function k (see [1] for details), define the maximum mean discrepancy between µ, ν with respect to k via mmd k (µ, ν) = E µ×µ [K(X, X)] + E ν×ν [k(Y, Y )] -2E µ×ν [k(X, Y )] The empirical optimal transference plan between conditional distributions for a given lower-semicontinuous cost function c, denoted π * ,c n is defined in [20] via the minimization over Γ(µ.ν) of the following functional: c(x, y)dπ + λ 1 1 n n i=1 mmd 2 k (P roj 1 #π, δ Yi ) + n i=1 mmd 2 k (P roj 1 #π ′ , δ Y ′ i ). ( 23 ) Existence and uniqueness depends on the cost function and usual conditions (smoothness and twist) are required, see [1] for details. Some properties of the measure pre-conditioners Proposition 9 When they exist, the measures from definitions 8 and 9 are absolutely continuous with respect to ν. Proof By definition, Ent(ρ|ν) = ∞ if ρ ̸ ≪ v, because ν is always feasible, the functional is not infinity and hence the minimizer is a.c. with respect to ν. □ Corollary 1 If ν = L p in Definitions 8 or 9, the minimizer has a density (w.r.t. Lebesgue). Although the proof is simple, the importance of Proposition 9 and Corollary 1 is fundamental for practice. If we can estimate the density, we can use it to improve the convergence of algorithms by numerical methods. See for example [23] where the entropic regularization allows a closed (and very simple) form of the density which then yields a dual-descent algorithm. Knowing explicitly the density allows us to find minimizers of Problem 2 via formulae and so we can focus our attention on estimating numerically these minimizers without carrying a second numerical error. Optimality (Euler-Lagrange) Most of the measure pre-conditioners defined on section 3.5 require the minimization of a functional. Let Ω ⊆ R p , in this section we study the first order conditions for minimization in (P 2 (Ω), d 2 ) which can be found in [11, Theorem 7.20] . Definition 11 (First variation of a functional in P(Ω)) Let F be a functional F : P 2 (Ω) → R, let ρ ∈ P 2 (Ω) be fixed and ϵ > 0, for any ρ ∈ P 2 ac ∩ L ∞ (Ω), define ν = ρ -ρ, we say that δF δρ (ρ) is the first variation of F evaluated at ρ if d dϵ ϵ=0 F (ρ + ϵν) = δF δρ (ρ)dν. Theorem 10 (Optimality criteria) For a functional F : P 2 (Ω) → R suppose that µ ∈ arg min ν∈P2(Ω) F (ν). Assume that for every ϵ > 0 and for every ρ absolutely continuous with L ∞ (M ) density F ((1 -ϵ)µ + ϵρ) < ∞ let c := essinf δF δρ (µ) . If δF δρ (µ) is continuous, δF δρ (µ)(x) ≥ c ∀x ∈ M, (24) δF δρ (µ)(x) = c ∀x ∈ supp(µ). ( 25 ) The proof can be found as Theorem 7.20 in [11] . Just as in the remark after Corollary 1, the main use of this tool is to focus the algorithmic implementation towards the computation of the first variation of the functional it minimizes. Convergence The objective of the reformulation of the general ML-problem in terms of Problem 2 and 1 is that we can adapt every stage of the learning process by using a measure estimation that fits the problem better. In order for us to know that we can recover the ML-problem in this process we need to know the types of convergence on which the sequences of measures formulated with the data converge to the underlying distribution. Many theorems and specific cases on density estimation have been studied, we recollect some of them here in terms of the definitions of section 4.1. Convergence of density estimations Observe that Theorem 2 and Proposition 6 allow different systems of convergence, i.e. depending on the 'strength' of the type of convergence m -→ of the probability measures, different requirements on C, d, L are needed. In this section we give a non-exhaustive list of modes of convergence for density estimation and the sequences in Section 4.1 that can be used as measure preconditioners. In this section one should notice that every type of convergence should be coupled with hypothesis that ensure the system is a full learner recovery system (Definition 2). Theorem 11 (Glivenko Cantelli in R) Let µ be any probability measure on R and µn be the standard empirical measure (Definition 3), if F (t) = µ((-∞, t]) and Fn(t) = µn((-∞, t]) then Fn → F uniformly on R as n → ∞ This theorem is well-known see for example [21, Theorem 7.4] or [32, Theormem 11.4.2.]. By account's of Donsker's theorem one can get the following improvement: Proposition 12 (Rate of convergence for continuous F ) If µ is a law on R for which F is continuous, the order of convergence of Theorem 11 satisfies n 1/2 sup t |Fn(t) -F (t)| ⇀ max 0≤s≤1 |Bs -sB 1 | ( 26 ) where {Bs} is a Brownian motion, i.e. the rate of convergence approaches the law of the absolute value of a Brownian bridge on [0, 1] and so it's law can be computed explicitly: P 0 sup 0≤s≤1 |Bs -sB 1 | < b = ∞ m=-∞ (-1) m e -2m 2 b 2 (27) See [21, Theorem 8 .10] and the following proposition for the explicit formula of it's law. Remark 12 The theorem presented here as Theorem 11 is just a specific version. In general, one refers to any theorem of this type as \"a Glivenko-Cantelli type theorem\" see for example [32] . Theorem 13 (Varadarajan) If π is any probability measure on X × Y and X × Y is a separable metric space then the standard empirical measures (Definition 3) for (X, Y ) converge weakly in probability to π. For a proof see [32, 11.4.1] . It is important to notice that the convergence is almost surely. In some cases, like the case of real numbers, the convergence can be upgraded. n n → ∞ and µ ≪ Leb, the empirical density estimate of Definition 5 converges uniformly in measure to µ, i.e. for every ϵ > 0, P ω : sup x∈R |fn(x, ω) -f (x)| < ϵ n→∞ ----→ 1. ( 28 ) For a proof see [17] . The following theorem is a specific case of the much more general convergence of Barycenters proved in [30] , in the paper the authors prove the d p -convergence in metric measure spaces satisfying a positive curvature condition. For a proof see [30] and apply it to the simple case where (R p , |•|, µ) is given as the initial measure space. In [20] the following proposition was shown: Convergence and full learner recovery systems In the previous section 4.4.1 we have listed several convergence results for different types of empirical measures. Empirical measures encompass our understanding of the sample. Theorems 11, 13 and 14, Propositions 12, 15 and 16 need to be coupled with regularity properties of L and the underlying class of functions C as in Proposition 6. This list shows that given an underlying model, it's intrinsic features will determine the type of measure preconditioners needed to ensure convergence on the specific convergence mode that the limiting measure admits. For example, Proposition 16 involves convergence in Total Variation norm from which one can infer that the measure pre-conditioning of Definition of 10 applies for a d-continuous (in the first coordinate) loss function L as in Proposition 6. 4. In contrast, Theorem 8 shows that the empirical (uniform) measure is not well-suited for every limiting distribution and so in the case of a continuous density, preconditioning by 10 is proved to have better results (theoretically) than the empirical measure. Estimating the marginal instead In the discussion of density estimation (Section 4.1) we haven't done any specific distinction on the particular form the data for Problems 2 and 1. Definitions 3-10 work for all kinds of data. In the particular case of the ML Problems 1 and 2, our objective is to model in the class C the dependence of Y on X penalized by the loss function L. We aim to study how good (with respect to L) a C-model f (X) approximates Y . In this context the distribution π refers to that of (X, Y ). Measure pre-conditioning amounts to approximating π using the sample in a way that benefits computations. We note that this gives rise to two different approaches: (a) We can estimate π directly via π n according to definitions 3 -10. (b) We can make assumptions on the conditional distribution of Y |X and then use definitions 3-10 for approximations on the X-marginal of π. Most of the study of this document has focused on approach (a). Let us give an example of the approach (b) to show it's interaction with measure preconditioning. Theorem 17 Assume that Y |X = x ∼ νx and that we have estimated νx via ν x n such that ν x n dp --→ νx uniformly on x, i.e. given ϵ there exists N > 0 such that for every n ≥ N dp(νx, ν x n ) < ϵ for every x assume also that µn dp --→ µ, and L : R p × R → R is continuous. Let f ∈ C and assume that there exists g ∈ L 1 (µ) such that L(f (x), y)dν x n (y) ≤ g(y). and that y → L(f (x), y)dν x n (y) is continuous and bounded, then L(f (x), y)dν x n (y)dµn(x) n→∞ ----→ Eπ[L(f (X), Y )]. Proof The proof is a direct consequence of dominated convergence applied twice, observe that L(f (x), y)dν x n dµn(x) -L(f (x), y)dπ = L(f (x), y)dν x n dµn(x) - L(f (x), y)dν x n dµ(x)+ L(f (x), y)dν x n dµ(x) -L(f (x), y)dπ. the first term goes to zero if we ensure y → L(f (x), y)dν x n (y) is continuous and bounded, the second term goes to zero by dominated convergence (using µ as reference measure). □ Remark 14 (On the general approach and the restrictiveness of the hypothesis on Theorem 17). Theorem 17 is only one example of the multiple approaches one can use to estimate π from µn and assumptions on Y |X, even though the hypothesis of Theorem 17 are very difficult to meet in practice, it is presented here to illustrate the general idea. Measure pre-conditioning on the marginals ν x allows the modeller to include the specific features of each data class. It is clear the many lines of investigations one can explore to get similar results (with less restrictive hypothesis), we choose not to develop any further and leave it for future research. The recipe: How to choose a measure and how to implement the algorithm The general approach for this document is to put in a single, standard, theoretical background many ideas that have come to light in ML-research. Namely, ML-reaserchers have realized that their algorithms improve in performance or convergence properties after a small \"tweak\" to either data or the loss function occurs. Stability of ML-algorithms has been widely known and is one of the main focus of ML-research. The idea of measure pre-conditioning is that the standard empirical distribution, though it may contain all the possible information in terms of inference (except for order) may not be well adapted to the specific problem one aims to minimize. It is well-known for example that if the functional to be minimized is convex, algorithms used for minimization can take advantage of convexity. This encourages the solver to find an empirical estimation from definitions 3-10 that makes their functional convex. Finding such a measure is what we call pre-conditioning, if the preconditioning satisfies any of the assumptions of Proposition 6 then one is ensured to have a full learner recovery system and hence have not lost anything on the process while achieving improved performance. One could instead use a pre-conditioner based on many reasons (such as having a specific algorithm to compute already at hand for example), this work explains how as soon as a condition like Proposition 6 is satisfied, one will end up with the same classifier/regressor. 5 The problem of Domain Adaptation and the impact of measure pre-conditioning Domain Adaptation (DA) is a sub-problem of transfer learning on which one aims to infer the parameters for a new learning agent in terms of an agent that learn in similar data. Many of the DA adaptation formulations are well-suited for Optimal Transport (OT), our framework of Problems 2 and 1 was motivated at first by the recent research in optimal transportation in Machine Learning (see [12] , [10] , [5] , [3] ) and so in this section we explore the implications of measure-preconditioning in the specific case of domain adaptation problems related to optimal transportation and the recent research in the area (see [5] , [3] and references therein for a more complete exposition of the use of optimal transportation in machine learning). min f ∈C n k=1 E[L(f (X s i ), Y s i )] ( 29 ) If f * s realizes the minimum in (29) , we say that it is the learnt agent or that f * s correspond to the learnt parameters. Now suppose we have another sample (X T 1 , X T 2 , . . . , X T n2 ) which we believe is similar in some features to the original sample. The domain adaptation problem is: How much can one learn from the previous learning? That is, how can we transfer the learning from the source domain to target domain?. The research field which attempts to answer Problem 3 is known as Domain adaptation for transfer learning. For a general introduction and approach see [5] , [27] and references therein. The problem of domain adaptation 3 is different to Problems 1, 2 as it aims to transfer the statistical knowledge obtained by a minimization on sourcedomain to a minimization on the target-domain. The formulation of Problem on (29) has the implicit assumption of the empirical distribution being imposed at level n. In this section we aim to explain how measure pre-conditioners as defined in section 4.1 can be used in the field of DA for transfer learning. Problem 4 (Domain Adaptation and transfer learning with varying losses and classes) Suppose that we have a sample (X s 1 , X s 2 , . . . , X s n ) of features together with the a sample of the dependent variable (Y s 1 , Y s 2 , . . . , Y s n ) and we use the learning agent to minimize a loss function Ls : R p × R → R among a class of functions C ∫ . The learning problem is to obtain the best possible parametric function f , among the class C explaining the data, i.e. min f ∈C ∫ Eπ n [Ls(f (X s ), Y s )] ( 30 ) and compare it with the perfect learner on target domain with class C t and loss function L t : R p × R → R: min f ∈Ct E π t [L t (f (X t ), Y t )] ( 31 ) If f * denotes the minimizing argument for (30), the Domain Adaptation problem is: How can we use f * to obtain good estimates for (31)? What is the structure of such agent? How does it compare to the actual minimizer of (31)? Suppose that every X s i ∼ µ s and X t i ∼ µ t , under \"similarity assumptions\" on µ s and µ t , one expects to be able to transfer learning to some accuracy. Of course \"similarity assumptions\" depends on the context of the ML-task in hand. For example, two measures might be considered similar in a classification problem that may not be considered similar in a generative model. In the same fashion, suppose that µ s and µ t satisfy that there exists a solution, T , for Problem (16) with a given cost function c : R p × R p → R. A good candidate for a new learnt agent can be immediately obtained via f * • T -1 . As seen in [3] , the error made by this agent relative to the total error obtained from training an agent from scratch can be controlled as soon as µ s and µ t are d 2 -close and C is rich enough. In the field of Domain Adpatation (DA) usually at least one of the following assumptions is made: Assumption 1 (Conditional structure of learning task) In the context of Problem 4, if (X s , Y s ) is the source variable and (X t , Y t ) the target variable, it is common to ask that (Y s i | X s i ) ∼ (Y t i | X t i ), ( 32 ) where Y | X denotes the random variable whose law is the regular conditional probability of Y given X. This assumption means that the probabilistic structure of the dependence of Y on X is the same in both domains. We understand this assumption as a strong hypothesis of similarity in the modellings. Assumption 2 (Identical dependence) In the context of Problem 4, if (X s , Y s ) is the source variable and (X t , Y t ) the target variable, it is common to ask that (Xs, Ys) ∼ (X t , Y t ) The identical dependence assumption has been used extensively but is in general not a good idea to pre-impose. The identical assumption implies that any sample of the source domain can be considered a sample of the target domain so if L s = L t and C s = C t then the learning transfer is perfect as we can identify the source data as target data in the empirical destimation of π s = π t The following assumption can be found in recent papers in DA-ML, see [5] for example. Assumption 3 (c-optimal map) There exists an optimal transport map (with respect to a cost function c : R p ×R p → R) Tc as in ( 16 ) that satisfies (X s i , Y s i ) ∼ (Tc(X s i ), Y t i ). Remark 15 Though it is straightforward to use Assumption 3 (postulated in [5] ) in the context of optimal transportation, it is of significant importance to understand the necessary conditions that yield this assumption. Remark 16 Note that these assumptions and the framework of DA is closely related to the line of investigation proposed in Remark 14 below. General Idea in the non-linear case Domain Adaptation should be used when the target and source measures are believed to be similar. If the source measure satisfies the assumptions of Brenier's Theorem (see [8, Theorem 2.32] ) and the loss function is quadratic (or strictly convex function of quadratic distance) the optimal transport map T transporting µ s onto µ t can be used as an learning agent on the target domain. We do this by first mapping onto the source domain using the optimal transport map and only then evaluating the agent that has learnt paramters, i.e. define f ad a candidate for the minimization of loss for learning agents in the target domain by f ad = f * (T -1 ). The work in [5] shows a convergence for this agent under Assumption 1. Main question: What cost should we impose? Note that Assumption 3 is an existence condition. If there exists a cost function for Assumption 3 one would need to check that it satisfies the conditions for existence and uniqueness of optimal transport maps like regularity and the twist condition (see [24] , [8] , [11] ). In the general approach for DA using transfer learning via optimal transport in the framework of Problem 4, two problems seem to arise more often in practice: P.i) When the conditions of the trainings are fixed and not to be chosen: study a learnt agent when L 1 , L 2 , C 1 , C 2 are given and fixed. P.ii) When we are able to choose L 1 , C 1 with the goal of maximizing (in any way) the transfer learning for a given loss function L 2 and class C 2 . A measure of transferrability In Problems 30 and 2, we start under a similarity assumption on the source measures. This follows an intuitive statement: in order to be able to transfer any learning, the original measures should share some features. We can't expect to transfer any learning if the problems have nothing in common. We may expect to transfer the learning (classifier) differentiating between dogs and cats to a new agent aiming to differentiate wolves and lions. In this case the distribution of dogs and cats is believed to be similar to that of wolves and lions. How much could we transfer? Could we guess beforehand how much learning we can transfer? As a thought experiment, let us study a way to measure the transfer of learning. There are many ways to measure transferability, see [3] , [5] or references therein. We propose another one, assume that π s and π t are as in Problem 3, let h : R → R be any strictly convex function with h(0) = 0, set d h (π s , π t ) = inf Π∈Π(π s ,π t ) h(L 1 (f 1 (x 1 ), y 1 )-L 2 (f 2 (x 2 ), y 2 ))dΠ((x 1 , y 1 ), (x 2 , y 2 )). (33) where f 1 is the solution for the C 1 , L 1 -source problem and f 2 the corresponding solution for the C 2 , L 2 -target problem. Evidently, a-priori the value of d h (π s , π t ) can not be computed as f 1 , f 2 are unknown and the value of (33) depends on the choice of models (C 1 , L 1 ) and (C 2 , L 2 ). We claim (33) is a reasonable way to measure transfer depending on C 1 , L 1 , C 2 , L 2 , in the sense that the closest d h is to 0 the more likely it is that a learnt agent for the L 1 problem with source data (X s , Y s ) would perform decently in the L 2 problem with data (X t , Y t ). This is to be expected as it may be reasonable to transfer the learnt agent for certain loss functions but not with all of them. Even though f 1 and f 2 are unknown, in some cases some estimates can be obtained. To the knowledge of the author no measure of transferrability of the form (33) has been studied which points to a promising line of investigation. Problem 1 Let us first address problem P.i) where all the conditions (C 1 , C 2 , L 1 , L 2 ) are fixed and we aim to measure the efficiency of a solution to (1) and (2). Measure pre-conditioning in the conditional average guess Let us consider here a different approach to the general Problem 4, suppose that we have solved the source problem i.e. f * ∈ arg min f ∈C1 E π s [L 1 (f (X), Y )]. (34) Similar to the ideas in [5] one can make assumptions like Assumption 3 in order to benefit from the source sample by using conditional distributions. Given y ∈ spt(Proj 2 #π s ) and f ∈ C 1 , assume we can find T f,y optimal transport map for the cost function c y (x, x) = |L 1 (f * (x), y) -L 2 (f * (x), y)| between the conditional distributions π s (x|Y = y) and π t (x|Y = y). The question is now how to generate an element in C 2 from the learnt information on the conditional distributions. The first immediate guess is to average with respect to the target distribution, that is if dπ t (x, y) = dπ t (x|Y = y)dν t (y) a guess for a learnt agent would be f ad = f * • (T f * ) -1 , where T f * (x) = Y T f,y (x)dν t (y). (35) In the general case, no estimates on the control of learning for agent (35) are known. It is expected that if the measures satisfy that d h from (33) is small then the agent obtained using (35) is good although so far no precise statements have been shown. Formula (35) is a reasonable guess because it takes into account the best agent at each y before averaging over all y ∈ Y . Open Question 1 In the context of Problem 4, is it true that if d h (π s , π t ) is small, then f * ad performs well in (31) when constructed using (35) and pre-conditioning? Is this performance quantifiable? Is it true that as n → ∞, Eπ n [L 2 (f * n (X), Y )] -min f ∈C2 [L 2 (f (X), Y )] → 0?. Can such performance be studied by d h of (33) when h(r) = |r| ?. How does (35) compare to f * • T 2 , where T 2 = Y (T f,y ) -1 (x)dν t (y)? (36) This questions are relevant both in the field of transfer learning and to measure pre-conditioning. The computation of T f,y may be difficult in practice and we expect measurepreconditioning for every y to benefit the performance of the intermediate algorithms without disruption on convergence. Numerical simulations are being performed to corrobate this idea and study the performance of (35) and will appear in subsequent works. Data-driven conditional OT On [31] the authors studied the following problem given a cost function c in the product space X × Z and a probability measure on X × Z: min T (•,z) ∀zT (•,z)#ρ(•|z)=µ(•|z) c(x, T (x, z))dρ(x, z) (37) which they denoted the data-driven optimal transport problem. In the same work, the authors showed that the minimization of (37) is equivalent to min T (•,z) max λ≥0 c(x, T (x, z))dρ(x, z) + λ Ent µ(•|z) 1 2 (T #ρ(, z) + µ(•, z)) (38) The dual formulation of (37) via (38) already hints a connection with our work. As the algorithm implemented in [31] is a sequential algorithm using gradient descent, it can be interpreted in the sense of measure pre-conditioners that entropically regularize at every discrete step n, just as Definition 8 in the framework of Wasserstein distance and problem 2. This means that an algorithm to compute data-driven conditional optimal transport can benefit directly from measure-preconditioning. Control on optimal transport domain adapted learning In this section we present different hypothesis and assumptions that yield stability results on transferred learning. The results are not as strong as those conjectured in section 5.4.1 but directly related to measure pre-conditioning. It is evident that there are many options on C 1 , C 2 , L 2 , L 2 that will ensure the transfer learning is efficient. In this section we reduce to present the most straight-forward formulations. Proposition 18 Let T be any map with T #µ = ν and dπ 1 (x, y) = dπ 2 (T (x), y) if C 1 • T = C 2 then arg min f ∈C1 Eπ s [L(f (X s ), Y s )] = arg min f ∈C2 Eπ t [L(f (X t ), Y t )] The proof is a direct consequence of the composition of classes C 1 • T = C 2 . Proposition 19 If C 1 = C 2 = C and L 1 = L 2 and if C is so that (x, y) → L 1 (f (x), y) is Lipschitz and bounded then for every f |Eπ 1 [L 1 (f (x), y)] -Eπ 2 [L 1 (f (x), y)]| ≤ d 1 (π 1 , π 2 ) and so the total loss of transfer learning when the learned agent is adapted is controlled by the d 1 -distance between joint measures. Proof The proposition follows directly from the Kantorovich-Rubinstein representation of the d 1 norm as d 1 is the suprema over Lipschitz functions. □ In [3] the authors proved the following theorem: See in [3, Theorem 3.1] . We now generalize this idea before we continue. Theorem 21 Let πs, π t be the joint measures for the the source (X s , Y s ) and (X t , Y t ) target domains respectively. Denote µs and µ t the projections into the Xcoordinates of πs, π t and by µ s x and µ t x the conditional distributions of Y s |X s and Y t |X t . Assume there exists a map T : R p → R p such that 1. T #µs = µ t 2. µ t T (x) = µ s x 3. C 2 = T • C 1 if f is the solution for (30) then f • T -1 is a perfect learner in the sense that it minimizers (31) . Proof The proof relies only on the disintegration of measures, as E π t [L(f (X t ), Y t )] = L(f (x), y)dµ t x (y) dµ t (x) = L(f (T (x), y)dµ s x dµs(x) where we have used the condition dµ t T (x) = dµ s (x) in the last equality. Minimization over C 2 and the condition C 2 = T • C 1 yields the result. □ Open Question 2 (Can learning error be totally controlled?) Assume f * minimizes the target problem, under what conditions on µ, ν, L, C 1 , C 2 does there exist C > 0 such that 1 n n2 i=1 E[L(f * (X t i ), Y t i ) -L(f ad (X t i ), Y t i )] ≤ Cd 2 (µs, µ t )? The previous theorems and the ideas of [5] respond this question in very restricted situations. Having a general context to answer this question similar to the one of 2 would be essential for the theory of domain adaptation. Numerical examples In this section we present 2 simple numeric examples using the mnist data set: 1. First we exemplify the convergence of Theorem 1 and Proposition 6 by considering convolutional neural networks applied to a gaussian-filter blurred version of the data set mnist. 2. In the second part of this section we apply the conditional average guess of section 5.4.1 to try to predict whether an image corresponds to a 6 or a 7 using the model trained only on differentiating 1s from 9s. The underlying hypothesis is \"that sixes are a lot like nines and ones are a lot like sevens\". We explain what this means and how to use the conditional average guess. Both experiments can be found publicly in the github repository joaxchon\\slash measure_precon with the goal of reproducibility. These examples should be understood as \"Toy Examples\" as numerical tests with much more detail and precision will be saved for a work in preparation with several co-authors. The idea of this section is to illustrate the main features of measure pre-conditioning and explain the results of the work. Convergence of agents under gaussian filter blurring We consider the mnist data set and use the keras and tensorflow packages to train a convolutional neural net under the modified images. We modify each image by first applying a gaussian filter with variance σ to blurr it. At each fixed level of σ we obtain a learnt agent f n,σ as in Problem 2 and show that both the losses and the accuracy converge to the agent f n,0 as in Proposition 6 considering the blurred image as corresponding to the measure µ X * N σ , where N σ denotes the unbiased normal distribution with variance σ. By proposition 6 and Theorem 1 we expect the learnt agents f n,σ to get close to f n,0 , although the precise formulation of the Theorem ensures the convergence as n → ∞ of lim σ→0 f n,σ to f 0 . Note that our theorem also ensures the converges of the weights (in appropriate sense). Proposition 6 ensures that if we apply a technique for unblurring (Weiner filters, Tychonov's regularization, etc) convergence of the learning agents is guaranteed. We see this result in the following figures: Figures 4 and 5 represent the total accuracy and total loss of the model during training. Different colors correspond to changes on the variance parameter of the gaussian filter. Given the convergence (in weak sense) of the convolutions with gaussians, proposition 6 indicates the convergence seen in the plots. Figures 6 and 7 show the behaviour of the agent as we pre-condition (by deblurring). The change in the training set improves the performance. Note that the conditions of Theorem 1 ensure that we will see similar behaviour as long as we ensure the method satisfies Definition 2. The conditional average guess and optimal domain adaptation for 6s and 7s In this section we use the mnist dataset to exemplify the technique of measure pre-conditioning for the conditional average guess of section 5.4.1. The idea is the following: 1. We train a convolutional neural network only on a data base formed by 1s and 9s. The objective is to classify whether a new imput is a 1 or a 9. We use sparse cross-entropy as loss function for this step of the learning. 2. We make the following assumption: The distribution of 1s is similar to the distribution of 7s and the distribution of 6s is similar to that of 9s. 3. We use the Python Library POT to approximate optimal transport between the distribution of 1s and 7s and that of 6s and 9s. We use c(x, y) = |x -y| as cost function. Independently of the loss function for the model, we note that the cost function c penalizes absolute distance without taking into account the shape, as pre-conditioning we flip every 6 to make it closer to a 9. 4. Finally we use formula (35) as new model to obtain a new agent. 5. We test this agent. Observe that (35) requires the knowledge of T (x) for every x in the support of the measure, nevertheless the computational package can only provide a matching between samples. In order to approximate (35) we approximate via f * =T -1 (Proj {X 1 i } (x)) • ||x -Proj {X 1 i } (x)|| ||Proj {X 1 i } (x) + Proj {X 9 i } (x)|| + T -1 (Proj {X 9 i } (x)) • ||x -Proj {X 9 i } (x)|| ||Proj {X 1 i } (x) + Proj {X 9 i } (x)|| where {X k i } n i=1 corresponds to the sample associated to y = k in the training set (the conditional sample). Results After the training and the computation of the learnt agent via (5.4.1), using the testing data and we obtain that 51.94% of 7s were correctly classified by the model and 99.584% of 6s were correctly labelled. This is due to the fact that the distribution of 6's and the distribution of flipped 9's is indeed very similar but the distribution of 1s and 7s have more differences. This is exactly what we expected as the Wasserstein distance between the distribution of 6's and flipped 9's is indeed very small, making the conditional guess of (5.4.1) efficient on identifying 60s with the knowledge of 9's and 1's.   We can see that the agent (5.4.1) mistakes 7s for 6's when the middle line on the 7 is big and hence increasing the Wasserstein distance to the distribution of 1's. In a following work in preparation, additional to the flipping method to relate 6s and 9s we will pre-condition the distribution of 1's by adding noise in a way to make it look more like 7's. Theorem 1 shows that the learnt agent converges to the original one as we reduce the noise to 0. The script can be found in joaxchon\\slash measure_precon\\conditional_average_adaptation.py Outside of the framework In this section we explain how the framework developed in this article can be extended to encompass more general situations (whose formulation is not exactly represented by ( 1 ) and ( 2 )) but benefit from the same ideas. Using pre-conditioners on WGANs The Wasserstein Generative Adversarial Networks (WGAN) introduced in [4] is a generalization of the generative adversarial networks (GAN) introduced in the seminal work [6] . The reason to consider the Wasserstein framework is due to the convergence properties of the Wasserstein metric together with the representation of Kantorovich-Rubinstein. The WGAN problem consists in computing arg min θ arg max w∈W E[f w (X)] -E[f w (g θ (Z))] (39) where X ∼ P 1 is prescribed, Z ∼ P 2 and {g θ } θ∈Θ is a parametric function space. Further work would study the same principles applied in this document to the more general version of the problem admitting (39) using maybe 2 parametric families C, C. The only difference between our problem and (39) is the presence of an extra outer minimization problem. It is clear that algorithms like TTC presented in [7] that take a dual approach can benefit from sequential measure-pre-conditionining. In the original formulation, as in [4] sup f ∈C f dµ -f dν + -λ (|∇f | -1) 2 dσ where Z ∼ σ iff Z = tX +(1-t)Y where t ∼ U [0, 1], note that we can replace µ and ν at level n via the empirical measures or measure pre-conditioners. This means that measure-preconditioning can be applied in more general circumstances than Problem 1 as the estimation of σ can be done via tX n + (1 -t)Y n where t ∼ U [0, 1] and the triangle inequality yields convergence. Covariate shift domain adaptation problem In general, the label-shift domain adaptation problem is usually written as min h,g 1 n n i=1 L(h(g(x s i )), y s i ) + λ Ent(µ g s |µ g t ) + Ω(h, g) (40) where h is the hypothesis, g is a representation mapping and Ω is a regularization term. The first term corresponds to losses in approximation while the second and the third correspond to regularizations. Compared to the framework used in Problems 30 and 31, ( 40 ) is a more general version. Nevertheless, the idea of measure pre-conditiniong can substitute the entropy term by using a sequence of entropic regularizations and Ω + L can be used as a modified loss function. The difference in algorithmic performance of both approaches is an interesting project. COOT and measure pre-conditioning In [10] , the following problem was introduced to handle at the same time the disparity between correlated distributions and the data marginals. In the case where X i ∈ R p , authors in [10] consider the matrix X = (X 1 , . . . , X n ) not only as a sample where the randomness comes form a single distribution but as a doubly-random matrix in the sense that each row is considered a sample and the columns are consider features, in this context let µ S denote the probability measure associated to samples and µ F the associated feature distribution one should perform optimal transport simultaneously in sampling and feature spaces. We expect the techniques of the two previous sections to also work in this context mutandis mutatis. 8 Researcher's criteria on measure pre-conditioning In section 4.5 we explained what a ML-developer should consider as recipe for applying measure pre-conditing. It explained that each modification of the n-level measure had different implications which should be pointed towards some (algorithmic) benefit. In general, it may be difficult to know a-priori exactly what to use and so this (and subsequent) work should be considered as a guideline. Trade-offs In low-dimensional regimes, absolutely continuous (w.r.t. Lebesgue) tend to behave better, while in higher dimensions highly concentrated measures tend to have better properties, see [18, Chapter 4] . This is already a hint on what to do, if the problem involved has few features, absolutely continuous measures may improve the performance of the algorithm. Conclusions and further work Recent work [10] has introduced new techniques for domain adaptation, the idea is to optimally match features and samples, it is still open lines of investigation how different measure pre-conditioning techniques would impact the co-optimal transport problem. The features and samples are in general of very different nature for which combining more than one of the techniques of section 3.5 could improve the performance of the algorithms. For example, it may be the case that features share a structure that can be exploited by a specific technique while the relation between samples may algorithmically benefit from another. Order of convergence Establishing that the ML problem gives a full learner recovery system is good in order to know convergence is ensured, in algorithmic practice we need more. We need to study the order of convergence and the imrpovements on this order by Measure pre-conditioners, this work is left for future work and other researchers. Data-driven model changes and convergence In the start of section 3.1 we asked question (iii): Given a choosing of π n 's, could we find sequences L n 's and C n so that the computations on the C nproblem with loss function L n associated to π n converge to 1? Could these problems improve the algorithmic performance? In section 3 we studied conditions on C and L to ensure Definition 2 and consequently Theorem 5. The question of how and when to change C n and L n at every step is still open and interesting. A good answer would yield heuristics to change the model given the data in terms of the parametric space, this means to not only change the way we measure the information from the data but also how we learn from it. This line of investigation is left for future work. 9.2 k-nearest neighbohrs and relation to meta-transport k-nearest neighbors and point-process notation The list of empirical estimating probabilities (section 4) is obviously nonexhausting. Algorithmic treatment of data such as k-nearest neighbors represent a potentially significant pre-conditioning method. The theory of this algorithms is usually developed through point-processes. The extension of this work to point-processes together with section 9.1.1 is a promising area for mathematical theory of learning. Meta-transport Another recent development in Optimal Transport based machine learning is the development of meta-optimal transport in [12] . The basic idea, similar to the basic idea of this document is to present a way to improve the performance of ML-algorithms through pre-working on them. The seminal work [12] develops completely algorithmic-focused techniques, as explained in section 8.1. This work is focus on the underlying structured of pre-condiitoning the samples, the statistics in Wasserstein space and how they impact the outputs of the algorithms. In some way, [12] tackles the pre-conditioning/pre-measure pre-conditioning in a different manner, with a clever approach based on numerical algorithms. We expect that a theory similar to the one developed in section 4 can also encapsulate the algorithmic pre-conditioning. This can be modelled via point-processes (as it's done for k-nearest neighbors). General disintegration estimates One can study different conditions on L, C, µ, π, Y |X such that a convergence similar to Theorem 17 occurs. This area is particularly technical as disintegration is not a continuous operation with respect to some metrics on spaces of probability measures. Generally, one does not necessarily need to estimate the disintegration but can explore different methods of convergence. An approach to full learner recovery systems (2) in the special case of assumptions on Y |X would be interesting and related with sections 5.4, 5.4.1 and literature as to [31] and references therein. 9.4 Problem 2 of section 5.2 If L can be chosen thinking ahead of the Target problem, choose the cost function by chosing an L 1 depending on L 2 or viceversa. The idea of the problem is to ensure learning can be transferred by picking the problems with the goal of transferring. A full theory with the approach of training with the goal of transferring would be interesting on it's own. Choosing the first Loss function to improve the second With the same approach as in Section 9.4, if we know that we aim to solve the target problem for L 2 , π t , C 2 , what loss function L 1 should we chose given π s and C 1 ? Similarly, allow C 1 to be chosen. We should chose L 1 in a way that data under π s behave similar to L 2 under π t . How one takes the target problem into consideration is an open question. Choosing the target loss model according to the source Assume we have solved Problem 1 with set of features L 1 , C 1 , π s and we know there is a distribution (unknown to us) on which we aim to transfer the knowledge, what loss function L 2 would ensure good properties of the learnt agent on target space? One can think of an L 2 loss function that penalizes the error of the learnt agent and simultaneously penalizes the difference between probabilities. This function would take into account that a mistake in the model is not relevant when one knows the error on difference of distributions is big. The L 2 loss function could be used to simultaneously control model error with (probability) transfer error. Problem 2 2 Let Ω ⊆ R n be convex and compact. Assume we have data X ∼ µ ∈ P(Ω) and we aim to do a Machine-learning model towards a dependent variable Y ∈ Y where (Y, d Y ) is a separable complete metric space, we denote by π ∈ P(Ω×Y) the joint distribution of (X, Y ). Given L : Y × Y → R (called a loss function), let C ⊆ Y Ω and assume d is a distance function on C, the C-optimal model for L under π is the following non-linear program arg min f ∈C Eπ [L(f (x), y)] . Definition 2 ( 2 Full learner recovery system) In the context of Problem 2, we say that (C, d, L, m -→) forms a full learner recovery system if it holds that 1. If πn m -→ π for all d-converging sequences sequences fn d Theorem 5 5 If (C, d, L, m -→) forms a full learner recovery system (Definition 2) where , d point-wise convergence and (x, y) → L(f (x), y) uniformly Lipschitz and uniformly bounded for f ∈ C (compact metric space). , L(x, y) is d-continuous on the first coordinate and uniformly bounded by some constant M > 0 on a compact metric space (C, d). Remark 13 13 Notice that from Theorem 11 one can infer the convergence of the Histogram (Definition 4) weakly in R p . Theorem 14 (Devroye) If H 2 Proposition 15 ( 15 Barycenters dp converge) If µ has compact support and µn is the a p-Wasserstein Barycenter of Definition 6, then µn dp --→ µ as n → ∞. Proposition 16 ( 16 Total variation)The mmd k minimizer of Definition 10, π mmd k n converges in total variation norm to the solution π * of unrestricted transport with respect to c (Definition 16), i.e.π mmd k n ||•|| T V ----→ π * .See [20, Theorem 1]. Problem 3 ( 3 General domain adaptation problem)Suppose that we have a sample (X s 1 , X s 2 , . . . , X s n ) of features together with the a sample of the dependent variable (Y s 1 , Y s 2 , . . . , Y s n ) and we use the learning agent to minimize a loss function L : R p × R → R among a class of functions C. The learning problem is to obtain the best possible parametric function f , among the class C explaining the data, i.e. Theorem 20 ( 1 n n k=1 δ x s i where x 1 2011 Courty-Flamary) If L(x, y) = |x -y| 2 and µ s = , x 2 , . . . , xn ∈ R n and there exist A positive definite matrix and a vector b such thatµ t = 1 n n k=1 δ Ax s i +b , set T (x) = Ax + b then f * • T -1is a perfect learning agent in the sense that it minimizes (31) . Fig. 1 5 Fig. 3 153 Fig. 1 Unblurred, σ = 0 Fig. 2 Blurred, σ = 0.5 Fig. 3 Blurred, σ = 1 Fig. 4 4 Fig. 4 Loss function during training Fig. 5 Accuracy of the model during training Fig. 6 6 Fig. 6 Change in loss during de-blurring Fig. 7 Change in accuracy durin de-blurring Fig. 8 8 Fig. 8 Example of a 6.Fig. 9 Flipped 6 looks like a 9. Fig. 9 9 Fig. 8 Example of a 6.Fig. 9 Flipped 6 looks like a 9. Fig. 10 10 Fig. 10 Missclassified 7. Fig. 11 Correctly classified 7."
}