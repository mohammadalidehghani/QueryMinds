{
  "title": "PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python",
  "abstract": "Machine learning is a general-purpose technology holding promises for many interdisciplinary research problems. However, significant barriers exist in crossing disciplinary boundaries when most machine learning tools are developed in different areas separately. We present Pykale -a Python library for knowledge-aware machine learning on graphs, images, texts, and videos to enable and accelerate interdisciplinary research. We formulate new green machine learning guidelines based on standard software engineering practices and propose a novel pipeline-based application programming interface (API). PyKale focuses on leveraging knowledge from multiple sources for accurate and interpretable prediction, thus supporting multimodal learning and transfer learning (particularly domain adaptation) with latest deep learning and dimensionality reduction models. We build PyKale on PyTorch and leverage the rich PyTorch ecosystem. Our pipeline-based API design enforces standardization and minimalism, embracing green machine learning concepts via reducing repetitions and redundancy, reusing existing resources, and recycling learning models across areas. We demonstrate its interdisciplinary nature via examples in bioinformatics, knowledge graph, image/video recognition, and medical imaging.",
  "introduction": "INTRODUCTION Machine learning is the cornerstone technology for artificial intelligence (AI), driving many advances in our everyday lives and industrial sectors. AI research becomes more and more interdisciplinary as many problems rely on expertise from various domains. We have also witnessed many machine learning models transverse different research areas and disciplines. For example, the success of convolutional neural networks (CNNs) [26] has spread from computer vision to graph analysis via graph convolutional networks (GCN) [23] and medical imaging via U-net [49] , and transformers [59] developed in natural language processing (NLP) have become a hot topic in solving vision tasks [6, 10, 60] . With rapid development and growing interests in machine learning, many researchers hope to solve real-world interdisciplinary problems using machine learning. However, even with the popularity of open-source software and high-level scripting language such as Python, navigating the abundant choices and variety of machine learning software is not trivial. Researchers often run into barriers when adapting a machine learning tool for a new task of their interest. Solving complex real-world problems in practice often involve analyzing multiple sources of data, e.g., multiple modalities, multiple domains, and multiple knowledge bases. Most machine learning software packages are developed with a specific domain of application in mind. While popular generic packages such as PyTorch and TensorFlow support multiple domains and are not tailored for a specific domain, their focus on generic frameworks makes them inadequate to directly support interdisciplinary research where both flexible configurations and high-level integration are important. In this paper, we propose PyKale, an open-source Python library to enable and accelerate interdisciplinary research via knowledgeaware multimodal learning and transfer learning on graphs, images, texts, and videos. It aims to fill the gaps between rich data sources, abundant machine learning libraries, and eager interdisciplinary researchers, with a focus on leveraging knowledge from multiple sources for accurate and interpretable prediction. To the best of our knowledge, this is the first publicly available Python library that considers both multimodal learning and transfer learning under a common framework of learning from multiple sources. It will make latest machine learning tools more accessible and accelerate the development of such tools. The name of the library consists of Py for Python, and Kale for Knowledge-aware learning. PyKale proposes a novel pipeline-based application programming interface (API) to enforce standardization and minimalism, as shown in Figure 1 . It advocates our newly formulated green machine learning concepts of reducing repetitions and redundancy (Fig. 2(a) ), reusing existing resources (Fig. 2(b )), and recycling learning models across areas (Fig. 2 (a) Reduce (b) Reuse (c) Recycle (d) The PyKale logo engineering practices, extending them, and tailoring the philosophies to machine learning. We include examples in bioinformatics, knowledge graph, image/video recognition, and medical imaging in PyKale. This library was motivated by needs in healthcare applications and thus considers healthcare as a primary domain of usage. PyKale is largely built on PyTorch and leverages many packages in the PyTorch ecosystem, foot_0 with the aim to become part of it. Our logo in Fig. 2 (d) reflects the above characteristics, using an icon of simplified kale leaves. Specifically, the main contributions are fourfold: ‚Ä¢ We introduce the PyKale library for knowledge-aware machine learning. It focuses on leveraging knowledge from multiple sources for accurate and interpretable prediction to enable and accelerate interdisciplinary research. ‚Ä¢ We propose a pipeline-based API for a standardized and minimal design to help break interdisciplinary barriers. We advocate green machine learning concepts of reduce, reuse, and recycle in such a design. ‚Ä¢ We demonstrate the usage of PyKale on real-world examples from multiple disciplines including bioinformatics, knowledge graph, image/video recognition, and medical imaging. ‚Ä¢ We provide many community-engaging features including a detailed documentation, a project board to show the progress and road map, and GitHub discussions open to all users. The rest of this paper is organized as follows. We first review the state of the art open source software packages that are related to PyKale in Section 2. Then we discuss the design principles and API structure of PyKale in Section 3. Next, we describe the usage of PyKale in Section 4 and show example use cases from different applications in Section 5. Finally, we show the openness of our package and discuss its limitations and future developments in Section 6, with conclusions drawn in Section 7. The PyKale library is publicly available at https://github.com/  pykale/pykale with accompanying data (mainly for testing at the moment) at https://github.com/pykale/data under an MIT license. PyKale can be installed from the Python Package Index (PyPI) via pip install pykale. PyKale documentation is hosted at https://pykale.readthedocs.io . The primary targeted users are researchers and practitioners who have experience in Python and PyTorch programming and need to apply or develop machine learning systems taking data from multiple sources for prediction tasks, particularly in interdisciplinary areas such as healthcare. This paper refers to release version 0.1.0rc2.",
  "body": "INTRODUCTION Machine learning is the cornerstone technology for artificial intelligence (AI), driving many advances in our everyday lives and industrial sectors. AI research becomes more and more interdisciplinary as many problems rely on expertise from various domains. We have also witnessed many machine learning models transverse different research areas and disciplines. For example, the success of convolutional neural networks (CNNs) [26] has spread from computer vision to graph analysis via graph convolutional networks (GCN) [23] and medical imaging via U-net [49] , and transformers [59] developed in natural language processing (NLP) have become a hot topic in solving vision tasks [6, 10, 60] . With rapid development and growing interests in machine learning, many researchers hope to solve real-world interdisciplinary problems using machine learning. However, even with the popularity of open-source software and high-level scripting language such as Python, navigating the abundant choices and variety of machine learning software is not trivial. Researchers often run into barriers when adapting a machine learning tool for a new task of their interest. Solving complex real-world problems in practice often involve analyzing multiple sources of data, e.g., multiple modalities, multiple domains, and multiple knowledge bases. Most machine learning software packages are developed with a specific domain of application in mind. While popular generic packages such as PyTorch and TensorFlow support multiple domains and are not tailored for a specific domain, their focus on generic frameworks makes them inadequate to directly support interdisciplinary research where both flexible configurations and high-level integration are important. In this paper, we propose PyKale, an open-source Python library to enable and accelerate interdisciplinary research via knowledgeaware multimodal learning and transfer learning on graphs, images, texts, and videos. It aims to fill the gaps between rich data sources, abundant machine learning libraries, and eager interdisciplinary researchers, with a focus on leveraging knowledge from multiple sources for accurate and interpretable prediction. To the best of our knowledge, this is the first publicly available Python library that considers both multimodal learning and transfer learning under a common framework of learning from multiple sources. It will make latest machine learning tools more accessible and accelerate the development of such tools. The name of the library consists of Py for Python, and Kale for Knowledge-aware learning. PyKale proposes a novel pipeline-based application programming interface (API) to enforce standardization and minimalism, as shown in Figure 1 . It advocates our newly formulated green machine learning concepts of reducing repetitions and redundancy (Fig. 2(a) ), reusing existing resources (Fig. 2(b )), and recycling learning models across areas (Fig. 2 (a) Reduce (b) Reuse (c) Recycle (d) The PyKale logo engineering practices, extending them, and tailoring the philosophies to machine learning. We include examples in bioinformatics, knowledge graph, image/video recognition, and medical imaging in PyKale. This library was motivated by needs in healthcare applications and thus considers healthcare as a primary domain of usage. PyKale is largely built on PyTorch and leverages many packages in the PyTorch ecosystem, foot_0 with the aim to become part of it. Our logo in Fig. 2 (d) reflects the above characteristics, using an icon of simplified kale leaves. Specifically, the main contributions are fourfold: ‚Ä¢ We introduce the PyKale library for knowledge-aware machine learning. It focuses on leveraging knowledge from multiple sources for accurate and interpretable prediction to enable and accelerate interdisciplinary research. ‚Ä¢ We propose a pipeline-based API for a standardized and minimal design to help break interdisciplinary barriers. We advocate green machine learning concepts of reduce, reuse, and recycle in such a design. ‚Ä¢ We demonstrate the usage of PyKale on real-world examples from multiple disciplines including bioinformatics, knowledge graph, image/video recognition, and medical imaging. ‚Ä¢ We provide many community-engaging features including a detailed documentation, a project board to show the progress and road map, and GitHub discussions open to all users. The rest of this paper is organized as follows. We first review the state of the art open source software packages that are related to PyKale in Section 2. Then we discuss the design principles and API structure of PyKale in Section 3. Next, we describe the usage of PyKale in Section 4 and show example use cases from different applications in Section 5. Finally, we show the openness of our package and discuss its limitations and future developments in Section 6, with conclusions drawn in Section 7. The PyKale library is publicly available at https://github.com/  pykale/pykale with accompanying data (mainly for testing at the moment) at https://github.com/pykale/data under an MIT license. PyKale can be installed from the Python Package Index (PyPI) via pip install pykale. PyKale documentation is hosted at https://pykale.readthedocs.io . The primary targeted users are researchers and practitioners who have experience in Python and PyTorch programming and need to apply or develop machine learning systems taking data from multiple sources for prediction tasks, particularly in interdisciplinary areas such as healthcare. This paper refers to release version 0.1.0rc2. RELATED WORK As an open source project, we have learned from numerous libraries in the public domain to build ours. Here, we can only briefly mention several that have been particularly influential or relevant. In particular, we focus on those PyTorch-based libraries that we have frequently studied in our development, regretfully omitting many libraries, such as those based on TensorFlow. PyTorch ecosystem PyTorch is a popular open source machine learning library, particularly for computer vision and NLP applications. The PyTorch ecosystem has a rich collection of tools and libraries for the development of advanced machine learning and AI systems. PyKale aims to fill the gap within the PyTorch ecosystem to support more interdisciplinary research based on multiple data sources. Therefore, we make extensive usage of existing libraries from the PyTorch ecosystem to reduce duplicated implementation. PyTorch Lightning. PyTorch Lightning is a popular deep learning framework providing a high-level interface for PyTorch. By removing boilerplate code, it simplifies the development of research code and improves the reproducibility, flexibility, and readability of the resulting models [11] . The goal of PyKale shares some similarity with PyTorch Lightning, but with a different focus on supporting interdisciplinary research. We have lots of inspirations from the design of PyTorch Lightning. 2.1.2 Other PyTorch libraries. PyKale depends on some other libraries from the PyTorch ecosystem including TensorLy for tensor analysis [24] , TorchVision for computer vision [37] , and PyTorch Geometric for graph analysis [13] . We also learned from MONAI for medical image analysis [36] , GPyTorch for Gaussian processes [16] , Kornia for computer vision [48] , and TorchIO for medical imaging preprocessing [45] . These libraries have focuses different from ours on interdisciplinary research and multiple data sources. The above libraries are listed as references at the end of our contributing guideline page. 2 Multimodal/transfer learning libraries To the best of our knowledge, no other libraries in the PyTorch ecosystem have the same pipeline-based API design as PyKale. Several PyTorch-based libraries on multimodal learning or transfer learning are summarized below. MultiModal Framework (MMF) . MMF [52] is the only library in the current PyTorch ecosystem focusing on multimodal learning of vision and language data in applications such as visual question answering, image captioning, visual dialog, hate detection and other vision and language tasks. PyKale differs from MMF not only in the API design, but also in the scientific fields covered and interdisciplinarity. PyKale aims to support interdisciplinary research such as medical imaging and drug discovery, and includes examples in these areas in the current release. 2.2.2 Transfer-Learning-Library. The Transfer-Learning-Library [21] is a library on transfer learning, providing domain adaptation and fine tuning algorithms for computer vision applications. PyKale differs not only in the API design but also in the multiple modalities of data supported, including also graphs and texts, as well as in interdisciplinarity. [50] is a library for multimodal recommender systems, leveraging auxiliary data (e.g., item descriptive text and image, social network, etc). PyKale differs from Cornac not only in the API design but also in the more diverse machine learning models and applications supported, not limited to recommender systems. [56] , with a full name \"(Yet) Another Domain Adaptation library\", is an excellent package built on PyTorch Lightning for unsupervised and semi-supervised domain adaptation. We refactored ADA and made many changes for adaption to our pipeline-based API. We include a docstring at the top of each module adapted from ADA to indicate the source at ADA. Beyond a new API design, PyKale extends ADA substantially to support video domain adaptation and also supports non-vision data for interdisciplinary research. Cornac. Cornac ADA. ADA There are some other smaller libraries on multimodal or transfer learning that are more narrowly focused than the above, such as Multimodal-Toolkit [17] . PyKale frames multimodal learning and transfer learning under one roof of knowledge-aware machine learning from multiple sources with a unified pipeline-based API, aiming to support interdisciplinary research rather than just popular vision or language tasks. PYKALE DESIGN 3.1 Green machine learning Green machine learning (and green AI) is a scarcely used term referring to energy-efficient computing [5, 15, 27] . Here, we propose a new green machine learning perspective for machine learning software development by formulating the 3R guiding principles below. We build these principles on standard software engineering practices by extending them and tailoring the philosophies to machine learning: ‚Ä¢ Reduce repetition and redundancy -Refactor code to standardize workflow and enforce styles, e.g., we refactored the deep drug-target binding affinity (DeepDTA) [64] into our PyKale pipeline (Fig. 1(c) ). -Identify and remove duplicated functionalities, e.g., construct data loading API for popular dataset to share among different projects. ‚Ä¢ Reuse existing resources -Reuse the same machine learning pipeline for different data and applications, such as using the same multilinear principal component analysis (MPCA) pipeline for gait [35] , brain [53] , and heart [55] . -Reuse existing libraries (e.g., those in the PyTorch ecosystem, such as PyTorch Geometric) for available functionalities rather than implementing them again. ‚Ä¢ Recycle learning models across areas -Identify commonalities between applications, e.g., the similarity between commercial recommender systems (predicting user-item interactions) and drug discovery (predicting drug-target interactions). -Recycle models for one application to another, e.g. from recommender systems [2] to drug discovery [61] . Although the above is largely based on standard software engineering practices, this new formulation offers a new perspective to focus on core principles of standardization and minimalism. It has guided us to design a unique pipeline-based API to unify workflow, break barriers between areas and applications, and cross boundaries to fuse existing ideas and nurture new ideas. Pipeline-based API Inspired by the convenience of machine learning pipelines in machine learning library like Spark MLlib [38] and scikit-learn [44] , we design PyKale with a pipeline-based API as shown in Fig. 1(a) . This design has six key steps and embodies our green machine learning principles above by organizing code along a standardized machine learning pipeline to identify commonalities, reduce redundancy, and minimize cognitive overhead. 1 # Load digits from multiple sources [digits_dann_lightn/main.py] 2 from kale.loaddata.digits_access import DigitDataset 3 from kale.loaddata.multi_domain import MultiDomainDatasets 4 5 source, target, _ = DigitDataset.get_source_target( 6 DigitDataset(\"MNIST\"), DigitDataset(\"USPS\"), data_path) 7 dataset = MultiDomainDatasets(source, target)) 8 9 # Preprocess digits [kale/loaddata/digits_access.py] import kale.prepdata.image_transform as image_transform self._transform = image_transform.get_transform(transform_kind) def get_train(self): return MNIST(data_path, train=True, transform=self._transform) # Embed digit representations [digits_dann_lightn/model.py] from kale.embed.image_cnn import SmallCNNFeature # Predict digit class and domain [digits_dann_lightn/model.py] from kale.predict.class_domain_nets import ClassNetSmallImage, DomainNetSmallImage # Build domain adaption pipeline [digits_dann_lightn/model.py] import kale.pipeline.domain_adapter as domain_adapter model = domain_adapter.create_dann_based(method=\"DANN\", dataset=dataset, feature_extractor= SmallCNNFeature(), task_classifier=ClassNetSmallImage(), critic=DomainNetSmallImage(), **train_params) # Utility functions [digits_dann_lightn/main.py] from kale.utils.csv_logger import setup_logger from kale.utils.seed import set_seed Code 1: Code snippets from the source code for the digits domain adaptation example at pykale/examples/digits_dann_lightn/main.py to demonstrate the unified pipeline-based API, simplified for inclusion here. In the following, we explain our unified API by starting with what the input and output are. We provide Python code snippets to help this explanation in Code 1, mainly using the domain adaptation for digit classification example with a pipeline in Fig. 1(b ). 3 Figure 1(c) shows another pipeline for drug discovery. More code snippets are in Section 5. 3.2.1 Load. The kale.loaddata module mainly takes source paths (local or online) as the input and constructs dataloaders for datasets as the output. Its primary function is to load data for input to the machine learning system/pipeline. See line 2-7 of Code 1 for an example of loading digit images from multiple sources and line 8-13 of Code 4 for an example of loading drug and targets data. 3.2.2 Preprocess. The kale.prepdata module takes the loaded raw input data as input and preprocesses (transforms) them into a suitable representation for the following machine learning modules. 3.2.3 Embed. The kale.embed module takes preprocessed, normalized data representations to learn new representations in a new space as the output. It includes dimensionality reduction algorithms (feature extraction and feature selection), such as MPCA [35] and CNNs. They can be viewed as encoders or embedding functions that learn suitable representations from data. This is a machine learning module. See line 17 of Code 1 for an example of (importing) a CNN feature extractor. 3.2.4 Predict. The kale.predict module takes the learned (or preprocessed, if skipping kale.embed ) representations to predict a desired target value as the output. Thus, this module provides prediction functions or decoders that learn a mapping from the input representation to a target prediction. This is also a machine learning module. See line 20-21 of Code 1 for an example of (importing) digit and domain classifiers. 3.2.5 Evaluate. The kale.evaluate module evaluates the prediction performance using some metrics. We reuse metrics from other libraries (e.g., sklearn.metrics in line 4 of Code 5) and only implement metrics not commonly available, such as the Concordance Index (CI) [1] for measuring the proportion of concordant pairs. See line 19 of Code 4 for its example usage. 3.2.6 Interpret. The kale.interpret module aims to provide functions for interpretation of the learned features, the model, or the prediction results/outputs, e.g., via further analysis or visualization, and we only implement functions not commonly available. This module has implemented functions for selecting and visualizing weights from a trained model. See line 16-20 of Code 5 for an example of visualizing weights of a linear model for interpretation. 3.2.7 Pipeline. The kale.pipeline module provides mature, offthe-shelf machine learning pipelines for \"plug-in usage\". Its submodules typically specify a machine learning workflow by combining several other modules. See line 24-31 of Code 1 for an example of calling a domain adaptation pipeline. Machine learning models Machine learning models in PyKale can be categorized into four main (possibly overlapping) groups. Multimodal learning. To support learning from data of multiple modalities, we need to first support learning from each individual modality. Thanks to the rich PyTorch ecosystem, we can build upon other libraries to have machine learning models supporting graphs, images, texts, and videos, primarily using PyTorch Dataloaders. The only missing major modality is audio but we have ongoing effort to include it in the near future, building upon torchaudio. Learning from heterogeneous data sources and data integration can be viewed as multimodal learning as well. To this end, PyKale has built a DeepDTA [42] pipeline kale.pipeline.deep_dti that learns from drug and target data, the chemical representation of which can be transformed into sequence or vector representations. PyKale also implemented our recent Graph information propagation Network (GripNet) [61] kale.embed.gripnet for link prediction and data integration on heterogeneous knowledge graphs. PyKale has an example drug_gripnet to show the model usage on public bioinformatics knowledge graph with drug and protein's information. Transfer learning. In transfer learning, PyKale currently focuses on domain adaptation [3, 43] . We largely inherited the excellent, modular architecture from ADA [56] , covering many important semi-supervised and unsupervised domain adaptation algorithms, such as domain-adversarial neural networks (DANN) [14] , conditional adversarial domain adaptation networks (CDAN) [33] , deep adaptation networks (DAN) [32] and joint adaptation networks (JAN) [34] , and Wasserstein distance guided representation learning (WDGRL) [51] . These algorithms are applicable to all modalities with appropriate representations. PyKale currently has two pipelines for domain adaptation: domain_adapter and video_domain_adapter in kale.pipeline. Deep learning. PyKale builds deep neural networks (DNNs) upon the PyTorch API. Current implementations include CNNs [26] / 3D CNNs [7, 57] , GCNs [23] , and attention-based networks such as transformers [59] and squeeze and excitation networks [18] (see more in Section 5). We use TorchVision [37] , PyTorch Geometric [13] , and PyTorch Lightning [11] in our implementation. Dimensionality reduction. PyKale has built a Python version of the MPCA algorithm [35] at kale.embed.mpca, as well as an MPCA-based pipeline at kale.pipeline.mpca_trainer, using both the scikit-Learn library [44] and the TensorLy library [24] . This pipeline has been successfully used for interpretable prediction in gait recognition from video sequences [35] , cardiovascular disease diagnosis [55] and prognosis [58] using cardiac magnetic resonance imaging (MRI), and brain state classification using functional MRI (fMRI). We are further building into PyKale other advanced tensorbased algorithms such as regularized Multilinear Regression and Selection (Remurs) [53] and sparse tubal-regularized multilinear regression (Sturm) [29] . Software engineering The PyKale team includes machine learning researchers and Research Software Engineers (RSEs). We have adopted good software engineering practices in a research context, often based on other libraries, particularly those in the PyTorch ecosystem. Version control and collaboration. We use git for version control and GitHub for collaborative working. The PyKale repository stipulates a license (MIT) and contributing guidelines. 4 These enable reuse of the software and sustainability of the project through 4 https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.md community contributions. This is a platform for long term availability of the resource and lays the foundation for community maintenance over an indefinite period. Documentation. We use \"docstrings\" to embed documentation within the source code to maximize synchronicity between code and documentation. Sphinx foot_3 is used to automatically build these (along with additional information in \"reStructuredText\" format) into html docs. Documentation is published via readthedocs. com and is kept up to date by Continuous Integration (CI). This is useful for keeping users and developers up-to-date with new features and bug fixes in a sustainable way. Detailed installation instructions are included. 6 3.4.3 Tests and continuous integration. We use the PyTest framework and currently have 88% test coverage. The test suite can be run locally, and also runs automatically on GitHub and must pass for code to be merged into the main branch. This ensures that new features do not create unintended side-effects. CI is implemented using GitHub workflows/actions. 7 Our CI checks include static analysis, pre-commit checks (e.g., maximum file size), documentation building (via Read the Docs), changelog update, project assignment for issues and pull requests, PyPI release of packages, PyTest tests on multiple platforms and multiple python versions, and Codecov code coverage report. This ensures that the version in the main branch is always the most up to date working version, and meets our standards of functionality and coding style. To maintain a small repository size (currently less than 1MB), we store test data in a separate repository at https://github.com/  pykale/data , which can be downloaded via download_file_by_url in kale.utils.download automatically. PYKALE USAGE Interdisciplinary research is a complex subject to support and care has to be taken to lower the barriers to entry. PyKale includes examples and tutorials to help users' exploration. Usage of pipeline-based API in examples PyKale examples are highly standardized. Each example typically has three essential modules (main.py, config.py, model.py), one optional directory (configs), and possibly other modules (trainer.py): ‚Ä¢ main.py is the main module to be run, showing the main workflow. ‚Ä¢ config.py is the configuration module that sets up the data, prediction problem, hyper-parameters, etc. The settings in this module are the default configuration. ‚Ä¢ configs is the directory to place customized configurations for individual runs. We use .yaml files (see Section 4.3) for this purpose. ‚Ä¢ model.py is the model module to define the machine learning model and configure its training parameters. ‚Ä¢ trainer.py is the trainer module to define the training and testing workflow. This module is only needed when NOT using PyTorch Lightning. Lu, et al. Building new modules or projects Users can build new modules or projects following the steps below. ‚Ä¢ Step 1 -Examples: Choose one of the examples of the users' interest (e.g., most relevant to the users' project) to browse through the configuration, main, and model modules, -download the data if needed, and run the example following instructions in the example's README. ‚Ä¢ Step 2a -New model: To develop new machine learning models under PyKale, -define the blocks in the users' pipeline to figure out what the methods are for data loading, preprocessing data, embedding (encoder/representation), prediction (decoder), evaluation, and interpretation, and modify existing pipelines with the users' customized blocks or build a new pipeline with pykale blocks and blocks from other libraries. ‚Ä¢ Step 2b -New applications: To develop new applications using PyKale, -clarify the input data and the prediction target to find matching functionalities in pykale (request if not found), and tailor data loading, preprocessing, and evaluation (and interpretation if needed) to the users' application. YAML configuration PyKale examples configure a machine learning system using YAML [4] . This is inspired by the usage of YAML in the GitHub package for the Isometric Network (ISONet) [47] , 8 with our adapted version illustrated in Code 2. As modern machine learning systems typically have many settings to configure, specifying many/all settings in command line or Python modules becomes difficult to manage and read. Using YAML greatly improves the readability and reproducibility, and makes configuration changes much easier, via a default configuration specified in config.py (top of Code 2) and customized configurations specified in a respective .yaml file (bottom of Code 2), which will be merged to overwrite the default setting at run time. Notebook tutorials with Binder and Colab We have eight real-world examples of PyKale usage. 9 However, tutorials without the need of any installation are important for new users to get familiar with the PyKale workflow and API. For these we must scale-back on real-world datasets due to the computational resources needed, as these lead to long runtimes, unsuitable for interactive learning. Therefore, we are simplifying our examples into Jupyter notebook tutorials so that each tutorial takes minutes instead of hours to run. This will strike a balance between computational requirements and runtime, without resorting to toy examples. 8 https://github.com/HaozhiQi/ISONet 9 https://github.com/pykale/pykale/tree/main/examples 1 # The file config.py that defines the default configuration 2 from yacs.config import CfgNode as CN 3 4 # Config definition 5 _C = CN() 6 7 # Dataset 8 _C.DATASET = CN() 9 _C.DATASET.ROOT = \"../data\" 10 _C.DATASET.NAME = \"CIFAR10\" 11 12 # Solver 13 _C.SOLVER = CN() 14 _C.SOLVER.SEED = 2020 15 _C.SOLVER.BASE_LR = 0.05 16 _C.SOLVER.TRAIN_BATCH_SIZE = 128 17 _C.SOLVER.MAX_EPOCHS = 100 18 19 # ISONet configs 20 _C.ISON = CN() 21 _C.ISON.DEPTH = 34 22 23 # Misc options 24 _C.OUTPUT_DIR = \"./outputs\" 1 # Customization in a .yaml file 2 SOLVER: 3 BASE_LR: 0.01 4 MAX_EPOCHS: 10 # For quick testing 5 ISON: 6 DEPTH: 38 To bring further convenience, we set up cloud-based services with both Binder foot_6 and Google Colaboratory (Colab) 11 for our notebook tutorials so that any users can run PyKale tutorials without the need of any installation. The first such tutorial has been released, 12 with screenshots in Figure 3 . More such tutorials are in development. USE CASES: PYKALE EXAMPLES PyKale currently has example applications from three areas below: ‚Ä¢ Image/video recognition: classification of images (objects, digits) or videos (actions in first-person videos); ‚Ä¢ Bioinformatics/graph analysis: prediction of links between entities in knowledge graphs (BindingDB, BioSNAP-Decagon); ‚Ä¢ Medical imaging: disease diagnosis from cardiac MRIs. The above examples deal with graphs, images, and videos. Our current APIs support text processing (e.g., for NLP tasks) but an example in this area is still in development. We are also conducting research in integrating audio features in action recognition so an (a) Binder (b) Google Colab example involving audio data is a future task as well. Examples in computer vision applications such as image and video recognition are a good start for most users due to the popularity of vision applications and a low barrier to entry (e.g., no need for specific domain knowledge as in drug discovery). Models first developed in computer vision can be reused or recycled for other applications. The data used in PyKale examples are real-world data frequently used in research papers. Subsequently, it may take quite some time to finish running these examples. For quick running and demonstration of the workflow, tutorials (Section 4.4) are simplified examples that serve as a better starting point. The following subsections give an overview of the data and algorithms used in the example machine learning systems in PyKale. CIFAR and digits classification Small-image datasets are good for building examples of real-world relevance. PyKale has three such examples: two on the CIFAR datasets [25] and one on digits datasets including MNIST [9, 28] , modified MNIST [14] , USPS [20] , and SVHN [40] . cifar_isonet is the first example in PyKale refactoring the ISONet code https:  //github.com/HaozhiQi/ISONet on CIFAR10 and CIFAR100 into the PyKale API. cifar_cnntransformer is another example on CIFAR showing the simple application of a mixed CNN and transformer model for vision tasks. Code 1 has shown code snippets of digits classification via domain adaptation using MNIST as the source domain and USPS as the target domain, in the digits_dann_lightn example. Examples on these popular datasets could help users familiar with them make easy connections between the PyKale API and what they are already familiar with. Action recognition via domain adaptation 5.2.1 Data. For this action recognition example, we constructed three first-person vision datasets, ADL ùë†ùëöùëéùëôùëô , GTEA-KITCHEN and EPIC ùëêùë£ùëùùëü 20 , with 222/454/10094 action videos respectively. We selected and reorganized three videos from ADL dataset [46] as three domains of ADL ùë†ùëöùëéùëôùëô , re-annotated GTEA [12, 30] and KITCHEN [8] datasets to build GTEA-KITCHEN, and adopted the public dataset EPIC ùëêùë£ùëùùëü 20 [39] . We will provide instructions on how to construct these datasets from the source at https://github.com/  pykale/data/tree/main/video_data/video_test_data (in progress). Each dataset has two modalities: RGB and optical flow. Algorithms. For video feature extraction, we built two stateof-the-art action recognition algorithms, I3D [7] and 3D ResNet [57] , into PyKale. For domain adaptation, we extended the domain adaptation framework for images (digits, adapted from [56] ) to videos. We followed the same pipeline as digits classification while providing additional specific functions for action videos and multimodal data. As shown in Code 3, the image modality parameter can be set to choose the proper data transform and loader for different modalities: RGB, optical flow, and joint, and all video feature extractors are accessed via a unified interface. Drug-target interaction prediction 5.3.1 Data. Predicting the binding affinity between drug compounds and target proteins is fundamental for drug discovery and drug repurposing. This example uses three public datasets (for three metrics Ki, Kd and IC50) from BindingDB [31] containing 52,284, 375,032, and 991,486 interaction pairs respectively, accessed via the Therapeutics Data Commons (TDC) platform [19] . Given the amino acid sequences of targets and SMILES (Simplified Molecular Input Line Entry System) strings of drug compounds, the task is to predict drug-target binding affinity. Following [22] , the affinity metrics are transformed into the logarithm form for more stable training and validation. behind PyKale. This paper is another effort to reach out to the wider research community to share this resource and get feedback for further improvements. Limitations and future development PyKale is an open-source project started in June 2020, with the first PyPI release in January 2021. It was motivated by the growing needs for machine learning systems that can deal with multiple sources of data, particularly in interdisciplinary areas such as healthcare. For example, clinicians often need to make use of a combination of medical images (e.g., X-rays, CTs, MRIs), biological data (gene, DNA, RNA), and electronic health record for decision making. Our focus on multimodal learning and transfer learning has defined a challenging scope, while holding the promises to break barriers in interdisciplinary research. To date, PyKale has built APIs supporting machine learning from graphs, images, texts, and videos, with four mature pipelines implemented. Nevertheless, we do not have an example on text data yet, and we have not built APIs for audio yet. Developing projects involving multiple data sources takes considerably longer time than developing those involving a single data source. The current version of PyKale has two examples on multimodal learning involving heterogeneous drug and target data and two examples on domain adaptation (transfer learning) for images and videos. These examples laid solid foundations for us to grow our research in these areas and build more advanced examples in future development. In addition, our tests currently have a coverage of 88%. We need further improvements for a higher coverage and more rigorous tests. CONCLUSIONS In this paper, we have introduced PyKale, a Python library for knowledge-aware machine learning from multiple sources, particularly from multiple modalities for multimodal learning and from multiple domains for transfer learning. This library was motivated by needs in healthcare applications (hence the acronym kale, a healthy vegetable) and aims to enable and accelerate interdisciplinary research. Building on standard software engineering practices, we proposed a new green machine learning perspective to advocate reducing repetitions and redundancy, reusing existing resources, and recycling learning models across areas. Following such principles, we designed our API to be pipeline-based to unify the workflow and increase the flexibility. This design can help to break barriers between different areas or applications and facilitate the fusion and nurture of ideas across discipline boundaries. The goal of PyKale is to facilitate interdisciplinary, knowledgeaware machine learning research for graphs, images, texts, and videos. It will make it easier to bring machine learning models developed in one area to the other, and integrate data from multiple sources for prediction tasks in interdisciplinary areas. Its focus on leveraging knowledge from multiple sources also helps accurate and interpretable prediction. To demonstrate such potential, we have shown example applications including bioinformatics, knowledge graph, image/video recognition, and medical imaging on real-world datasets. (c)) by building on standard software Drug target interaction prediction pipeline Figure 1 : 1 Figure 1: The proposed pipeline-based API in PyKale and two real-world examples. Figure 2 : 2 Figure 2: Green machine learning concepts in PyKale. 3. 2 . 8 28 Utilities. The kale.utils module provides common utility functions, such as setting random seeds, logging results, or downloading data. See line 34-35 of Code 1 for examples of importing the seed-setting and csv-logging submodules. Code 2 : 2 Code snippets to demonstrate the usage of YAML to configure machine learning systems in PyKale, from pykale/examples/cifar_isonet/config.py, which is adapted from https://github.com/HaozhiQi/ISONet. Figure 3 : 3 Figure 3: PyKale digits domain adaptation example on cloudbased services. 1 # 4 5 13 # 1413 Transform and dataset for multi-modal video data 2 from kale.prepdata.video_transform import get_transform 3 from kale.loaddata.video_datasets import BasicVideoDataset transform = get_transform(transform_kind, image_modality) 6 dataset = BasicVideoDataset(data_path, train_list, 7 imagefile_template=\"frame_{:010d}.jpg\" 8 if image_modality in [\"rgb\"] 9 else \"flow_{}_{:010d}.jpg\", Action video feature extractor 14 from kale.embed.video_feature_extractor import 15 get_video_feat_extractor 16 feature_extractor = get_video_feat_extractor(\"I3D\", image_modality, 17 attention, num_classes) Code 3: Code snippets to demonstrate the example on action recognition via domain adaptation. https://pytorch.org/ecosystem/ https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.md https://github.com/pykale/pykale/tree/main/examples/digits_dann_lightn Preprocessing steps include data normalization, augmentation, and other transformations of data representation not involving machine learning. Its submodules are typically imported in kale.loaddata. See line 10-14 of Code 1 for an example of standardizing digit images with predefined transforms. https://www.sphinx-doc.org/ https://pykale.readthedocs.io/en/latest/installation.html https://github.com/pykale/pykale/tree/main/.github/workflows https://mybinder.org/ https://colab.research.google.com/ https://github.com/pykale/pykale/blob/main/examples/digits_dann_lightn/tutorial. ipynb"
}