{
  "title": "Compressive Classification (Machine Learning without learning)",
  "abstract": "Compressive learning is a framework where (so far unsupervised) learning tasks use not the entire dataset but a compressed summary (sketch) of it. We propose a compressive learning classification method, and a novel sketch function for images.",
  "introduction": "Introduction and background Machine Learning (ML)-inferring models from datasets of numerous learning examples-recently showed unparalleled success on a wide variety of problems. However, modern massive datasets necessitate a long training time and large memory storage. The recent Compressive Learning (CL) framework alleviates those drawbacks by computing a compressed summary of the dataset-its sketch-prior to any learning [1] . The sketch is easily computed in a single parallelizable pass, and its required size (to capture enough information for successful learning) does not grow with the number of examples: CLs time and memory requirements are thus unaffected by the dataset size. So far, CL focused on unsupervised ML tasks, where learning examples don't belong to a (known) class [1, 2, 3] . We show that CL easily extends to supervised ML tasks by proposing (Sec. 2) and experimentally validating (Sec. 3) a first simple compressive classification method using only a sketch of the labeled dataset (Fig. 1 ). We also introduce a sketch feature function leveraging a random convolutional neural network to better capture information in images. While not as accurate as ML methods learning from the full dataset, this compressive classification scheme still attains remarkable accuracy considering its unlearned nature. Our method also enjoys from a nice geometric interpretation, i.e., Maximum A Posteriori classification performed in the Reproducible Kernel Hilbert Space associated with the sketch. (Unsupervised) Compressive Learning: Unsupervised ML usually amount to estimate parameters of a distribution P, from a dataset X : = {x i ∼ iid P} N i=1 ⊂ R n of examples- associated to an empirical distribution PX := 1 N xi∈X δ xi , with δ u the Dirac measure at u. While most unsupervised ML algorithms require (often multiple times) access to the entire dataset X , CL algorithms require only access to the sketch: a single vector z X ∈ C m summarizing X . This dataset sketch z X actually serves as a proxy for the true distribution sketch A(P), i.e., a linear embedding of the \"infinite-dimensional\" probability distribution P into C m , a space of lower dimension: A(P) := E x∼P f (x) z X := A( PX ) = 1 N xi∈X f (x i ), (1) where f is a random nonlinear feature map to C m . This map defines a positive definite kernel κ(u, v) := E f (u), f (v) , and κ in turn provides a Reproducible Kernel Hilbert Space (RKHS) H κ to embed distributions; A indirectly maps P to its Mean Map κ(•, P) := E x∼P κ(•, x) ∈ H κ [4, 5, 6]. Existing * E-mail: {vincent.schellekens, laurent.jacques}@uclouvain.be. ISPGroup, ELEN/ICTEAM, UCLouvain (UCL), B1348 Louvain-la-Neuve, Belgium. VS and LJ are funded by Belgian National Science Foundation (F.R.S.-FNRS). R n f (•) . . . . . . x i . . . f f R n y 0 =? x 0 . . . f (•) argmax Classification phase Observation phase memory methods [2, 3] use Random Fourier Features [7] as map f : z x 0 h•, z X1 i h•, z XK i z X K z X 1 average 1 N1 X xi2X1 f (xi) average 1 NK X xi2XK f (xi) k ⇤ ' arg max k pk (x 0 , Pk) z x i p1 pK f RFF (x) = exp(i ω T j x) m j=1 with ω j ∼ iid Λ, (2) and κ is then shift-invariant and the Fourier transform of the distribution Λ: κ(x, x ) = (xx ) := (FΛ)(xx ) [8] . CL is promising because the sketch z X retains sufficient information (to compete with traditional ML) whenever its size m exceeds some value independent on the number of examples N , yielding algorithms that scale well when N increases. Random Convolutional Neural Networks (CNN): Shiftinvariant kernels are not that relevant when dealing with images (they are sensitive to image translations for example). Recent studies have shown that the last layer of a randomly weighted (convolutional) neural network CNN (combining convolutions with random weights, nonlinear activations, and pooling operations) captures surprisingly meaningful image features [9, 10, 11, 12] . We thus propose the feature map f CNN (x) = CNN(x) ∈ R as sketch map f for images: the associated kernel κ is (for a fully connected network) an arccosine kernel, that surpasses shift-invariant kernels for solving image classification tasks with kernel methods [12] .",
  "body": "Introduction and background Machine Learning (ML)-inferring models from datasets of numerous learning examples-recently showed unparalleled success on a wide variety of problems. However, modern massive datasets necessitate a long training time and large memory storage. The recent Compressive Learning (CL) framework alleviates those drawbacks by computing a compressed summary of the dataset-its sketch-prior to any learning [1] . The sketch is easily computed in a single parallelizable pass, and its required size (to capture enough information for successful learning) does not grow with the number of examples: CLs time and memory requirements are thus unaffected by the dataset size. So far, CL focused on unsupervised ML tasks, where learning examples don't belong to a (known) class [1, 2, 3] . We show that CL easily extends to supervised ML tasks by proposing (Sec. 2) and experimentally validating (Sec. 3) a first simple compressive classification method using only a sketch of the labeled dataset (Fig. 1 ). We also introduce a sketch feature function leveraging a random convolutional neural network to better capture information in images. While not as accurate as ML methods learning from the full dataset, this compressive classification scheme still attains remarkable accuracy considering its unlearned nature. Our method also enjoys from a nice geometric interpretation, i.e., Maximum A Posteriori classification performed in the Reproducible Kernel Hilbert Space associated with the sketch. (Unsupervised) Compressive Learning: Unsupervised ML usually amount to estimate parameters of a distribution P, from a dataset X : = {x i ∼ iid P} N i=1 ⊂ R n of examples- associated to an empirical distribution PX := 1 N xi∈X δ xi , with δ u the Dirac measure at u. While most unsupervised ML algorithms require (often multiple times) access to the entire dataset X , CL algorithms require only access to the sketch: a single vector z X ∈ C m summarizing X . This dataset sketch z X actually serves as a proxy for the true distribution sketch A(P), i.e., a linear embedding of the \"infinite-dimensional\" probability distribution P into C m , a space of lower dimension: A(P) := E x∼P f (x) z X := A( PX ) = 1 N xi∈X f (x i ), (1) where f is a random nonlinear feature map to C m . This map defines a positive definite kernel κ(u, v) := E f (u), f (v) , and κ in turn provides a Reproducible Kernel Hilbert Space (RKHS) H κ to embed distributions; A indirectly maps P to its Mean Map κ(•, P) := E x∼P κ(•, x) ∈ H κ [4, 5, 6]. Existing * E-mail: {vincent.schellekens, laurent.jacques}@uclouvain.be. ISPGroup, ELEN/ICTEAM, UCLouvain (UCL), B1348 Louvain-la-Neuve, Belgium. VS and LJ are funded by Belgian National Science Foundation (F.R.S.-FNRS). R n f (•) . . . . . . x i . . . f f R n y 0 =? x 0 . . . f (•) argmax Classification phase Observation phase memory methods [2, 3] use Random Fourier Features [7] as map f : z x 0 h•, z X1 i h•, z XK i z X K z X 1 average 1 N1 X xi2X1 f (xi) average 1 NK X xi2XK f (xi) k ⇤ ' arg max k pk (x 0 , Pk) z x i p1 pK f RFF (x) = exp(i ω T j x) m j=1 with ω j ∼ iid Λ, (2) and κ is then shift-invariant and the Fourier transform of the distribution Λ: κ(x, x ) = (xx ) := (FΛ)(xx ) [8] . CL is promising because the sketch z X retains sufficient information (to compete with traditional ML) whenever its size m exceeds some value independent on the number of examples N , yielding algorithms that scale well when N increases. Random Convolutional Neural Networks (CNN): Shiftinvariant kernels are not that relevant when dealing with images (they are sensitive to image translations for example). Recent studies have shown that the last layer of a randomly weighted (convolutional) neural network CNN (combining convolutions with random weights, nonlinear activations, and pooling operations) captures surprisingly meaningful image features [9, 10, 11, 12] . We thus propose the feature map f CNN (x) = CNN(x) ∈ R as sketch map f for images: the associated kernel κ is (for a fully connected network) an arccosine kernel, that surpasses shift-invariant kernels for solving image classification tasks with kernel methods [12] . Compressive learning classification Observation phase: Supervised ML infers a mathematical model from a labeled dataset X := {(x i , y i )} N i=1 where each signal x i ∈ R n belongs to a class C k as designated by its class label y i ∈ [K]. Denoting p k := P(x ∈ C k ) = P(y = k), the signals are assumed drawn from an unknown density P: x i ∼ iid P = K k=1 p k p(x| x ∈ C k ) =: k p k P k (x). (3) As illustrated in Fig. 1 (top), our supervised compressive learning framework considers that X is not explicitly available but compressed as a collection of K class sketches z X k defined as: We can also require approximated a priori class probabilities pk , e.g., pk = N k N if we count the class occurrences N k = |X k |, or setting an uniform prior pk = 1 K otherwise. Classification phase: Under (3), the optimal classifier (minimal error probability) for a test example x is the Maximum A Posteriori (MAP) estimator k MAP := arg max k p k P k (x ), where P k is generally hard to estimate. In our CL framework, we classify x from z X k and pk only (Fig. 1 , bottom): we acquire its sketch z x = f (x ) and maximize the correlation with the class sketch weighted by pk , i.e., we assign to x the label z X k = A( PX k ) where X k := {x i ∈ C k }. ( 4 k * := arg max k pk z x , z X k (CC) Note that this Compressive Classifier (CC) does not require parameter tuning. Interestingly, under a few approximations, this procedure can be seen as a MAP estimator in the RKHS H κ . Indeed, we first note that if m is large, the law of large numbers (LLN) provides the kernel approximation (KA) f (u), f (v) κ(u, v), ∀u, v ∈ R n . (KA) Assuming N k is also large, another use of the LLN gives the mean map approximation (MMA): we have both pk p k and z u , z X k = 1 N k xi∈X k f (u), f (x i ) (KA) 1 N k xi∈X k κ(u, x i ) E x∼P k κ(u, x) =: κ(u, P k ) ∀u ∈ R n . (MMA) Consequently, under the KA and MMA approximations, k * arg max k p k κ(x , P k ), (5) or in other words, we replace P k in the MAP estimator by its Mean Map κ(•, P k )-its embedding in H κ -such that CC computes a MAP estimation inside the RKHS H κ . In all generality κ(•, P k ) is not a probability density function, but can be interpreted as a smoothing of P k by convolution with (u) := κ(u, 0) if κ is a properly scaled shift-invariant kernel. Alternatively, (5) can be seen as a Parzen-windows classifier-a nonparametric Support Vector Machine (without weights learning)-evaluated compressively thanks to the sketch [13, 14] . Experimental proof of concept Synthetic datasets: We build two datasets that are not linearly separable (Fig. 2 left ), and sketch them using f = f RFF with Λ ∼ N (0, In σ -2 ): therefore κ(u, v) ∝ exp(-u-v 2 2σ 2 ). As shown Fig. 2 (right), the test accuracy of CC improves with m until reaching-when the KA is good enough-a constant floor depending on the compatibility between κ and P. Accuracy is almost optimal when κ is close to the constituents of P (e.g., 1 st dataset, σ = 0.1), but degrades when the kernel scale and/or shape mismatches the data clusters (e.g., 1 st dataset, σ = 10; or 2 nd dataset). CC thus reaches good accuracy provided m is large enough and κ is well adapted to the task. Standard datasets: We also test CC on some well-known \"real-life\" datasets from the UCI ML Repository [15] . Table 1 compares the error rates of CC and SVM, a fully learned approach. Although worse than SVM, CC is surprisingly accurate considering its compressive nature, low computational cost (especially when m = 50), and that κ is a basic, non-tuned kernel. Image classification: More challenging are image classification datasets: handwritten digit recognition (MNIST [16] ) and vehicle/animal recognition (CIFAR-10 [17] ). We use f = f CNN (the default architecture provided by [18] ) because it yielded  Table 2: Image datasets: train (white) and test (gray) average error rates ± standard deviation (in %, 10 repetitions), for SVM and CC with m ∈ {250, 5000}. Discussion and conclusion We proposed a very simple and flexible compressive classification method, relying only on class sketches: accumulated random nonlinear signatures f (•) of the learning examples. This classifier is cheap to evaluate (e.g., in low-power hardware, following ideas from [19] ), involves no parameter tuning, and has an interesting interpretation: a MAP estimator inside the RKHS H κ associated with the kernel κ defined by f . Preliminary experimental results, relying on a basic Gaussian κ, are an encouraging proof of concept, but indicate room for improvement if the mapping f (and associated kernel κ) are optimized according to the true data distribution; for example, image classification accuracy improves when f is a random CNN (defining a shift-variant κ). Intuitively, κ should be such that the Mean Maps κ(•, P k ) ∈ H κ of different classes k are \"well separated\" (ideally as much separated as the initial, unknown densities P k ). This could be done by adding some a priori assumptions on the densities P k , or by first getting a rough estimation of them through a form of distilled sensing [20] . To be reliable, compressive classification also requires precise, nonasymptotic guarantees, e.g., using results from [5] and [7] . Figure 1 : 1 Figure 1: Observation phase: we record only a summary of the dataset X as the K class sketches z X k : the class average of non-linear maps zx i f (x i ) of the examples x . Classification phase: a sample x gets the class label k * that maximizes the correlation between its sketch z x and the stored class sketches; this can be interpreted as a MAP classifier in a RKHS Hκ. ) arXiv:1812.01410v1 [cs.LG] 4 Dec 2018 Figure 2 : 2 Figure 2: Left: synthetic 2-d datasets of N = 10 4 examples from K = 3 equiprobable classes, separated into 2/3 for \"training\" (observation phase) and 1/3 for testing (classification phase). Right: testing accuracy (average over 10 trials) of our compressive classification method for different values of σ (noted var) and increasing m (solid), compared to MAP classification (dashed). Table 1 : 1 4 examples from K = 3 equiprobable classes, separated into 2/3 for \"training\" (observation phase) and 1/3 for testing (classification phase). Right: testing accuracy (average over 10 trials) of our compressive classification method for different values of σ (noted var) and increasing m (solid), compared to MAP classification (dashed).Standard datasets: train set (white, 2/3 of data) and test set (gray) average error rates ± standard deviation (in %, 100 repetitions), for SVM and CC with m ∈ {50, 1000}, and with σ = 2 (data re-scaled inside [-1, +1] n ).better accuracy than f RFF , and compare CC to the same CNN architecture with a classification layer, with all weights learned in one pass over X for fairness. Again CC is outperformed by the learned approach, but still achieves reasonable, non-trivial accuracy. Surprisingly, CC performs here better on the test set than on the training set. N n K SVM m = 50 m = 1000 Iris 150 4 3 2.00 4.00 6.51 ± 1.81 8.22 ± 3.25 5.51 ± 1.23 6.18 ± 2.40 Wine 178 13 3 0.84 1.69 4.56 ± 2.34 13.75 ± 4.09 2.43 ± 0.72 8.19 ± 1.29 Breast cancer 569 30 2 3.67 2.13 7.00 ± 1.40 9.22 ± 2.33 3.93 ± 0.39 6.23 ± 0.69 Adult (3 attr.) 30718 3 2 21.03 21.06 23.88 ± 4.37 36.09 ± 6.67 23.11 ± 1.05 35.04 ± 1.63 N n CNN m = 250 m = 5000 MNIST 60000 10000 28 × 28 × 1 1.60 ± 0.12 1.63 ± 0.11 17.73 ± 1.43 16.83 ± 1.39 16.60 ± 1.54 15.80 ± 1.61 CIFAR10 50000 10000 32 × 32 × 3 39.08 ± 1.48 40.28 ± 1.36 71.76 ± 1.85 71.12 ± 1.72 72.83 ± 2.00 72.02 ± 1.85"
}