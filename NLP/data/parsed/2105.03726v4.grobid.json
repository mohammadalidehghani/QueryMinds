{
  "title": "Industrial practitioners' mental models of adversarial machine learning",
  "abstract": "Although machine learning is widely used in practice, little is known about practitioners' understanding of potential security challenges. In this work, we close this substantial gap and contribute a qualitative study focusing on developers' mental models of the machine learning pipeline and potentially vulnerable components. Similar studies have helped in other security fields to discover root causes or improve risk communication. Our study reveals two facets of practitioners' mental models of machine learning security. Firstly, practitioners often confuse machine learning security with threats and defences that are not directly related to machine learning. Secondly, in contrast to most academic research, our participants perceive security of machine learning as not solely related to individual models, but rather in the context of entire workflows that consist of multiple components. Jointly with our additional findings, these two facets provide a foundation to substantiate mental models for machine learning security and have implications for the integration of adversarial machine learning into corporate workflows, decreasing practitioners' reported uncertainty, and appropriate regulatory frameworks for machine learning security.",
  "introduction": "Introduction Adversarial machine learning (AML) studies the reliability of learning based systems in the context of an adversary [6, 12, 69] . For example, tampering with some features often suffices to change the classifier's outputs to a class chosen by the adversary [9, 24, 80] . Analogously, slightly altering the training data enables the attacker to decrease performance of the classifier [11, 72] . Another change in the training data allows the attacker to enforce a particular output class when a specified stimulus is present [20, 36] . Most state-of-the-art attacks and mitigations are in an ongoing arms race [5, 19, 82] . Although machine learning (ML) is increasingly used in industry, very little is known about ML security in practice. At the same time, previous works show that practitioners are concerned about AML [47, 60] , and failures already occur [53] , very little is known about ML security in practice. To tackle this question, we conduct a first study to explore mental models of AML. Mental models are relatively enduring, internal conceptual representations of external systems that originated in cognitive science [30, 38] . In other security related areas, correct mental models have been found to ease the communication of security warnings [16] or enable users to implement security best-practices [81] . Mental models also serve to enable better interactions with a given system [87] , or to design better user interfaces [29] . Our methodology builds upon these previous works by using qualitative methods to investigate the perception of vulnerabilities in ML applications. More concretely, we conducted 15 semi-structured interviews and drawing tasks with industrial practitioners from European start-ups and coded both drawings and the transcripts of the interviews. As the first work in this direction, we lay the foundations for practitioners' mental models of AML by describing two facets of these models. The first concerns the separation of ML related security (AML) and security unrelated to ML (non-AML security). In many cases, the borders between these two fields are blurry: a participant may start talking about evasion and finish the sentence with a reference to cryptographic keys. The second facet concerns the view of the ML model within a project. In contrast to the focus on an isolated model in AML research [5, 6, 12, 19, 69] , our practitioners often describe one or more pipelines with potentially several applications of ML. Finally, we found more facets which are left for an in-depth arXiv:2105.03726v4 [cs.CR] 29 Jun 2022 data design training model deployment poisoning [55, 72] , backdooring [20, 36] model stealing [83] model reverse engineering [67] membership inference [74] evasion [24, 80] adversarial reprogramming [27] adversarial initialization [32] weight perturbations [21] sponge attacks [75] Figure 1 : AML threats within the ML pipeline. Each attack is visualized as an arrow pointing from the step controlled to the point where the attack affects the pipeline. investigation by future work. These include the application setting, prior education, and the perceived relevance of AML. Our interviews showed that most of our participants lack an adequate and differentiated understanding to secure ML systems in production. At the same time, more than a third of our participants feels insecure about AML. These concerns seem justified as we found evidence for semi-automated fraud on ML systems in the wild. However, our findings have more practical implications. Our results allow us to address the current lack of understanding by (I) increasing awareness for AML and decreasing uncertainty about AML, (II) developing tools that help practitioners to assess and evaluate security of ML applications, and (III) drafting regulations that contain adequate security assessments and reduce insecurity about AML. However, more work is needed to understand the individual and shared mental models of practitioners and assess the real world security risks when applying ML.",
  "body": "Introduction Adversarial machine learning (AML) studies the reliability of learning based systems in the context of an adversary [6, 12, 69] . For example, tampering with some features often suffices to change the classifier's outputs to a class chosen by the adversary [9, 24, 80] . Analogously, slightly altering the training data enables the attacker to decrease performance of the classifier [11, 72] . Another change in the training data allows the attacker to enforce a particular output class when a specified stimulus is present [20, 36] . Most state-of-the-art attacks and mitigations are in an ongoing arms race [5, 19, 82] . Although machine learning (ML) is increasingly used in industry, very little is known about ML security in practice. At the same time, previous works show that practitioners are concerned about AML [47, 60] , and failures already occur [53] , very little is known about ML security in practice. To tackle this question, we conduct a first study to explore mental models of AML. Mental models are relatively enduring, internal conceptual representations of external systems that originated in cognitive science [30, 38] . In other security related areas, correct mental models have been found to ease the communication of security warnings [16] or enable users to implement security best-practices [81] . Mental models also serve to enable better interactions with a given system [87] , or to design better user interfaces [29] . Our methodology builds upon these previous works by using qualitative methods to investigate the perception of vulnerabilities in ML applications. More concretely, we conducted 15 semi-structured interviews and drawing tasks with industrial practitioners from European start-ups and coded both drawings and the transcripts of the interviews. As the first work in this direction, we lay the foundations for practitioners' mental models of AML by describing two facets of these models. The first concerns the separation of ML related security (AML) and security unrelated to ML (non-AML security). In many cases, the borders between these two fields are blurry: a participant may start talking about evasion and finish the sentence with a reference to cryptographic keys. The second facet concerns the view of the ML model within a project. In contrast to the focus on an isolated model in AML research [5, 6, 12, 19, 69] , our practitioners often describe one or more pipelines with potentially several applications of ML. Finally, we found more facets which are left for an in-depth arXiv:2105.03726v4 [cs.CR] 29 Jun 2022 data design training model deployment poisoning [55, 72] , backdooring [20, 36] model stealing [83] model reverse engineering [67] membership inference [74] evasion [24, 80] adversarial reprogramming [27] adversarial initialization [32] weight perturbations [21] sponge attacks [75] Figure 1 : AML threats within the ML pipeline. Each attack is visualized as an arrow pointing from the step controlled to the point where the attack affects the pipeline. investigation by future work. These include the application setting, prior education, and the perceived relevance of AML. Our interviews showed that most of our participants lack an adequate and differentiated understanding to secure ML systems in production. At the same time, more than a third of our participants feels insecure about AML. These concerns seem justified as we found evidence for semi-automated fraud on ML systems in the wild. However, our findings have more practical implications. Our results allow us to address the current lack of understanding by (I) increasing awareness for AML and decreasing uncertainty about AML, (II) developing tools that help practitioners to assess and evaluate security of ML applications, and (III) drafting regulations that contain adequate security assessments and reduce insecurity about AML. However, more work is needed to understand the individual and shared mental models of practitioners and assess the real world security risks when applying ML. Background and related work In this section, we review related work on AML and recall different attacks that have recently been discussed. We also review literature on mental models with regard to humancomputer interaction, usable security and ML. Adversarial machine learning AML studies the security of ML algorithms [6, 12, 69] . We attempt to give an informal overview of all attacks in AML, and additionally illustrate them in Figure 1 . Poisoning/backdooring. Early works in poisoning altered the training data [72] or labels [11] to decrease accuracy of the resulting classifier, for example SVM. For deep learning, due to the flexibility of the models, introducing backdoors is more common [20, 36] . Backdoors are chosen input patterns that reliably trigger a specified classification output. Defending such backdoors has lead to an arms race [82] . Evasion/adversarial examples. Early work in evasion decreased the test-time accuracy of spam classification [24] . It was later shown that also more complex models change their output for small, malicious input perturbations [9, 80] . Albeit all classifiers are principally vulnerable towards evasion, recent works focus on the arms race in deep learning [5, 19] . Membership inference. After first inferring attributes [4] of the training data, research later showed that entire points can be leaked from a model [74] . More concretely, the attacker deduces, given the output of a trained ML model, whether a data record was part of the training data or not. As for other attacks, numerous defenses are being proposed [37, 64] . Model stealing. Tramèr et al. [83] recently introduced model stealing. During this attack, the attacker copies the ML model functionality without consent of the model's owner. The attacker, given black box access to the original model, tries to reproduce a model with similar performance. As for the previous attacks, mitigations have been proposed [39, 68] . Weight perturbations. Fault tolerance of neural networks has long been studied in the ML community [17, 65] . Recently, maliciously altered weights are used to introduce a specific backdoor [35] . Few works exist to defend malicious change to the weights in general, not only related to backdoor introduction [78, 88] . For the sake of completeness, we conclude with a description of additional, recent attacks, some of which are part of our questionnaires (see Appendix D.3). In adversarial initialization, the initial weights of a neural network foot_0 are targeted to harm convergence or accuracy during training [32, 54] . In adversarial reprogramming, an input perturbation mask forces the classifier at test time to perform another classification task than originally intended [27] . For example, a cat/dog classifier is reprogrammed to classify digits. In model reverse engineering, crafted inputs allow to deduce from a trained model the usage of dropout and other architectural choices [67] . Fi-nally, sponge attacks aim to increase energy consumption of the classifier at test time [75] . Practical Relevance of AML. In general, AML research has been criticized for the limited practical relevance of its threat models [28, 31] . A possible reason is our lack of knowledge about AI security in practice. Few works attempt to tackle this gap, including for example Lin and Biggio [53] . They give an overview about AI attacks that were carried out in practice based on AI related incidents covered in newspapers. Furthermore, Boenisch et al. [14] conducted a survey and developed an awareness score, which however encompasses AML, privacy, and non-AML security. Concerning which threats are relevant in practice in industry, Kumar et al. [47] and Mirsky et al. [60] found that practitioners are most concerned about model theft and poisoning. Yet, in academia, most work focused on evasion so far. To shed more light on AML in practice, we interview industrial practitioners and take a first step towards a theory of mental models of AML. To this end, we now introduce and review mental models. Mental models Mental models are relatively enduring and accessible, but limited, internal conceptual representations of external systems [26] that enable people to interact with given systems. Hence, the field of human computer interaction (HCI) studied this concept quite early [73] . Mental models, most recently, saw an increasing relevance in usable security. We now recall prior application scenarios and highlight relevant conceptual contributions in the context of security and ML. Mental models in HCI and usable security. The relevance of mental models has been subject to a lengthy debate in HCI research [76, 85] . In many cases, the focus was to capture, depict and analyze mental models of specific objects of investigation. Examples of topics include, but are not limited to, the design of online search applications [7] , interface design [44] , and interfaces for blind people [25] . Research in usable security has recently focused on mental models of security in general [2, 87] , privacy in general [71] , security warnings [16] , incident response [70] , the internet [41] , the design of security dashboards [58] , the Tor anonymity network [29] , privacy and security in smart homes [81, 90] , encryption [1, 89] , HTTPS [45] , and cryptocurrency systems [57] . With regard to the respective object of investigation, these contributions paved the way for improvements of user interface designs [29] , adequate security communication [16] , as well as the development of security policies and implementation of best-practices [81] . It has been argued that security mental models contain structural and functional properties [89] . For each application, users develop a cognitive representation of its inherent components, their interconnection and correspondingly possible security threats. This representation helps them to understand where threats could emerge and how they could take effect. Mental models evolve dynamically upon individual interaction with a given application [13] . Mental models in ML. In order to interact with an ML application, humans need a mental model of how it combines evidence for prediction [66] . This is all the more important for ML-based applications which often inherit a certain opacity. As Lage et al. [48] pointed out, the number of necessary cognitive chunks is the most important type of complexity in order to understand applications. During interaction with black-box processes, humans strive for reduced complexity which may lead to the development of inaccurate or oversimplified mental models [33, 42] . A dedicated line of research therefore elaborates on the relevance and nature of mental models in the context of explainable artificial intelligence. Mental models have been found to serve as scaffolds not only for a given ML application [84] , but also for its embedding in organizational practices [91] . For data science teams, these workflows usually consist of predefined steps (Figure 1 ) and necessitate interpersonal collaboration [62] . Following Arrieta et al. [3] , we argue that individual collaborators within these teams (e.g., ML engineers, software engineers) develop separate internal representations of a given workflow or application. The need for appropriate mental models thereby increases with the enlarged scope of ML applications [49] and involved stakeholders [51, 79] . Recent work in this line of research called for qualitative studies at the intersection of the HCI and ML communities, to better understand the cognitive expectations practitioners have on ML systems [8, 42] . Suchlike studies seem all the more relevant as various industry initiatives propagate a humancentric approach to AI, explicitly referring to mental models. 2 However, the current scientific discourse lacks a dedicated consideration of cognition in AML. In order to fill this gap, we present the first qualitative study to elicit mental models of adversarial aspects in ML. Methodology This section describes the design of our semi-structured interviews, the drawing task, our recruiting strategy, the participants, and the data analysis. Our methodology was designed to investigate the perception of ML security and is, to the best of our knowledge, the first mental model study of AML. Study design and procedure To assess participants' perceptions, we conducted semistructured interviews enriched with drawing tasks. We draw inspiration from recent work in usable security which also investigated mental models [45, 89] . Before the interview, participants were informed about the general purpose of our study and the applied privacy measures. We further assured each participant that their answers would not be judged. Participants were then instructed to complete a questionnaire on demographics, organizational background and a self-reflected familiarity with field-related concepts (Appendix D) before the interview. This questionnaire was filled with or without the authors' presence. The answers have later been used to put participants' perceptions in context to their organizational and individual background. The threefold structure of our interviews covered 1) a specification of a given ML project a participant was involved in, 2) the underlying ML pipeline of this project and 3) possible security threats within the project. We chose this approach as the different attack vectors form part of the ML-pipeline as shown in Section 2.1. The detailed interview guideline can be found in Appendix C. As a last step of our interviews, we confronted the participants with exemplary attacker models for some of the threats considered relevant in industrial application of ML [47] . To assess practitioners' understandings of these threats, study participants had to elaborate on these attack vectors within their specific setup (Appendix D.2). To assess the participants' knowledge about (A)ML in general, participants were asked to fill an additional questionnaire after the interview (Appendix D.3). In this questionnaire, we tested general knowledge in ML and independently asked for a self-reflected familiarity rating with some of the attacks we discussed in Section 2.1. This questionnaire was handed to the participants after the interview as to avoid priming. We conducted one pilot interview to evaluate our study design. This first participant met all criteria of our target population in terms of employment, education and prior knowledge. As his explanations and drawings matched our expectations, we only added a specific question regarding the collaborators within a given ML-based project. The average interview lasted 40 minutes and was jointly conducted by the first two authors of this paper between April and July 2019. To minimize interviewer biases, we equally distributed the interviews, where one author was the lead interviewer and the other took notes. Due to the COVID-19 pandemic, interviews were conducted remotely and relied on a freely available digital whiteboard foot_2 . Recruitment Recruitment for a study on applied ML in corporate environments presents a challenge, as only a small proportion of the overall population works with ML. Furthermore, the topic touches compliance and intellectual property of participating organizations. Hence, many companies are skeptical about the exchange with third parties. Consequently, many current contributions with industrial practitioners as study participants are conducted by corporate research groups (e.g., [34, 47] ). We tried to initiate interviews with two large multinational companies. Unfortunately, both denied our request after internal risk assessments. Therefore, we focused on smaller companies where we could present our research project directly to decision-makers and convince them to participate in our study. We relied on the authors' networks (pilot participant, P11) and public databases for start-ups (more details in Appendix A) to find potential participants and used directmessaging on LinkedIn and emails to get in contact. Recruitment of study participants happened in parallel to interview conduction. Some participants forwarded our interview request to internal colleagues, so that we talked to multiple employees of some participating companies (see Table 1 ). We aimed to recruit experienced and knowledgeable participants and hence our requirements were a background in ML or computer science and positions such as data scientists, software engineers, product managers, or tech leads. We did not require any prior knowledge in security. After 8 interviews, no new topics (in our case for example new pipeline elements, whether defenses were mentioned, or how attacks were depicted in drawings) emerged. The research team thus agreed after 15 interviews that saturation was reached [15] , and we stopped recruiting. The participants were randomly assigned an ID (a number between 1 and 20) which was used throughout our analysis. All participants were offered an euro 20 voucher as compensation for their time. Participants We summarize demographic information in Table 1 . One participant, P10, did not hand in the questionnaire and is consequently not included in the following statistics. 14 participants identified as male, one identified as female, our sample is thus skewed towards males when considering ML practitioners [40] . As previous work found security perception of women and men to exhibit only some differences [59] , this bias is acceptable for a first exploration but should be studied in depth in future work. Our participants had an average age of 34 years (standard deviation (STD) 4.27). As intended for a first exploration of practitioners' perception of AML, our sample covered various application domains and organizational roles which we now describe in detail. Education and prior knowledge. The majority of participants (9 of 14) has a PhD, with all participants holding some academic degree. While our sample skews towards PhDs compared to the overall population of ML practitioners [40] , previous work reports no correlation between overall education and security awareness [14] . Most participants (12 of 14) reported that they had attended lectures or seminars on ML. Roughly half (6 of 14) reported to have a similar background in security. To obtain a more objective measure we conducted a test about ML knowledge and asked participants to rate their familiarity with AML attacks (details in Appendix B). While we found that all participants were indeed knowledgeable in ML, we found that few attacks were well known to them. Employment. Regarding the size of the companies, four participants worked in companies with less than ten employees, five in companies with less than 50 and the remaining six participants in companies with less than 200 employees. The companies' application areas were as diverse as healthcare, security, human resources, and others. Most participants were working in their current positions 6 years (STD 4.9). Their roles were diverse: Most (8 of 15) were in managing positions. Three were software or ML engineers, three more researchers. One of the participants stated to be both a researcher and a founder. One participant did not report his role. Finally, we asked participants to report which goals were part of their companies' AI/ML checklist. Almost all participants (13 of 14) reported that performance mattered in their company. Half (7 of 14) stated that privacy was important. Slightly less than half (6 of 14) focused on explainability and security. Least participants (4 of 14) listed fairness as a goal in their products. To conclude, when interpreting these numbers, one should keep in mind that not all five goals apply equally to all application domains. Furthermore, our sample is too small to derive per area or per company insights, and we thus leave a detailed analysis for future work. Data analysis We adopted an inductive approach, where we followed recent work in social sciences and usable security that constructed theories based on qualitative data [45, 63] . To distill observable patterns in interview transcripts and drawings, we applied two rounds of open coding, e.g. we assigned one or several codes to sentences, words, or parts of the drawings. We then performed Strauss and Corbin's descriptive axial coding to group our data into categories and selective coding to relate these categories to our research questions [77] . Throughout the coding process, we used analytic memos to keep track of thoughts about emerging themes. The final set of codes for interview transcripts and drawings is listed in Appendix E. As a first step, the first two authors independently conducted open coding sentence by sentence and sketch by sketch. This allowed for the generation of new codes without predefined hypotheses. Afterwards, the resulting codes were discussed and the research team agreed on adding specific codes for text snippets relating to the confusion of standard security and AML. As a second step, two coders independently coded the data again. After all iterations of coding, conflicts were resolved and the codebook was adapted accordingly. During axial coding, the obtained codes were grouped into categories. The first two authors independently came up with proposed categories which have then been discussed within an in-person meeting. While the grouping was undisputed for some of the categories presented in Appendix E (e.g. AML attacks, pipeline elements), for others the research team decided for (e.g. confusion, relevance) or against (e.g. type of ML model applied) the inclusion of a corresponding category only after detailed discussion. In addition, dedicated codes for the perception of participants (e.g. perceives AML as a feature, not a bug or security issue) were added to the codebook. Once the research team agreed on a final codebook, all transcripts and drawings were coded again using corresponding software. 4 In doing so, we aimed for inferring contextual statements instead of singular entities. The codes and categories served as a baseline for selective coding. Independently, the researchers came up with observations and proposals for specific mental models. Every proposal included a definition of the observation, related codes, exemplary quotes and drawings. The first two authors then met multiple times to discuss the observations and the corresponding relations of codes and categories. The resulting code tree contains 77 interview codes in 12 groups, 44 for drawings (in 5 groups), as depicted in Appendix E. Over all interviews, the coders agreed on 989 codes while disagreeing on 136. Analogously, there were 275 codes on drawings in total, with 42 disagreements. We further calculated Cohen's kappa [23] to measure the level of agreement among the coders. For interview transcripts, we reached κ = 0.71; for the codes assigned to drawings κ = 0.85. These values indicate a good level of coding agreement since both values are greater than 0.61 [50] . Given the semi-technical nature of our codebook, we consider these values as substantial inter-coder agreement. Irrespective of this and in line with best practices in qualitative research, we believe that it is important to elaborate how and why disagreements in coding arose and disclose the insights gained from discussions about them. Each coder brought a unique perspective on the topic that contributed to a more complete picture. Due to the diverse background of our research team in AML, usable security and economic geography, most conflicts arose regarding the relevance of technical and organizational elements of transcripts and drawings. These were resolved during conceptual and on-the-spot discussions within the research team. Expectations of mental models Given previous work on mental models and ML, we designed our study in a way that participants would first visualize their pipeline and later add corresponding attacks and defenses. For the pipeline, we expected that participants would name basic steps or components, such as data (collection), training, and testing. In general, we assumed participants' descriptions would vary in technical detail. Regarding AML, one of our motivations to conduct this study was to learn which knowledge our participants had. As a recent phenomenon, AML might not be known at all in practice, although practitioners might be aware of attacks relevant to their specific application. In particular, we did not expect practitioners to depict attacks using a starting and target point, as done in Figure 1 . Ethical considerations The ethical review board of our university reviewed and approved our study design. We limited the collection of personal data as much as possible and used ID's for participants throughout the analysis. Since all participants were employed at existing companies and partially shared business-critical information, we aimed to avoid company-specific disclosures in this paper. Finally, we complied with both local privacy regulations and the general data protection regulation (GDPR). Empirical results In this section, we discuss our findings from the interviews and drawings. Given the unexplored nature of mental models of AML, we focus on two main facets, and discuss additional findings that require a more in depth analysis (in the sense of future work) at the end of this section. The first of the two main facets is the (mingled) relationship between ML security (AML) and security unrelated to ML (non-AML security). We found that our participants, while not referring to AML and non-AML security interchangeably, still exhibited an often vague boundary between the two topics. The second facet concerns the view on ML as part of a larger workflow or product in industry, as opposed to the focus on an isolated model in academia. As a description of a high level workflow requires a high level perspective, we investigate whether it is equivalent to one, which we find not to be true. Afterwards, we then discuss potential facets requiring a more in depth investigation: the application setting, prior knowledge of the participant, and the perceived relevance of AML. Non-AML security and AML Non-AML security deals with the protection against digital attacks in general. In our case, it encompasses topics like access control, cryptography, malicious code execution, etc. Non-AML security provides sound solutions by deploying defenses or implementing design choices. In AML, threats are much more connected with the functioning of ML. For many AML attacks, it is unclear which defenses work due to the ongoing arms-race. Although both topics are conceptually different, we found that our participants did not distinguish between security unrelated to ML and AML, as visualized in Figure 2 . In our interviews, on the one hand, the boundary between non-AML security and AML often appeared blurry or unclear, with the corresponding concepts intertwined. On the other hand, there were crucial differences in the perception between non-AML security and AML threats. One difference is that whereas security defenses were often clearly stated as such, AML mitigations 5 were often applied without security incentives. Finally, we find a tendency to not believe in AML threats. Many participants denied responsibility, doubted an attacker would benefit, or stated the attack does not exist in the wild. There was no such tendency in non-AML security. Mingling AML and non-AML security We first provide examples showing that non-AML security and AML were not distinguished by our participants. Afterwards, we investigate if non-AML security and AML are used interchangeably, by investigating the co-occurrence of codes. Vagueness of the boundary between security and AML. There are many examples for a vague boundary between non-AML security and AML. For example P20 reasoned about evasion: \"this would require someone to exactly know how we deploy, right? and, where we deploy to, and which keys we use.\" At the beginning, the scenario seems unclear, but the reference to (cryptographic) keys or access tokens shows that the participant has moved to classical security. Analo-gously, when P18 reasoned about membership inference: \"but that could be only if you break in [...] if you login in to our computer and then do some data manipulation.\" Again, this participant was reasoning about failed access control as opposed to an AML attack via an API. Sometimes, ambiguity in naming confused our participants. For example, P11 thought aloud: \"poisoning [...] the only way to install a backdoor into our models would be that we use python modules that are somewhat wicked or have a backdoor.\" In this case, the term 'backdoor' in our questionnaire caused a non-AML security mindset involving libraries in contrary to our original intention to query participants about neural network backdoors. The same reasoning can also be seen in P11's drawing (compare Figure 3 ), where 'backdoor' points to python modules. Finally, P12 stated: \"maybe the poisoning will be for the neural network. From our point of view you would have to get through the Google cloud infrastructure.\" From an AML perspective, the attack is carried out via data which is uploaded from the user. Yet, the infrastructure is perceived as an obstacle for the attack. Correlations between non-AML security and AML attacks. In the previous paragraph, we showed that the boundaries between AML and non-AML security are blurred in our interviews. Another example is P6 reasoning about IP loss: \"we are very much concerned I'd say the models themselves and the training data we have that is a concern if people steal that would be bad.\" In this case, it is left out how the attack is performed. Analogously, P9 remarked: \"We could of course deploy our models on the Android phones but we don't want anybody to steal our models.\" To investigate whether our participants are more concerned about some property or feature (data, IP, the model functionality) than about how it is stolen or harmed, we examined the co-occurrence of AML and non-AML security codes that refer to similar properties in our interviews. For example, the codes 'model stealing' and 'code breach' both describe a potential loss of the model (albeit the security version is broader). Both codes occur together six times, with 'code breach' being tagged one additional time. Furthermore, the code 'model reverse engineering', listed only two times, occurs both times with both 'model stealing' and 'code breach'. However, not all cases are that clear. For example 'membership inference' and 'data breach' only occur together two times. The individual codes are more frequent, and were mentioned by three ('membership inference') and eleven ('data breach') participants. Analogously, attacks on availability (such as DDoS) in ML and non-AML security were only mentioned once together. Such availability attacks were brought up in an ML context twice, in non-AML security four times. Codes like 'evasion' and 'poisoning', in contrast, are not particularly related to any non-AML security concern. We conclude that AML and security are not interchangeable in our participants' mental models to refer to attacks with a shared goal. Differences between AML and non-AML security In the previous subsection, we found that our participants did not distinguish non-AML security and AML. To show that this is not true in general, we now focus on the differences between the two topics. To this end, we start with the perception of defenses and then consider the overall perception of threats in AML and security. We conclude with a brief remark on the practical relevance of AML. Defenses. Out of fifteen interviews, in thirteen some kind of defense or mitigation was mentioned; whereas all corresponding interviewees mentioned a non-AML security defense (encryption, passwords, sand-boxing, etc). An AML mitigation appeared in eight. In contrast to security defenses, however, AML defenses were often implemented as part of the pipeline, and not seen in relation to security or AML. As an example, P9, P15, and P18 reported to have humans in the loop, however not for defensive purposes. P10 and P16 were aware that this makes an attack more difficult. For example, P16 stated: \"maybe this poisoning of the data [...] is potentially more possible. There, we would have to manually check the data itself. We don't [...] blindly trust feedback from the user.\" Analogous observations hold techniques like explainable models (3 participants apply, 1 on purpose) or retraining (2 apply, additional 2 as mitigation). For example, P14 said: \"when we find high entropy in the confidences of the data [...] for those kind of specific ranges we send them back to the data sets to train a second version of the algorithm.\" In this case, retraining was used to improve the algorithm, not as a mitigation. We conclude that albeit no definite solution to vulnerability exists, many techniques that increase the difficulty for an attacker are implemented by our participants. At the same time, many practitioners are unaware which techniques potentially make an attack harder. Perception of threats. There is also a huge difference in the perception of threats in non-AML security and AML. In security, threats were somewhat taken for granted. For example, P9 was concerned about security of the server's passwords \"because anybody can reverse-engineer or sniff it or something.\" Analogously, P6 said to pay attention to \"the infrastructure so that means that the network the machines but also the application layer we need to look at libraries.\" On the other hand, almost a third of our participants (4 of 15) externalized responsibility for AML threats. For example, P3 said their \"main vulnerability from that perspective would probably be more the client would be compromised.\" Analogously, P1 remarked that ML security was a \"concern of the other teams.\" In both cases, the participants referred to another entity, and reasoned that they were not in charge to alleviate risks. Other reasons not to act include participants not having encountered an AML threat yet, and concluded AML was not relevant. More concretely, P9 remarked: \"we also have a community feature where people can upload images. And there could be some issues where people could try to upload not safe or try to get around something. But we have not observed that much yet. So it's not really a concern, poisoning.\" Roughly half of the participants (7 of 15) reported to doubt the attackers' motivation or capabilities in the real world. For example, P1 said: \"I have a hard time imagining right now in our use-cases what an attacker might gain from deploying such attacks.\" P20, who worked in the medical domain, stated: \"I'm left thinking, like, why, what could you, achieve from that, by fooling our model. I'm not sure what the benefit is for whoever is trying to do that.\" Finally, many participants (9 of 15) believed that they have techniques in place which function as defenses. As an in-depth evaluation of which mitigations are effective in which setting is beyond the scope of this paper, we leave it for future work. Practical relevance of AML. The fact that most participants did not consider AML threats relevant might be an expression of these threats being academic and not occurring in practice. Yet, our interviews showed that there are already variants of AML attacks in the wild. More concretely, P10 stated: \"What we found is [...] common criminals doing semiautomated fraud using gaps in the AI or the processes, but they probably don't know what AML, like adversarial machine learning is and that they are doing that. So we have seen plenty of cases are intentional circumventions, we haven't quite seen like systematic scientific approaches to crime.\" Our participants lack of concern might then be an indicator that harmful AML attacks are (still) rare in practice. Summary We found that non-AML security and AML were mingled in our participants' mental models: the boundaries between the corresponding threats were often unclear. Yet, security and AML were not interchangeably used to refer to attacks with a shared goal. Furthermore, non-AML security threats were treated differently than AML threats: the latter were often considered less relevant. Whereas it remains an open question whether AML and non-AML security should be treated differently in practice, the fact that they are currently Model M AML Research Our Findings Figure 4 : High-level intuition of Section 4.2. While AML research studies individual models, our participants often describe workflows with potentially several models, sometimes even the embedding system of the ML project. poorly distinguished might due to low exposure to AML. At the same time, our interviews provided evidence for AML attacks in practice. ML models and ML workflows Many of our participants did not only refer to an ML model, but discussed a workflow or an entire system. This is in stark contrast to AML research, where models are often studied in isolation, possibly due to a lack of available data. This finding is visualized in Figure 4 . In this subsection, we first discuss our participants view on ML models and the described systems. We then investigate whether such views are equivalent to a high level view on ML related projects, and conclude the section with a short discussion on some of our participants' struggles to assess threats at a high level. Model versus system view We first focus on the description of the ML model itself. Afterwards, we describe practitioners' views of ML models within larger systems and conclude the section with relating both findings to the technical level of abstraction. ML model perspective. The general perception of the ML pipeline (Figure 1 ) seems to affect mainly the relevance of ML-models as such within the pipeline. More concretely, participants talked about models as pipeline components. Many (11 of 15) of our participants presented their projects in chronological order or with an implicit flow. Examples are visible in Figure 3 or Figure 6 . Moreover, 6 out of 15 participants explained a pipeline not only as being composed by several steps, but remarked potentially several applications of ML within, or that several (different) pipelines exist. For example, P14 reported that \"the models are chained one after the other,\" and P7 stated that \"we have both like unsupervised training and unsupervised training.\" We conclude that often there is not a single model deployed, but data may be processed by several models, potentially in sequential order. System perspective. Moreover, participants showed a strong focus on the surrounding or embedding of their MLbased project. In other words, not only the pipeline around the model was important, but also the surrounding infrastructure of the project. Out of 15 participants, 5 described their ML pipeline as a classifier as embedded into the larger project context (for example visible in Figure 3 or Figure 5 ). Related to this embedding, in two of the interviews, the topic of technical debt (or long-term maintenance) arose. In this context, P6 stated: \"how [...] we can also have to something that is maintainable in the long term.\" Technical abstraction level The previous findings suggest a high level of technical abstraction in our interviews. While this is true on average, some (5 of 15) participants described their project minutely. For example P12 described their application almost at the code level: \"[...] we want to have for each node, that is basically the union of those two columns [...].\" However, whereas the same participants also described their project as a workflow, they did not talk about the embedding of the project. On the other hand, P18 remarked on their \"supervising\" (e.g., high level) perspective, yet provided no context. We conclude that our sample does not allow conclusions about the level of technical abstraction and perspective on ML model, which is thus left for future work. We did find, however, that a high level perspective seemed to make threat assessment harder for at least some participants. Asked to specify a certain threat model, P19 stated for example: \"It's like everywhere. Internal threats, external threats. Trying to mess with the communication, trying to mess if we model something.\" In a similar manner, P14 explained that an adversary could \"try to put some pythons in non conforming ways to trigger networks.\" Both descriptions are hard to interpret in technical terms, although both participants seemed aware of security threats in general. The same problem persists for defenses that our participants apply to encounter AML-specific security threats. P18, for example, first explained that \"the countermeasures are all in the API.\" After rechecking the documentation, the participant was able to provide further details on the applied defenses. Summary Our findings illustrate an important point which at the same time is very intuitive. Whereas most research papers focus on a single model when investigating ML security, in practice, models are trained and deployed in the context of other models or as components of larger workflows. At the same time, one pipeline may also contain several applications of ML. These views are not to be confused with the technical detail of a projects' description. We furthermore find evidence that the right level of detail is crucial to providing useful information. Additional facets of mental models Eliciting mental models with only fifteen interviews seems ambitious, in particular in the context of a technique so versatile as ML. In the following, we thus discuss potential aspects of mental models that have to be studied in more depth in future work. These aspects include, but are not limited to the application setting, the effect of prior knowledge, and the perceived relevance of AML. We also found evidence of structural and functional components in our participant's mental models. As the occurrence of these in AML mental models can be anticipated from prior work in mental models [89] , we leave the corresponding discussion to Appendix F. Application setting Our sample is too small to make general statements about the application area. However, since almost a third (4 of 15) participants work in cybersecurity, we attempt to investigate whether working in security affects sensitivity to AML. Hence, we first divide the participants into security and nonsecurity groups, starting with participants working in securityrelated fields. P10, who worked in a setting with cybersecurity reported: \"there is some standard AML attacks on ML you can use, but we design our system knowing that very well; on the other hand, we know that there is no perfect security, so, again defense is in monitoring and vigilance, but it's not something that can be fully automated in our opinion.\" P10 was in general very sensitive towards AML. P4, also from a cybersecurity setting, was less concerned about evasion: \"I can't imagine yet how it can be applied for real life, for example [...] since we are pretty close on our development.\" Yet, P4 also stated the need to gather more information about AML. Hence, also participants who worked in security-related areas had diverse mental models with respect to concrete attacks. Participants from non-security fields have similarly diverse mental models. This diversity is also reflected in the drawings. P11 (Figure 3 ) added some attacks (in red) before we provided explanations of evasion, backdooring and membership inference (added in blue). P18 (Figure 6 ), on the other hand, did not add any threats in their drawing. Analogously, opinions also differ in the interviews; e.g., P15 who worked in an non-security setting, was aware of security issues: \"one interesting thing of course is that the solution is in some ways constraint by adversarial security considerations so for example you cannot use natural language generation very much because of potential adversarial behavior.\" On the other hand, and confirming the drawing, P18 reported that \"we do not really protect the machine learning part.\" Investigating the diversity of mental models induced by the application area in more depth is thus left for future work. Prior knowledge Another potential factor on a practitioner's mental model is knowledge about or exposure to the topic at hand. However, we find no strong relation between education and capability or knowledge about AML in our sample. For example, one participant self-reported high knowledge in AML, but also stated: \"maybe the poisoning will be for the neural network.\" Here, a general attack, poisoning, is related to a specific model (neural networks). On the other end of the spectrum, P9 did not self-report any knowledge about security or AML, but correctly remarked: \"Somebody could send us 100.000 images and collect all the results and try to build a model from that.\" We conclude that in our sample, self-reported prior knowledge is not related to AML knowledge. Yet, more work is needed to understand more in depth the complex relationship between exposure, education, and mental models of AML. Perceived relevance of AML Last but not least, we found little awareness of AML in our sample. As already discussed in section 4.1.2, this might be a consequence of little exposure to AML attacks in the wild. On the other hand, we found all levels of concern about AML in our sample. More concretely, a third of our the practitioners (5 of 15) did not mention AML at all before we explicitly asked. Another third reported that they were not very concerned about AML. For example, P1 stated that evasion, or \"injecting malicious data to basically make the model [...] predict the wrong things\" was \"a concern that is not as high on my priority list.\" P15, analogously, said: \"mainly the machine learning pipeline this is the less critical security problem,\" reasoning that \"simply a performance would be unexpected.\" Yet, over a third (6 of 15) of the participants reported to feel insecure about AML when confronted with the topic. Of these six participants, two previously showed low priority on AML, and three did not mention AML at all. An example of insecurity is P4, who stated they needed \"some more research on it.\" Some participants, like P19, were concerned about specific attacks: \"I maybe need to learn more about this membership.\" In summary, some practitioners consider AML threats important, whereas some participants did not know AML well, and yet others did not consider it an important threat. From each of these three groups, there was at least one participant that felt not well informed. After the interviews (e.g., off the record) some participants stated that their awareness for AML had increased due to the interview. Many also inquired about defenses against specific threats, further confirming that they were indeed concerned about specific attacks. Future work Our findings expose the lack of knowledge about AML in practice, and thus show the need for additional research at the intersection of AML and cognitive science. In this section, we summarize these potential directions of future work. We first discuss theoretical research on mental models of AML and secondly more practical research that applies findings derived from mental models to AML. A theory on mental models of AML Our work is a first step to describe mental models of AML. For well-grounded mental models, more research is needed to investigate different aspects, as discussed in the previous section about the technical detail, application area and prior knowledge, for example. However, more research is also required concerning the development of mental models, and how a user based threat taxonomy (as opposed to a research based taxonomy) could look like. Temporal evolvement of mental models of AML. A better understanding about the development of individual mental models could help to assess necessary steps to make practitioners take into account AML. In addition, research on how mental models are shared between various AI practitioners might help to implement adequate defenses within and across corporate workflows. Corresponding starting points can be found in cognitive science [61] , where the convergence of mental models has been studied as a three-phase process of orientation, differentiation and integration [43] . Inherent threat taxonomies of mental models. Whereas academia has proposed clear threat models in ML security, it is unclear whether or to which degree these are also used or useful in practice. In this context, it could be interesting to consider existing taxonomies by Biggio et al. [10] and Barreno et al. [6] . These frameworks seem promising to investigate which specific structural elements practitioners consider relevant for specific attack vectors and how they perceive the causal evolution of these attacks. In line with recent work by Wang et al. [86] , such user-centric attack taxonomies might help to understand practitioners' reasoning on AML. Applying mental models to AML Secondly, but not less important, is the question how AML research can benefit from the study of mental models and which problems could be tackled in this context. Examples include the usability of AML tools and libraries, a more realistic threat modelling in AML research as well as a general assessment of AML attacks in the wild. Utility and usability of AML tools and libraries. We found that practitioners' mental models depend on available and provided information. Future research should therefore elaborate on the needed specificity of the available information. Furthermore, an evaluation of the available AML tools and libraries with regards to capabilities and needs of industrial practitioners might ease their usage across application domains. In line with recent work on fairness [52] and ethics [22] , we consider this crucial for designing usable and accessible tools, corporate guidelines and regulations. Practical threat modelling for AML research. As stated in Section 2, AML research has been criticized for the limited practical relevance of its threat models [28, 31] . Mental models could alleviate this issue in two ways. On the one hand, understanding which threats occur in which applications and how they are perceived helps to shift research towards designing practical and usable defenses. On the other hand, a deeper understanding of why non-AML security and AML are mingled allows us to adapt and improve current threat modelling. To this end, however, it is also important to know which threats need to be studied in the first place. AML in the wild. Given the previous insight and evidence of semi-automated, ML-related fraud, a more detailed assessment of which attacks are conducted in the wild would be beneficial. Future work could investigate this with a focus on different groups of ML practitioners, including for example ML engineers, auditors, and researchers, or dependant on the application. Furthermore, our work outlines that the model perspective usually taken in AML is of limited use in practice. More work is needed to study AML in the context of entire ML pipelines and end-to-end workflows. Practical implications Similar to Kumar et al. [47] , we find that most of our participants lack an adequate and differentiated understanding to secure ML systems in production. Given that we found only reports of semi-automated fraud in our sample in Section 4.1.2, the absence of strong AML in practice might explain this lack of knowledge. Yet, as discussed in Section 4.3.3, 6 of 15 participants felt insecure about ML security. We thus now discuss the diverse implications of our study on how to tackle these insecurities and the overall lack of knowledge. We start with the question how to raise awareness for AML. Afterwards, discuss the implications of our findings for the the embedding of AML in corporate workflows and finish with implications for regulatory frameworks of AML. Raising awareness of and increasing confidence about AML. Although we did not ask about privacy specifically, the general data protection regulation was often mentioned by our participants. For example, P6 stated: \"we are also subject to GDPR so we cannot just ignore the security aspects of the process.\" Like other participants (P12, P18), P6 mentioned GDPR before we had asked about membership inference and thus privacy. Legislation might thus be a tool to increase awareness of AML. Independently, a third of our participants felt insecure about AML (Section 4.3.3). Given that several participants reported used software (P9, P14, for example \"TensorFlow\"), infrastructure (P14) or service provider (P3, P12, P20, for example \"Google\"), advertising tools to assess AML risks might be helpful for our participants. In particular as AML libraries 6 , but also overviews like the Adversarial ML Threat Matrix foot_6 already exist. Our findings on the confusion between AML and non-AML security (Section 4.1.2) suggest these tools need to either enforce dedicated audits for both AML and non-AML security or combined countermeasures to address both areas jointly. Another solution to the feeling of insecurity, reported by our participants themselves (Section 4.3.3, P19: \"I maybe need to learn more about this membership\"), could be to provide materials for education. Embedding AML into corporate workflows. Whereas academia generally studies AML with the perspective of an individual model, in practice, the entire ML pipeline and broader AI workflow need to be considered. As discussed in Section 4.2, in our interviews, for example P6 and P16 (see Figure 5 ) described the entire workflow of their AI application, whereas other participants focused on the ML pipelines (for example P18, as visible in Figure 6 ). To successfully integrate AML into corporate workflows, however, more effort is needed. All actors working on an ML product need to be able to identify relevant and possible attacks and implementable defenses. Potential factors to consider here are for example different applications areas, as discussed in Section 4.3.1. Also the existing knowledge of the target audience should be considered, as the in Section 4.3.2 discussed variation of knowledge in our sample shows. Creating appropriate regulatory and standardization frameworks for AML. Lastly, our study has implications for regulatory approaches that enable appropriate security assessments. The differences in application (Section 4.3.1) and prior knowledge (Section 4.3.2) we found imply that regulatory frameworks need to find a way to formally encompass these differences with regards to necessary security measures. The currently proposed 'Legal Framework for AI' by the European Commission, for example, differentiates certain types of ML applications of which some are prohibited or classified as high-risk and thus require a certain risk management. Furthermore, as discussed in Section 4.2, our results indicate that it is essential to communicate such frameworks at the right technical abstraction level to encompass both technical ML practitioners and non-technical stakeholders. Standardization efforts could incorporate this requirement by providing adequate information at multiple mental abstraction levels [18] . For example, recently proposed frameworks like the NIST Taxonomy and Terminology of AML 8 explicitly lists references that might help practitioners develop more complex mental models. As mentioned above, a similar regulatory approach to privacy, the European general data protection regulation, had served as a scaffold for their privacy perception. Limitations We followed an inductive approach to investigate mental models through qualitative analysis. Hence, the data collected is self-reported and subjected to a coding process. We continued coding and refining codes until a good level of inter-coder agreement was reached. Nonetheless, all our findings are subject to interpretation and do not generalize beyond the sample, both of which is inherent to qualitative analyses. Finally, due to the COVID-19 pandemic, all interviews were conducted remotely and the interface limitations of the digital whiteboard might have impacted the participants' sketches. Given the qualitative approach and reached saturation, the small sample size of 15 is indeed acceptable [29, 89] . Due to the small sample size, however, several factors cannot be addresses in depth, as discussed in Section 4.3. Examples include, but are not limited to, the application setting and the perceived relevance. Ideally, future work provides a more in depth analysis of these topics in a larger quantitative study. 8 https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8269-draft.pdf All participants were employed at European organizations with <200 employees. This is due to the fact that while several multinational companies stated great interest in our research, they denied participation after internal risk assessments. As mental models of ML systems are always embedded in organizational practices [91] , we strongly encourage future research to assess our findings within larger samples including more variety, for example academics, small and large companies. Given that previous work found differences in general security behavior depending on gender [59] , and cultural background [46] , we also strongly encourage a more in depth analysis of these aspects. Furthermore, AML itself is a subject of study of which the perception evolves continuously. With an increasing awareness for security within applied machine learning, the findings presented can only be valid temporarily. Machine learning is applied in a wide range of settings. Consequently, not all attacks are relevant within each application domain. For example, a healthcare setting is subjected to other threats than a cybersecurity setting. For the sake of studying abstract facets of mental models, we did not consider the application in the present work. Yet, we would like to point out the necessity to study this aspect of AML in general. Conclusion Based on our semi-structured interviews with industrial practitioners, we take a first step towards a theory of mental models of AML. We described two facets of practitioners' mental models and sketched more facets as an anchor for in-depth investigation by future work. These include the technical abstraction level, application setting, prior education, and the perceived relevance of AML. We provided more details on the first facet, or the blurry relationship between AML and non-AML security. These two topics were often mingled, yet not used interchangeably by our participants. The second facet can be understood as a first step to refined threat models in AML research. As apposed to a single model, our participants instead described workflows and relationships between potentially several ML models in a larger system context. A clear understanding of the elicited mental models allows to improve information for practitioners and adjustments of corporate workflows. More concretely, our results help to raise awareness for AML, thus making practitioners feel less insecure. We further suggest that both application area and prior knowledge are considered when embedding AML into corporate workflows. Finally, regulatory frameworks might reduce uncertainty about AML and increase the awareness for possible AML threats. However, a wide range of subsequent research towards an encompassing theory of mental models in AML is still required. Last but nor least, we are convinced that the AML community will benefit from further practical assessment of attacks in practice, as our work already provides evidence of semi-automated fraud in the wild. we added two rather unknown terms, adversarial initialization [32] and neural trojans [56] (similar to backdoors). The results are depicted in Figure 7 . Only one participant reported to be familiar with one attack (evasion). In general, most participants reported to have heard of most common attacks (evasion, poisoning, membership inference, and model stealing). As expected for the sanity check, adversarial initialization and neural trojans were largely unknown. C Interview protocol Thank you so much for taking the time to give us your perspective on security in machine learning. This study consists in III parts. Part I aims at exploring your role in ML-projects. Part II addresses the underlying machine learning pipeline. In part III, we want to know how you perceive the security of machine learning. In part II and III, please visualize the topics (and relationships between them) that we ask you about. There are no rules, no wrong way to do it, and don't worry about spelling things perfectly. Nothing is off limits and you can use any feature of the digital whiteboard. After this last part, we will ask you about your knowledge about security of machine learning before this study. • Can you tell us a bit more about the goal of this project? • Who else is involved in this project? • What is your collaborators role in the project? Part II: Machine learning pipeline • What kind of pipeline do you currently apply within this machine learning based project? • Which part of this pipeline is crucial for your business, or identical to your product? • Have you encountered any issues relating to security in the projects you described? • Where in the pipeline did these security-related issues originate? • Can you specify the cause of the security-related issues? • Can you specify how these security-related issues evolve in your pipeline? • Which goal pursues an adversary with a such a threat? • What is the security violation of the threat? • How specific is the depicted threat? • Are you aware of any further possible security threats in the scope of your project or pipeline? • Which countermeasures do you implement against any of the aforementioned threats? Thank you so much for taking the time to give us your perspective on security in machine learning. Figure 2 : 2 Figure 2: High-level intuition Section 4.1. While in research, non-AML security and AML are rather distinct, our participants do not always clearly distinguish the two fields. Figure 3 : 3 Figure 3: Drawing of P11. Red markings were added by the participant before, blue after being confronted with selected attacks. Figure 5 : 5 Figure 5: Drawing of P16. Colors were added after selected attack were presented to the participant. Red refers to evasion, purple to reverse engineering, blue to membership inference. Figure 6 : 6 Figure 6: Drawing of P18. Red star indicates the most important component of the pipeline, not an attack. Part I: Machine learning project • Can you briefly describe what AI-or machine learningbased project you are currently involved in? Figure 7 : 7 Figure7: Self-reported familiarity of interviewed participants with different attacks on ML. Total of participants is 14, as one participant did not hand in questionnaire. Table 1 : 1 Participants with their random IDs. Capital letters denote that participants work in the same company. We denote the application domain and the working experience (Exp.) in years. Knowledge in ML, Security and AML is encoded as completed lectures (++), seminar/self-study (+) or none (). Company Education ID Application domain Exp. ML Sec. AML Degree 1 Human resources 7 ++ + PhD 3 A Healthcare 0.4 PhD 4 B Cybsersecurity 8 ++ + PhD 6 C Business intelligence 15 ++ ++ + PhD 7 Computer vision 12 ++ BSc 9 Computer vision 9 ++ MSc 10 Cybersecurity no questionnaire handed in 11 Business intelligence 1 ++ PhD 12 Retail and commerce 1.4 ++ PhD 14 AI as a service 5 ++ + PhD 15 Computer linguistics 5 + + MSc 16 C Business intelligence 3 ++ + + PhD 18 A Healthcare 1.5 ++ PhD 19 B Cybersecurity 15 ++ ++ + MSc 20 A Healthcare 1.2 ++ MSc Classifiers with convex optimization problems (for example SVM) cannot be targeted, as the mathematical solution to the learning problem does not depend on the initial weights. e.g., https://pair.withgoogle.com/chapter/mental-models/ https://awwapp.com/ Available at https://www.taguette.org/ and https://www.maxqda.com/. We are aware that AML is far from being solved, and communicated this to our participants if required. In this study, we define defenses as techniques which increase the difficulty for an attacker, like retraining or explainability. For example the Adversarial Robustness Toolbox, CleverHans, Robust-Bench, or the SecML library, just to name a few. https://github.com/mitre/advmlthreatmatrix https://www.crunchbase.com/ for European companies operating in AI and having raised more than 1 million dollar funding"
}