{
  "title": "Machine Learning and Computational Mathematics",
  "abstract": "Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of \"black box\" i",
  "introduction": "Introduction Neural network-based machine learning (ML) has shown very impressive success on a variety of tasks in traditional artificial intelligence. This includes classifying images, generating new images such as (fake) human faces and playing sophisticated games such as Go. A common feature of all these tasks is that they involve objects in very high dimension. Indeed when formulated in mathematical terms, the image classification problem is a problem of approximating a high dimensional function, defined on the space of images, to the discrete set of values corresponding to the category of each image. The dimensionality of the input space is typically 3 times the number of pixels in the image, where 3 is the dimensionality of the color space. The image generation problem is a problem of generating samples from an unknown high dimensional distribution, given a set of samples from that distribution. The Go game problem is about solving a Bellman-like equation in dynamic programming, since the optimal strategy satisfies such an equation. For sophisticated games such as Go, this Bellman-like equation is formulated on a huge space. All these are made possible by the ability to accurately approximate high dimensional functions, using modern machine learning techniques. This opens up new possibilities for attacking problems that suffer from the \"curse of dimensionality\" (CoD): As dimensionality grows, computational cost grows exponentially fast. This CoD problem has been an essential obstacle for the scientific community for a very long time. Take, for example, the problem of solving partial differential equations (PDEs) numerically. With traditional numerical methods such as finite difference, finite element and spectral methods, we can now routinely solve PDEs in three spatial dimensions plus the temporal dimension. Most of the PDEs currently studied in computational mathematics belong to this category. Well known examples include the Poisson equation, the Maxwell equation, the Euler equation, the Navier-Stokes equations, and the PDEs for linear elasticity. Sparse grids can increase our ability to handling PDEs to, say 8 to 10 dimensions. This allows us to try solving problems such as the Boltzmann equation for simple molecules. But we are totally lost when faced with PDEs, say in 100 dimension. This makes it essentially impossible to solve Fokker-Planck or Boltzmann equations for complex molecules, many-body Schrödinger, or the Hamilton-Jacobi-Bellman equations for realistic control problems. This is exactly where machine learning can help. Indeed, starting with the work in [20, 10, 21] , machine learning-based numerical algorithm for solving high dimensional PDEs and control problems has been one of the most exciting new developments in recent years in scientific computing, and this has opened up a host of new possibilities for computational mathematics. We refer to [17] for a review of this exciting development. Solving PDEs is just the tip of the iceberg. There are many other problems for which CoD is the main obstacle, including: • classical many-body problem, e.g. protein folding • turbulence. Even though turbulence can be modeled by the three dimensional Navier-Stokes equation, it has so many active degrees of freedom that an effective model for turbulence should involve many variables. • solid mechanics. In solid mechanics, we do not even have the analog of the Navier-Stokes equation. Why is this the case? Well, the real reason is that the behavior of solids is essentially a multi-scale problem that involves scales from atomistic all the way to macroscopic. • multi-scale modeling. In fact most multi-scale problems for which there is no separation of scales belong to this category. An immediate example is the dynamics of polymer fluids or polymer melts. Can machine learning help for these problems? More generally, can we extend the success of machine learning beyond traditional AI? We will try to convince the reader that this is indeed the case for many problems. Besides being extremely powerful, neural network-based machine learning has also got the reputation of being a set of tricks instead of a set of systematic scientific principles. Its performance depends sensitively on the value of the hyper-parameters, such as the network widths and depths, the initialization, the learning rates, etc. Indeed just a few years ago, parameter tuning was considered to be very much of an art. Even now, This is still the case for some tasks. Therefore a natural question is: Can we understand these subtleties and propose better machine learning models whose performance is more robust? In this article, we review what has been learned on these two issues. We discuss the impact that machine learning has already made or will make on computational mathematics, and how the ideas from computational mathematics, particularly numerical analysis, can be used to help understanding and better formulating machine learning models. On the former, we will mainly discuss the new problems that can now be addressed using ML-based algorithms. Even though machine learning also suggests new ways to solve some traditional problems in computational mathematics, we will not say much on this front.",
  "body": "Introduction Neural network-based machine learning (ML) has shown very impressive success on a variety of tasks in traditional artificial intelligence. This includes classifying images, generating new images such as (fake) human faces and playing sophisticated games such as Go. A common feature of all these tasks is that they involve objects in very high dimension. Indeed when formulated in mathematical terms, the image classification problem is a problem of approximating a high dimensional function, defined on the space of images, to the discrete set of values corresponding to the category of each image. The dimensionality of the input space is typically 3 times the number of pixels in the image, where 3 is the dimensionality of the color space. The image generation problem is a problem of generating samples from an unknown high dimensional distribution, given a set of samples from that distribution. The Go game problem is about solving a Bellman-like equation in dynamic programming, since the optimal strategy satisfies such an equation. For sophisticated games such as Go, this Bellman-like equation is formulated on a huge space. All these are made possible by the ability to accurately approximate high dimensional functions, using modern machine learning techniques. This opens up new possibilities for attacking problems that suffer from the \"curse of dimensionality\" (CoD): As dimensionality grows, computational cost grows exponentially fast. This CoD problem has been an essential obstacle for the scientific community for a very long time. Take, for example, the problem of solving partial differential equations (PDEs) numerically. With traditional numerical methods such as finite difference, finite element and spectral methods, we can now routinely solve PDEs in three spatial dimensions plus the temporal dimension. Most of the PDEs currently studied in computational mathematics belong to this category. Well known examples include the Poisson equation, the Maxwell equation, the Euler equation, the Navier-Stokes equations, and the PDEs for linear elasticity. Sparse grids can increase our ability to handling PDEs to, say 8 to 10 dimensions. This allows us to try solving problems such as the Boltzmann equation for simple molecules. But we are totally lost when faced with PDEs, say in 100 dimension. This makes it essentially impossible to solve Fokker-Planck or Boltzmann equations for complex molecules, many-body Schrödinger, or the Hamilton-Jacobi-Bellman equations for realistic control problems. This is exactly where machine learning can help. Indeed, starting with the work in [20, 10, 21] , machine learning-based numerical algorithm for solving high dimensional PDEs and control problems has been one of the most exciting new developments in recent years in scientific computing, and this has opened up a host of new possibilities for computational mathematics. We refer to [17] for a review of this exciting development. Solving PDEs is just the tip of the iceberg. There are many other problems for which CoD is the main obstacle, including: • classical many-body problem, e.g. protein folding • turbulence. Even though turbulence can be modeled by the three dimensional Navier-Stokes equation, it has so many active degrees of freedom that an effective model for turbulence should involve many variables. • solid mechanics. In solid mechanics, we do not even have the analog of the Navier-Stokes equation. Why is this the case? Well, the real reason is that the behavior of solids is essentially a multi-scale problem that involves scales from atomistic all the way to macroscopic. • multi-scale modeling. In fact most multi-scale problems for which there is no separation of scales belong to this category. An immediate example is the dynamics of polymer fluids or polymer melts. Can machine learning help for these problems? More generally, can we extend the success of machine learning beyond traditional AI? We will try to convince the reader that this is indeed the case for many problems. Besides being extremely powerful, neural network-based machine learning has also got the reputation of being a set of tricks instead of a set of systematic scientific principles. Its performance depends sensitively on the value of the hyper-parameters, such as the network widths and depths, the initialization, the learning rates, etc. Indeed just a few years ago, parameter tuning was considered to be very much of an art. Even now, This is still the case for some tasks. Therefore a natural question is: Can we understand these subtleties and propose better machine learning models whose performance is more robust? In this article, we review what has been learned on these two issues. We discuss the impact that machine learning has already made or will make on computational mathematics, and how the ideas from computational mathematics, particularly numerical analysis, can be used to help understanding and better formulating machine learning models. On the former, we will mainly discuss the new problems that can now be addressed using ML-based algorithms. Even though machine learning also suggests new ways to solve some traditional problems in computational mathematics, we will not say much on this front. Machine learning-based algorithms for problems in computational science In this and the next section, we will discuss how neural network models can be used to develop new algorithms. For readers who are not familiar with neural networks, just think of them as being some replacement of polynomials. We will discuss neural networks afterwards. Nonlinear multi-grid method and protein folding In traditional multi-grid method [5] , say for solving the linear systems of equation that arise from some finite difference or finite element discretization of a linear elliptic equation, our objective is to minimize a quadratic function like I h (u h ) = 1 2 u T h L h u h -f T h u h Here h is the grid size of the discretization. The basic idea of the multi-grid method is to iterate between solving this problem and a reduced problem on a coarser grid with grid size H. In order to do this, we need the following • a projection operator: P : u h → u H , that maps functions defined on the fine grid to functions defined on the coarse grid. • the effective operator at scale H: L H = P T L h P . This defines the objective function on the coarse grid: I H (u H ) = 1 2 u T H L H u H -f T H u H • a prolongation operator Q : u H → u h , that maps functions defined on the coarse grid to functions defined on the fine grid. Usually one can take Q to be P T . The key idea here is coarse graining, and iterating between the fine scale and the coarse-grained problem. The main components in coarse graining is a set of coarsegrained variables and the effective coarse-grained problem. Formulated this way, these are obviously general ideas that can be relevant for a wide variety of problems. In practice, however, the difficulty lies in how to obtain the effective coarse-grained problem, a step that is trivial for linear problems, and this is where machine learning can help. We are going to use the protein folding problem as an example to illustrate the general idea for nonlinear problems. Let {x j } be the positions of the atoms in a protein and the surrounding solvent, and U = U ({x j }) be the potential energy of the combined protein-solvent system. The potential energy consists of the energies due to chemical bonding, Van der Waals interaction, electro-static interaction, etc. The protein folding problem is to find the \"ground state\" of the energy U : \"Minimize\" U. Here we have added quotation marks since really what we want to do is to sample the distribution ρ β = 1 Z e -βU , β = (k B T ) -1 To define the coarse-grained problem, we assume that we are given a set of collective variables: s = (s 1 , • • • , s n ), s j = s j (x 1 , • • • , x N ), (n < N ). One possibility is to use the dihedral angles as the coarse-grained variables. In principle, one may also use machine learning methods to learn the \"best\" set of coarse-grained variables but this direction will not be pursued here. Having defined the coarse-grained variables, the effective coarse-grained problem is simply the free energy associated with this set of coarse-grained variables: A(s) = - 1 β ln p(s), p β (s) = 1 Z e -βU (x) δ(s(x) -s) dx, Unlike the case for linear problems, for which the effective coarse-grained model is readily available, in the current situation, we have to find the function A first. The idea is to approximate A by neural networks. The issue here is how to obtain the training data. Contrary to most standard machine learning problems where the training data is collected beforehand, in applications to computational science and scientific computing, the training data is collected \"on-the-fly\" as learning proceeds. This is referred to as the \"concurrent learning\" protocol [11] . In this regard, the standard machine learning problems for which the training data is collected beforehand are examples of \"sequential learning\". The key issue for concurrent learning is an efficient algorithm for generating the data in the best way. The training dataset should on one hand be representative enough and on the other hand be as small as possible. A general procedure for generating such datasets is suggested in [11] . It is called the EELT (exploration-examination-labeling-training) algorithm and it consists of the following steps: • exploration: exploring the s space. This can be done by sampling 1  Z e -βA(s) with the current approximation of A. • examination: for each state explored, decide whether that state should be labeled. One way to do this is to use an a posteriori error estimator. One possible such a posteriori error estimator is the variance of the predictions of an ensemble of machine learning models, see [41] . • labeling: compute the mean force (say using restrained molecular dynamics) F (s) = -∇ s A(s). from which the free energy A can be computed using standard thermodynamic integration. • training: train the appropriate neural network model. To come up with a good neural network model, one has to take into account the symmetries in the problem. For example, if we coarse grain a full atom representation of a collection of water molecules by eliminating the positions of the hydrogen atoms, then the free energy function for the resulting system should have permutation symmetry and this should be taken into account when designing the neural network model (see the next subsection). Figure 1 : The folded and extended states of Trp-cage, reproduced with permission from [37] . This can also be viewed as a nonlinear multi-grid algorithm in the sense that it iterates between sampling p β on the space of the coarse-grained variables and the (constrained) Gibbs distribution ρ β for the full atom description. This is a general procedure that should work for a large class of nonlinear \"multi-grid\" problems. Shown in Figure 1 is the extended and folded structure of Trp-cage. This is a small protein with 20 amino acids. We have chosen the 38 dihedral angles as the collective variables. The full result is presented in [37] . Molecular dynamics with ab initio accuracy Molecular dynamics is a way of studying the behavior of molecular and material systems by tracking the trajectories of all the nuclei in the system. The dynamics of the nuclei is assumed to obey Newton's law, with some potential energy function (typically called potential energy surface or PES) V that models the effective interaction between the nuclei: m i d 2 x i dt 2 = -∇ x i V, V = V (x 1 , x 2 , . .., x i , ..., x N ), How can we get the function V ? Traditionally, there have been two rather different approaches. The first is to compute the inter-atomic forces (-∇V ) on the fly using quantum mechanics models, the most popular one being the density functional theory (DFT). This is known as the Car-Parrinello molecular dynamics or ab initio molecular dynamics [6, 8] . This approach is quite accurate but is also very expensive, limiting the size of the system that one can handle to about 1000 atoms, even with high performance supercomputers. The other approach is to come up with empirical potentials. Basically one guesses a functional form of V with a small set of fitting parameters which are then determined by a small set of data. This approach is very efficient but unreliable. This dilemma between accuracy and efficiency has been an essential road block for molecular dynamics for a long time. With machine learning, the new paradigm is to use DFT to generate the data, and then use machine learning to generate an approximation to V . This approach has the Figure 2 : The results of EELT algorithm: Number of configurations explored vs. the number of data points labeled. Only a very small percentage of the configurations are labeled. Reproduced with permission from Linfeng Zhang. See also [40] . potential to produce an approximation to V that is as accurate as the DFT model and as efficient as the empirical potentials. To achieve this goal, we have to address two issues. The first is the generation of data. The second is coming up with the appropriate neural network model. These two issues are the common features for all the problems that we discuss here. The issue of adaptive data generation is very much the same as before. The EELT procedure can still be used. The details of how each step is implemented is a little different. We refer to [40] for details. Figure 2 shows the effect of using the EELT algorithm. As one can see, a very small percentage of the configurations explored are actually labeled. For the Al-Mg example, only ∼0.005% configurations explored by are selected for labeling. For the second issue, the design of appropriate neural networks, the most important considerations are: 1. Extensiveness, the neural network should be extensive in the sense that if we want to extend the system, we just have to extend the neural network accordingly. One way of achieving this is suggested by Behler and Parrinello [4] . 2. Preserving the symmetry. Besides the usual translational and rotational symmetry, one also has the permutational symmetry: If we relabel a system of copper atoms, its potential energy should not change. It makes a big difference in terms of the accuracy of the neural network model whether one takes these symmetries into account (see [23] and Figure 3 ). One very nice and general way of addressing the symmetry problem is to design the neural network model as the composition of two networks: An embedding network followed by a fitting network. The task for the embedding network is to represent enough symmetry-preserving functions to be fed into the fitting network [39] . With these issues properly addressed, one can come up with very satisfactory neural network-based representation of V (see Figure 4 ). This representation is named Deep Potential [23, 39] and the Deep Potential-based molecular dynamics is named DeePMD [38] . As has been demonstrated recently in [30, 25] , DeePMD, combined with state of the Figure 3 : The effect of symmetry preservation on testing accuracy. Shown in red are the results of poor man's way of imposing symmetry (see main text for explanation). One can see that testing accuracy is drastically improved. Reproduced with permission from Linfeng Zhang. Figure 4 : The test accuracy of the Deep Potential for a wide variety of systems. Reproduced with permission from Linfeng Zhang. See also [39] . art high performance supercomputers, can help to increase the size of the system that one can model with ab initio accuracy by 5 orders of magnitude. 3 Machine learning-based algorithms for high dimensional problems in scientific computing Stochastic control The first application of machine learning for solving high dimensional problems in scientific computing was presented in [20] . Stochastic control was chosen as the first example due to its close analogy with machine learning. Consider the discrete stochastic dynamical system: s t+1 = s t + b t (s t , a t ) + ξ t+1 . (1) Here s t and a t are respectively the state and control at time t, ξ t is the noise at time t. Our objective is to solve: min {at} T -1 t=0 E {ξt} T -1 t=0 c t (s t , a t ) + c T (s T ) (2) within the set of feedback controls: a t = A t (s t ). (3) We approximate the functions A t by neural network models: A t (s) ≈ Ãt (s|θ t ), t = 0, • • • , T -1 (4) The optimization problem (2) then becomes: min {θt} T -1 t=0 E {ξt} T -1 t=0 c t (s t , Ãt (s t |θ t )) + c T (s T )}. (5) Unlike the situation in standard supervised learning, here we have T set of neural networks to be trained simultaneously. The network architecture is shown in Figure 5 Figure 5 : Network architecture for solving stochastic control in discrete time. The whole network has (N + 1)T layers in total that involve free parameters to be optimized simultaneously. Each column (except ξ t ) corresponds to a sub-network at t. Reproduced with permission from Jiequn Han. See also [20] . Compared with the standard setting for machine learning, one can see a clear analogy in which (1) plays the role for the residual networks and the noise {ξ t } plays the role of data. Indeed, stochastic gradient descent (SGD) can be readily used to solve the optimization problem (5) . An example of the application of this algorithm is shown in Figure 6 for the problem of energy storage with multiple devices. Here n is the number of devices. For more details, we refer to [20] . Reproduced with permission from Jiequn Han. See also [20] . Nonlinear parabolic PDEs Consider parabolic PDEs of the form: ∂u ∂t + 1 2 σσ T : ∇ 2 x u + µ • ∇u + f σ T ∇u = 0, u(T, x) = g(x) We study a terminal-value problem instead of initial-value problem since one of the main applications we have in mind is in finance. To develop machine learning-based algorithms, we would like to first reformulate this as a stochastic optimization problem. This can be done using backward stochastic differential equations (BSDE) [33] . inf Y 0 ,{Zt} 0≤t≤T E|g(X T ) -Y T | 2 , (6) s.t. X t = ξ + t 0 µ(s, X s ) ds + t 0 Σ(s, X s ) dW s , (7) Y t = Y 0 - t 0 h(s, X s , Y s , Z s ) ds + t 0 (Z s ) T dW s . (8) It can be shown that the unique minimizer of this problem is the solution to the PDE with: Y t = u(t, X t ) and Z t = σ T (t, X t ) ∇u(t, X t ). (9) With this formulation, one can develop a machine learning-based algorithm along the following lines, adopting the ideas for the stochastic control problems discussed earlier [10, 21] : • After time discretization, approximate the unknown functions X 0 → u(0, X 0 ) and X t j → σ T (t j , X t j ) ∇u(t j , X t j ) by feedforward neural networks ψ and φ. • Using the BSDE, one constructs an approximation û that takes the paths {X tn } 0≤n≤N and {W tn } 0≤n≤N as the input data and gives the final output, denoted by û({X tn } 0≤n≤N , {W tn } 0≤n≤N as an approximation to u(t N , X t N ). • The error in the matching between û and the given terminal condition defines the expected loss function l(θ) = E g(X t N ) -û {X tn } 0≤n≤N , {W tn } 0≤n≤N 2 . This algorithm is called the Deep BSDE method. As applications, let us first study a stochastic control problem, but we now solve this problem using the Hamilton-Jacobi-Bellman (HJB) equation. Consider the well-known LQG (linear quadratic Gaussian) problem at dimension d = 100: dX t = 2 √ λ m t dt + √ 2 dW t , (10) with the cost functional: J({m t } 0≤t≤T ) = E T 0 m t 2 2 dt + g(X T ) . The corresponding HJB equation is given by ∂u ∂t + ∆u -λ ∇u 2 2 = 0 (11) Using the Hopf-Cole transform, we can express the solution in the form: u(t, x) = - 1 λ ln E exp -λg(x + √ 2W T -t ) . ( 12 ) This can be used to calibrate the accuracy of the Deep BSDE method. 0 10 20 30 40 50 lambda 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 u(0,0,...,0) Deep BSDE Solver Monte Carlo As a second example, we study the Black-Scholes equation with default risk: ∂u ∂t + ∆u -(1 -δ) Q(u(t, x)) u(t, x) -R u(t, x) = 0 where Q is some nonlinear function. This form of modeling the default risk was proposed and/or used in the literature for low dimensional situation (d = 5, see [21] for references). The Deep BSDE method was used for this problem with d = 100 [21] . Reproduced with permission from Jiequn Han. See also [21] . The Deep BSDE method has been applied to pricing basket options, interest ratedependent options, Libor market model, Bermudan Swaption, barrier option (see [17] for references). Moment closure for kinetic equations modeling gas dynamics The dynamics of gas can be modeled very accurately by the well-known Boltzmann Equation: ∂ t f + v • ∇ x f = 1 ε Q(f ), v ∈ R 3 , x ∈ Ω ⊂ R 3 , ( 13 ) where f is the phase space density function, ε is the Knudsen number: ε = mean free path macroscopic length , Q is the collision operator that models the collision process between gas particles. When ε 1, this can be approximated by the Euler equation: ∂ t U + ∇ x • F (U ) = 0, (14) where U = (ρ, ρu, E) T , ρ = f dv, u = 1 ρ f v dv. and F (U ) = (ρu, ρu ⊗ u + pI, (E + p)u) T Euler's equation can be obtained by projecting the Boltzmann equation on to the low order moments involved, and making use of the ansatz that the distribution function f is close to be local Maxwellian. 1.1 Some History and Background 1 CONTINUUM MODELS Kn 10 ¡2 10 ¡1 1.0 10.0 ½ ½ Euler Eqn equilibrium non-equilibrium NSF Eqn kinetic regime free flight transition regime ½ ½ ½ ! Figure 1: Overview of the range of Knudsen number and various model regimes. the moment systems lead to stable hyperbolic equations. However, in practical explicit systems hyperbolicity is given only in a finite range due to linearization. In Junk (1998) and Junk (2002) it is shown that the fully nonlinear maximum-entropy approach has sever drawbacks and singularities. Furthermore, the hyperbolicity leads to discontinuous sub-shock solutions in the shock profile. A variant of the moment method has been proposed by Eu (1980) and is used, e.g., in Myong (2001). Recently, a maximum-entropy 10-moment system has been used by Suzuki and van Leer (2005). Both fundamental approaches of kinetic theory, Chapman-Enskog and Grad, exhibit severe disadvantages. Higher order Chapman-Enskog expansions are unstable and Grad's method introduces subshocks and show slow convergence. It seems to be desirable to combine both methods in order to remedy their disadvantages. Such an hybrid approach have already been discussed by Grad in a side note in Grad (1958) . He derives a variant of the regularized 13-moment equations given below, but surprisingly he neither gives any details nor is he using or investigating the equations. In the last fifty years the paper Grad (1958) was cited as standard source for introduction into kinetic theory, but, apparently, this specific idea got entirely lost and seems not to be known in present day literature. The Chapman-Enskog expansion is based on equilibrium and the corrections describe the non-equilibrium perturbation. A hybrid version which uses a non-equilibrium as basis is discussed in Karlin et al. (1998) . They deduced linearized equations with unspecified coefficients. Starting from Burnett equations Jin and Slemrod (2001) derived an extended system of evolution equations which resembles the regularized 13-moment system. It is solved numerically in Jin et al. (2002) . However, the tensorial structure of their relations is not in accordance with Boltzmann's equation. Starting from higher moment systems Müller et al. (2003) discussed a parabolization which includes higher order expressions into hyperbolic equations. The regularized 13-moment-equations presented below were rigorously derived from Boltzmann's equation in Struchtrup and Torrilhon (2003) . The key ingredient is a Chapman-Enskog expansion around a pseudo-equilibrium which is given by the constitutive relations of Grad for stress tensor and heat flux. The final system consists of evolution equations for the fluid fields: density, velocity, temperature, stress tensor and heat flux. The closure The different regimes of gas dynamics. Reproduced with permission from Jiequn Han. See also [22] . What happens when ε is not small? In this case, a natural idea is to seek some generalization of Euler's equation using more moments. This program was initiated by Harold Grad who constructed the well-known 13-moment system using the moments of {1, v, (v -u) ⊗ (v -u), |v -u| 2 (v -u)}. This line of work has encountered several difficulties. First, there is no guarantee that the equations obtained are well-posed. Secondly there is always the \"closure problem\": When projecting the Boltzmann equation on a set of moments, there are always terms which involve moments outside the set of moments considered. In order to obtain a closed system, one needs to model these terms in some way. For Euler's equation, this is done using the local Maxwellian approximation. This is accurate when ε is small, but is no longer so when ε is not small. It is highly unclear what should be used as the replacement. In [22] , Han et al developed a machine learning-based moment method. The overall objective is to construct an uniformly accurate (generalized) moment model. The methodology consists of two steps: 1: Learn a set of optimal generalized moments through an auto-encoder. Here by optimality we mean that the set of generalized moments retains a maximum amount of information about the original distribution function and can be used to recover the distribution function with a minimum loss of accuracy. This can be done as follows: Find an encoder Ψ and a decoder Φ that recovers the original f from U , W W = Ψ(f ) = wf dv, Φ(U , W )(v) = h(v; U , W ). Minimize w,h E f ∼D f -Φ(Ψ(f )) 2 . U and W form the set of generalized hydrodynamic variables that we will use to model the gas flow. 2: Learn the fluxes and source terms in the PDE for the projected PDE. The effective PDE for U and W can be obtained by formally projecting the Boltzmann equation on this set of (generalized) moments. This gives us a set of PDEs of the form: ∂ t U + ∇ x • F (U , W ; ε) = 0, ∂ t W + ∇ x • G(U , W ; ε) = R(U , W ; ε). ( 15 ) where F (U , W ; ε) = vU f dv, G(U , W ; ε) = vW f dv, R(U , W ; ε) = ε -1 W Q(f )dv. Our task now is to learn F , G, R from the original kinetic equation. Again the important issues are (1) get an optimal dataset, and (2) enforce the physical constraints. Here two notable physical constraints are (1) conservation laws and (2) symmetries. Conservation laws are automatically respected in this approach. Regarding symmetries, besides the usual static symmetry, there is now a new dynamic symmetry: the Galilean invariance. These issues are all discussed in [22] . We also refer to [22] for numerical results for the models obtained this way. Mathematical theory of machine learning While neural network-based machine learning has demonstrated a wide range of very impressive successes, it has also acquired a reputation of being a \"black magic\" rather than a solid scientific technique. This is due to the fact that (1) we lack a basic understanding of the fundamental reasons behind its success; (2) the performance of these models and algorithms is quite sensitive to the choice of the hyper-parameters such as the architecture of the network and the learning rate; and (3) some techniques, such as batch normalization, does appear to be a black magic. To change this situation, we need to (1) improve our understanding of the reasons behind the success and the fragility of neural network-based models and algorithms and (2) find ways to formulate more robust models and design more robust algorithms. In this section we address the first issue. The next section will be devoted to the second issue. Here is a list of the most basic questions that we need to address: • Why does it work in such high dimension? • Why simple gradient descent works for training neural network models? • Is over-parametrization good or bad? • Why does neural network modeling require such extensive parameter tuning? At this point, we do not yet have clear answers to all these questions. But some coherent picture is starting to emerge. We will focus on the problem of supervised learning, namely approximating a target function using a finite dataset. For simplicity, we will limit ourselves to the case when the physical domain of interest is X = [0, 1] d . An introduction to neural network-based supervised learning The basic problem in supervised learning is as follows: Given a natural number n ∈ N and a sequence {(x j , y j ) = (x j , f * (x j )), j ∈ {1, 2, . . . , n}}, of pairs of input-output data, we want to recover the target function f * as accurately as possible. We will assume that the input data {x j , j ∈ {1, 2, . . . , n}}, is sampled from the probability distribution µ on R d . Step 1. Choose a hypothesis space. This is a set of trial functions H m where m ∈ N is the dimensionality of H m . One might choose piecewise polynomials or wavelets. In modern machine learning the most popular choice is neural network functions. Twolayer neural network functions (one input layer, one output layer which usually does not count, and one hidden layer) take the form f m (x, θ) = 1 m m j=1 a j σ( w j , x ) (16) where σ : R → R is a fixed scalar nonlinear function and where θ = {(a j , w j ) j∈{1,2,...,m} } are the parameters to be optimized (or trained). A popular example for the nonlinear function σ : R → R is the ReLU (rectified linear unit) activation function: σ(z) = max{z, 0}, for all z ∈ R. We will restrict our attention to this activation function. Roughly speaking, deep neural network (DNN) functions are obtained if one composes two-layer neural network functions several times. One important class of DNN models are residual neural networks (ResNet). They closely resemble discretized ordinary differential equations and take the form z l+1 = z l + M j=1 a j,l σ( z l , w j,l ), z 0 = V x, f L (x, θ) = α, z L (17) for l ∈ {0, 1, . . . , L-1} where L, M ∈ N. Here the parameters are θ = (α, V , (a j,l ) j,l , (w j,l ) j,l ). ResNets are the model of choice for truly deep neural network models. Step 2. Choose a loss function. The primary consideration for the choice of the loss function is to fit the data. Therefore the most obvious choice is the L 2 loss: Rn (f ) = 1 n n j=1 |f (x j ) -y j | 2 = 1 n n j=1 |f (x j ) -f * (x j )| 2 . ( 18 ) This is also called the \"empirical risk\". Sometimes we also add regularization terms. Step 3. Choose an optimization algorithm. The most popular optimization algorithms in machine learning are different versions of the gradient descent (GD) algorithm, or its stochastic analog, the stochastic gradient descent (SGD) algorithm. Assume that the objective function we aim to minimize is of the form F (θ) = E ξ∼ν l(θ, ξ) . ( 19 ) The simplest form of SGD iteration takes the form θ k+1 = θ k -η∇l(θ k , ξ k ), (20) for k ∈ N 0 where {ξ k , k ∈ N 0 = {0, 1, 2, . . . }} is a sequence of i.i.d. random variables sampled from the distribution ν and η is the learning rate which might also change during the course of the iteration. In contrast, GD takes the form θ k+1 = θ k -η∇E ξ∼ν l(θ k , ξ) . ( 21 ) Obviously this form of SGD can be adapted to loss functions of the form (18) which can be regarded as an expectation with ν being the empirical distribution on the training dataset. This DNN-SGD paradigm is the heart of modern machine learning. Approximation theory The simplest way of approximating functions is to use polynomials. For polynomial approximation, there are two kinds of theorems. The first is the Weierstrass' Theorem which asserts that continuous functions can be uniformly approximated by polynomials on compact domains. The second is Taylor's Theorem which tells us that the rate of convergence depends on the smoothness of the target function. Using the terminology in neural network theory, Weierstrass' Theorem is the \"Universal Approximation Theorem\" (UAT). It is a useful fact. But Taylor's Theorem is more useful since it tells us something about the rate of convergence. The form of Taylor's Theorem used in approximation theory are the direct and inverse approximation theorems which assert that a given function can approximated by polynomials with a particular rate if and only if certain norms of that function is finite. This particular norm, which measures the regularity of the function, is the key quantity that characterizes this approximation scheme. For piecewise polynomials, these norms are some Besov space norms [36] . For L 2 , a typical result looks like follows: inf f ∈Hm f -f m L 2 (X) ≤ C 0 h α f H α (X) (22) Here H α stands for the α-th order Sobolev norm [36] , m is the number of degrees of freedom. On a regular grid, the grid size is given by h ∼ m -1/d (23) An important thing to notice is that the convergence rate in (22) suffers from CoD: If we want to reduce the error by a factor of , we need to increase m by a factor m ∼ -d if α = 1. For d = 100 which is not very high dimension by the standards of machine learning, this means that we have to increase m by a factor of -100 . This is why polynomials and piecewise polynomials are not useful in high dimensions. Another way to appreciate this is as follows. The number of monomials of degree p in dimension d is C d p+d . This grows very fast for large values of d and p. What should we expect in high dimension? One example that we can learn from is Monte Carlo methods for integration. Consider the problem of approximating I(g) = E x∼µ g(x) using I m (g) = 1 m j g(x j ) where {x j , j ∈ [m]} is a set of i.i.d samples of the probability distribution µ. A direct computation gives E(I(g) -I m (g)) 2 = var(g) m , var(g) = E x∼µ g 2 (x) -(E x∼µ g(x)) 2 This exact relation tells us two things. (1) The convergence rate of Monte Carlo integration is independent of dimension. (2) The error constant is given by the variance of the integrand. Therefore to reduce error, one has to do variance reduction. Had we used grid-based quadrature rules, the accuracy would have also suffered from CoD. It is possible to improve the Monte Carlo rate by more sophisticated ways of choosing {x j , j ∈ [m]}, say using number-theoretic-based quadrature rules. But these typically give rise to an O(1/d) improvement for the convergence rate and it diminishes as d → ∞. Based on these considerations, we aim to find function approximations that satisfy: inf f ∈Hm R(f ) = inf f ∈Hm f -f * 2 L 2 (dµ) f * 2 * m The natural questions are then: • How can we achieve this? That is, what kind of hypothesis space should we choose? • What should be the \"norm\" • * (associated with the choice of H m )? Here we put norm in quotation marks since it does not have to be a real norm. All we need is that it controls the approximation error. Regarding the first question, let us look an illustrative example. Consider the following Fourier representation of the function f and its approximation f m (say FFT-based): f (x) = R d a(ω)e i(ω,x) dω, f m (x) = 1 m j a(ω j )e i(ω j ,x) Here {ω j } is a fixed grid, e.g. uniform grid. For this approximation, we have f -f m L 2 (X) ≤ C 0 m -α/d f H α (X) which suffers from CoD. Now consider the alternative representation f (x) = R d a(ω)e i(ω,x) π(dω) = E ω∼π a(ω)e i(ω,x) (24) where π is a probability distribution. Now to approximate f , it is natural to use Monte Carlo. Let {ω j } be an i.i.d. sample of π, f m (x) = 1 m m j=1 a(ω j )e i(ω j ,x) , then we have E|f (x) -f m (x)| 2 = var(f ) m This approximation does not suffer from CoD. Notice that f m (x) = 1 m m j=1 a j σ(ω T j x) is nothing but a two-layer neural network with activation function σ(z) = e iz (here a j = a(ω j )). We believe that this simple argument is really at the heart of why neural network models do so well in high dimension. Now let us turn to a concrete example of the kind of approximation theory for neural network models. We will consider two-layer neural networks. H m = {f m (x) = 1 m j a j σ(w T j x)}, θ = {(a j , w j ), j ∈ [m]} Consider function f : X = [0, 1] d → R of the following form f (x) = Ω aσ(w T x)ρ(da, dw) = E (a,w)∼ρ [aσ(w T x)], x ∈ X where Ω = R 1 × R d+1 , ρ is a probability distribution on Ω. Define: f B = inf ρ∈P f E ρ [a 2 w 2 1 ] 1/2 where P f := {ρ : f (x) = E ρ [aσ(w T x)]}. This is called the Barron norm [2, 12, 13] . The space 12, 13] (see also [3, 26, 14] ). B = {f ∈ C 0 : f B < ∞} is called the Barron space [2, In analogy with classical approximation theory, we can also prove some direct and inverse approximation theorem [13] . Theorem 1 (Direct Approximation Theorem). If f B < ∞, then for any integer m > 0, there exists a two-layer neural network function f m such that f -f m L 2 (X) f B √ m Theorem 2 (Inverse Approximation Theorem). Let N C def = { 1 m m k=1 a k σ(w T k x) : 1 m m k=1 |a k | 2 w k 2 1 ≤ C 2 , m ∈ N + }. Let f * be a continuous function. Assume there exists a constant C and a sequence of functions f m ∈ N C such that f m (x) → f * (x) for all x ∈ X. Then there exists a probability distribution ρ * on Ω, such that f * (x) = aσ(w T x)ρ * (da, dw), for all x ∈ X and f * B ≤ C. Estimation error Another issue we have to worry about is the performance of the machine learning model outside the training dataset. This issue also shows up in classical approximation theory. Illustrated in Figure 10 is the classical Runge phenomenon for polynomial interpolation on a uniform grid. One can see that away from the grid points, the error of the interpolant can be very large. This is a situation that we would like to avoid. What we do in practice is to minimize the training error: but we are interested in the testing error, which is a sampled version of the population risk: Rn (θ) = 1 n j (f (x j , θ) -f * (x j )) 2 R(θ) = E x∼µ (f (x, θ) -f * (x)) 2 The question is how we can control the difference between these two errors. One way of doing this is to use the notion of Rademacher complexity. The important fact for us here is that the Rademacher complexity controls the difference between training and testing errors (also called the \"generalization gap\"). Indeed, let H be a set of functions, and S = (x 1 , x 2 , ..., x n ) be a dataset. Then, up to logarithmic terms, we have sup h∈H E x [h(x)] - 1 n n i=1 h(x i ) ∼ Rad S (H) where the Rademacher complexity of H with respect to S is defined as Rad S (H) = 1 n E ξ sup h∈H n i=1 ξ i h(x i ) , (25) where {ξ i } n i=1 are i.i.d. random variables taking values ±1 with equal probability. The question then becomes to bound the Rademacher complexity of a hypothesis space. For the Barron space, we have [2] : Theorem 3. Let F Q = {f ∈ B, f B ≤ Q}. Then we have Rad S (F Q ) ≤ 2Q 2 ln(2d) n where n = |S|, the size of the dataset S. A priori estimates for regularized models Consider the regularized model L n (θ) = Rn (θ) + λ log(2d) n θ P , θn = argmin L n (θ) (26) where the path norm is defined by: θ P = 1 m m k=1 |a k | 2 w k 2 1 1/2 The following result was proved in [12] : Theorem 4. Assume f * : X → [0, 1] ∈ B. There exist constants C 0 , such that for any δ > 0, if λ ≥ C 0 , then with probability at least 1 -δ over the choice of the training dataset, we have R( θn ) f * 2 B m + λ f * B log(2d) n + log(1/δ) + log(n) n . Similar approximation theory and a priori error estimates have been proved for other machine learning models. Here is a brief summary of these results. • Random feature model: The corresponding function space is the reproducing kernel Hilbert space (RKHS). • Residual networks (ResNets): The corresponding function space is the so-called flow-induced space introduced in [13] . • Multi-layer neural networks: A candidate for the corresponding function space is the multi-layer space introduced in [15] . What is really important is the \"norms\" that control the approximation error and the generalization gap. These quantities are defined for functions in the corresponding spaces. After the approximation theorems and Rademacher complexity estimates are in place, one can readily prove a theorem of the following type for regularized models: Up to logarithmic terms, the minimizers of the regularized models satisfy: R( f ) Γ(f * ) m + γ(f * ) √ n where m is the number of free parameters, n is the size of the training dataset. Note that for the multilayer spaces, the results proved in [15] are not as sharp. We only discussed the analysis of the hypothesis space. There are many more other questions. We refer to [19] for more discussion on the current understanding of neural network-based machine learning. Machine learning from a continuous viewpoint Now we turn to alternative formulations of machine learning. Motivated by the situation for PDEs, we would like to first formulate machine learning in a continuous setting and then discretize to get concrete models and algorithms. The key here is that continuous problems that we come up with should be nice mathematical problems. For PDEs, this is accomplished by requiring them to be \"well-posed\". For problems in calculus of variations, we require the problem to be \"convex\" in some sense and lower semi-continuous. The point of these requirements is to make sure that the problem has a unique solution. Intuitively, for machine learning problems, being \"nice\" means that the variational problem should have a simple landscape. How to formulate this precisely is an important research problem for the future. As was pointed out in [16] , the key ingredients for the continuous formulation are as follows: • representation of functions (as expectations) • formulating the variational problem (as expectations) • optimization, e.g. gradient flows Representation of functions Two kinds of representations are considered in [16] : Integral transform-based representation and flow-based representation. The simplest integral-transform based representation is a generalization of (24): f (x; θ) = R d a(w)σ(w T x)π(dw) =E w∼π a(w)σ(w T x) =E (a,w)∼ρ aσ(w T x) =E u∼ρ φ(x, u) Here θ denotes the parameters in the model: θ can be a(•) or the prob distributions π or ρ. This representation corresponds to two-layer neural networks. A generalization to multi-layer neural networks is presented in [15] . Next we turn to flow-based representation: dz dτ =E w∼πτ a(w, τ )σ(w T z) (27) =E (a,w)∼ρτ aσ(w T z) (28) =E u∼ρτ φ(z, u), z(0, x) = x (29) f (x, θ) = 1 T z(1, x) In this representation, the parameter θ can be either {a τ (•)} or {π τ } or {ρ τ } The stochastic optimization problem Stochastic optimization problems are of the type: min θ E w∼ν g(θ, w) These kinds of problems can readily be approached by stochastic algorithms, which is a key component in machine learning. For example, instead of the gradient descent algorithm: θ k+1 = θ k -η∇ θ E w∼ν g(θ, w) = θ k -η∇ θ n j=1 g(θ, w j ) one can use the stochastic gradient descent: θ k+1 = θ k -η∇ θ g(θ, w k ) where {w k } is a set of random variables sampled from ν. The following are some examples of the stochastic optimization problems that arise in modern machine learning: • Supervised learning: In this case, the minimization of the population risk becomes R(f ) = E x∼µ (f (x) -f * (x)) 2 • Eigenvalue problems for quantum many-body Hamiltonian: I(φ) = (φ, Hφ) (φ, φ) = E x∼µ φ φ(x)Hφ(x) φ(x) 2 , µ φ (dx) = 1 Z |φ(x)| 2 dx Here H is the Hamiltonian of the quantum system. • Stochastic control problems: L({a t } T -1 t=0 ) = E T -1 t=0 c t (s t , a t )) + c T (s T ) Substituting the representations discussed earlier to these expressions for the stochastic optimization problems, we obtain the final variational problem that we need to solve. One can either discretize these variational problems directly and then solve the discretized problem using some optimization algorithms, or one can write down continuous forms of some optimization algorithms, typically gradient flow dynamics, and then discretize these continuous flows. We are going to discuss the second approach. Optimization: Gradient flows To write continuous form of the gradient flows, we draw some inspiration from statistical physics. Take the supervised learning as an example. We regard the population risk as being the \"free energy\", and following Halperin and Hohenberg [24] , we divide the parameters into two kinds, conserved and non-conserved. For example, a is a non-conserved parameter and π is conserved since its total integral has to be 1. For non-conserved parameter, as was suggested in [24] , one can use the \"model A\" dynamics: ∂a ∂t = -δR δa which is simply the usual L 2 gradient flow. For conserved parameters such as π, one should use the \"model B\" dynamics which works as follows: First define the \"chemical potential\" V = δR δπ . From the chemical potential, one obtains the velocity field v and the current J: J = πv, v = -∇V The continuity equation then gives us the gradient flow dynamics: ∂π ∂t + ∇ • J = 0. This is also the gradient flow under the Wasserstein metric. Discretizing the gradient flows To obtain practical models, one needs to discretize these continuous problems. The first step is to replace the population risk by the empirical risk using the training data. The more non-trivial issue is how to discretize the gradient flows in the parameter space. The parameter space has the following characteristics: (1) It has a simple geometry -unlike the real space which may have a complicated geometry. (2) It is also usually high dimensional. For these reasons, the most natural numerical methods for the discretization in the parameter space is the particle method which is the dynamic version of Monte Carlo. Smoothed particle method might be helpful to improve the performance. In relatively low dimensions, one might also consider the spectral method, particularly some sparse version of the spectral method, due to the relative simple geometry of the parameter space. Take for example the discretization of the conservative flow for the integral transformbased representation. With the representation: f (x; θ) = E (a,w)∼ρ aσ(w T x), the gradient flow equation becomes: ∂ t ρ = ∇(ρ∇V ), V = δR δρ (30) 2.0 2.5 3.0 3.5 4.0 4.5 log 10 (m) 1.8 2.0 2.2 2.4 2.6 log 10 (n) Test errors 2.0 2.5 3.0 3.5 4.0 4.5 log 10 (m) 1.8 2.0 2.2 2.4 2.6 log 10 (n) Test errors 6.0 5.4 4.8 4.2 3.6 3.0 2.4 1.8 1.2 0.6 Figure 11: (Left) continuous viewpoint; (Right) conventional NN models. Target function is a single neuron. Reproduced with permission from Lei Wu. The particle method discretization is based on: ρ(a, w, t) ∼ 1 m j δ (a j (t),w j (t)) = 1 m j δ u j (t) where u j (t) = (a j (t), w(t)). One can show that in this case, (30) reduces to du j dt = -∇ u j I(u 1 , • • • , u m ) where I(u 1 , • • • , u m ) = R(f m ), u j = (a j , w j ), f m (x) = 1 m j a j σ(w T j x) This is exactly gradient descent for \"scaled\" two-layer neural networks. In this case, the continuous formulation also coincides with the \"mean-field\" formulation for two-layer neural networks [7, 32, 34, 35] . The scaling factor 1/m in front of the \"scaled\" two-layer neural networks is actually quite important and makes a big difference for the test performance of the network. Shown in Figure 11 is the heat map of the test error for two-layer neural network models with and without this scaling factor. The target function is the simple single neuron function: f * (x) = σ(x 1 ). The important observation is that in the absence of this scaling factor, the test error shows a \"phase transition\" between a phase where the neural network model performs like an associated random feature model and another phase where it shows much better performance than the random feature model. Such a phase transition is one of the reasons that choosing the right set of hyper-parameters, here the network width m, is so important. However, if one uses the scaled form, this phase transition phenomenon is avoided and the performance is more robust [31] . The optimal control problem for flow-based representation The flow-based representation naturally leads to a control problem. This viewpoint has been used explicitly or implicitly in machine learning for quite some time (see for example [27] ). The back propagation algorithm, for example, is an example of control-theory based algorithm. Another more recent example is the development of maximum principle-based algorithm, first introduced in [28] . Despite these successes, we feel that there is still a lot of room for using the control theory viewpoint to develop new algorithms. We consider the flow-based representation in a slightly more general form dz dτ = E u∼ρτ φ(z, u), z(0, x) = x where z is the state, ρ τ is the control at time τ . Our objective is to minimize R over {ρ τ } R({ρ τ }) = E x∼µ (f (x) -f * (x)) 2 = R d (f (x) -f * (x)) 2 dµ where as before f (x) = 1 T z(1, x) (31) One most important result for this control problem is Pontryagin's maximum principle (PMP). To state this result, let us define the Hamiltonian H : R d × R d × P 2 (Ω) : → R as H(z, p, µ) = E u∼µ [p T φ(z, u)]. Pontryagin's maximum principle asserts that the solutions of the control problem must satisfy: ρ τ = argmax ρ E x [H z t,x τ , p t,x τ , ρ ], ∀τ ∈ [0, 1], (32) where for each x, {(z t,x τ , p t,x τ )} are defined by the forward/backward equations: dz t,x τ dτ = ∇ p H = E u∼ρτ (•;t) [φ(z t,x τ , u)] dp t,x τ dτ = -∇ z H = E u∼ρτ (•;t) [∇ T z φ(z t,x τ , u)p t,x τ ], (33) with the boundary conditions: z t,x 0 = x (34) p t,x 1 = -2(f (x; ρ(•; t)) -f * (x))1. Pontryagin's maximum principle is slightly stronger than the KKT condition for the stationary points in that ( 32 ) is a statement of optimality rather than criticality. In fact (32) also holds when the parameters are discrete and this has been used in [29] to develop efficient numerical algorithms for this case. With the help of PMP, it is also easy to write down the gradient descent flow for the optimization problem. Formally, one can simply write down the gradient descent flow for (32) for each τ : ∂ t ρ τ (u, t) = ∇ • (ρ τ (u, t)∇V (u; ρ)) , ∀τ ∈ [0, 1], (36) where V (u; ρ) = E x [ δH δρ z t,x τ , p t,x τ , ρ τ (•; t) ], and {(z t,x τ , p t,x τ )} are defined as before by the forward/backward equations. To discretize the gradient flow, we can simply use: See also [28] . Figure 12 shows the results of the extended MSA compared with different versions of SGD for two kinds of initialization. One can see that in terms of the number of iterations, extended MSA outperforms all the SGDs. In terms of wall clock time, the advantage of the extended MSA is diminished significantly. This is possibly due to the inefficiencies in the implementation of the optimization algorithm (here the BFGS) used for solving (32) . We refer to [28] for more details. In any case, it is clear that there is a lot of room for improvement. Concluding remarks In conclusion, we have discussed a wide range of problems for which machine learningbased algorithms have made and/or will make a significant difference. These problems are relatively new to computational mathematics. We believe strongly that machine learningbased algorithms will also significantly impact the way we solve more traditional problems in computational mathematics. However, research in this direction is still at a very early stage. Another important area that machine learning can be of great help is multi-scale modeling. The moment-closure problem discussed above is an example in this direction. There are many more possible applications, see [8] . Machine learning seems to be able to provide the missing link in making advanced multi-scale modeling techniques really practical. For example in the heterogeneous multi-scale method (HMM) [1, 9] , one important component is to extract the relevant macro-scale information from micro-scale simulation data. This step has always been a major obstacle in HMM. It seems quite clear that machine learning techniques can of great help here. We also discussed how the viewpoint of numerical analysis can help to improve the mathematical foundation of machine learning as well as propose new and possibly more robust formulations. In particular, we have given a taste of how high dimensional approximation theory should look like. We also demonstrated that commonly used machine learning models and training algorithms can be recovered from some particular discretization of continuous models, in a scaled form. From this discussion, one can see that neural network models are quite natural and rather inevitable. What have we really learned from machine learning? Well, it seems that the most important new insight from machine learning is the representation of functions as expectations. We reproduce them here for convenience: • integral-transform based: f (x) = E (a,w)∼ρ aσ(w T x) f (x) = E θ L ∼π L a (L) θ L σ(E θ L-1 ∼π L-1 . . . σ(E θ 1 ∼π 1 a 1 θ 2 ,θ 1 σ(a 0 θ 1 • x)) . . . ) • flow-based: dz dτ =E (a,w)∼ρτ aσ(w T z), z(0, x) = x (40) f (x, θ) =1 T z(1, x) (41) From the viewpoint of computational mathematics, this suggests that the central issue will move from specific discretization schemes to more effective representations of functions. This review is rather sketchy. Interested reader can consult the three review articles [17, 18, 19] for more details. Figure 6 : 6 Figure 6: Relative reward for the energy storage problem. The space of control function is R n+2 → R 3n for n = 30, 40, 50, with multiple equality and inequality constrains.Reproduced with permission from Jiequn Han. See also [20] . Figure 7 : 7 Figure 7: Left: Relative error of the deep BSDE method for u(t=0, x=(0, . . . , 0)) when λ = 1, which achieves 0.17% in a runtime of 330 seconds. Right: Optimal cost u(t=0, x=(0, . . . , 0)) against different λ.Reproduced with permission from Jiequn Han. See also [21] . Figure 8 : 8 Figure 8: The solution of the Black-Scholes equation with default risk at d = 100. The Deep BSDE method achieves a relative error of size 0.46% in a runtime of 617 seconds.Reproduced with permission from Jiequn Han. See also [21] . Figure 9 : 9 Figure9: The different regimes of gas dynamics. Reproduced with permission from Jiequn Han. See also [22] . Figure 10 : 10 Figure 10: The Runge phenomenon: f * (x) = 1 1+25x 2 . Reproduced with permission from Chao Ma. Figure 12 : 12 Figure 12: Comparison of the extended MSA with different versions of stochastic gradient descent algorithms. The top figures show results for small initialization. The bottom figures show results for bigger initialization. Reproduced with permission from Qianxiao Li.See also [28] ."
}