{
  "title": "Automated Graph Machine Learning: Approaches, Libraries, Benchmarks and Directions",
  "abstract": "Graph machine learning has been extensively studied in both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To tackle the challenge, automated graph machine learning, which aims at discovering the best hyper-parameter and neural architecture configuration for different graph tasks/data without manual design, is gaining an increasing number of attentions from the research community. In this paper, we extensively discuss automated graph machine learning approaches, covering hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We briefly overview existing libraries designed for either graph machine learning or automated machine learning respectively, and further in depth introduce AutoGL, our dedicated and the world's first open-source library for automated graph machine learning. Also, we describe a tailored benchmark that supports unified, reproducible, and efficient evaluations. Last but not least, we share our insights on future research directions for automated graph machine learning. This paper is the first systematic and comprehensive discussion of approaches, libraries as well as directions for automated graph machine learning.",
  "introduction": "INTRODUCTION G RAPH data is ubiquitous in our daily life. We can use graphs to model the complex relationships and dependencies between entities ranging from small molecules in proteins and particles in physical simulations to large national-wide power grids and global airlines. Therefore, graph machine learning, i.e., machine learning on graphs, has long been an important research direction for both academics and industry [1] . In particular, network embedding [2] , [3] , [4] , [5] and graph neural networks (GNNs) [6] , [7] , [8] have drawn increasing attention in the last decade. They are successfully applied to recommendation systems [9] , [10] , [11] , [12] , information retrieval [13] , [14] , [15] , [16] , fraud detection [17] , bioinformatics [18] , [19] , physical simulation [20] , traffic forecasting [21] , [22] , knowledge representation [23] , drug re-purposing [24] , [25] and pandemic prediction [26] for Covid-19. Despite the popularity of graph machine learning algorithms, the existing literature heavily relies on manual hyper-parameter or architecture design to achieve the best performance, resulting in costly human efforts when a vast number of models emerge for various graph tasks. Take GNNs as an example, at least one hundred new general-purpose architectures have been published in top-tier machine learning and data mining conferences in the year of 2021 alone, not to mention cross-disciplinary researches of task-specific designs. More and more human efforts are inevitably needed if we stick to the manual try-and-error paradigm in designing the optimal algorithms for targeted tasks. On the other hand, automated machine learning (AutoML) has been extensively studied to reduce human efforts in developing and deploying machine learning models [27] , [28] . Complete AutoML pipelines have the potential to automate every step of machine learning, including auto data collection and cleaning, auto feature engineering, and auto model selection and optimization, etc. Due to the popularity of deep learning models, hyperparameter optimization (HPO) [29] , [30] , [31] , [32] and neural architecture search (NAS) [33] , [34] are most widely studied. AutoML has achieved or surpassed human-level performance [35] , [36] , [37] with little human guidance in areas such as computer vision [38] , [39] . Automated graph machine learning, combining advantages of AutoML and graph machine learning, naturally serves as a promising research direction to further boost the model performance, which has attracted an increasing number of interests from the community. In this paper, we provide a systematic overview of approaches for automated graph machine learning 1 , introduce related public libraries as well as our AutoGL, the world's first open-source library for automated graph machine learning, describe a tailored benchmark that supports unified, reproducible, and efficient evaluations, and share our insights on challenges and future research directions. Particularly, we focus on two major topics: HPO and NAS of graph machine learning. For HPO, we focus on how to de-velop scalable methods. For NAS, we follow the literature and compare different methods from search spaces, search strategies, and performance estimation strategies. We also briefly discuss several recent automated graph learning works that feature in different aspects such as architecture pooling, structure learning, accelerator and joint software-hardware design etc. Besides, how different methods tackle the challenges of AutoML on graphs are discussed along the way as well. Then, we review libraries related to automated graph machine learning and discuss AutoGL, the first dedicated framework and open-source library for automated graph machine learning. We highlight the design principles of AutoGL and briefly introduce its usages, which are all specially designed for AutoML on graphs. Last but not least, we point out the potential research directions for both graph HPO and graph NAS, including but not limited to Scalability, Explainability, Outof-distribution generalization, Robustness, and Hardware-aware design etc. We believe this paper will greatly facilitate and further promote the studies and applications of automated graph machine learning in both academia and industry. The rest of the paper is organized as follows. In Section 2, we intoduce the fundamentals and preliminaries for automated graph machine learning by briefly introducing basic formulations of graph machine learning and AutoML. We comprehensively discuss HPO based approaches on graph machine learning in Section 3 and NAS based methods for graph machine learning in Section 4. Then, in Section 5.1, we overview related libraries for graph machine learning and automated machine learning and in depth introduce AutoGL, our dedicated and the world's first open-source library tailored for automated graph machine learning. We discuss the tailored benchmark that enables fair, fully reproducible, and efficient empirical comparisons in Section 6. Last but not least, we outline future research opportunities in Section 7 and conclude the whole paper in Section 8.",
  "body": "INTRODUCTION G RAPH data is ubiquitous in our daily life. We can use graphs to model the complex relationships and dependencies between entities ranging from small molecules in proteins and particles in physical simulations to large national-wide power grids and global airlines. Therefore, graph machine learning, i.e., machine learning on graphs, has long been an important research direction for both academics and industry [1] . In particular, network embedding [2] , [3] , [4] , [5] and graph neural networks (GNNs) [6] , [7] , [8] have drawn increasing attention in the last decade. They are successfully applied to recommendation systems [9] , [10] , [11] , [12] , information retrieval [13] , [14] , [15] , [16] , fraud detection [17] , bioinformatics [18] , [19] , physical simulation [20] , traffic forecasting [21] , [22] , knowledge representation [23] , drug re-purposing [24] , [25] and pandemic prediction [26] for Covid-19. Despite the popularity of graph machine learning algorithms, the existing literature heavily relies on manual hyper-parameter or architecture design to achieve the best performance, resulting in costly human efforts when a vast number of models emerge for various graph tasks. Take GNNs as an example, at least one hundred new general-purpose architectures have been published in top-tier machine learning and data mining conferences in the year of 2021 alone, not to mention cross-disciplinary researches of task-specific designs. More and more human efforts are inevitably needed if we stick to the manual try-and-error paradigm in designing the optimal algorithms for targeted tasks. On the other hand, automated machine learning (AutoML) has been extensively studied to reduce human efforts in developing and deploying machine learning models [27] , [28] . Complete AutoML pipelines have the potential to automate every step of machine learning, including auto data collection and cleaning, auto feature engineering, and auto model selection and optimization, etc. Due to the popularity of deep learning models, hyperparameter optimization (HPO) [29] , [30] , [31] , [32] and neural architecture search (NAS) [33] , [34] are most widely studied. AutoML has achieved or surpassed human-level performance [35] , [36] , [37] with little human guidance in areas such as computer vision [38] , [39] . Automated graph machine learning, combining advantages of AutoML and graph machine learning, naturally serves as a promising research direction to further boost the model performance, which has attracted an increasing number of interests from the community. In this paper, we provide a systematic overview of approaches for automated graph machine learning 1 , introduce related public libraries as well as our AutoGL, the world's first open-source library for automated graph machine learning, describe a tailored benchmark that supports unified, reproducible, and efficient evaluations, and share our insights on challenges and future research directions. Particularly, we focus on two major topics: HPO and NAS of graph machine learning. For HPO, we focus on how to de-velop scalable methods. For NAS, we follow the literature and compare different methods from search spaces, search strategies, and performance estimation strategies. We also briefly discuss several recent automated graph learning works that feature in different aspects such as architecture pooling, structure learning, accelerator and joint software-hardware design etc. Besides, how different methods tackle the challenges of AutoML on graphs are discussed along the way as well. Then, we review libraries related to automated graph machine learning and discuss AutoGL, the first dedicated framework and open-source library for automated graph machine learning. We highlight the design principles of AutoGL and briefly introduce its usages, which are all specially designed for AutoML on graphs. Last but not least, we point out the potential research directions for both graph HPO and graph NAS, including but not limited to Scalability, Explainability, Outof-distribution generalization, Robustness, and Hardware-aware design etc. We believe this paper will greatly facilitate and further promote the studies and applications of automated graph machine learning in both academia and industry. The rest of the paper is organized as follows. In Section 2, we intoduce the fundamentals and preliminaries for automated graph machine learning by briefly introducing basic formulations of graph machine learning and AutoML. We comprehensively discuss HPO based approaches on graph machine learning in Section 3 and NAS based methods for graph machine learning in Section 4. Then, in Section 5.1, we overview related libraries for graph machine learning and automated machine learning and in depth introduce AutoGL, our dedicated and the world's first open-source library tailored for automated graph machine learning. We discuss the tailored benchmark that enables fair, fully reproducible, and efficient empirical comparisons in Section 6. Last but not least, we outline future research opportunities in Section 7 and conclude the whole paper in Section 8. FUNDAMENTALS AND PRELIMINARIES OF AUTO-MATED GRAPH MACHINE LEARNING We briefly present basic problem formulations for graph machine learning, automated machine learning as well as unique characteristics for automated graph machine learning before moving to the next section. Graph Machine Learning Consider a graph G = (V, E) where V = v 1 , v 2 , ..., v |V| is a set of nodes and E ⊆ V ×V is a set of edges. The neighborhood of node v i is denoted as N (i) = {v j : (v i , v j ) ∈ E}. The nodes can also have features denoted as F ∈ R |V|×f , where f is the number of features. We use bold uppercases (e.g., X) and bold lowercases (e.g., x) to represent matrices and vectors, respectively. Most tasks of graph machine learning can be divided into the following two categories: • Node-level tasks: the tasks are associated with individual nodes or pairs of nodes. Typical examples include node classification and link prediction. • Graph-level tasks: the tasks are associated with the whole graph, such as graph classification and graph generation. For node-level tasks, graph machine learning models usually learn a node representation H ∈ R |V|×d and then adopt a classifier or predictor on the node representation to solve the task. For graphlevel tasks, a representation for the whole graph is learned and fed into a classifier/predictor. GNNs are the current state-of-the-art in learning node and graph representations. The message-passing framework of GNNs [40] is formulated as follows. m (l) i = AGG (l) a (l) ij W (l) h (l) i , ∀j ∈ N (i) (1) h (l+1) i = σ COMBINE (l) m (l) i , h (l) i , (2) where h (l) i denotes the node representation of node v i in the l th layer, m (l) is the message for node v i , AGG (l) (•) is the aggregation function, a (l) ij denotes the weights from node v j to node v i , COMBINE (l) (•) is the combining function, W (l) are learnable weights, and σ(•) is an activation function. The node representation is usually initialized as node features H (0) = F, and the final representation is obtained after L message-passing layers H = H (L) . For the graph-level representation, pooling methods (also called readout) are applied to the node representations h G = POOL (H) , (3) i.e., h G is the representation of G. Automated Machine Learning (AutoML) Many AutoML algorithms such as HPO and NAS can be formulated as the following bi-level optimization problem: min α∈A L val (W * (α), α) s.t. W * (α) = arg min W (L train (W, α)) , (4) where α is the optimization objective of the AutoML algorithm, e.g., hyper-parameters in HPO and neural architectures in NAS, A is the feasible space for the objective, and W(α) are trainable weights in the graph machine learning models. Essentially, we aim to optimize the objective in the feasible space so that the model achieves the best results in terms of a validation function, and W * indicates that the weights are fully optimized in terms of a training function. Different AutoML methods differ in how the feasible space is designed and how the objective functions are instantiated and optimized since directly optimizing Eq. ( 4 ) requires enumerating and training every feasible objective, which is prohibitive in practice. Typical formulations of automated graph machine learning need to properly integrate the above formulations in Section 2.1 and Section 2.2 to form a new optimization problem. Automated Graph Machine Learning Automated graph machine learning, which non-trivially combines the strength of AutoML and graph machine learning, faces the following challenges. • The uniqueness of graph machine learning: Unlike audio, image, or text, which has a grid structure, graph data lies in a non-Euclidean space [41] . Thus, graph machine learning usually has unique architectures and designs. For example, typical NAS methods focus on the search space for convolution and recurrent operations, which is distinct from the building blocks of GNNs [42] . • Complexity and diversity of graph tasks: As aforementioned, graph tasks per se are complex and diverse, ranging from node-level to graph-level problems, and with different settings, objectives, and constraints [43] . How to impose proper inductive bias and integrate domain knowledge into a graph AutoML method is indispensable. • Scalability: Many real graphs such as social networks or the Web are incredibly large-scale with billions of nodes and edges [44] . Besides, the nodes in the graph are interconnected and cannot be treated as independent samples. Designing scalable AutoML algorithms for graphs poses significant challenges since both graph machine learning and AutoML are already notorious for being compute-intensive. Approaches with HPO or NAS for graph machine learning reviewed in later sections target at handling at least one of these three challenges. As such, we will discuss approaches for automated graph machine learning from two aspects: i) HPO for graph machine learning and ii) NAS for graph machine learning. HPO FOR GRAPH MACHINE LEARNING In this section, we review HPO for graph machine learning. The main challenge here is scalability, i.e., a real graph can have billions of nodes and edges, and each trial on the graph is computationally expensive. Next, we elaborate on how different methods tackle the efficiency challenge. Notice that we omit some straightforward HPO methods such as random search and grid search [29] since they are applied to graphs without any modification. Tu et al. [45] propose AutoNE, the first HPO method specially designed to tackle the efficiency problem of graphs, to facilitate the graph hyper-parameter optimization for large-scale graph representation learning. AutoNE proposes a transfer paradigm that samples subgraphs as proxies for the large graph. Specifically, AutoNE has three modules: the sampling module, the signature extraction module, and the meta-learning module. In the sampling module, multiple representative subgraphs are sampled from the large graph using a multi-start random walk strategy. Each subgraph learns a representation by the signature extraction module. Then, AutoNE conducts HPO on the sampled subgraphs using Bayesian optimization [31] and records the results. Finally, using the HPO results and representation of subgraphs to extract metaknowledges, AutoNE fine-tunes hyper-parameters on the large graph using the meta-learning module. In this way, AutoNE achieves satisfactory results while maintaining scalability since the knowledge of multiple HPO trials on the sampled subgraphs and a few HPO trails on the large graph are properly integrated. Wang et al. [46] propose e-AutoGR to further increase the explainability of hyper-parameter optimization for automated graph representation learning, with the help of hyper-parameter importance decorrelation. e-AutoGR employs six fully explainable graph features, i.e., number of nodes, number of edges, number of triangles, global clustering coefficient, maximum total degree value and number of components, as measures for similarity between different graphs. A hyper-parameter decorrelation algorithm (HyperDeco) is proposed to decorrelate the mixed relations among different hyper-parameters given various graph features so that more accurate importance of different hyper-parameters towards model performances can be estimated through any regression approaches. The authors theoretically validate the correctness of the proposed hyper-parameter decorrelation algorithm and empirically discover that first-order proximity is most important for AROPE [47] , number of walks together with window size is of great importance for DeepWalk [48] , and dropout is particularly important for GCN [49] . Guo et al. [50] propose ITuNE to replace the sampling process of AutoNE with graph coarsening to generate a hierarchical graph synopsis. A similarity measurement module is also proposed to ensure that the coarsened graph shares sufficient similarity with the large graph. Compared with sampling, such graph synopsis can better preserve graph structural information. Therefore, JITuNE argues that the best hyper-parameters in the graph synopsis can be directly transferred to the large graph. Besides, since the graph synopsis is generated in a hierarchy, the granularity can be more easily adjusted to meet the time constraints of downstream tasks. Yuan et al. [51] propose HESGA as another strategy to improve efficiency using a hierarchical evaluation strategy together with evolutionary algorithms. Specifically, HESGA proposes to evaluate the potential of hyper-parameters by interrupting training after a few epochs and calculating the performance gap with respect to the initial performance with random model weights. This gap is used as a fast score to filter out unpromising hyperparameters. Then, the standard full evaluation, i.e., training until convergence, is adopted as the final assessor to select the best hyper-parameters to be stored in the population of the evolutionary algorithm. Besides efficiency, Yoon et al. [52] propose AutoGM to focuses on studying a unified framework for various graph machine learning algorithms. Specifically, AutoGM finds that many popular GNNs and PageRank can be characterized in a framework similar to Eq. ( 1 ) with five hyper-parameters: the number of messagepassing neighbors, the number of message-passing steps, the aggregation function, the dimensionality, and the non-linearity. AutoGM also adopts Bayesian optimization to optimize these hyper-parameters. Yuan et al. [53] focus on the impact of selecting two types of GNN hyper-parameters (i.e., graph-related layers and task-specific layers) on the performance of GNN for molecular property prediction. They employed CMA-ES for HPO, which is a derivativefree and evolutionary black-box optimization method. The results reveal that optimizing the two types of hyper-parameters separately can result in improvement on GNN performance, and removing any of the two types of hyper-parameters may result in deteriorated performance. Even doing this means a larger search space, which seems to be more challenging given the same number of trials (limited computational resources), such a strategy can surprisingly achieve better performance. Meanwhile, their study further confirms the importance of HPO for GNNs in molecular property prediction problems. Many molecular datasets are far smaller than other datasets in typical deep learning applications. Most HPO methods have not been explored in terms of their performances on these small datasets in molecular domain. Yuan et al. [54] conduct a theoretical analysis of common and specific features for two stateof-the-art HPO algorithms: i.e., TPE and CMA-ES, and they compare them with random search (RS). Experimental studies are carried out on several benchmarks in MoleculeNet, from different perspectives to investigate the impact of RS, TPE, and CMA-ES on HPO of GNNs for molecular property prediction. Their experimental results indicate that TPE is the most suited HPO method for GNN under molecular property prediction problems with limited computational resources. Meanwhile, RS is the simplest method capable of achieving comparable performance with TPE and CMA-ES. GCN models are sensitive to the choice of hyper-parameters such as dropout rate and learning weight decay [46] , especially for deep GCN models. Zhu et al. [55] therefore target at automating the training of GCN models through hyper-parameter optimization. To be specific, they propose a self-tuning GCN (ST-GCN) approach by incorporating hypernets in each graph convolutional layer, enabling the joint optimization over GCN model parameters and hyper-parameters. They further extend the approach through incorporating the population based training scheme and adopt a population based training framework to self-tuning GCN, thus alleviating local minima problem via exploring hyper-parameter space globally. Experimental results on three benchmark datasets demonstrate the effectiveness of their approaches in terms of optimizing multi-layer GCNs. Bu et al. [56] analyze the performance of different evolutionary algorithms on automated graph machine learning through experimental study. The experimental results show that evolutionary algorithms can serve as an effective alternative to the traditional hyper-parameter optimization algorithms such as random search, grid search and Bayesian Optimization for GNN. Sun et al. [57] propose AutoGRL, an automated graph representation learning framework for node classification task. Au-toGRL consists of an appropriate search space with four components: data augmentation, feature engineering, hyper-parameter optimization, and architecture search. Given graph data, AutoGRL searches for the best graph representation learning model in the search space using an efficient searching algorithm. Extensive experiments are conducted on four real-world node classification datasets to demonstrate that AutoGRL can automatically find competitive graph representation learning models on specific graph data effectively and efficiently. Yang et al. [58] address the underexplored issue of obtaining reliable and trustworthy predictions using automated Graph Neural Networks (GNNs). It integrates uncertainty estimation into the Hyperparameter Optimization (HPO) problem through a bilevel formulation in a novel model named HyperU-GCN. The upperlevel problem focuses on reasoning uncertainties by developing a probabilistic hypernetwork through a variational Bayesian approach. The lower-level problem targets how the weights in the Graph Convolutional Network (GCN) respond to a distribution of hyperparameters. By incorporating model uncertainty into the hyperparameter space, HyperU-GCN is able to achieve calibrated predictions, similar to Bayesian model averaging over hyperparameters. Experimental results on six public datasets indicate that this approach outperforms several state-of-the-art methods in terms of node classification accuracy and expected calibration error (ECE). Lloyd et al. [59] focus on the challenges of embedding knowledge graphs into low-dimensional spaces, a process that is computationally expensive largely due to hyperparameter optimization. They introduce a novel approach using Sobol sensitivity analysis to evaluate the significance of different hyperparameters in affecting the quality of the embeddings. Through thousands of trials and subsequent regression analysis, they identify considerable variability in the importance of different hyperparameters across various knowledge graphs. This variability is attributed to differences in dataset characteristics. Additionally, the paper makes a unique contribution by identifying data leakage issues in the UMLS knowledge graph and presenting a leakage-robust variant, termed UMLS-43. Yoon et al. [60] address the challenge of selecting the most suitable graph algorithm for specific real-world applications due to the proliferation of algorithms with different problem formu-lations, computational times, and memory footprints. To resolve this, they propose AUTOGM, an automated system for graph mining algorithm development. The paper introduces a unified framework, UNIFIEDGM, which simplifies the search space for graph algorithms by requiring only five parameters for algorithm determination. AUTOGM then uses Bayesian Optimization to find the optimal parameter set for UNIFIEDGM. To assess algorithmic efficacy within a given computational budget, the authors introduce a novel budget-aware objective function. Tests on various real-world datasets show that AUTOGM generates novel graph algorithms that offer the best speed-accuracy trade-off compared to existing models. Zhang et al. [61] address the issue of inefficient hyperparameter (HP) tuning in the context of knowledge graph (KG) learning. The authors first conduct a thorough analysis of different hyper-parameters and their transferability from smaller subgraphs to full graphs. Based on these insights, they introduce a two-stage search algorithm called KGTuner. In the first stage, the algorithm efficiently explores hyper-parameter configurations using small subgraphs. In the second stage, the best-performing configurations are fine-tuned on the full, large-scale graph. Experimental results demonstrate that KGTuner outperforms baseline algorithms, achieving an average relative improvement of 9.1% across four different embedding models when applied to large-scale KGs in the open graph benchmark. Yang et al. [62] present a systematic analysis of the impact of hyperparameters on both factorization-based and graph-samplingbased graph embedding techniques for homogeneous graphs. The authors design generalized techniques that include a wide range of hyperparameters and conduct an exhaustive experimental study with over 3,000 trained embedding models per dataset. The findings reveal that optimal hyperparameter settings, rather than the complexity of the embedding models, largely account for performance gains. The study shows that well-tuned hyperparameters can outperform a collection of 18 state-of-the-art graph embedding models by a margin of 0.30-35.41% across various tasks. Importantly, the paper notes that there is no universal set of hyperparameters that are optimal for all tasks, but offers taskspecific recommendations for hyperparameter settings, which can serve as valuable guidelines for future research in embeddingbased graph analyses. NAS FOR GRAPH MACHINE LEARNING NAS methods can be compared in three aspects [33] : search space, search strategy, and performance estimation strategy. Next, we review NAS methods for graph machine learning from these three aspects and discuss some designs uniquely for graphs. We mainly review NAS for GNNs fitting Eq. ( 1 ), which is the focus of the literature. We summarize the characteristics of different methods in Table 1 . Search Space The first challenge of NAS on graphs is the search space design since the building blocks of graph machine learning are usually distinct from other deep learning models such as CNNs or RNNs. For GNNs, the search space can be divided into the following five categories. Micro search space Following the message-passing framework in Eq. ( 1 ), the micro search space defines how nodes exchange messages with ✓ ✓ ✗ ✗ Fixed ✓ ✗ RNN controller + RL --AGNN [63] ✓ ✗ ✗ ✗ Fixed ✓ ✗ Self-designed controller + RL Inherit Weights -SNAG [64] ✓ ✓ ✗ ✗ Fixed ✓ ✗ RNN controller + RL Inherit Weights Simplify the micro search space PDNAS [65] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot -NAS-GNN [66] ✓ ✗ ✗ ✓ Fixed ✓ ✗ Evolutionary algorithm --AutoGraph [67] ✓ ✓ ✗ ✓ Various ✓ ✗ Evolutionary algorithm --GeneticGNN [68] ✓ ✗ ✗ ✓ Fixed ✓ ✗ Evolutionary algorithm --EGAN [69] ✓ ✓ ✗ ✗ Fixed ✓ ✓ Differentiable One-shot Sample small graphs for efficiency NAS-GCN [70] ✓ ✓ ✓ ✗ Fixed ✗ ✓ Evolutionary algorithm -Handle edge features LPGNAS [71] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Search for quantization options GraphGym [72] ✓ ✓ ✗ ✓ Various ✓ ✓ Random search -Transfer across datasets and tasks SGAS [73] ✓ ✗ ✗ ✗ Fixed ✓ ✓ Self-designed algorithm --Peng et al. [74] ✓ ✗ ✗ ✗ Fixed ✗ ✓ CEM-RL [75] -Search spatial-temporal modules GNAS [76] ✓ ✓ ✗ ✗ Various ✓ ✓ Differentiable One-shot -AutoSTG [77] ✗ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot+meta learning Search spatial-temporal modules DSS [78] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Dynamically update search space SANE [79] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot -AutoAttend [80] ✓ ✓ ✗ ✗ Fixed ✓ ✓ Evolutionary algorithm One-shot Cross-layer attention DiffMG [81] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Support heterogeneous graphs DeepGNAS [82] ✓ ✓ ✗ ✗ Various ✓ ✗ Controller +RL -Alleviate over-smoothing LLC [83] ✗ ✓ ✗ ✗ Various ✓ ✗ Differentiable One-shot -FL-AGCNS [84] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Evolutionary algorithm One-shot Federated learning setting G-Cos [85] ✓ ✗ ✗ ✗ Fixed ✓ ✗ Evolutionary algorithm One-shot Accelerator search PAS [86] ✓ ✓ ✓ ✗ Fixed ✗ ✓ Differentiable One-shot -FGNAS [87] ✓ ✗ ✗ ✗ Fixed ✓ ✗ RNN controller +RL -Software-hardware co-design GraphPAS [88] ✓ ✗ ✗ ✗ Fixed ✓ ✗ Evolutionary algorithm Sharing population Parallel search ALGNN [89] ✓ ✓ ✗ ✓ Various ✓ ✗ MOPSO [90] -Consider consumption cost EGNAS [91] ✓ ✓ ✗ ✗ Fixed ✓ ✓ Differentiable One-shot Handle edge features AutoGEL [92] ✓ ✓ ✓ ✗ Fixed ✓ ✓ SNAS [93] One-shot Handle edge features GASSO [94] ✓ ✗ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Graph structure learning G-RNA [95] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Evolutionary algorithm One-shot Search with robustness metrics GRACES [96] ✓ ✗ ✓ ✗ Fixed ✗ ✓ Differentiable One-shot Handle distribution shifts GAUSS [97] ✓ ✗ ✗ ✗ Fixed ✓ ✗ Evolutionary algorithm One-shot Handle large-scale graphs PasCa [98] ✓ ✗ ✗ ✗ Various ✓ ✗ Bayesian Optimization -Decouple neural message passing PMMM [99] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Search meta paths DHGAS [100] ✓ ✓ ✗ ✗ Fixed ✓ ✗ Differentiable One-shot Search spatial-temporal modules a const ij = 1 GCN a gcn ij = 1 √ |N (i)||N (j)| GAT a gat ij = LeakyReLU (ATT (Wa [h i , h j ])) SYM-GAT a sym ij = a gat ij + a gat ji COS a cos ij = cos (Wah i , Wah j ) LINEAR a lin ij = tanh (sum (Wah i + Wah j )) GENE-LINEAR a gene ij = tanh (sum (Wah i + Wah j )) W ′ a others in each layer. Commonly adopted micro search spaces [42] , [63] compose the following components: • Aggregation function AGG(•): SUM, MEAN, MAX, and MLP. • Aggregation weights a ij : common choices are listed in Table 2. • Number of heads when using attentions: 1, 2, 4, 6, 8, 16, etc. • Combining function COMBINE(•): CONCAT, ADD, and MLP. • Dimensionality of h l : 8, 16, 32, 64, 128, 256, 512, etc. • Non-linear activation function σ(•): Sigmoid, Tanh, ReLU, Identity, Softplus, Leaky ReLU, ReLU6, and ELU. However, directly searching all these components results in thousands of possible choices in a single message-passing layer. Thus, it may be beneficial to prune the space to focus on a few crucial components depending on applications and domain knowledge [64] . Macro search space Similar to residual connections and dense connections in CNNs, node representations in one layer of GNNs do not necessarily solely depend on the immediate previous layer [101] , [102] . These connectivity patterns between layers form the macro search space. Formally, such designs are formulated as H (l) = j<l F jl H (j) , (5) where F jl (•) can be the message-passing layer in Eq. ( 1 ), ZERO (i.e., not connecting), IDENTITY (e.g., residual connections), or an MLP. Since the dimensionality of H (j) can vary, IDENTITY can only be adopted if the dimensionality of each layer matches. Pooling methods To handle graph-level tasks, information from all the nodes are aggregated to form graph-level representations using the pooling operation in Eq. (3). Jiang et al. [70] propose a pooling search space including row-or column-wise sum, mean, or maximum, attention pooling, attention sum, and flatten. More advanced methods such as hierarchical pooling [103] could also be added to the search space with careful designs. For example, PAS [86] further proposes to search for adaptive pooling architectures. Firstly they design a unified framework consisting of four modules: Aggregation, Pooling, Read out and Merge, which can cover existing human-designed pooling methods (global and hierarchical) for graph classification. Based on this framework, a novel search space is designed by incorporating popular operations in humandesigned architectures. To further enable efficient search, a coarsening strategy is proposed to continuously relax the search space, with the utilization of differentiable search methods. Extensive experiments on six real-world datasets from three domains are conducted, and the results demonstrate the effectiveness and efficiency of the proposed framework. Hyper-parameters Besides architectures, other training hyper-parameters can be incorporated into the search space, i.e., similar to jointly conducting NAS and HPO. Typical hyper-parameters include the learning rate, the number of epochs, the batch size, the optimizer, the dropout rate, and the regularization strengths such as the weight decay. These hyper-parameters can be jointly optimized with architectures or separately optimized after the best-architectures are found. HPO methods in Section 3 can also be combined here. Layers Another critical model choice not incorporated in the above four categories is the number of message-passing layers. Unlike CNNs, most currently successful GNNs are shallow, e.g., with no more than three layers, possibly due to the over-smoothing problem [104] , [101] . Limited by this problem, the existing NAS methods for GNNs preset the number of layers as a fixed small number. Except for a recent attempt DeepGNAS [82] , how to automatically design deep GNNs while integrating techniques to alleviate over-smoothing remains mostly unexplored. Search Strategy The search strategy can be broadly divided into three categories: architecture controllers trained with reinforcement learning (RL), differentiable methods, and evolutionary algorithms. Controller + RL A widely adopted NAS search strategy is to use a controller to generate the neural architecture descriptions and train the controller with reinforcement learning to maximize the model performance as rewards. For example, if we consider neural architecture descriptions as a sequence, we can use RNNs as the controller [35] . Such methods can be directly applied to GNNs with a suitable search space and performance evaluation strategy. Differentiable Differentiable NAS methods such as DARTS [36] and SNAS [93] have gained popularity in recent years. Instead of optimizing different operations separately, differentiable methods construct a single super-network (known as the one-shot model) containing all possible operations. Formally, we denote y = o (x,y) (x) = o∈O exp(z (x,y) o ) o ′ ∈O exp(z (x,y) o ′ ) o(x), (6) where o (x,y) (x) is an operation in the GNN with input x and output y, O are all candidate operations, and z (x,y) are learnable vectors to control which operation is selected. Briefly speaking, each operation is regarded as a probability distribution of all possible operations. In this way, the architecture and model weights can be jointly optimized via gradient-based algorithms. The main challenges lie in making the NAS algorithm differentiable, where several techniques such as Gumbel-softmax [105] and concrete distribution [106] are resorted to. When applied to GNNs, slight modification may be needed to incorporate the specific operations defined in the search space, but the general idea of differentiable methods remains unchanged. Evolutionary Algorithms Evolutionary algorithms are a class of optimization algorithms inspired by biological evolution. For NAS, randomly generated architectures are considered initial individuals in a population. Then, new architectures are generated using mutations and crossover operations based on the population. The architectures are evaluated and selected to form the new population, and the same process is repeated. The best architectures are recorded while updating the population, and the final solutions are obtained after sufficient updating steps. For GNNs, regularized evolution (RE) NAS [39] has been widely adopted. RE's core idea is an aging mechanism, i.e., in the selection process, the oldest individuals in the population are removed. Genetic-GNN [107] also proposes an evolution process to alternatively update the GNN architecture and the learning hyper-parameters to find the best fit of each other. Combinations It is also feasible to combine these three types of search strategies mentioned above. For example, AGNN [63] proposes a reinforced conservative search strategy by adopting both RNNs and evolutionary algorithms in the controller and train the controller with RL. By only generating slightly different architectures, the controller can find well-performing GNNs more efficiently. Peng et al. [74] adopt CEM-RL [75] , which combines evolutionary and differentiable methods. Performance Estimation Strategy Due to the large number of possible architectures, it is infeasible to fully train each architecture independently. Next, we review some performance estimation strategies. A commonly adopted \"trick\" to speed up performance estimation is to reduce fidelity [33] , e.g., by reducing the number of epochs or the number of data points. This strategy can be directly generalized to GNNs. Another strategy successfully applied to CNNs is sharing weights among different models, known as parameter sharing or weight sharing [37] . For differentiable NAS with a large oneshot model, parameter sharing is naturally achieved since the architectures and weights are jointly trained. However, training the one-shot model may be difficult since it contains all possible operations. To further speed up the training process, single-path one-shot model [108] has been proposed where only one operation between an input and output pair is activated during each pass. For NAS without a one-shot model, sharing weights among different architecture is more difficult but not entirely impossible. For example, since it is known that some convolutional filters are common feature extractors, inheriting weights from previous architectures is feasible and reasonable in CNNs [109] . However, since there is still a lack of understandings of what weights in GNNs represent, we need to be more cautious about inheriting weights [64] . AGNN [63] proposes three constraints for parameter inheritance: same weight shapes, same attention and activation functions, and no parameter sharing in batch normalization and skip connections. Recent Featured Works In this section, we discuss several recent advances in automated graph machine learning that feature in taking topological structure learning, robustness and generalization, scalability, data heterogeneity, efficient architecture search or software-hardware codesign into considerations. Architecture Search with Graph Structure Learning Qin et al. [94] investigate the important question that how NAS is able to select the desired GNN architectures by conducting a measurement study with experiments, which discovers that gradient based NAS methods tend to select proper architectures based on the usefulness of different types of information with respect to the target task. The explorations further show that gradient based NAS also suffers from noises hidden in the graph, resulting in searching suboptimal GNN architectures. Based on these findings, they propose a Graph differentiable Architecture Search model with Structure Optimization (GASSO), which allows differentiable search of the architecture with gradient descent and is able to discover graph neural architectures with better performance through employing graph structure learning as a denoising process in the search procedure. The proposed GASSO model is capable of simultaneously searching the optimal architecture and adaptively adjusting graph structure by jointly optimizing graph architecture search and graph structure denoising. Extensive experiments on real-world graph datasets demonstrate that the proposed GASSO model is able to achieve state-of-the-art performance compared with existing baselines. Robust and Generalizable Graph NAS G-RNA [95] enhances the robustness of graph NAS methods against adversarial attacks. G-RNA designs a robust search space for the message-passing mechanism by incorporating graph structure mask operations. The graph structure mask operations cover important robust essences of graph structure and could recover various existing defense methods as well. The framework also defines a robustness metric to guide the search process and filter robust architectures. Specifically, G-RNA uses an attack proxy to produce several adversarial samples based on the clean graph, and it searches robust GNNs using the robustness metric with clean and generated adversarial samples. Besides the robustness, GRACES [96] addresses the limitations of existing graph neural architecture search methods in dealing with distribution shifts between training and test graphs. GRACES tailors a customized GNN architecture suitable for each graph instance to handle the distribution shifts. GRACES uses a self-supervised disentangled graph encoder [110] , [111] to characterize invariant factors hidden in diverse graph structures. It further proposes a prototype based architecture self-customization strategy to tailor specialized GNN architectures for graphs based on the similarities of their representations with operation prototypes vectors in the latent space. Finally, the customized supernet provides differentiable weights on the mixture of different operations. GRACES model can be easily optimized in an end-to-end fashion through gradient based methods 4.4.3 Large-scale Graph NAS Guan et al. [97] presents the graph architecture search at scale (GAUSS) method, designed to address the limitations of existing graph NAS approaches in handling large-scale graphs. Traditional graph NAS methods are computationally intensive and suffer from the consistency collapse issues, making them unsuitable for large graphs. GAUSS tackles these problems by introducing an efficient light-weight supernet and a joint architecture-graph sampling technique. Specifically, it proposes a graph sampling-based singlepath one-shot supernet to minimize computational load. To handle consistency issues, the method incorporates a unique architecture peer learning mechanism on sampled sub-graphs, as well as an architecture importance sampling algorithm. These innovations aim to smooth the highly non-convex optimization objective and stabilize the architecture sampling process. Theoretical analyses and empirical tests on five different datasets, ranging from 10 4 to 10 8 vertices, show that GAUSS outperforms existing GNAS methods, marking it as the first framework capable of efficiently handling large-scale graphs with billions of edges within a single GPU day. Zhang et al. [98] further introduces PasCa, a new system aimed at systematically exploring the design space for scalable graph neural networks. It addresses the limitations of current GNNs that are not well-suited for scalability. Through the deconstruction of the neural message-passing mechanism, the authors propose a novel scalable graph neural architecture paradigm (SGAP) that includes a design space with up to 150,000 different architectures. To navigate this expansive design space, it also presents an autosearch engine capable of multi-objective optimization to find GNN architectures that are both efficient and accurate. Empirical studies across ten benchmark datasets reveal that the architectures discovered by PasCa, specifically PasCa-V3, not only offer competitive predictive accuracy but also achieve up to 28.3 times faster training speeds compared to state-of-the-art methods like JK-Net [102] . Heterogeneous Graph NAS Heterogeneous information networks (HINs) are used to describe real-world data with intricate entities and relationships. Several recent works [112] , [113] , [99] study the neural architecture search on HINs to handle the heterogeneous node types and relationships. Specifically, Li et al. [99] proposes a method called Partial Message Meta Multigraph search (PMMM) for automatically optimizing the neural architecture design on Heterogeneous Information Networks (HINs), aiming at better stability and flexibility. It adopts an efficient differentiable framework to search for a meaningful meta multigraph, which captures more flexible and complex semantic relations than a meta graph. The authors also propose a stable algorithm called partial message search to ensure that the searched meta multigraph consistently outperforms manually designed meta-structures. Experimental results on six benchmark datasets for node classification and recommendation tasks demonstrate the effectiveness of PMMM. The proposed method outperforms state-of-the-art heterogeneous GNNs, discovers meaningful meta multigraphs, and exhibits significantly improved stability. Zhang et al. [100] first consider the temporal information on heterogeneous graphs, and propose a method called Dynamic Heterogeneous Graph Attention Search (DHGAS) for automating the design of Dynamic Heterogeneous Graph Neural Networks (DHGNNs). The existing DHGNNs are manually designed and lack adaptability to diverse dynamic heterogeneous graph scenarios. To overcome this limitation, the authors introduce a search space that considers spatial-temporal dependencies and heterogeneous interactions in graphs and develop an efficient search algorithm. The proposed DHGAS method can automatically discover optimal DHGNN architectures without human guidance. The authors introduce a unified dynamic heterogeneous graph attention (DHGA) framework that enables nodes to attend to their heterogeneous and dynamic neighbors. They also design a localization space to determine attention application and a parameter Graph Transformer Architecture Search Besides searching the architecture of graph neural networks, AutoGT [114] is proposed to search the architecture of graph transformer, which is another type of strong graph encoder [115] . However, unlike their applications in text and image data, using Transformers for graph data involves additional complexities due to the non-euclidean nature of graphs. The authors identify two main challenges. The first challenge is how to creat a unified search space for graph Transformers. The second challenge is how to handle the complex relationship between the architecture of each Transformer layer and graph encoding strategies. To address these challenges, they introduce Automated Graph Transformer (AutoGT), a neural architecture search framework designed for graphs. AutoGT uses a unified graph Transformer formulation and includes a comprehensive search space that considers both architectural and encoding options. To manage the coupling between architecture and graph encodings, the authors propose an encoding-aware performance estimation strategy. Through rigorous experiments, they demonstrate that AutoGT outperforms stateof-the-art hand-crafted models, establishing its effectiveness and broad applicability. [85] is a GNN and accelerator co-search framework that can automatically search for matched GNN structures and accelerators to maximize both task accuracy and acceleration efficiency. Specifically, G-CoS integrates two major components: i) a generic GNN accelerator search space which is applicable to various GNN structures and ii) a one-shot GNN and accelerator co-search algorithm that enables simultaneous and efficient search for optimal GNN structures as well as their matched accelerators. Extensive experiments and ablation studies show that the GNNs together with accelerators generated by G-CoS consistently outperforms state-of-the-art GNNs and GNN accelerators in terms of both task accuracy and hardware efficiency, while only requiring a few hours for the end-to-end generation of the best matched GNNs and their accelerators. Similarly, LPGNAS [71] jointly searches for architectures and quantisation choices so that both model and buffer sizes can be greatly reduced while keeping similar accuracy as other methods. Their empirical results show that 4-bit weights, 8-bit activations quantisation strategy might be the key for GNNs. ALGNN [89] considers the computation cost and complexity of the searched model using a multi-objective optimization method. Search of Efficient Architectures G-Cos Co-design of Software and Hardware Lu et al. [87] propose FGNAS as the first software-hardware codesign framework for automating the search and deployment of GNNs. Using FPGA as the target platform, the FGNAS framework is able to perform the FPGA-aware graph neural architecture search. FPGA is employed as the vehicle for illustration and implementation of the methods. Specific hardware constraints are considered so that quantization is adopted to compress the model. Under specific hardware constraints, they show the FGNAS framework can successfully identify a solution of higher accuracy while using shorter time than random search and the traditional twostep tuning. To evaluate the design, they conduct experiments on benchmark datasets, i.e., Cora, CiteSeer and PubMed, and the results show that the proposed FGNAS framework has better capability in optimizing the accuracy of GNNs when the hardware implementation is specifically constrained. Discussions In this section, we will discuss other unique NAS designs for graphs in terms of search space, transferability and scalability. The search space Besides the basic search space presented in Section 4.1, different graph tasks may require other search spaces. For example, metapaths are critical for heterogeneous graphs [81] , edge features are essential in modeling molecular graphs [70] and many graph tasks [92] , [91] , and spatial-temporal modules are needed in skeleton-based recognition [74] and traffic forcasting [77] . A suitable search space usually requires careful designs and domain knowledge. Transferability It is non-trivial to transfer GNN architectures across different datasets and tasks due to the complexity and diversity of graph tasks. GraphGym [116] propose to adopt a fixed set of GNNs as anchors and rank the performance of these GNNs on different tasks and datasets. Then, the rank correlation serves as a metric to measure the similarities between different datasets and tasks. The best-performing GNNs of the most similar tasks are transferred to solve the target tasks. The scalability challenge of large-scale graphs Similar to AutoNE introduced in Section 3, EGAN [69] proposes to sample small graphs as proxies and conduct NAS on the sampled subgraphs to improve the efficiency of NAS. While achieving some progress, more advanced and principle approaches are further needed to handle billion-scale graphs. LIBRARIES FOR AUTOMATED GRAPH MACHINE LEARNING Although there have been quite a few libraries for both graph machine learning and automated machine learning, there is no but one library for automated graph machine learning. Therefore, we will briefly overview libraries for graph machine learning and automated machine learning, followed by the in-depth introduction of the world's first dedicated open-source automated graph machine learning library, AutoGL. Libraries for Graph Machine Learning and Automated Machine Learning Publicly available libraries are important to facilitate and advance the research and applications of AutoML on graphs. First, we briefly list libraries for graph machine learning and automated machine learning, respectively. Graph Machine Learning Libraries Popular libraries for graph machine learning include PyTorch Geometric [117] , Deep Graph Library [118] , GraphNets [119] , AliGraph [120] , Euler [121] , PBG [122] , PGL [123] , TF-GNN [124] , Stellar Graph [125] , Spektral [126] , CodDL [127] , OpenNE [128] , OpenHGNN [129] , GEM [130] , Karateclub [131] and classical NetworkX [132] . However, these libraries do not support AutoML. Automated Machine Learning Libraries On the other hand, AutoML libraries such as NNI [133] , AutoKeras [134] , Au-toSklearn [135] , Hyperopt [136] , TPOT [137] , AutoGluon [138] , MLBox [139] , and MLJAR [140] are widely adopted. Unfortunately, because of the uniqueness and complexity of graph tasks, they cannot be directly applied to automate graph machine learning. Despite their successes, integrating these libraries to fully support automated graph machine learning is non-trivial. This motivates us to design a specific library tailored for automated graph machine learning. AutoGL: A Library for Automated Graph Machine Learning To fill this gap, we present Automated Graph Learning (AutoGL) 2 , the first dedicated framework and library for automated graph machine learning. The overall framework of AutoGL is shown in Figure 1 . We summarize and abstract the pipeline of AutoML on graphs into five modules: auto feature engineering, neural architecture search, model training, hyper-parameter optimization, and auto ensemble. For each module, we provide plenty of stateof-the-art algorithms, standardized base classes, and high-level APIs for easy and flexible customization. The AutoGL library is built upon PyTorch Geometric (PyG) [117] , a widely adopted graph machine learning library. AutoGL has the following key characteristics: • Open source: The code 3 and detailed documentation foot_2 are available online. • Easy to use: AutoGL is designed to be user-friendly. Users can conduct quick AutoGL experiments with less than ten lines of code. • Flexible to be extended: The modular design, high-level base class APIs, and extensive documentation of AutoGL allow flexible and easy customized extensions. Detailed Designs of AutoGL In this section, we introduce AutoGL designs in detail. AutoGL is designed in a modular and object-oriented fashion to enable clear logic flows, easy usages, and flexible extensions. All the APIs exposed to users are abstracted in a high-level fashion to avoid redundant re-implementation of models, algorithms, and train/evaluation protocols. All the five main modules, i.e., auto feature engineering, neural architecture search, model training, hyper-parameter optimization and auto ensemble, have taken into account the unique characteristics of graph machine learning. Next, we elaborate on the detailed designs for each module. AutoGL Dataset We first briefly introduce our dataset management. AutoGL Dataset is currently based on Dataset from PyTorch Geometric and supports common benchmarks for node and graph classification, including the recent Open Graph Benchmark [43] . We present the complete list of datasets in Table 3 , and users can also easily customize datasets following our documentation. Specifically, we provide widely adopted node classification datasets including Cora, CiteSeer, PubMed [141] , Amazon Computers, Amazon Photo, Coauthor CS, Coauthor Physics [142] , Reddit [143] , and graph classification datasets such as MU-TAG [144] , PROTEINS [145] , IMDB-B, IMDB-M, COL-LAB [146] , etc. Datasets from Open Graph Benchmark [43] are also supported. Auto Feature Engineering The graph data is first processed by the auto feature engineering module, where various nodes, edges, and graph-level features can be automatically added, compressed, or deleted to help boost the graph learning process after. Graph topological features can also be extracted to utilize graph structures better. Currently, we support 24 feature engineering operations abstracted into three categories: generators, selectors, and graph features. The generators aim to create new node and edge features based on the current node features and graph structures. The selectors automatically filter out and compress features to ensure they are compact and informative. Graph features focus on generating graph-level features. We summarize the supported generators in Table 4 , including Graphlets [147] , EigenGNN [148] , PageRank [149] , local degree profile, normalization, one-hot degrees, and one-hot node IDs. For selectors, GBDT [150] and FilterConstant are supported. An automated feature engineering method DeepGL [151] is also supported, functioning as both a generator and a selector. For graph feature, Netlsd [152] and a set of graph feature extractors implemented in NetworkX [132] are wrapped, e.g., NxTransitivity, NxAverageClustering, etc. We also provide convenient wrappers that support feature engineering operations in PyTorch Geometric [117] and NetworkX [132] . Users can easily customize feature engineering methods by inheriting from the class BaseGenerator, BaseSelector, and BaseGraph, or BaseFeatureEngineer if the methods do not fit in our categorization. Neural Architecture Search In AutoGL, Neural Architecture Search (NAS) aims to automate the construction of Graph Neural Networks. The best GNN model will be searched using various NAS methods to fit the current datasets. In Neural Architecture Search module, Algorithm, GNNSpace, and Estimator submodule is developed to further solve the search problem. GNNSpace defines the whole search range where we explore the best models. Algorithms are used to determine which architectures should be evaluated next, and the Estimators are used for deriving the performances of target architectures. We have supported various NAS models, including algorithms specified for graph data like AutoNE [45] and AutoGR [46] and general-purpose algorithms like random search [29] , Tree Parzen Estimator [30] , etc. Users can customize HPO algorithms by inheriting from the BaseHPOptimizer class. Model Training This module handles the training and evaluation process of graph machine learning tasks with two functional sub-modules: Model and Trainer. Model handles the construction of graph machine learning models, e.g., GNNs, by defining learnable parameters and the forward pass. Trainer controls the optimization process for the given model.  GAT [153] , and GraphSAGE [143] , GIN [154] , and pooling methods such as Top-K Pooling [155] are supported. Users can quickly implement their own graph models by inheriting from the BaseModel class and add customized tasks or optimization methods by inheriting from BaseTrainer. Hyper-Parameter Optimization The Hyper-Parameter Optimization (HPO) module aims to automatically search for the best hyper-parameters of a specified model and training process, including but not limited to architecture hyper-parameters such as the number of layers, the dimensionality of node representations, the dropout rate, the activation function, and training hyper-parameters such as the learning rate, the weight decay, the number of epochs. The hyper-parameters, their types (e.g., integer, numerical, or categorical), and feasible ranges can be easily set. We have supported various HPO algorithms, including algorithms specified for graph data like AutoNE [45] and AutoGR [46] and general-purpose algorithms like random search [29] , Tree Parzen Estimator [30] , etc. Users can customize HPO algorithms by inheriting from the BaseHPOptimizer class. Auto Ensemble This module can automatically integrate the optimized individual models to form a more powerful final model. Currently, we have adopted two kinds of ensemble methods: voting and stacking. Voting is a simple yet powerful ensemble method that directly averages the output of individual models. Stacking trains another meta-model to combine the output of models. We have supported general linear models (GLM) and gradient boosting machines (GBM) as meta-models. AutoGL Solver On top of the modules mentioned above, we provide another highlevel API Solver to control the overall pipeline. In Solver, the five modules are integrated systematically to form the final model. Solver receives the feature engineering module, a model list, the HPO module, and the ensemble module as initialization arguments to build an Auto Graph Learning pipeline. Given a dataset and a task, Solver first perform auto feature engineering to clean and augment the input data, then optimize all the given models using the model training and HPO module. At last, the optimized best models will be combined by the Auto Ensemble module to form the final model. Solver also provides global controls of the AutoGL pipeline. For example, the time budget can be explicitly set to restrict the maximum time cost, and the training/evaluation protocols can be selected from plain dataset splits or cross-validation. Evaluation of AutoGL In this section, we provide experimental results. Note that we mainly want to showcase the usage of AutoGL and its main functional modules rather than aiming to achieve the new stateof-the-art on benchmarks or compare different algorithms. For node classification, we use Cora, CiteSeer, and PubMed with the standard dataset splits from [49] . For graph classification, we follow the setting in [156] and report the average accuracy of 10fold cross-validation on MUTAG, PROTEINS, and IMDB-B. AutoGL Results We turn on all the functional modules in AutoGL, and report the fully automated results in Table 5 and Table 6 . We use the best single model for graph classification under the cross-validation setting. We observe that in all the benchmark datasets, AutoGL achieves better results than vanilla models, demonstrating the importance of AutoML on graphs and the effectiveness of the proposed pipeline in the released library.  Hyper-Parameter Optimization Table 7 reports the results of two implemented HPO methods, i.e., random search and TPE [30] , for the semi-supervised node classification task. As shown in the table, as the number of trials increases, both HPO methods tend to achieve better results. Besides, both methods outperform vanilla models without HPO. Note that a larger number of trials do not guarantee better results because of the potential overfitting problem. We further test these HPO methods with ten trials for the graph classification task and report the results in Table 8 . The results generally show improvements over the default hand-picked parameters on all datasets. Auto Ensemble Table 9 reports the performance of the ensemble module as well as its base learners for the node classification task. We use voting as the example ensemble method and choose GCN and GAT as the base learners. The table shows that the ensemble module achieves better performance than both the base learners, demonstrating the effectiveness of the implemented ensemble module. Advanced Functions of AutoGL We have presented AutoGL, the first library for automated graph machine learning, which is open-source, easy to use, and flexible to be extended. Currently, we are actively developing AutoGL and have supported the following advanced functionalities: • Support more graph models. We have supported selfsupervised graph models and robust graph models now. • Handle more graph tasks. We have supported heterogeneous node classification tasks now and plan to support more complex spatial-temporal graphs. • Support more graph library backends. We have supported Deep Graph Library (DGL) [118] backend including homogeneous node classification, link prediction, and graph classification tasks. AutoGL is also compatible with PyG 2.0 [117] now. All kinds of inputs and suggestions are also warmly welcomed. BENCHMARK To promote the further development of automated graph machine learning, NAS-Bench-Graph [157] is proposed as one benchmark tailored to enable unified, reproducible, and efficient evaluations for graph NAS. Two key issues are identified: i) the lack of a standard experimental setting, making comparisons across research unreliable and non-reproducible, and ii) the computational inefficiency of graph NAS methods. To resolve these issues, NAS-Bench-Graph constructs a unified search space, encompassing 26,206 unique GNN architectures, and offers a standardized evaluation protocol. All architectures have been trained and evaluated on nine representative graph datasets, with metrics such as training, validation, and test performance, latency, and the number of parameters recorded. This results in a look-up table that allows for fair and efficient comparisons without additional computational costs. The authors also showcase the benchmark's compatibility with existing GraphNAS libraries like AutoGL and NNI. The work claims to be the first of its kind to offer a benchmark in the domain of GraphNAS. Benchmark Design Here we elucidates the methodology employed in the development of our benchmark for graph NAS, encapsulating search space design (Section 6.1.1), utilized datasets (Section 6.1.2), and experimental configurations (Section 6.1.3). Search Space Design To achieve a judicious equilibrium between effectiveness and efficiency, we propose an intricately crafted, yet computationally tractable search space. In particular, we conceptualize the macro search space governing Graph Neural Network (GNN) architectures as a Directed Acyclic Graph (DAG), thereby providing a formal structure for the computational paradigm 5 . In this DAG, each computational node signifies a vertex representation, while each edge symbolizes a specific operation. The DAG is composed of six nodes, including the input and output nodes. Notably, each intermediate node is restricted to having a single incoming edge. As a result, the aforementioned DAG encompasses nine selectable configurations, as delineated in Figure 2 . Any intermediate nodes devoid of successor nodes are concatenated to the output node. In addition to this overarching macro space, we also incorporate optional fully-connected layers in the pre-processing and post-processing layers, following the design principles established in GraphGym [72] and PasCa [98] . To circumvent an exponential increase in the complexity of the search space, the number of layers in both pre-processing and post-processing phases are treated as hyperparameters, a point that will be elaborated upon in Section 6.1.3. To culminate the architecture, a task-specific fullyconnected layer is employed to generate the model's prediction. 5 . For disambiguation, we refer to nodes and edges in the computational graph, as opposed to vertices and links that constitute the graph data. Fig. 2 : Nine different choices of our macro search space. Each node is a representation of vertices and each edge is an operation [157] . For the set of admissible operations, we focus on seven GNN layers that have garnered widespread acceptance in the literature: GCN [49] , GAT [153] , GraphSAGE [143] , GIN [154] , ChebNet [158] , ARMA [159] , and k-GNN [160] . Furthermore, we introduce the Identity operation to facilitate residual connections and a fully-connected layer that does not exploit graph structures. In summation, the search space we designed is remarkably expansive, consisting of 26,206 unique architectures after accounting for isomorphic structures (i.e., structures that may exhibit different topological characteristics yet perform identical functionalities, further elaborated in Appendix A.5). Importantly, this search space encapsulates a myriad of GNN variants, including but not limited to the previously mentioned methods, and extends to more advanced architectures such as JK-Net [102] and residual-and dense-like GNNs [101] . Datasets In the course of this study, we employ nine diverse, publicly available datasets extensively utilized in the realm of Graph Neural Architecture Search (GraphNAS). Specifically, these datasets include Cora, CiteSeer, and PubMed [141] , alongside Coauthor-CS, Coauthor-Physics, Amazon-Photo, and Amazon-Computer [161] , as well as ogbn-arXiv and ogbn-proteins [162] . These datasets span an array of sizes, ranging from several thousands to millions of edges, and encompass diverse application domains such as citation networks, e-commerce graphs, and protein interaction networks. Regarding dataset partitioning strategies, we adhere to the publicly available semi-supervised settings for Cora, CiteSeer, and PubMed as delineated by [163] . This involves utilizing 20 labeled nodes per class for training and 500 nodes for validation purposes. For the Amazon and Coauthor datasets, we employ a random partitioning scheme for train/validation/test splits in a semi-supervised fashion, as recommended by [161] . Specifically, each class is represented by 20 nodes for training, 30 nodes for validation, and the remaining nodes are used for testing. In the case of ogbn-arXiv and ogbn-proteins, we abide by the official dataset splits. Pertaining to the ogbn-proteins dataset, preliminary experiments indicated that the utilization of Graph Isomorphism Network (GIN) and k-Graph Neural Network (k-GNN) operations resulted in parameter explosion, thereby yielding uninterpretable outcomes. Additionally, employing Graph Attention Networks (GAT) and Chebyshev Spectral Graph Convolution Networks (ChebNet) led to out-of-memory errors on our high-capacity GPUs equipped with 32GB of memory. To mitigate these computa- Settings Hyper-parameters: For fair and reproducible comparisons, we propose a unified evaluation protocol and consider the following hyper-parameters with tailored ranges: • Number of pre-process layers: 0 or 1. • Number of post-process layers: 0 or 1. • Dimensionality of hidden units: 64, 128, or 256. • Dropout rate: 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8. • Optimizer: SGD or Adam. • Learning Rate (LR): 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001. • Weight Decay: 0 or 0.0005. • Number of training epochs: 200, 300, 400, 500. For each dataset, we establish fixed hyper-parameters across all architectures in order to ensure a fair comparison. It should be noted that exhaustively enumerating combinations of architectures and hyper-parameters would lead to an impractical number of architecture hyper-parameter pairs in the order of billions. Hence, we adopt a two-step approach, first optimizing the hyperparameters to a suitable value that can accommodate various Graph Neural Network (GNN) architectures, and subsequently focusing on the GNN architectures themselves. Specifically, we select 30 GNN architectures from our search space as \"anchors\" and employ random search for hyper-parameter optimization [29] . The set of 30 anchor architectures comprises 20 randomly chosen architectures from our search space, along with 10 classic GNN architectures including Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), Graph Isomorphism Networks (GIN), GraphSAGE, and ARMA with 2 and 3 layers. We optimize the hyper-parameters by maximizing the average performance of these anchor architectures. The specific hyper-parameters selected for each dataset are shown in Table 10 . Metrics: During the training of each architecture, we record a comprehensive set of metrics covering both model effec-tiveness and efficiency. These metrics include the loss values and evaluation metric at each epoch for both the training, validation, and testing sets, as well as the model latency and the number of parameters. The hardware and software configurations utilized in our experiments are provided as follows: • Operating System: Ubuntu 18.04.6 LTS for PubMed, ogbn-arXiv, and CentOS Linux release 7.6.1810 for the others. • CPU: Intel(R) Xeon(R) Gold 6129 CPU @ 2.30GHz for PubMed, ogbn-arXiv, and Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz for the others. • GPU: NVIDIA GeForce RTX 3090 with 24GB of memories for PubMed, ogbn-arXiv, and NVIDIA Tesla V100 with 16GB of memories for the others. • Software: Python 3.9.12, PyTorch 1.11.0+cu113, PyTorch-Geometric 2.0.4 [117] . Moreover, to account for the inherent variability in the training process, all experiments are repeated three times using different random seeds. The average training time of architectures on each dataset is reported in Table 11 . The total computational cost incurred in creating our benchmark dataset amounts to approximately 8,000 GPU hours. Example Usages This section demonstrates the utilization of NAS-Bench-Graph in conjunction with established open-source libraries such as AutoGL [80] and NNI [164] . Specifically, we employ two NAS algorithms, namely GNAS [165] and Auto-GNN [63] , within the AutoGL framework. Additionally, we employ Random Search [166] , Evolutionary Algorithm (EA), and Policy-based Reinforcement Learning (RL) algorithms within NNI. To ensure equitable comparisons, we restrict each algorithm's access to the performance data of only 2% of the total architectures within the search space. The obtained results are presented in Table 12 . Furthermore, we include the performance metrics of the top 5% architectures, representing the 20-quantiles for each dataset, within the aforementioned table. The detailed example codes are provided as follows. All the codes and recorded metrics for the trained models are available at https://github.com/THUMNLab/NAS-Bench-Graph . Next, we provide some example usages. At first, the benchmark of a certain dataset, e.g., Cora, can be read as: The data is stored as a Python dictionary. In order to capture the metrics of interest, it is imperative to define an architecture by specifying its macro space and corresponding operations. In our approach, we impose a restriction on the directed acyclic graph (DAG) of the computation graph, dictating that each intermediate node may have only one input node. This constraint allows us to represent the macro space using a list of integers, wherein each integer denotes the index of the input node for a given computing node. Specifically, the value of 0 corresponds to the raw input, 1 corresponds to the first computing node, and so forth. Additionally, the operations associated with the architecture can be described using a list of strings of equal length, providing a concise representation of the architectural choices made. For example, to specify the architecture shown in Figure 3 , we can use the following code: We assume all leaf nodes (i.e., nodes without descendants) are connected to the output, so there is no need to specific the output node. Besides, the list can be specified in any order, e.g., the following code can specific the same architecture:  We have also provided the source codes of using our benchmark together with two public libraries for GraphNAS, Au-toGL and NNI. See https://github.com/THUMNLab/AutoGL/tree/  agnn and https://github.com/THUMNLab/NAS-Bench-Graph/  blob/main/runnni.py for details. From the experimental findings, it is evident that all algorithms exhibit superior performance compared to the top 5% benchmark, indicating their ability to acquire meaningful patterns in the NAS-Bench-Graph. Nevertheless, no algorithm demonstrates consistent superiority across all datasets. Notably, Random Search remains a robust baseline in comparison to alternative methods and even achieves the highest performance on two datasets, partially supporting the findings of Li et al. [166] in the context of general NAS. These results underscore the pressing need for further research in the domain of graph NAS. In order to delve into the learning behaviors of diverse graph NAS methods, we present the curves depicting optimal performance as a function of the number of architectures, as illustrated in Figure 4 . It is evident that different algorithms exhibit distinct characteristics. For instance, EA and AGNN demonstrate sporadic \"jumps\" in performance, indicating substantial performance improvements, while RL exhibits more gradual and consistent performance enhancements. A meticulous examination of these learning curves may serve as a source of inspiration for the development of novel algorithms for graph NAS. FUTURE DIRECTIONS We have discussed existing literature in automated graph machine learning approaches and libraries. Our discussion in detail contains how HPO and NAS can be applied to graph machine learning to handle problems in automated graph machine learning. We also introduce AutoGL, a dedicated framework and library for automated graph machine learning. In this section, we will suggest future directions deserving further investigations from both academia and industry. There exist plenty of challenges and opportunities worthy of future explorations. • Scalability: AutoML has been successfully applied to various graph scenarios, however, there are still lots of future directions deserving further investigation regarding scalability to large-scale graphs. On the one hand, although HPO for large-scale graph machine learning has been preliminarily explored in literature [45] , the Bayesian Optimization utilized in the model suffers from limited efficiency. Thus it will be interesting and challenging to explore how we can reduce the computational costs to realize fast hyper-parameter optimization. On the other hand, the scalability of NAS for graph machine learning has drawn few attentions from the researchers despite applications involving large-scale graphs are very common in real world, leaving a large space for further explorations. • Explainability: Existing automated graph machine learning approaches are mainly based on black-box optimizations. For example, it is unclear why certain NAS models can perform better compared with others, and the explainability of NAS algorithms still lack systematic research efforts. There have been some preliminary studies on explainability of graph machine learning [167] , and on explainable graph hyperparameter optimization [46] via hyper-parameter importance decorrelation. However, further and deeper investigations on the explainability of automated graph machine learning are still of great importance. • Out-of-distribution generalization: When applied to new graph datasets and tasks, there still need huge human ef-forts to construct task-specific graph HPO configurations and graph NAS frameworks, e.g., spaces and algorithms. The generalization of current graph HPO configurations and NAS frameworks are limited, especially training and testing data come from different distributions [168] , [169] , [170] , [171] . It will be a promising direction to study the out-of-distribution generalization abilities for both graph HPO and graph NAS algorithms which are capable of handling continuously and rapidly changing tasks. • Robustness: Since many applications of AutoML on graphs are risk-sensitive, e.g., finance and healthcare, the robustness of the models is indispensable for actual usages. Though there exist some initial studies on the robustness [172] of graph machine learning, how to generalize these techniques into automated graph machine learning has not been explored. • Graph models for AutoML: In this paper, we mainly focus on how AutoML methods are extended to graphs. The other direction, i.e., using graphs to help AutoML, is also feasible and promising. For example, we can model neural networks as a directed acyclic graph (DAG) to analyze their structures [173] , [116] or adopt GNNs to facilitate NAS [107] , [174] , [175] , [176] . Ultimately, we expect graphs and AutoML to form tighter connections and further facilitate each other. • Hardware-aware models: To further improve the scalability of automated graph machine learning, hardware-aware models may be a critical step, especially in real industrial environments. Both hardware-aware graph models [177] and hardware-aware AutoML models [178] , [179] , [180] have been studied, but integrating these techniques is still in the early stage and poses significant challenges. • Comprehensive evaluation protocols: Currently, most Au-toML on graphs are tested on small traditional benchmarks such as three citations graphs, i.e., Cora, CiteSeer, and PubMed [141] . However, these benchmarks have been identified as insufficient to compare different graph machine learning models [142] , not to mention AutoML on graphs. More comprehensive evaluation protocols are needed, e.g., on recently proposed graph machine learning benchmarks [43] , [181] , or new dedicated graph AutoML benchmarks [157] similar to the NAS-bench series [182] are needed. • Broader Scope of Applications: While automated graph machine learning techniques have been applied to a range of practical use-cases, there's considerable potential for using these newly-developed techniques in the information retrieval (such as search engines, recommender systems) for achieving effective, reliable, and user-friendly predictions. A viable approach could involve using this specialized domain knowledge as a form of prior to guide both the hyperparameter optimization and architecture search strategies. CONCLUSION In this paper, we discuss the current state-of-the-art automated graph machine learning approaches, libraries. In particular, we in depth elaborate how graph hyperparameter optimization (HPO) and graph neural architecture search (NAS) have been developed to facilitate automated graph machine learning. We also introduce AutoGL, our dedicated framework and open source library for automated graph machine learning, and NAS-Bench-Graph, our tailored benchmark that enables fair, fully reproducible, and ef-ficient empirical comparisons. Last but not least, we point out challenges and suggest promising directions deserving further investigations. Fig. 1 : 1 Fig. 1: The overall framework of AutoGL. from r e a d b e n c h i m p o r t l i g h t r e a d b e n c h = l i g h t r e a d ( ' c o r a ' ) Fig. 3 : 3 Fig. 3: An example architecture. a r c h = Arch ( [ 0 , 1 , 1 , 2 ] , [ ' gcn ' , ' cheb ' , ' g i n ' , ' f c ' ] )Then, four recorded metrics in the benchmark including the validation and test performance, the latency, and the number of parameters, can be obtained by a look-up table: i n f o = b e n c h [ a r c h . v a l i d h a s h ( ) ] i n f o [ ' v a l i d p e r f ' ] # v a l i d a t i o n p e r f o r m a n c e i n f o [ ' p e r f ' ] # t e s t p e r f o r m a n c e i n f o [ ' l a t e n c y ' ] # l a t e n c y i n f o [ ' p a r a ' ] # number o f p a r a m e t e r s We provide the full data, including the training/validation/testing performance at each epoch at: https://figshare.com/articles/ dataset/NAS-bench-Graph/20070371. Since we run each dataset with three random seeds, each dataset has 3 files. The full metric can be obtained similarly as follows: from r e a d b e n c h i m p o r t r e a d b e n c h = r e a d ( ' c o r a 0 . bench ' ) # d a t a s e t and s e e d i n f o = b e n c h [ a r c h . v a l i d h a s h ( ) ] e p o c h = 50 i n f o [ ' dur ' ] [ e p o c h ] [ 0 ] # t r a i n i n g p e r f o r m a n c e i n f o [ ' dur ' ] [ e p o c h ] [ 1 ] # v a l i d a t i o n p e r f o r m a n c e i n f o [ ' dur ' ] [ e p o c h ] [ 2 ] # t e s t i n g p e r f o r m a n c e i n f o [ ' dur ' ] [ e p o c h ] [ 3 ] # t r a i n i n g l o s s i n f o [ ' dur ' ] [ e p o c h ] [ 4 ] # v a l i d a t i o n l o s s i n f o [ ' dur ' ] [ e p o c h ] [ 5 ] # t e s t i n g l o s s i n f o [ ' dur ' ] [ e p o c h ] [ 6 ] # b e s t p e r f o r m a n c e Fig. 4 : 4 Fig.4: The learning curve depicting the optimal performance as a function of the number of searched architectures is presented herein. The reported results are obtained by averaging measurements from five independent experiments, each conducted with distinct random seeds. The background of the figure displays the standard errors associated with the reported values. TABLE 1 : 1 A summary of different graph neural architecture search (NAS) methods for automated graph machine learning. Other Characteristics - Performance Estimation Search Strategy Search space Tasks Micro Macro Pooling HP Layers Node Graph Method GraphNAS [42] TABLE 2 : 2 The typical search space of different types of aggregation weights aij. We omit the layer superscript for brevity. Type Formulation CONST TABLE 3 : 3 Common optimization methods are packaged as high-level APIs to provide neat and clean interfaces. More advanced training controls and regularization methods in graph tasks like early stopping and weight decay are also supported.The model training module supports both node-level and graph-level tasks, e.g., node classification and graph classification. Commonly used models for node classification such as GCN [49] ,The statistics of the supported datasets. For datasets with more than one graph, #Nodes and #Edges are the average numbers of all the graphs. #Features correspond to node features by default, and edge features are specified. Dataset Task #Graphs #Nodes #Edges #Features #Classes Cora Node 1 2,708 5,429 1,433 7 CiteSeer Node 1 3,327 4,732 3,703 6 PubMed Node 1 19,717 44,338 500 3 Reddit Node 1 232,965 11,606,919 602 41 Amazon Computers Node 1 13,381 245,778 767 10 Amazon Photo Node 1 7,487 119,043 745 8 Coauthor CS Node 1 18,333 81,894 6,805 15 Coauthor Physics Node 1 34,493 247,962 8,415 5 ogbn-products Node 1 2,449,029 61,859,140 100 47 ogbn-proteins Node 1 132,534 39,561,252 8(edge) 112 ogbn-arxiv Node 1 169,343 1,166,243 128 40 ogbn-papers100M Node 1 111,059,956 1,615,685,872 128 172 Mutag Graph 188 17.9 19.8 7s 2 PTC Graph 344 14.3 14.7 18 2 ENZYMES Graph 600 32.6 62.1 3 6 PROTEINS Graph 1,113 39.1 72.8 3 2 NCI1 Graph 4,110 29.8 32.3 37 2 COLLAB Graph 5,000 74.5 2,457.8 - 3 IMDB-B Graph 1,000 19.8 96.5 - 2 IMDB-M Graph 1,500 13.0 65.9 - 3 REDDIT-B Graph 2,000 429.6 497.8 - 2 REDDIT-MULTI5K Graph 5,000 508.5 594.9 - 5 REDDIT-MULTI12K Graph 11,929 391.4 456.9 - 11 ogbg-molhiv Graph 41,127 25.5 27.5 9, 3(edge) 2 ogbg-molpcba Graph 437,929 26.0 28.1 9, 3(edge) 128 ogbg-ppa Graph 158,100 243.4 2,266.1 7(edge) 37 TABLE 4 : 4 Supported generators in the auto feature engineering module. Name Description graphlet Local graphlet numbers eigen EigenGNN features. pagerank PageRank scores. PYGLocalDegreeProfile Local Degree Profile features PYGNormalizeFeatures Row-normalize all node features PYGOneHotDegree One-hot encoding of node degrees. onehot One-hot encoding of node IDs TABLE 5 : 5 The results of node classification Model Cora CiteSeer PubMed GCN 80.9 ± 0.7 70.9 ± 0.7 78.7 ± 0.6 GAT 82.3 ± 0.7 71.9 ± 0.6 77.9 ± 0.4 GraphSAGE 74.5 ± 1.8 67.2 ± 0.9 76.8 ± 0.6 AutoGL 83.2 ± 0.6 72.4 ± 0.6 79.3 ± 0.4 TABLE 6 : 6 The results of graph classification Model MUTAG PROTEINS IMDB-B Top-K Pooling 80.8 ± 7.1 69.5 ± 4.4 71.0 ± 5.5 GIN 82.7 ± 6.9 66.5 ± 3.9 69.1 ± 3.7 AutoGL 87.6 ± 6.0 73.3 ± 4.4 72.1 ± 5.0 TABLE 7 : 7 The results of different HPO methods for node classification Cora CiteSeer PubMed Method Trials GCN GAT GCN GAT GCN GAT None 80.9 ± 0.7 82.3 ± 0.7 70.9 ± 0.7 71.9 ± 0.6 78.7 ± 0.6 77.9 ± 0.4 1 81.0 ± 0.6 81.4 ± 1.1 70.4 ± 0.7 70.1 ± 1.1 78.3 ± 0.8 76.9 ± 0.8 random 10 82.0 ± 0.6 82.5 ± 0.7 71.5 ± 0.6 72.2 ± 0.7 79.1 ± 0.3 78.2 ± 0.3 50 81.8 ± 1.1 83.2 ± 0.7 71.1 ± 1.0 72.1 ± 1.0 79.2 ± 0.4 78.2 ± 0.4 1 81.8 ± 0.6 81.9 ± 1.0 70.1 ± 1.2 71.0 ± 1.2 78.7 ± 0.6 77.7 ± 0.6 TPE 10 82.0 ± 0.7 82.3 ± 1.2 71.2 ± 0.6 72.1 ± 0.7 79.0 ± 0.4 78.3 ± 0.4 50 82.1 ± 1.0 83.2 ± 0.8 72.4 ± 0.6 71.6 ± 0.8 79.1 ± 0.6 78.1 ± 0.4 TABLE 8 : 8 The results of different HPO methods for graph classification HPO MUTAG Top-K Pooling GIN PROTEINS Top-K Pooling GIN IMDB-B Top-K Pooling GIN None 76.3 ± 7.5 82.7 ± 6.9 69.5 ± 4.4 66.5 ± 3.9 71.0 ± 5.5 69.1 ± 3.7 random 82.7 ± 6.8 87.6 ± 6.0 73.3 ± 4.4 71.0 ± 5.9 71.5 ± 4.1 71.3 ± 4.0 TPE 83.9 ± 10.1 86.7 ± 6.2 72.3 ± 5.5 71.0 ± 7.2 71.6 ± 2.5 70.2 ± 3.7 In In In In In In In In In TABLE 9 : 9 The performance of the ensemble module of AutoGL for the node classification task. Base Model Cora CiteSeer PubMed GCN 81.1 ± 0.9 69.6 ± 1.1 78.5 ± 0.4 GAT 82.0 ± 0.5 70.4 ± 0.6 77.7 ± 0.5 Ensemble 82.2 ± 0.4 70.8 ± 0.5 78.5 ± 0.4 TABLE 10 : 10 The hyper-parameters and hardware used for each dataset. #Pre and #Post denotes the number of pre-process and post-process layers, respectively. Dataset #Pre #Post Dimension Dropout Optimizer LR WD # Epoch Cora 0 1 256 0.7 SGD 0.1 0.0005 400 CiteSeer 0 1 256 0.7 SGD 0.2 0.0005 400 PubMed 0 0 128 0.3 SGD 0.2 0.0005 500 Coauthor-CS 1 0 128 0.6 SGD 0.5 0.0005 400 Coauthor-Physics 1 1 256 0.4 SGD 0.01 0 200 Amazon-Photo 1 0 128 0.7 Adam 0.0002 0.0005 500 Amazon-Computers 1 1 64 0.1 Adam 0.005 0.0005 500 ogbn-arxiv 0 1 128 0.2 Adam 0.002 0 500 ogbn-proteins 1 1 256 0 Adam 0.01 0.0005 500 TABLE 11 : 11 The average training time of architectures on each dataset. Dataset Time Dataset Time Dataset Time Cora 5.8s Coauthor-CS 8.6s Amazon-Computers 9.8s CiteSeer 6.2s Coauthor-Physics 15.4s ogbn-arXiv 71s PubMed 7.8s Amazon-Photo 8.8s ogbn-proteins 50min tional inefficiencies, we circumscribed the candidate operations for ogbn-proteins to Graph Convolution Networks (GCN), Attention- based Recurrent Multi-layer Average networks (ARMA), Graph- SAGE, Identity mappings, and fully connected layers. Conse- quently, this constraint yielded a feasible architecture space com- prising 2,021 candidate models for ogbn-proteins. TABLE 12 : 12 The performance of NAS methods in AutoGL and NNI using NAS-Bench-Graph. The best performance for each dataset is marked in bold. We also show the performance of the top 5% architecture (i.e., 20-quantiles) as a reference line. The results are averaged over five experiments with different random seeds and the standard errors are shown in the bottom right. 0.17 70.89 0.16 77.79 0.02 90.97 0.06 92.43 0.04 92.43 0.03 84.74 0.20 72.00 0.02 78.71 0.11 Auto-GNN 81.80 0.00 70.76 0.12 77.69 0.16 91.04 0.04 92.42 0.16 92.38 0.01 84.53 0.14 72.13 0.03 78.54 0.30 NNI Random 82.09 0.08 70.49 0.08 77.91 0.07 90.93 0.07 92.35 0.05 92.44 0.02 84.78 0.14 72.04 0.05 78.32 0.14 EA 81.85 0.20 70.48 0.12 77.96 0.12 90.60 0.07 92.22 0.08 92.43 0.02 84.29 0.29 71.91 0.06 77.93 0.21 RL 82.27 0.21 70.66 0.12 77.96 0.09 90.98 0.01 92.48 0.03 92.42 0.06 84.90 0.19 72.13 0.05 78.52 0.18 Library Method Cora CiteSeer PubMed CS Physics Photo ComputersarXiv proteins AutoGL 82.04 The top 5% GNAS 80.63 69.07 76.60 90.01 91.67 91.57 82.77 71.69 78.37 This manuscript is based on AutoGL v0.2.0-pre released on 11st, July 2021. Pleases visit the website for the most up-to-the version. https://github.com/THUMNLab/AutoGL/ https://autogl.readthedocs.io/"
}