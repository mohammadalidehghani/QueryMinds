{
  "title": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality in Machine Learning",
  "abstract": "We propose a clustering-based iterative algorithm to solve certain optimization problems in machine learning, where we start the algorithm by aggregating the original data, solving the problem on aggregated data, and then in subsequent steps gradually disaggregate the aggregated data. We apply the algorithm to common machine learning problems such as the least absolute deviation regression problem, support vector machines, and semi-supervised support vector machines. We derive model-specific data aggregation and disaggregation procedures. We also show optimality, convergence, and the optimality gap of the approximated solution in each iteration. A computational study is provided.",
  "introduction": "Introduction In this paper, we propose a clustering-based iterative algorithm to solve certain optimization problems in machine learning when data size is large and thus it becomes impractical to use out-of-the-box algorithms. We rely on the principle of data aggregation and then subsequent disaggregations. While it is standard practice to aggregate the data and then calibrate the machine learning algorithm on aggregated data, we embed this into an iterative framework where initial aggregations are gradually disaggregated to the extent that even an optimal solution is obtainable. Early studies in data aggregation consider transportation problems [1, 10] , where either demand or supply nodes are aggregated. Zipkin [31] studied data aggregation for linear programming (LP) and derived error bounds of the approximate solution. There are also studies on data aggregation for 0-1 integer programming [8, 13] . The reader is referred to Rogers et al [22] and Litvinchev and Tsurkov [16] for comprehensive literature reviews for aggregation techniques applied for optimization problems. For support vector machines (SVM), there exist several works using the concept of clustering or data aggregation. Evgeniou and Pontil [11] proposed a clustering algorithm that creates large size clusters for entries surrounded by the same class and small size clusters for entries in the mixed-class area. The clustering algorithm is used to preprocess the data and the clustered data is used to solve the problem. The algorithm tends to create large size clusters for entries far from the decision boundary and small size clusters for the other case. Wang et al [26] developed screening rules for SVM to discard non-support vectors that do not affect the classifier. Nath et al [19] and Doppa et al [9] proposed a second order cone programming (SOCP) formulation for SVM based on chance constraints and clusters. The key idea of the SOCP formulations is to reduce the number of constraints (from the number of the entries to number of clusters) by defining chance constraints for clusters. After obtaining an approximate solution by solving the optimization problem with aggregated data, a natural attempt is to use less-coarsely aggregated data, in order to obtain a finer approximation. In fact, we can do this iteratively: modify the aggregated data in each iteration based on the information at hand. This framework, which iteratively passes information between the original problem and the aggregated problem [22] , is known as Iterative Aggregation Disaggregation (IAD). The IAD framework has been applied for several optimization problems such as LP [17, 24, 25] and network design [2] . In machine learning, Yu et al [28, 29] used hierarchical micro clustering and a clustering feature tree to obtain an approximate solution for support vector machines. In this paper, we propose a general optimization algorithm based on clustering and data aggregation, and apply it to three common machine learning problems: least absolute deviation regression (LAD), SVM, and semi-supervised support vector machines (S 3 VM). The algorithm fits the IAD framework, but has additional properties shown for the selected problems in this paper. The ability to report the optimality gap and monotonic convergence to global optimum are features of our algorithm for LAD and SVM, while our algorithm guarantees optimality for S 3 VM without monotonic convergence. Our work for SVM is distinguished from the work of Yu et al [28, 29] , as we iteratively solve weighted SVM and guarantee optimality, whereas they iteratively solve the standard unweighted SVM and thus find only an approximate solution. On the other hand, it is distinguished from Evgeniou and Pontil [11] , as our algorithm is iterative and guarantees global optimum, whereas they used clustering to preprocess data and obtain an approximate optimum. Nath et al [19] and Doppa et al [9] are different because we use the typical SVM formulation within an iterative framework, whereas they propose an SOCP formulation based on chance constraints. Our data disaggregation and cluster partitioning procedure is based on the optimality condition derived in this paper: relative location of the observations to the hyperplane (for LAD, SVM, S 3 VM) and labels of the observations (for SVM, S 3 VM). For example, in the SVM case, if the separating hyperplane divides a cluster, the cluster is split. The condition for S 3 VM is even more involved since a single cluster can be split into four clusters. In the computational experiment, we show that our algorithm outperforms the current state-of-the-art algorithms when the data size is large. The implementation of our algorithms is based on in-memory processing, however the algorithms work also when data does not fit entirely in memory and has to be read from disk in batches. The algorithms never require the entire data set to be processed at once. Our contributions are summarized as follows. 1. We propose a clustering-based iterative algorithm to solve certain optimization problems, where an optimality condition is derived for each problem. The proposed algorithmic framework can be applied to other problems with certain structural properties (even outside of machine learning). The algorithm is most beneficial when the time complexity of the original optimization problem is high. 2. We present model specific disaggregation and cluster partitioning procedures based on the optimality condition, which is one of the keys for achieving optimality. 3. For the selected machine learning problems, i.e., LAD and SVM, we show that the algorithm monotonically converges to a global optimum, while providing the optimality gap in each iteration. For S 3 VM, we provide the optimality condition. We present the algorithmic framework in Section 2 and apply it to LAD, SVM, and S 3 VM in Section 3. A computational study is provided in Section 4, followed by a discussion on the characteristic of the algorithm and how to develop the algorithm for other problems in Section 5.",
  "body": "Introduction In this paper, we propose a clustering-based iterative algorithm to solve certain optimization problems in machine learning when data size is large and thus it becomes impractical to use out-of-the-box algorithms. We rely on the principle of data aggregation and then subsequent disaggregations. While it is standard practice to aggregate the data and then calibrate the machine learning algorithm on aggregated data, we embed this into an iterative framework where initial aggregations are gradually disaggregated to the extent that even an optimal solution is obtainable. Early studies in data aggregation consider transportation problems [1, 10] , where either demand or supply nodes are aggregated. Zipkin [31] studied data aggregation for linear programming (LP) and derived error bounds of the approximate solution. There are also studies on data aggregation for 0-1 integer programming [8, 13] . The reader is referred to Rogers et al [22] and Litvinchev and Tsurkov [16] for comprehensive literature reviews for aggregation techniques applied for optimization problems. For support vector machines (SVM), there exist several works using the concept of clustering or data aggregation. Evgeniou and Pontil [11] proposed a clustering algorithm that creates large size clusters for entries surrounded by the same class and small size clusters for entries in the mixed-class area. The clustering algorithm is used to preprocess the data and the clustered data is used to solve the problem. The algorithm tends to create large size clusters for entries far from the decision boundary and small size clusters for the other case. Wang et al [26] developed screening rules for SVM to discard non-support vectors that do not affect the classifier. Nath et al [19] and Doppa et al [9] proposed a second order cone programming (SOCP) formulation for SVM based on chance constraints and clusters. The key idea of the SOCP formulations is to reduce the number of constraints (from the number of the entries to number of clusters) by defining chance constraints for clusters. After obtaining an approximate solution by solving the optimization problem with aggregated data, a natural attempt is to use less-coarsely aggregated data, in order to obtain a finer approximation. In fact, we can do this iteratively: modify the aggregated data in each iteration based on the information at hand. This framework, which iteratively passes information between the original problem and the aggregated problem [22] , is known as Iterative Aggregation Disaggregation (IAD). The IAD framework has been applied for several optimization problems such as LP [17, 24, 25] and network design [2] . In machine learning, Yu et al [28, 29] used hierarchical micro clustering and a clustering feature tree to obtain an approximate solution for support vector machines. In this paper, we propose a general optimization algorithm based on clustering and data aggregation, and apply it to three common machine learning problems: least absolute deviation regression (LAD), SVM, and semi-supervised support vector machines (S 3 VM). The algorithm fits the IAD framework, but has additional properties shown for the selected problems in this paper. The ability to report the optimality gap and monotonic convergence to global optimum are features of our algorithm for LAD and SVM, while our algorithm guarantees optimality for S 3 VM without monotonic convergence. Our work for SVM is distinguished from the work of Yu et al [28, 29] , as we iteratively solve weighted SVM and guarantee optimality, whereas they iteratively solve the standard unweighted SVM and thus find only an approximate solution. On the other hand, it is distinguished from Evgeniou and Pontil [11] , as our algorithm is iterative and guarantees global optimum, whereas they used clustering to preprocess data and obtain an approximate optimum. Nath et al [19] and Doppa et al [9] are different because we use the typical SVM formulation within an iterative framework, whereas they propose an SOCP formulation based on chance constraints. Our data disaggregation and cluster partitioning procedure is based on the optimality condition derived in this paper: relative location of the observations to the hyperplane (for LAD, SVM, S 3 VM) and labels of the observations (for SVM, S 3 VM). For example, in the SVM case, if the separating hyperplane divides a cluster, the cluster is split. The condition for S 3 VM is even more involved since a single cluster can be split into four clusters. In the computational experiment, we show that our algorithm outperforms the current state-of-the-art algorithms when the data size is large. The implementation of our algorithms is based on in-memory processing, however the algorithms work also when data does not fit entirely in memory and has to be read from disk in batches. The algorithms never require the entire data set to be processed at once. Our contributions are summarized as follows. 1. We propose a clustering-based iterative algorithm to solve certain optimization problems, where an optimality condition is derived for each problem. The proposed algorithmic framework can be applied to other problems with certain structural properties (even outside of machine learning). The algorithm is most beneficial when the time complexity of the original optimization problem is high. 2. We present model specific disaggregation and cluster partitioning procedures based on the optimality condition, which is one of the keys for achieving optimality. 3. For the selected machine learning problems, i.e., LAD and SVM, we show that the algorithm monotonically converges to a global optimum, while providing the optimality gap in each iteration. For S 3 VM, we provide the optimality condition. We present the algorithmic framework in Section 2 and apply it to LAD, SVM, and S 3 VM in Section 3. A computational study is provided in Section 4, followed by a discussion on the characteristic of the algorithm and how to develop the algorithm for other problems in Section 5. Algorithm: Aggregate and Iterative Disaggregate (AID) We start by defining a few terms. A data matrix consists of entries (rows) and attributes (columns). A machine learning optimization problem needs to be solved over the data matrix. When the entries of the original data are partitioned into several sub-groups, we call the sub-groups clusters and we require every entry of the original data to belong to exactly one cluster. Based on the clusters, an aggregated entry is created for each cluster to represent the entries in the cluster. This aggregated entry (usually the centroid) represents one cluster, and all aggregated entries are considered in the same attribute space as the entries of the original data. The notion of the aggregated data refers to the collection of the aggregated entries. The aggregated problem is a similar optimization problem to the original optimization problem, based on the aggregated data instead of the original data. Declustering is the procedure of partitioning a cluster into two or more sub-clusters. We consider optimization problems of the type min x,y n i=1 f i (x i ) + f (y) s.t. g 1 i (x i , y) ≥ 0, for every i = 1, • • • , n, g 2 i (x i ) ≥ 0, for every i = 1, • • • , n, g(y) ≥ 0, ( 1 ) where n is the number of entries of x, x i is i th entry of x, and arbitrary functions f i , g 1 i , and g 2 i are defined for every i = 1, • • • , n. One of the common features of such problems is that the data associated with x is aggregated in practice and an approximate solution can be easily obtained. Well-known problems such as LAD, SVM, and facility location fall into this category. The focus of our work is to design a computationally tractable algorithm that actually yields an optimal solution in a finite number of iterations. Our algorithm needs four components tailored to a particular optimization problem or a machine learning model. 1. A definition of the aggregated data is needed to create aggregated entries. 2. Clustering and declustering procedures (and criteria) are needed to cluster the entries of the original data and to decluster the existing clusters. 3. An aggregated problem (usually weighted version of the problem with the aggregated data) should be defined. 4. An optimality condition is needed to determine whether the current solution to the aggregated problem is optimal for the original problem. The overall algorithm is initialized by defining clusters of the original entries and creating aggregated data. In each iteration, the algorithm solves the aggregated problem. If the obtained solution to the aggregated problem satisfies the optimality condition, then the algorithm terminates with an optimal solution to the original problem. Otherwise, the selected clusters are declustered based on the declustering criteria and new aggregated data is created. The algorithm continues until the optimality condition is satisfied. We refer to this algorithm, which is summarized in Algorithm 1, as Aggregate and Iterative Disaggregate (AID). Observe that the algorithm is finite as we must stop when each cluster is an entry of the original data. In the computational experiment section, we show that in practice the algorithm terminates much earlier. Algorithm 1 AID (Aggregate and Iterative Disaggregate) 1: Create clusters and aggregated data 2: Do 3: Solve aggregated problem 4: Check optimality condition 5: if optimality condition is violated then decluster the clusters and redefine aggregated data 6: While optimality condition is not satisfied In Figure 1 , we illustrate the concept of the algorithm. In Figure 1 (a), small circles represent the entries of the original data. They are partitioned into three clusters (large dotted circles), where the crosses represent the aggregated data (three aggregated entries). We solve the aggregated problem with the three aggregated entries in Figure 1(a) . Suppose that the aggregated solution does not satisfy the optimality condition and that the declustering criteria decide to partition all three clusters. In Figure 1 (b), each cluster in Figure 1 (a) is split into two sub-clusters. Suppose that the optimality condition is satisfied after several iterations. Then, we terminate the algorithm with guaranteed optimality. Figure 1 (c) represents possible final clusters after several iterations from Figure 1(b) . Observe that some of the clusters in Figure 1 (b) remain the same in Figure 1 (c), due to the fact that we selectively decluster. We use the following notation in subsequent sections. I = {1, 2, • • • , n}: Index set of entries, where n is the number of entries (observations) J = {1, 2, • • • , m}: Index set of attributes, where m is the number of attributes K t = {1, 2, • • • , |K t |}: Index set of the clusters in iteration t C t = {C t 1 , C t 2 , • • • , C t |K t | }: Set of clusters in iteration t, where C t k is a subset of I for any k in K t T : Last iteration of the algorithm when the optimality condition is satisfied 3 AID for Machine Learning Problems Least Absolute Deviation Regression The multiple linear least absolute deviation regression problem (LAD) can be formulated as E * = min β∈R m i∈I |y i - j∈J x ij β j |, (2) where x = [x ij ] ∈ R n×m is the explanatory variable data, y = [y i ] ∈ R n is the response variable data, and β ∈ R m is the decision variable. Since the objective function of ( 2 ) is the summation of functions over all i in I, LAD fits (1), and we can use AID. Let us first define the clustering method. Given target number of clusters |K 0 |, any clustering algorithm can be used to partition n entries into |K 0 | initial clusters C 0 = {C 0 1 , C 0 2 , • • • , C 0 |K 0 | }. Given C t in iteration t, for each k ∈ K t , we generate aggregated data by x t kj = i∈C t k x ij |C t k | , for all j ∈ J, and y t k = i∈C t k yi |C t k | , where x t ∈ R |K t |×m and y t ∈ R |K t |×1 . To balance the clusters with different cardinalities, we give weight |C t k | to the absolute error associated with C t k . Hence, we solve the aggregated problem F t = min β t ∈R m k∈K t |C t k ||y t k - j∈J x t kj β t j |. (3) Observe that any feasible solution to (3) is a feasible solution to (2) . Let βt be an optimal solution to (3). Then, the objective function value of βt to (2) with the original data is E t = i∈I |y i - j∈J x ij βt j |. (4) Next, we present the declustering criteria and construction of C t+1 . Given C t and βt , we define the clusters for iteration t + 1 as follows. Step 1 C t+1 = ∅. Step 2 For each k ∈ K t , Step 2(a) If y i -j∈J x ij βt j for all i ∈ C t k have the same sign, then C t+1 ← C t+1 ∪ {C t k }. Step 2(b) Otherwise, decluster C t k into two clusters: C t k+ = {i ∈ C t k |y i -j∈J x ij βt j > 0} and C t k-= {i ∈ C t k |y i -j∈J x ij βt j ≤ 0}, and set C t+1 ← C t+1 ∪ {C t k+ , C t k-}. The above procedure keeps cluster C t k if all original entries in the clusters are on the same side of the regression hyperplane. Otherwise, the procedure splits C t k into two clusters C t k+ and C t k-, where the two clusters contain original entries on the one and the other side of the hyperplane. It is obvious that this rule implies a finite algorithm. In Figure 2 , we illustrate AID for LAD. In Figure 2 (a), the small circles and crosses represent the original and aggregated entries, respectively, where the large dotted circles are the clusters associated with the aggregated entries. The straight line represents the regression line βt obtained from an optimal solution to (3). In Figure 2 (b), the shaded and empty circles are the original entries below and above the regression line, respectively. Observe that two clusters have original entries below and above the regression line. Hence, we decluster the two clusters based on the declustering criteria and obtain new clusters and aggregated data for the next iteration in Figure 2(c ).  Now we are ready to present the optimality condition and show that βt is an optimal solution to (2) when the optimality condition is satisfied. The optimality condition presented in the following proposition is closely related to the clustering criteria. Proposition 1. If y i -j∈J x ij βt j for all i ∈ C t k have the same sign for all k ∈ K t , then βt is an optimal solution to (2). In other words, if all entries in C t k are on the same side of the hyperplane defined by βt for all k ∈ K t , then βt is an optimal solution to (2). Further, E t = F t . Proof. Let β * be an optimal solution to (2) . Then, we derive E * = i∈I |y i - j∈J x ij β * j | = k∈K t i∈C t k |y i - j∈J x ij β * j | ≥ k∈K t | i∈C t k y i - j∈J x ij β * j | = k∈K t |C t k ||y t k - j∈J x t kj β * j | ≥ k∈K t |C t k ||y t k - j∈J x t kj βt j | = k∈K t | i∈C t k y i - i∈C t k j∈J x ij βt j | = k∈K t i∈C t k |y i - j∈J x ij βt j | = i∈I |y i - j∈J x ij βt j | = E t , where the third line holds since βt is optimal to (3) and the fourth line is based on the condition that all observations in C t k are on the same side of the hyperplane defined by βt , for all k ∈ K t . Since βt is feasible to (2) , clearly E * ≤ E t , which shows E * = E t . This implies that βt is an optimal solution to (2). Observe that k∈K t |C t k ||y t k -j∈J x t kj βt j | in the fifth line is equivalent to F t . Hence, we also showed E t = F t by the fifth to ninth lines. We also show the non-decreasing property of F t in t and the convergence. Proposition 2. We have F t-1 ≤ F t for t = 1, • • • , T . Further, F T = E T = E * . Proof. For simplicity, let us assume that {C t-1 1 } = C t-1 \\ C t , {C t 1 , C t 2 } = C t \\ C t-1 , and C t-1 1 = C t 1 ∪ C t 2 . That is, C t-1 1 is the only cluster in C t-1 such that the entries in C t-1 1 have both positive and negative signs, and C t-1 1 is partitioned into C t 1 and C t 2 for iteration t. Then, we derive F t-1 = C t-1 1 y t-1 1 - j∈J x t-1 1j βt-1 j + k∈K t-1 \\{1} C t-1 k y t-1 k - j∈J x t-1 kj βt-1 j ≤ C t-1 1 y t-1 1 - j∈J x t-1 1j βt j + k∈K t-1 \\{1} C t-1 k y t-1 k - j∈J x t-1 kj βt j = i∈C t-1 1 y i - i∈C t-1 1 j∈J x ij βt j + k∈K t-1 \\{1} C t-1 k y t-1 k - j∈J x t-1 kj βt j = i∈C t 1 y i - i∈C t 1 j∈J x ij βt j + i∈C t 2 y i - i∈C t 2 j∈J x ij βt j + k∈K t-1 \\{1} C t-1 k y t-1 k - j∈J x t-1 kj βt j ≤ i∈C t 1 y i - i∈C t 1 j∈J x ij βt j + i∈C t 2 y i - i∈C t 2 j∈J x ij βt j + k∈K t-1 \\{1} C t-1 k y t-1 k - j∈J x t-1 kj βt j = C t 1 y t 1 - j∈J x t 1j βt j + C t 2 y t 2 - j∈J x t 2j βt j + k∈K t-1 \\{1} C t-1 k y t-1 k - j∈J x t-1 kj βt j = C t 1 y t 1 - j∈J x t 1j βt j + C t 2 y t 2 - j∈J x t 2j βt j + k∈K t \\{1,2} C t k y t k - j∈J x t kj βt j = F t , where the second line holds since βt-1 is an optimal solution to the aggregate problem in iteration t -1, and the seventh line follows from the fact that there exist q ∈ K t-1 \\ {1} and k ∈ K t \\ {1, 2} such that C t-1 q = C t k . For the cases with multiple clusters in t are declustered, we can use the similar technique. This completes the proof. By Proposition 2, in any iteration, F t can be interpreted as a lower bound to (2) . Further, the optimality gap E best -F t E best is non-increasing in t, where E best = min s=1,••• ,t {E s }. Support Vector Machines One of the most popular forms of support vector machines (SVM) includes a kernel satisfying the Mercer's theorem [18] and soft margin. Let φ : x i ∈ R m → φ(x i ) ∈ R m be the mapping function that maps from the m-dimensional original feature space to m -dimensional new feature space. Then, the primal optimization problem for SVM is written as E * = min w,b,ξ 1 2 w 2 + M ξ 1 s.t. y i (wφ(x i ) + b) ≥ 1 -ξ i , ξ i ≥ 0, i ∈ I, (5) where x = [x ij ] ∈ R n×m is the feature data, y = [y i ] ∈ {-1, 1} n is the class (label) data, and w ∈ R m , b ∈ R, and ξ ∈ R n + are the decision variables, and the corresponding dual optimization problem is written as max i∈I α i - 1 2 i,j∈I K(x i , x j )α i α j y i y j s.t. i∈I α i y i = 0, 0 ≤ α i ≤ M, i ∈ I, (6) where K(x i , x j ) = φ(x i ), φ(x j ) is the kernel function. In this case, φ(x i ) in ( 5 ) can be interpreted as new data in m -dimensional feature space with linear kernel. Hence, without loss of generality, we derive all of our findings in this section for (5) with the linear kernel, while all of the results hold for any kernel function satisfying the Mercer's theorem. However, in Appendix B, we also describe AID with direct use of the kernel function. By using the linear kernel, ( 5 ) is simplified as E * = min w,b,ξ 1 2 w 2 + M ξ 1 s.t. y i (wx i + b) ≥ 1 -ξ i , ξ i ≥ 0, i ∈ I, (7) where w ∈ R m . Since • 1 in the objective function of (7) is the summation of the absolute values over all i in I and the constraints are defined for each i in I, SVM fits (1) . Hence, we apply AID to solve (7) . Let us first define the clustering method. The algorithm maintains that the observations i 1 and i 2 with different labels (y i1 = y i2 ) cannot be in the same cluster. We first cluster all data i with y i = 1, and then we cluster those with y i = -1. Thus we run the clustering algorithm twice. This gives initial clusters C 0 = {C 0 1 , C 0 2 , • • • , C 0 |K 0 | }. Given C t in iteration t, for each k ∈ K t , we generate aggregated data by x t k = i∈C t k x i |C t k | and y t k = i∈C t k yi |C t k | ∈ {-1, 1}, where x t ∈ R |K t |×m and y t ∈ {-1, 1} |K t | . Note that, since we create a cluster with observations with the same label, we have y t k = y i for all i ∈ C t k . (8) By giving weight |C t k | to ξ t k , we obtain F t = min w t ,b t ,ξ t 1 2 w t 2 + M k∈K t |C t k |ξ t k s.t. y t k w t x t k + b t ≥ 1 -ξ t k , ξ t k ≥ 0, k ∈ K t , (9) where w t ∈ R m , b t ∈ R, and ξ t ∈ R |K t | + are the decision variables. Note that ξ in (7) has size of n, whereas the aggregated data has |K t | entries. Note also that (9) is weighted SVM [27] , where weight is |C t k | for aggregated entry k ∈ K t . Next we present the declustering criteria and construction of C t+1 . Let (w * , ξ * , b * ) and ( wt , ξt , bt ) be optimal solutions to ( 7 ) and ( 9 ), respectively. Given C t and ( wt , ξt , bt ), we define the clusters for iteration t + 1 as follows. Step 1 C t+1 ← ∅. Step 2 For each k ∈ K t , Step 2(a) If (i) 1 -y i ( wt x i + bt ) ≤ 0 for all i ∈ C t k or (ii) 1 -y i ( wt x i + bt ) > 0 for all i ∈ C t k , then C t+1 ← C t+1 ∪ {C t k }. Step 2(b) Otherwise, decluster C t k into two clusters: C t k+ = {i ∈ C t k |1 -y i ( wt x i + bt ) > 0} and C t k-= {i ∈ C t k |1 -y i ( wt x i + bt ) ≤ 0}, and set C t+1 ← C t+1 ∪ {C t k+ , C t k-}. In Figure 3 , we illustrate AID for SVM. In Figure 3 (a), the small white circles and crosses represent the original entries with labels 1 and -1, respectively. The small black circles and crosses represent the aggregated entries, where the large circles are clusters associated with the aggregated entries. The plain line represents the separating hyperplane ( wt , bt ) obtained from an optimal solution to (9) , where the margins are implied by the dotted lines. The shaded large circles represent the clusters violating the optimality condition in Proposition 3. In Figure 3 (b), below the bottom dotted line is the area such that observations with label 1 (circles) have zero error and above the top dotted line is the area such that observations with label -1 (crosses) have zero error. Observe that two clusters have original entries below and above the corresponding dotted lines. Based on the declustering criteria, the two clusters are declustered and we obtain new clusters in Figure 3(c) . Note that a feasible solution to (7) does not have the same dimension as a feasible solution to (9) . In order to analyze the algorithm, we convert feasible solutions to (7) and (9) to feasible solutions to ( 9 ) and (7) , respectively.  1. Conversion from (7) to (9) Given a feasible solution (w, b, ξ) 9 ) as follows: w t := w, b t := b, and 9 ) to (7) Given a feasible solution (w 7 ) as follows: w := w t , b := b t , and ∈ (R m , R, R n + ) to (7), we define a feasible solution (w t , b t , ξ t ) ∈ (R m , R, R |K t | + ) to ( ξ t k := i∈C t k ξi |C t k | for k ∈ K t . 2. Conversion from ( t , b t , ξ t ) ∈ (R m , R, R |K t | + ) to (9), we define a feasible solution (w, b, ξ) ∈ (R m , R, R n + ) to ( ξ i := max{0, 1 -y i (w t x i + b t )} for i ∈ I. Given an optimal solution (w * , b * , ξ * ) to (7) , by using the above mappings, we denote by ( ŵ * , b * , ξ * ) the corresponding feasible solution to (9) . Likewise, given an optimal solution ( wt , bt , ξt ) to ( 9 ), we denote by ( ŵt , bt , ξt ) the corresponding feasible solution to (7) . The objective function value of ( ŵt , bt , ξt ) to ( 7 ) is evaluated by E t = 1 2 ŵt 2 + M ξt 1 . (10) In Propositions 3 and 4, we present the optimality condition and monotone convergence property. Proposition 3. For all k ∈ K t , if (i) 1 -y i ( wt x i + bt ) ≤ 0 for all i ∈ C t k or (ii) 1 -y i ( wt x i + bt ) > 0 for all i ∈ C t k , then ( ŵt , bt , ξt i ) is an optimal solution to (7) . In other words, if all entries in C t k are on the same side of the margin-shifted hyperplane of the separating hyperplane ( wt , bt ), then ( ŵt , bt , ξt i ) is an optimal solution to (7) . Further, E t = F t . Proof. We can derive 1 2 w * 2 + M i∈I ξ * i = 1 2 w * 2 + M k∈K t |C t k | i∈C t k ξ * i |C t k | = 1 2 ŵ * 2 + M k∈K t |C t k | ξ * k ≥ 1 2 wt 2 + M k∈K t |C t k | ξt k 1 2 w * 2 + M i∈I ξ * i = 1 2 wt 2 + M k∈K t max{0, |C t k | -|C t k |y t k ( wt x t k + bt )} = 1 2 wt 2 + M k∈K t max{0, |C t k | -y t k wt i∈C t k x i -y t k |C t k | bt } = 1 2 wt 2 + M k∈K t max 0, i∈C t k [1 -y i ( wt x i + bt )] = 1 2 wt 2 + M k∈K t i∈C t k max{0, 1 -y i ( wt x i + bt )} = 1 2 ŵt 2 + M k∈K t i∈C t k ξt i ≥ 1 2 w * 2 + M i∈I ξ * i , where the second line follows from the definition of ξ * k , the third line holds since ( wt , bt , ξt ) is an optimal solution to (9), the fourth line is by the definition of ξt , the fifth line is by the definition of x t k , the eighth line is true because of the assumption such that all observations are on the same side of the margin-shifted hyperplane of the separating hyperplane (optimality condition), and the last line holds since (w * , b * , ξ * ) is an optimal solution to (7) . Observe that the inequalities above must hold at equality. This implies that ( ŵt , bt , ξt i ) is an optimal solution to (7) . Because ( ŵt , bt ) defines an optimal hyperplane, we are also able to obtain the corresponding dual optimal solution αt ∈ R n for (6). However, unlike primal optimal solutions, αt cannot be directly constructed from dual solution ᾱt ∈ R |K t | of ( wt , bt ) for the aggregated problem within the current settings. Within a modified setting presented later in this section, we can explain the relationship between ᾱt and αt , modified optimality condition, declustering procedure, and the construction of αt based on ᾱt . Proposition 4. We have F t-1 ≤ F t for t = 1, • • • , T . Further, F T = E T = E * . Proof. Recall that ( wt , bt , ξt ) is an optimal solution to (9) with aggregated data x t and y t . Let ( wt-1 , bt-1 , ξt-1 ) be a feasible solution to (9) with aggregated data x t-1 and y t-1 such that wt-1 = wt , bt-1 = bt , and ξt-1 = max{0, 1 -y t-1 k ( wt x t-1 k + bt )} for k ∈ K t-1 . In other words, ( wt-1 , bt-1 , ξt-1 ) is a feasible solution to (9) with aggregated data x t-1 and y t-1 , but generated based on ( wt , bt , ξt ). For simplicity, let us assume that {C t-1 1 } = C t-1 \\ C t , {C t 1 , C t 2 } = C t \\ C t-1 , and C t-1 1 = C t 1 ∪ C t 2 . The cases such that more than one cluster of C t-1 are declustered in iteration t can be derived using the same technique. Observe that there exists a pair (q, k), q ∈ K t-1 \\ {1} and k ∈ K t \\ {1, 2} such that ξt-1 q = ξt k (11) for all q in K t-1 \\ {1} and the match between K t-1 \\ {1} and K t \\ {1, 2} is one-to-one. This is because the aggregated data for these clusters remains same and the hyper-plane used, ( wt-1 , bt-1 ) and ( wt , bt ), are the same. Hence, we derive F t-1 = 1 2 wt-1 2 + M |C t-1 1 | ξt-1 1 + M k∈K t-1 \\{1} |C t-1 k | ξt-1 k ≤ 1 2 wt-1 2 + M |C t-1 1 | ξt-1 1 + M k∈K t-1 \\{1} |C t-1 k | ξt-1 k F t-1 ≤ 1 2 wt 2 + M |C t-1 1 | ξt-1 1 + M k∈K t \\{1,2} |C t k | ξt k ≤ 1 2 wt 2 + M |C t 1 | ξt 1 + M |C t 2 | ξt 2 + M k∈K t \\{1,2} |C t k | ξt k F t-1 = 1 2 wt 2 + M k∈K t |C t k | ξt k = F t , where the first inequality holds because wt-1 , bt-1 , ξt-1 is an optimal solution to (9) with aggregated data x t-1 and y t-1 , the second inequality follows by the fact that wt-1 = wt and by (11) , and the last inequality is true because |C t-1 1 | ξt-1 1 = |C t-1 1 | max 0, 1 -y t-1 1 ( wt x t-1 1 + bt ) = |C t-1 1 | max 0, 1 -y t-1 1 ( wt x t-1 1 + bt ) = max 0, |C t-1 1 | -|C t-1 1 |y t-1 1 wt x t-1 1 -|C t-1 1 |y t-1 1 bt = max 0, |C t 1 | + |C t 2 | -y t-1 1 wt ( i∈C t 1 x i + i∈C t 2 x i ) -|C t 1 |y t 1 bt -|C t 2 |y t 2 bt |C t-1 1 | ξt-1 1 = max 0, |C t 1 | -|C t 1 |y t 1 wt x t 1 -|C t 1 |y t 1 bt + |C t 2 | -|C t 2 |y t 2 wt x t 2 -|C t 2 |y t 2 bt ≤ max 0, |C t 1 | -|C t 1 |y t 1 wt x t 1 -|C t 1 |y t 1 bt + max 0, |C t 2 | -|C t 2 |y t 2 wt x t 2 -|C t 2 |y t 2 bt = |C t 1 | max{0, 1 -y t 1 ( wt x t 1 + bt )} + |C t 2 | max{0, 1 -y t 2 ( wt x t 2 + bt )} = |C t 1 | ξt 1 + |C t 2 | ξt 2 . where the fourth line holds by (i ) |C t-1 1 | = |C t 1 | + |C t 2 |,(ii) y t 1 = y t 2 = y t-1 1 (by ( 8 )), and (iii) by the definition of x t-1 1 , and the fifth line holds due to the definition of x t 1 and x t 2 . This completes the proof. So far, we have explained the algorithm based on the primal formulation of SVM. However, we can also explain the relationship between the dual of the original and aggregated problems by proposing a modified procedure. Let us divide observations in C t k into three sets. 1. 1 -y i ( wt x i + bt ) < 0 for i ∈ C t k 2. 1 -y i ( wt x i + bt ) = 0 for i ∈ C t k 3. 1 -y i ( wt x i + bt ) > 0 for i ∈ C t k These three sets correspond to the following three cases for the original data given hyperplane ( wt , bt ) from an optimal solution of (9). 1. Observations correctly classified: 1 -y i ( wt x i + bt ) < 0 and ξt i = 0 in (7) and αi = 0 in the dual of ( 7 ) 2. Observations on the hyperplane: 1 -y i ( wt x i + bt ) = 0 and ξt i = 0 in (7) and 0 < αi < M in the dual of (7) 3. Observations in the margin or misclassified: 1 -y i ( wt x i + bt ) > 0 and ξt i > 0 in (7) and αi = M in the dual of (7) Suppose we are given ᾱt , a dual optimal solution that corresponds to ( wt , bt ). Then we can construct dual optimal solution αt for the original problem from ᾱt by αt i = ᾱt |C k | for i ∈ C t k , k ∈ K t . With this definition, now all original observations in a cluster belong to exactly one of the three categories above. We first show that αt is a feasible solution. Let us consider cluster k ∈ K t . We derive i∈C t k αt i y i = i∈C t k αt i y k = i∈C t k ᾱt k |C t k | y k = y k |C t k | ᾱt k |C t k | = y k ᾱt k , where the first equality holds because all labels are the same for a cluster and the second equality is obtained by plugging the definition of αt i . Because y k ᾱt k = i∈C t k αt i y i and k∈K t y k ᾱt k = 0, we conclude that αt is a feasible solution to (6) . In order to show optimality, we show that αt and ᾱt give the same hyperplane. Let us consider cluster k ∈ K t . We derive i∈C t k αt i y i x i = i∈C t k αt i y k x i = i∈C t k ᾱt k |C t k | y k x i = ᾱt k y k i∈C t k xi |C t k | = ᾱt k y k x t k , where the first equality holds because all labels are the same for a cluster, the second equality is obtained by plugging the definition of αt i , and the last equality is due to the definition of x t k . Because ᾱt k y k x t k = i∈C t k αt i y i x i , by summing over all clusters, we obtain wt = ᾱt k y k x t k = i∈C t k αt i y i x i = ŵt , which completes the proof. Semi-Supervised SVM The task of semi-supervised learning is to decide classes of unlabeled (unsupervised) observations given some labeled (supervised) observations. Semi-supervised SVM (S 3 VM) is an SVM-based learning model for semi-supervised learning. In S 3 VM, we need to decide classes for unlabeled observations in addition to finding a hyperplane. Let I l = {1, • • • , l} and I u = {l + 1, • • • , n} be the index sets of labeled and unlabeled observations, respectively. The standard S 3 VM with linear kernel is written as the following minimization problem over both the hyperplane parameters (w, b) and the unknown label vector d := [d l+1 • • • d n ], E * = min w,b,d 1 2 w 2 + M l i∈I l max{0, 1 -y i (wx i + b)} + M u i∈Iu max{0, 1 -d i (wx i + b)}, (12) where 12 ) is rewritten as x = [x ij ] ∈ R n×m is the feature data, y = [y i ] ∈ {-1, 1} l is E * = min w,ξ,b,d 1 2 w 2 + M l i∈I l ξ i + M u i∈Iu ξ i s.t. y i (wx i + b) ≥ 1 -ξ i , ξ i ≥ 0, i ∈ I l , d i (wx i + b) ≥ 1 -ξ i , ξ i ≥ 0, i ∈ I u . (13) Observe that y i , i ∈ I l , is given as data, whereas d i , i ∈ I u , is unknown and decision variable. Note that (13) has non-convex constraints. In order to eliminate the non-convex constraints, Bennett and Demiriz [3] proposed a mixed integer quadratic programming (MIQP) formulation E * = min w,b,d ξ,η + ,η - 1 2 w 2 + M l i∈I l ξ i + M u i∈Iu η + i + η - i s.t. y i (wx i + b) ≥ 1 -ξ i , ξ i ≥ 0, i ∈ I l , wx i + b + η + i + M (1 -d i ) ≥ 1, i ∈ I u , 0 ≤ η + i ≤ M d i , i ∈ I u , -(wx i + b) + η - i + M d i ≥ 1, i ∈ I u 0 ≤ η - i ≤ M (1 -d i ), i ∈ I u , d i ∈ {0, 1}, i ∈ I u , (14) where M > 0 is a large number and w ∈ R m , b ∈ R, d ∈ {-1, 1} |Iu| , η + ∈ R |Iu| + , η -∈ R |Iu| + are the decision variables. Note that d i in ( 14 ) is different from d i in (12) . In (14) , if d i = 1 then observation i is in class 1 and if d i = 0 then observation i is in class -1. Note also that, by the objective function, if d i = 1 then η - i becomes 0 and if d i = 0 then η + i becomes 0 at optimum. A Branch-and-Bound algorithm to solve ( 12 ) is proposed by Chapelle et al [6] and an MIQP solver is used to solve (14) in [3] . However, both of the works only solve small size problems. See Chapelle et al [7] for detailed survey of the literature. Observe that ( 14 ) fits (1) . Hence, we use AID to solve ( 14 ) with larger size instances, which were not solved by the works in [3, 6] . Similar to the approach used to solve SVM in Section 3.1, we define clusters C t = {C t 1 , C t 2 , • • • , C t |K t l | } for the labeled data, where K t l is the index set of the clusters of labeled data in iteration t and each cluster contains observations with same label. In addition, we have clusters for the unlabeled data D t = {D t 1 , D t 2 , • • • , D t |K t u | }, where K t u is the index set of the clusters of unlabeled data in iteration t. In the S 3 VM case, initially we need to run a clustering algorithm three times. We generate aggregated data by x t k = i∈C t k xi |C t k | and y t k = i∈C t k yi |C t k | for each k ∈ K t l given C t , x t k = i∈D t k xi |D t k | for each k ∈ K t u given D t . Using the aggregated data, we obtain the aggregated version of (13) as F t = min w t ,ξ t ,b t ,d t 1 2 w 2 + M l k∈K t l |C t k |ξ t k + M u k∈K t u |D t k |ξ t k s.t. y t k (w t x t k + b t ) ≥ 1 -ξ t k , ξ t k ≥ 0, k ∈ K t l , d t k (w t x t k + b t ) ≥ 1 -ξ t k , ξ t k ≥ 0, k ∈ K t u , (15) ywpark where y t k , k ∈ K t l , is known and d t k , k ∈ K t u , is unknown. Observe that (15) can be solved optimally by the Branch-and-Bound algorithm in [6] or by an MIQP solver. In the following lemma, we show that, given an optimal hyperplane (w * , b * ), optimal values of ξ * ∈ R n and d ∈ {0, 1} |Iu| can be obtained. Lemma 1. Let (w * , b * , ξ * , d * ) be an optimal solution for (13) . For i ∈ I u , if ξ * i > 0, then we must have d * i (w * x i + b * ) ≥ 0. For i ∈ I u , if ξ * i = 0 and max{0, 1 -d i (w * x i + b * )} = 0 only for one of d i = 1 or d i = -1, then we must have d * i = 1 if w * x i + b * ≥ 0, d * i = -1 if w * x i + b * < 0. A similar property holds for an optimal solution of the aggregated problem (15) . Proof. Suppose that ξ * i > 0. Hence, we have ξ * i = 1 -d * i (w * x i + b * ) > 0. If w * x i + b * < 0, then setting d * i = -1 decreases ξ * i most because 1 -(-1)(w * x i + b * ) < 1 -(1)(w * x i + b * ). Likewise, if w * x i + b * ≥ 0, then setting d * i = 1 decreases ξ * i . For the analysis, we define the following sets. 1. For labeled observations in I l , given hyperplane (w, b), let us define subsets of I l . I + (w,b) = {i ∈ I l |1 -y i (wx i + b) > 0} I - (w,b) = {i ∈ I l |1 -y i (wx i + b) ≤ 0} 2. For unlabeled observations in I u , given hyperplane (w, b) and labels d, let us define subsets of I u . Next we present the declustering criteria. Let (w * , b * , ξ * , d * ) and ( wt , bt , ξt , dt ) be optimal solutions to ( 13 ) and (15) , respectively. Given C t and ( wt , bt , ξt , dt ), we define the clusters for iteration t + 1 as follows. I ++ (w,b,d) = {i ∈ I u |1 -d i (wx i + b) > 0, wx i + b > 0} I +- (w,b,d) = {i ∈ I u |1 -d i (wx i + b) > 0, wx i + b ≤ 0} I -+ (w,b,d) = {i ∈ I u |1 -d i (wx i + b) ≤ 0, wx i + b > 0} I -- (w,b,d) = {i ∈ I u |1 -d i (wx i + b) ≤ 0, wx i + b ≤ 0} Note that d i that Step 1 C t+1 ← ∅, D t+1 ← ∅ Step 2 For each k ∈ K t l Step 2(a) If 1 -y i ( wt x i + bt ) ≤ 0 for all i ∈ C t k , or if 1 -y i ( wt x i + bt ) ≥ 0 for all i ∈ C t k , then C t+1 ← C t+1 ∪ {C t k } Step 2(b) Otherwise, first, decluster C t k into two clusters: C t k+ = {i ∈ C t k |1 -y i ( wt x i + bt ) > 0} and C t k-= {i ∈ C t k |1 -y i ( wt x i + bt ) ≤ 0}. Next, C t+1 ← C t+1 ∪ {C t k+ , C t k-}. Step 3 For each k ∈ K t u , Step 3(a) Partition D t k into four sub-clusters. D t k++ = {i ∈ D t k |1 -dt k ( wt x i + bt ) > 0, wt x i + bt > 0} = {i ∈ D t k ∩ I ++ ( wt , bt , dt ) } D t k+-= {i ∈ D t k |1 -dt k ( wt x i + bt ) > 0, wt x i + bt ≤ 0} = {i ∈ D t k ∩ I +- ( wt , bt , dt ) } D t k-+ = {i ∈ D t k |1 -dt k ( wt x i + bt ) ≤ 0, wt x i + bt > 0} = {i ∈ D t k ∩ I -+ ( wt , bt , dt ) } D t k--= {i ∈ D t k |1 -dt k ( wt x i + bt ) ≤ 0, wt x i + bt ≤ 0} = {i ∈ D t k ∩ I -- ( wt , bt , dt ) } Step 3(b) If one of D t k++ , D t k+-, D k-+ , and D t k--equals to D t k , then D t+1 ← D t+1 ∪ {D t k }. Otherwise, we set D t+1 ← D t+1 ∪ {D t k++ , D t k+-, D k-+ , D t k--}. Note that any of D t k++ , D t k+-, D t k-+ , or D t k--can be empty. In Figure 4 , we illustrate AID for S 3 VM. As labeled observations follow the illustration in Figure 3 , we only illustrate unlabeled observations. In Figure 4 (a), the small white circles are the original entries and the black circles are the aggregated entries. The plain line represents the separating hyperplane ( wt , bt ) obtained from an optimal solution to (15) , where the margins are implied by the dotted lines. The original and aggregated observations have been assigned to either + or -(1 or -1, respectively): the labels of aggregated entries are from the optimal solution of the aggregated problem, the labels of the original entries are based on ( wt , bt ) and Lemma 1. Observe that two clusters (gray large circles) violate the optimality conditions. In Figure 4 (b), one of the two violating clusters is partitioned into four subclusters: (i) entries with + labels and under the zero error boundary, (ii) entries with -labels and under the zero error boundary, (iii) entries with + labels and above the zero error boundary, and (iv) entries with -labels and above the zero error boundary. The other cluster is partitioned into two subclusters. Based on the declustering criteria, the two clusters are declustered and we obtain new clusters in Figure 4(c) . Note that new labels will be decided after solving (15) with the new aggregated data. Note that a feasible solution to (13) does not have the same dimension as a feasible solution to (15) . In order to analyze the algorithm, we convert a feasible solution to (15) to a feasible solution to (13) . Given a feasible solution (w (13) as follows.  Using the above procedure, we map an optimal solution ( wt , bt , ξt , dt ) to (15) to a feasible solution ( ŵt , bt , ξt , dt ) to (13) . The objective function value of ( ŵt , bt , ξt , dt ) is evaluated by t , b t , ξ t , d t ) ∈ (R m , R, R |K t l |+|K t u | , R |K t u |) to (15), we define a feasible solution (w, b, ξ, d) ∈ (R m , R, R n , R |Iu| ) to w := w t , b := b t d i = 1, if w t x i + b t < 0, -1, if w t x i + b t ≥ 0, for i ∈ D t k and k ∈ K t u , ξ i := max{0, 1 -y i (w t x i + b t )} for i ∈ I l ξ i := max{0, 1 -d t i (w t x i + b t ) E t = 1 2 ŵt 2 + M l i∈I l ξt i + M u i∈Iu ξt i . ( 16 ) We next explain the optimality condition and show its correctness. Let C g = {C g 1 , C g 2 , • • • , C g |K g l | } and D g = {D g 1 , D g 2 , • • • , D g |K g u | } be arbitrary clusters of labeled and unlabeled data where K g l = {1, 2, • • • , |K g l |} and K g u = {1, 2, • • • , |K g u |} are the associated index sets of clusters, respectively. Let us consider the following optimization problem. G * = min w,b,d, C g ,D g 1 2 w 2 + M l i∈I l max{0, 1 -y i (wx i + b)} + M u i∈Iu max{0, 1 -d i (wx i + b)} s.t. C g k ⊆ I + (w,b) or C g k ⊆ I - (w,b) , k ∈ K g l , D g k ⊆ I ++ (w,b,d) or D g k ⊆ I +- (w,b,d) or D g k ⊆ I -+ (w,b,d) or D g k ⊆ I -- (w,b,d) , k ∈ K g u , (17) where w ∈ R m , b ∈ R, d ∈ {-1, 1} |Iu| , and C g and D g are the cluster decision sets. In fact, compare to ( 12 ), (17) has additional constraints and clustering decision to make. Observe that given an optimal solution to (12), we can easily find C g and D g satisfying the constraints in ( 17 ) by simply classifying each observation. Hence, it is trivial to see that E * = G * . (18) For the analysis, we will use G * and (17) instead of E * and (12), respectively. Next, let us consider the following aggregated problem. Lemma 2. There is a one-to-one correspondence between feasible solutions of ( 17 ) and ( 19 ) which preserves the objective function value. H * = min w,b,d, C g ,D g 1 2 w 2 + M l k∈K g l |C g k | max{0, 1 -y k (wx k + b)} + M u k∈K g u |D g k | max{0, 1 -d k (wx k + b)} s.t. C g k ⊆ I + (w,b) or C g k ⊆ I - (w,b) , k ∈ K g l , D g k ⊆ I ++ (w,b,d) or D g k ⊆ I +- (w,b,d) or D g k ⊆ I -+ (w,b,d) or D g k ⊆ I -- (w,b,d) , k ∈ K g u , x k = i∈C g k xi |C g k | , y k = i∈C g k yi |C g k | , k ∈ K g l , x k = i∈D g k xi |D g k | , k ∈ K g u , (19) Proof. For k ∈ K g l , we derive i∈C g k max{0, 1 -y i (wx i + b)} = i∈C g k max{0, 1 -y k (wx i + b)} = max{0, i∈C g k 1 -y k (wx i + b) } i∈C g k max{0, 1 -y i (wx i + b)} = max{0, |C g k | -|C g k |y k w i∈C g k xi |C g k | -y k |C g k |b} = |C g k | max{0, 1 -y k (wx k + b)}, where the first line holds since all i in C g k have the same label by the initial clustering, the second line holds since C g k ⊆ I + (w,b) or C g k ⊆ I - (w,b) for any k ∈ K g l , and the fourth line follows from the constraint in (19) . For k ∈ K g u , we derive i∈D g k max{0, 1 -d i (wx i + b)} = i∈D g k max{0, 1 -d k (wx i + b)} = max{0, i∈D g k 1 -d k (wx i + b) } = max{0, |D g k | -|D g k |d k w i∈D g k xi |D g k | -d k |D g k |b} = |D g k | max{0, 1 -d k (wx k + b)}, where the first and second lines hold since D g k ⊆ I ++ (w,b,d) or D g k ⊆ I +- (w,b,d) or D g k ⊆ I -+ (w,b,d) or D g k ⊆ I -- (w,b,d) for any k ∈ K g u . Observe that the above two results can be shown in the reverse order. Hence, it is easy to see that there is a one-to-one correspondence between feasible solutions of ( 17 ) and ( 19 ), and the objective function values of the corresponding feasible solutions are the same. Note that Lemma 2 implies that, for an optimal solution of ( 19 ), the corresponding solution for ( 17 ) is an optimal solution for (17) . This gives the following corollary. Corollary 1. We have G * = H * . In Proposition 5, we present the optimality condition. Proposition 5. Let us assume that 1. for all k ∈ K t l , (i) 1 -y i ( wt x i + bt ) ≤ 0 for all i ∈ C t k or (ii) 1 -y i ( wt x i + bt ) ≥ 0 for all i ∈ C t k 2. for all k ∈ K t u , exactly one of the following holds. (i) 1 -dt i ( wt x i + bt ) ≤ 0 and wt x i + bt ≤ 0 for all i ∈ D t k (ii) 1 -dt i ( wt x i + bt ) ≤ 0 and wt x i + bt > 0 for all i ∈ D t k (iii) 1 -dt i ( wt x i + bt ) ≥ 0 and wt x i + bt > 0 for all i ∈ D t k (iv) 1 -dt i ( wt x i + bt ) ≥ 0 and wt x i + bt ≤ 0 for all i ∈ D t k Then, ( ŵt , bt , ξt , dt ) is an optimal solution to (13) . In other words, if (i) all observations in C t k and D t k are on the same side of the margin-shifted hyperplane of the separating hyperplane ( wt , bt ) and (ii) all observations in D t k have the same label, then ( ŵt , bt , ξt , dt ) is an optimal solution to (13). Proof. Observe that the conditions stated match with the definition of and I -- (w , b , d ) . Hence, C t and D t satisfy the constraints of ( 19 ), which implies that ( wt , bt , ξt , dt ) is an optimal solution to (19) . By Lemma 2, ( ŵt , bt , ξt , dt ) is an optimal solution to (17) . Finally, since E * = G * by ( 18 ), we conclude that ( ŵt , bt , ξt , dt ) is an optimal solution to (13). I + (w,b) , I - (w,b) , I ++ (w,b,d) , I +- (w,b,d) , I -+ (w,b,d) , Observe that, unlike LAD and SVM, we do not have the non-decreasing property of F t . Due to binary variable d i , the non-decreasing property of F t no longer holds. Computational Experiments All experiments were performed on Intel Xeon X5660 2.80 GHz dual core server with 32 GB RAM, running Windows Server 2008 64 bit. We implemented AID for LAD and SVM in scripts of R statistics [21] and Python, respectively, and AID for S 3 VM is implemented in C# with CPLEX. For LAD, R statistics package quantreg [14] is used to solve (2) and (3). In detail, function rq() is used with the Frisch-Newton interior point method (fn) option. Due to the absence of large-scale real world instances, we randomly generate three sets of LAD instances. Set A: n ∈ {2, 4, 8, 16} × 10 5 and m ∈ {10, 100, 500, 800}, where (n, m) = (16 × 10 5 , 800) is excluded due to memory issues Set B: n = 10 6 and m ∈ {50, 100, 150, 200, 250, 300, 350, 400, 450, 500} Set C: n ∈ {4, 6, 8, 10, 12, 14, 16} × 10 5 and m ∈ {50, 500} Set A is used for the experiment checking the performance of AID over various pairs of n and m, whereas Sets B and C are used for checking the performance of AID over fixed n and m, respectively. For SVM, Python package scikit-learn [20] is used to solve ( 7 ) and ( 9 ). In detail, functions svc() and linearSVC() are used, where the implementations are based on libsvm [4] and liblinear [12] , respectively. We use two benchmark algorithms because libsvm is one of the most popular and widely used implementation, while liblinear is known to be faster for SVM with the linear kernel. For SVM, we generate two sets of instances by sampling from two large data sets: (i) 1.3 million observations and 342 attributes obtained from a real world application provided by IBM (ii) rcv1.binary data set with 677,399 observations and 47,236 attributes from [4] . We denote them as IBM and RCV, respectively. This generation procedure enables us to analyze performances of the algorithms for data set with similar characteristics and various sizes. For each (n, m) pair for SVM, we generate ten instances and present the average performance of the ten instances for each (n, m) pair. Note that Set 1 instances are smaller than Set 2 instances. We use Set 1 to test AID against libsvm and Set 2 to test AID against liblinear, because liblinear is faster and capable of solving larger problems for SVM with the linear kernel. Recall that AID is capable of using any solver for the aggregated problem. Hence, when testing against libsvm, AID uses libsvm for solving the aggregated problems. Similarly, when testing against liblinear, AID uses liblinear. For S 3 VM, aggregated problems (15) are solved by CPLEX. We set the 1,800 seconds time limit for the entire algorithm. We consider eight semi-supervised learning benchmark data sets from [5] . Table 1 lists the characteristics of the data sets. For each data set, two sets of twelve data splits are given: one with 10 and the other with 100 labeled observations for each split set for the first seven data sets, whereas 1,000 and 10,000 labeled observations are used for each split set for SecStr. In all experiments, AID iterates until the corresponding optimality condition is satisfied or the optimality gap is small (we set 10 -3 and 10 -4 for LAD and SVM, respectively). The tolerance for fn and libsvm are set to 10 -3 according to the definition of the corresponding packages. The default tolerance for liblinear is 10 -3 with the maximum of 1,000 iterations. Because we observed early terminations and inconsistencies with the default setting, we use the maximum of 100,000 iterations for liblinear. For S 3 VM, we run AID for one and five iterations and terminate before we reach optimum. See Section 4.3 for detail. For the initial clustering methods for LAD, SVM, and S 3 VM, we do not fully rely on standard clustering algorithms, as it takes extensive amount of time to optimize a clustering objective function due to large data size. Instead, for the LAD initial clustering method, we first sample a small number of original entries, build regression model, and obtain a solution β init . Let r ∈ R n be the residual of the original data defined by β init . We use one iteration of k-means based on data (r, y) ∈ R n×2 to create C 0 . For the SVM initial clustering method, we sample a small number of original entries and find a hyperplane (w init , b init ). Then we use one iteration of k-means based on data d ∈ R n , where d ∈ R n is the distance of the original entries to (w init , b init ). For the S 3 VM initial clustering methods, we use one iteration of k-means based on the original data to create C 0 . The initial clustering and aggregated data generation times are small for all methods for LAD, SVM, and S 3 VM. For AID, we need to specify the number of initial clusters, measured as the initial aggregation rate. User parameter r 0 = |K 0 | n is the initial aggregation rate given as a parameter. It defines the number of initial clusters. We generalize the notion of the aggregation rate with r t , the aggregation rate in iteration t. The number of clusters is important because too many clusters lead to a large problem and too few clusters lead to a meaningless aggregated problem. Note that r 0 also should be problem specific. For example, we must have |K 0 | > m for LAD. Hence, we set r 0 = max{ 3m n , 0.0005} if m × n > 5 × 10 8 , r 0 = max{ 2m n , 0.0005} otherwise. This means that |K 0 | is at least two or three times larger than m and |K 0 | is at least 0.05% of n. For SVM, we set r 0 = max{ 1.1m  n , 0.0001} for all instances. However, since S 3 VM instances are not extremely large in our experiment, we fix it to some constant. For the S 3 VM instances, we test both of r 0 = 0.01 and 0.05 if n ≤ 10, 000, and both of r 0 = 0.0001 and 0.0005 otherwise, where the number of clusters must be at least 10 to avoid a meaningless aggregated problem. In order to compare the performance, execution times (in seconds) T AID , T fn , T libsvm , and T liblinear for AID, fn, libsvm, and liblinear, respectively, are considered. For SVM, standard deviations σ(T AID ), σ(T libsvm ), and σ(T liblinear ) are used to describe the stability of the algorithms. In order to further describe the performance of AID, we use the following measures. r T = |K T | n is the final aggregation rate at the termination ρ = T AID T fn or T AID T libsvm or T AID T liblinear ∆ = E AID -E fn E fn or E AID -E libsvm E libsvm or E AID -E liblinear E liblinear Γ = training classification rate of AID -training classification rate of benchmark For example, we set r 0 = 3 33 = 1  11 in Figure 1 (a) and terminate the algorithm with r T = 9 33 = 3 11 in Figure 1(c ). Note that ρ < 1 indicates that AID is faster. We use ∆ to check the relative difference of objective function values. For LAD, ∆ is also used to check if the solution qualities are the same. For SVM, Γ is used to measure the solution quality differences. Performance for LAD In Table 2 , the computational performance of AID for LAD is compared against the benchmark fn for Set A. For many LAD instances in Table 2 , fn is faster. For the smallest instance, fn is 34 times faster. However, ratio ρ decreases in general as n and m increase. This implies that AID is competitive for larger size data. In fact, AID is five times faster than fn for the two largest LAD instances considered. The values of ∆ indicate that fn and AID give the same quality solutions within numerical error bounds, as ∆ measures the relative difference in the sum of the absolute error. The final aggregation rate r T also depends on problem size. As n increases, r T decreases because original entries can be grouped into larger size clusters. As m increases, r T increases because it is more difficult to cluster the original entries into larger size clusters. Further discussion on how r t changes over iterations is presented in Section 5. In Figure 5 , comparisons of execution times of AID and fn are presented for Sets B and C. In Figure 5 (a), Set B is considered to check the performances over fixed n = 10 6 . With fixed n, AID is slower when m is small, but AID starts to outperform at m = 400. The corresponding ρ values are constantly decreasing from 6.6 (when m = 50) to 0.7 (when m = 500). This observation also supports the results presented in Figure 5 (b) and 5(c), where the comparisons of Set C are presented. When m is fixed to 50, the execution time of AID increases faster than fn. However, when m is fixed to 500, AID is faster than fn and the execution time of AID grows slower than fn. Therefore, we conclude that AID for LAD is faster than fn when n and m are large and is especially beneficial when m is large enough. For Figures 5(a ), 5(b), and 5(c), the corresponding number of iterations (T ) are randomly spread over 9 ∼ 11, 10 ∼ 12, and 8 ∼ 10, respectively; we did not find a trend. Instance AID fn Comparison n m r 0 r T T T AID T fn ∆ ρ 200,000 10 0.05% 2.40% 8 50 1 0.01% 34.59 200,000 100 0.10% 10.00% 9 84 22 0.01% 3.83 200,000 500 0.50% 16.50% 7 451 393 0.02% 1.15 200,000 800 0.80% 22.40% 7 1,347 1,062 0.01% 1.27 400,000 10 0.05% 2.20% 8 99 3 0.00% 29.84 400,000 100 0.05% 5.80% 9 163 44 0.01% 3.68 400,000 500 0.25% 13.30% 8 904 904 0.01% 1 400,000 800 0.40% 21.10% 8 2,689 2,125 0.01% 1.27 800,000 10 0.05% 0.90% 5 139 9 0.03% 15.46 800,000 100 0.05% 5.00% 9 336 96 0.01% 3.51 800,000 500 0.13% 10.30% 9 1,788 1,851 0.01% 0.97 800,000 800 0.30% 10.30% 7 2,992 15,215 0.03% 0.2 1,600,000 10 0.05% 0.40% 4 235 18 0.06% 13.29 1,600,000 100 0.05% 3.20% 8 612 196 0.01% 3.12 1,600,000 500 0.09% 5.35% 8 2,460 12,164 0.01% 0.2 Table 2: Performance of AID for LAD for Set A Performance for SVM In Tables 3 and 4 , the computational performance of AID for SVM is compared against the benchmark libsvm for Set 1. In Table 5 , comparison of AID and liblinear for Set 2 is presented. In all experiments, we fix penalty constant at M = 0.1. In Table 3 , the result for Set 1 IBM data is presented. Observe that AID is faster than libsvm for all cases, as ρ values are strictly less than 1 for all cases. Observe that ρ values tend to decrease in n and m. This is highly related to final aggregation rate r T . Observe that, similar to the LAD result, r T decreases in n and increase in m. As n increases, it is more likely to have clusters with more original entries. This decreases r T and number of iterations T . It implies that we solve fewer aggregated problems and the sizes of the aggregated problems are smaller. On the other hand, as m increases, r T also increases. This increases T and aggregated problem sizes. However, since the complexity of svmlib increases faster than AID in increasing m, ρ decreases in m. Due to possibility of critical numerical errors, we also check the objective function value differences (∆) and training classification rate differences (Γ). The solution qualities of AID and libsvm are almost equivalent in terms of the objective function values and training classification rates. The result for Set 1 RCV data is presented in Table 4 . AID is again faster than libsvm for all cases. The values of ρ are much smaller than the values from Table 3 . This can be explained by the smaller values of r T and T . Because AID converges faster for RCV data, it terminates early and takes much less time. Recall that larger T and r T imply that more aggregated problems with larger sizes are additionally solved. Similarly to the result in Table 3 , ρ values tend to decrease in n and m, where the trend is much clearer for RCV data. By checking ∆ and Γ, we observe that the solution of AID and libsvm are equivalent. One interesting observation is that the trend of the number of iterations (T ) is different from Table 3 . For Set 1 RCV data, T tends to increase in n and decrease in m. This is exactly opposite from the result for Set 1 IBM data. This can be explained by very small values of r T compared to the values in Table 3 . In order to visually compare the performances, in Figure 6 , we plot ρ values and computation times of AID and libsvm for IBM and RCV data. Figures 6(a ) and 6(c) assert that AID is scalable, while its relative performance keeps improving with respect to libsvm as shown in Figures 6(b ) and 6(d). For both size AID libsvm Comparison n m r 0 r T T T AID σ(T AID ) T libsvm σ(T libsvm ) Γ ∆ ρ 30,000 10 0.08% 1.0% 6.1 3 0 10 2 0.00% 0.00% 0.26 30 0.23% 6.6% 8.5 4 0 12 1 0.00% 0.00% 0.34 50 0.37% 10.4% 8.4 6 1 19 3 0.00% 0.00% 0.30 70 0.52% 14.2% 8.2 8 1 26 1 0.00% 0.00% 0.32 90 0.67% 15.1% 8 10 1 32 3 0.00% 0.00% 0.31 50,000 10 0.05% 0.5% 6 5 1 15 2 0.00% -0.01% 0.30 30 0.14% 4.0% 8.1 8 3 31 6 0.00% 0.00% 0.25 50 0.22% 7.3% 8.9 9 1 44 5 0.00% 0.00% 0.21 70 0.31% 10.0% 9 14 1 57 4 0.00% 0.00% 0.24 90 0.40% 11.0% 8.5 16 2 66 3 0.00% 0.00% 0.24 100,000 10 0.02% 0.4% 5.7 11 6 53 24 0.00% -0.03% 0.21 30 0.07% 2.7% 9.1 14 2 75 7 0.00% 0.00% 0.19 50 0.11% 4.7% 9.3 18 2 108 12 0.00% 0.00% 0.17 70 0.16% 6.0% 9 23 2 133 8 0.00% 0.00% 0.17 90 0.20% 7.3% 9 30 2 168 9 0.00% 0.00% 0.18 150,000 10 0.02% 0.1% 3.8 14 7 86 96 0.00% -0.09% 0.16 30 0.05% 1.9% 9 23 3 157 81 0.00% 0.00% 0.15 50 0.07% 3.6% 9.6 27 2 174 14 0.00% 0.00% 0.16 70 0.10% 5.0% 9.5 36 5 234 15 0.00% 0.00% 0.15 90 0.13% 5.3% 9.1 39 3 280 11 0.00% 0.00% 0.14 Table 4: Average performance of AID for SVM against libsvm (Set 1 RCV data) data sets, the computation times of AID grow slower than fn and AID saves more computation time as n and m increase.  Although we present AID for SVM with kernels in Appendix B, we only show result for linear SVM in this experiment. For SVM with linear kernel, Liblinear [12] is known to be one of the fastest algorithms. Preliminary experiments showed that Set 1 instances are too small to obtain benefits from AID, and liblinear is faster for all cases. Hence, for comparison against liblinear, we consider Set 2 (larger instances sampled from IBM data). The result is shown in Table 5 . Because liblinear is a faster solver, for some cases liblinear is faster than AID, especially when n and m are small. Among 20 cases (n-m pairs), liblinear wins 45% with 10.52 times faster than AID at maximum, and AID wins 55% with 27.62 times faster than liblinear. However, the solution time of liblinear has very large variation. This is because liblinear struggles to terminate for some instances. On the other hand, AID has relatively small variations in solution time. Therefore, even though AID is not outperforming for all cases, we conclude AID is more stable and competitive. Note that objective function value difference ∆ is large for some cases. With extremely large clusters (giving large weights in aggregated problems), we observe that liblinear does not give an accurate and stable result for aggregated problems of AID for Set 2. In Figure 7 , we plot computation times and ρ values of AID and liblinear for Set 2. Because ρ values do not scale well, we instead present log 10 ρ in Figure 7(b) . From Table 5 and Figure 7 we observe that AID outperforms for larger instances. The number of negative log 10 ρ values (implying AID is faster) tend to increase as n and m increase. Performance for S 3 VM As pointed out in [5] , it is difficult to solve MIQP model ( 14 ) optimally for large size data. In a pilot study, we observed that AID can optimally solve (14) for some data sets with hundreds of observations and several attributes. However, for the data sets in Table 1 , AID was not able to terminate within a few hours. Also, no previous work in the literature provides computational result for the data sets by solving (14) directly. Therefore, in this experiment for S 3 VM, we do not compare the execution times of AID and benchmark algorithms. Instead, we compare classification rates, which is the fraction of unlabeled observations that are correctly labeled by the algorithm. The comparison is only with algorithms for S 3 VM from [5, 15] . See [5] for comprehensive comparisons of other semi-supervised learning models. Because AID is not executed until optimality, we terminate after one and five iterations, which are denoted as AID1 and AID5, respectively. size AID liblinear Comparison n m r 0 r T T T AID σ(T AID ) T liblinear σ(T liblinear ) Γ ∆ ρ 200,000 10 0.02% 0.1% 3.1 10 4 4 9 0.00% 0.12% 2.40 30 0.03% 0.8% 5.9 20 5 205 379 -0.04% 3.56% 0.10 50 0.06% 3.0% 9.3 35 8 6 2 0.00% 0.23% 5.99 70 0.08% 3.8% 9.7 38 3 12 5 0.00% 0.00% 3.32 90 0.10% 4.3% 9.5 43 3 12 3 0.00% 0.00% 3.58 400,000 10 0.01% 0.0% 1.9 15 4 98 236 0.00% 0.79% 0.15 30 0.02% 0.3% 4.6 32 6 844 1,624 0.00% 14.04% 0.04 50 0.03% 1.1% 6.1 44 11 141 180 0.00% 14.85% 0.31 70 0.04% 2.7% 9.9 92 46 38 30 0.00% 0.01% 2.43 90 0.05% 3.0% 9.4 78 13 30 9 0.00% 0.28% 2.63 600,000 10 0.01% 0.0% 1.9 25 6 2 1 0.00% 0.16% 10.52 30 0.02% 0.3% 4.9 55 10 485 1,210 0.00% 1.51% 0.11 50 0.02% 1.2% 6.8 102 34 836 1,619 0.00% 0.11% 0.12 70 0.03% 1.9% 7.7 125 54 1,154 1,370 -0.02% 7.86% 0.11 90 0.03% 2.3% 8.6 125 30 715 1,193 -0.03% 0.44% 0.17 800,000 10 0.01% 0.0% 1.4 30 8 23 61 0.00% 0.42% 1.32 30 0.01% 0.4% 5.4 83 12 29 28 0.00% 3.01% 2.91 50 0.02% 1.1% 7 125 34 3,443 4,011 0.00% 3.16% 0.04 70 0.02% 1.6% 7.4 135 32 1,023 2,228 0.00% 0.31% 0.13 90 0.03% 1.9% 7.6 173 57 942 888 0.00% 5.93% 0.18 Table 5: Average performance of AID for SVM against liblinear (Set 2 IBM data) 0 500 1000 1500 2000 2500 3000 3500 10 30 50 70 90 10 30 50 70 90 10 30 50 70 90 10 30 50 70 90 200000 400000 600000 800000 Execution Time (sec) Data Set (n,m) AID liblinear (a) Execution times -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 10 30 50 70 90 10 30 50 70 90 10 30 50 70 90 10 30 50 70 90 200000 400000 600000 800000 log ρ Data Set (n,m) (b) ρ In the computational experiment, we consider penalty parameters (M l , M u ) ∈ (5, 1) × {10 0 , 10 -1 , 10 -2 , 10 -3 , 10 -4 } and initial aggregation rate r 0 ∈ {0.01, 0.05}. Also, we consider the following two techniques for unbalanced data. l and I - l be the set of labeled observations with labels 1 and -1, respectively. In order to give larger weights for the minority class, we multiply M l by max 1, |I - l | |I + l | for i ∈ I - l and M l by max 1, |I + l | |I - l | for i ∈ I + l . We first enumerate all possible combinations of the parameters and unbalanced data techniques and report the best classification rates of AID1 and AID5 with linear kernel. In Table 6 , the results of AID1 and AID5 are compared against the benchmark algorithms. Recall that l is the number of labeled observations. In Table 6 , we present the average classification rates of AID1, AID5, and the benchmark algorithms in [5] and [15] . In the second row, TSVM with linear and RBF kernels are from [5] and S4VM with linear and RBF kernels are from [15] . The bold faced numbers represent that the corresponding algorithm gives the best classification rates. For data sets COIL2 and SecStr, we only report the result for AID1 and AID5, as other algorithms do not provide results. From Table 6 , we conclude that the classification rates of AID1 and AID5 are similar, while the execution times of AID1 are significantly smaller. Therefore, we conclude that AID1 is more efficient and we focus on AID1 for the remaining experiments. Next, we compare AID1 with TSVM with the RBF kernel from Table 6 , as TSVM with the RBF kernel is the best among the benchmark algorithms from [5] and [15] . In  Table 7: Average performances of AID (best parameters) and TSVM-RBF Recall that we report the best result by enumerating all parameters in Tables 6 and 7 . In the second experiment, we fix parameters M l = 5, M u = 1, r 0 = 0.01 for AID1. Since unbalanced data techniques significantly affect the result, we select balance cost for data sets USPS and BCI and balance constraint for data sets Digit1, g241c, g241d and Text. The result is compared against TSVM with the RBF kernel in Table 8 . As we fix parameters, the classification rates of AID1 are worse than the rates in Table 7 . However, AID1 is still competitive as AID1 and TSVM have the same number of wins. l=10 l=100 AID1 TSVM AID1 TSVM data Linear RBF Linear RBF Digit1 83.4% 82.2% 90.4% 93.9% USPS 80.4% 74.8% 87.1% 90.2% BCI 51.2% 50.9% 69.1% 66.8% g241c 74.3% 75.3% 76.0% 81.5% g241d 49.9% 49.2% 53.6% 77.6% Text 63.0% 68.8% 75.8% 75.5% # wins 4 2 2 4 Table 8 : Average performances of AID (fixed parameters) and TSVM-RBF 5 Guidelines for Applying AID to Other Problems AID is designed to solve problems with a large number of entries that can be clustered well. From the experiments in Section 4, we observe that AID is beneficial when data size is large. AID especially outperforms alternatives when the time complexity of the alternative algorithm is high. Recall that AID is applicable for problems following the form of (1). In this section, we discuss how AID can be applied for other optimization problems. We also discuss the behavior of AID as observed from additional computational experiments for LAD and SVM. Designing AID Aggregated data The main principle in generating aggregated data is to create each aggregated entry to represent the original entries in the corresponding cluster. However, the most important factor to consider when defining aggregated data is the interaction with the aggregated problem and optimality condition. The definition of aggregated entries plays a key role in deriving optimality and other important properties. In the proof of Proposition 1, x is converted to x t in the second line of the equations. In the proof of Proposition 2, x t-1 is converted to x in the third line of the equations, which is subsequently converted into x in the sixth line. Although any aggregated data definition representing the original data is acceptable, we found that the centroids work well for all of the three problems studied in this paper. Aggregated problem The aggregated problem is usually the weighted version of the original problem, where weights are obtained as a function of cardinalities of the clusters. In this paper, we directly use the cardinalities as weights. We emphasize that weights are used to give priority to larger clusters (or associated aggregated entries), and defining the aggregated problem without weights is not recommended. Recall that defining the aggregated problem is closely related to the aggregated data definition and optimality condition. When the optimality condition is satisfied, the aggregated problem should give an optimal solution to the original problem. This can be proved by showing equivalent objective function values of aggregated and original problems at optimum. Hence, matching objective function values of the two problems should also be considered when designing the aggregated problem. Optimality condition The optimality condition is the first step to develop when designing AID, because the optimality condition affects the aggregated data definition and problem. However, developing the optimality condition is not trivial. Properties at optimum for the given problem should be carefully considered. For example, the optimality condition for LAD is based on the fact that the residuals have the same sign if the observations are on the same side of the hyperplane. This allows us to separate the terms in the absolute value function when proving optimality. For SVM, we additionally use label information, because the errors also depend on the label. For S 3 VM, we use even more information: the classification decision of unlabeled entries. Designing an optimality condition and proving optimality become non-trivial when constraints and variables are more complex. The proofs become more complex in the order of LAD, SVM, and S 3 VM. Defining Initial Clusters Initial clustering algorithm From pilot computational experiments with various settings, we observed that the initial clustering accuracy is not the most important factor contributing to the performance of AID. This can be explained by the declustering procedure in early iterations. In the early iterations of AID, the number of clusters rapidly increases as most clusters violate the optimality condition. These new clusters are better than the k-means algorithm output using the same number of clusters in the sense that the declustered clusters are more likely to satisfy the optimality condition. Because the first few aggregated problems are usually small and can be solved quickly, the main concern in selecting an initial clustering algorithm is the computational time. Therefore, we recommend to use a very fast clustering algorithm to cluster the original entries approximately. For LAD and SVM, we use one iteration of k-means with two and one dimensional data, respectively. If one iteration of k-means is not precise enough then BIRCH [30] , which has complexity of O(n), may be considered. Initial aggregation rate Avoiding a trivial aggregated problem is very important when deciding the initial aggregation rate. Depending on the optimization problem, this can restrict the minimum number of clusters or the number of aggregated entries. For example, we must have at least m aggregate observations to have a nonzero-SSE model for LAD. Similar restrictions exist for SVM and S 3 VM. We recommend to pick the smallest aggregation rate among all aggregation rates preventing trivial aggregated problems mentioned above because we can obtain better clusters (more likely to satisfy optimality condition) by solving smaller aggregated problems and by declustering. With the one iteration k-means setting, the number of clusters also affects the initial clustering time, as the time complexity is O(|K 0 |mn), where |K 0 | is the number of clusters at the beginning. In Figure 8 , we plot the solution time of AID for SVM for the IBM and RCV data sets with n = 150, 000 and m = 10. In each plot, the horizontal axis represents the initial aggregation rate r 0 , and the left and right vertical axes are for the execution time and number of iterations, respectively. The stacked bars show the total time of AID, where each bar is split into the initialization time (clustering) and loop time (declustering and aggregated problem solving). The series of black circles represent the number of iterations of AID. As r 0 increases, we have a larger number of initial clusters. Hence, with the current initial clustering setting (one iteration of k-means), the initialization time (white bars) increases as r 0 increases. Although the number of iterations decreases in r 0 , the loop time is larger when r 0 is large because the size of the aggregated problems is larger. 0 1 2 3 4 5 6 0 50 100 150 200 250 300 0.0002 0.0004 0.0008 0.0016 0.0032 0.0064 0.0128 0.0256 0.0512 Number of iterations Time (seconds) Initial aggregatation rate (r0) Initialization time loop time Num iterations (a) IBM 0 1 2 3 4 5 6 0 50 100 150 200 250 300 0.0002 0.0004 0.0008 0.0016 0.0032 0.0064 0.0128 0.0256 0.0512 Number of iterations Time (seconds) Initial aggregatation rate (r0) Initialization time loop time Num iterations (b) RCV Aggregation Rates and Relative Location of Clusters Evgeniou and Pontil [11] mention that clusters that are far from the SVM hyperplane tend to have large size, while the size of clusters that are near the SVM hyperplane is small. This also holds for AID for LAD, SVM, and S 3 VM. We demonstrate this property with LAD. In order to check the relationship between the aggregation rate and the residual, we check 1. aggregation rates of entire clusters, 2. aggregation rates of clusters that are near the hyperplane (with residual less than median), and 3. aggregation rates of clusters that are far from the hyperplane (with residual greater than median). In Figure 9 , we plot the aggregation rate of entire clusters (series with + markers), with residual less than the median (series with black circles), and with residuals greater than the median (series with empty circles). We plot the result for 9 instances in a 3 by 3 grid (3 values of n and 3 values of m), where each subplot's horizontal and vertical axes are for iteration (t) and aggregation rates (r t ). We can observe that the aggregation rate of clusters that are near the hyperplane increases rapidly, while far clusters' aggregation rates stabilize after a few iterations. The aggregation rates of near clusters increase as m increases. Conclusion We propose a clustering-based iterative algorithm and apply it to common machine learning problems such as LAD, SVM, and S 3 VM. We show that the proposed algorithm AID monotonically converges to the global optimum (for LAD and SVM) and outperforms the current state-of-the-art algorithms when data size is large. The algorithm is most beneficial when the time complexity of the optimization problem is high, so that solving smaller problems many times is affordable. m=100 m=500 m=800 0% 10% 20% 30% 40% 50% 60% 1 3 5 7 9 0% 10% 20% 30% 40% 50% 60% 1 3 5 7 9 0% 10% 20% 30% 40% 50% 60% 1 3 5 7 9 n=800000 0% 10% 20% 30% 40% 50% 60% 1 3 5 7 9 0% 10% 20% 30% 40% 50% 60% 1 3 5 7 9 0% 10% 20% 30% 40% 50% 60% 1 3 5 7 9 n=400000 0% 10% 20% 30% 40% 50% 60% 1 3 5 7 9 0% 10% 20% 30% 40% 50% 60% 1 3 5 7 0% 10% 20% 30% 40% 50% 60% 1 3 5 7 n=200000 Clusters with large residuals Clusters with small residuals All clusters 7 Acknowledgment We appreciate the referees for their helpful comments that strengthen the paper. all clusters violate the optimality conditions and are declustered. In the later iterations, it takes longer time to solve the aggregated problem in each iteration and the optimality gap is rapidly decreasing in t, because the clusters are finer but the number of clusters is larger. For both algorithms, we observe that the training classification rates become stable after several iterations although the optimality gaps are still large. AID with libsvm AID with liblinear t r t Ft E best Opt Iter Cum Train r t Ft E best Opt Iter Cum Train gap time time rate gap time time rate 0 0.06% 19 139,971 1000%+ 14 14 66.3% 0.06% 19 140,349 1000%+ 13 13 66.3% 1 0.11% 248 139,971 1000%+ 22 35 64.1% 0.11% 256 140,349 1000%+ 30 43 64.0% 2 0.22% 687 89,104 1000%+ 23 58 75.4% 0.22% 687 93,743 1000%+ 30 74 74.3% 3 0.40% 941 27,713 1000%+ 27 85 91.8% 0.40% 934 44,699 1000%+ 38 112 85.6% 4 0.68% 1,079 17,366 1000%+ 38 123 1,043 20,870 1000%+ 52 163 95.1% 5 1.12% 1,135 8,038 608% 60 183 99.4% 1.14% 1,125 9,389 734% 53 216 99.4% 6 1.88% 1,182 1,616 37% 114 296 99.6% 1.94% 1,179 2,138 81% 49 265 99.6% 7 2.73% 1,203 1,227 2.00% 234 531 99.6% 2.99% 1,205 1,218 1.04% 71 336 99.5% 8 2.93% 1,208 1,208 0.04% 697 1,227 99.5% 3.29% 1,209 1,214 0.42% 362 698 99.5% 9 4.58% 1,210 1,210 0.03% 518 1,216 99.5% Table 10: Number of iterations of AID for the original IBM data (n = 1.3 million and m = 342) Figure 1 : 1 Figure 1: Illustration of AID (a) Clusters C t and βt (b) Declustered (c) New clusters C t+1 Figure 2 : 2 Figure 2: Illustration of AID for LAD (a) Clusters C t and ( wt , bt ) (b) Declustered (c) New clusters C t+1 Figure 3 : 3 Figure 3: Illustration of AID for SVM the class (label) data, and w ∈ R m , b ∈ R, and d ∈ {-1, 1} |Iu| are the decision variables. By introducing error term ξ ∈ R n + , ( minimizes error is determined by the sign of wx i + b by Lemma 1. This means that I ++ (w,b,d) , I +- (w,b,d) , I -+ (w,b,d) , and I -- (w,b,d) can be defined without d. } for i ∈ I u Clusters D t and ( wt , bt ) Figure 4 : 4 Figure 4: Illustration of AID for S 3 VM where w ∈ R m , b ∈ R, and d ∈ {-1, 1} |K g u | , and C g and D g are the cluster decision sets. Note that x k and y k are now decision variables that depend on C g k and D g k . Note that due to characteristic of the subsets I ++ (w,b,d) , I +- (w,b,d) , I -+ (w,b,d) , and I -- (w,b,d) , we can replace d i by d k in the definition of the subsets. Set 1 : 1 n ∈ {3, 5, 10, 15} × 10 4 and m ∈ {10, 30, 50, 70, 90} from IBM and RCV Set 2: n ∈ {2, 4, 6, 8} × 10 5 and m ∈ {10, 30, 50, 70, 90} from IBM Figure 5 : 5 Figure 5: Execution times of AID and fn for LAD ρ (Set 1 RCV data) Figure 6 : 6 Figure 6: Plots for performance of AID for SVM against libsvm Figure 7 : 7 Figure 7: Performance of AID for SVM against liblinear (Set 2 IBM data) 1 . 1 balance constraint: i∈Iu (wxi+bi) |Iu| = i∈I l yi |I l | 2. balance cost: Let I + Figure 8 : 8 Figure 8: Impact of r 0 for Set 1 IBM and RCV data Figure 9 : 9 Figure 9: Aggregation rate r t over iterations for LAD instances with various n and m Table 1 : 1 S 3 VM Data from Chapelle [5] Data n (entries) m (attributes) l (labeled) Digit1 1,500 241 10 and 100 USPS 1,500 241 10 and 100 BCI 400 117 10 and 100 g241c 1,500 241 10 and 100 g241d 1,500 241 10 and 100 Text 1,500 11,960 10 and 100 COIL2 1,500 241 10 and 100 SecStr 83,679 315 1,000 and 10,000 Table 3 : 3 Average performance of AID for SVM against libsvm (Set 1 IBM data) size AID libsvm Comparison n m r 0 r T T T AID σ(T AID ) T libsvm σ(T libsvm ) Γ ∆ ρ 30,000 10 0.08% 0.2% 4.1 2 0 25 1 0.00% 0.00% 0.064 30 0.23% 0.7% 3.7 2 0 41 1 0.00% 0.00% 0.038 50 0.37% 1.1% 3.3 2 0 71 1 0.00% 0.00% 0.022 70 0.52% 1.6% 3.6 2 0 113 2 -0.01% 0.00% 0.016 90 0.67% 2.0% 3.6 2 0 146 2 0.03% 0.00% 0.013 50,000 10 0.05% 0.2% 4.1 3 0 67 2 -0.05% 0.00% 0.039 30 0.14% 0.5% 3.8 3 0 147 4 0.01% 0.00% 0.018 50 0.22% 0.9% 4 3 0 254 5 -0.01% 0.00% 0.012 70 0.31% 1.2% 3.9 3 0 349 6 0.01% 0.00% 0.009 90 0.40% 1.6% 3.9 3 0 422 6 -0.01% 0.00% 0.008 100,000 10 0.02% 0.1% 4.6 6 1 367 29 0.01% 0.00% 0.016 30 0.07% 0.4% 4.9 7 0 856 82 0.00% 0.00% 0.008 50 0.11% 0.7% 4.9 7 0 1,312 167 0.00% 0.00% 0.005 70 0.16% 1.0% 5 8 2 1,524 163 0.00% 0.00% 0.005 90 0.20% 1.3% 5 12 6 1,918 285 0.00% 0.00% 0.006 150,000 10 0.02% 0.1% 5.7 11 1 1,120 70 0.02% 0.00% 0.010 30 0.05% 0.4% 5.1 11 1 2,503 424 0.00% 0.00% 0.004 50 0.07% 0.6% 5.1 11 1 3,469 655 0.00% 0.00% 0.003 70 0.10% 0.8% 5.1 12 1 3,963 820 0.02% 0.00% 0.003 90 0.13% 1.1% 5.2 13 1 4,402 663 0.00% 0.00% 0.003 Table 7 , 7 we observe that AID1 performs l=10 l=100 AID5 AID1 TSVM TSVM S4VM S4VM AID5 AID1 TSVM TSVM S4VM S4VM data linear linear linear RBF linear RBF linear linear linear RBF linear RBF Digit1 86.6% 86.6% 79.4% 82.2% 76.0% 63.6% 92.6% 92.0% 82.0% 93.9% 91.5% 94.9% USPS 80.1% 80.4% 69.3% 74.8% 78.7% 80.1% 86.5% 87.1% 78.9% 90.2% 87.7% 91.0% BCI 52.9% 52.2% 50.0% 50.9% 51.8% 51.3% 71.7% 69.1% 57.3% 66.8% 70.5% 66.1% g241c 79.7% 79.7% 79.1% 75.3% 54.6% 52.8% 82.6% 82.6% 81.8% 81.5% 75.3% 74.8% g241d 59.5% 49.9% 53.7% 49.2% 56.3% 52.7% 72.6% 59.3% 76.2% 77.6% 72.2% 60.9% Text 66.4% 66.4% 71.40% 68.79% 52.1% 52.6% 75.77% 75.8% 77.69% 75.48% 69.9% 54.1% COIL2 90.9% 90.2% NA NA NA NA 87.0% 88.5% NA NA NA NA SecStr 64.3% 62.4% NA NA NA NA 69.70% 65.8% NA NA NA NA Table 6 : 6 Average classification rates of AID (best parameters) and the benchmark algorithms better than TSVM when l = 10, whereas the two algorithms tie when l = 100. Note that COIL2 and SecStr are excluded from the comparison as results from the benchmark algorithms are not available. l=10 l=100 AID1 TSVM AID1 TSVM data Linear RBF Linear RBF Digit1 86.6% 82.2% 92.0% 93.9% USPS 80.4% 74.8% 87.1% 90.2% BCI 52.2% 50.9% 69.1% 66.8% g241c 79.7% 75.3% 82.6% 81.5% g241d 49.9% 49.2% 59.3% 77.6% Text 66.4% 68.8% 75.8% 75.5% # wins 5 1 3 3"
}