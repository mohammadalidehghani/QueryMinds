{
  "title": "Accepted as a paper at ICLR 2023 Workshop on Machine Learning for Remote Sensing EVALUATION CHALLENGES FOR GEOSPATIAL ML",
  "abstract": "As geospatial machine learning models and maps derived from their predictions are increasingly used for downstream analyses in science and policy, it is imperative to evaluate their accuracy and applicability. Geospatial machine learning has key distinctions from other learning paradigms, and as such, the correct way to measure performance of spatial machine learning outputs has been a topic of debate. In this paper, I delineate unique challenges of model evaluation for geospatial machine learning with global or remotely sensed datasets, culminating in concrete takeaways to improve evaluations of geospatial model performance. MOTIVATION Geospatial machine learning (ML), for example with remotely sensed data, is being used across consequential domains, including public health (Nilsen et al., 2021; Draidi Areed et al., 2022) conservation (Sofaer et al., 2019) , food security (Nakalembe, 2018), and wealth estimation (Jean et al., 2016; Chi et al., 2022) . By both their use and their very nature, geospatial predictions have a purpose beyond model benchmarking; mapped data are to be read, scrutinized, and acted upon. Thus, it is critical to rigorously and comprehensively evaluate how well a predicted map represents the state of the world it is meant to reflect, or how well a spatial ML model performs across the many conditions in which it might be used.",
  "introduction": "Unique structures in remotely sensed and geospatial data complicate or even invalidate use of traditional ML evaluation procedures. Partially as a result of misunderstandings of these complications, the stated performance of several geospatial models and predictive maps has come into question (Fourcade et al., 2018; Ploton et al., 2020) . This in turn has sparked disagreement on what the \"right\" evaluation procedure is. With respect to a certain set of spatial evaluation methods (described in §4.1), one is jointly presented with the arguments that \"spatial cross-validation is essential in preventing overoptimistic model performance\" (Meyer et al., 2019) and \"spatial cross-validation methods have no theoretical underpinning and should not be used for assessing map accuracy\" (Wadoux et al., 2021) . That both statements can simultaneously hold reflects the importance of using a diverse set of evaluation methods tailored to the many ways in which a geospatial ML model might be used. In this paper, I situate the challenges of geopsatial model evaluation in the perspective of an ML researcher, synthesizing prior work across ecology, geology, statistics, and machine learning. I aim in part to disentangle key factors that complicate effective evaluation of model and map performance. First and foremost, evaluation procedures should be designed to measure as closely as possible the quantity or phenomena they are intended to assess ( §2). After the relevant performance measures are established, considerations can be made about what is feasible with the available data ( §3). With all of this in mind, possible evaluation procedures ( §4) can be compared and tailored to the task at hand. Recognizing the interaction of these distinct but related steps exposes opportunities to improve geospatial performance assessment, both in individual studies and more broadly ( §5).",
  "body": "Unique structures in remotely sensed and geospatial data complicate or even invalidate use of traditional ML evaluation procedures. Partially as a result of misunderstandings of these complications, the stated performance of several geospatial models and predictive maps has come into question (Fourcade et al., 2018; Ploton et al., 2020) . This in turn has sparked disagreement on what the \"right\" evaluation procedure is. With respect to a certain set of spatial evaluation methods (described in §4.1), one is jointly presented with the arguments that \"spatial cross-validation is essential in preventing overoptimistic model performance\" (Meyer et al., 2019) and \"spatial cross-validation methods have no theoretical underpinning and should not be used for assessing map accuracy\" (Wadoux et al., 2021) . That both statements can simultaneously hold reflects the importance of using a diverse set of evaluation methods tailored to the many ways in which a geospatial ML model might be used. In this paper, I situate the challenges of geopsatial model evaluation in the perspective of an ML researcher, synthesizing prior work across ecology, geology, statistics, and machine learning. I aim in part to disentangle key factors that complicate effective evaluation of model and map performance. First and foremost, evaluation procedures should be designed to measure as closely as possible the quantity or phenomena they are intended to assess ( §2). After the relevant performance measures are established, considerations can be made about what is feasible with the available data ( §3). With all of this in mind, possible evaluation procedures ( §4) can be compared and tailored to the task at hand. Recognizing the interaction of these distinct but related steps exposes opportunities to improve geospatial performance assessment, both in individual studies and more broadly ( §5). MAP ACCURACY AND MODEL PERFORMANCE: CONTRASTING VIEWS Estimating accuracy indices and corresponding uncertainties of geospatial predictions is essential to reporting geospatial ML performance ( §2.1), especially when prediction maps will be used for downstream analyses or policy decisions. At the same time, the potential value of a geospatial ML model likely extends beyond that of a single mapped output ( §2.2). Delineating the (possibly many) facets of desired model and map use is key to measuring geospatial ML performance ( §2.3). MAP ACCURACY AS A POPULATION PARAMETER TO BE ESTIMATED Establishing notation we will use throughout, let ŷ(ℓ) denote a model's predicted value at location ℓ, and y(ℓ) the reference, or \"ground truth\" value (which we assume can be measured). To calculate a map accuracy index as a population parameter for accuracy index F is to calculate A(D) = F ({(ŷ(ℓ), y(ℓ))} ℓ∈D ) where D is the target population of map use (e.g. all (lat, lon) pairs in a global grid, or all administrative units in a set of countries). Examples of common F include root mean squared error, and area under the ROC curve, among many others (Maxwell et al., 2021) . Typically, one only has a limited set of values y for locations in an evaluation set ℓ ∈ S eval from which to compute a statistic Â(S eval ) to estimate A(D). Wadoux et al. ( 2021 ) discuss the value of using a design-independent probability sample for design-based estimation of A (in contrast, model-based estimation makes statistical assumptions about the data (Brus, 2021)). Here a design-independent sample is one collected independently of the model training process. A probability sample is one for which every location in D has a positive probability of appearing in S eval , and these probabilities are known for all ℓ ∈ S eval (see, e.g. Lohr (2021) ). Wadoux et al. (2021) emphasize that when S eval is a design independent probability sample from population D, design-based inference can be used to estimate A(D) with Â(S eval ), regardless of the prediction model or distribution of training data. Computing statistically valid estimates of map accuracy indices is clearly a key component of reporting overall geospatial ML model performance. It is often important to understand how accuracy and uncertainty in predictions vary across sub-populations D r1 , D r2 . . . ⊂ D (such as administrative regions or climate zones (Meyer & Pebesma, 2022) ). If local accuracy indexes A(D r1 ), A(D r2 ) . . . are low in certain sub-regions, this could expose concerns about fairness or model applicability. MODEL PERFORMANCE EXTENDS BEYOND MAP ACCURACY Increasingly, geospatial ML models are designed with the goal of being used outside of the regions where training labels are available. Models trained with globally available remotely sensed data might be used to \"fill in\" spatial gaps common to other data modalities ( §3.2). The goals of spatial generalization, spatial extrapolation or spatial domain adaption can take different forms: e.g. applying a model trained with data from one region to a wholly new region, or using data from a few clusters or subregions to extend predictions across the entire region. When spatial generalizability is desired, performance should be assessed specifically with respect to this goal ( §4). While spatial generalization is a key component of performance for many geospatial models, it too is just one facet of geospatial model performance. Proposed uses of geospatial ML models and their outputs include estimation of natural or causal parameters (Proctor et al., 2023) , and reducing autocorrelation of prediction residuals in-sample (Song & Kim, 2022) . Other important facets of geospatial ML performance are model interpretability (Brenning, 2022) and usability, including the resources required to train, deploy and maintain models (Rolf et al., 2021) . CONTRASTING PERSPECTIVES ON PERFORMANCE ASSESSMENT The differences between estimating map accuracy as a population parameter ( §2.1) and assessing a model's performance in the conditions it is most likely to be used ( §2.2) are central to one of the discrepancies introduced in §1. Meyer et al. (2019) ; Ploton et al. (2020) ; Meyer & Pebesma (2022) state concerns in light of numerous ecological studies applying non-spatial validation techniques with the explicit purpose of spatial generalization. They rightly caution that when data exhibit spatial correlation ( §3.1), non-spatial validation methods will almost certainly over-estimate predictive performance in these use cases. Wadoux et al. (2021) , in turn, argue that performance metrics from spatial validation methods will not necessarily tell you anything about A as a population parameter. A second discrepancy between these two perspectives hinges on what data is assumed to be available (or collectable). While there are some major instances of probability samples being collected for evaluation of global-scale maps (Boschetti et al., 2016; Stehman et al., 2021) , this is far from standard standard in geospatial ML studies (Maxwell et al., 2021) . More often, datasets are created \"by merging all data available from different sources\" (Meyer & Pebesma, 2022) . Whatever the intended use of a geospatial model, the availability of and structures within geopsatial and remotely sensed data must be contended with in order to reliably evaluate any sort of performance. STRUCTURES AND PATTERNS IN SPATIAL AND REMOTELY SENSED DATA Geospatial and remotely sensed data exhibit distinct structures. For example, the chosen extent and scale of a spatial prediction unit (ℓ in §2) has important implications for the design, use, and evaluation of geospatial ML models, evidenced by the phenomena of \"modifiable areal unit problem\" and \"ecological fallacy\" (Haining, 2009; Nikparvar & Thill, 2021; Yuan & McKee, 2022) . Here, I focus on two key factors of geospatial data that affect the validity of geospatial ML evaluation methods. SPATIAL STRUCTURES: (AUTO)CORRELATION AND COVARIATE SHIFT One key phenomena exhibited by many geopsatial data is that values of a variable (e.g. tree canopy height) are often correlated across locations. Formally, for random process Z, the spatial autocorrelation function is defined as R ZZ (ℓ i , ℓ j ) = E[Z(ℓ i )Z(ℓ j )]/σ i σ j , where σ i , σ j are the standard deviations associated with Z(ℓ i ), Z(ℓ j ). For geospatial variables, we might expect R ZZ (ℓ i , ℓ j ) > 0 when ℓ i and ℓ j are closer together, namely that values of Z at nearby points tend to be closer in value. The degree of spatial autocorrelation in data can be assessed with statistics such as Moran's I and Geary's C, and semi-variogram analsyes (see, e.g. (Gaetan & Guyon, 2010) ). Spatial autocorrelations and correlations between predictor and label variables can be an important source of structure to leverage in geospatial ML models (Rolf et al., 2020; Klemmer & Neill, 2021 ), yet they also present challenges. Models can \"over-rely\" on spatial correlations in the data, leading to over-estimated accuracy despite poor spatial generalization performance. Overfitting to spatial relationships in training data is of particular concern in the when data distributions differ between training regions and regions of use. Such covariate shifts are common in geospatial data, e.g. across climate zones, or spectral shifts in satellite imagery (Tuia et al., 2016; Hoffimann et al., 2021) . Presence of spatial correlations or domain shift alone do not invalidate assessing map accuracy with a probability sample ( §2.1). However, when evaluation is limited to existing data, issues of data availability and representivity can amplify the challenges of geospatial model evaluation. AVAILABILITY, QUALITY, AND REPRESENTIVITY OF GEOSPATIAL EVALUATION DATA Many geospatial datasets exhibit gaps in coverage or quality of data. Meyer & Pebesma (2022) evidence trends of geographic clustering around research sites primarily in a small number of countries, across three datasets used for global mapping in ecology. Oliver et al. (2021) find geographical bias in coverage of species distribution data aggregated from field observation, sensors measurements, and citizen science efforts. Burke et al. (2021) note that the frequency at which nationally representative data on agriculture, population, and economic factors are collected varies widely across the globe. While earth observation data such as satellite imagery have comparatively much higher coverage across time and space (Burke et al., 2021) , coverage of commercial products has been shown to be biased toward regions of high commercial value (Dowman & Reuter, 2017) . Filling in data gaps is goal for which geospatial ML can be transformative ( §2.2), yet these same gaps complicate model evaluation. When training data are clustered in small regions, this can affect our ability to train a high-performing model. When evaluation data are clustered in small regions, this affects our ability to evaluate geospatial ML model performance at all. SPATIALLY-AWARE EVALUATION METHODS: A BRIEF OVERVIEW In §3, we established that geospatial data generally exhibit spatial correlations and data gaps, even when target use areas D are small. It is well documented that calculating accuracy indices with non-spatial validation methods (e.g. standard k-fold cross-validation) will generally over-estimate performance in such settings. Spatially-aware evaluation methods can control the spatial distribution of training and validation set points to better simulate conditions of intended model use. SPATIAL CROSS-VALIDATION METHODS Several spatial cross-validation methods have been proposed that reduce spatial dependencies between train set points ℓ ∈ S train from evaluation set points ℓ ∈ S eval . Spatial cross-validation methods typically stratify training and evaluation instances by larger geographies (Roberts et al., 2017; Valavi et al., 2018) e.g. existing boundaries, spatial blocks, or automatically generated clusters. Buffered cross-validation methods (such as spatial leave-one-out (Le Rest et al., 2014) , leave-pair out (Airola et al., 2019) and k-fold cross validation (Pohjankukka et al., 2017) ) control the minimum distance from any training point to any evaluation point. In addition to evaluating model performance, spatial cross-validation has also been suggested as a way to to improve model selection and parameter estimation in geospatial ML (Meyer et al., 2019; Schratz et al., 2019; Roberts et al., 2017) . While separating ℓ ∈ S train from ℓ ∈ S eval can reduce the amount of correlation between training and evaluation data, a spatial split also induces a higher degree of spatial extrapolation to the learning setup and potentially reduces variation in the evaluation set labels. As a result, it is possible for spatial validation methods to systematically under-report performance, especially in interpolation regimes (Roberts et al., 2017; Wadoux et al., 2021) . In a different flavor from the evaluation methods above, Milà et al. (2022) propose to match the distribution of nearest neighbor distances between train and evaluation set points to the corresponding distances between train set and target use area. OTHER SPATIALLY-AWARE VALIDATION EVALUATION METHODS When the intended use of a geospatial model is to generate predictions outside the training distribution, it is critical to test the model's ability to generalize across different conditions. For example, studies have varied the amount of spatial extrapolation required by changing parameters of the spatial validation setups in §4.1 , e.g. with buffered leave one out (Ploton et al., 2020; Brenning, 2022) and checkerboard designs (Roberts et al., 2017; Rolf et al., 2021) . Jean et al. (2016) assess extrapolation ability across pairs of countries by iteratitvely training in one region and evaluating performance in another. Rolf et al. (2021) find that the distances at which a geospatial model has extrapolation power can differ substantially depending on the properties of the prediction variable. It is always critical to put the reported performance of geospatial ML models in context. Visualizing the spatial distributions of predictions and error residuals can help expose overreliance on spatially correlated predictors (Meyer et al., 2019) and sub-regions with low local model performance. Comparing performance to that of a baseline model built entirely on spatial predictors can contextualize the value-add of a new geospatial model (Fourcade et al., 2018; Rolf et al., 2021) . TAKING STOCK: CONSIDERATIONS AND OPPORTUNITIES Comprehensive reporting of performance is critical for geospatial ML methods, especially as stated gains in research progress make their way to maps and decisions of real-world consequence. Evaluating performance of geospatial models is especially challenging in the face of spatial correlations and limited availability or representivity of data. This means non-spatial data splits are generally unsuitable for geospatial model evaluation with most existing datasets. Spatially-aware validation methods are an important indicator of model performance including spatial generalization; however, they generally do not provide valid statistical estimates of prediction map accuracy. This brings us to end with three key opportunities for improving the landscape of geospatial ML evaluation: Opportunity 1: Invest in evaluation data to measure map accuracy and overall performance of geospatial models. When remote annotations are appropriate, labeling tools (e.g. Robinson et al. (2022) ) can facilitate the creation of probability-sampled evaluation datasets. Data collection and aggregation efforts can focus on filling existing geospatial data gaps (Paliyam et al., 2021) or simulating real-world prediction conditions like covariate or domain shift (Koh et al., 2021) . Opportunity 2: Invest in evaluation frameworks to precisely and transparently and report performance and valid uses of a geospatial ML model (à la \"model cards\" (Mitchell et al., 2019) ). This includes improving spatial baselines, expanding methods for reporting uncertainty over space, and delineating \"areas of applicability\" for geospatial models, e.g. as in Meyer & Pebesma (2022) . Opportunity 3: If the available data and evaluation frameworks are insufficient, explain the limitations of what types of performance can be evaluated. Distinguish between performance measures that estimate a statistical parameter and those that indicate potential skill for a possible use case."
}