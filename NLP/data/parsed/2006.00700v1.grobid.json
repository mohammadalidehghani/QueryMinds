{
  "title": "When Machine Learning Meets Multiscale Modeling in Chemical Reactions",
  "abstract": "Due to the intrinsic complexity and nonlinearity of chemical reactions, direct applications of traditional machine learning algorithms may face with many difficulties. In this study, through two concrete examples with biological background, we illustrate how the key ideas of multiscale modeling can help to reduce the computational cost of machine learning a lot, as well as how machine learning algorithms perform model reduction automatically in a time-scale separated system. Our study highlights the necessity and effectiveness of an integration of machine learning algorithms and multiscale modeling during the study of chemical reactions.",
  "introduction": "of machine learning to chemical reactions, including the supervised and unsupervised learning, see e.g. Refs. [10] [11] [12] [13] [14] . The successful attempts of machine-learning-based modeling pave a new way to understand the complicated dynamics of chemical reactions. However, most chemical reactions involve plenty of reactants, multiple potential reaction routines, diverse reaction rates and so on. Without considering the this intrinsic multi-component and multiscale nature of the system, direct applications of machine learning algorithms may face inevitable difficulties (see examples below for details). Motivated by the requirements on a real complex system, especially a simultaneous maintenance of the efficiency of macroscopic models and the accuracy of microscopic models, the view of multiscale modeling is introduced. It focuses on a proper separation of the system or phenomenon into several scales with minimum overlap, a correct characterization of the relation between different levels of physical models, as well as a systematical procedure of coarse-graining 15 . Multiscale modeling offers a unified way to examine the system of chemical reactions, by looking into the reactions occurring at different time scales and the relations between them. Therefore, it is expected that a proper integration of machine learning algorithms with ideas and methodology of multiscale modeling and analysis will shed some light into this field. And this leads to the major motivation of our current study. To be concrete, we will justify our arguments from two aspects: (1) By using the explicit correspondence between mesoscopic chemical master equations and macroscopic mass-action equations in Kurtz's limit, the challenging task of learning detailed probability distribution function (PDF) is converted into learning low-order moments. Obviously, the latter is much easier. In this case, the computational cost of direct machine learning is greatly reduced by incorporating the multiscale modeling. (2) When fast and slow reactions appear simultaneously in the same system, meaning there is a time-scale separation among the chemical reactions, the ODENet -a kind of machine learning algorithms with sparse identification show an astonishing ability of deriving simplified models under Quasi Steady State Approximation (QSSA) automatically. Therefore, machine learning could help to model multiscale chemical reactions too. These two examples clearly demonstrate that machine learning and multiscale modeling are closely related to each other. A proper integration of two approaches will greatly facilitate our study of chemical reactions. The whole paper is organized as follows. A basic architecture of the ODENet, a special kind of machine learning algorithms which is designed to derive the explicit form of ODEs from the pregiven time series data, is introduced in Section II. Along with the basic ideas and techniques for multiscale modeling and analysis for chemical reactions, including the Kurtz's limit from chemical master equations to mass-action equations, and the quasi steady-state approximation. In Section III, we illustrate our key ideas through two examples -the development and differentiation of cells, as well as the self-regulatory gene transcription and translation. The usefulness of an integration of machine learning and multiscale modeling could be clear learned. The last section contains some discussions.",
  "body": "of machine learning to chemical reactions, including the supervised and unsupervised learning, see e.g. Refs. [10] [11] [12] [13] [14] . The successful attempts of machine-learning-based modeling pave a new way to understand the complicated dynamics of chemical reactions. However, most chemical reactions involve plenty of reactants, multiple potential reaction routines, diverse reaction rates and so on. Without considering the this intrinsic multi-component and multiscale nature of the system, direct applications of machine learning algorithms may face inevitable difficulties (see examples below for details). Motivated by the requirements on a real complex system, especially a simultaneous maintenance of the efficiency of macroscopic models and the accuracy of microscopic models, the view of multiscale modeling is introduced. It focuses on a proper separation of the system or phenomenon into several scales with minimum overlap, a correct characterization of the relation between different levels of physical models, as well as a systematical procedure of coarse-graining 15 . Multiscale modeling offers a unified way to examine the system of chemical reactions, by looking into the reactions occurring at different time scales and the relations between them. Therefore, it is expected that a proper integration of machine learning algorithms with ideas and methodology of multiscale modeling and analysis will shed some light into this field. And this leads to the major motivation of our current study. To be concrete, we will justify our arguments from two aspects: (1) By using the explicit correspondence between mesoscopic chemical master equations and macroscopic mass-action equations in Kurtz's limit, the challenging task of learning detailed probability distribution function (PDF) is converted into learning low-order moments. Obviously, the latter is much easier. In this case, the computational cost of direct machine learning is greatly reduced by incorporating the multiscale modeling. (2) When fast and slow reactions appear simultaneously in the same system, meaning there is a time-scale separation among the chemical reactions, the ODENet -a kind of machine learning algorithms with sparse identification show an astonishing ability of deriving simplified models under Quasi Steady State Approximation (QSSA) automatically. Therefore, machine learning could help to model multiscale chemical reactions too. These two examples clearly demonstrate that machine learning and multiscale modeling are closely related to each other. A proper integration of two approaches will greatly facilitate our study of chemical reactions. The whole paper is organized as follows. A basic architecture of the ODENet, a special kind of machine learning algorithms which is designed to derive the explicit form of ODEs from the pregiven time series data, is introduced in Section II. Along with the basic ideas and techniques for multiscale modeling and analysis for chemical reactions, including the Kurtz's limit from chemical master equations to mass-action equations, and the quasi steady-state approximation. In Section III, we illustrate our key ideas through two examples -the development and differentiation of cells, as well as the self-regulatory gene transcription and translation. The usefulness of an integration of machine learning and multiscale modeling could be clear learned. The last section contains some discussions. II. METHODS A. Basic Architecture of ODENet The ordinary differential equations network was proposed 4, 16 as a continuous version of the famous ResNet 17 for dealing with time series data modeled by ordinary differential equations (ODEs). Mathematically, the consecutively repeating building blocks -each layer of a residual network can be expressed as y k+1 = y k + f (y k ; θ k ), where y k is the output of k th hidden layer, y k+1 is the output of (k + 1) th hidden layer and f (y k ; θ k ) represents the function of a network layer parameterized by θ k . After a simple algebraic transformation, we can get y k+1 -y k h = f (y k ;θ k ) h , which is the Euler's discretization scheme of ODEs, dy dt = f (y; θ ) h . (1) As a consequence, the forward propagation process of a residue network is actually equivalent to the numerical solvation of a group of corresponding ordinary differential equations. Alternatively, it also means if we use an ODE solver to solve the ODEs directly, the process of forward propagation in a residue network is accomplished too. This significant finding lays down the theoretical foundation of ODENet. The application of ODE solvers could easily cope with input data with unequal time intervals, fight against medium-level noises, control the numerical errors and dynamically adjust its convergence criteria. To enhance the ability of learning the explicit governing ODEs from the pre-given time series data, in a previous work we combined the ODENet with symbolic regression and sparse identification 4 . Symbolic regression means the explicit form of f (y; θ ) is characterized through parameters θ by expanding f (y) on a complete set of orthogonal basis functions Γ(y), i.e. f (y; θ ) = θ Γ(y). Consequently, the learning of ODEs becomes to determine the unknown parameters θ from the data. In practice, polynomials are the most often used basis functions. Sparse identification means in the loss function L, an additional regulation term θ 1 is added in order to remove redundant free parameters θ as many as possible. So that the loss function contains two parts: L = y -y 1 + ε θ 1 . (2) The first part controls the difference between the training data y and the predicted data y by ODENet, while the second part aims at a minimal model according to the Occam's razor. Here ε is a hyperparameter. To obtain the optimal parameters θ , the classical Back Propagation (BP) algorithm 18 is adopted to make an update, which will be repeated for many iterations until the loss function converges or is less than the threshold. Please see Fig. 1 on the flowchart of ODENet or refer to Ref. 4 for further details. B. Multiscale Modeling of Chemical Reactions Without loss of generality, we consider a chemical system with N species and M reactions 19 , ν 1 j S 1 + ν 2 j S 2 + • • • + ν N j S N k j -→ ν 1 j S 1 + ν 2 j S 2 + • • • + ν N j S N , j = 1, 2, • • • , M, (3) where k j > 0 denotes the rate constant of the reaction j. The nonnegative integers {ν i j } and {ν i j } denote the stoichiometric coefficients of the reactants and products respectively. The stoichiometric matrix is introduced as U = [(u i j )] N×M with elements being u i j = ν i jν i j . Chemical Master Equations We focus on the molecular number of species (S 1 , S 2 , • • • , S N ) represented by a stochas- tic variable n = (n 1 , n 2 , • • • , n N ) T in a reaction vessel of volume V . When the magnitude of (n 1 , n 2 , • • • , n N ) T is relatively small compared with the Avogadro's constant, the randomness comes into play due to the intrinsic stochasticity of molecular collisions. From the perspective of ensemble average, we can denote the probability of the system in the state n by p(n,t), where the time-dependence is usually omitted as p(n). With respect to the reactions in (3), the probability distribution obeys the following chemical master equations (CMEs), in a compact form as, d dt p(n) = M ∑ j=1 p(n -u j )Φ j (n -u j ) -p(n)Φ j (n) , (4) accompanied by the initial condition p(n)| t=0 = p 0 (n). Here u j is the j-th column of stoichiomet- ric matrix U = (u 1 , u 2 , • • • , u M ) , and Φ j (n) is the mesoscopic propensity function characterizing the probability Φ j (n)dt for which the j-th reaction occurs once within the time interval [t,t + dt). In general, the state-dependent mesoscopic propensity function Φ j (n) of CMEs is assumed to follow the laws of mass-action, Φ j (n) = k j V N ∏ l=1 V -ν l j C ν l j n l , (5) which is the product of molecular number in a polynomial form and the rate coefficient k j for n l ≥ ν l j , ∀l = 1, 2, • • • , N. When there are not enough particles to form a reactant, saying S l , such that n l < ν l j , the propensity reduces to zero, Φ j (n) = 0. Stochastic Simulations In most cases, the chemical master equations in ( 4 ) are a huge group of ordinary differential equations, which are quite computational consuming. Alternative efficient sampling algorithms are needed. The Gillespie algorithm (GA) 20 , which is able to generate typical time evolutionary trajectories of species according to the reaction mechanisms and reaction rates in a stochastic way, maybe the most famous one. Gillespie implemented two stochastic simulation algorithms. The one is the direct method (DM) and the other is the first-reaction method (FRM). These two methods are theoretically equivalent, so we here only implement the first reaction method. The FRM generates putative time for every reaction and chooses a time at which the corresponding reaction would occur while no other reaction occurred before that. By independently running the Gillespie algorithm once and again, statistics on the corresponding stochastic trajectories will converge to the corrected probability distribution given by the chemical master equations. They constitute the training data set to feed into the machine learning algorithms. Moment-Closure Equations in Kurtz's Limit Although CMEs provide a relatively accurate way to model general chemical reaction systems, it leads to a heavy burden in both modeling and experiments since the dimensionality of the transition matrix is usually extremely high. Moreover, the time-consuming numerical simulation of CMEs becomes a common bottleneck when the number of species or reactions is large. In order to make a simplification, we turn to look at the mean density of species, c i = ∑ n V -1 n i p(n), (6) when the molecular number of reactants becomes large. To deduce the macroscopic kinetics of the concentration c i , we multiply (4) by the number density V -1 n i and take the summation over all admissible state {n} on both sides, which yields, d dt ∑ n V -1 n i p(n) = M ∑ j=1 ∑ n V -1 n i p(n -u j )Φ j (n -u j ) -p(n)Φ j (n) = M ∑ j=1 u i j ∑ n V -1 p(n)Φ j (n) , (7) where in the last step we have used the variable substitution nu j = n and have neglected the boundary terms. Direct calculation shows that the volume density of mesoscopic propensity func- tion deduces, V -1 Φ j (n) = φ j (V -1 n) + O(V -1 ), with φ j (c) = k j ∏ N l=1 c l ν l j /ν l j ! being the usual macroscopic propensity function. Taking the limit of V → +∞, n → +∞ while keeping V -1 n finite, we have the following massaction equations (MAEs) d dt c i (t) = M ∑ j=1 (ν i j -ν i j )φ j (c), (8) on a nonnegative continuous state space {c|c ∈ R N ≥0 }. The MAEs in ( 8 ) is the macroscopic description derived from the mesoscopic CMEs of the reaction system (3). A rigorous mathematical justification of the above limit process was first done by Kurtz in the 1970s 21 . Similar procedure can be carried out for high-order moments of PDF, like the second-order variance studied in the first example in Section III. Remark II.1 According to the results proved by Kurtz 21 , in the limit of V → +∞, for any finite time the solution of CMEs in (4) will converge in probability to the solution of the corresponding MAEs in (8), provided the initial conditions lim V →+∞ V -1 n(t = 0) = c(t = 0), which is a straightforward consequence of the Central Limit Theorem. Our derivation above from the CMEs in (4)   to MAEs in (8) for the reaction system (3) serves as a formal illustration of Kurtz's theorem. 1 ε dB dt = G(A, B), (9) where A and B respectively stand for slow and fast variables after some kind of proper nondimensionalization. ε 1 is a small parameter characterizing the gap between fast and slow time scales in the dynamics. With respect to above dynamics, QSSA states that in the slow time scale dominated by the changes in A, B can be regarded as remaining at a dynamically equilibrium state (quasi steady state) due to their fast reactive nature, meaning approximately we have G(A, B) = 0. If B can be uniquely solved from this algebraic relation, i.e. B = g(A), the original time-scale separated dynamics could be simplified as dA dt = F(A, g(A)), B = g(A). (10) QSSA is a very classical model reduction approach and has been widely used in the study of chemical reactions, see e.g. Ref. 22 for details. III. RESULTS AND DISCUSSION In this section, through two concrete examples -the single proliferative compartment model (SPCM) of IFE (interfollicular epidermis) maintenance as well as a gene network with autoregulatory negative feedback, we are going to show how machine learning and multiscale modeling help each other in the study of chemical reactions. A. Single Proliferative Compartment Model The Basic Model In the first example, the SPCM of IFE maintenance considered by Clayton et al. 23 is adopted to illustrate how multiscale modeling helps to reduce the computational cost of machine learning during inferring the detailed reaction mechanisms and reaction rates. According to the observations by Clayton et al. 23 , the clone fate of proliferating epidermal progenitor cells (EPCs) plays an essential role in adult epidermal homeostasis. And the key clone size distribution is modeled by chemical master equations, whose explicit forms are the major goal of machine learning. By taking the explicit correspondence between mesoscopic chemical master equations and macroscopic mass-action equations in the Kurtz's limit, the challenging task of learning detailed probability distribution function is converted into learning low-order moments. Obviously, the latter is much easier. A similar idea has been previously applied by one of the authors to investigate the kinetics of amyloid aggregation, but without referring to machine learning 24, 25 . Consider two reactant species in the single-proliferative compartment model, including proliferating EPCs (denoted as A) and post-mitotic cells in the basal layer (B). There are four reactions which involve symmetric cell division and asymmetric cell division. As shown in Fig. 2a With respect to the SPCM and coefficients given in Fig. 2 , 10 6 times independent stochastic simulations are performed by using the Gillespie algorithm. They constitute the training data set to feed into our following ODENet-based machine learning procedure. Learning Mass-Action Equations by ODENet Here our major goal is to obtain the SPCM in Fig. 2a and the explicit rate constants. However,      d n A dt = α 11 n A + α 12 n B + α 13 n A 2 + α 14 n A n B + α 15 n B 2 , d n B dt = α 21 n A + α 22 n B + α 23 n A 2 + α 24 n A n B + α 25 n B 2 . ( 11 ) Now we implement the ODENet to learn the dynamics in (11). Clearly, not all reaction rate constants will appear in the final model. Those redundant coefficients will be picked out by ODENet and removed through sparse identification. After training and regression, only three non-zero coefficients α 11 = 0.0079, α 21 = 1.0903 and α 22 = -0.3094 are kept in the final results. Learning High-Order Moment Equations During the learning procedure of ODENet, since all coefficients in front of quadratic terms in (11) are removed, we can make a conclusion that only first-order reactions are present in the current system. Then with respect to above learned dynamics and coefficients, the desired single Deriving Chemical Master equations The relations among desired rate constants k 1 , k 2 , r 1 , r 2 , λ , Γ and those learned parameters α's and β 's are stated through the following matrix, i.e.                          1 -1 -1 -1 0 0 -1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 2 -2 -2 -2 0 0 -1 3 0 1 1 0 0 0 0 0 0 1 0 2 0 1 0 0 -1 1 0 1 1 0 1 -1 -1 -1 0 -1                          V              r 1 λ r 2 λ k 1 k 2 λ Γ              û =                          α 11 α 21 -α 22 β 11 β 12 β 21 β 22 -β 31 β 32 β 33                          b . (14) Direct calculations show that the rank of the augmented matrix (rank(V | b) = 7) is larger than that of the coefficient matrix (rank(V ) = 6), meaning the linear equations in ( 14 ) constitute an overdetermined system, which can be solved through the Least Square Method. The unique leastsquare solution is given by û = V T V -1 V T b, whose relative errors with respect to the true values are less than 8%. Parameters k 1 k 2 r 1 r 2 λ Γ true value 0 0 0.0836 0.0764 1.1 0.31 learned value 0.0123 -0.0197 0.0831 0.0824 1.1059 0.3074 relative errors ∼ ∼ 0.60% 7.85% 0.54% 0.84% TABLE I. Comparison on the learned rate constants for (12) by ODENet with the true values. Even though k 1 and k 2 are not exactly identified as zero, their values are about one order of magnitude smaller than the others. In this sense, we have successful reconstructed the original SPCM based on the stochastic time trajectories of n A and n B in the training data set. As further validated in Fig. 3 , the joint probability distribution of the desired single-proliferative compartment model is honestly reproduced (see SI for the marginal probability distribution), which highlights the efficiency and effectiveness of our integrated approach of ODENet with multiscale modeling during the study of chemical reactions. B. A Gene Network with Autoregulatory Negative Feedback The Basic Model In the second example, we plan to show how machine learning can be used for model reduction, an important aspect of multiscale modeling with vast applications in chemical reactions. To illustrate our ideas, let us consider a gene network with autoregulatory negative feedback, which includes five reactants -the gene (G), mRNA (M), protein (P), and two gene-protein complexes (GP, GP 2 ). Among them, there are eight reactions (see Fig. 4a ). k 0 , k s , k dm are rate constants of transcription from the gene G, translation into the protein P, and mRNA degradation, respectively. The gene can bind with either one or two proteins, whose forward and backward reaction rate constants are denoted as k 1 , k -1 , k 2 and k -2 separately. Furthermore, it is assumed that GP produces mRNA at the same rate k 0 as the transcription rate of G alone. Macroscopically, the gene network in Fig. 4a is described by chemical mass-action equations,                              d dt c P = k s c M -k 1 c G c P + k -1 c GP -k 2 c P c GP + k -2 c GP 2 , d dt c M = k 0 c G + k 0 c GP -k dM c M , d dt c G = -k 1 c G c P + k -1 c GP , d dt c GP = k 1 c G c P -k -1 c GP -k 2 c P c GP + k -2 c GP 2 , d dt c GP 2 = k 2 c P c GP -k -2 c GP 2 . (15) It is noted that the total gene concentration is a constant due to the conservation law, i.e. c G + c GP + c GP 2 = c total . To produce a time-scale separation of reactions, we choose k 1 = 3, k -1 = 2. Model Reduction by ODENet Due to the existence of time-scale separation, it is possible to make a simplification of the reaction system in (15). Classically, this is done by analytical methods, like Quasi Steady-State Assumption and Partial Equilibrium Assumption 22, 26 . Here, we are going to show how the simplification procedure can be carried out automatically by ODENet. Pearson's coefficient dc P /dt dc M /dt dc G /dt dc GP /dt dc GP 2 /dt dc P /dt 1 0.9681 0.5020 0.4228 0.4215 dc M /dt 1 0.2743 0.1832 0.1820 dc G /dt 1 0.9892 0.9812 dc GP /dt 1 0.9956 dc GP 2 /dt 1 TABLE II. Pearson's correlation coefficients among time derivatives of five concentration variables in (15). At the first step, with the help of traditional classification algorithms, like the correlation analysis based on the Pearson's coefficient between concentration derivatives (see Table. II), the fast and slow variables can be easily separated into two groups. Inspired by the classical results of Michaelis-Menton kinetics, we suppose three fast variables c G , c GP , c GP 2 (see Fig. 4b-4e) are characterized by fractional functions, whose numerator and denominator are polynomials of c P (up to the second-order in the current study). In contrast, c M does not appear in the fractional functions, since the last three formulas in (15) contain no terms of c M . Consequently, the simplified model we are seeking for is given by                      d dt c P = k s c M -k 1 c P H (c G ) + k -1 H (c GP ) -k 2 c P H (c GP ) + k -2 H (c GP 2 ), d dt c M = -k dM c M + k 0 H (c G ) + k 0 H (c GP ) , H (c G ) = Ω 1 Ω , H (c GP ) = Ω 2 Ω , H (c GP 2 ) = Ω 3 Ω , (16) where Ω = β 1 + β 2 c p + β 3 c 2 P , Ω 1 = α 11 + α 21 c P + α 31 c 2 P , Ω 2 = α 12 + α 22 c P + α 32 c 2 P , Ω 3 = α 13 + α 23 c P + α 33 c 2 P . Parameters β 1 β 2 β 3 α 11 α 21 α 31 QSSA 0.53 0.67 1 0.0159 0 0 ODENet 0.54 0.66 1 0.0163 0 0 Relative error 1.89% 1.49% ∼ 2.52% ∼ ∼ α 12 α 22 α 32 α 13 α 23 α 33 QSSA 0 0.0201 0 0 0 0.03 ODENet 0 0.020 0 0 0 0.03 Relative error ∼ 0.50% ∼ ∼ ∼ 0% TABLE III. Comparison on the learned parameters for (16) by ODENet with those by QSSA. All values are normalized by β 3 . β 1 , • • • , β 3 and α 11 , • • • , α 33 are twelve free parameters to be specified. As summarized in Table. III, the simplified model learned by ODENet is very close to that by QSSA (see next section). In particular, terms of α 21 c P , α 31 c 2 P , α 12 , α 32 c 2 P , α 13 and α 23 c P are removed by sparse identification during the learning procedure. A major difference between two simplification methods lies in the extra four underlined terms on the right-hand side of the first formula in (16). In QSSA, these four terms are exactly cancelled by each other. While during the simplification procedure aided by ODENet, we can only conclude that their sum is quite small instead of exactly zero (see SI). Comparison with QSSA Our above ODENet aided model reduction is consistent with the classical quasi steady-state approximation. Since G, GP, GP 2 are considered as the fast intermediates, in contrast to the slow species P and M, a direct application of QSSA to (15) leads to c G c total = K 3 Ω , c GP c total = K 2 c P Ω , c GP 2 c total = c P 2 Ω , (17) where Ω = K 3 + K 2 c P + c P 2 , K 1 = k -1 /k 1 , K 2 = k -2 /k 2 , K 3 = k -1 k -2 /(k 1 k 2 ) which has been used to evaluate the performance of our ODENet aided model reduction. IV. CONCLUSION Nowadays, various machine learning algorithms, like deep learning and reinforcement learning, have found their applications in diverse fields with great success. While in the field of chemical reactions, related studies begin to emerge, yet are still quite few. In the current paper, through two concrete biochemical examples, the single proliferative compartment model and a gene network with autoregulatory negative feedback, we present our key ideas on how machine learning and multiscale modeling can help each other during the study of chemical reactions. And, as we believe, an effective integration of two approaches will be crucial for the success of related studies in this direction. Potential generalizations of our current work include but are not limited to: FIG. 1 . 1 FIG. 1. An integration of ODENet with multi-scale modeling in the study of chemical reactions. The upper panel illustrates the ODENet-based learning procedure of reaction mechanism under the help of multiscale modeling, while the lower panel gives the automatic procedure for model reduction aided by ODENet. The flowchart of ODENet is shown in the middle. C . Model Reduction by QSSA Consider a very general chemical reaction system with time scale separation, which is written in an abstract matrix form,      dA dt = F(A, B), FIG. 2 . 2 FIG. 2. Single proliferative compartment model. (a)The mechanism of single proliferative compartment model. EPCs (red circles) have an unlimited self-division potential to maintain the epidermis at a rate of r 1 λ . Proliferating EPCs cells divide into two post mitotic basal cells (blue stars) at a rate of r 2 λ . Asymmetric divisions of EPCs into itself and post mitotic basal cells are at a rate of 1 -(r 1 + r 2 )λ . After mitosis in the basal layer, the post mitotic basal cells leak at a rate of Γ. k 1 and k 2 represent the rate constants for two additional possible reactions inferred by the ODENet. 10 typical stochastic trajectories for (b) n A and (c) n B are generated by GA. The learned results of ODENet are compared with the training data generated by GA on the (d) average and (e) variance of cell numbers. Here the rate constants in SPCM are set as λ = 1.1, r 1 = 0.0836, r 2 = 0.0764, Γ = 0.31 per week in accordance with 23 . The initial PDF is taken as a delta distribution with p 0 (10, 0) = 1. a direct application of ODENet to learn the time evolution of p(n A , n B ) (or the chemical master equations) from the training data generated by stochastic simulations is prohibited due to heavy computational cost. Therefore, by taking advantage of the knowledge of multiscale modeling in chemical reactions, especially the explicit correspondence between mesoscopic chemical master equations and macroscopic mass-action equations in the Kurtz's limit, we turn to learn low-order moments instead of the probability distribution function governed by chemical mass-action equations.With respect to training data of nA = ∑ n A p (n A , n B ) and n B = ∑ n B p (n A , n B) by averaging the stochastic trajectories generated through Gillespie algorithms, we need to determine the exact types of chemical reactions involving with these two reactants, their reaction orders and reaction rate constants. Without loss of generality, here we make a cutoff on the chemical reactions up to the second order, corresponding to a combination of A, B, A + A, A + B, B + B, which reads FIG. 3 . 3 FIG. 3. Comparison of PDF generated by GA with the learned results of ODENet. Joint probability distributions p(n A , n B ) are shown in (a,e) 1, (b,f) 5, (c,g) 10 and (d,h) 20 weeks respectively. FIG. 4 . 4 FIG. 4. Validation of ODENet aided model reduction. (a) A cartoon illustration of the gene network with negative feedback, including transcription, translation, degradation and a negative feedback loop. Predictions of the reduced model in (16) (blue crosses) are compared with the original model in (15) (red solid lines) on concentrations of (b) protein and mRNA in the slow time scale, and (c) gene and (d-e) gene-protein complexes in the fast time scale. 4 ,k 2 42 = 9, k -2 = 6, k 0 = 0.05, k s = 0.01, k dm = 0.01, meaning the concentrations of G, GP, GP 2 can quickly reach dynamical balance in comparison with those of mRNA and protein. The initial conditions are set as c G = c GP = c GP 2 = 0.01, c P = 0, c M = 5. , andc total = c G + c GP + c GP 2 is a constant.The corresponding reduced equations are-k dM c M + k 0 (K 3 + K 2 c P )c total /Ω,"
}