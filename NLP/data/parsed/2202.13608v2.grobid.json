{
  "title": "Semi-supervised Learning on Large Graphs: is Poisson Learning a Game-Changer?",
  "abstract": "We explain Poisson learning [2] on graph-based semi-supervised learning to see if it could avoid the problem of global information loss problem as Laplace-based learning methods on large graphs. From our analysis, Poisson learning is simply Laplace regularization with thresholding, cannot overcome the problem.",
  "introduction": "Introduction Given a graph G = (V, W ) with V = {x 0 , • • • x n-1 } as a set of training data and W is a n × n real nonnegative weight matrix (weights of the edges of the graphs). Supposed that we are given m < n labels y 0 • • • y m-1 (real or binary) of m nodes (x 0 • • • x m-1 ) , one wishes to infer the labels of the remaining nodes (x m • • • x n-1 ): u(x i ) : V → R (binary class labels are usually of the form sign(u)). Graph-based models have been the main tools for semi-supervised learning.",
  "body": "Introduction Given a graph G = (V, W ) with V = {x 0 , • • • x n-1 } as a set of training data and W is a n × n real nonnegative weight matrix (weights of the edges of the graphs). Supposed that we are given m < n labels y 0 • • • y m-1 (real or binary) of m nodes (x 0 • • • x m-1 ) , one wishes to infer the labels of the remaining nodes (x m • • • x n-1 ): u(x i ) : V → R (binary class labels are usually of the form sign(u)). Graph-based models have been the main tools for semi-supervised learning. Laplace Learning Traditional Laplace learning [6] would use the Laplacian regularization in this variational form: u = arg min u T Lu | u(x i ) = y i , 0 ≤ i < m (1) with L = D -W being the graph Laplacian with D being the degree diagonal matrix (D = diag(W × 1 n )). The solution of (1) satisfies u(x i ) = y i | 0 ≤ i < m, (2) Lu(x i ) = 0 | m ≤ i < n. (3) Laplace Regularization Laplace learning can be modified to introduce a loss-based data term [3] as follows: u = arg min(y -u) 2 + λu T Lu, (4) with the loss (yu) 2 only on labeled data points is added to the Laplace regularization term. Global Information Loss Problem The problem with Laplace learning, or Laplace regularization is that in large graphs, the regularization term u T Lu does transfer the label from labeled nodes to far away nodes in the graphs. Concretely, the following problems are proven: 1. u does not carry label information for the whole dataset, becoming a non-informative function [3] , unrelated to the data distribution [1] . This leads to the learnt function on the graph to be \"peaky\" that peaks at the labeled points and quickly becomes close to a constant value away from the labeled point. 2. Commute time distance ct does not contain graph information for fixed x i , x j as n → ∞ (for a constant c) [5] : ct(x i , x j ) → c d i + c d j . 3. Laplacian kernel between data points are close to zero for fixed x i , x j as n → ∞: K(x i , x j ) → 0. There are many methods that claimed to overcome the problem without any rigorous proof. Exceptions are two methods with mathematically proven properties that could overcome the problems of Laplace-based learning. • l p Laplacian regularization-based distance results in a function that is most related to data distribution when p = d + 1, with d being the intrinsic dimension of the data distribution [1] , overcoming the first problem. This method is computationally infeasible. • l p norm-based distances computed from L † are proven not have the second problem [4] . This method is computationally feasible for medium-sized graphs. Poisson Learning Poisson learning [2] is different from Laplace learning in replacing boundary condition (u (x i ) = y i | 0 ≤ i < m) with Lu(x i ) = c ∈ R| 0 ≤ i < m . It is claimed to avoid the peaky label function problem and let the known label propagates further compared to Laplace learning. For ȳ = mean(y 1 • • • y m ), the solution of Poisson learning satisfies: Lu(x i ) = y i -ȳ | 0 ≤ i < m, (5) Lu(x i ) = 0 | m ≤ i < n, (6) n i=1 d i u(x i ) = 0. ( 7 ) This correspond to the solution of the following variational problem: u = arg min 1 2 u T Lu - m i=1 (y i -ȳ)u(x i ) | n i=1 d i u(x i ) = 0. (8) Essentially, the difference between Laplace learning and Poisson learning is on the labeled nodes: the former fixes the labels of the nodes while the latter fixes the smoothness of labels on the nodes to some constants. Basically, in both Laplace and Poisson learning methods, the labels are propagated by minimizing u T Lu. Kernel viewpoint Let u(X) = [v, z] be the function u on the two parts of the data, v = u[0 • • • m -1] ∈ R m being the labeled part of u and z = u[m • • • n -1] ∈ R n-m being the unlabeled part of u. Let K = L † be the Moore-Penrose inverse of L, being the Laplacian kernel K. In the RKHS induced by the kernel of each method, let φ(x i ) denotes the image of sample x i . We show that all the above methods have decision functions of the form with a constant c ∈ R, called offset, acting as classification thresholding: u(x i ) = j α j K ij + c =< x i , j α j φx j > +c. (9) Laplace learning Laplace learning become: u = arg min f (u)(= [v T , z T ]L[v, z]). ( 10 ) Let L 1 = L[0 • • • m -1, 0 • • • m -1], L 2 = L[m • • • n -1, m • • • n -1], L 12 = L[0 • • • m -1, m • • • n -1] and L 21 = L[m • • • n -1, 0 • • • m -1]. Then f (u) = v T L 1 v + 2z T L 21 v + z T L 2 z. Taking the derivative of f on the variable part z (as v is the fixed part), then the first order condition becomes: ∂f ∂v = 2L 2 z + 2L 21 v = 0 (11) z = -L -1 2 L 21 v + cker(L 2 ) (12) Kernel representation: Let K 2 = L -1 2 , then K 2 is the Laplacian kernel on the unlabeled part of the graph. Weight vector α: -L 21 v: the weight of each nodes becomes the sum of edge weights multiplied by the labels to the labeled nodes, i.e., only border nodes have weights. α i = m j=1 w ij y j (13) Offset c: usually not taken into account, namely c = 0 Laplace regularization The Laplace regularization method, sometimes called soft constraint method, has the following form: u = arg min λ(y -u) 2 + u T Lu. ( 14 ) The solution is u = (L + λI) -1 y = L † y + y λ . ( 15 ) λ allows for a linear interpolation between the solution of Ky(= L † y) and y. With appropriate scaling of y to account for class imbalance, Ky can be considered as the nearest class mean classifier. Kernel representation: K = L † Weight vector α: only on labeled nodes (j < m), α i = y i . Offset Poisson learning We show the solution of the unconstrained problem satisfies the constraint. u = arg min 1 2 u T Lu - m i=1 (y i -ȳ)u(x i ) (16) Taking derivative, with t ∈ R n , t i = y i -ȳ for 0 ≤ i < m and t i = 0 otherwise. Given that the graph is connected: Lu -t = 0 u = L † t + c1 n (17) for some c ∈ R, 1 n (= ker(L)) being the vector of all 1 in R n . We now prove that there exists a c that satisfies the constraint. n i=1 d i u(x i ) =< d, L † t + c1 n > (18) =< d, L † t > +c n i=1 d i (19) (20) Therefore, to have n i=1 d i u(x i ) = 0, c = - < d, L † t > n i=1 d i (21) This is the unique solution of Laplace learning model. Kernel representation: K = L † Weight vector α: only on labeled nodes (i < m), α i = y i -ȳ. Offset c: c = -<d,L † t> n i=1 di . Comparison • All three methods can be interpreted as in (9). It is different from SVMs in the sense that alphai might not be equal to 0. • For Laplace learning, weight vector α might not sum to 0 even with centralizing y to have labels summed to 0. This might make biased decision favoring the class with more weights to labeled nodes (such as labeled nodes of high density). • For Laplace regularization, weight vector α might not sum to 0, but with centralizing y, the decision function is nearest class mean classifier. • For Poisson learning can be seen as Laplace regularization with a chosen threshold c. Conclusion 1. What is the problem with Laplace learning? The weight vector α depend on edge weights adjacent to labeled nodes. Centralizing labels does not solve the problem. Solution? Centralizing α will make it a nearest class mean classifier, with class-mean is a weighted sum of border nodes. 2. Laplace regularization with label centralization becomes nearest class mean classifier. 3. What is Poisson learning in the RKHS? It becomes nearest class mean classifier with an offset (or Laplace regularization with an offset). 4. Can Poisson learning avoid the global information loss problem on large graph? The answer is N O due to L † representation. 5. What is the advantage of Poisson learning? Offset c, which acts as a threshold for classification. Can it improve classification errors on Laplace regularization? Possible if the offset is meaningful. Can it improves AUC scores on Laplace regularization? No, they give the same AUC scores. 6. What is the meaning of the offset c? It depends on how the weights are constructed. One way to interpret is that i sign(u i )d i = 0 would be giving the two class an equal volume. 7. What happened on the extremely small training sizes [2]? Actually, it is the problem with Laplace learning, even with equal numbers of labeled points for each class, densities (on the underlining distributions) at the sampled points may vary greatly, making u unstable. Laplace regularization (with label centralization) and Poisson learning do not have this problem. More data would tend to avoid this problem as labeled data density converges to mean class density. Table 1 : 1 Comparing decision functions of methods Method Kernel α Classification functions Laplace L  † 2 -L 21 v nearest class prototype Regularization L  † y nearest class mean Poisson L  † y -ȳ (i < m) nearest class mean c: usually not taken into account, namely c = 0"
}