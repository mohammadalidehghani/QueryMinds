{
  "title": "Energy-Harvesting Distributed Machine Learning",
  "abstract": "This paper provides a first study of utilizing energy harvesting for sustainable machine learning in distributed networks. We consider a distributed learning setup in which a machine learning model is trained over a large number of devices that can harvest energy from the ambient environment, and develop a practical learning framework with theoretical convergence guarantees. We demonstrate through numerical experiments that the proposed framework can significantly outperform energy-agnostic benchmarks. Our framework is scalable, requires only local estimation of the energy statistics, and can be applied to a wide range of distributed training settings, including machine learning in wireless networks, edge computing, and mobile internet of things.",
  "introduction": "I. INTRODUCTION The environmental impact of large-scale machine learning is a major challenge against the sustainability of future smart ecosystems. For instance, the carbon emission of training a single machine learning model can get as large as the lifetime of five cars [1] . The environmental impact will be even greater with the emergence of machine learning in distributed environments, where millions of devices are expected to participate in training on a regular basis. This, combined with the fact that state-of-the-art machine learning models are trained over billions of parameters [2] , calls for a novel design paradigm for large-scale machine learning. In this paper, we propose energy harvesting [3] for the design of sustainable distributed machine learning systems. We consider a distributed training scenario with N clients (users), who wish to collaborate to train a machine learning model. Each user holds a local dataset D i , and the goal is to train a machine learning model over the joint dataset D 1 , . . . , D N . Training is performed through distributed stochastic gradient descent (SGD) coordinated through a central server, who maintains a global model. At each iteration of training, the server sends the current estimate of the model parameters to the users. Users then locally update the global model by computing a local gradient on their local dataset, and send their local updates to the server. The server then aggregates the local updates from the users, updates the global model, and sends the updated model back to the users. Unlike the conventional distributed SGD setting, in this work, users receive energy through an energy harvesting process [3] - [15] , and can only participate in training if they have energy available to do so. Energy and resource efficiency in machine learning has been studied in various notable works [16] , [17] . Broadly, these settings can be categorized into two. The first line of work focuses on minimizing the energy consumption of the compute or communication framework [16] . The second line of work, on the other hand, is focused on minimizing the training loss within a given energy budget, where all of the energy is available at the beginning of training [17] . In contrast, our work focuses on training with devices that can harvest small amounts of energy from the ambient environment, where energy arrivals are intermittent and non-homogeneous across different devices. Prior to this work, user sampling for distributed machine learning has been primarily investigated in the context of improving communication efficiency or convergence rate [18] - [24] . In these works, the primary goal is to either select a small set of users to participate at a given training iteration in order to reduce the overall communication overhead or due to bandwidth limitations, or to select a few informative users to maximize the convergence rate of training, with the assumption that all users are available to participate in training if selected. In contrast, in our setting, users can only participate in training if they have available energy. Moreover, the energy availability of different users can be different. Several notable works have considered distributed learning when users have a chance to drop out, unlike the current setup, in these settings, user dropouts occur uniformly at random [25] - [27] . We demonstrate that energy-harvesting can be a good candidate for machine learning in distributed networks, through a practical distributed training framework with theoretical convergence guarantees. Our experiments show that the proposed framework significantly outperforms the alternative distributed SGD benchmarks that are agnostic to the energy arrival process. We hope our work to open up new research directions in leveraging energy-harvesting for sustainable machine learning in large-scale mobile and edge networks.",
  "body": "I. INTRODUCTION The environmental impact of large-scale machine learning is a major challenge against the sustainability of future smart ecosystems. For instance, the carbon emission of training a single machine learning model can get as large as the lifetime of five cars [1] . The environmental impact will be even greater with the emergence of machine learning in distributed environments, where millions of devices are expected to participate in training on a regular basis. This, combined with the fact that state-of-the-art machine learning models are trained over billions of parameters [2] , calls for a novel design paradigm for large-scale machine learning. In this paper, we propose energy harvesting [3] for the design of sustainable distributed machine learning systems. We consider a distributed training scenario with N clients (users), who wish to collaborate to train a machine learning model. Each user holds a local dataset D i , and the goal is to train a machine learning model over the joint dataset D 1 , . . . , D N . Training is performed through distributed stochastic gradient descent (SGD) coordinated through a central server, who maintains a global model. At each iteration of training, the server sends the current estimate of the model parameters to the users. Users then locally update the global model by computing a local gradient on their local dataset, and send their local updates to the server. The server then aggregates the local updates from the users, updates the global model, and sends the updated model back to the users. Unlike the conventional distributed SGD setting, in this work, users receive energy through an energy harvesting process [3] - [15] , and can only participate in training if they have energy available to do so. Energy and resource efficiency in machine learning has been studied in various notable works [16] , [17] . Broadly, these settings can be categorized into two. The first line of work focuses on minimizing the energy consumption of the compute or communication framework [16] . The second line of work, on the other hand, is focused on minimizing the training loss within a given energy budget, where all of the energy is available at the beginning of training [17] . In contrast, our work focuses on training with devices that can harvest small amounts of energy from the ambient environment, where energy arrivals are intermittent and non-homogeneous across different devices. Prior to this work, user sampling for distributed machine learning has been primarily investigated in the context of improving communication efficiency or convergence rate [18] - [24] . In these works, the primary goal is to either select a small set of users to participate at a given training iteration in order to reduce the overall communication overhead or due to bandwidth limitations, or to select a few informative users to maximize the convergence rate of training, with the assumption that all users are available to participate in training if selected. In contrast, in our setting, users can only participate in training if they have available energy. Moreover, the energy availability of different users can be different. Several notable works have considered distributed learning when users have a chance to drop out, unlike the current setup, in these settings, user dropouts occur uniformly at random [25] - [27] . We demonstrate that energy-harvesting can be a good candidate for machine learning in distributed networks, through a practical distributed training framework with theoretical convergence guarantees. Our experiments show that the proposed framework significantly outperforms the alternative distributed SGD benchmarks that are agnostic to the energy arrival process. We hope our work to open up new research directions in leveraging energy-harvesting for sustainable machine learning in large-scale mobile and edge networks. II. SYSTEM MODEL A. Training Setup We consider a distributed training setup in a network with N devices (users). The users are connected through a central server who coordinates the training. User i has a local dataset D i , consisting of D i data points. We define the total number of data points in the network as D = i∈[N ] D i . The goal is to train a model w that minimizes a global loss function F (w) = 1 D N i=1 Di j=1 l(w, x ij ) (1) where l(w, x ij ) denotes the loss of data point x ij from the local dataset of user i. Note that the loss function in (1) is arXiv:2102.05639v1 [cs.LG] 10 Feb 2021 evaluated with respect to the entire set of data points that belong to the N users. As such, equation (1) can also be written as F (w) = N i=1 p i F i (w) (2) where p i = Di D such that n i=1 p i = 1, and F i (w) = 1 D i Di j=1 l(w, x ij ) (3) represents the local loss function of user i. Training is performed through distributed SGD, in which the model parameters are updated iteratively in the negative direction of the gradient. Each iteration is represented by a discrete time instant t ∈ {0, 1, 2, . . .}. The current estimation of the model parameters at iteration t is represented by a d-dimensional vector w (t) ∈ R d , where d is the model size. We now review the conventional distributed SGD protocol. In this setting, at the beginning of each iteration, the server sends w (t) to the users. Then, user i ∈ {1, . . . , N } computes a local stochastic gradient, g i (w (t) , ξ (t) i ) ∇F i (w (t) , ξ (t) i ) (4) by using a (uniformly) random sample ξ (t) i from the local dataset D i . Hence, the stochastic gradient is an unbiased estimator of the true gradient of user i, E ξ (t) i [∇F i (w (t) , ξ (t) i )] = ∇F i (w (t) ), (5) where ∇F i (w (t) ) is the gradient of the local loss function in (3) . The gradient of the global loss function in (1) is given by, ∇F (w (t) ) N i=1 p i ∇F i (w (t) ). (6) After the local computations, users send their local gradients from (4) to the server. The server then updates the model, w (t+1) = w (t) -η N i=1 p i g i (w (t) , ξ (t) i ) where η is the learning rate (step size), and sends the updated model back to the users for the next iteration. B. Energy Harvesting Profile of the Users This work considers devices that are powered by the energy harvested from the ambient environment, such as RF, solar, or kinetic energy [3] , [4] . We assume that one step of the SGD protocol costs a unit amount of energy at each user, which includes computing the local gradient from (4) and sending it to the server. It is also assumed that each user has a unit battery that can store enough energy for one step SGD. We let E t i denote the energy arrival process at user i, in particular E t i = 1 if user i receives energy at time t and E t i = 0 otherwise. The specific distribution of the energy arrivals depends on the harvesting process. Our focus is on the following energy harvesting scenarios. 1) Deterministic Energy Arrivals: We first consider a deterministic energy harvesting scenario in which energy arrivals are known by each user in advance. We assume that energy may arrive at arbitrary non-overlapping time instances, and let I i = {t : E t i = 1} denote the set of time instances at which user i receives energy. We also define I t i = max t :t ≤t, t ∈Ii t for the time of the most recent energy arrival up to t, and Īt i = min t :t >t, t ∈Ii t for the time of the next energy arrival after time t. Finally, for a given t, we define the duration between I t i and Īt i as, T t i = Īt i -I t i (8) 2) Stochastic Energy Arrivals: We next consider the stochastic energy harvesting scenario where energy arrivals are modeled through a stochastic process. Unlike the deterministic setting, users do not know the exact time instant at which energy will be received, but only the probabilistic model governing the underlying harvesting process. Our focus is on the following stochastic arrival scenarios. (Binary Arrivals) In the binary energy arrival setup, at each time instant, user i receives a unit amount energy with probability β i . More specifically, we let E t i ∼ Bern(β i ): E t i = 1 with probability β i 0 with probability 1 -β i (9) where β i ∈ (0, 1], to represent whether or not user i receives energy at time t. Parameter β i quantifies how frequent user i receives energy, and may vary from one user to another. (Uniform Arrivals) We next consider a uniform energy arrival scenario in which device i receives a unit amount of energy at a uniformly random time instant every T i time instants. Formally, for any t such that t mod T i = 0, user i receives a unit amount of energy at a uniformly random time instant within {t, . . . , t + T i -1}. Note that this is not an immediate generalization of the first setting, as in the former setup there is a non-zero probability that user i will never receive energy in T i time instants. In contrast, in the second setting, user i receives a unit amount energy with probability 1 at every T i time instants, but the exact time instant at which energy is received is unknown. As we demonstrate in our experiments, the conventional distributed SGD strategy from Section II might bias the model towards users that have more frequent energy arrivals, causing a performance loss in training. As such, the training strategy should take into account the energy arrival patterns of the users. Main Problem. Given the above training and energy harvesting settings, the main problem we study in our work is, \"How to design a distributed stochastic gradient descent framework for energy harvesting devices, where energy arrivals are intermittent and heterogeneous, while ensuring theoretical convergence guarantees?\". In the sequel, we provide a simple energy harvesting distributed learning strategy with provable convergence guarantees. The proposed strategy takes into account the intermittent energy availability due to the energy harvesting process of the individual users while ensuring that the model does not if E t i = 1 then 5: Sample an integer J uniformly random from {0, . . . , T t i -1}. 6: Update U t+J i = 1. 7: if U t i = 1 then 8: Compute the local gradient gi(w (t) , ξ i ). 9: Send T t i gi(w (t) , ξ i ) to the server. Server: 10: Update the model according to (11) . 11: Send the model parameters w (t+1) to the users. bias towards any particular user. III. ENERGY HARVESTING DISTRIBUTED SGD A. Distributed SGD with Deterministic Energy Arrivals We first study the deterministic energy harvesting scenario and provide a simple distributed training framework with theoretical convergence guarantees. The individual steps of our framework is provided in Algorithm 1. Our framework consists of three main components, user scheduling, local gradient computations, and server-side model update. 1) User scheduling: The first component of our framework is user scheduling for training. Conventional user selection algorithms for distributed SGD are designed under the assumption that all users are inherently available to participate in the training process if selected, and employ a user sampling strategy to reduce the communication load or aim at selecting the users that will maximize the convergence rate for training [18] - [21] . In contrast, in our setup, not all users can participate in the training process at all rounds. This is due to the intermittent energy arrivals, if a user has no energy at a given time instant, they will not be able to participate in training. A naive approach would be to utilize the conventional distributed SGD algorithm from (7) . However, doing so may bias the trained model towards users who have more frequent energy availability. Another approach is to wait until all users become available, and then use the conventional distributed SGD algorithm from (7) . However, waiting for all users to have enough energy can significantly increase the total training time needed to achieve a target performance level. Instead, we propose a practical scheduling strategy that can be performed locally by the users, while ensuring that the model does not bias towards any user. In this setting, whenever a user receives energy, i.e., E t i = 1 for some t, the user samples an integer J uniformly at random from the set {0, . . . , T t i -1}, and participates at iteration t + J. 2) Local gradient computation: At the beginning of each training iteration, the server sends the current estimate of the model parameters w (t) to the users. If a user decides to number of iterations T , initial model parameters w (0) . output Model parameters (weights) w (T ) . 1: for iteration t = 0, . . . , T -1 do Users i = 1, . . . , N : 2: if E t i = 1 then 3: Compute the local gradient gi(w (t) , ξ i ). 4: Send γ t i gi(w (t) , ξ i ) to the server. Server: 5: Update the model according to (12) . 6: Send the model parameters w (t+1) to the users. participate in the current training iteration t, according to the scheduling strategy from Section III-A1, it computes the local gradient from (4). Then, the user sends to the server a scaled version of their local gradient, T t i g i (w (t) , ξ (t) i ) = T t i ∇F i (w (t) , ξ (t) i ) (10) 3) Server-side model update: After receiving the local computations from (10) from the participating users, the server updates the model as: w (t+1) = w (t) -η i∈St p i T t i g i (w (t) , ξ (t) i ) where S t denotes the set of users who have participated at round t. Note that due to the stochastic nature of the user scheduling process, S t is random. As we demonstrate in Section IV, this process provides theoretical convergence guarantees for the model. Moreover, the user scheduling process does not require a central coordinator and can be performed locally by the users, solely based on local energy estimations, hence is scalable to large networks. B. Distributed SGD with Stochastic Energy Arrivals We next consider distributed training under the stochastic energy harvesting setting. The training strategy again consists of three main components, user scheduling, local gradient computation, and server-side model update. We employ a besteffort user scheduling strategy, where each user participates in training as soon as they receive energy, by computing the local gradient from (4), and sending to the server a scaled gradient γ t i g i (w (t) , ξ (t) i ), where γ t i = 1 βi and γ t i = T i for the binary and uniform energy arrival settings, respectively. After receiving the local computations from the participating users, the server updates the model as, w (t+1) = w (t) -η i∈St p i γ t i g i (w (t) , ξ (t) i ) (12) The individual steps of this process are provided in Algorithm 2. IV. CONVERGENCE ANALYSIS We now state the convergence guarantees of our framework, by first reviewing a few common technical assumptions [19] , [28] that will be needed for our convergence analysis. Assumption 1. (Bounded variance) The variance of the stochastic gradients from (4) are bounded: E ξ (t) i [||g i (w (t) , ξ (t) i )-∇F i (w (t) )|| 2 ] ≤ σ 2 for i ∈ [N ] (13) Assumption 2. (Second moment bound) The expected squared norm of the stochastic gradients from (4) are bounded: E ξ (t) i [||g i (w (t) , ξ (t) i )|| 2 ] ≤ G 2 for i ∈ [N ] (14) We also assume that the local loss functions F i (w) for i ∈ [N ] (and thus the global loss function F (w)) are µ-strongly convex and L-smooth, as in [19, Assumptions 1 and 2]. Next, we provide a key technical lemma. Lemma 1. (Unbiasedness) For distributed SGD with deterministic energy arrivals, E St i∈St p i T t i g i (w (t) , ξ (t) i ) = N i=1 p i g i (w (t) , ξ (t) i ), (15) hence the user scheduling scheme is unbiased. Moreover, for distributed SGD with stochastic energy arrivals, the unbiasedness condition from (15) holds by replacing T t i with 1 βi and T i for binary and uniform arrivals, respectively. Proof. We first define a Bernoulli random variable α t i to represent whether or not user i participates at iteration t: α t i = 1 if user i participates at time t 0 otherwise (16) Then, for any given t, P [α t i = 1] = P [J = t -I t i ] = 1 T t i (17) By letting α t (α t 1 , . . . , α t N ), we find that, E St i∈St p i T t i g i (w (t) , ξ (t) i ) = E αt N i=1 α t i p i T t i g i (w (t) , ξ (t) i ) (18) = N i=1 p i T t i 1 T t i g i (w (t) , ξ (t) i ) (19) where (18) follows from S t = N i=1 α t i , and ( 19 ) is from (17) . The proof for stochastic arrivals follows the same lines along with the observation that, for the best-effort user scheduling strategy P [α t i = 1] = P [E t i = 1] . We now state our convergence guarantees. Theorem 1. For training a machine learning model from (1), using the distributed SGD algorithm with deterministic energy arrivals and a constant learning rate η ≤ min 1 2µ , 1 L . E[F (w (T ) )] -F (w * ) ≤ L µ (1 -ηµ) T (F (w (0) ) -F (w * ) - ηC 2 ) + ηLC 2µ (20) in T iterations, where w * denotes the optimal model parameters that minimize the global loss function in (1), and C N i=1 T i,max -1 p 2 i + N i=1 N j=1 p i p j G 2 , ( 21 ) where T i,max max{T 1 i , . . . , T T i } for i = 1 . . . , N . Remark 1. The first term in the right hand side of (20) vanishes as T → ∞, whereas the second term ηLC 2µ represents a non-vanishing error term due to the constant learning rate. By using a decreasing learning rate as in [19] , [21] , this term can also be made vanishing as T → ∞. Proof. (Sketch) The proof follows standard steps for the convergence analysis of distributed SGD algorithms [18] , [19] , [28] , hence we provide a proof sketch in the sequel. By letting g t i g i (w (t) , ξ t ), w * arg min w F (w), and ξ t (ξ (t) 1 , . . . , ξ (t) N ), from (11) we find that, E St,ξt [ w (t+1) -w * 2 ] = E St,ξt [ w (t) -w * 2 ] -2ηE St,ξt [ w (t) -w * , i∈St p i T t i g t i ]+η 2 E St,ξt [ i∈St p i T t i g t i 2 ] (22) From Lemma 1, (5), and µ-strong convexity, we observe that, E St,ξt [ w (t) -w * , i∈St p i T t i g t i ] = E St,ξt [ w (t) -w * , i∈St p i T t i g t i - N i=1 p i ∇F i (w (t) ) ] + E St,ξt [ w (t) -w * , N i=1 p i ∇F i (w (t) ) ] (23) = w (t) -w * , ∇F (w (t) ) ≥ F (w (t) ) -F (w * ) + µ 2 w * -w (t) 2 (25) We also have from Lemma 1 that, E St,ξt [ i∈St p i T t i g t i 2 ] = E St,ξt [ i∈St p i T t i g t i - N i=1 p i g t i 2 ] + E St,ξt [ N i=1 p i g t i 2 ] (26) By combining (22) , (25) , and (26) , we find that, E St,ξt [ w (t+1) -w * 2 ] ≤ (1-ηµ)E St,ξt [ w (t) -w * 2 ] -2η(F (w (t) ) -F (w * )) +η 2 E St,ξt [ i∈St p i T t i g t i - N i=1 p i g t i 2 ]+η 2 E St,ξt [ N i=1 p i g t i 2 ] (27) By defining α t i as in ( 16 ) and α t = (α t 1 , . . . , α t N ), E St,ξt [ i∈St p i T t i g t i - N i=1 p i g t i 2 ] = E αt,ξt [ N i=1 p i (α t i T t i g t i -g t i ) 2 ] (28) = N i=1 p 2 i E αt,ξt [ α t i T t i g t i -g t i 2 ] + N i=1 N j=1 j =i E αt,ξt [ p i (α t i T t i g t i -g t i ), p j (α t j T t j g t j -g t j ) ] (29) = N i=1 p 2 i E αt,ξt [ α t i T t i g t i -g t i 2 ] (30) = N i=1 p 2 i (T t i ) 2 E ξt [E αt|ξt [(α t i - 1 T t i ) 2 g t i 2 |ξ t ]] (31) ≤ N i=1 p 2 i (T i,max -1)G 2 (32) where (30) holds from (17) and that (α t i , g t i ) is independent from (α t j , g t j ) for all i = j; (32) is from ( 17 ) and ( 14 ). Finally, η 2 E St,ξt [ N i=1 p i g t i 2 ] ≤ N i=1 p 2 i E ξt [ g t i 2 ] + N i=1 N j=1 j =i p i p j E ξt [ g t i g t j ] (33) ≤ N i=1 p 2 i E ξt [ g t i 2 ] + N i=1 N j=1 j =i p i p j 2 E ξt [ g t i 2 + g t j 2 ] (34) ≤ N i=1 N j=1 p i p j G 2 (35) where ( 33 ) is from the Cauchy-Schwarz inequality; (34) is from the AM-GM inequality; (35) is from (14) . By combining ( 27 ) and ( 32 ) with ( 35 ) and noting that -2η(F (w (t) )-F (w * )) ≤ 0, E St,ξt [ w (t+1) -w * 2 ] ≤ (1-ηµ)E St,ξt [ w (t) -w * 2 ] +η 2 N i=1 (T i,max -1) p 2 i + N i=1 N j=1 p i p j G 2 (36) The remainder of the proof follows from standard induction arguments as in [19] , [21] , hence is omitted. Corollary 1. For distributed SGD with stochastic energy arrivals, Theorem 1 holds by replacing T i,max with 1 βi for binary arrivals and with T i for uniform arrivals, respectively. The convergence analysis follows the same steps. V. EXPERIMENTS In our experiments, we consider a conventional image classification task with 10 classes on the CIFAR-10 dataset [29] , distributed over 40 users uniformly at random. Training is performed via distributed SGD using the convolutional neural network architecture from [25] (about 10 6 model parameters). To demonstrate the impact of non-homogeneous energy-arrivals, users are partitioned into 4 equal-sized groups A 0 , . . . , A 3 such that A k = {i : i mod 4 = k}, and the energy profiles of users in group A k are set as: E t i = 1 ∀t such that t mod τ k = 0 0 otherwise ( 37 ) for i ∈ A k , where (τ 0 , τ 1 , τ 2 , τ 3 ) = (1, 5, 10, 20) . Therefore, users in group A 0 receive energy at every time-instant t, whereas users in groups A 1 , A 2 , and A 3 receive energy at every 5, 10, and 20 time-instants, respectively. We compare our framework with the following distributed SGD benchmarks: Benchmark 1. We first implement the distributed SGD framework from Section II when users participate in training as soon as they have energy available, by computing the gradient from ( 4 ) and sending it to the server, and then wait for the next energy arrival. Note that in this setting users do not scale the gradients with respect to the energy arrivals. Benchmark 2. We then consider the distributed SGD framework from Section II when the global model is updated only if all users have enough energy to participate in training. That is, the server waits until all users have energy, then sends the current model parameters to the users, users compute the stochastic gradient from (4) and send it back to the server, and then the server updates the model as in (7) . Hence, in this case, the model is updated once every t = 20 iterations. Finally, we also implement the conventional distributed SGD framework from Section II when all users are available at every iteration, which represents our target (desired) accuracy level. We demonstrate our results in terms of the test accuracy with respect to time t in Figure 1 . Our results show that Algorithm 1 achieves the same accuracy level (about 80%) as conventional distributed SGD, whereas the two benchmarks achieve an accuracy of 64% and 52%, respectively, within t = 1000 iterations. This is due to the fact that the first benchmark favors users with more frequent energy arrivals, hence the model is biased. The second benchmark waits for all users to have enough energy before making a single SGD update, hence, even though the training algorithm is unbiased, its convergence rate is very slow. In contrast, Algorithm 1 converges fast while achieving good accuracy. VI. CONCLUSION We have studied distributed machine learning when users have intermittent energy availability, and demonstrated a simple distributed learning strategy with provable convergence guarantees. Future directions include exploring optimal scheduling and training strategies with energy accumulation. We hope our study to open up further research on energy harvesting for sustainable learning in distributed and mobile networks. Algorithm 1 1 Distributed SGD with Deterministic Energy Arrivalsinput Number of devices N , local dataset Di of device i ∈ [N ],number of iterations T , initial model parameters w (0) . output Model parameters (weights) w (T ) .1: for user i = 1, . . . , N do 2: Initialize U t i = 0 for t ∈ [T ]. 3: for iteration t = 0, . . . , T -1 do Users i = 1, . . . , N : 4: Algorithm 2 2 Distributed SGD with Stochastic Energy Arrivals input Number of devices N , local dataset Di of device i ∈ [N ], Fig. 1 . 1 Fig. 1. Test accuracy of Algorithm 1 compared to the benchmark distributed SGD algorithms for N = 40 users on the CIFAR-10 dataset."
}