{
  "title": "Distributed Stochastic Multi-Task Learning with Graph Regularization",
  "abstract": "We propose methods for distributed graph-based multi-task learning that are based on weighted averaging of messages from other machines. Uniform averaging or diminishing stepsize in these methods would yield consensus (single task) learning. We show how simply skewing the averaging weights or controlling the stepsize allows learning different, but related, tasks on the different machines.",
  "introduction": "Introduction We consider a distributed learning problem in a multi-task setting: each machine i has access to samples from a different data distribution D i , with potentially a different optimal predictor, and thus a different learning task, but where we still assume some similarity between different tasks. The goal of each machine is to find a good predictor for its own task, based on its own local data, as well as communicating with the other machines so as to leverage the similarity to other related tasks. Distributed multi-task learning lies between a homogeneous distributed learning setting (e.g. Shamir and Srebro, 2014) , where all machines have data from the same source distribution, and inhomogeneous consensus problems (e.g. Ram et al., 2010; Boyd et al., 2011; Balcan et al., 2012) , where each machine sees data from a different source, but the goal is to reach a single consensus predictor. In many distributed learning problems, different machines do indeed see different distributions. For example, machines might serve different geographical regions. In a more extreme \"federated learning\" (Konecny et al., 2015) scenario, each machine is a single user device, and its data distribution might reflect e.g. the user's speech, language biases, usage patterns, etc. Such heterogeneity requires departing from a homogeneous model. But if the data distribution on each machine is different, we might as well learn a personalized predictor for each machine, while still leveraging commonalities as in multi-task learning, instead of insisting on consensus. Unlike when seeking consensus, we could learn a predictor entirely locally, ignoring data on other machines. But the premise of multi-task learning is that by communicating with other machines we can improve our predictions, reduce the sample complexity, and hopefully also reduce the computational cost on each machine by distributing the computation. Central to multi-task learning is the notion of relatedness between tasks. In a high-dimensional setting, with large number of variables, we might expect a small common set of predictive variables, where the form of the dependence on variables in this common set varies between tasks (Turlach et al., 2005; Obozinski et al., 2011; Lounici et al., 2011; Wang et al., 2015) . Another approach is to assume that the predictors lie in a shared lower dimensional subspace (Ando and Zhang, 2005; Yuan et al., 2007; Wang et al., 2016) or all have low-norm under some shared linear representation (Amit et al., 2007; Argyriou et al., 2008) . Both the shared sparsity and shared subspaces models have recently been considered in a distributed learning setting (Wang et al., 2015 (Wang et al., , 2016)) , and nuclear-norm regularized multi-task learning has been studied from a distributed optimization perspective (Baytas et al., 2016) . In this paper, we consider graph-based multi-task learning, where relatedness between tasks is specified through a weighted graph over the tasks. Neighboring tasks in the graph are expected to be similar, with a penalty for dis-similarity specified by the weight between them (see precise formulation in Section 2) (Maurer, 2006; Evgeniou et al., 2005) . This also generalized a simpler \"fully connected\" multi-task model where all predictors are close to each other (Evgeniou and Pontil, 2004) . A predictor-homogeneous assumption can also be viewed as an extreme case where all weights go to infinity, forcing all predictors to be identical. In distributed multi-task learning, graph-based relatedness is especially appealing if the relatedness graph also matches the graph of network links between machines, as might be the case, e.g. in a geographical setting or with physical sensors. We therefor emphasize and prefer methods with communication only between neighboring tasks on the graph. In designing methods for graph-based multi-task learning, we are interested in methods that (1) are natural and simple-all our algorithms have a similar and natural structure, involving weighted averaging of messages from neighboring machines and a local gradient or prox calculation; (2) have low communication costs, are sample efficient, and preferably also have low computational cost; and (3) are backed by rigorous guarantees on the amount of communication, samples and computation required. Graph-based multi-task learning has been recently studied by Vanhaesebrouck et al. (2017) and Liu et al. (2017) , both considering the problem as distributed optimization of the multitask regularized empirical objective, similar to our approach in Section 3.2). Vanhaesebrouck et al. suggested an asynchronous gossip-type algorithms and an ADMM procedure, while Liu et al. proposed using SDCA, and also considered learning the relatedness graph itself. Neither provides any statistical analysis, nor analysis of the iteration complexity and communication cost based on the methods. We conduct detailed comparison of convergence properties with these methods in Appendix H, providing upper bounds of their iteration complexities when possible; our methods have faster convergence than the guarantees we could obtain for them. Also, neither directly considers the underlying learning problem (minimizing the actual expected errors), and so neither studies stochastic methods (in the flavor of our Section 4). Here, we show how methods that arise naturally by skewing averaging weights or controlling stepsize of consensus learning methods do yield good guarantees. We also propose stochastic methods which allow reducing the computational cost, and we compare the empirical performance of both our batch and stochastic methods to those of Vanhaesebrouck et al. (2017) and Ma et al. (2015) . Notations In this paper, boldface lower-case letters denote column vectors, boldface capital letters denote matrices, vec(U) is the vectorial form of a matrix U which concatenates columns of U, and U⊗V is the Kronecker product between two matrices U and V. Furthermore, u, v = u v denotes the inner product of two vectors u and v, while U, V = tr U V denotes inner product of two matrices U and V of the same dimensions. We use u = u, u to denote the length of a vector u, U F = vec(U) the Frobenius norm of a matrix U, and U M = tr (UMU ) = UM, U the norm of U with respect to some positive definite matrix M. A function f (x) is Lipschitz if |f (x) -f (y)| ≤ L x -y , ∀x, y. A convex function f (x) is β-smooth and µ-strongly convex if µ 2 x -y 2 ≤ f (x) -f (y) -∇f (y), x -y ≤ β 2 x -y 2 , ∀x, y. This definition extends to functions of matrices, by replacing the vector norm with the Frobenius norm in the above inequality.",
  "body": "Introduction We consider a distributed learning problem in a multi-task setting: each machine i has access to samples from a different data distribution D i , with potentially a different optimal predictor, and thus a different learning task, but where we still assume some similarity between different tasks. The goal of each machine is to find a good predictor for its own task, based on its own local data, as well as communicating with the other machines so as to leverage the similarity to other related tasks. Distributed multi-task learning lies between a homogeneous distributed learning setting (e.g. Shamir and Srebro, 2014) , where all machines have data from the same source distribution, and inhomogeneous consensus problems (e.g. Ram et al., 2010; Boyd et al., 2011; Balcan et al., 2012) , where each machine sees data from a different source, but the goal is to reach a single consensus predictor. In many distributed learning problems, different machines do indeed see different distributions. For example, machines might serve different geographical regions. In a more extreme \"federated learning\" (Konecny et al., 2015) scenario, each machine is a single user device, and its data distribution might reflect e.g. the user's speech, language biases, usage patterns, etc. Such heterogeneity requires departing from a homogeneous model. But if the data distribution on each machine is different, we might as well learn a personalized predictor for each machine, while still leveraging commonalities as in multi-task learning, instead of insisting on consensus. Unlike when seeking consensus, we could learn a predictor entirely locally, ignoring data on other machines. But the premise of multi-task learning is that by communicating with other machines we can improve our predictions, reduce the sample complexity, and hopefully also reduce the computational cost on each machine by distributing the computation. Central to multi-task learning is the notion of relatedness between tasks. In a high-dimensional setting, with large number of variables, we might expect a small common set of predictive variables, where the form of the dependence on variables in this common set varies between tasks (Turlach et al., 2005; Obozinski et al., 2011; Lounici et al., 2011; Wang et al., 2015) . Another approach is to assume that the predictors lie in a shared lower dimensional subspace (Ando and Zhang, 2005; Yuan et al., 2007; Wang et al., 2016) or all have low-norm under some shared linear representation (Amit et al., 2007; Argyriou et al., 2008) . Both the shared sparsity and shared subspaces models have recently been considered in a distributed learning setting (Wang et al., 2015 (Wang et al., , 2016)) , and nuclear-norm regularized multi-task learning has been studied from a distributed optimization perspective (Baytas et al., 2016) . In this paper, we consider graph-based multi-task learning, where relatedness between tasks is specified through a weighted graph over the tasks. Neighboring tasks in the graph are expected to be similar, with a penalty for dis-similarity specified by the weight between them (see precise formulation in Section 2) (Maurer, 2006; Evgeniou et al., 2005) . This also generalized a simpler \"fully connected\" multi-task model where all predictors are close to each other (Evgeniou and Pontil, 2004) . A predictor-homogeneous assumption can also be viewed as an extreme case where all weights go to infinity, forcing all predictors to be identical. In distributed multi-task learning, graph-based relatedness is especially appealing if the relatedness graph also matches the graph of network links between machines, as might be the case, e.g. in a geographical setting or with physical sensors. We therefor emphasize and prefer methods with communication only between neighboring tasks on the graph. In designing methods for graph-based multi-task learning, we are interested in methods that (1) are natural and simple-all our algorithms have a similar and natural structure, involving weighted averaging of messages from neighboring machines and a local gradient or prox calculation; (2) have low communication costs, are sample efficient, and preferably also have low computational cost; and (3) are backed by rigorous guarantees on the amount of communication, samples and computation required. Graph-based multi-task learning has been recently studied by Vanhaesebrouck et al. (2017) and Liu et al. (2017) , both considering the problem as distributed optimization of the multitask regularized empirical objective, similar to our approach in Section 3.2). Vanhaesebrouck et al. suggested an asynchronous gossip-type algorithms and an ADMM procedure, while Liu et al. proposed using SDCA, and also considered learning the relatedness graph itself. Neither provides any statistical analysis, nor analysis of the iteration complexity and communication cost based on the methods. We conduct detailed comparison of convergence properties with these methods in Appendix H, providing upper bounds of their iteration complexities when possible; our methods have faster convergence than the guarantees we could obtain for them. Also, neither directly considers the underlying learning problem (minimizing the actual expected errors), and so neither studies stochastic methods (in the flavor of our Section 4). Here, we show how methods that arise naturally by skewing averaging weights or controlling stepsize of consensus learning methods do yield good guarantees. We also propose stochastic methods which allow reducing the computational cost, and we compare the empirical performance of both our batch and stochastic methods to those of Vanhaesebrouck et al. (2017) and Ma et al. (2015) . Notations In this paper, boldface lower-case letters denote column vectors, boldface capital letters denote matrices, vec(U) is the vectorial form of a matrix U which concatenates columns of U, and U⊗V is the Kronecker product between two matrices U and V. Furthermore, u, v = u v denotes the inner product of two vectors u and v, while U, V = tr U V denotes inner product of two matrices U and V of the same dimensions. We use u = u, u to denote the length of a vector u, U F = vec(U) the Frobenius norm of a matrix U, and U M = tr (UMU ) = UM, U the norm of U with respect to some positive definite matrix M. A function f (x) is Lipschitz if |f (x) -f (y)| ≤ L x -y , ∀x, y. A convex function f (x) is β-smooth and µ-strongly convex if µ 2 x -y 2 ≤ f (x) -f (y) -∇f (y), x -y ≤ β 2 x -y 2 , ∀x, y. This definition extends to functions of matrices, by replacing the vector norm with the Frobenius norm in the above inequality. Graph-based multi-task learning Consider a distributed setting with m machines, where each machine i has access to a data distribution D i and would like to learn a predictor w i ∈ R d for each machines with small expected loss F i (w i ) = E z i ∼D i [ℓ(w i , z i )]. A known weighted graph, with known non-negative weights {a ik }, specifies the relatedness between tasks. Specially, we would like to consider predictor matrices W = [w 1 , w 2 , . . . , w m ] ∈ R d×m from the set Ω = W : w i 2 ≤ B 2 , ∀i = 1, . . . , m, i =k a ik 2 w i -w k 2 ≤ S 2 , i.e., we would like the norm of each individual predictor to be bounded (so that it has low complexity and generalizes well), and the weighted dis-similarities between related predictors to also be small. Taking an agnostic PAC-learning approach, our goal is to minimize the overall population objective F (W) := 1 m m i=1 E z i ∼D i [ℓ(w i , z i )] , (1) and be competitive with respect to predictors in the set Ω. Denoting W * = arg min W∈Ω F (W) the optimal predictor from Ω, and we would like to learn a predictor W with F (W) ≤ F (W * ) + ε. In our analysis, we take the instantaneous loss ℓ(w, z) to be L-Lipschitz continuous, and sometimes also assume it is smooth. In the latter case, we assume machine i's loss ℓ(w i , z i ) is β ismooth in w i , and so the global loss F (W) is β F m -smooth in W with β F = max i=1,...,m β i . Even ignoring the constraint on the similarity between predictors, the sample complexity for each individual task (i.e. the number of samples from D i required to ensure F i (w i ) ≤ F i (w * i ) + ε) is n L = O L 2 B 2 2 . That is, with a total of O mL 2 B 2 2 samples, we can learn W with the desired guarantee F (W) ≤ F (W * ) + ε without any communication between the machines, by, e.g., solving an independent ℓ 2 -regularized ERM problem on each machine. This local approach is the baseline on which any method involving communication between the machines should improve. Graph Laplacian The term i =k a ik 2 w i -w k 2 can be written equivalently using the graph Laplacian. Let A = [a ik ] ∈ R m×m be the adjacency matrix, and L = diag (A1) -A be the corresponding graph Laplacian (L ik = l =i a il if i = k, and L ik = -a ik otherwise), so that i =k a ik 2 w i -w k 2 = i,k L ik w i , w k = tr WLW . The eigenvalues of L will play an important role and we denote them by 0 = λ 1 ≤ • • • ≤ λ m . Regularized ERM One way for learning the predictors is to solve the regularized empirical risk minimization (ERM) problem. Let F i (w i ) = foot_0 n n j=1 ℓ(w i , z ij ) be the local empirical loss of machine i, and let Z = {z ij : i = 1, . . . , m, j = 1, . . . , n} be the sample set. The regularized ERM objective is W = arg min W 1 m m i=1 F i (w i ) F (W) + η 2m m i=1 w i 2 + τ 2m tr WLW R(W) , (2) where η, τ ≥ 0 are regularization parameters. Let W = arg min W F (W) + R(W) be the solution to (2). To understand the statistical property of multi-task learning and facilitate further discussion, we first analyze the generalization error of W. Inspired by Maurer (2006) , who showed essentially the same learning guarantee for the solution of a constrained ERM problem (i.e., arg min W∈Ω F (W)), we provide guarantee for the regularized ERM solution W. Our motivation for studying regularized ERM rather than constrained ERM is that it is easier to solve unconstrained problem using (proximal) gradient methods, and we avoid computing projection onto the constraint set Ω, which is difficult in a distributed setting. 1  While the analysis of Maurer (2006) was based on the Rademacher complexity of Ω (and required the solution to lie in Ω), our proof uses the stability based argument for generalization with strongly convex regularizers (Shalev-Shwartz et al., 2009) . Our analysis also reveals a fundamental connection between single-and multi-task learning: to obtain generalization of a single task in the distributed setting, we only need concentration for the sampling process of that task. In our case, we consider strong convexity w.r.t. the W M -norm where M = I + τ η L. Lemma 1. Assume that the instantaneous loss ℓ(w, z) is L-Lipschitz with respect to w. Then for the ERM solution defined in (2), we have E Z F ( W) -F ( W) ≤ 4L 2 mn m i=1 1 η+τ λ i . Corollary 2. Set η = 2LB 1+m•ρ(B,S) mn B 2 and τ = 2LB 1+m•ρ(B,S) mn S 2 /m in (2), where ρ(B, S) := 1 m m i=2 1 1 + λ i mB 2 /S 2 . Then E Z F ( W) -F (W * ) ≤ 4LB 1+m•ρ(B,S) mn . The quantity ρ(B, S) measures task relatedness and thus the benefit of multi-task learning. It depends on the parameters (B, S) and the graph, but not the data. The value of ρ(B, S) ranges from 0 (when λ i mB 2 S 2 ) to m-1 m ≤ 1 (when λ i mB 2 S 2 ), corresponding to two extreme cases. • When S is small and the graph is connected with high weights, the predictors are encouraged to be similar to each other (we have a consensus problem if S = 0 and the graph is connected), and ρ(B, S) is close to 0. The generalization error is then O LB √ mn , corresponding to that of single task learning using mn samples. • When S is large or the graph is disconnected, tasks are not very related and ρ(B, S) is close to 1. In this case, the generalization error behaves like O LB √ n , and we are essentially performing local learning with n samples for each task. For a fixed number of machines m and graph Laplacian L, to achieve ε excess population error by the above approach, the number of samples used by each machines is n C = O L 2 B 2 (1/m+ρ(B,S)) ε 2 = O ((1/m + ρ(B, S)) • n L ). Therefore, when the tasks are related and ρ(B, S) is small, the sample complexity of multi-task learning is significantly smaller than n L needed by the local approach. To implement the regularized ERM approach in the distributed setting, we could have each machines send n C samples to a central machine, and then minimize the regularized empirical loss on that machine. We refer to this baseline as the centralized approach-it is sample efficient, but expensive in terms of communication and computation. We are interested in distributed multi-task learning algorithms that are also sample efficient, i.e. use only O(n C ) samples on each machine (or at least, not much more then this), but have low computation and communication costs. This can be done either by low-communication distributed optimization of the regularized empirical error (2). Distributed algorithms for ERM In this section, we propose efficient distributed algorithms for minimizing the regularized empirical objective (2). The simplest approach is perhaps to perform gradient descent on F (W ). Interestingly, such updates take the form: w t+1 i = m k=1 µ t+1 ki w t k -α t+1 ∇ F i (w t i ), (3) where α t+1 > 0 is the stepsize at iteration t+1, and the weights for combining neighboring predictors are µ t+1 ki = 1 -α t+1 (η + τ k a ik ) : if i = k, α t+1 τ a ik : otherwise. (4) With an appropriate step-size schedule (or even a fixed stepsize if the loss is smooth), this method converges to W. Furthermore, the updates require only communication along the relatedness graph, since the update for each machines involves only predictors from neighboring machines (with nonzero affinities). This is already a very natural and intuitive method for distributed multitask learning, and we will return to it later. When the loss is smooth, the method can be accelerated using Nesterov's techniques (Nesterov, 2004 , as detailed in Appendix C) without any increase in communication costs nor substantial increase in computation. But first, we suggest two more powerful alternatives. Taking steps based on the gradients amounts to considering, in each iteration, a linearization of the objective, that is of both the empirical loss F (W) and the regularizer R(W). However, in order to obtain a distributable update, it is sufficient to linearize only one of these components while treating the other more explicitly, since each one of them separately can be efficiently optimized in a distributed way: the empirical loss F (W) decomposes over machines, and so can be directly optimized in a distributed way, while R(W) is data independent and could be optimized implicitly based on the common knowledge of the relatedness graph. In the following, we consider two distributed schemes, each based on directly handling one of the components, and each preferable in a different regime depending on the relatedness graph and the structure and cost of communication. Directly solving the regularizer We first consider methods which directly handle the regularization term R(W). To do so, we consider the change of variable U t = W t M 1 foot_1 where M = I + τ η L, we can rewrite the ERM objective as min U F (UM -1 2 ) + η 2m U 2 F . (5) We propose to optimize this objective using gradient descent with respect to U, which reduces to the updates in the W-space: for t = 0, . . . , W t+1 = 1 -α t+1 η W t -α t+1 ∇ F (W t ) • M -1 (6) where α t+1 > 0 is the stepsize at iteration t + 1. In each iteration, machine i performs the following update with µ t+1 ki = α t+1 (M -1 ) ki : w t+1 i = 1 -α t+1 η w t i - m k=1 µ t+1 ki ∇ F k (w t k ). ( 7 ) This update can be implemented in the distributed setting with a broadcast channel: it requires that each machine has access to gradients of all machines, which can be achieved using one round of global, all-to-all communication (not respecting the graph). We could compute M -1 offline ahead of time, and need not re-calculated at each iteration. When the loss is smooth, we can accelerate (7) using Nesterov's techniques without additional communication costs. Setting a constant stepsize 1 α t+1 = β F + η, which is the smoothness parameter of the objective (5) in U 2 , to achieve -suboptimality in (2), the iteration complexity of the accelerated algorithm is O β F +η η log 1 . To achieve ε excess error in the population loss, we set the optimization error = O(ε) and plug in the choice of η from Corollary 2, yielding the iteration complexity O β F B 2 /ε . Directly optimizing the loss The above algorithm requires dense, broadcast communication for solving the proximal step defined by the graph. In a decentralized setting, it is desired to develop algorithms which use only local, peer-to-peer communication. This can be achieved by the updates below, where we linearize the graph regularizer but fully optimize over the loss: W t+1 = arg min W ∇R(W t ), W -W t + 1 2mα t+1 W -W t 2 F + F (W), (8) where α t+1 is the stepsize at iteration t+1. As (8) decouples over machines, machine i independently computes a proximal operation using local data: w t+1 i = arg min u 1 2α t+1 u -(w t i -mα t+1 ∇ w i R(W t )) 2 + F i (u). By the optimality condition of this update, we have w t+1 i = m k=1 µ t+1 ki w t k -α t+1 ∇ F i (w t+1 i ), (9) where the weights for combining neighboring predictors are the same as those in (4). Comparing ( 9 ) with the similar update (3) where we linearized both the regularizer and the loss, we observe that ( 9 ) is also a form of gradient method, with the gradient of loss evaluated at the \"future\" point. The advantage of ( 9 ) is that the gradient ∇R(W) is data-independent and is obtained using only one round of local communication from each machine to its neighbors. Furthermore, the computation decouples over machines, and each machine optimizes the nonlinearized loss without communication. In fact, we need not solve the proximal steps exactly since the (accelerated) proximal gradient method is tolerant to errors in the steps (Schmidt et al., 2011) , and sufficiently accurate solutions can often be obtained in time nearly linear in the number of examples processed using variance-reduced finite-sum methods such as SVRG (Johnson and Zhang, 2013) . Overall, this is a communication-efficient approach in which each machine tries to spend significant amount of time performing local computations on its own data, and to communicate only infrequently. Note that similar proximal type operations also appear in the ADMM algorithm of Vanhaesebrouck et al. (2017) , but the decoupling of tasks is different, because in the local problems of ADMM, each machine optimizes over also a copy of neighboring predictors. We can again accelerate (9) using Nesterov's techniques, and set 1 mα t+1 = β R = η+τ λm m , which is the smoothness parameter of R(W) in W. Then, to achieve ε excess error in the population objective, the number of iterations needed by the accelerated algorithm is O β R η/m = O λmmB 2 S 2 , using the choice of η and τ from Corollary 2. We also show that this algorithm is tolerant to delay and analyze its convergence under bounded delay in Appendix G. Stochastic algorithms In ERM, we collect training samples on each machine ahead of time, and solve a fixed optimization problem defined by them. But in real-world scenarios, we might have access to virtually unlimited data, or a constantly available stream of examples. In this case, it might be statistically wasteful to reuse examples over iterations. Or, even if we do have a finite amount of data, as we shall see, Table 1: Algorithms for distributed stochastic multi-task learning with graph regularization. Here ε is the excess error in the population objective ; n C = O L 2 B 2 •(1/m+ρ(B,S)) ε 2 and n L = O L 2 B 2 2 ; |E| denotes the number of edges in the graph. For simplicity, schematic updates ignores acceleration, but the rates are given for the accelerated algorithms. Each cell shall be interpreted as O(•) which hides poly-logarithmic dependencies. Algorithms Communication rounds Vectors (∈ R d ) communicated per machine Sample complexity per machine Total Samples processed per machine local 0 0 n L = n C 1 m +ρ(B,S) n L centralized n C n C = n L • ( 1 m + ρ(B, S)) m • n C ERM: directly solving regularizer 1. g t+1 i = k µ t+1 ki ∇ F k (w t k ) where µ t+1 ki = α t+1 (M -1 ) ki 2. w t+1 i = w t i -g t+1 i B 2 ε m • B 2 ε n C n C • B 2 ε = n C • 4 √ n L ERM: directly optimizing loss 1. w t i = k µ t+1 ki w t k where µ t+1 ki = (I -α t+1 ηM) ki 2. w t+1 i = w t i -α t+1 ∇ F t+1 i (w t+1 i ) λmmB 2 S 2 |E| m • λmmB 2 S 2 n C n C • λmmB 2 S 2 Stochastic: directly solving regularizer Algorithm 2, b = O n C ε B 2 1. g t+1 i = k µ t+1 ki ∇ F t+1 k (w t k ) where µ t+1 ki = α t+1 (M -1 ) ki 2. w t+1 i = w t i -g t+1 i B 2 ε m • B 2 ε n C n C Stochastic: directly optimizing loss 1. w t i = k µ t+1 ki w t k where µ t+1 ki = (I -α t+1 ηM) ki 2. w t+1 i = w t i -α t+1 ∇ F t+1 i (w t+1 i ) |E| m per iteration n S , probably ∈ (n C , n L ) n S we can get the same communication and statistical guarantee while processing only a minibatch at a time, thus significantly reducing computational cost. We consider stochastic variants of the approaches in Section 3 to directly optimize the population loss F (W), using fresh samples in each update. Directly solving the regularizer Analogous to (7), we could perform minibatch SGD with b samples per machine to approximate the gradient of the population loss: for t = 0, . . . , w t+1 i = w t i - m k=1 µ t+1 ki ∇ F t+1 k (w t k ). ( 10 ) where We can accelerate (10) using the accelerated stochastic approximation (AC-SA) algorithm of Lan (2012) . We provide the detailed accelerated algorithm in both the U-space and W-space in Algorithm 2 (Appendix D). We have the following guarantee after running it for T iterations. F t+1 k (w t k ) = 1 b b j=1 ℓ(w t k , z t+1 Theorem 3. Set the initialization W 0 = 0 and stepsizes θ t+1 = t+1 2 , α t+1 = t+1 2 min m 2β F , √ 12mB 2 (T +2) 3 2 σ in Algorithm 2. Then E F (W T ag ) -F (W * ) ≤ O σ √ mB 2 √ bT + β F B 2 T 2 . Sample complexity T * = n b * = O β F B 2 ε(m,n) , also matching that of ERM. However, since each stochastic gradient uses only b = o(n) samples, the local computation ∇ F t+1 (W t ) is significantly reduced. Directly optimizing the loss Analogous to (8), we can use the stochastic algorithm where at iteration t + 1, machine i computes w t+1 i = arg min u 1 2α t+1 u -w t i -mα t+1 ∇ w i R(W t ) 2 + 1 b b j=1 ℓ(u, z t+1 ij ). ( 11 ) For b = n, it has the same per iteration computation cost as the ERM counterpart (both process n samples in each iteration). But, intuitively, it would outperform the ERM algorithm for the same number of iterations/communications because it uses more fresh samples. We can prove the convergence of this algorithm, but do not have a satisfactory analysis showing it is sample efficient. We conjecture that its sample complexity per machine, denoted by n S , is in the range (n C , n L ). We implemented the accelerated version of this simple algorithm and this conjecture seems to be supported by our experiments. In Appendix E, we provide a more complicated algorithm based on the minibatch-prox algorithm of Wang et al. (2017) , that is sample efficient and trade off communication and memory costs. Comparison of the different approaches Table 1 summarizes the communication and computation complexities of the proposed algorithms. Some of our methods require solving local regularized-ERM type problems on each machine. We do not analyze the precise complexity and required accuracy of such local computation, but keep track of the number of samples processed on each machine, i.e. sum of the sizes of the subproblems over the iterations, as the proxy for computational complexity. We emphasize that, despite the simplicity of our ERM methods, their have faster convergence than what we could obtain for previous methods; see detailed discussions in Appendix H. Our stochastic algorithms mirror the ERM algorithms in terms of updates, but can be computationally much more efficient. Connection to consensus learning The iterations we consider all involve taking a weighted average of messages (iterates or gradients) from other machines and a local gradient or prox computation. These same type of iterates have also been suggested and studied as methods for solving the consensus problem-that is, finding a single consensus predictor w that is good for all machines and minimizes F (W) = 1 m m i=1 F i (w i ). But the consensus problem is fundamentally different from our \"pluralistic\" multi-task problem, with a different optimum. In this section we will understand what makes the same form of updates, namely updates of the form (3), ( 7 ), ( 9 ) or their stochastic variants, converge to either the consensus solution or to the pluralistic multi-task solution. In particular, we show how consensus methods are obtained as special cases of these updates, or as limits of the multi-task approach. Averaging gradients Let us begin with the update of the form (7) or its stochastic variant (10), where we take a weighted average of gradients from other machines. When the averaging weights are uniform, i.e. µ t ki = α t /m for all i, k, and as long as all machines start from the same initialization (e.g. w t i = 0), the iterates will continue to be identical across machines throughout optimization (i.e. we will have w t i = w t j for all i, j, t), thus maintaining consensus. Furthermore, the update (7) then boils down to precisely gradient descent on the empirical consensus objective F (W) + η 2m W 2 F , while the stochastic variant ( 7 ) is precisely a mini-batch stochastic gradient descent update on the consensus objective, with a mini-batch consisting of the union of the samples used across machines. Indeed, mini-batch SGD is a common approach for solving the distributed consensus problem, or for distributed learning in a homogeneous setting (where we assume the same distribution across machines, or at least the same good predictor). What we saw in Section 3, is that by changing to non-uniform weights, given by µ ∝ M -1 , we can allow pluralism and converge to the multi-task solution. We can furthermore observe how uniform weights (and therefor gradient descent/mini-batch SGD on the consensus problem) are obtained as a limit of the multi-task weights µ ∝ M -1 . If the graph is connected, λ 1 = 0 is the only zero eigenvalue of the Laplacian L with an associated eigenvector of u = [1, . . . , 1] (if the graph is not connected, we cannot expect consensus, as each connected component will behave independently). Therefor M -1 = (I + τ η L) -1 has a leading eigenvalue of 1 of multiplicity one, associated with the eigenvector u. As S → 0 and so τ → ∞, that is we are demanding increasing similarity between machines, the leading eigenvalue of M -1 remains 1 while all other eigenvalues go to zero, implying that M -1 → 1 m uu and so µ t ki = α t M -1 ki → α t /m. That is, as we demand increasing similarity between machines, and thus converge to a consensus situation, the updates converge to standard consensus gradient descent or mini-batch SGD updates. Averaging iterates Let us now turn to updates of the form (3), the related prox updates (9), and their stochastic variants. Nedić and Ozdaglar (2009) proposed updates precisely of the form (3) as a decentralized procedure for the consensus problem. They showed that when the averaging weights µ t ki are doubly stochastic and do not vary between iterations (i.e. µ t ki = µ ki , ∀ k i µ ki = 1 and ∀ j k µ ki = 1), and the stepsize on the gradient goes to zero, i.e. α t t→∞ ---→ 0, the updates (3) converge to the consensus solution. In our case, the averaging weights, as defined in (4), deviate from double-stochasticity, since k µ t ki = 1α t η. Furthermore, and possibly more significantly, to obtain our convergence guarantees for smooth loss, we do not take α t to zero. Even if we were to use diminishing stepsizes in our derivations, we would have α t → 0, but in that case the averaging weights would not be fixed over iterations (as is the case in consensus optimization) and we would have µ t → I. To see how consensus updates are obtained as a limiting case of our multi-task setting, we again consider a connected graph and study what happens as S → 0 and so τ → ∞, while B and therefor η remain fixed. This corresponds to a fixed amount of local regularization, and increasing expectation that neighboring nodes are similar. Under this scaling, we would indeed have α = 1/(η + τ λ m ) → 0, where λ m > 0 since the graph is connected. Furthermore, we have that αη → 0 while ατ → 1/λ m > 0. Plugging this scaling into the multi-task averaging weights (4), we obtain the doubly stochastic weights: µ t ki → 1 -1 λm k a ik : if i = k, 1 λm a ik : otherwise. ( 12 ) To summarize, a significant differentiation between consensus and multi-task learning is therefor in whether α t diminishes relative to (µ t -I). When our relatedness constraints approach consensus, α t can diminish while µ t is non-trivial and doubly stochastic. In fact, in studying consensus optimization, Yuan et al. (2016) recently noted that when α t does not diminish, the methods does not converge to the consensus solution but only to a neighborhood of it. In light of our analysis, we now understand that this \"neighborhood\" corresponds to the multi-task learning solution, which indeed becomes increasingly similar to the consensus solution as S → 0. Connection to the decentralized algorithm of Scaman et al. (2017) When the graph is connected, the consensus constraint w 1 = • • • = w m can be equivalently written as W √ L = 0, since the null space of L contains only vectors of constants. Then the multi-task formulation (2) is a relaxation of min W √ L=0 1 m m i=1 F i (w i ) + η 2m m i=1 w i 2 (13) with the quadratic term τ 2 tr WLW penalizing the constraint violation. The quadratic penalty τ 2 tr WLW may lead to a large condition number for our algorithm (8) as τ → ∞. Recently, Scaman et al. (2017) proposed an algorithm with optimal iteration/communication complexities for decentralized consensus learning, which performs accelerated gradient descent on the dual problem of ( 13 ), with updates (before acceleration): W t+1 = arg max W V t , W -F (W), V t+1 = V t -αW t+1 L, (14) where V 0 = 0 and α > 0 is the stepsize. It can be seen that their algorithm consists of the same type of basic operations (weighted local average of predictors, and solutions of local subproblems involving non-linearized loss) as ours. As noted by the authors, this is a form of distributed augmented Lagrangian method without the quadratic penalty. Experiments We examine different graph-based multi-task learning methods on the task of least squares regression using synthetic data. More details of the experiments (including data generation and more results) are given in Appendix I. The tasks are grouped into C clusters and the true predictors within the same cluster are generated from the same Gaussian distribution, thus smaller C implies higher task relatedness. We have input dimension d = 100, number of tasks m = 100, training set size n = 500, and vary number of task clusters C over {1, 5, 10, 50}. We also generate a dev set of 10000 samples per task for tuning hyper-parameters, and test set of 10000 samples per task for approximately evaluating the population loss. The affinity graph A ∈ R 100×100 is a (connected) 10-nearest neighbor graph with binary weights built on the true predictors. The methods compared here are: Local, which solves a local ERM problem (with ℓ 2 -regularization) with n samples for each task; Centralized, which solves the regularized ERM problem (2) with n samples for each task; ADMM, which is the synchronized version of the algorithm of Vanhaesebrouck et al. (2017) ; SDCA, which is the algorithm used by Liu et al. (2017) for fixed graph; our algorithms are denoted as B/S (batch/stochastic) + SR/OL (solve regularizer/optimize loss). Empirical risk minimization We fist compare the iterative methods on the regularized ERM problem (2), to which the analysis for ADMM and SDCA applies. We tune the ℓ 2 regularization parameter for Local and (η, τ ) for Centralized, and then fix the optimal (η, τ ) for other methods. We also tune the quadratic penalty parameter for ADMM, the task separability and stepsize parameters for SDCA, and stepsize parameter for BSR/BOL (although the default value based on the smoothness parameter already works well for them). For SSR/SOL, we draw random samples from the fixed training set (with size n), and simply fix the minibatch size to be n/10. Figure 1 (left panel) shows for each method the estimated F (W) over iterations (or rounds of communication) in the top row, and over the amount of computation (measured by the number of passes over the training set) in the bottom row. Observe that all iterative algorithms converge to the same ERM solution, our algorithms tend to consistently outperform ADMM and SDCA. Stochastic optimization We next demonstrate the efficiency of true stochastic algorithms (using fresh samples for each update) at C = 10. We allow the algorithms to process a total of 10000 fresh samples on each machine, and vary the minibatch size b over {40, 80, 100, 200, 500}. The parameters (η, τ ) are fixed to those used in the ERM experiments. Figure 1 (right panel) shows for each method the estimated F (W) over iterations (or rounds of communication) in the left plot, and over the amount of fresh samples processed (or total computation cost) in the right plot. As a reference, the error of Local and Centralized (using n = 500 samples per machine) are also given in the plots. We observe that with fresh samples, stochastic algorithms are competitive to ERM algorithms in terms of sample complexity, while being computationally more efficient. A Proof of Lemma 1 Recall that the ERM problem is defined as W = arg min W F (W) + R(W) := 1 m m i=1 F i (w i ) + η 2m m i=1 w i 2 + τ 2m tr WLW , where η, τ ≥ 0 are regularization parameters, Z = {z ij : i = 1, . . . , m, j = 1, . . . , n} is the sample set. And recall that λ i , i = 1, . . . , m are the eigenvalues of L. Assume that the instantaneous loss ℓ(w, z) is L-Lipschitz in w. We would like to show that E Z F ( W) -F ( W) ≤ 4L 2 mn m i=1 1 η + τ λ i . Proof. In the following, we define M = I + τ η L which is positive definite. Furthermore, perform the following change of variables U = WM 1 2 , ⇔ w i = (UM -1 2 ) • e i where e i is the i-th standard basis in R m . We can then rewrite the losses using the new variables: 1 m ℓ(w i , z i ) = 1 m ℓ(UM -1 2 e i , z i ) =: h i (U, z i ), for i = 1, . . . , m, and the empirical objective as min U∈R d×m H(U) := 1 n n j=1 h 1 (U, z 1j ) +   m i=2 1 n n j=1 h i (U, z ij ) + η 2m U 2 F   . ( 15 ) We can view (15) as performing ERM in the space of U, using the instantaneous loss h 1 (U, z 1 ) with n independent samples {z 1j } j=1,...,n , and using the term in bracket as the z 1 -independent regularizer. Recall that the ERM solution to an objective with Lipschitz loss and strongly convex regularizer is stable. Obviously, the regularization term in ( 15 ) is η m -strongly convex in U. We now bound the Lipschitz constant of h 1 (U, z 1 ) in U. Observe that ∇ U h 1 (U, z 1 ) = 1 m ∇ w i ℓ(w 1 , z 1 ) • e 1 M -1 2 , and as a result the Lipschitz constant is bounded by ∇ U h 1 (U, z 1 ) F = 1 m tr ∇ w 1 ℓ(w 1 , z 1 ) • e 1 M -1 e 1 • ∇ w 1 ℓ(w 1 , z 1 ) ≤ L (M -1 ) 11 m where we have used the L-Lipschitz continuity of ℓ(w 1 , z 1 ) which implies ∇ w 1 ℓ(w 1 , z 1 ) ≤ L. According to Shalev-Shwartz et al. (2009) [Theorem 6], for any fixed {z ij } i=2,...,m j=1,...,n , it holds for the ERM solution U = arg min U H(U) = WM 1 2 that E {z 1j }   E z 1 [h 1 ( U, z 1 )] - 1 n n j=1 h 1 ( U, z 1j )   ≤ 4 L (M -1 ) 11 m 2 (ηn/m) = 4L 2 (M -1 ) 11 ηmn . Translating this in terms of the original variables, we have ∀ {z ij } i=2,...,m j=1,...,n , E {z 1j } F 1 ( W) -F 1 ( W) ≤ 4L 2 (M -1 ) 11 ηn where F 1 (W) = E z 1 [ℓ(w 1 , z 1 )] and F 1 (W) = 1 n n j=1 ℓ(w 1 , z 1j ) . By the convexity of |•| and the Jensen's inequality, this implies E Z F 1 ( W) -F 1 ( W) = E {z ij } i=2,...,m j=1,...,n E {z 1j } F 1 ( W) -F 1 ( W) ≤ E {z ij } i=2,...,m j=1,...,n E {z 1j } F 1 ( W) -F 1 ( W) ≤ 4L 2 (M -1 ) 11 ηn . This result shows that, to obtain generalization for a single task, we only need concentration for the sampling process of that task. By the same argument, we obtain similar inequalities regarding stability for losses on each machine. Finally, we have by the triangle inequality that E Z F ( W) -F ( W) ≤ 1 m m i=1 E Z F i ( W) -F i ( W) ≤ 1 m m i=1 4L 2 (M -1 ) ii ηn = 4L 2 tr M -1 ηmn = 4L 2 m i=1 1 1+τ λ i /η ηmn which is what we set out to prove. B Proof of Lemma 2 Based on Lemma 1, we now show that by properly setting the regularization parameters in the regularized ERM problem (2), i.e., η = 2LB 1+m•ρ(B,S) mn B 2 and τ = 2LB 1+m•ρ(B,S) mn S 2 /m , we have that E Z F ( W) -F (W * ) ≤ 4LB 1 + m • ρ(B, S) mn . where ρ(B, S) := 1 m m i=2 1 1+λ i mB 2 /S 2 . Proof. Observe that E Z F ( W) ≤ E Z F ( W) + 4L 2 mn m i=1 1 η + τ λ i ≤ E Z F ( W) + R( W) + 4L 2 mn m i=1 1 η + τ λ i ≤ E Z F (W * ) + R(W * ) + 4L 2 mn m i=1 1 η + τ λ i = F (W * ) + R(W * ) + 4L 2 mn m i=1 1 η + τ λ i where we have used Lemma 1 in the first inequality, and that W is the empiric risk minimizer in the third inequality. Since W * ∈ Ω, we can bound the excess error as ε(m, n) = E Z F ( W) -F (W * ) ≤ R(W * ) + 4L 2 mn m i=1 1 η + τ λ i ≤ 1 2 ηB 2 + 1 2m τ S 2 + 4L 2 mn m i=1 1 η + τ λ i . ( 16 ) Now, set η = B 2 and τ = m S 2 for some that will be specified later. Continuing from ( 16 ) yields ε(m, n) ≤ + 4L 2 mn m i=1 1 B 2 + m S 2 λ i = + 1 • 4L 2 B 2 n • 1 m m i=1 1 1 + λ i mB 2 /S 2 ≤ + 1 • 4L 2 B 2 mn + 4L 2 B 2 n • 1 m m i=2 1 1 + λ i mB 2 /S 2 ≤ + 1 • 4L 2 B 2 mn + 4L 2 B 2 n • ρ(B, S) . Minimizing the RHS over gives = 2LB 1 mn + ρ(B,S) n , and ε(m, n) ≤ 4LB 1 mn + ρ(B, S) n . C The accelerated proximal gradient algorithm We provide the accelerated proximal gradient algorithms in Algorithm 1, which are used to accelerate our ERM algorithms in the main text. The proximal operator is defined as prox β h (x) = arg min y β 2 yx 2 + h(y) where β > 0 and h(x) is convex and possibly non-smooth. Algorithm 1 ProxGrad(g,h,β,µ): Accelerated proximal gradient descent. Input: Objective has the form f (w) = g(w) + h(w), where g(w) is β-smooth and µ-strongly convex, and h(w ) is convex. Initialize w 0 , y 1 ← w 0 for t = 1, . . . , T do w t ← prox β h y t -1 β ∇g(y t ) , y t+1 ← w t + √ β- √ µ √ β+ √ µ w tw t-1 end for Output: w T is the approximate solution. D Analysis of stochastic optimization by directly solving the regularizer In each iteration of this algorithm, we draw b samples per machine to approximate the gradient of the population loss and perform minibatch SGD, which amounts to linearizing the loss on a minibatch. The key to being sample efficient is to respect the geometry imposed by the graph Laplacian. As in Section 3.1, define the change of variable U t = W t M 1 2 where M = I + mB 2 S 2 L. Our population objective is F (W) = F (UM -1 2 ), and the predictor U * = W * M 1 2 satisfies the constraint that U * 2 F = tr W * I + mB 2 S 2 L (W * ) ≤ 2mB 2 . We can perform minibatch SGD in the Uspace: U t+1 = arg min U α t+1 ∇ F t+1 (U t M -1 2 ) • M -1 2 , U -U t + 1 2 U -U t 2 F , for t = 0, . . . , where F t+1 (W t ) = 1 mb m i=1 b j=1 ℓ(w t i , z t+1 ij ) and z t+1 ij j=1,...,b are b samples drawn by machine i at iteration t + 1, and α t+1 > 0 is a stepsize parameter. In the W-space, the above update reduces to W t+1 = W t -α t+1 ∇ F t+1 (W t ) • M -1 , Clearly, this update requires inverting the graph Laplacian. We can further accelerate this method using the accelerated stochastic approximation (AC-SA) algorithm of Lan (2012) . We give the detailed stochastic algorithm by directly solving the regularizer (with linearized loss) in Algorithm 2. Algorithm 2 Accelerated minibatch SGD. This algorithm maintains three iterate sequences: U t is the sequence of prox centers, U t md is the \"middle\" sequence with which we evaluate the stochastic gradient and build models (approximations) of the objective, and U t ag is the \"aggregated\" sequence with which we evaluate the objective values. Input: The stepsize sequences θ t+1 and α t+1 for t = 0, . . . . Initialize W 0 ← 0, W 0 ag ← W 0 U 0 ← 0, U 0 ag ← U 0 for t = 0, . . . , T -1 do W t md ← θ t+1 -1 W t + (1 -θ t+1 -1 )W t ag U t md ← θ t+1 -1 U t + (1 -θ t+1 -1 )U t ag W t+1 ← W t -α t+1 ∇ F t+1 (W t md ) • M -1 U t+1 ← U t -α t+1 ∇ F t+1 (U t md M -1 2 ) • M -1 2 W t+1 ag ← θ t+1 -1 W t+1 + (1 -θ t+1 -1 )W t ag U t+1 ag ← θ t+1 -1 U t+1 + (1 -θ t+1 -1 )U t ag end for Output: W T ag (or equivalently U T ag ) is the approximate solution. The key quantity for analyzing the convergence property of minibatch SGD is the variance of stochastic gradients in the U-space, which we now derive. We can view ξ = (z 1 , . . . , z m ) as the combined sample, ℓ multi (W, ξ) = 1 m m i=1 ℓ(w i , z i ) as the averaged instantaneous loss, so that F t+1 (W) = 1 b b j=1 ℓ multi (W, ξ t+1 j ) approximates E ξ [ℓ multi (W, ξ)] with b combined samples. The lemma below bounds the variance of stochastic gradient estimated with one combined sample. Lemma 4. The variance of stochastic gradient in the U-space is bounded: E ξ ∇ℓ multi UM -1 2 , ξ • M -1 2 -E ξ ∇ℓ multi UM -1 2 , ξ • M -1 2 2 F ≤ σ 2 where σ 2 := 4L 2 m 2 (1 + m • ρ(B, S)). Proof. By direct calculation, we have E ξ ∇ℓ multi UM -1 2 , ξ • M -1 2 -E ξ ∇ℓ multi UM -1 2 , ξ • M -1 2 2 F = 1 m 2 E ξ [∇ w 1 ℓ(w 1 , z 1 ) -E z i [∇ w 1 ℓ(w 1 , z 1 )] , . . . , ∇ wm ℓ(w m , z m ) -E zm [∇ wm ℓ(w m , z m )]] 2 M -1 = 1 m 2 i,k E z i ,z k ∇ w i ℓ(w i , z i ) -E z i [∇ w i ℓ(w i , z i )], ∇ w k ℓ(w k , z k ) -E z k [∇ w k ℓ(w k , z k )] • (M -1 ) ik = 1 m 2 m i=1 ∇ w i ℓ(w i , z i ) -E z i [∇ w i ℓ(w i , z i )] 2 • (M -1 ) ii (17) ≤ 4L 2 m 2 tr M -1 = 4L 2 m 2 m i=1 1 1 + λ i mB 2 /S 2 = 4L 2 m 2 (1 + m • ρ(B, S)) = σ 2 where we have used the independence between z i and z k for i = k so that the cross terms vanishes in (17), and the triangle inequality and that ∇ w i ℓ(w i , z i ) ≤ L in the inequality. Averaging the b independent stochastic gradients on a minibatch reduces the gradient variance to σ 2 /b (see, e.g., Dekel et al., 2012, eqn 7) . Note that β F m is the smoothness parameter of F (UM -1 2 ) w.r.t. U, and the distance generating function 1 2 U 2 F is 1-strongly convex w.r.t. the U F -norm. Plugging these problem parameters into (Lan, 2012)(Corollary 1) yields Theorem 3. E A sample-efficient stochastic algorithm by directly optimizing the loss The key to sample efficiency in the stochastic setting is to couple the individual learning tasks with the graph, and respect the geometry of the U-space (e.g., in deriving the generalization performance in Lemma 1, we rely on strong convexity in the norm U F ). This motivates us to derive a sample-efficient stochastic algorithm based on the minibatch-prox method (Wang et al., 2017) . The minibatch-prox method solves a subproblem involving nonlinearized loss on a minibatch in each iteration, and was shown to have the optimal sample complexity for stochastic convex optimization regardless of the minibatch size (recall from Section 4.1 that mnibatch SGD achieves the optimal sample complexity only for small enough minibatch size), and it was the basis for developing communication-and memory-efficient algorithm for distributed stochastic consensus learning in Wang et al. (2017) . We detail the minibatch-prox based algorithm in Algorithm 3, which consists of two nested loops. In the outer loop, we perform minibatch-prox in the space of U; in each iteration of the outer loop we use b samples per machines to approximate the nonlinearized loss, and approximately solves a subproblem involving the full Laplacian in the W-space. The solutions to the subproblems (which is then a small ERM problem with fixed samples) are computed approximately by the inner loops, where we perform acclerated gradient descent in the space of W. Algorithm 3 Distributed minibatch prox. Initialize W 0 ← 0 for t = 0, . . . , T -1 do Approximately solve W t+1 ≈ W t+1 = arg min W γ 2 tr (W -W t )M(W -W t ) + F t+1 (W) to ζ t+1 -suboptimality using the accelerated proximal gradient algorithm ProxGrad(γ tr (W -W t )M(W -W t ) , F t+1 (W), γ(1 + mB 2 S 2 λ m ), γ) end for Output: W = 1 T T t=1 W t is the approximate solution. The minibatch-prox algorithm for minimizing F (UM -1 2 ) works as follows: U t+1 ≈ U = arg min U γ 2 U -U t 2 F + F t+1 (UM -1 2 ), for t = 0, . . . , (18) where in each iteration we draw b fresh samples per machine to approximate F (W) by F t+1 (W) = 1 mb m i=1 b j=1 ℓ(w i , z t+1 ib ) . Note that we allow inexact solutions to the objective in (18). The corresponding update of (18) in the W-space is W t+1 ≈ W t+1 = arg min W f t+1 (W) where f t+1 (W) = γ 2 tr (W -W t )M(W -W t ) + F t+1 (W). ( 19 ) We provide the learning guarantee of the minibatch-prox algorithm in the following theorem. Theorem 5. Suppose that we initialize Algorithm 3 with W = 0 and set γ = 2 T b • L √ 1+m•ρ(B,S) m 3 2 B . Assume that for all t ≥ 0, the error in minimizing (19) satisfies f t+1 (W t+1 ) -min W f t+1 (W) ≤ ζ t+1 = min T b 1 2 , T b 3 2 • LB(1 + m • ρ(B, S)) 3 2 m 5 2 t 3 . Then for W T = 1 T T t=1 W t , we have E F (W T ) -F (W * ) = O LB √ 1+m•ρ(B,S) √ mbT . Proof. Let L U = L √ tr(M -1 ) m where tr M -1 = 1 + m • ρ(B, S) . By an analysis similar to that of Lemma 1 (and essentially due to f t+1 (W)'s γ-strong convexity w.r.t. the norm • M ), we obtain the \"stability\" of the exact minimizer to (19 ), i.e., E[ F t+1 ( W t+1 ) -F ( W t+1 )] ≤ 4L 2 tr(M -1 ) γm 2 b = 4L 2 U γb . Furthermore, if the suboptimality of W t+1 satisfies f t+1 (W t+1 ) -f t+1 ( W t+1 ) ≤ ζ t+1 , by the γ-strong convexity of f t+1 (W) w.r.t. the Euclidean norm, we have w t+1 i -w t+1 i ≤ 2ζ t+1 γ , for i = 1, . . . , m, and consequently by the Lipschitz continuity of the loss, we have F t+1 (W t+1 ) -F ( W t+1 ) ≤ 2L 2 ζ t+1 γ = 2L 2 U γ • m 2 ζ t+1 tr (M -1 ) . This reconstructs the essential lemma required by the minibatch-prox analysis (Wang et al., 2017, Lemma 2) . We can then invoke the learning guarantee of minibatch-prox (Wang et al., 2017, Theorem 7) , by using our L U in place of their L, and our m 2 ζ t+1 tr(M -1 ) in place of their η t . In the end, we have E F (W T ) -F (W * ) ≤ O LB tr (M -1 ) √ mbT = O LB 1 + m • ρ(B, S) √ mbT . For fixed n = bT , minibatch-prox attains the generalization error O LB 1+m•ρ(B,S) mn for any minibatch size b. Though the error in solving each subproblem ( 19 ) seems stringent as it decreases over iterations, we can apply the linearly convergent accelerated proximal gradient method in the inner loops to the subproblems. For any minibatch size b, the number of outer iterations is T = n b , and the number of inner iterations for each outer iteration (the initial error for the subproblems are bounded with a warm-start, see Appendix F) is O λmmB 2 S 2 , so the total number of communication rounds is the multiplication O n b • λmmB 2 S 2 . This algorithm allows us to trade off communication and memory: We could use small number of samples b in each outer iteration (limited by the local memory), but the total number communication rounds increase with 1 b . The most communication-efficient setting is b = n, in which case we are essentially solving one ERM problem with mn samples (by linearzing the regularizer). Finally, we note that each update of the simple algorithm (11) (without the outer+inner loop structure) and a single inner iteration of the minibatch-prox subproblem (19) have the same communication/computation costs. F Warm start when directly optimizing the loss Lemma 6. Consider the objective of the proximal operator min y f (y) = β 2 y -x 2 + h(y). where h(y) is L-Lipschitz, and let x * = arg min y f (y). Then we have x * -x ≤ L/β, and the suboptimality of x is bounded f (x) -f (x * ) ≤ L 2 /β. Proof. By the first-order optimality of x * , we have 0 = β(x * -x) + ∇h(x * ) where ∇h(x * ) is a subgradient of h at x * . By the assumption that h(y) is L-Lipschitz, we have ∇h(x * ) ≤ L and consequently x *x = ∇h(x * ) /β ≤ L/β. For the suboptimality of x, it follows again from the Lipschitz continuity of h that f (x) -f (x * ) = 0 + h(x) - β 2 x * -x 2 -h(x * ) ≤ h(x) -h(x * ) ≤ L x -x * ≤ L 2 /β. This lemma indicates that for solving the local objectives when directly optimizing the loss, e.g., (8), we can initialize from W t -1 β ∇R(W t ) which mixes the local predictor with those of the neighbors, and the initial suboptimality of this warm start is bounded by O L 2 β F . A similar result holds when the distance term is defined by other non-Euclidean norms. For example, in Section 4.1, we need to solve subproblems of the form (19), where the distance in the W-space is defined by the W M -norm. By an analysis similar to that of Lemma 6 and noting that M -1 ≤ 1, we obtain the distance between w t i and the optimal solution w t+1 i is at most L/γ. As a result, the suboptimality of solving (19) when initialized from W t is at most L 2 /γ. G Directly optimizing the loss with bounded delays When directly optimizing the loss (while linearizing the regularizer), consider the case where the synchronization step is not perfect. Instead of waiting for neighboring machines to finish their local proximal step and sending in their new weight parameters, each machine can use the stale parameters for neighboring machines. Can we still solve the original ERM problem in this case? Consider the iteration t + 1 on machine i (with delays, t is now considered a local iteration counter). Let the set of neighboring machines be N i . Due to delay in communication, we have a noisy gradient ∇ i R(W t ) = 1 m ηw t i + τ k a ik (w t i -w t-d ik (t) k ) , i = 1, . . . , m. Here d ik (t) ∈ [0, Γ] is the delay of machine k relative to machine i (at iteration t + 1): Machine i is using the weight of machine k from d ik (t) steps ago. In this section, we allow the delay to vary over time, as long as it is upper bounded by Γ. Based on this noisy gradient, machine i computes the following proximal gradient step w t+1 i = prox β F i m w t i - 1 β ∇ i R(W t ) (20) with some stepsize β > 0. We need to analyze the convergence of the proximal gradient method with errors in the gradient, as done by Schmidt et al. (2011) . The difference from their work is that the error in our gradients comes from delay (stale weight parameters). Comparing with the case without delay, we have the \"error\" in the local gradient: ∇ i R(W t ) -∇ i R(W t ) = τ m k a ik (w t k -w t-d ik (t) k ). From iteration td ik (t) to iteration t, the k-th machine has performed d ik (t) gradient proximal operations. The intuition is that, by the non-expansiveness of the proximal operator, the error in gradient would not cause too much error in the iterates, and then by the smoothness of the objective, this would in turn only results in small error in gradient of the next step. It is important to note that, all machines are influenced by each other and the local errors are propagated to the entire graph. Based on the non-expansive property of the proximal operator and the additional assumption of the adjacency matrix being doubly-stochastic, it is straightforward to show the following convergence guarantee for the (non-accelerated) proximal gradient algorithm. The algorithm converges at a slower linear rate than without delays. Theorem 7. Assume that the affinity matrix A is doubly-stochastic, i.e., k∈N i a ik = 1 for all i, and the delay in the update rule (20) has delay bounded by Γ. Set the inverse stepsize β = η+τ m . Then after t ≥ 1 iterations of the algorithm, we have max i=1,...,m w t i -w i ≤ 1 - η η + τ t 1+Γ • max i=1,...,m w 0 i -w i . Proof. Since W is the optimal solution to the ERM problem, we have that w i = prox β F i m w i - 1 β ∇ i R( W) , i = 1, . . . , m. Then, by the non-expansiveness of the proximal operator, we obtain w t+1 i -w i = prox β F i m w t i - 1 β ∇ i R(W t ) -prox β F i m w i - 1 β ∇ i R( W) ≤ w t i - 1 β ∇ i R(W t ) -w i - 1 β ∇ i R( W) = 1 - η + τ k∈N i a ik βm (w t i -w i ) + k∈N i τ a ik βm (w t-d ik (t) k -w k ) ≤ 1 - η + τ k∈N i a ik βm w t i -w i + τ βm k∈N i a ik w t-d ik (t) k -w k ≤ 1 - η + τ k∈N i a ik βm w t i -w i + τ βm k∈N i a ik max t-Γ≤t ≤t w t k -w k (21) where we have used the triangle inequality in the second inequality. Assume that the affinity matrix A is doubly-stochastic, so that k∈N i a ik = 1 for all i. Denote V (t) = max i=1,...,m w t i -w i . Then (21) implies that w t+1 i -w i ≤ 1 -η+τ βm V (t) + τ βm max t-Γ≤t ≤t V (t ) holds for all i, and as a result V (t + 1) ≤ 1 - η + τ βm V (t) + τ βm max t-Γ≤t ≤t V (t ). As long as β ≥ η+τ m , we have 1 -η+τ βm ∈ [0, 1]. Then according to Feyzmahdavian et al. (2014, Lemma 3), we have V (t) ≤ 1 - η βm t 1+Γ V (0). Setting β to be the smallest possible value η+τ m yields the desired result. H Comparisons with previous distributed multi-task learning algorithms We now provide upper bounds of the iteration complexities for the distributed multi-task learning algorithms of Vanhaesebrouck et al. (2017) and Liu et al. (2017) in the ERM setting. We convert their notations into ours to be consistent. H.1 Iteration complexity of the algorithm of Liu et al. (2017) The full algorithm of Liu et al. (2017) performs alternating optimization over the task relationship and the local predictors on each machine. In order to to compare their algorithm with ours on the efficiency of learning predictors, we consider a fixed task correlation matrix M = I + τ η L in their objective (corresponding to Ω in eqn (1) of their paper). With fixed M, their algorithm performs distributed SDCA (Ma et al., 2015) for optimizing over the predictors. In each round of distributed SDCA, one constructs an upper bound of the objective that is separable over the machines (predictors), so that each machine solves a subproblem defined by its local data, and then one around of communication is used to aggregate local updates. When the instantaneous losses are β F -smooth and each local subproblem is solved exactly (i.e., we set Θ = 0 in their analysis), the number of global (communication) rounds needed for obtaining an approximate solution is, according to Liu et al. (2017, Lemma 7 and Theorem 8) , of the order (ignoring the logarithmic factor on final optimization error) max α α Kα m i=1 α [i] Kα [i] • max i M -1 ii • β F η . Here, the first term measures the \"task separability\" with value in [1, m] (see the definitions of K and α [i] in their Theorem 1, and the discussion of separability in Section 6.3). On the other hand, we have max i M -1 ii ≤ σ max M -1 ≤ 1. As a result, the iteration complexity of distributed SDCA is O β F η × (task separability in [1,m]). This iteration complexity is similar to that of our ERM algorithm by directly solving the regularizer ( O β F η ), but has worse dependence on the condition number and an unclear multiplicative constant on the tasks separability. H.2 Comparison with the collaborative algorithm of Vanhaesebrouck et al. (2017) We now compare with the collaborative learning algorithm of Vanhaesebrouck et al. (2017) in the synchronous and decentralized setting. In their algorithm, each machine augments its local optimization parameters to include a copy of predictor from each neighboring machine. Let Θ i be the set of |N i | + 1 variables w k for k ∈ N i ∪ {i}, and Θ k i is the copy of w k on machine i. We can reformulate the global objective (2) as arg min {Θ i } m i=1 m i=1 H i (Θ i ) where H i (Θ i ) = 1 m F i (Θ i i ) + η 2m Θ i i 2 + τ 4m k∈N i a ik Θ i i -Θ k i 2 subject to Θ i i = Θ i k , for all (i, k) s.t. k ∈ N i . (22) Vanhaesebrouck et al. ( 2017 ) then introduce variables associated with each edge (4 set of variables per edge) and apply ADMM to the resulting problem. An advantage of ADMM is that it allows decoupling of the local problems when updating primal variables, where the local problem involves the nonlinearized loss function. Although Vanhaesebrouck et al. (2017) suggest that the convergence results of synchronous decentralized ADMM (Wei and Ozdaglar, 2013; Shi et al., 2014) apply to this formulation (see their Appendix D), we note however that ( 22 ) is not in the standard form covered by these results. In particular, the classical decentralized concensus problem has the form min x 1 ,...,xm m i=1 f i (x i ) s.t. x i = x j for all (i, j) where j ∈ N i . Here, neighboring machines share the same set of optimization parameters and they would like to reach complete consensus, whereas in ( 22 ) neighboring machines can have different set of variables and they only try to achieve consensus on the shared parameters. As a result, it is nontrivial to derive the iteration complexity of the collaborative learning algorithm of Vanhaesebrouck et al. (2017) based on the same quantities used in the analysis of our algorithms. I Experiments In this section we examine the empirical performance of the proposed algorithms. We consider the problem of linear regression on synthetic data. For the i-th task, we generate data from y = w * i , x + , where is noise drawn from the Normal distribution N (0, 3), x ∈ R d is drawn from a multivariate Normal distribution with mean zero and covariance matrix Σ where Σ ij = 2 -|i-j|/3 , and w * i ∈ R d is a coefficient vector for the i-th task generated from the following clustered multi-task structure. Each w * i is drawn from a mixture of C clusters; there is a reference model r j for each cluster j = 1, . . . , C, and the task specific model w * i is a small perturbation of the corresponding cluster reference model: w * i = r j + ξ i , if w * i is drawn from cluster j. The cluster reference model r j is generated by sampling each entry i.i.d. from U nif [-0.5, 0.5], while the perturbation vector ξ i is generated by sampling each entry i.i.d. from U nif [-0.05, 0.05] . This construction gives us task specific models which are similar to each other when they belong to the same cluster. The corresponding similarity graph is a 10-nearest neighbor graph (so the graph is connected) with binary weights built on {w i } i=1,...,m , i.e., each task is connected to 10 other tasks whose models are most similar. We tested a few graph-based multi-task learning methods. • Local: solves a local ERM problem (with only ℓ 2 regularization) with n samples for each task. • Centralized: solves the graph-regularized ERM problem (2) with n samples for each task. • ADMM: the synchronized version of the ADMM algorithm of Vanhaesebrouck et al. (2017) . • SDCA: the distributed SDCA algorithm of Liu et al. (2017) for fixed graph. • Our algorithms: denoted as B/S (batch/stochastic) + SR/OL (solve regularizer/optimize loss). In the experiments below, we have problem dimension d = 100, number of tasks m = 100, training set size n = 500, and vary number of task clusters C over {1, 5, 10, 50} (smaller C implies overall stronger task similarity). We also generate a dev set of 10000 samples per task for tuning hyper-parameters, and test set of 10000 samples per task for approximately evaluating the population loss. Empirical risk minimization We fist compare the iterative methods on the regularized ERM problem (2), to which the analysis for ADMM and SDCA applies. We tune the ℓ 2 regularization parameter for Local and (η, τ ) for Centralized, and then fix the optimal (η, τ ) for other methods. We also tune the quadratic penalty parameter for ADMM, the task separability and stepsize parameters for SDCA, and stepsize parameter for BSR/BOL (although the default value based on the smoothness parameter already works well for them). For SSR/SOL, we draw random samples from the fixed training set (with size n), and simply fix the minibatch size to be n/10. Figure 2 shows for each method the estimated F (W) over iterations (or rounds of communication) in the top row, and over the amount of computation (measured by the number of passes over the training set) in the bottom row. Observe that all iterative algorithms converge to the same ERM solution, our algorithms tend to consistently outperform ADMM and SDCA. Stochastic optimization We next demonstrate the efficiency of true stochastic algorithms (using fresh samples for each update) at C = 10. We allow the algorithms to process a total of 10000 fresh samples on each machine, and vary the minibatch size b over {40, 80, 100, 200, 500}. The parameters (η, τ ) are fixed to those used in the ERM experiments. Figure 3 shows for each method the estimated F (W) over iterations (or rounds of communication) in the left plot, and over the amount of fresh samples processed (or total computation cost) in the right plot. As a reference, the error of Local and Centralized (using n = 500 samples per machine) are also given in the plots. We observe that with fresh samples, stochastic algorithms are kj ), and z t+1 kj : j = 1, . . . , b are b samples drawn by machine k at iteration t + 1. Let n = bT be the number of samples used in Algorithm 2. According to Theorem 3, as long as the minibatch size b≤ b * = O n ε(m,n) β F B 2 ,the first term in the error bound is dominant and we achieve the generalization error O σ √ mB 2 √ n = O LB 1+m•ρ(B,S) mn as in ERM, so we are still sample efficient in the stochastic setting. Time complexity Algorithm 2 processes the drawn samples only once. While maintaining the sample efficiency, we can set the minibatch size to the largest value b = b * , and this leads to the total number of iterations (and local communication rounds) Figure 1 : 1 Figure 1: Results for regularized ERM (left panel) and our stochastic methods with different b (right panel). Figure 2 : 2 Figure 2: Performance of different methods for regularized empirical risk minimization. Although for convex optimization, the constrained form and the regularized form are equivalent due to the Lagrange duality, solving the constrained form may still require repeatedly solving the regularized form and searching for the Lagrange multiplier. This is because ∇ 2 vec(U) F (UM -1 2 ) = (M -1 2 ⊗ I) • ∇ 2 vec(W) F (W) • (M -1 2 ⊗ I), and ||∇ 2 vec(U) F (UM -1 2 )|| ≤ ||M -1 2 || • ||∇ 2 vec(W) F (W)|| • ||M -1 2 || ≤ β F m ."
}