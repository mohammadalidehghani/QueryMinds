{
  "title": "PRIVATE MACHINE LEARNING VIA RANDOMISED RE-SPONSE",
  "abstract": "We introduce a general learning framework for private machine learning based on randomised response. Our assumption is that all actors are potentially adversarial and as such we trust only to release a single noisy version of an individual's datapoint. Our approach forms a consistent way to estimate the true underlying machine learning model and we demonstrate this in the case of logistic regression.",
  "introduction": "PRIVATE MACHINE LEARNING Our desire is to develop a strategy for machine learning driven by the requirement that private data should be shared as little as possible and that no-one can be trusted with an individual's data, neither a data collector/aggregator, nor the machine learner that tries to fit a model. Randomised Response, see for example Warner (1965) , is relevant in this context in which a datapoint x n is replaced with a randomised 'noisy' version xn . A classical example is voting in an election in which an individual voter votes for one of two candidates A or B and is asked to lie (with probability p) about whom they voted for . This results in noisy data and estimating the fraction f A of voters that voted for candidate A based on this noisy data fA = 1 N N n=1 I (x n = A) (1) can give a potentially significantly incorrect estimate. As Warner (1965) showed, since we know the probabilistic mechanism that generated the noisy data, a better estimate of the fraction of voters voting for candidate A is given by f A = fA + p 1 -2p (2) In a machine learning context, the kind of scenario we envisage is that users may have labelled face images as \"happy\" or \"sad\" on their mobile phones and the company MugTome wishes to train a \"happy/sad\" face classifier; however, users do not wish to send the raw face images to MugTome and also wish to be able to plausibly deny which label they gave any training image. To preserve privacy, each user will send to MugTome only a single corrupted datapoint -a single corrupted image and a single corrupted label. It is straightforward to extend our approach to deal with users sending multiple corrupted datapoints. However, since MugTome will potentially then know which corrupted datapoints belong to each user, they will have more information to help reveal the underlying clean datapoint. Since we assume we cannot trust MugTome, MugTome may attempt to recover the underlying true datapoint. For example, if a user sends three class labels c 1 , c 2 , c 3 , c i ∈ {0, 1}, then MugTome can have a good guess of the underlying true class label by simple taking the majority class c = I (c 1 + c 2 + c 3 > 2). Indeed, in general, if M corrupted datapoints are independently generated for a user, then MugTome's ability to reveal the true class (or attribute) increases dramatically. where p(c true ) is the prior belief on the true class. This posterior distribution concentrates exponentially quickly (in M ) around the true value c true . Similarly, if a pollster asks each voter three times what they voted, then the questioner would have a very good idea of the true vote of each voter; to protect the voter's privacy, the voter would then have to trust that the pollster either does not pass on any information that states that the three votes came from the same person or that the pollster doesn't attempt themselves to figure out what the voter voted for. Similarly, in a medical setting in which a patient privately owns a datapoint, releasing M synthetic versions (corruptions) of that datapoint can compromise privacy if which synthetic datapoints belong to each person is also known. To guarantee that privacy is retained would require patients to trust people with their data, namely that any data aggregation process will remove their patient ID. However, this is something out of the control of the patient and as such we do not consider generating multiple synthetic datapoints (see for example Bindschaedler et al. (2017) ) a 'safe' mechanism. For these reasons, we wish to make a process in which an individual only reveals a single corrupted datapoint; from that point onwards in the machine learning training process, no other trust in that process is required. To motivate our general approach to private machine learning we discuss the voting example in more detail in section(3). Connections to other forms of privacy preserving machine learning are discussed in section( 7 ). The justification for our approach hinges on the properties of the Spread Divergence, which we review in the following section.",
  "body": "PRIVATE MACHINE LEARNING Our desire is to develop a strategy for machine learning driven by the requirement that private data should be shared as little as possible and that no-one can be trusted with an individual's data, neither a data collector/aggregator, nor the machine learner that tries to fit a model. Randomised Response, see for example Warner (1965) , is relevant in this context in which a datapoint x n is replaced with a randomised 'noisy' version xn . A classical example is voting in an election in which an individual voter votes for one of two candidates A or B and is asked to lie (with probability p) about whom they voted for . This results in noisy data and estimating the fraction f A of voters that voted for candidate A based on this noisy data fA = 1 N N n=1 I (x n = A) (1) can give a potentially significantly incorrect estimate. As Warner (1965) showed, since we know the probabilistic mechanism that generated the noisy data, a better estimate of the fraction of voters voting for candidate A is given by f A = fA + p 1 -2p (2) In a machine learning context, the kind of scenario we envisage is that users may have labelled face images as \"happy\" or \"sad\" on their mobile phones and the company MugTome wishes to train a \"happy/sad\" face classifier; however, users do not wish to send the raw face images to MugTome and also wish to be able to plausibly deny which label they gave any training image. To preserve privacy, each user will send to MugTome only a single corrupted datapoint -a single corrupted image and a single corrupted label. It is straightforward to extend our approach to deal with users sending multiple corrupted datapoints. However, since MugTome will potentially then know which corrupted datapoints belong to each user, they will have more information to help reveal the underlying clean datapoint. Since we assume we cannot trust MugTome, MugTome may attempt to recover the underlying true datapoint. For example, if a user sends three class labels c 1 , c 2 , c 3 , c i ∈ {0, 1}, then MugTome can have a good guess of the underlying true class label by simple taking the majority class c = I (c 1 + c 2 + c 3 > 2). Indeed, in general, if M corrupted datapoints are independently generated for a user, then MugTome's ability to reveal the true class (or attribute) increases dramatically. where p(c true ) is the prior belief on the true class. This posterior distribution concentrates exponentially quickly (in M ) around the true value c true . Similarly, if a pollster asks each voter three times what they voted, then the questioner would have a very good idea of the true vote of each voter; to protect the voter's privacy, the voter would then have to trust that the pollster either does not pass on any information that states that the three votes came from the same person or that the pollster doesn't attempt themselves to figure out what the voter voted for. Similarly, in a medical setting in which a patient privately owns a datapoint, releasing M synthetic versions (corruptions) of that datapoint can compromise privacy if which synthetic datapoints belong to each person is also known. To guarantee that privacy is retained would require patients to trust people with their data, namely that any data aggregation process will remove their patient ID. However, this is something out of the control of the patient and as such we do not consider generating multiple synthetic datapoints (see for example Bindschaedler et al. (2017) ) a 'safe' mechanism. For these reasons, we wish to make a process in which an individual only reveals a single corrupted datapoint; from that point onwards in the machine learning training process, no other trust in that process is required. To motivate our general approach to private machine learning we discuss the voting example in more detail in section(3). Connections to other forms of privacy preserving machine learning are discussed in section( 7 ). The justification for our approach hinges on the properties of the Spread Divergence, which we review in the following section. SPREAD DIVERGENCE Throughout we use the notation p(X = x) for a random variable X in state x. However, to reduce notational overhead, where unambiguous, we write simply p(x). A divergence D(p||q) (see, for example Dragomir (2005) ) is a measure of the difference between two distributions p and q with the property D(p||q) ≥ 0 and D(p||q) = 0 ⇔ p = q (4) An important class is the f -divergence, defined as D f (p||q) = E q(x) f p(x) q(x) (5) where f (x) is a convex function with f (1) = 0. A special case of an f -divergence is the well-known Kullback-Leibler divergence KL(p||q) = E p(x) log p(x) q(x) which is widely used to train models using maximum likelihood. For the Spread Divergence, from q(x) and p(x) we define new distributions q(x) and p(x) that have the same support. Using the notation x to denote integration (•) dx for continuous x, and x∈X for discrete x with domain X , we define a random variable x with the same domain as x and distributions p(x) = x p(x|x)p(x), q(x) = x p(x|x)q(x) (6) where p(x|x) 'spreads' the mass of p and q such that p(x) and q(x) have the same support. For example, if we use a Gaussian p(x|x) = N x x, σ 2 , then p and q both have support R. The spread divergence has a requirement on the noise p(x|x), namely that D(p||q) = 0 ⇔ p = q; that is, if the divergence of the spreaded distributions is zero, then the original non-spreaded distribution will match. As shown in Zhang et al. (2018) this is guaranteed for certain 'spread noise' distributions. In particular, for continuous x and x of the same dimension and injective function f , a sufficient condition for a valid spread noise p(x|x) = K(x -f (x)) is that the kernel K(x) has strictly positive Fourier Transform. For discrete variables, a sufficient condition is that p(x = i|x = j) = P ij is that P ij > 0 and the matrix P is square and invertible. Spread divergences have a natural connection to privacy preservation and Randomised Response (Warner, 1965) . The spread divergence suggests a general strategy to perform private machine learning. We first express the machine learning problem as min θ D f (p(X)||p θ (X)) for a specified model p θ (X). Then, given only noisy data X, we fit the model by min θ D f p( X)||p θ ( X) . To explain in more detail how this works, we first describe randomised response in a classical voting context and then justify how to generalise this to principled training of machine learning models based on corrupted data. A CLASSICAL VOTING EXAMPLE There are two candidates in an election, candidate \"one\" and candidate \"zero\" and Alice would like to know the fraction of voters that voted for candidate \"one\". We write the dataset of voting as a collection of binary values {x 1 , . . . , x N }, x n ∈ {0, 1}. LEARNING θ USING CLEAN DATA If we assume that Alice has full knowledge of which candidate each voter voted for, then clearly Alice may simply count the fraction of people that voted for \"one\" and set θ = 1 N N n=1 x n (7) It will be useful to first consider how to arrive at the same result from a modelling perspective. We can consider an independent Bernoulli model p θ (X 1 = x 1 , . . . , X N = x N ) = N n=1 p θ (X n = x n ) (8) where p θ (X = 1) = θ (9) so that p θ (X = x) = θ x (1 -θ) 1-x (10) We also construct an empirical data distribution that places mass only on the observed joint state, namely p(X 1 , . . . , X N ) = N n=1 δ(X n , x n ) (11) where δ(x, x ) is the Kronecker delta function. Then 1 N KL(p(X 1 , . . . , X N )||p θ (X 1 , . . . , X N )) = L N (θ) + const. (12) where L N (θ) = 1 N N n=1 log p θ (X n = x n ) (13) = 1 N N n=1 (x n log θ + (1 -x n ) log (1 -θ)) (14) and minimising KL(p||p θ ) (or maximising L N (θ)) with respect to θ recovers the fraction of votes that are 1, equation( 7 ). This shows how we can frame estimating the quantity θ from uncorrupted private data as a divergence minimisation problem. LEARNING θ USING CORRUPTED DATA Returning to the privacy setting, Bob would also like to know the fraction of votes that are 1. However, Alice does not want to send to Bob the raw data x 1 , . . . , x N since the votes of any individual should not be explicitly revealed. To preserve privacy, Alice sends noisy data x1 , . . . , xN to Bob. In this case we draw a single joint sample x1 , . . . , xN from the distribution p( X1 , . . . , XN |X 1 , . . . , X N ) = N n=1 p( Xn |X n ) (15) where the 'spread noise' model is p( Xn = i|X n = j) = P ij . Hence, if x n = 0 Alice draws a sample xn = 0 with probability P 00 and xn = 1 with probability P 10 . Given a sampled noisy dataset x1 , . . . , xN we form an empirical spreaded data distribution p( X1 , . . . , XN ) = N n=1 δ Xn , xn Similarly, the corrupted joint model is given by pθ ( X1 , . . . , XN ) = N n=1 pθ ( Xn ) (17) where pθ ( X = j) = j∈{0,1} p( X = j|X = j)p θ (X = j) (18) On receiving the noisy dataset x1 , . . . , xN , Bob can try to estimate θ by minimising 1 N KL p( X1 , . . . , XN )||p θ ( X1 , . . . , XN ) = - 1 N N n=1 log pθ (x n ) + const. ( 19 ) with respect to θ. Equivalently, he may maximise the scaled spread log likelihood LN (θ) = 1 N N n=1 log pθ (x n ) (20) For this simple model, Bob can easily explicitly calculate pθ (x = 1) = p(x = 1|x = 1)p θ (x = 1)+p(x = 1|x = 0)p θ (x = 0) = P 11 θ +P 10 (1 -θ) (21) Similarly, pθ (x = 0) = P 01 θ + P 00 (1 -θ). In this case, equation(20) becomes f0 log (P 00 (1 -θ) + P 01 θ) + f1 log (P 10 (1 -θ) + P 11 θ) where f1 = 1 N N n=1 xn (23) Using f0 + f1 = 1, P 00 + P 10 = 1, P 01 + P 11 = 1, the maximum of the spread log likelihood is at θ = f1 + P 10 1 -P 10 -P 01 (24) which forms Bob's estimate of the underlying fraction of voters that voted for candidate \"one\". For example, if there were no noise P 10 = P 01 = 0, Bob would estimate θ = f1 , simply recovering the fraction of votes that are 1 in the original data. In the limit of a large number of votes N → ∞ and true probability θ 0 of a voter voting for candidate \"one\", then f0 tends to P 00 (1 -θ 0 ) + P 01 θ 0 and Bob's estimate recovers the true underlying voting probability θ = θ 0 . Hence, even though Bob only receives a corrupted set of votes, in the limit of a large number of votes, he can nevertheless estimate the true fraction of people that voted for candidate \"one\". PRIVATE MACHINE LEARNING USING RANDOMISED RESPONSE The above example suggests a general strategy to perform private machine learning: 1. Phrase problem as likelihood maximisation: We first assume that a machine learning task for private data x 1 , . . . , x N can be expressed as learning a data model p θ (X) by optimising an objective L N (θ) = 1 N N n=1 log p θ (X n = x n ) (25) 2. Form a corrupted dataset: Draw a single joint sample x1 , . . . , xN from the distribution p( X1 , . . . , XN |X 1 , . . . , X N ) = N n=1 p( Xn |X n ) (26) where p( X|X) is a defined spread noise distribution and known by both the owner of the private data and the receiver of the corrupted data. To do this, we go through each element of the dataset x n and replace it with a corruption xn sampled from p( Xn = xn |X n = x n ). 3. Send data to learner: We then send to the learner the corrupted dataset x1 , . . . , xN , the model to be learned p θ (X) and the corruption probability p( X|X). 4. Estimate θ from corrupted data: Having received the corrupted data x1 , . . . , xN , the learner fits θ by maximising the objective LN (θ) = 1 N N n=1 log pθ (x n ) (27) where pθ (x) = x p(x|x)p θ (x) (28) 4.1 JUSTIFICATION If we assume that each element x n of the training data x 1 , . . . , x N is identically and independently sampled from a model p θ0 (X n = x n ), then each corrupted observation xn is a sample from the same distribution given by pθ0 ( XN = y n ) = x p( Xn = y n |X = x)p θ0 (X = x) (29) By the law of large numbers the objective equation( 27 ) approaches its average over the data generating mechanism lim N →∞ L N (θ) a.s. --→ x pθ0 ( X = x) log pθ ( X = x) (30) and maximising the spread likelihood objective LN (θ) becomes equivalent to minimising KL pθ0 ( X)||p θ ( X) (31) Provided that the spread noise is valid (see section(2)), then KL pθ0 ( X)||p θ ( X) = 0 ⇒ θ = θ 0 (32) for an identifiable model p θ . Thus θ est = argmax θ LN (θ) (33) is a consistent estimator. This means that (in the large data limit and assuming the training data is generated from the model), even though we only train on corrupted data, we are optimising an objective LN (θ) which has a global minimum close to that of the objective on uncorrupted data L N (θ). Indeed, the estimator is consistent in the sense that as the amount of training data increases, we will recover the true clean data generating mechanism. Hence, provided that the corruption process is based on spread noise, then we can still learn the model parameters θ even by training on only corrupted data. In our motivating voting scenario in section(3), we saw explicitly that the estimate θ of the true underlying voting fraction is consistent and indeed, this is a general property of our approach. 3) for the model with binary variable p(x = 1) = θ, p(x = 0) = 1 -θ. In each case we plot along the x-axis the true θ 0 from 0 to 1 and on the y-axis the value of θ that maximises J ∞ (θ). In each plot we use a different flip probability. For a consistent estimator we would require that each plot is a straight x = y line, which only occurs in the case of no noise, p f = 0. TRAINING ON NOISE ONLY A common approach in private machine learning is to form synthetic (noisy, corrupted) data and then simply train the standard model on this noisy data -see for example Li et al. (2019) . In our notation, this would be equivalent to maximising the likelihood L N (θ) ≡ 1 N N n=1 log p θ (X = xn ) (34) As above, assuming that the training data is generated from an underlying model p θ0 (X n = x n ), by the law of large numbers, lim N →∞ L N (θ) a.s. --→ x pθ0 ( X = x) log p θ (X = x) (35) In general, the optimum of this objective does not occur when θ = θ 0 and therefore training on noisy data alone does not form a consistent estimator of the true underlying model. We discuss learning with noisy labels more extensively in the context of logistic regression in section(B) in which we show that provided the label flip noise is not too high p 0→1 + p 1→0 < 1, and for zero mean isotropically Gaussian distributed inputs, maximum likelihood training with corrupted class labels does form a consistent estimator. Hence, whilst one cannot guarantee that maximum likelihood training of logistic regression on noisy data will result in a consistent estimator, there are special situations in which this may work. RECONSTRUCTION APPROACH A seemingly natural alternative to our method is to attempt to reconstruct the clean datapoint from the noisy datapoint and use that within a standard learning framework. This approach would give an objective J N (θ) = 1 N N n=1 xn p(x n |x n ) log p θ (x n ) (36) Here we need to define a posterior distribution p(x n |x n ) to reconstruct the clean datapoint. Since the learner only has knowledge of the prior p θ (x) it is natural to set p(x n |x n ) = p θ (x n |x n ) ≡ p(x n |x n )p θ (x n ) xn p(x n |x n )p θ (x n ) (37) By the law of large numbers J N converges to its expectation with respect to the true data generating mechanism p θ0 (x) = p(x|x)p θ0 (x), so that lim N →∞ J N (θ) a.s. --→ x,x p θ0 (x)p θ (x|x) log p θ (x) ≡ J ∞ (θ) (38) In general, the optimum of J ∞ (θ) is not at θ = θ 0 . To demonstrate this, we plot in figure(1) the optimal θ for a simple Bernoulli model for which we can calculate J ∞ (θ) exactly. As we see, for all but zero flip noise, p f = 0, the estimator does not correctly identify the underlying probabilty generating mechanism. For this reason, we do not pursue this approach further. OTHER DIVERGENCES An extension of the above is to learn θ by minimising other f -divergences D f (p θ (Y )||p(Y |y)) = E p(Y |y) f pθ (Y ) p(Y |y) (39) However, this generalisation to any f -divergence is harder to justify since the expectation of this objective (by averaging over the noise realisations) p(y)D f (p θ (Y )||p(Y |y)) (40) will not in general give a divergence between spreaded distributions. This means that in the limit of a large number of datapoints, it is not guaranteed to recover the true data generating process, except for special choices of the f -divergence, such as the KL divergence. We leave a discussion of this for future work. PRIVATE LOGISTIC REGRESSION As an application of the above framework to a standard machine learning model, we now discuss how to form a private version of logistic regression. Returning to our motivating example, users may have labelled face images as \"happy\" or \"sad\" on their mobile phones and the company MugTome wishes to train a \"happy/sad\" face classifier; however, users do not wish to send the raw face images to MugTome and also wish to be able to plausibly deny which label they gave any training image. In this case we have a set of training data x 1 , . . . , x N , x n ∈ R D and corresponding binary class labels c 1 , . . . , c N , c n ∈ {0, 1}. We wish to fit a logistic regression model p θ (c|x) = φ((2c -1)θ T c x) (41 ) where φ(x) = 1/(1 + e -x ) is the logistic function. We follow the general approach outlined in section(4). 1. The model: For observation (x, c) and parameter θ p θ (c, x) = p θc (c|x)p θx (x) (42) where p θc (c|x) is the standard logistic regression model above and p θx (x) is a model of the input x. The training objective is L N (θ) = 1 N N n=1 log p θ (c n , x n ) (43) = 1 N N n=1 log p θc (c n |x n ) + 1 N N n=1 log p θx (x n ) (44) We note that this is a separable objective for L N (θ) = L c N (θ c ) + L x N (θ x ), L(θ) = 1 N N n=1 log pθ (c n , xn ) (46) = 1 N N n=1 log xn,cn p(c n |c n )p(x n |x n )p θc (c n |x n )p θx (x n ) (47) Unfortunately, in all but special cases, the integral (for continuous x) or sum (for discrete x) required to evaluate L is not tractable and numerical approximation is required. For this stage, there are many options available and we present below the approach taken in the experiments. Interestingly, we note that, unlike training on clean data, the objective L(θ) is not separable into a function of θ c plus a function of θ x , meaning that learning the class prediction parameter θ c is coupled with learning the input distribution parameter θ x . IMPLEMENTATION In general, the spread noise defines a distribution on a pair of spread variables p(c, x|c, x) and the full joint distribution, including the original model is p(c, x, c, x) = p(c, x|c, x)p θc (c|x)p θx (x) (48 ) For continuous x, the spread likelihood is then obtained from p(c, x) = c x p(c, x, c, x) (49) In general, this sum/integral over x is intractable due to the high-dimensionality of x. We use a standard approach to lower bound the log likelihood (for a single datapoint) by log p(c, x) ≥ -E q(c,x|c,x) [log q(c, x|c, x)] + E q(c,x|c,x) [log p(c, x|c, x)p θc (c|x)p θx (x)] ( 50 ) where q is a distribution chosen to make the bound tight, see for example Barber (2012) . This allows us to use an EM-style procedure in which we iterate between the two steps : (M-step) fix q and optimise θ and (E-step) fix θ and update q. 1. Iteration k M-step: Update θ to increase the \"energy\" θ k+1 = argmax θ E(θ; q k ) (51) where (for multiple datapoints) E(θ; q) ≡ N n=1 E q(cn,xn|cn,xn) [log p θc (c n |x n )] + N n=1 E q(xn|cn,xn) [log p θx (x n )] (52) An advantage of this approach is that E(θ; q) is separable and we can update the class prediction parameter θ c independently of the input distribution parameter θ x . In practice we will typically only do a partial optimisation (gradient ascent step) over θ to guarantee an increase in the energy. 2. Iteration k E-step: The bound is tightest when q is set to the posterior (see for example Barber (2012) ), q k+1 (c, x|c, x) = p(c, x|c, x) = p(c|c)p(x|x)p(c|x)p(x) Z(c, x) (53) where p(c|x) = p θ k c (c|x), p(x) = p θ k x (c|x) and the normaliser is given by Z(c, x) ≡ c p(c|c)p(x|x)p(c|x)p(x)dx (54) To implement the M-step, Equation( 52 ) requires expectations of the form c p(c, x|c, x)f (x, c)dx (55) for some function f (x, c). Assuming that the posterior will be reasonably peaked around the noisy data we use sampling with an importance distribution ρ(c, x|c, x) = ρ(c|c)ρ(x|x) (56) The expectation is then motivated by c x p(c, x|c, x)f (x, c) = c x ρ(c|c)ρ(x|x) p(c|c)p(x|x)p(c|x)p(x) ρ(c|c)ρ(x|x)Z(c, x) f (x, c) (57) Choosing ρ(c|c) = p(c|c) Z ρ (c) , ρ(x|x) = p(x|x)p(x) Z ρ (x) (58) for normalising functions Z ρ (c), Z ρ (x) we then run a standard importance sampling approximation (see section(A)). For a given noisy datapoint (c n , xn ) we generate a set of S samples c 1 n , . . . , c S n from ρ(c|c n ) and samples x 1 n , . . . , x S n from ρ(x|x n ) and compute the importance weights w(s|n) = φ (2c s n -1)θ T c x s n s φ ((2c s n -1)θ T c x s n ) (59) The energy equation( 52 ) separates into two independent terms (see section(A)) E(θ c ; q) ≈ N n=1 S s=1 w(s|n) log φ (2c s n -1)θ T c x s n (60) and E(θ x ; q) ≈ N n=1 S s=1 w(s|n) log p θx (x s n ) (61) Equation( 60 ) is a weighted version of the standard logistic regression log likelihood, L c (θ c ) in equation( 44 ); similarly equation( 61 ) is a weighted version of L x (θ x ). The advantage therefore is that, given the importance samples, the learning procedure for θ requires only a minor modification of the standard maximum likelihood training procedure on clean data. The full procedure is that we randomly initialise θ and then, for each datapoint n, draw samples and accumulate the gradient across samples and datapoints. After doing a gradient ascent step in θ, we update the importance distributions and repeat until convergence. The Importance Sampling approximation is a convenient approach, motivated by the assumption that corrupted datapoints will be close to their uncorrupted counterparts. Whilst we used a bound as part of the approximation, this is not equivalent to using a parametric q distribution; by sampling we form a consistent estimator of the tightest possible lower bound. In other words, we are simply using Importance Sampling to estimate the expectations required within a standard Expectation Maximisation algorithm, see for example Barber (2012) . We also tried learning a parametric q, similar to standard variational approaches to log likelihood maximisation, but didn't find any improvement on the Importance Sampling approach. LEARNING THE PRIOR If we have access to clean data, the optimal input model p θx (x) can be learned from maximising the likelihood L x (θ x ). However, our general assumption is that we will never have access to clean data. There may be situations in which the learner has a good model of p θx (x), without compromising privacy (for example a publicly available dataset for a similar prediction problem might be available) in which case it makes sense to set the prior to this known model. In the absence of a suitable prior we can attempt to learn p θx (x) from the corrupted data by maximising L(θ). For simplicity we assume a factorised model and for a D-dimensional input vector x = (x[1], . . . , x[D]) write p θx (x) = D d=1 p(x[d]|d) (62) for a collection of learnable univariate distributions p(x[d]|d), d = 1, . . . , D. Under this assumption, and using the Importance Sampling approach in equation( 61 ), this means that p(x[d]|d) can be learned by maximising E x = N n=1 S s=1 w(s|n) D d=1 log p(x n s [d]|d) ( 63 ) Since this is a separable objective, we can learn each p(x n s [d]|d) independently. For simplicity, we assume a discrete distribution for x[d] that contains K states (or bins). Then E x [d] = K k=1 N n=1 S s=1 w(s|n)I (x n s [d] ∈ k) log p(k|d) (64) where I (x n s [d] ∈ k) is 1 if the sample x n s [d] is in the k th state and 0 otherwise. Optimising with respect to p(k|d) we obtain p(k|d) = N n=1 S s=1 w(s|n)I (x n s [d] ∈ k) K k=1 N n=1 S s=1 w(s|n)I (x n s [d] ∈ k) (65) For the M-step of the algorithm we then make a gradient update for θ c and update the prior using equation( 65 ). EXPERIMENTS We implemented our approach in section(5) to train a logistic regression classifier to distinguish between the MNIST digits 7 and 9 based on noisy data (250 train and 900 test examples from each class). We chose to train on a small dataset since this constitutes the most challenging scenario and helps highlight potential differences between rival approaches. The MNIST images x have pixesl with 256 states and we used a discrete distribution to model x. For our experiments we assume a corruption model p(c|c) that flips the label 0 → 1 and 1 → 0 with probability p f with probability p f . We also assume here for simplicity assume a factorised input corruption model p(x|x) = D d=1 p(x[d]|x[d] ) in which with probability 1 -p f and uniformly from the other states of that pixel with probability p f . In this case, computing the Importance Sampling distribution is straightforward since the posterior is factorised over the image pixels. We considered three settings for the prior (required to compute the Importance Sampling distribution) : (i) flat prior, (ii) learned prior using EM, (iii) true factorised prior based on computing the marginal distribution of each pixel on the training dataset. In the 'true prior' case we assumed that we know the true marginal distribution of each pixel p(x[d]|d) -in general, this information would be private, but it is interesting to consider how much improvement is obtained by knowing this quantity. We compare the following approaches: Log Reg Clean Data We trained logistic regression on clean data. This sets an upper limit on the expected performance. Log Reg on Noisy Data We trained a standard logistic regression model but using the corrupted data. This forms a simple baseline comparison. Spread Log Reg with Learned Prior We used our Spread Likelihood approach to learn the prior. (a) Spread Log with 'True Prior' In general our assumption is that the true prior will not be known (since this requires users to release their private data to the prior learner). However, this forms an interesting comparison and expected upper bound on the performance of the spread approach. Spread Log with Flat Prior In this case we used an informative, flat prior on all pixel states. We ran 10 experiments for each level of flip noise p f from 0.1, 0.2, 0.3, 0.4 and then tested the prediction accuracy of the learned logistic classifiers on clean hold out data, see figure(3). (b) (c) (d) (e) (f) (g) (h) (i) (j) For all but small noise levels, the results show the superiority of the spread learning approach over simply training on noisy data, consistent with our theory that training the standard model on noisy data does not in general form a consistent estimator. The best performing spread approach is that which uses the true prior -however, in general this true prior will not be available. For this experiment, there appears to be little difference between using a flat prior and a learned prior. The performance of standard logistic regression training but using corrupted data is surprisingly effective, at least at low noise levels. However the performance degrades quickly for higher noise levels. A partial explanation for why logistic regression may give good results simply trained on noisy data is given in section(B). GAUSSIAN INPUT PRIOR p(x) We also demonstrate here training logistic regression treating the pixels as continuous. If we an independent have (per pixel) a Gaussian prior p(x i ) = N x i µ i , σ2 i (66) and independent Gaussian spread noise p(x i |x i ) = N xi x i , σ 2 i (67) then using the Importance Sampling posterior is ρ(x i |x i ) = N x i mean = b i a i , var = 1 a i (68) where a i = 1 σ 2 i + 1 σ2 i , b i = xi σ 2 i + µ i σ2 i (69) We also used Gaussian spread noise to corrupt the images and train a binary classifier to distinguish between the MNIST digits 7 and 9 based on noisy data (4500 train and 900 test examples from each class). For simplicity, we assumed factorised distributions with prior p( x i ) = N x i µ i , σ2 i , 0. 1 0.2 0.3 0.4 0.70 0.75 0.80 0.85 0.90 0.95 log reg SD true prior SD learn prior SD flat prior log reg noise training approach with true prior; \"SD learn prior\": spread approach with learned prior; \"SD flat prior\": spread approach with flat prior; \"log reg noise\": standard logistic regression training but trained on noisy data. p(x i |x i ) = N xi x i , σ 2 i . We chose spread flip noise p f = 0.2 for the class labels and uniform spread noise with variance σ 2 i = 0.1; the prior p(x) was set to be quite uninformative with mean µ i = 0 and variance σ2 i = 10. This level of noise means that approximately 20% of the class labels are incorrect in the data passed to MugTome and the associated image is significantly blurred, see figure(4). For standard logistic regression we found that for a learning rate of 0.2, 400 iterations gave the best performance, with 95.5% train accuracy and 95.7% test accuracy. Using our Importance Sampling scheme with S = 2 samples per noisy datapoint, the trained θ when tested on clean images had 94.4% test accuracy. This shows that despite the high level of class label and image noise, MugTome are able to learn an effective classifier, preserving the privacy of the users. The loss in test and training accuracy, despite this high noise level is around a modest 1%. When using higher spread noise with variance σ 2 = 0.5, the θ learned on the noisy data had a clean data test accuracy 93%, which is also a modest decrease in accuracy for a significant increase in privacy. For future work it would be interesting to consider other forms of noise, for example downsampling images. However, downsampling does not form an injective mapping and as such we cannot guarantee that we can find a consistent estimator for the underlying model. DISCUSSION There are many forms of private machine learning. Some attempt to transform a datapoint x to a form x such that a protected attribute a (such as gender) cannot be recovered from x , yet the prediction of an output y (for example using p(y|x )) is retained. For example this could be achieved by using a loss function such as (see for example Li et al. (2019) ) (b) (c) (d) (e) (f) L(θ, φ, ψ) = n [L y (y n , y(x n ; θ)) -L a (a n , a(x n ; φ))] ( 70 ) where n is the data index, y(x ; θ) is a function that takes input x and outputs a prediction y; a(x ; φ) is a function that takes input x and outputs an attribute prediction a and x = f (x; ψ) gives a representation of the input; L y , L a are loss functions. In this protected attribute setting, typically some form of the clean dataset is required to learn the parameters θ, φ, ψ. Another common form of private machine learning is based on differential privacy (Dwork & Roth, 2014) , with the aim to make it difficult to discern whether a datapoint x n was used to train the predictor y(x; θ). That is, given a trained model, differential privacy attempts to restrict the ability to differentiate whether any individual's datum was used to train the model. A closely related concept to randomised response is that of plausible deniability, namely privately corrupting a datapoint x n such that no-one (except the datapoint provider) can confidently state what the original (private) value of x n is. Recently Bindschaedler et al. (2017) used this to create synthetic datapoints, which were subsequently used with a standard machine learning training approach. The authors showed that generating synthetic data x from a distribution p(x|x) that takes dependency amongst the elements of the vector x results in better machine learning predictors than sampling from a factorised distribution. In synthetic data generation approaches the assumption is that the statistical characteristics are similar to the real data. However, care is required since if the generating mechanism is powerful, it may generate data which is very similar to the private data. In general these synthetic data generating approaches do not take into consideration when learning the parameters of the machine learning model what that synthetic data generation mechanism is. This is analogous to simply using the corrupted votes to directly estimate the fraction of voters that voted for a candidate, equation(1), rather than using knowledge of the data generation approach, equation(2). SUMMARY We discussed a general privacy preserving mechanism based on random response in a datapoint is replaced by a corrupted versions. We showed that, provided the corruption process is a valid spread noise, then a maximum likelihood approach forms a consistent estimator. That is, even though the model is only trained on corrupted, synthetic data, it is possible to recover the true underlying data genering mechnanism on the clean data. We applied this approach to a simple logistic regression model, showing that the approach can work well, even with high levels of noise. The approach is readily applicable to a large class of much more complex models and other divergences. S. L. Warner. Randomised response: a survey technique for eliminating evasive answer bias. Journal of the American Statistical Association, 60(309):63-69, 1965. Mingtian Zhang, Peter Hayes, Tom Bird, Raza Habib, and David Barber. Spread divergences, 2018. A PRIVACY PRESERVING LOGISTIC REGRESSION The posterior is given by p(c, x|c, x) = p(c|c)p(x|x)p(c|x)p(x) c x p(c|c)p(x|x)p(c|x)p(x) = p(c|c)p(x|x)p(c|x)p(x) Z(c, x) (71) For the learning, we need to take expectations c x p(c, x|c, x)f (x, c)s (72) We use importance sampling to approximate this expectation c x p(c, x|c, x)f (x, c) = c x ρ(c|c)ρ(x|x) p(c, x|c, x) ρ(c|c)ρ(x|x) f (x, c) (73) = c x ρ(c|c)ρ(x|x) p(c|c)p(x|x)p(c|x)p(x) ρ(c|c)ρ(x|x)Z(c, x) f (x, c) (74) Choosing ρ(c|c) = p(c|c) c p(c|c) = p(c|c) Z ρ (c) , ρ(x|x) = p(x|x)p(x) x p(x|x)p(x) = p(x|x)p(x) Z ρ (x) (75) we have c x p(c, x|c, x)f (x, c) = Z ρ (c)Z ρ (x) c x ρ(c|c)ρ(x|x) p(c|x) Z(c, x) f (x, c) (76) Here Z(c, x) = c x ρ(c|c)ρ(x|x) p(c|c)p(x|x)p(c|x)p(x) ρ(c|c)ρ(x|x) (77) = c x ρ(c|c)ρ(x|x) p(c|c)p(x|x)p(c|x)p(x) ρ(c|c)ρ(x|x) (78) = Z ρ (c)Z ρ (x) c x ρ(c|c)ρ(x|x)p(c|x) (79) Putting this together and using the same samples to estimate the numerator and denominator expectations, c x p(c, x|c, x)f (x, c) = c x ρ(c|c)ρ(x|x) p(c|x) c x ρ(c|c)ρ(x|x)p(c|x) f (x, c) (80) ≈ s p(c s |x s ) s p(c s |x s ) f (x s , c s ) (81) = s w(s)f (x s , c s ) (82) for importance weight w(s) = p(c s |x s ) s p(c s |x s ) (83) E [z 1 z 2 ] -E [z 1 ]E [z 2 ] = θ T Σθ 0 (97) E z 2 1 -E [z 1 ] 2 = θ T 0 Σθ 0 (98) E z 2 2 -E [z 2 ] 2 = θ T Σθ (99) We can then write the large data limit log likelihood as a two dimensional expectation L ∞ (θ) = E [(p 1→1 φ(z 1 ) + p 0→1 (1 -φ(z 1 ))) log φ(z 2 )] + E [1 -p 1→1 φ(z 1 ) -p 0→1 (1 -φ(z 1 ))) log (1 -φ(z 2 )))] (100) For simplicity, consider µ = 0, Σ = s 2 I, θ T 0 θ 0 = 1, θ T θ = 1, θ T 0 θ = cos(α). It is straightforward to show that in this case the gradient with respect to α is zero when α = 0, namely when θ = θ 0 . However, in general, for non-isotropic data covariance Σ, the gradient is non-zero at θ = θ 0 . To derive the above result, we note that the covariance for z in this case is simply where the functions are defined as C ≡ s 2 1 cos α cos α 1 ( Z 1 ( 1 ) = s 1 , Z 2 ( 1 , 2 ) = s ( 1 cos α + 2 sin α) (105) Differentiating L ∞ (α) with respect to α, we obtain sE N ( 0,I) [(γ(Z 1 ( 1 )) -φ(Z 2 ( 1 , 2 ))) ( 2 cos α -1 sin α)] (106) When α = 0 we note that Z 2 ( 1 , 2 ) is independent of 2 and that the above is therefore is zero. Hence L ∞ (α) has zero gradient at α = 0. It is straightforward to show that the second derivative of L ∞ (α) (evaluated at α = 0) is sE N ( 1 0,1) [-1 γ(s 1 )] -s 2 E N ( 1 0,1) [φ(s 1 )(1 -φ(s 1 ))] (107) The second term in equation( 107 ) above is clearly negative. Using integration by parts (and noting that we may assume s > 0), one may easily show that the first term is also negative provided that p 1→1 > p 0→1 . Hence we arrive at the (perhaps surprising) result that for zero mean isotropic Gaussian distributed input data, training on noisy data (c, x) in which the class labels have been flipped with some probability, results in a consistent estimator for θ 0 , provided the flip noise is not too high, namely p 1→1 > p 0→1 , or equivalently, p 0→1 + p 1→0 < 1. This result holds even in the case of asymmetric flip noise p 0→1 = p 1→0 . More generally, even if the data p(x) is not Gaussian distributed, from the Central Limit Theorem, p(z) is likely to be close to Gaussian distributed for high dimensional inputs. Hence, for input data x that is roughly isotropically distributed, we can expect that training using maximum likelihood for any classifier of the form p(c = 1|x) = φ θ T x will likely be close to recovering the true θ 0 that generated the data (in the limit of a large number of datapoints). The above analysis considered only noise on the class label. If, independently of the class label we add isotropic Gaussian noise to the observations, then the projection z will still be isotropic Gaussian distributed for Gaussian inputs p(x) and the above argument trivially extends to this case as well. Hence, one can expect training (using standard logistic regression but with corrupted inputs and flipped labels) to be partially successful at recovering the true data generating process provided that the input data is close to isotropically distributed, motivating a whitening pre-processing step of the input data. For example, if MugTome know the corruption mechanism p(c m |c true ) the posterior of the class is given by p(c true |c 1 , . . . , c M ) ∝ p(c true ) M m=1 p(c m |c true ) Figure 1 : 1 Figure1: Training based on the reconstruction approach, section(4.3) for the model with binary variable p(x = 1) = θ, p(x = 0) = 1 -θ. In each case we plot along the x-axis the true θ 0 from 0 to 1 and on the y-axis the value of θ that maximises J ∞ (θ). In each plot we use a different flip probability. For a consistent estimator we would require that each plot is a straight x = y line, which only occurs in the case of no noise, p f = 0. in which the logistic regression parameters θ c are conditionally independent (conditioned on the training data) of the input parameters θ x . 2. Form the corrupted dataset: We wish to send noisy data x1 , . . . , xN , c1 , . . . , cN to the learner. To do so we need to define a corruption model p(c, x|c, x). For simplicity, we consider a corruption model of the form p(c, x|c, x) = p(c|c)p(x|x) (45) The corruption processes of p(c|c) and p(x|x) are problem specific; see the experiments section(6) for some examples. 3. Send to learner corrupted data and model: The corrupted labels and inputs are sent to the learner (c 1 , x1 ), . . . , (c N , xN ) along with the model p θc (c|x), p θx (x) and corruption process p(c|c), p(x|x). 4. Learn the model parameters θ: The spread log likelihood is Figure 2 : 2 Figure 2: (a) An example MNIST \"7\" alongside its noisy examples (b) p f = 0.1, (c) p f = 0.2, (d) p f = 0.3, (e) p f = 0.4) which is sent to Mugshot.com noise. Each pixel remains in state 1 -p f and is otherwise sampled uniformly from the available states of that pixel. The bottom row shows an example of a clean \"9\" (f) and corruptions. Figure 3 : 3 Figure3: The test accuracy (on clean data) of the trained logistic regression models, averaged over 10 different randomly chosen training datasets of 500 datapoints. The x-axis is the corruption probability p f . \"log reg\": Standard logistic regression training on clean data; \"SD true prior\": spread divergence training approach with true prior; \"SD learn prior\": spread approach with learned prior; \"SD flat prior\": spread approach with flat prior; \"log reg noise\": standard logistic regression training but trained on noisy data. Figure 4 : 4 Figure 4: (a) An example MNIST \"7\" alongside its noisy example which is sent to MugTome with Gaussian noise variance (b) σ 2 = 0.1 and (c) σ 2 = 0.5 ; (d,e,f) similarly for an MNIST \"9\". 101)We now use the decomposition C = M M T , with Cholesky factor sample from z is equivalent to z ∼ M for ∼ N ( 0, I). Definingγ(x) = p 1→1 φ(x) + p 0→1 (1 -φ(x))(103)we can then write the expected log likelihood as a function of φ:L ∞ (α) = E N ( 0,I) [γ(Z 1 ( 1 )) log φ(Z 2 ( 1 , 2 )) + (1 -γ(Z 1 ( 1 ))) log (1 -φ(Z 2 ( 1 , 2 )))](104)"
}