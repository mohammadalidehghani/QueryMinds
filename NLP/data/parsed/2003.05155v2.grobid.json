{
  "title": "Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology",
  "abstract": "Machine learning is an established and frequently used technique in industry and academia but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners have a need for guidance throughout the life cycle of a machine learning application to meet business expectations. We therefore propose a process model for the development of machine learning applications, that covers six phases from defining the scope to maintaining the deployed machine learning application. The first phase combines business and data understanding as data availability oftentimes affects the feasibility of the project. The sixth phase covers state-of-the-art approaches for monitoring and maintenance of a machine learning applications, as the risk of model degradation in a changing environment is eminent. With each task of the process, we propose quality assurance methodology that is suitable to adress challenges in machine learning development that we identify in form of risks. The methodology is drawn from practical experience and scientific literature and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support but lacks to address machine learning specific tasks. Our work proposes an industry and application neutral process model tailored for machine learning applications with focus on technical tasks for quality assurance.",
  "introduction": "Introduction Many industries, such as manufacturing [1, 2] , personal transportation [3] and healthcare [4, 5] are currently undergoing a process of digital transformation, challenging established processes with machine learning-driven approaches. The expanding demand is highlighted by the Gartner report [6] , claiming that organizations expect to double the number of machine learning (ML) projects within a year. However, 75 to 85 percent of practical ML projects currently do not match their sponsors' expectations, according to surveys of leading technology companies [7] . Fischer et al. [8] name data and software quality among others as the key challenges in the machine learning life cycle. Another reason is the lack of guidance through standards and development process models arXiv:2003.05155v2 [cs.LG] 24 Feb 2021 specific to ML applications. Industrial organizations, in particular, rely heavily on standards to guarantee a consistent quality of their products or services. A Japanese industry Consortium (QA4AI) was founded to address those needs [9] . Due to the lack of a process model for ML applications, many project organizations rely on alternative models that are closely related to ML, such as, the Cross-Industry Standard Process model for Data Mining (CRISP-DM) [10] [11] [12] . It is grounded on industrial data mining experience [12] and is considered most suitable for industrial projects amongst related process models [13] . In fact, CRISP-DM has become the de-facto industry standard [14] process model for data mining, with an expanding number of applications [15] , e.g., in quality diagnostics [16] , marketing [17] , and warranty [18] . However, we have identified two major shortcomings of CRISP-DM: First, CRISP-DM focuses on data mining and does not cover the application scenario of ML models inferring real-time decisions over a long period of time (see fig. 1 ). The ML model has to be adaptable to a changing environment or the model's performance will degrade over time, such that, a permanent monitoring and maintaining of the ML model is required after the deployment. Second, and more worrying, CRISP-DM lacks guidance on quality assurance methodology. This oversight is particularly evident in comparison to standards in the area of information technology [19] but also apparent in alternative process models for data mining [20, 21] . In our definition, quality is not only defined by the product's fitness for its purpose [14] , but the quality of the task executions in any phase during the development of a ML application. This ensures that errors are caught as early as possible to minimize costs in the later stages during the development. The initial effort and cost to perform the quality assurance methodology is expected to outbalance the risk of fixing errors in a later state, that are typically more expensive due to increased project complexity [22, 23] . Our process model follows the principles of CRISP-DM, in particular by keeping the model industry and application neutral, but is modified to the particular requirements of ML applications and proposes quality assurance methodology that became industry best practice. Our contributions focus primarily on the technical tasks needed to produce evidence that every step in the development process is of sufficient quality to warrant the adoption into business processes. The following second section describes the related work and ongoing research in the development of process models for machine learning applications. In the third chapter, the tasks and quality assurance methodology are introduced for each process phase. Finally, a conclusion and an outlook are given in the fourth chapter.",
  "body": "Introduction Many industries, such as manufacturing [1, 2] , personal transportation [3] and healthcare [4, 5] are currently undergoing a process of digital transformation, challenging established processes with machine learning-driven approaches. The expanding demand is highlighted by the Gartner report [6] , claiming that organizations expect to double the number of machine learning (ML) projects within a year. However, 75 to 85 percent of practical ML projects currently do not match their sponsors' expectations, according to surveys of leading technology companies [7] . Fischer et al. [8] name data and software quality among others as the key challenges in the machine learning life cycle. Another reason is the lack of guidance through standards and development process models arXiv:2003.05155v2 [cs.LG] 24 Feb 2021 specific to ML applications. Industrial organizations, in particular, rely heavily on standards to guarantee a consistent quality of their products or services. A Japanese industry Consortium (QA4AI) was founded to address those needs [9] . Due to the lack of a process model for ML applications, many project organizations rely on alternative models that are closely related to ML, such as, the Cross-Industry Standard Process model for Data Mining (CRISP-DM) [10] [11] [12] . It is grounded on industrial data mining experience [12] and is considered most suitable for industrial projects amongst related process models [13] . In fact, CRISP-DM has become the de-facto industry standard [14] process model for data mining, with an expanding number of applications [15] , e.g., in quality diagnostics [16] , marketing [17] , and warranty [18] . However, we have identified two major shortcomings of CRISP-DM: First, CRISP-DM focuses on data mining and does not cover the application scenario of ML models inferring real-time decisions over a long period of time (see fig. 1 ). The ML model has to be adaptable to a changing environment or the model's performance will degrade over time, such that, a permanent monitoring and maintaining of the ML model is required after the deployment. Second, and more worrying, CRISP-DM lacks guidance on quality assurance methodology. This oversight is particularly evident in comparison to standards in the area of information technology [19] but also apparent in alternative process models for data mining [20, 21] . In our definition, quality is not only defined by the product's fitness for its purpose [14] , but the quality of the task executions in any phase during the development of a ML application. This ensures that errors are caught as early as possible to minimize costs in the later stages during the development. The initial effort and cost to perform the quality assurance methodology is expected to outbalance the risk of fixing errors in a later state, that are typically more expensive due to increased project complexity [22, 23] . Our process model follows the principles of CRISP-DM, in particular by keeping the model industry and application neutral, but is modified to the particular requirements of ML applications and proposes quality assurance methodology that became industry best practice. Our contributions focus primarily on the technical tasks needed to produce evidence that every step in the development process is of sufficient quality to warrant the adoption into business processes. The following second section describes the related work and ongoing research in the development of process models for machine learning applications. In the third chapter, the tasks and quality assurance methodology are introduced for each process phase. Finally, a conclusion and an outlook are given in the fourth chapter. Related Work CRISP-DM defines a reference framework for carrying out data mining projects and sets out activities to be performed to complete a product or service. The activities are organized in six phases (see table 1 ). The successful completion of a phase initiates the execution of the subsequent activity. CRISP-DM includes iterations of revisiting previous steps until success or completion criteria are met. It can be therefore characterized as a waterfall life cycle with backtracking [20] . During the development of applications, processes and tasks to be performed can be derived from the standardized process model. Methodology instantiates these tasks, i.e. stipulates how to do a task (or how it should be done). For each activity, CRISP-DM defines a set of (generic) tasks that are stable and general. Hereby, tasks are called stable when they are designed to keep the process model up to date with new modeling techniques to come and general when they are intended to cover many possible project scenarios. CRISP-DM has been specialized, e.g., to incorporate temporal data mining (CRISP-TDM; [24] ), null-hypothesis driven confirmatory data mining (CRISP-DM0; Difference between data mining processes and machine learning applications. A) In the data mining process information is directly extracted from data to find pattern und gain knowledge. B) A machine learning application consists of two steps. A machine learning model on data is trained and applied to perform inference on new data. Note that the model itself can be studied to gain insight within a knowledge discovery process. [25] ), evidence mining (CRISP-EM; [26] ), and data mining in the healthcare (CRISP-MED-DM; [27] ). Complementary to CRISP-DM, process models for ML applications have been proposed [28, 29] (see Table 1 ). Amershi et al. [28] conducted an internal study at Microsoft on challenges of ML projects and derived a process model with nine different phases. However, their process model lacks quality assurance methodology and does not cover the business needs. Breck et al. [29] proposed 28 specific tests to quantify issues in the ML pipeline to reduce the technical debt [30] of ML applications. These tests estimate the production readiness of a ML application, i.e., the quality of the application in our context. However, their tests do not completely cover all project phases, e.g., excluding the business understanding activity. Practical experiences reveal that business understanding is a necessary first step that defines the success criteria and the feasibility for the subsequent tasks. Without considering the business needs, the ML objectives might be defined orthogonal to the business objectives and causes to spend a great deal of effort producing the rights answers to the wrong questions. To our knowledge, Marbán et al. [20] were the first to consider quality in the context of process models for data mining. Borrowing ideas from software development, their work suggests creating traceability, test procedures, and test data for challenging the product's fitness for its purpose during the evaluation phase. We address these issues by devising a process model for the development of practical ML applications. In addition, we will provide a curated list of references for an in-depth analysis on the specific tasks. Quality Assurance in Machine Learning Projects We propose a process model that we call CRoss-Industry Standard Process model for the development of Machine Learning applications with Quality assurance methodology (CRISP-ML(Q)) to highlight its compatibility to CRISP-DM. It is designed for the development of machine applications i.e. application scenarios where a ML model is deployed and maintained as part of a product or service (see fig. 1 ). As a first contribution, quality assurance methodology is introduced in each phase and task of the process model (see fig. 2 ). The quality methodology serves to mitigate risks that affect the success and efficiency of the machine learning application. As a second contribution, CRISP-ML(Q) covers a monitoring and maintenance phase to address risks of model degradation in a changing environment. This extends the scope of the process model as compared to CRISP-DM, see Table 1 . Moreover, business and data understanding are merged into a single phase because industry practice has taught us that these two activities, which are separate in CRISP-DM, are strongly intertwined, since business objectives can be derived or changed based on available data (see Table 1 ). A similar approach has been outlined in the W-Model [31] . CRISP-ML(Q) CRISP-DM Amershi et al. [ 28] Breck et al. [29] Business & Data Understanding Business Understanding Requirements -Data Understanding Collection Data Data Preparation Data Preparation Cleaning Infrastructure Labeling Feature Engineering Modeling Modeling Training Model Evaluation Evaluation Evaluation -Deployment Deployment Deployment -Monitoring & Maintenance -Monitoring Monitoring Table 1: Comparing different process models for DM and ML projects. Business and data understanding phases are merged in CRISP-ML(Q) and a separate maintenance phase is introduced in comparison to CRISP-DM. Amershi et al. [28] and Breck et al. [29] lack the business understanding phase. Deep red color highlight data and petrol blue color model related phases. In what follows, we describe selected tasks from CRISP-ML(Q) and propose quality assurance methodology to determine whether these tasks were performed according to current standards from industry best practice and academic literature, which have proven to be general and stable and are suitable to mitigate the task specific risks. The selection reflects tasks and methods that we consider the most important. The flow chart in fig. 2 explains the CRISP-ML(Q) approach for quality assurance. Requirements and constraints define the objectives of a generic phase, instantiate specific steps and tasks and identify risks, that can affect the efficiency and success of the ML application. If risks aren't feasible, appropriate quality assurance methods are chosen to mitigate risks in an iterative approach using guidelines and checklists. While general risk management has diverse disciplines [19] , this approach focuses on risks that affect the efficiency and success of the ML application and require technical tasks for risk mitigation. Note that the processes and quality measures in this document are not designed for safety-critical systems. Safety-critical systems might require different or additional processes and quality measures. yes Instantiate step and task Identify risks Define requirements and constraints Choose quality assurance method Start CRISP-ML(Q) phase Mitigate risks no risks feasable ? Phase finished ? no yes Start next phase Business and Data Understanding The initial phase is concerned with tasks to define the business objectives and translate it to ML objectives, to collect and verify the data quality and to finaly assess the project feasibility. Define the Scope of the ML Application CRISP-DM names the data scientist responsible to define the scope of the project. However, in daily business, the separation of domain experts and data scientists carries the risk, that the application will not satisfy the business needs. Moreover, the availability of training samples will to a large extent influence the feasibility of the data-based application [28] . It is, therefore, best practice to merge the requirements of the business unit with ML requirements while keeping in mind data related constraints in a joint step. Success Criteria We propose to measure the success criteria of a ML project on three different levels: the business success criteria, the ML success criteria and the economic success criteria. According to the IEEE standard for developing software life cycle processes [19] , the requirement measurable is one of the essential principles of quality assurance methodology. In addition, each success criterion has to be defined in alignment to each other and with respect to the overall system requirements [32] to prevent contradictory objectives. Business Success Criteria: Define the purpose and the success criteria of the ML application from a business point of view. For example, if an ML application is planned for a quality check in production and is supposed to outperform the current manual failure rate of 3%, the business success criterion could be derived as e.g. \"failure rate less than 3%\". ML Success Criteria: Translate the business objective into ML success criteria (see table 2 ). It is advised to define a minimum acceptable level of performance to meet the business goals (e.g. for a Minimal Viable Product (MVP)) In the mentioned example, the minimal success criterion is defined as \"accuracy greater 97%\", but data scientists might optimize further, for example, the true-positive rate to miss items with quality issues. Economic Success Criteria: It is best practice to add an economic success criterion in the form of a Key Performance Indicator (KPI) to the project. A KPI is an economical measure for the relevance of the ML application. In the mentioned example, a KPI can be defined as \"cost savings with automated quality check per part\". Feasibility Checking the feasibility before setting up the project is considered best practice for the overall success of the ML approach [33] and can minimize the risk of premature failures due to false expectations. A feasibility test of the ML application should assess the situation and whether further development should be pursued. It is crucial, that the assessment includes the availability, size and quality of the training sample set. In practice, a major source of project delays is the lack of data availability (see section 3.1.4). A small sample size carries the risk of low performance on out-of-sample data [34] . The risk might be mitigated by e.g. adding domain knowledge or increasing data quality. However, if the sample size is not sufficient the ML project should be terminated or put on hold at this stage. Applicability of ML technology: Literature search for either similar applications on a similar domain or similar methodological approaches on a different domain could assess the applicability of the ML technology. It is common to demonstrate the feasibility of a ML application with a proof of concept (PoC) when the ML algorithm is used for the first time in a specific domain. If a PoC already exists, setting up a software project that focuses on the deployment directly is more efficient, e.g. in case of yet another price estimation of used cars [35] . Legal constraints: It is beyond the scope of this paper to discuss legal issues but they are essential for any business application [36, 37] . Legal constraints are frequently augmented by ethical and social considerations like fairness and trust [38] [39] [40] . Requirements on the application: The success criteria that have been defined in section 3.1.2 have to be augmented with requirements that arise from running the application in the target domain or if not accessible an assumed target domain [32] . The requirements include robustness, scalability, explainability and resource demand and are used for the development and verification in later phases (see section 3.3). The challenge during the development is to optimize the success criteria while not violating the requirements and constraints. Data Collection Costs and time is needed to collect a sufficient amount of consistent data by preparing and merging data from different sources and different formats (see section 3.2). A ML project might be delayed until the data is collected or could even be stopped if the collection of data of sufficient quality (see section 3.1.5) is not feasible. Data version control: Collecting data is not a static task but rather an iterative task. Modification on the data set (see section 3.2) should be documented to mitigate the risk of obtaining irreproducible or wrong results. Version control on the data is one of the essential tools to assure reproducibility and quality as it allows to track errors and unfavorable modifications during the development. Data Quality Verification The following three tasks examine whether the business and ML objectives can be achieved with the given quality of the available data. A ML project is doomed to fail if the data quality is poor. The lack of a certain data quality will trigger the previous data collection task (see section 3.1.4). Data description: The data description forms the basis for the data quality verification. A description and an exploration of the data is performed to gain insight about the underlying data generation process. The data should be described on a meta-level and by their statistical properties. Furthermore, a technically well funded visualization of the data should help to understand the data generating process [41] . Information about format, units and description of the input signals is expanded by domain knowledge. Data requirements: The data requirements can be defined either on the meta-level or directly in the data and should state the expected conditions of the data, i.e. whether a certain sample is plausible. The requirements can be, e.g., the expected feature values (a range for continuous features or a list for discrete features), the format of the data and the maximum number of missing values. The bounds of the requirements has to be defined carefully to include all possible real world values but discard non-plausible data. Data that does not satisfy the expected conditions could be treated as anomalies and have to be evaluated manually or excluded automatically. To mitigate the risk of anchoring bias in the definition phase discussing the requirements with a domain expert is advised [29] . Documentation of the data requirements could be done in the form of a schema [42, 43] . Data verification: The initial data, added data but also the production data has to be checked according to the requirements (see section 3.6). In cases where the requirements are not met, the data will be discarded and stored for further manual analysis. This helps to reduce the risk of decreasing the performance of the ML application through adding low-quality data and helps to detect varying data distributions or unstable inputs. To mitigate the risk of insufficient representation of extreme cases, it is best practice to use data exploration techniques to investigate the sample distribution. Review of Output Documents The Business & Data Understanding phase delivers the scope for the development (section 3.1.3), the success criteria (section 3.1.2) of a ML application and a data quality verification report (section 3.1.5) to approve the feasibility of the project. The output documents need to be reviewed to rank the risks and define the next tasks. If certain quality criteria are not met, re-iterations of previous tasks are possible. Data Preparation Building on the experience from the preceding data understanding phase, data preparation serves the purpose of producing a data set for the subsequent modeling phase. However, data preparation is not a static phase and backtracking circles from later phases are necessary if, for example, the modeling phase or the deployment phase reveal erroneous data. To path the way towards ML lifecycle in a later phase, methods for data preparation that are suitable for automation as demonstrated by Fischer et al. [8] are preferable. Select Data Feature selection: Selecting a good data representation based on the available measurements is one of the challenges to assure the quality of the ML application. It is best practice to discard underutilized features as they provide little to none modeling benefit but offer possible loopholes for errors i.e. instability of the feature during the operation of the ML application [30] . In addition, the more features are selected the more samples are necessary. Intuitively an exponentially increasing number of samples for an increasing number of features is required to prevent the data from becoming sparse in the feature space. This is termed as the curse of dimensionality [44, 45] . Thus, it is best practice to select just necessary features. A checklist for the feature selection task is given in [46] . Note that data often forms a manifold of lower dimensions in the feature space and models have to learn this respectively [47] . Feature selection methods can be separated into three categories: 1) filter methods select features from data without considering the model, 2) wrapper methods use a learning model to evaluate the significance of the features and 3) embedded methods combines the feature selection and the classifier construction steps. A detailed explanation and in-depth analysis on the feature selection problem are given in [48] [49] [50] [51] . Feature selection could carry the risk of selection bias but could be reduced when the feature selection is performed within the cross-validation of the model (see section 3.3) to account for all possible combinations [52] . However, the selection of the features should not be relied purely on the validation and test error but should be analyzed by a domain expert as potential biases might occur due to spurious correlation in the data. Lapuschkin et al. [53, 54] showed that classifiers could exploit spurious correlations, here the copyright tag on the horse class, to obtain a remarkable test performance and, thus, fakes a false sense of generalization. In such cases, explanation methods [55] could be used to highlight the significance of features (see section 3.4) and analyzed from a human's perspective. Data selection: Discarding samples should be well documented and strictly based on objective quality criteria. However, certain samples might not satisfy the necessary quality i.e. doesn't satisfy the requirements defined in section 3.1.5 and are not plausible and, thus, should be removed from the data set. Unbalanced Classes: In cases of unbalanced classes, where the number of samples per class is skewed, different sampling strategies could improve the results. Over-sampling of the minority class and/or under-sampling of the majority class [56] [57] [58] [59] have been used. Over-sampling increases the importance of the minority class but could result in overfitting on the minority class. Under-Sampling by removing data points from the majority class has to be done carefully to keep the characteristics of the data and reduce the chance of introducing biases. However, removing points close to the decision boundary or multiple data points from the same cluster should be avoided. Comparing the results of different sampling techniques' reduces the risk of introducing bias to the model. Clean Data Noise reduction: The gathered data often includes, besides the predictive signal, noise and unwanted signals from other sources. Signal processing filters could be used to remove the irrelevant signals from the data and improve the signal-to-noise ratio [60, 61] . However, filtering the data should be documented and evaluated because of the risk that an erroneous filter could remove important parts of the signal in the data. Data imputation: To get a complete data set, missing, NAN and special values could be imputed with a model readable value. Depending on the data and ML task the values are imputed by mean or median values, interpolated, replaced by a special value symbol [62] (as the pattern of the values could be informative), substituted by model predictions [63] , matrix factorization [64] or multiple imputations [65] [66] [67] or imputed based on a convex optimization problem [68] . To reduce the risk of introducing substitution artifacts, the performance of the model should be compared between different imputation techniques. Construct Data Feature engineering: New features could be derived from existing ones based on domain knowledge. This could be, for example, the transformation of the features from the time domain into the frequency domain, discretization of continuous features into bins or augmenting the features with additional features based on the existing ones. In addition, there are several generic feature construction methods, such as clustering [69] , dimensional reduction methods such as Kernel-PCA [70] or auto-encoders [71] . Nominal features and labels should be transformed into a one-hot encoding while ordinal features and labels are transformed into numerical values. However, the engineered features should be compared against a baseline to assess the utility of the feature. Underutilized features should be removed. Models that construct the feature representation as part of the learning process, e.g. neural networks, avoid the feature engineering steps [72] . Data augmentation: Data augmentation utilizes known invariances in the data to perform a label preserving transformation to construct new data. The transformations could either be performed in the feature space [57] or input space, such as applying rotation, elastic deformation or Gaussian noise to an image [73] . Data could also be augmented on a meta-level, such as switching the scenery from a sunny day to a rainy day. This expands the data set with additional samples and allows the model to capture those invariances. Standardize Data File format: Some ML tools require specific variable or input types (data syntax). Indeed in practice, the comma separated values (CSV) format is the most generic standard (RFC 4180). ISO 8000 recommends the use of SI units according to the International System of Quantities. Defining a fix set of standards and units, helps to avoid the risks of errors in the merging process and further in detecting erroneous data (see section 3.1.5). Normalization: Without proper normalization, the features could be defined on different scales and might lead to strong bias to features on larger scales. In addition, normalized features lead to faster convergence rates in neural networks than without [74, 75] . Note that the normalization, applied to the training set has to be applied also to the test set using the same normalization parameters. Modeling The choice of modeling techniques depends on the ML and the business objectives, the data and the boundary conditions of the project the ML application is contributing to. The requirements and constraints that have been defined in section 3.1 are used as inputs to guide the model selection to a subset of appropriate models. The goal of the modeling phase is to craft one or multiple models that satisfy the given constraints and requirements. An outline of the modeling phase is depicted in fig. 3 . Universe of all models Subset of appropriate models Performance Robustness Scalability Explainability Model Complexity Resource Demand Ra nk ing Model Figure 3 . An outline of the modeling phase. Only a subset of models fulfill the constraints and requirements defined in section 3.1 and section 3.1.3 and have to be evaluated using quality measures (see table 2 ) . Literature research on similar problems: It is best practice to screen the literature (e.g. publications, patents, internal reports) for a comprehensive overview on similar ML tasks, since ML has become an established tool for a wide number of applications. New models can be based on published insights and previous results can serve as performance baselines. Define quality measures of the model: The modeling strategy has to have multiple objectives in mind [76] . We suggest to evaluate the models on at least six complementary properties (see fig. 3 ). Besides a performance metric, soft measures such as robustness, explainability, scalability, resource demand and model complexity have to be evaluated (see table 2 ). The measures can be weighted differently depending on the application. In practical application, explainability [34, 77, 78] or robustness might be valued more than accuracy. Additionally, the model's fairness [38, 39] or trust might have to be assessed and mitigated. Performance The model's performance on unseen data. Robustness The model's resiliency to inconsistent inputs and to failures in the execution environment. Scalability The model's ability to scale to high data volume in the production system. Explainability The model's direct or post-hoc explainability. Model Complexity The model's capacity should suit the data complexity. Resource Demand The model's resource demand for deployment. Table 2: Quality measure of machine learning models Model Selection: There are plenty of ML models and introductory books on classical methods [45, 79] and Deep Learning [72] can be used to compare and understand their characteristics. The model selection depends on the data and has to be tailored to the problem. There is no such model that performs the best on all problem classes (No Free Lunch Theorem for ML [80] ). It is best practice to start with models of lower capacity, which can serve as baseline, and gradually increase the capacity. Validating each step assures its benefit and avoid unnecessary complexity of the model. Incorporate domain knowledge: In practice, a specialized model for a specific task performs better than a general model for all possible tasks. However, adapting the model to a specific problem involves the risk of incorporating false assumption and could reduce the solution space to a non-optimal subset. Therefore, it is best practice to validate the incorporated domain knowledge in isolation against a baseline. Adding domain knowledge should always increase the quality of the model, otherwise, it should be removed to avoid false bias. Model training: The trained model depends on the learning problem and as such are tightly coupled. The learning problem contains an objective, optimizer, regularization and cross-validation [45, 72] . The objective of the learning problem depends on the application. Different applications value different aspects and have to be tweaked in alignment with the business success criteria. The objective is a proxy to evaluate the performance of the model. The optimizer defines the learning strategy and how to adapt the parameters of the model to improve the objective. Regularization which can be incorporated in the objective, optimizer and in the model itself is needed to reduce the risk of overfitting and can help to find unique solutions. Cross-validation is performed for feature selection, to optimize the hyper-parameters of the model and to test its generalization property to unseen data [81] . Cross-validation [45] is based on a splitting of historical data in training, validation and testing data, where the latter is used as a proxy for the target environment [82] . Frameworks such as Auto-ML [83, 84] or Neural Architecture Search [85] enables to partly automatize the hyper-parameters optimization and the architecture search. Using unlabeled data and pre-trained models: Labeling data could be very expensive and might limit the available data set size. Unlabeled data might be exploited in the training process, e.g. by performing unsupervised pre-training [86, 87] and semi-supervised learning algorithms [88, 89] . Complementary, transfer learning could be used to pre-train the network on a proxy data (e.g. from simulations) that resembles the original data to extract common features [90] . Model Compression: Compression or pruning methods could be used to obtain a more compact model. In kernel methods low rank approximations of the kernel matrix is an essential tool to tackle large scale learning problems [91, 92] . Neural Networks use a different approach [93] by either pruning the network weights [94] or applying a compression scheme on the network weights [95] . Ensemble methods: Ensemble methods train multiple models to perform the decision based on the aggregate decisions of the individual models. The models could be of different types or multiple instantiations of one type. This results in a more fault-tolerant system as the error of one model could be absorbed by the other models. Boosting, Bagging or Mixture of Experts are mature techniques to aggregate the decision of multiple models [96] [97] [98] . In addition, ensemble models are used to compute uncertainty estimates and can highlight areas of low confidence [99, 100] . Assure reproducibility A major principle of scientific methods and the characteristics of robust ML applications is reproducibility. However, ML models are difficult to reproduce due to the mostly non-convex and stochastic training procedures and randomized data splits. It has been proposed to distinguish reproducibility on two different levels. First, one has to assure that the method itself is reproducible and secondly its results [101] . Method reproducibility: This task aims at reproducing the model from an extensive description or sharing of the used algorithm, data set, hyper-parameters and runtime environment (e.g. software versions, hardware and random seeds [102] ). The algorithm should be described in detail i.e. with (pseudo) code and on the meta-level including the assumptions. Result reproducibility: It is best practice to validate the mean performance and assess the variance of the model on different random seeds [103, 104] . Reporting only the top performance of the model [103, 105] is common but dubious practice. Large performance variances indicate the sensitivity of the algorithm and question the robustness of the model. Experimental Documentation: Keeping track of the changed model's performance and its causes by precedent model modifications allows model comprehension by addressing which modifications were beneficial and improve the overall model quality. The documentation should contain the listed properties in the method reproducibility task. Tool-based approach on version control and meta-data handling while experimenting on ML models and hyper-parameters exist [106] . Evaluation Validate performance: A risk occurs when information from a test set leak into the validation or even training set. Hence, it is best practice to hold back an additional test set, which is disjoint from the validation and training set, stored only for a final evaluation and never shipped to any partner to be able to measure the performance metrics (blind-test). The test set should be assembled and curated with caution and ideally by a team of experts that are capable to analyze the correctness and ability to represent real cases. In general, the test set should cover the whole input distribution and consider all invariances, e.g. transformations of the input that do not change the label, in the data. Another major risk is that the test set cannot cover all possible inputs due to the large input dimensionality or rare corner cases, i.e. inputs with low probability of occuring [107] [108] [109] . Extensive testing reduces this risk [82] . It is recommended to separate the teams and the procedures collecting the training and the test data to erase dependencies and avoid methodology dependence. Additionally, it is recommended to perform a sliced performance analysis to highlight weak performance on certain classes or time slices. Determine robustness: A major risk occurs if ML applications are not robust to perturbed, e.g. noisy or wrong, or even designed adversarial input data as show by Chan-Hon-Tong [110] . This requires methods to statistically estimate the model's local and global robustness. One approach is adding different kinds of noisy or falsified input to the data or varying the hyperparameters to characterize the model's generalization ability. Formal verification approaches [32] and robustness validation methods using cross-validation techniques on historical data [82] exist. The model's robustness should match the quality claims made in table 2 . Increase explainability for ML practitioner & end user: Explainability of a model helps to find errors and allows strategies, e.g. by enriching the data set, to improve the overall performance [111] . In practice, inherently interpretable models are not necessary inferior to complex models in case of structured input data with meaningful features [77] . Thrun et al. [112] show the advantages of the glass box model Explainable Boosting Machine, that visualizes feature-wise contributions to the predictions at comparable performance to common models. To achieve explainability and gain a deeper understanding of what a model has already learned and to avoid spurious correlations [54] , it is best practice to carefully observe the features which impact the model's prediction the most and check whether they are plausible from a domain experts' point of view [113] [114] [115] . Moreover, case studies have shown that explainability helps to increase trust and users' acceptance [116] and could guide humans in ML assisted decisions [78] . Unified frameworks to explore model explainabilty are available (e.g. [117, 118] ). Compare results with defined success criteria: Finally, domain and ML experts have to decide if the model can be deployed. Therefore, it is best practice to document the results of the evaluation phase and compare them to the business and ML success criteria defined in section 3.1.2. If the success criteria are not met, one might backtrack to earlier phases (modeling or even data preparation) or stop the project. Identified limitations of robustness and explainability during evaluation might require an update of the risk assessment (e.g. Failure Mode and Effects Analysis (FMEA)) and might also lead to backtracking to the modeling phase or stopping the project. Deployment The deployment phase of a ML model is characterized by its practical use in the designated field of application. Define inference hardware: Choose the hardware based on the requirements defined in section 3.1.3 or align with an existing hardware. While cloud services offer scalable computation resources, embedded system have hard constraints. ML specific options are e.g. to optimize towards the target hardware [119] regarding CPU and GPU availability, to optimize towards the target operation system (demonstrated for Android and iOS by Sehgal and Kehtarnavaz [120] ) or to optimize the ML workload for a specific platform [121] . Monitoring and maintenance (see section 3.6) have to be considered in the overall architecture. Model evaluation under production condition: The risk persists that the production data does not resemble the training data. Previous assumptions on the training data might not hold in production and the hardware that gathered the data might differ. Therefore it is best practice to evaluate the performance of the model under incrementally increasing production conditions by iteratively running the tasks in section 3.4. On each incremental step, the model has to be calibrated to the deployed hardware and the test environment. This allows identifying wrong assumptions on the deployed environment and the causes of model degradation. Domain adaptation techniques can be applied [122, 123] to enhance the generalization ability of the model. This step will also give a first indication whether the business and economic success criteria, which was defined in section 3.1.2, could be met. Assure user acceptance and usability: Even after passing all evaluation steps, there might be the risk that the user acceptance and the usability of the model is underwhelming. The model might be incomprehensible and or does not cover corner cases. It is best practice to build a prototype and run an field test with end users [82] . Examine the acceptance, usage rate and the user experience. A user guide and disclaimer shall be provided to the end users to explain the system's functionality and limits. Minimize the risks of unforeseen errors: The risks of unforeseen errors and outage times could cause system shutdowns and a temporary suspension of services. This could lead to user complaints and the declining of user numbers and could reduce the revenue. A fall-back plan, that is activated in case of e.g. erroneous model updates or detected bugs, can help to tackle the problem. Options are to roll back to a previous version, a pre-defined baseline or a rule-based system. A second option to counteract unforeseen errors is to implement software safety cages that control and limit the outputs of the ML application [124] or even learn safe regions in the state space [125] . Deployment strategy: Even though the model is evaluated rigorously during each previous step, there is the risk that errors might be undetected through the process. Before rolling out a model, it is best practice to setup an e.g. incremental deployment strategy that includes a pipeline for models and data [76, 126] . When cloud architectures are used, strategies can often be aligned on general deployment strategies for cloud software applications [127] . The impact of such erroneous deployments and the cost of fixing errors should be minimized. Monitoring and Maintenance With the expansion from knowledge discovery to data-driven applications to infer real-time decisions, ML models are used over a long period and have a life cycle which has to be managed. The risk of not maintaining the model is the degradation of the performance over time which leads to false predictions and could cause errors in subsequent systems. The main reason for a model to become impaired over time is rooted in the violation of the assumption that the training data and the input data for inference come from the same distribution. The causes of the violations are: The hardware that the model is deployed on and the sensor hardware will age over time. Wear parts in a system will age and friction characteristics of the system might change. Sensors get noisier or fail over time. This will shift the domain of the system and has to be adapted by the model or by retraining it. • System updates: Updates on the software or hardware of the system can cause a shift in the environment. For example, the units of a signal got changed during an update. Without notifications, the model would use this scaled input to infer false predictions. After the underlying problem is known, we can formulate the necessary methods to circumvent stale models and assure the quality. We propose two sequential tasks in the maintenance phase to assure or improve the quality of the model. In the monitor task, the staleness of the model is evaluated and returns whether the model has to be updated or not. Afterward, the model is updated and evaluated to gauge whether the update was successful. Monitor: Baylor et al. [76] proposes to monitor all input signals and notify when an update has occurred. Therefore, statistics of the incoming data and the predicted labels can be compared to the statistics of the training data. Complementary, the schema defined in section 3.1.5 can be used to validate the correctness of the incoming data. Inputs that do not satisfy the schema can be treated as anomalies and denied by the model [76] . Libraries exist to help implementing an automatic data validation system [43] . If the labels of the incoming data are known e.g. in forecasting tasks, the performance of the model can be directly monitored and recorded. An equal approach can be applied to the outputs of the model that underlie a certain distribution if environment conditions are stable and can give an estimate on the number of actions performed when interacting with an environment [30] . The monitoring phase also includes a comparison of the performance with the defined success criteria. Based on the monitoring results, it can be decided upon whether the model should be updated e.g. if input signals change significantly, the number of anomalies reaches a certain threshold or the performance has reached a lower bound. The decision whether the model has to be updated should consider the costs of updating the model and the costs resulting from erroneous predictions due to stale models. Update: In the updating step, new data is collected to re-train the model under the changed data distribution. Consider that new data has to be labeled which could be very expensive. Instead of training a completely new model from scratch, it is advised to fine-tune the existing model to new data. It might be necessary to perform some of the modeling steps in section 3.3 to cope with the changing data distribution. Every update step has to undergo a new evaluation (section 3.4) before it can be deployed. The performance of the updated model should be compared against the previous versions and could give insights on the time scale of model degradation. It should be noted, that ML systems might influence their own behavior during updates due to direct, e.g. by influencing its future training data selection, or indirect, e.g. via interaction through the world, feedback loops [30] . The risk of positive feedback loops causing system instability has to be addressed e.g. by not only monitoring but limiting the actions of the model. In addition, as part of the deployment strategy, a module is needed that tracks the application usage and performance and handles several deployment strategies like A/B testing [76, 126] . The module can e.g. be set up in form of a microservice [23] or a directed graph [128] . To reduce the risk of serving erroneous models, an automatic or human controlled fallback to a previous model needs to be implemented. The automation of the update strategy can be boosted up to a continuous training and continuous deployment of the ML application [76] while covering the defined quality assurance methods. Discussion We have introduced CRISP-ML(Q), a process model for ML applications with quality assurance methodology, that helps organizations to increase efficiency and the success rate in their ML projects. It guides ML practitioners through the entire ML development life-cycle, stepping into the phases and tasks of the iterative process including maintenance and monitoring. Whenever tasks specific risks can be identified, we provide quality-oriented methods to mitigate those risks. All methods provided are considered best practices in ML projects in industry and academia. Our survey is indicative of the existence of specialist literature, but its contributions are not covered in ML textbooks and are not part of the academic curriculum. Hence, novices to industry practice often lack a profound state-of-the-art knowledge to mitigate risks and ensure project success. Stressing quality assurance methodology is particularly important because many ML practitioners focus solely on improving the predictive performance. Conclusions An important future step on the basis of our and related work is the standardization of a process model. This would contribute to more successful ML projects and thus would have a major impact on the ML community [14] . Note that the process and quality measures in this work are not designed for safety-relevant systems. Their study and and the discussion of legal constrains are left to future work. We encourage industry from automotive and other domains to implement CRISP-ML(Q) in their machine learning applications and contribute their knowledge to establish a CRoss-Industry Standard Process model for the development of machine learning applications with Quality assurance methodology in the future. Figure1. Difference between data mining processes and machine learning applications. A) In the data mining process information is directly extracted from data to find pattern und gain knowledge. B) A machine learning application consists of two steps. A machine learning model on data is trained and applied to perform inference on new data. Note that the model itself can be studied to gain insight within a knowledge discovery process. Figure 2 . 2 Figure 2. Illustration of the CRISP-ML(Q) approach for quality assurance. The flow chart shows the instantiation of one specific tasks in a development phase, and the dedicated steps to identify and mitigate risks. • Non-stationary data distribution: Data distributions change over time and result in a stale training set and, thus, the characteristics of the data distribution are represented incorrectly by the training data. Either a shift in the features and/or in the labels are possible. This degrades the performance of the model over time. The frequency of the changes depends on the domain. Data of the stock market are very volatile whereas the visual properties of elephants won't change much over the next years. • Degradation of hardware:"
}