{
  "title": [
    {
      "sentence": "Theoretical Models of Learning to Learn *",
      "tokens": [
        "Theoretical",
        "Models",
        "of",
        "Learning",
        "to",
        "Learn",
        "*"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "A Machine can only learn if it is biased in some way.",
      "tokens": [
        "A",
        "Machine",
        "can",
        "only",
        "learn",
        "if",
        "it",
        "is",
        "biased",
        "in",
        "some",
        "way",
        "."
      ]
    },
    {
      "sentence": "Typically the bias is supplied by hand, for example through the choice of an appropriate set of features.",
      "tokens": [
        "Typically",
        "the",
        "bias",
        "is",
        "supplied",
        "by",
        "hand",
        ",",
        "for",
        "example",
        "through",
        "the",
        "choice",
        "of",
        "an",
        "appropriate",
        "set",
        "of",
        "features",
        "."
      ]
    },
    {
      "sentence": "However, if the learning machine is embedded within an environment of related tasks, then it can learn its own bias by learning sufficiently many tasks from the environment [4, 6] .",
      "tokens": [
        "However",
        ",",
        "if",
        "the",
        "learning",
        "machine",
        "is",
        "embedded",
        "within",
        "an",
        "environment",
        "of",
        "related",
        "tasks",
        ",",
        "then",
        "it",
        "can",
        "learn",
        "its",
        "own",
        "bias",
        "by",
        "learning",
        "sufficiently",
        "many",
        "tasks",
        "from",
        "the",
        "environment",
        "[",
        "4",
        ",",
        "6",
        "]",
        "."
      ]
    },
    {
      "sentence": "In this paper two models of bias learning (or equivalently, learning to learn) are introduced and the main theoretical results presented.",
      "tokens": [
        "In",
        "this",
        "paper",
        "two",
        "models",
        "of",
        "bias",
        "learning",
        "(",
        "or",
        "equivalently",
        ",",
        "learning",
        "to",
        "learn",
        ")",
        "are",
        "introduced",
        "and",
        "the",
        "main",
        "theoretical",
        "results",
        "presented",
        "."
      ]
    },
    {
      "sentence": "The first model is a PAC-type model based on empirical process theory, while the second is a hierarchical Bayes model.",
      "tokens": [
        "The",
        "first",
        "model",
        "is",
        "a",
        "PAC-type",
        "model",
        "based",
        "on",
        "empirical",
        "process",
        "theory",
        ",",
        "while",
        "the",
        "second",
        "is",
        "a",
        "hierarchical",
        "Bayes",
        "model",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Introduction Hume's analysis [10] shows that there is no a priori basis for induction.",
      "tokens": [
        "Introduction",
        "Hume",
        "'s",
        "analysis",
        "[",
        "10",
        "]",
        "shows",
        "that",
        "there",
        "is",
        "no",
        "a",
        "priori",
        "basis",
        "for",
        "induction",
        "."
      ]
    },
    {
      "sentence": "In a machine learning context, this means that a learner must be biased in some way for it to generalise well [11] .",
      "tokens": [
        "In",
        "a",
        "machine",
        "learning",
        "context",
        ",",
        "this",
        "means",
        "that",
        "a",
        "learner",
        "must",
        "be",
        "biased",
        "in",
        "some",
        "way",
        "for",
        "it",
        "to",
        "generalise",
        "well",
        "[",
        "11",
        "]",
        "."
      ]
    },
    {
      "sentence": "Typically such bias is introduced by hand through the skill and insights of experts, but despite many notable successes, this process is limited by the experts' abilities.",
      "tokens": [
        "Typically",
        "such",
        "bias",
        "is",
        "introduced",
        "by",
        "hand",
        "through",
        "the",
        "skill",
        "and",
        "insights",
        "of",
        "experts",
        ",",
        "but",
        "despite",
        "many",
        "notable",
        "successes",
        ",",
        "this",
        "process",
        "is",
        "limited",
        "by",
        "the",
        "experts",
        "'",
        "abilities",
        "."
      ]
    },
    {
      "sentence": "Hence a desirable goal is to find ways of automatically learning the bias.",
      "tokens": [
        "Hence",
        "a",
        "desirable",
        "goal",
        "is",
        "to",
        "find",
        "ways",
        "of",
        "automatically",
        "learning",
        "the",
        "bias",
        "."
      ]
    },
    {
      "sentence": "Bias learning is a form of learning to learn, and the two expressions will be used interchangeably throughout this document.",
      "tokens": [
        "Bias",
        "learning",
        "is",
        "a",
        "form",
        "of",
        "learning",
        "to",
        "learn",
        ",",
        "and",
        "the",
        "two",
        "expressions",
        "will",
        "be",
        "used",
        "interchangeably",
        "throughout",
        "this",
        "document",
        "."
      ]
    },
    {
      "sentence": "The purpose of this chapter is to present an overview of two models of supervised bias learning.",
      "tokens": [
        "The",
        "purpose",
        "of",
        "this",
        "chapter",
        "is",
        "to",
        "present",
        "an",
        "overview",
        "of",
        "two",
        "models",
        "of",
        "supervised",
        "bias",
        "learning",
        "."
      ]
    },
    {
      "sentence": "The first [4, 3] is based on Empirical Process theory (henceforth the EP model) and the second [6] is based on Bayesian inference and information theory (henceforth the Bayes model).",
      "tokens": [
        "The",
        "first",
        "[",
        "4",
        ",",
        "3",
        "]",
        "is",
        "based",
        "on",
        "Empirical",
        "Process",
        "theory",
        "(",
        "henceforth",
        "the",
        "EP",
        "model",
        ")",
        "and",
        "the",
        "second",
        "[",
        "6",
        "]",
        "is",
        "based",
        "on",
        "Bayesian",
        "inference",
        "and",
        "information",
        "theory",
        "(",
        "henceforth",
        "the",
        "Bayes",
        "model",
        ")",
        "."
      ]
    },
    {
      "sentence": "Empirical process theory is a general theory that includes the analysis of pattern classification first introduced by Vapnik and Chervonenkis [13, 12] .",
      "tokens": [
        "Empirical",
        "process",
        "theory",
        "is",
        "a",
        "general",
        "theory",
        "that",
        "includes",
        "the",
        "analysis",
        "of",
        "pattern",
        "classification",
        "first",
        "introduced",
        "by",
        "Vapnik",
        "and",
        "Chervonenkis",
        "[",
        "13",
        ",",
        "12",
        "]",
        "."
      ]
    },
    {
      "sentence": "Note that these are models of supervised bias learning and as such have little to say about learning to learn in a reinforcement learning setting.",
      "tokens": [
        "Note",
        "that",
        "these",
        "are",
        "models",
        "of",
        "supervised",
        "bias",
        "learning",
        "and",
        "as",
        "such",
        "have",
        "little",
        "to",
        "say",
        "about",
        "learning",
        "to",
        "learn",
        "in",
        "a",
        "reinforcement",
        "learning",
        "setting",
        "."
      ]
    },
    {
      "sentence": "In this introduction a high level overview of the features common to both models will be presented, and then in later sections the details and main results of each model will be discussed.",
      "tokens": [
        "In",
        "this",
        "introduction",
        "a",
        "high",
        "level",
        "overview",
        "of",
        "the",
        "features",
        "common",
        "to",
        "both",
        "models",
        "will",
        "be",
        "presented",
        ",",
        "and",
        "then",
        "in",
        "later",
        "sections",
        "the",
        "details",
        "and",
        "main",
        "results",
        "of",
        "each",
        "model",
        "will",
        "be",
        "discussed",
        "."
      ]
    },
    {
      "sentence": "In ordinary models of machine learning the learner is presented with a single task.",
      "tokens": [
        "In",
        "ordinary",
        "models",
        "of",
        "machine",
        "learning",
        "the",
        "learner",
        "is",
        "presented",
        "with",
        "a",
        "single",
        "task",
        "."
      ]
    },
    {
      "sentence": "Learning the \"right bias\" in such a model does not really make sense, because the ultimate bias is one which completely solves the task.",
      "tokens": [
        "Learning",
        "the",
        "``",
        "right",
        "bias",
        "''",
        "in",
        "such",
        "a",
        "model",
        "does",
        "not",
        "really",
        "make",
        "sense",
        ",",
        "because",
        "the",
        "ultimate",
        "bias",
        "is",
        "one",
        "which",
        "completely",
        "solves",
        "the",
        "task",
        "."
      ]
    },
    {
      "sentence": "Thus in single-task learning, bias learning or learning to learn is the same as learning.",
      "tokens": [
        "Thus",
        "in",
        "single-task",
        "learning",
        ",",
        "bias",
        "learning",
        "or",
        "learning",
        "to",
        "learn",
        "is",
        "the",
        "same",
        "as",
        "learning",
        "."
      ]
    },
    {
      "sentence": "In order to learn bias one has introduce extra assumptions about the learning process.",
      "tokens": [
        "In",
        "order",
        "to",
        "learn",
        "bias",
        "one",
        "has",
        "introduce",
        "extra",
        "assumptions",
        "about",
        "the",
        "learning",
        "process",
        "."
      ]
    },
    {
      "sentence": "The central assumption of both the Bayes model and the EP model of bias learning is that the learner is embedded within an environment of related problems.",
      "tokens": [
        "The",
        "central",
        "assumption",
        "of",
        "both",
        "the",
        "Bayes",
        "model",
        "and",
        "the",
        "EP",
        "model",
        "of",
        "bias",
        "learning",
        "is",
        "that",
        "the",
        "learner",
        "is",
        "embedded",
        "within",
        "an",
        "environment",
        "of",
        "related",
        "problems",
        "."
      ]
    },
    {
      "sentence": "The learner's task is to find a bias that is appropriate for the entire environment, not just for a single task.",
      "tokens": [
        "The",
        "learner",
        "'s",
        "task",
        "is",
        "to",
        "find",
        "a",
        "bias",
        "that",
        "is",
        "appropriate",
        "for",
        "the",
        "entire",
        "environment",
        ",",
        "not",
        "just",
        "for",
        "a",
        "single",
        "task",
        "."
      ]
    },
    {
      "sentence": "A simple example of an environment of learning problems with a common bias is handwritten character recognition.",
      "tokens": [
        "A",
        "simple",
        "example",
        "of",
        "an",
        "environment",
        "of",
        "learning",
        "problems",
        "with",
        "a",
        "common",
        "bias",
        "is",
        "handwritten",
        "character",
        "recognition",
        "."
      ]
    },
    {
      "sentence": "A preprocessing stage that identifies and removes any (small) rotations, dilations and translations of an image of a character will be advantageous for recognising all characters.",
      "tokens": [
        "A",
        "preprocessing",
        "stage",
        "that",
        "identifies",
        "and",
        "removes",
        "any",
        "(",
        "small",
        ")",
        "rotations",
        ",",
        "dilations",
        "and",
        "translations",
        "of",
        "an",
        "image",
        "of",
        "a",
        "character",
        "will",
        "be",
        "advantageous",
        "for",
        "recognising",
        "all",
        "characters",
        "."
      ]
    },
    {
      "sentence": "If the set of all individual character recognition problems is viewed as an environment of learning tasks, this preprocessor represents a bias that is appropriate to all tasks in the environment.",
      "tokens": [
        "If",
        "the",
        "set",
        "of",
        "all",
        "individual",
        "character",
        "recognition",
        "problems",
        "is",
        "viewed",
        "as",
        "an",
        "environment",
        "of",
        "learning",
        "tasks",
        ",",
        "this",
        "preprocessor",
        "represents",
        "a",
        "bias",
        "that",
        "is",
        "appropriate",
        "to",
        "all",
        "tasks",
        "in",
        "the",
        "environment",
        "."
      ]
    },
    {
      "sentence": "Preprocessing can also be viewed as feature extraction, and there are many classes of learning problems that possess common feature sets.",
      "tokens": [
        "Preprocessing",
        "can",
        "also",
        "be",
        "viewed",
        "as",
        "feature",
        "extraction",
        ",",
        "and",
        "there",
        "are",
        "many",
        "classes",
        "of",
        "learning",
        "problems",
        "that",
        "possess",
        "common",
        "feature",
        "sets",
        "."
      ]
    },
    {
      "sentence": "For example, one can view face recognition as a collection of related learning problems, one for each possible face classifier, and it is likely that there exists sets of features that are good for learning all faces.",
      "tokens": [
        "For",
        "example",
        ",",
        "one",
        "can",
        "view",
        "face",
        "recognition",
        "as",
        "a",
        "collection",
        "of",
        "related",
        "learning",
        "problems",
        ",",
        "one",
        "for",
        "each",
        "possible",
        "face",
        "classifier",
        ",",
        "and",
        "it",
        "is",
        "likely",
        "that",
        "there",
        "exists",
        "sets",
        "of",
        "features",
        "that",
        "are",
        "good",
        "for",
        "learning",
        "all",
        "faces",
        "."
      ]
    },
    {
      "sentence": "A similar conclusion applies to other domains such as speech recognition (all the individual word classifiers may be viewed as separate learning problems possessing a common feature set), fingerprint recognition, and so on.",
      "tokens": [
        "A",
        "similar",
        "conclusion",
        "applies",
        "to",
        "other",
        "domains",
        "such",
        "as",
        "speech",
        "recognition",
        "(",
        "all",
        "the",
        "individual",
        "word",
        "classifiers",
        "may",
        "be",
        "viewed",
        "as",
        "separate",
        "learning",
        "problems",
        "possessing",
        "a",
        "common",
        "feature",
        "set",
        ")",
        ",",
        "fingerprint",
        "recognition",
        ",",
        "and",
        "so",
        "on",
        "."
      ]
    },
    {
      "sentence": "The classical approach to statistical pattern recognition in these domains is to first guess a set of features and then to learn each problem by estimating a simple (say linear) function of the features.",
      "tokens": [
        "The",
        "classical",
        "approach",
        "to",
        "statistical",
        "pattern",
        "recognition",
        "in",
        "these",
        "domains",
        "is",
        "to",
        "first",
        "guess",
        "a",
        "set",
        "of",
        "features",
        "and",
        "then",
        "to",
        "learn",
        "each",
        "problem",
        "by",
        "estimating",
        "a",
        "simple",
        "(",
        "say",
        "linear",
        ")",
        "function",
        "of",
        "the",
        "features",
        "."
      ]
    },
    {
      "sentence": "The choice of features represents the learner's bias, thus in bias learning the goal is to get the learner to learn the features instead of guessing them.",
      "tokens": [
        "The",
        "choice",
        "of",
        "features",
        "represents",
        "the",
        "learner",
        "'s",
        "bias",
        ",",
        "thus",
        "in",
        "bias",
        "learning",
        "the",
        "goal",
        "is",
        "to",
        "get",
        "the",
        "learner",
        "to",
        "learn",
        "the",
        "features",
        "instead",
        "of",
        "guessing",
        "them",
        "."
      ]
    },
    {
      "sentence": "In order to perform a theoretical analysis of bias learning, we assume the tasks in the environment are generated according to some underlying probability distribution.",
      "tokens": [
        "In",
        "order",
        "to",
        "perform",
        "a",
        "theoretical",
        "analysis",
        "of",
        "bias",
        "learning",
        ",",
        "we",
        "assume",
        "the",
        "tasks",
        "in",
        "the",
        "environment",
        "are",
        "generated",
        "according",
        "to",
        "some",
        "underlying",
        "probability",
        "distribution",
        "."
      ]
    },
    {
      "sentence": "For example, if the learner is operating in an environment where it must learn to recognise faces, the distribution over learning tasks will have its support restricted to face recognition type problems.",
      "tokens": [
        "For",
        "example",
        ",",
        "if",
        "the",
        "learner",
        "is",
        "operating",
        "in",
        "an",
        "environment",
        "where",
        "it",
        "must",
        "learn",
        "to",
        "recognise",
        "faces",
        ",",
        "the",
        "distribution",
        "over",
        "learning",
        "tasks",
        "will",
        "have",
        "its",
        "support",
        "restricted",
        "to",
        "face",
        "recognition",
        "type",
        "problems",
        "."
      ]
    },
    {
      "sentence": "The learner acquires information about the environment by sampling from this distribution to generate multiple learning problems, and then sampling from each learning problem to generate multiple training sets.",
      "tokens": [
        "The",
        "learner",
        "acquires",
        "information",
        "about",
        "the",
        "environment",
        "by",
        "sampling",
        "from",
        "this",
        "distribution",
        "to",
        "generate",
        "multiple",
        "learning",
        "problems",
        ",",
        "and",
        "then",
        "sampling",
        "from",
        "each",
        "learning",
        "problem",
        "to",
        "generate",
        "multiple",
        "training",
        "sets",
        "."
      ]
    },
    {
      "sentence": "The learner can then search for bias that is appropriate for learning all the tasks.",
      "tokens": [
        "The",
        "learner",
        "can",
        "then",
        "search",
        "for",
        "bias",
        "that",
        "is",
        "appropriate",
        "for",
        "learning",
        "all",
        "the",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "In the EP model, the learner is provided with a family of hypothesis spaces and it searches for an hypothesis space that contains good solutions to all the training sets.",
      "tokens": [
        "In",
        "the",
        "EP",
        "model",
        ",",
        "the",
        "learner",
        "is",
        "provided",
        "with",
        "a",
        "family",
        "of",
        "hypothesis",
        "spaces",
        "and",
        "it",
        "searches",
        "for",
        "an",
        "hypothesis",
        "space",
        "that",
        "contains",
        "good",
        "solutions",
        "to",
        "all",
        "the",
        "training",
        "sets",
        "."
      ]
    },
    {
      "sentence": "Such a hypothesis space can then be used to learn novel tasks drawn from the same environment.",
      "tokens": [
        "Such",
        "a",
        "hypothesis",
        "space",
        "can",
        "then",
        "be",
        "used",
        "to",
        "learn",
        "novel",
        "tasks",
        "drawn",
        "from",
        "the",
        "same",
        "environment",
        "."
      ]
    },
    {
      "sentence": "The key result of the EP model (theorem 2 in section 3) gives a bound on the number of tasks and number of examples of each task required to ensure that a hypothesis space containing good solutions to all training sets will, with high probability, contain good solutions to novel tasks drawn from the same environment.",
      "tokens": [
        "The",
        "key",
        "result",
        "of",
        "the",
        "EP",
        "model",
        "(",
        "theorem",
        "2",
        "in",
        "section",
        "3",
        ")",
        "gives",
        "a",
        "bound",
        "on",
        "the",
        "number",
        "of",
        "tasks",
        "and",
        "number",
        "of",
        "examples",
        "of",
        "each",
        "task",
        "required",
        "to",
        "ensure",
        "that",
        "a",
        "hypothesis",
        "space",
        "containing",
        "good",
        "solutions",
        "to",
        "all",
        "training",
        "sets",
        "will",
        ",",
        "with",
        "high",
        "probability",
        ",",
        "contain",
        "good",
        "solutions",
        "to",
        "novel",
        "tasks",
        "drawn",
        "from",
        "the",
        "same",
        "environment",
        "."
      ]
    },
    {
      "sentence": "This ability to learn novel tasks after seeing sufficiently many examples of sufficiently many tasks is the formal definition of learning to learn under the EP model.",
      "tokens": [
        "This",
        "ability",
        "to",
        "learn",
        "novel",
        "tasks",
        "after",
        "seeing",
        "sufficiently",
        "many",
        "examples",
        "of",
        "sufficiently",
        "many",
        "tasks",
        "is",
        "the",
        "formal",
        "definition",
        "of",
        "learning",
        "to",
        "learn",
        "under",
        "the",
        "EP",
        "model",
        "."
      ]
    },
    {
      "sentence": "The Bayes model is the same as the EP model in that the learner is assumed to be embedded within an environment of related tasks and can sample from the environment to generate multiple training sets corresponding to different tasks.",
      "tokens": [
        "The",
        "Bayes",
        "model",
        "is",
        "the",
        "same",
        "as",
        "the",
        "EP",
        "model",
        "in",
        "that",
        "the",
        "learner",
        "is",
        "assumed",
        "to",
        "be",
        "embedded",
        "within",
        "an",
        "environment",
        "of",
        "related",
        "tasks",
        "and",
        "can",
        "sample",
        "from",
        "the",
        "environment",
        "to",
        "generate",
        "multiple",
        "training",
        "sets",
        "corresponding",
        "to",
        "different",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "However, the Bayes bias learner differs in the way it uses the information from the multiple training sets.",
      "tokens": [
        "However",
        ",",
        "the",
        "Bayes",
        "bias",
        "learner",
        "differs",
        "in",
        "the",
        "way",
        "it",
        "uses",
        "the",
        "information",
        "from",
        "the",
        "multiple",
        "training",
        "sets",
        "."
      ]
    },
    {
      "sentence": "In the Bayes model, the distribution over learning tasks in the environment is interpreted as an objective prior distribution.",
      "tokens": [
        "In",
        "the",
        "Bayes",
        "model",
        ",",
        "the",
        "distribution",
        "over",
        "learning",
        "tasks",
        "in",
        "the",
        "environment",
        "is",
        "interpreted",
        "as",
        "an",
        "objective",
        "prior",
        "distribution",
        "."
      ]
    },
    {
      "sentence": "The learner does not know this distribution, but does have some idea of a set Π of possible prior distributions to which the true distribution belongs.",
      "tokens": [
        "The",
        "learner",
        "does",
        "not",
        "know",
        "this",
        "distribution",
        ",",
        "but",
        "does",
        "have",
        "some",
        "idea",
        "of",
        "a",
        "set",
        "Π",
        "of",
        "possible",
        "prior",
        "distributions",
        "to",
        "which",
        "the",
        "true",
        "distribution",
        "belongs",
        "."
      ]
    },
    {
      "sentence": "The learner starts out with a hyper-prior distribution on Π and based on the data in the training sets, updates the hyper-prior to a hyper-posterior using Bayes' rule.",
      "tokens": [
        "The",
        "learner",
        "starts",
        "out",
        "with",
        "a",
        "hyper-prior",
        "distribution",
        "on",
        "Π",
        "and",
        "based",
        "on",
        "the",
        "data",
        "in",
        "the",
        "training",
        "sets",
        ",",
        "updates",
        "the",
        "hyper-prior",
        "to",
        "a",
        "hyper-posterior",
        "using",
        "Bayes",
        "'",
        "rule",
        "."
      ]
    },
    {
      "sentence": "The hyper-posterior is then used as a prior distribution when learning novel tasks.",
      "tokens": [
        "The",
        "hyper-posterior",
        "is",
        "then",
        "used",
        "as",
        "a",
        "prior",
        "distribution",
        "when",
        "learning",
        "novel",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "In section 4 results will be presented showing how the information needed to learn each task (in a Shannon sense) decays to the minimum possible for the environment as the number of tasks and number of examples of each tasks seen already grows.",
      "tokens": [
        "In",
        "section",
        "4",
        "results",
        "will",
        "be",
        "presented",
        "showing",
        "how",
        "the",
        "information",
        "needed",
        "to",
        "learn",
        "each",
        "task",
        "(",
        "in",
        "a",
        "Shannon",
        "sense",
        ")",
        "decays",
        "to",
        "the",
        "minimum",
        "possible",
        "for",
        "the",
        "environment",
        "as",
        "the",
        "number",
        "of",
        "tasks",
        "and",
        "number",
        "of",
        "examples",
        "of",
        "each",
        "tasks",
        "seen",
        "already",
        "grows",
        "."
      ]
    },
    {
      "sentence": "Within the Bayes model, this is the formal definition of learning to learn.",
      "tokens": [
        "Within",
        "the",
        "Bayes",
        "model",
        ",",
        "this",
        "is",
        "the",
        "formal",
        "definition",
        "of",
        "learning",
        "to",
        "learn",
        "."
      ]
    },
    {
      "sentence": "Before moving on to the details of these models, it is worth pausing to assess what bias learning solves, and what it doesn't-and in a sense can never-solve.",
      "tokens": [
        "Before",
        "moving",
        "on",
        "to",
        "the",
        "details",
        "of",
        "these",
        "models",
        ",",
        "it",
        "is",
        "worth",
        "pausing",
        "to",
        "assess",
        "what",
        "bias",
        "learning",
        "solves",
        ",",
        "and",
        "what",
        "it",
        "doesn't-and",
        "in",
        "a",
        "sense",
        "can",
        "never-solve",
        "."
      ]
    },
    {
      "sentence": "On face value, being able to learn the right bias appears to violate Hume's conclusion that there can be no a priori basis for induction.",
      "tokens": [
        "On",
        "face",
        "value",
        ",",
        "being",
        "able",
        "to",
        "learn",
        "the",
        "right",
        "bias",
        "appears",
        "to",
        "violate",
        "Hume",
        "'s",
        "conclusion",
        "that",
        "there",
        "can",
        "be",
        "no",
        "a",
        "priori",
        "basis",
        "for",
        "induction",
        "."
      ]
    },
    {
      "sentence": "However this is not the case, for the bias learner learner is still fundamentally limited by the possible choices of bias available.",
      "tokens": [
        "However",
        "this",
        "is",
        "not",
        "the",
        "case",
        ",",
        "for",
        "the",
        "bias",
        "learner",
        "learner",
        "is",
        "still",
        "fundamentally",
        "limited",
        "by",
        "the",
        "possible",
        "choices",
        "of",
        "bias",
        "available",
        "."
      ]
    },
    {
      "sentence": "For example, if a learner is learning a set of features for an environment in which there are in fact no small feature sets, then any bias it comes up with (i.e.",
      "tokens": [
        "For",
        "example",
        ",",
        "if",
        "a",
        "learner",
        "is",
        "learning",
        "a",
        "set",
        "of",
        "features",
        "for",
        "an",
        "environment",
        "in",
        "which",
        "there",
        "are",
        "in",
        "fact",
        "no",
        "small",
        "feature",
        "sets",
        ",",
        "then",
        "any",
        "bias",
        "it",
        "comes",
        "up",
        "with",
        "(",
        "i.e",
        "."
      ]
    },
    {
      "sentence": "any feature set) will be a very poor bias for that environment.",
      "tokens": [
        "any",
        "feature",
        "set",
        ")",
        "will",
        "be",
        "a",
        "very",
        "poor",
        "bias",
        "for",
        "that",
        "environment",
        "."
      ]
    },
    {
      "sentence": "Thus, there is still guesswork involved in determining the appropriate way to hyper-bias the learner.",
      "tokens": [
        "Thus",
        ",",
        "there",
        "is",
        "still",
        "guesswork",
        "involved",
        "in",
        "determining",
        "the",
        "appropriate",
        "way",
        "to",
        "hyper-bias",
        "the",
        "learner",
        "."
      ]
    },
    {
      "sentence": "The main advantage of bias learning is that this hyper-bias can be much weaker than the bias: the right hyper-bias for many environments is just that there exists a set of features, whereas specifying the right bias means actually finding the features.",
      "tokens": [
        "The",
        "main",
        "advantage",
        "of",
        "bias",
        "learning",
        "is",
        "that",
        "this",
        "hyper-bias",
        "can",
        "be",
        "much",
        "weaker",
        "than",
        "the",
        "bias",
        ":",
        "the",
        "right",
        "hyper-bias",
        "for",
        "many",
        "environments",
        "is",
        "just",
        "that",
        "there",
        "exists",
        "a",
        "set",
        "of",
        "features",
        ",",
        "whereas",
        "specifying",
        "the",
        "right",
        "bias",
        "means",
        "actually",
        "finding",
        "the",
        "features",
        "."
      ]
    }
  ]
}