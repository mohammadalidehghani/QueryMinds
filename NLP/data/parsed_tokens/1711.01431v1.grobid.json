{
  "title": [
    {
      "sentence": "The Case for Meta-Cognitive Machine Learning: On Model Entropy and Concept Formation in Deep Learning",
      "tokens": [
        "The",
        "Case",
        "for",
        "Meta-Cognitive",
        "Machine",
        "Learning",
        ":",
        "On",
        "Model",
        "Entropy",
        "and",
        "Concept",
        "Formation",
        "in",
        "Deep",
        "Learning"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "Machine learning is usually defined in behaviourist terms, where external validation is the primary mechanism of learning.",
      "tokens": [
        "Machine",
        "learning",
        "is",
        "usually",
        "defined",
        "in",
        "behaviourist",
        "terms",
        ",",
        "where",
        "external",
        "validation",
        "is",
        "the",
        "primary",
        "mechanism",
        "of",
        "learning",
        "."
      ]
    },
    {
      "sentence": "In this paper, I argue for a more holistic interpretation in which finding more probable, efficient and abstract representations is as central to learning as performance.",
      "tokens": [
        "In",
        "this",
        "paper",
        ",",
        "I",
        "argue",
        "for",
        "a",
        "more",
        "holistic",
        "interpretation",
        "in",
        "which",
        "finding",
        "more",
        "probable",
        ",",
        "efficient",
        "and",
        "abstract",
        "representations",
        "is",
        "as",
        "central",
        "to",
        "learning",
        "as",
        "performance",
        "."
      ]
    },
    {
      "sentence": "In other words, machine learning should be extended with strategies to reason over its own learning process, leading to so-called meta-cognitive machine learning.",
      "tokens": [
        "In",
        "other",
        "words",
        ",",
        "machine",
        "learning",
        "should",
        "be",
        "extended",
        "with",
        "strategies",
        "to",
        "reason",
        "over",
        "its",
        "own",
        "learning",
        "process",
        ",",
        "leading",
        "to",
        "so-called",
        "meta-cognitive",
        "machine",
        "learning",
        "."
      ]
    },
    {
      "sentence": "As such, the de facto definition of machine learning should be reformulated in these intrinsically multiobjective terms, taking into account not only the task performance but also internal learning objectives.",
      "tokens": [
        "As",
        "such",
        ",",
        "the",
        "de",
        "facto",
        "definition",
        "of",
        "machine",
        "learning",
        "should",
        "be",
        "reformulated",
        "in",
        "these",
        "intrinsically",
        "multiobjective",
        "terms",
        ",",
        "taking",
        "into",
        "account",
        "not",
        "only",
        "the",
        "task",
        "performance",
        "but",
        "also",
        "internal",
        "learning",
        "objectives",
        "."
      ]
    },
    {
      "sentence": "To this end, we suggest a \"model entropy function\" to be defined that quantifies the efficiency of the internal learning processes.",
      "tokens": [
        "To",
        "this",
        "end",
        ",",
        "we",
        "suggest",
        "a",
        "``",
        "model",
        "entropy",
        "function",
        "''",
        "to",
        "be",
        "defined",
        "that",
        "quantifies",
        "the",
        "efficiency",
        "of",
        "the",
        "internal",
        "learning",
        "processes",
        "."
      ]
    },
    {
      "sentence": "It is conjured that the minimization of this model entropy leads to concept formation.",
      "tokens": [
        "It",
        "is",
        "conjured",
        "that",
        "the",
        "minimization",
        "of",
        "this",
        "model",
        "entropy",
        "leads",
        "to",
        "concept",
        "formation",
        "."
      ]
    },
    {
      "sentence": "Besides philosophical aspects, some initial illustrations are included to support the claims.",
      "tokens": [
        "Besides",
        "philosophical",
        "aspects",
        ",",
        "some",
        "initial",
        "illustrations",
        "are",
        "included",
        "to",
        "support",
        "the",
        "claims",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Introduction Machine learning is often approached from a behaviourist perspective, in which external feedback in the form of a reinforcement signal is the major driving force of improvement.",
      "tokens": [
        "Introduction",
        "Machine",
        "learning",
        "is",
        "often",
        "approached",
        "from",
        "a",
        "behaviourist",
        "perspective",
        ",",
        "in",
        "which",
        "external",
        "feedback",
        "in",
        "the",
        "form",
        "of",
        "a",
        "reinforcement",
        "signal",
        "is",
        "the",
        "major",
        "driving",
        "force",
        "of",
        "improvement",
        "."
      ]
    },
    {
      "sentence": "Though this method has lead to many successes, it is confronted with interesting and unsolved challenges like tackling overfitting, providing comprehensibility, building reusable abstractions and concept formation, among many other [Kotsiantis et al., 2007; Bengio, 2009] .",
      "tokens": [
        "Though",
        "this",
        "method",
        "has",
        "lead",
        "to",
        "many",
        "successes",
        ",",
        "it",
        "is",
        "confronted",
        "with",
        "interesting",
        "and",
        "unsolved",
        "challenges",
        "like",
        "tackling",
        "overfitting",
        ",",
        "providing",
        "comprehensibility",
        ",",
        "building",
        "reusable",
        "abstractions",
        "and",
        "concept",
        "formation",
        ",",
        "among",
        "many",
        "other",
        "[",
        "Kotsiantis",
        "et",
        "al.",
        ",",
        "2007",
        ";",
        "Bengio",
        ",",
        "2009",
        "]",
        "."
      ]
    },
    {
      "sentence": "The problem with these behaviourist approaches is that they ignore the central importance of internal processes when considering learning.",
      "tokens": [
        "The",
        "problem",
        "with",
        "these",
        "behaviourist",
        "approaches",
        "is",
        "that",
        "they",
        "ignore",
        "the",
        "central",
        "importance",
        "of",
        "internal",
        "processes",
        "when",
        "considering",
        "learning",
        "."
      ]
    },
    {
      "sentence": "Model internals are often regarded just as a means to achieve higher performance.",
      "tokens": [
        "Model",
        "internals",
        "are",
        "often",
        "regarded",
        "just",
        "as",
        "a",
        "means",
        "to",
        "achieve",
        "higher",
        "performance",
        "."
      ]
    },
    {
      "sentence": "Analogous to studying human behaviour, however, appreciating the mechanisms of learning boils down to the question: \"when have we really learnt?\"",
      "tokens": [
        "Analogous",
        "to",
        "studying",
        "human",
        "behaviour",
        ",",
        "however",
        ",",
        "appreciating",
        "the",
        "mechanisms",
        "of",
        "learning",
        "boils",
        "down",
        "to",
        "the",
        "question",
        ":",
        "``",
        "when",
        "have",
        "we",
        "really",
        "learnt",
        "?",
        "''"
      ]
    },
    {
      "sentence": "In this paper, we argue that a computer has learnt when: • the programme becomes better at the task at hand; • the programme can perform the task more efficiently; • the code becomes \"more structured\" and simpler.",
      "tokens": [
        "In",
        "this",
        "paper",
        ",",
        "we",
        "argue",
        "that",
        "a",
        "computer",
        "has",
        "learnt",
        "when",
        ":",
        "•",
        "the",
        "programme",
        "becomes",
        "better",
        "at",
        "the",
        "task",
        "at",
        "hand",
        ";",
        "•",
        "the",
        "programme",
        "can",
        "perform",
        "the",
        "task",
        "more",
        "efficiently",
        ";",
        "•",
        "the",
        "code",
        "becomes",
        "``",
        "more",
        "structured",
        "''",
        "and",
        "simpler",
        "."
      ]
    },
    {
      "sentence": "One possible analogy to better understand the above statements can be found in software engineering.",
      "tokens": [
        "One",
        "possible",
        "analogy",
        "to",
        "better",
        "understand",
        "the",
        "above",
        "statements",
        "can",
        "be",
        "found",
        "in",
        "software",
        "engineering",
        "."
      ]
    },
    {
      "sentence": "When considering code that performs a specific task, we do not care only about its functionality, but also about its execution speed/efficiency and other so-called \"non-functional requirements\".",
      "tokens": [
        "When",
        "considering",
        "code",
        "that",
        "performs",
        "a",
        "specific",
        "task",
        ",",
        "we",
        "do",
        "not",
        "care",
        "only",
        "about",
        "its",
        "functionality",
        ",",
        "but",
        "also",
        "about",
        "its",
        "execution",
        "speed/efficiency",
        "and",
        "other",
        "so-called",
        "``",
        "non-functional",
        "requirements",
        "''",
        "."
      ]
    },
    {
      "sentence": "Furthermore, a carefully modularized design probably reflects more understanding than an endless enumeration of IF-ELSE clauses.",
      "tokens": [
        "Furthermore",
        ",",
        "a",
        "carefully",
        "modularized",
        "design",
        "probably",
        "reflects",
        "more",
        "understanding",
        "than",
        "an",
        "endless",
        "enumeration",
        "of",
        "IF-ELSE",
        "clauses",
        "."
      ]
    },
    {
      "sentence": "In other words, finding a more efficient and structured way to represent/reproduce information and to perform a learning task, is as central to machine learning as the reproduction of results.",
      "tokens": [
        "In",
        "other",
        "words",
        ",",
        "finding",
        "a",
        "more",
        "efficient",
        "and",
        "structured",
        "way",
        "to",
        "represent/reproduce",
        "information",
        "and",
        "to",
        "perform",
        "a",
        "learning",
        "task",
        ",",
        "is",
        "as",
        "central",
        "to",
        "machine",
        "learning",
        "as",
        "the",
        "reproduction",
        "of",
        "results",
        "."
      ]
    },
    {
      "sentence": "Different to humans, of course, machines are measurable.",
      "tokens": [
        "Different",
        "to",
        "humans",
        ",",
        "of",
        "course",
        ",",
        "machines",
        "are",
        "measurable",
        "."
      ]
    },
    {
      "sentence": "This provides us with a unique opportunity to study the nature of learning in principle, at the same time improving Machine Intelligence.",
      "tokens": [
        "This",
        "provides",
        "us",
        "with",
        "a",
        "unique",
        "opportunity",
        "to",
        "study",
        "the",
        "nature",
        "of",
        "learning",
        "in",
        "principle",
        ",",
        "at",
        "the",
        "same",
        "time",
        "improving",
        "Machine",
        "Intelligence",
        "."
      ]
    },
    {
      "sentence": "We are not claiming that model complexity/efficiency has not been subject to past research efforts.",
      "tokens": [
        "We",
        "are",
        "not",
        "claiming",
        "that",
        "model",
        "complexity/efficiency",
        "has",
        "not",
        "been",
        "subject",
        "to",
        "past",
        "research",
        "efforts",
        "."
      ]
    },
    {
      "sentence": "On the contrary, many techniques and design principles have attempted to improve exactly these properties -like Occam's razor, Bayesian structure learning, pruning, the use of prototypes to compact information, regularization as a strategy to reduce energy, weight sharing in RNNs or CNNs to decrease model complexity, etc.",
      "tokens": [
        "On",
        "the",
        "contrary",
        ",",
        "many",
        "techniques",
        "and",
        "design",
        "principles",
        "have",
        "attempted",
        "to",
        "improve",
        "exactly",
        "these",
        "properties",
        "-like",
        "Occam",
        "'s",
        "razor",
        ",",
        "Bayesian",
        "structure",
        "learning",
        ",",
        "pruning",
        ",",
        "the",
        "use",
        "of",
        "prototypes",
        "to",
        "compact",
        "information",
        ",",
        "regularization",
        "as",
        "a",
        "strategy",
        "to",
        "reduce",
        "energy",
        ",",
        "weight",
        "sharing",
        "in",
        "RNNs",
        "or",
        "CNNs",
        "to",
        "decrease",
        "model",
        "complexity",
        ",",
        "etc",
        "."
      ]
    },
    {
      "sentence": "Indeed, the whole evolution of Deep Learning can be seen as one specific approach in the quest to find models that are more structured (i.e.",
      "tokens": [
        "Indeed",
        ",",
        "the",
        "whole",
        "evolution",
        "of",
        "Deep",
        "Learning",
        "can",
        "be",
        "seen",
        "as",
        "one",
        "specific",
        "approach",
        "in",
        "the",
        "quest",
        "to",
        "find",
        "models",
        "that",
        "are",
        "more",
        "structured",
        "(",
        "i.e",
        "."
      ]
    },
    {
      "sentence": "have a lower entropy), by organizing and training them in a layer-wise fashion [Bengio, 2009] .",
      "tokens": [
        "have",
        "a",
        "lower",
        "entropy",
        ")",
        ",",
        "by",
        "organizing",
        "and",
        "training",
        "them",
        "in",
        "a",
        "layer-wise",
        "fashion",
        "[",
        "Bengio",
        ",",
        "2009",
        "]",
        "."
      ]
    },
    {
      "sentence": "The focus has been mainly on training algorithms and designing model architectures that are adapted to these kinds of \"deep\" structures [Deng and Yu, 2014] .",
      "tokens": [
        "The",
        "focus",
        "has",
        "been",
        "mainly",
        "on",
        "training",
        "algorithms",
        "and",
        "designing",
        "model",
        "architectures",
        "that",
        "are",
        "adapted",
        "to",
        "these",
        "kinds",
        "of",
        "``",
        "deep",
        "''",
        "structures",
        "[",
        "Deng",
        "and",
        "Yu",
        ",",
        "2014",
        "]",
        "."
      ]
    },
    {
      "sentence": "Similar to efforts in multiobjective machine learning, these techniques are considered as a means to improve (externally measured) performance rather than a goal in itself [Jin and Sendhoff, 2008] .",
      "tokens": [
        "Similar",
        "to",
        "efforts",
        "in",
        "multiobjective",
        "machine",
        "learning",
        ",",
        "these",
        "techniques",
        "are",
        "considered",
        "as",
        "a",
        "means",
        "to",
        "improve",
        "(",
        "externally",
        "measured",
        ")",
        "performance",
        "rather",
        "than",
        "a",
        "goal",
        "in",
        "itself",
        "[",
        "Jin",
        "and",
        "Sendhoff",
        ",",
        "2008",
        "]",
        "."
      ]
    },
    {
      "sentence": "We, however, do believe that minimizing the model's structural complexity and optimizing its efficiency of representation, is not only a means to improve (externally validated) performance, but a central pillar to machine intelligence that leads to concept formulation and should be made explicit.",
      "tokens": [
        "We",
        ",",
        "however",
        ",",
        "do",
        "believe",
        "that",
        "minimizing",
        "the",
        "model",
        "'s",
        "structural",
        "complexity",
        "and",
        "optimizing",
        "its",
        "efficiency",
        "of",
        "representation",
        ",",
        "is",
        "not",
        "only",
        "a",
        "means",
        "to",
        "improve",
        "(",
        "externally",
        "validated",
        ")",
        "performance",
        ",",
        "but",
        "a",
        "central",
        "pillar",
        "to",
        "machine",
        "intelligence",
        "that",
        "leads",
        "to",
        "concept",
        "formulation",
        "and",
        "should",
        "be",
        "made",
        "explicit",
        "."
      ]
    },
    {
      "sentence": "In this sense, our vision aligns to that of Ray Kurzweil, who claimed that \"the theory behind deep learning.",
      "tokens": [
        "In",
        "this",
        "sense",
        ",",
        "our",
        "vision",
        "aligns",
        "to",
        "that",
        "of",
        "Ray",
        "Kurzweil",
        ",",
        "who",
        "claimed",
        "that",
        "``",
        "the",
        "theory",
        "behind",
        "deep",
        "learning",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": "is that you have a model that reflects the hierarchy in the natural phenomenon you're trying to learn [Hof, 2013] .\"",
      "tokens": [
        "is",
        "that",
        "you",
        "have",
        "a",
        "model",
        "that",
        "reflects",
        "the",
        "hierarchy",
        "in",
        "the",
        "natural",
        "phenomenon",
        "you",
        "'re",
        "trying",
        "to",
        "learn",
        "[",
        "Hof",
        ",",
        "2013",
        "]",
        ".",
        "''"
      ]
    },
    {
      "sentence": "This paper is structured as follows.",
      "tokens": [
        "This",
        "paper",
        "is",
        "structured",
        "as",
        "follows",
        "."
      ]
    },
    {
      "sentence": "The theoretical ideas are laid out and the case for a new operational definition of machine learning is made.",
      "tokens": [
        "The",
        "theoretical",
        "ideas",
        "are",
        "laid",
        "out",
        "and",
        "the",
        "case",
        "for",
        "a",
        "new",
        "operational",
        "definition",
        "of",
        "machine",
        "learning",
        "is",
        "made",
        "."
      ]
    },
    {
      "sentence": "We put forward the conjecture that the optimization of model entropy, leads to concept formation.",
      "tokens": [
        "We",
        "put",
        "forward",
        "the",
        "conjecture",
        "that",
        "the",
        "optimization",
        "of",
        "model",
        "entropy",
        ",",
        "leads",
        "to",
        "concept",
        "formation",
        "."
      ]
    },
    {
      "sentence": "Last, conclusions and further steps to operationalize these concepts are formulated.",
      "tokens": [
        "Last",
        ",",
        "conclusions",
        "and",
        "further",
        "steps",
        "to",
        "operationalize",
        "these",
        "concepts",
        "are",
        "formulated",
        "."
      ]
    },
    {
      "sentence": "2 Central assertions 2.1 Learning can not be explained in extrinsic terms only Conventional wisdom depicts machine learning as the optimization of a (non-)parametric model with respect to some performance measure.",
      "tokens": [
        "2",
        "Central",
        "assertions",
        "2.1",
        "Learning",
        "can",
        "not",
        "be",
        "explained",
        "in",
        "extrinsic",
        "terms",
        "only",
        "Conventional",
        "wisdom",
        "depicts",
        "machine",
        "learning",
        "as",
        "the",
        "optimization",
        "of",
        "a",
        "(",
        "non-",
        ")",
        "parametric",
        "model",
        "with",
        "respect",
        "to",
        "some",
        "performance",
        "measure",
        "."
      ]
    },
    {
      "sentence": "This view is clearly reflected in the de facto definition of machine learning by Mitchell [Mitchell, 1997] : \"A computer program is said to learn from an experience X with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experimental data D\".",
      "tokens": [
        "This",
        "view",
        "is",
        "clearly",
        "reflected",
        "in",
        "the",
        "de",
        "facto",
        "definition",
        "of",
        "machine",
        "learning",
        "by",
        "Mitchell",
        "[",
        "Mitchell",
        ",",
        "1997",
        "]",
        ":",
        "``",
        "A",
        "computer",
        "program",
        "is",
        "said",
        "to",
        "learn",
        "from",
        "an",
        "experience",
        "X",
        "with",
        "respect",
        "to",
        "some",
        "class",
        "of",
        "tasks",
        "T",
        "and",
        "performance",
        "measure",
        "P",
        ",",
        "if",
        "its",
        "performance",
        "at",
        "tasks",
        "in",
        "T",
        ",",
        "as",
        "measured",
        "by",
        "P",
        ",",
        "improves",
        "with",
        "experimental",
        "data",
        "D",
        "''",
        "."
      ]
    },
    {
      "sentence": "Traditional machine learning techniques typically exploit shallow-structured, and often fixed, architectures.",
      "tokens": [
        "Traditional",
        "machine",
        "learning",
        "techniques",
        "typically",
        "exploit",
        "shallow-structured",
        ",",
        "and",
        "often",
        "fixed",
        ",",
        "architectures",
        "."
      ]
    },
    {
      "sentence": "Nevertheless, there is a general consensus that the learning of \"higherorder\" concepts is problematic, and that the solution to this issue is somehow connected to deep architectures that create ever higher forms of abstraction.",
      "tokens": [
        "Nevertheless",
        ",",
        "there",
        "is",
        "a",
        "general",
        "consensus",
        "that",
        "the",
        "learning",
        "of",
        "``",
        "higherorder",
        "''",
        "concepts",
        "is",
        "problematic",
        ",",
        "and",
        "that",
        "the",
        "solution",
        "to",
        "this",
        "issue",
        "is",
        "somehow",
        "connected",
        "to",
        "deep",
        "architectures",
        "that",
        "create",
        "ever",
        "higher",
        "forms",
        "of",
        "abstraction",
        "."
      ]
    },
    {
      "sentence": "Experimental research as well as neurological evidence on the organization of the brain, supports this finding [Bianchini and Scarselli, 2014] .",
      "tokens": [
        "Experimental",
        "research",
        "as",
        "well",
        "as",
        "neurological",
        "evidence",
        "on",
        "the",
        "organization",
        "of",
        "the",
        "brain",
        ",",
        "supports",
        "this",
        "finding",
        "[",
        "Bianchini",
        "and",
        "Scarselli",
        ",",
        "2014",
        "]",
        "."
      ]
    },
    {
      "sentence": "The limitation of architecture complexity is preferred, primarily because their behaviour could be understood and the training of more complex or adaptive architectures leads to a explosion of complexity.",
      "tokens": [
        "The",
        "limitation",
        "of",
        "architecture",
        "complexity",
        "is",
        "preferred",
        ",",
        "primarily",
        "because",
        "their",
        "behaviour",
        "could",
        "be",
        "understood",
        "and",
        "the",
        "training",
        "of",
        "more",
        "complex",
        "or",
        "adaptive",
        "architectures",
        "leads",
        "to",
        "a",
        "explosion",
        "of",
        "complexity",
        "."
      ]
    },
    {
      "sentence": "That was until recently.",
      "tokens": [
        "That",
        "was",
        "until",
        "recently",
        "."
      ]
    },
    {
      "sentence": "The recent advanced in so-called \"Deep Learning\", have focused on training algorithms that are adapted to new kinds of deep architectures [Deng and Yu, 2014] , and heuristic strategies to attain specific structural properties like sparse coding that lead to higher forms of abstractions.",
      "tokens": [
        "The",
        "recent",
        "advanced",
        "in",
        "so-called",
        "``",
        "Deep",
        "Learning",
        "''",
        ",",
        "have",
        "focused",
        "on",
        "training",
        "algorithms",
        "that",
        "are",
        "adapted",
        "to",
        "new",
        "kinds",
        "of",
        "deep",
        "architectures",
        "[",
        "Deng",
        "and",
        "Yu",
        ",",
        "2014",
        "]",
        ",",
        "and",
        "heuristic",
        "strategies",
        "to",
        "attain",
        "specific",
        "structural",
        "properties",
        "like",
        "sparse",
        "coding",
        "that",
        "lead",
        "to",
        "higher",
        "forms",
        "of",
        "abstractions",
        "."
      ]
    },
    {
      "sentence": "With the exception of studies on \"interpretability\" [Jin and Sendhoff, 2008] , structural properties are mainly considered a by-product, a (desirable) side effect of the applied training mechanisms.",
      "tokens": [
        "With",
        "the",
        "exception",
        "of",
        "studies",
        "on",
        "``",
        "interpretability",
        "''",
        "[",
        "Jin",
        "and",
        "Sendhoff",
        ",",
        "2008",
        "]",
        ",",
        "structural",
        "properties",
        "are",
        "mainly",
        "considered",
        "a",
        "by-product",
        ",",
        "a",
        "(",
        "desirable",
        ")",
        "side",
        "effect",
        "of",
        "the",
        "applied",
        "training",
        "mechanisms",
        "."
      ]
    },
    {
      "sentence": "Though the organization and complexity of model topologies is acknowledged to be crucial, current approaches are mainly limited to analysing the data space, i.e.",
      "tokens": [
        "Though",
        "the",
        "organization",
        "and",
        "complexity",
        "of",
        "model",
        "topologies",
        "is",
        "acknowledged",
        "to",
        "be",
        "crucial",
        ",",
        "current",
        "approaches",
        "are",
        "mainly",
        "limited",
        "to",
        "analysing",
        "the",
        "data",
        "space",
        ",",
        "i.e",
        "."
      ]
    },
    {
      "sentence": "the implemented regression functions or decision boundaries [Bianchini and Scarselli, 2014] .",
      "tokens": [
        "the",
        "implemented",
        "regression",
        "functions",
        "or",
        "decision",
        "boundaries",
        "[",
        "Bianchini",
        "and",
        "Scarselli",
        ",",
        "2014",
        "]",
        "."
      ]
    },
    {
      "sentence": "There is a problem with this approach.",
      "tokens": [
        "There",
        "is",
        "a",
        "problem",
        "with",
        "this",
        "approach",
        "."
      ]
    },
    {
      "sentence": "Consider an neural network algorithm that needs to learn a simple concept like an \"XOR\" function depicted in Fig.",
      "tokens": [
        "Consider",
        "an",
        "neural",
        "network",
        "algorithm",
        "that",
        "needs",
        "to",
        "learn",
        "a",
        "simple",
        "concept",
        "like",
        "an",
        "``",
        "XOR",
        "''",
        "function",
        "depicted",
        "in",
        "Fig",
        "."
      ]
    },
    {
      "sentence": "1 .",
      "tokens": [
        "1",
        "."
      ]
    },
    {
      "sentence": "An infinite number of neural networks with very similar or identical decision boundaries can be constructed -of which two are shown in Fig.",
      "tokens": [
        "An",
        "infinite",
        "number",
        "of",
        "neural",
        "networks",
        "with",
        "very",
        "similar",
        "or",
        "identical",
        "decision",
        "boundaries",
        "can",
        "be",
        "constructed",
        "-of",
        "which",
        "two",
        "are",
        "shown",
        "in",
        "Fig",
        "."
      ]
    },
    {
      "sentence": "2 .",
      "tokens": [
        "2",
        "."
      ]
    },
    {
      "sentence": "From an external point of view, there is no way to discriminate between these two models: describing the difference between these two models can only occur in terms of the model internals.",
      "tokens": [
        "From",
        "an",
        "external",
        "point",
        "of",
        "view",
        ",",
        "there",
        "is",
        "no",
        "way",
        "to",
        "discriminate",
        "between",
        "these",
        "two",
        "models",
        ":",
        "describing",
        "the",
        "difference",
        "between",
        "these",
        "two",
        "models",
        "can",
        "only",
        "occur",
        "in",
        "terms",
        "of",
        "the",
        "model",
        "internals",
        "."
      ]
    },
    {
      "sentence": "Of course the weight space, which represents the model of a neural network, is related to the data space, as it performs calculations on the data.",
      "tokens": [
        "Of",
        "course",
        "the",
        "weight",
        "space",
        ",",
        "which",
        "represents",
        "the",
        "model",
        "of",
        "a",
        "neural",
        "network",
        ",",
        "is",
        "related",
        "to",
        "the",
        "data",
        "space",
        ",",
        "as",
        "it",
        "performs",
        "calculations",
        "on",
        "the",
        "data",
        "."
      ]
    },
    {
      "sentence": "In other words: Data representation and model computation should be considered as two sides of the same coin.",
      "tokens": [
        "In",
        "other",
        "words",
        ":",
        "Data",
        "representation",
        "and",
        "model",
        "computation",
        "should",
        "be",
        "considered",
        "as",
        "two",
        "sides",
        "of",
        "the",
        "same",
        "coin",
        "."
      ]
    },
    {
      "sentence": "As a result the structural properties of both the model and data space are key to the modelling of higher abstractions.",
      "tokens": [
        "As",
        "a",
        "result",
        "the",
        "structural",
        "properties",
        "of",
        "both",
        "the",
        "model",
        "and",
        "data",
        "space",
        "are",
        "key",
        "to",
        "the",
        "modelling",
        "of",
        "higher",
        "abstractions",
        "."
      ]
    },
    {
      "sentence": "Sparse coding is a perfect example of this.",
      "tokens": [
        "Sparse",
        "coding",
        "is",
        "a",
        "perfect",
        "example",
        "of",
        "this",
        "."
      ]
    },
    {
      "sentence": "Without sparse coding, although the information is intrinsically \"present\" in the data, neural networks become intractable to train due to the extremely volatile and complex decision surface.",
      "tokens": [
        "Without",
        "sparse",
        "coding",
        ",",
        "although",
        "the",
        "information",
        "is",
        "intrinsically",
        "``",
        "present",
        "''",
        "in",
        "the",
        "data",
        ",",
        "neural",
        "networks",
        "become",
        "intractable",
        "to",
        "train",
        "due",
        "to",
        "the",
        "extremely",
        "volatile",
        "and",
        "complex",
        "decision",
        "surface",
        "."
      ]
    },
    {
      "sentence": "From this perspective we follow the observations that have been made by Bengio in [Bengio et al., 2013] on representation learning.",
      "tokens": [
        "From",
        "this",
        "perspective",
        "we",
        "follow",
        "the",
        "observations",
        "that",
        "have",
        "been",
        "made",
        "by",
        "Bengio",
        "in",
        "[",
        "Bengio",
        "et",
        "al.",
        ",",
        "2013",
        "]",
        "on",
        "representation",
        "learning",
        "."
      ]
    },
    {
      "sentence": "One of the interesting phenomena is \"information entanglement\" [Glorot et al., 2011] .",
      "tokens": [
        "One",
        "of",
        "the",
        "interesting",
        "phenomena",
        "is",
        "``",
        "information",
        "entanglement",
        "''",
        "[",
        "Glorot",
        "et",
        "al.",
        ",",
        "2011",
        "]",
        "."
      ]
    },
    {
      "sentence": "In this case, the model space is of a lower dimensionality or complexity than the data space.",
      "tokens": [
        "In",
        "this",
        "case",
        ",",
        "the",
        "model",
        "space",
        "is",
        "of",
        "a",
        "lower",
        "dimensionality",
        "or",
        "complexity",
        "than",
        "the",
        "data",
        "space",
        "."
      ]
    },
    {
      "sentence": "The projection of the data onto a high-dimensional space using sparse coding, then, has the advantage that the representations are more likely to be linearly separable, or at least less nonlinear.",
      "tokens": [
        "The",
        "projection",
        "of",
        "the",
        "data",
        "onto",
        "a",
        "high-dimensional",
        "space",
        "using",
        "sparse",
        "coding",
        ",",
        "then",
        ",",
        "has",
        "the",
        "advantage",
        "that",
        "the",
        "representations",
        "are",
        "more",
        "likely",
        "to",
        "be",
        "linearly",
        "separable",
        ",",
        "or",
        "at",
        "least",
        "less",
        "nonlinear",
        "."
      ]
    },
    {
      "sentence": "On the other hand, when the model complexity is increased considerably (e.g.",
      "tokens": [
        "On",
        "the",
        "other",
        "hand",
        ",",
        "when",
        "the",
        "model",
        "complexity",
        "is",
        "increased",
        "considerably",
        "(",
        "e.g",
        "."
      ]
    },
    {
      "sentence": "by adding layers), the neural network becomes untrainable using traditional techniques, because the dimensionality of the search space explodes.",
      "tokens": [
        "by",
        "adding",
        "layers",
        ")",
        ",",
        "the",
        "neural",
        "network",
        "becomes",
        "untrainable",
        "using",
        "traditional",
        "techniques",
        ",",
        "because",
        "the",
        "dimensionality",
        "of",
        "the",
        "search",
        "space",
        "explodes",
        "."
      ]
    },
    {
      "sentence": "Deep learning techniques tackle this issue by -among other techniques -pre-initializing the model-space of particular layer in a maximum-likelihood/minimal-energy state.",
      "tokens": [
        "Deep",
        "learning",
        "techniques",
        "tackle",
        "this",
        "issue",
        "by",
        "-among",
        "other",
        "techniques",
        "-pre-initializing",
        "the",
        "model-space",
        "of",
        "particular",
        "layer",
        "in",
        "a",
        "maximum-likelihood/minimal-energy",
        "state",
        "."
      ]
    }
  ]
}