{
  "title": [
    {
      "sentence": "POWER CONSUMPTION VARIATION OVER ACTIVATION FUNCTIONS",
      "tokens": [
        "POWER",
        "CONSUMPTION",
        "VARIATION",
        "OVER",
        "ACTIVATION",
        "FUNCTIONS"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "The power machine learning models consume when making predictions can be affected by a model's architecture.",
      "tokens": [
        "The",
        "power",
        "machine",
        "learning",
        "models",
        "consume",
        "when",
        "making",
        "predictions",
        "can",
        "be",
        "affected",
        "by",
        "a",
        "model",
        "'s",
        "architecture",
        "."
      ]
    },
    {
      "sentence": "This paper presents various estimates of power consumption for a range of different activation functions, a core factor in neural network model architecture design.",
      "tokens": [
        "This",
        "paper",
        "presents",
        "various",
        "estimates",
        "of",
        "power",
        "consumption",
        "for",
        "a",
        "range",
        "of",
        "different",
        "activation",
        "functions",
        ",",
        "a",
        "core",
        "factor",
        "in",
        "neural",
        "network",
        "model",
        "architecture",
        "design",
        "."
      ]
    },
    {
      "sentence": "Substantial differences in hardware performance exist between activation functions.",
      "tokens": [
        "Substantial",
        "differences",
        "in",
        "hardware",
        "performance",
        "exist",
        "between",
        "activation",
        "functions",
        "."
      ]
    },
    {
      "sentence": "This difference informs how power consumption in machine learning models can be reduced.",
      "tokens": [
        "This",
        "difference",
        "informs",
        "how",
        "power",
        "consumption",
        "in",
        "machine",
        "learning",
        "models",
        "can",
        "be",
        "reduced",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "INTRODUCTION The field of deep neural networks has reported strong progress in many problem areas, including natural language processing (NLP), image recognition, and game playing.",
      "tokens": [
        "INTRODUCTION",
        "The",
        "field",
        "of",
        "deep",
        "neural",
        "networks",
        "has",
        "reported",
        "strong",
        "progress",
        "in",
        "many",
        "problem",
        "areas",
        ",",
        "including",
        "natural",
        "language",
        "processing",
        "(",
        "NLP",
        ")",
        ",",
        "image",
        "recognition",
        ",",
        "and",
        "game",
        "playing",
        "."
      ]
    },
    {
      "sentence": "Many of the advances in these areas have been the fruit of using larger and thus more computationally demanding neural networks.",
      "tokens": [
        "Many",
        "of",
        "the",
        "advances",
        "in",
        "these",
        "areas",
        "have",
        "been",
        "the",
        "fruit",
        "of",
        "using",
        "larger",
        "and",
        "thus",
        "more",
        "computationally",
        "demanding",
        "neural",
        "networks",
        "."
      ]
    },
    {
      "sentence": "Amodei & Hernandez (2018) find that the cost of training doubled every few months between the releases of AlexNet (Krizhevsky et al., 2012) and AlphaZero Silver et al.",
      "tokens": [
        "Amodei",
        "&",
        "Hernandez",
        "(",
        "2018",
        ")",
        "find",
        "that",
        "the",
        "cost",
        "of",
        "training",
        "doubled",
        "every",
        "few",
        "months",
        "between",
        "the",
        "releases",
        "of",
        "AlexNet",
        "(",
        "Krizhevsky",
        "et",
        "al.",
        ",",
        "2012",
        ")",
        "and",
        "AlphaZero",
        "Silver",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "(2018) .",
      "tokens": [
        "(",
        "2018",
        ")",
        "."
      ]
    },
    {
      "sentence": "In NLP, power consumption has also risen: Strubell et al.",
      "tokens": [
        "In",
        "NLP",
        ",",
        "power",
        "consumption",
        "has",
        "also",
        "risen",
        ":",
        "Strubell",
        "et",
        "al",
        "."
      ]
    },
    {
      "sentence": "(2019) determine the carbon footprint of a contemporary machine translation architecure search to be in the order of hundreds of intercontinental flights, for models that offer only marginal performance improvement.",
      "tokens": [
        "(",
        "2019",
        ")",
        "determine",
        "the",
        "carbon",
        "footprint",
        "of",
        "a",
        "contemporary",
        "machine",
        "translation",
        "architecure",
        "search",
        "to",
        "be",
        "in",
        "the",
        "order",
        "of",
        "hundreds",
        "of",
        "intercontinental",
        "flights",
        ",",
        "for",
        "models",
        "that",
        "offer",
        "only",
        "marginal",
        "performance",
        "improvement",
        "."
      ]
    },
    {
      "sentence": "This paper examines activation functions, a core part of neural networks.",
      "tokens": [
        "This",
        "paper",
        "examines",
        "activation",
        "functions",
        ",",
        "a",
        "core",
        "part",
        "of",
        "neural",
        "networks",
        "."
      ]
    },
    {
      "sentence": "The activation function is the non-linearity at the core of each network node.",
      "tokens": [
        "The",
        "activation",
        "function",
        "is",
        "the",
        "non-linearity",
        "at",
        "the",
        "core",
        "of",
        "each",
        "network",
        "node",
        "."
      ]
    },
    {
      "sentence": "It is applied over the input and bias parameters at a given node for each inference that a model makes.",
      "tokens": [
        "It",
        "is",
        "applied",
        "over",
        "the",
        "input",
        "and",
        "bias",
        "parameters",
        "at",
        "a",
        "given",
        "node",
        "for",
        "each",
        "inference",
        "that",
        "a",
        "model",
        "makes",
        "."
      ]
    },
    {
      "sentence": "This makes for a potentially large number of computation being required to make predictions, predicated on network structure and size.",
      "tokens": [
        "This",
        "makes",
        "for",
        "a",
        "potentially",
        "large",
        "number",
        "of",
        "computation",
        "being",
        "required",
        "to",
        "make",
        "predictions",
        ",",
        "predicated",
        "on",
        "network",
        "structure",
        "and",
        "size",
        "."
      ]
    },
    {
      "sentence": "When it comes to individual calculations, there is also broad variance.",
      "tokens": [
        "When",
        "it",
        "comes",
        "to",
        "individual",
        "calculations",
        ",",
        "there",
        "is",
        "also",
        "broad",
        "variance",
        "."
      ]
    },
    {
      "sentence": "The complexity of low-level instructions for each these functions also varies widely, from the simple rectified linear unit to the transcendental hyperbolic tangent.",
      "tokens": [
        "The",
        "complexity",
        "of",
        "low-level",
        "instructions",
        "for",
        "each",
        "these",
        "functions",
        "also",
        "varies",
        "widely",
        ",",
        "from",
        "the",
        "simple",
        "rectified",
        "linear",
        "unit",
        "to",
        "the",
        "transcendental",
        "hyperbolic",
        "tangent",
        "."
      ]
    },
    {
      "sentence": "This variance has the potential to lead to differences in power consumption.",
      "tokens": [
        "This",
        "variance",
        "has",
        "the",
        "potential",
        "to",
        "lead",
        "to",
        "differences",
        "in",
        "power",
        "consumption",
        "."
      ]
    }
  ]
}