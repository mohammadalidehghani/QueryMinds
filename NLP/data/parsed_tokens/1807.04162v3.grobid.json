{
  "title": [
    {
      "sentence": "THE THERMODYNAMICS OF MACHINE LEARNING",
      "tokens": [
        "THE",
        "THERMODYNAMICS",
        "OF",
        "MACHINE",
        "LEARNING"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "In this work we offer an information-theoretic framework for representation learning that connects with a wide class of existing objectives in machine learning.",
      "tokens": [
        "In",
        "this",
        "work",
        "we",
        "offer",
        "an",
        "information-theoretic",
        "framework",
        "for",
        "representation",
        "learning",
        "that",
        "connects",
        "with",
        "a",
        "wide",
        "class",
        "of",
        "existing",
        "objectives",
        "in",
        "machine",
        "learning",
        "."
      ]
    },
    {
      "sentence": "We develop a formal correspondence between this work and thermodynamics and discuss its implications.",
      "tokens": [
        "We",
        "develop",
        "a",
        "formal",
        "correspondence",
        "between",
        "this",
        "work",
        "and",
        "thermodynamics",
        "and",
        "discuss",
        "its",
        "implications",
        "."
      ]
    },
    {
      "sentence": "1 Here we aim to invoke the same philosophy as in the introduction to Watanabe (2018).",
      "tokens": [
        "1",
        "Here",
        "we",
        "aim",
        "to",
        "invoke",
        "the",
        "same",
        "philosophy",
        "as",
        "in",
        "the",
        "introduction",
        "to",
        "Watanabe",
        "(",
        "2018",
        ")",
        "."
      ]
    },
    {
      "sentence": "2 That is, we imagine the data satisfies De Finetti's theorem, for which infinite exchangeable processes usually can be described by products of conditionally independent distributions, but don't want to worry too much about the complicated details since there are subtle special cases (Accardi, 2018).",
      "tokens": [
        "2",
        "That",
        "is",
        ",",
        "we",
        "imagine",
        "the",
        "data",
        "satisfies",
        "De",
        "Finetti",
        "'s",
        "theorem",
        ",",
        "for",
        "which",
        "infinite",
        "exchangeable",
        "processes",
        "usually",
        "can",
        "be",
        "described",
        "by",
        "products",
        "of",
        "conditionally",
        "independent",
        "distributions",
        ",",
        "but",
        "do",
        "n't",
        "want",
        "to",
        "worry",
        "too",
        "much",
        "about",
        "the",
        "complicated",
        "details",
        "since",
        "there",
        "are",
        "subtle",
        "special",
        "cases",
        "(",
        "Accardi",
        ",",
        "2018",
        ")",
        "."
      ]
    },
    {
      "sentence": "3 Here and throughout H(A) is used to denote entropies H(A) =i p(A) log p(A).",
      "tokens": [
        "3",
        "Here",
        "and",
        "throughout",
        "H",
        "(",
        "A",
        ")",
        "is",
        "used",
        "to",
        "denote",
        "entropies",
        "H",
        "(",
        "A",
        ")",
        "=i",
        "p",
        "(",
        "A",
        ")",
        "log",
        "p",
        "(",
        "A",
        ")",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "INTRODUCTION Let X, Y be some paired data, for example: a set of images X and their labels Y .",
      "tokens": [
        "INTRODUCTION",
        "Let",
        "X",
        ",",
        "Y",
        "be",
        "some",
        "paired",
        "data",
        ",",
        "for",
        "example",
        ":",
        "a",
        "set",
        "of",
        "images",
        "X",
        "and",
        "their",
        "labels",
        "Y",
        "."
      ]
    },
    {
      "sentence": "We imagine the data comes from some true, unknown data generating process Φ 1 , from which we have drawn a training set of N pairs: T N ≡ (x N , y N ) ≡ {x 1 , y 1 , x 2 , y 2 , .",
      "tokens": [
        "We",
        "imagine",
        "the",
        "data",
        "comes",
        "from",
        "some",
        "true",
        ",",
        "unknown",
        "data",
        "generating",
        "process",
        "Φ",
        "1",
        ",",
        "from",
        "which",
        "we",
        "have",
        "drawn",
        "a",
        "training",
        "set",
        "of",
        "N",
        "pairs",
        ":",
        "T",
        "N",
        "≡",
        "(",
        "x",
        "N",
        ",",
        "y",
        "N",
        ")",
        "≡",
        "{",
        "x",
        "1",
        ",",
        "y",
        "1",
        ",",
        "x",
        "2",
        ",",
        "y",
        "2",
        ",",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ", x N , y N } ∼ φ(x N , y N ).",
      "tokens": [
        ",",
        "x",
        "N",
        ",",
        "y",
        "N",
        "}",
        "∼",
        "φ",
        "(",
        "x",
        "N",
        ",",
        "y",
        "N",
        ")",
        "."
      ]
    },
    {
      "sentence": "(1) We further imagine the process is exchangeable 2 and the data is conditionally independent given the governing process Φ: p(x N , y N |φ) = i p(x i |φ)p(y i |x i , φ).",
      "tokens": [
        "(",
        "1",
        ")",
        "We",
        "further",
        "imagine",
        "the",
        "process",
        "is",
        "exchangeable",
        "2",
        "and",
        "the",
        "data",
        "is",
        "conditionally",
        "independent",
        "given",
        "the",
        "governing",
        "process",
        "Φ",
        ":",
        "p",
        "(",
        "x",
        "N",
        ",",
        "y",
        "N",
        "|φ",
        ")",
        "=",
        "i",
        "p",
        "(",
        "x",
        "i",
        "|φ",
        ")",
        "p",
        "(",
        "y",
        "i",
        "|x",
        "i",
        ",",
        "φ",
        ")",
        "."
      ]
    },
    {
      "sentence": "(2) As machine learners, we believe that by studying the training set, we should be able to infer or predict new draws from the same data generating process.",
      "tokens": [
        "(",
        "2",
        ")",
        "As",
        "machine",
        "learners",
        ",",
        "we",
        "believe",
        "that",
        "by",
        "studying",
        "the",
        "training",
        "set",
        ",",
        "we",
        "should",
        "be",
        "able",
        "to",
        "infer",
        "or",
        "predict",
        "new",
        "draws",
        "from",
        "the",
        "same",
        "data",
        "generating",
        "process",
        "."
      ]
    },
    {
      "sentence": "Call a set of M future draws from the data generating process T M ≡ {X M , Y M } the test set.",
      "tokens": [
        "Call",
        "a",
        "set",
        "of",
        "M",
        "future",
        "draws",
        "from",
        "the",
        "data",
        "generating",
        "process",
        "T",
        "M",
        "≡",
        "{",
        "X",
        "M",
        ",",
        "Y",
        "M",
        "}",
        "the",
        "test",
        "set",
        "."
      ]
    },
    {
      "sentence": "The predictive information (Bialek et al., 2001) is the mutual information between the training set and a infinite test set, equivalently the amount of information the training set provides about the generative process itself: I pred (T N ) ≡ lim M →∞ I(T N ; T M ) = I(T N ; Φ) = I(X N , Y N ; Φ).",
      "tokens": [
        "The",
        "predictive",
        "information",
        "(",
        "Bialek",
        "et",
        "al.",
        ",",
        "2001",
        ")",
        "is",
        "the",
        "mutual",
        "information",
        "between",
        "the",
        "training",
        "set",
        "and",
        "a",
        "infinite",
        "test",
        "set",
        ",",
        "equivalently",
        "the",
        "amount",
        "of",
        "information",
        "the",
        "training",
        "set",
        "provides",
        "about",
        "the",
        "generative",
        "process",
        "itself",
        ":",
        "I",
        "pred",
        "(",
        "T",
        "N",
        ")",
        "≡",
        "lim",
        "M",
        "→∞",
        "I",
        "(",
        "T",
        "N",
        ";",
        "T",
        "M",
        ")",
        "=",
        "I",
        "(",
        "T",
        "N",
        ";",
        "Φ",
        ")",
        "=",
        "I",
        "(",
        "X",
        "N",
        ",",
        "Y",
        "N",
        ";",
        "Φ",
        ")",
        "."
      ]
    },
    {
      "sentence": "(3) The predictive information measures the underlying complexity of the data generating process (Still, 2014) , and is fundamentally limited and must grow sublinearly in the dataset size (Bialek et al., 2001) .",
      "tokens": [
        "(",
        "3",
        ")",
        "The",
        "predictive",
        "information",
        "measures",
        "the",
        "underlying",
        "complexity",
        "of",
        "the",
        "data",
        "generating",
        "process",
        "(",
        "Still",
        ",",
        "2014",
        ")",
        ",",
        "and",
        "is",
        "fundamentally",
        "limited",
        "and",
        "must",
        "grow",
        "sublinearly",
        "in",
        "the",
        "dataset",
        "size",
        "(",
        "Bialek",
        "et",
        "al.",
        ",",
        "2001",
        ")",
        "."
      ]
    },
    {
      "sentence": "Hence, the predictive information is a vanishing fraction of the total information in the training set 3 : lim N →∞ I pred (T N ) H(T N ) = 0 (4) A vanishing fraction of the information present in our training data is in any way useful for future tasks.",
      "tokens": [
        "Hence",
        ",",
        "the",
        "predictive",
        "information",
        "is",
        "a",
        "vanishing",
        "fraction",
        "of",
        "the",
        "total",
        "information",
        "in",
        "the",
        "training",
        "set",
        "3",
        ":",
        "lim",
        "N",
        "→∞",
        "I",
        "pred",
        "(",
        "T",
        "N",
        ")",
        "H",
        "(",
        "T",
        "N",
        ")",
        "=",
        "0",
        "(",
        "4",
        ")",
        "A",
        "vanishing",
        "fraction",
        "of",
        "the",
        "information",
        "present",
        "in",
        "our",
        "training",
        "data",
        "is",
        "in",
        "any",
        "way",
        "useful",
        "for",
        "future",
        "tasks",
        "."
      ]
    },
    {
      "sentence": "A vanishing fraction of the information contained in the training data is signal, the rest is noise.",
      "tokens": [
        "A",
        "vanishing",
        "fraction",
        "of",
        "the",
        "information",
        "contained",
        "in",
        "the",
        "training",
        "data",
        "is",
        "signal",
        ",",
        "the",
        "rest",
        "is",
        "noise",
        "."
      ]
    },
    {
      "sentence": "We claim the goal of learning is to learn a representation of data, both locally and globally that captures the predictive information while being maximally compressed: that separates the signal from the noise.",
      "tokens": [
        "We",
        "claim",
        "the",
        "goal",
        "of",
        "learning",
        "is",
        "to",
        "learn",
        "a",
        "representation",
        "of",
        "data",
        ",",
        "both",
        "locally",
        "and",
        "globally",
        "that",
        "captures",
        "the",
        "predictive",
        "information",
        "while",
        "being",
        "maximally",
        "compressed",
        ":",
        "that",
        "separates",
        "the",
        "signal",
        "from",
        "the",
        "noise",
        "."
      ]
    }
  ]
}