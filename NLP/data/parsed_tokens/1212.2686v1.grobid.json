{
  "title": [
    {
      "sentence": "Joint Training of Deep Boltzmann Machines for Classification",
      "tokens": [
        "Joint",
        "Training",
        "of",
        "Deep",
        "Boltzmann",
        "Machines",
        "for",
        "Classification"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "We introduce a new method for training deep Boltzmann machines jointly.",
      "tokens": [
        "We",
        "introduce",
        "a",
        "new",
        "method",
        "for",
        "training",
        "deep",
        "Boltzmann",
        "machines",
        "jointly",
        "."
      ]
    },
    {
      "sentence": "Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classification tasks.",
      "tokens": [
        "Prior",
        "methods",
        "require",
        "an",
        "initial",
        "learning",
        "pass",
        "that",
        "trains",
        "the",
        "deep",
        "Boltzmann",
        "machine",
        "greedily",
        ",",
        "one",
        "layer",
        "at",
        "a",
        "time",
        ",",
        "or",
        "do",
        "not",
        "perform",
        "well",
        "on",
        "classification",
        "tasks",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Deep Boltzmann machines A deep Boltzmann machine (Salakhutdinov and Hinton, 2009 ) is a probabilistic model consisting of many layers of random variables, most of which are latent.",
      "tokens": [
        "Deep",
        "Boltzmann",
        "machines",
        "A",
        "deep",
        "Boltzmann",
        "machine",
        "(",
        "Salakhutdinov",
        "and",
        "Hinton",
        ",",
        "2009",
        ")",
        "is",
        "a",
        "probabilistic",
        "model",
        "consisting",
        "of",
        "many",
        "layers",
        "of",
        "random",
        "variables",
        ",",
        "most",
        "of",
        "which",
        "are",
        "latent",
        "."
      ]
    },
    {
      "sentence": "Typically, a DBM contains a set of D input features v that are called the visible units because they are always observed during both training and evaluation.",
      "tokens": [
        "Typically",
        ",",
        "a",
        "DBM",
        "contains",
        "a",
        "set",
        "of",
        "D",
        "input",
        "features",
        "v",
        "that",
        "are",
        "called",
        "the",
        "visible",
        "units",
        "because",
        "they",
        "are",
        "always",
        "observed",
        "during",
        "both",
        "training",
        "and",
        "evaluation",
        "."
      ]
    },
    {
      "sentence": "The DBM is usually applied to classification problems and thus often represents the class label with a one-of-k code in the form of a discrete-valued label unit y. y is observed (on examples for which it is available) during training.",
      "tokens": [
        "The",
        "DBM",
        "is",
        "usually",
        "applied",
        "to",
        "classification",
        "problems",
        "and",
        "thus",
        "often",
        "represents",
        "the",
        "class",
        "label",
        "with",
        "a",
        "one-of-k",
        "code",
        "in",
        "the",
        "form",
        "of",
        "a",
        "discrete-valued",
        "label",
        "unit",
        "y.",
        "y",
        "is",
        "observed",
        "(",
        "on",
        "examples",
        "for",
        "which",
        "it",
        "is",
        "available",
        ")",
        "during",
        "training",
        "."
      ]
    },
    {
      "sentence": "The DBM also contains several hidden units, which are usually organized into L layers h (i) of size N i , i = 1, .",
      "tokens": [
        "The",
        "DBM",
        "also",
        "contains",
        "several",
        "hidden",
        "units",
        ",",
        "which",
        "are",
        "usually",
        "organized",
        "into",
        "L",
        "layers",
        "h",
        "(",
        "i",
        ")",
        "of",
        "size",
        "N",
        "i",
        ",",
        "i",
        "=",
        "1",
        ",",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ", L,with each unit in a layer conditionally independent of the other units in the layer given the neighboring layers.",
      "tokens": [
        ",",
        "L",
        ",",
        "with",
        "each",
        "unit",
        "in",
        "a",
        "layer",
        "conditionally",
        "independent",
        "of",
        "the",
        "other",
        "units",
        "in",
        "the",
        "layer",
        "given",
        "the",
        "neighboring",
        "layers",
        "."
      ]
    },
    {
      "sentence": "These conditional independence properties allow fast Gibbs sampling because an entire layer of units can be sampled at a time.",
      "tokens": [
        "These",
        "conditional",
        "independence",
        "properties",
        "allow",
        "fast",
        "Gibbs",
        "sampling",
        "because",
        "an",
        "entire",
        "layer",
        "of",
        "units",
        "can",
        "be",
        "sampled",
        "at",
        "a",
        "time",
        "."
      ]
    },
    {
      "sentence": "Likewise, mean field inference with fixed point equations is fast because each fixed point equation gives a solution to an entire layer of variational parameters.",
      "tokens": [
        "Likewise",
        ",",
        "mean",
        "field",
        "inference",
        "with",
        "fixed",
        "point",
        "equations",
        "is",
        "fast",
        "because",
        "each",
        "fixed",
        "point",
        "equation",
        "gives",
        "a",
        "solution",
        "to",
        "an",
        "entire",
        "layer",
        "of",
        "variational",
        "parameters",
        "."
      ]
    },
    {
      "sentence": "A DBM defines a probability distribution by exponentiating and normalizing an energy function P (v, h, y) = 1 Z exp (-E(v, h, y)) Preliminary work presented to Bruno Olshausen's lab and Google Brain, December 2012. where Z = v ′ ,h ′ ,y ′ exp (-E(v ′ , h ′ , y ′ )) .",
      "tokens": [
        "A",
        "DBM",
        "defines",
        "a",
        "probability",
        "distribution",
        "by",
        "exponentiating",
        "and",
        "normalizing",
        "an",
        "energy",
        "function",
        "P",
        "(",
        "v",
        ",",
        "h",
        ",",
        "y",
        ")",
        "=",
        "1",
        "Z",
        "exp",
        "(",
        "-E",
        "(",
        "v",
        ",",
        "h",
        ",",
        "y",
        ")",
        ")",
        "Preliminary",
        "work",
        "presented",
        "to",
        "Bruno",
        "Olshausen",
        "'s",
        "lab",
        "and",
        "Google",
        "Brain",
        ",",
        "December",
        "2012.",
        "where",
        "Z",
        "=",
        "v",
        "′",
        ",",
        "h",
        "′",
        ",",
        "y",
        "′",
        "exp",
        "(",
        "-E",
        "(",
        "v",
        "′",
        ",",
        "h",
        "′",
        ",",
        "y",
        "′",
        ")",
        ")",
        "."
      ]
    },
    {
      "sentence": "Z, the partition function, is intractable, due to the summation over all possible states.",
      "tokens": [
        "Z",
        ",",
        "the",
        "partition",
        "function",
        ",",
        "is",
        "intractable",
        ",",
        "due",
        "to",
        "the",
        "summation",
        "over",
        "all",
        "possible",
        "states",
        "."
      ]
    },
    {
      "sentence": "Maximum likelihood learning requires computing the gradient of log Z. Fortunately, the gradient can be estimated using an MCMC procedure (Younes, 1999; Tieleman, 2008) .",
      "tokens": [
        "Maximum",
        "likelihood",
        "learning",
        "requires",
        "computing",
        "the",
        "gradient",
        "of",
        "log",
        "Z.",
        "Fortunately",
        ",",
        "the",
        "gradient",
        "can",
        "be",
        "estimated",
        "using",
        "an",
        "MCMC",
        "procedure",
        "(",
        "Younes",
        ",",
        "1999",
        ";",
        "Tieleman",
        ",",
        "2008",
        ")",
        "."
      ]
    },
    {
      "sentence": "Block Gibbs sampling of the layers makes this procedure efficient.",
      "tokens": [
        "Block",
        "Gibbs",
        "sampling",
        "of",
        "the",
        "layers",
        "makes",
        "this",
        "procedure",
        "efficient",
        "."
      ]
    },
    {
      "sentence": "The structure of the interactions in h determines whether further approximations are necessary.",
      "tokens": [
        "The",
        "structure",
        "of",
        "the",
        "interactions",
        "in",
        "h",
        "determines",
        "whether",
        "further",
        "approximations",
        "are",
        "necessary",
        "."
      ]
    },
    {
      "sentence": "In the pathological case where every element of h is conditionally independent of the others given the visible units, the DBM is simply an RBM and logZ is the only intractable term of the log likelihood.",
      "tokens": [
        "In",
        "the",
        "pathological",
        "case",
        "where",
        "every",
        "element",
        "of",
        "h",
        "is",
        "conditionally",
        "independent",
        "of",
        "the",
        "others",
        "given",
        "the",
        "visible",
        "units",
        ",",
        "the",
        "DBM",
        "is",
        "simply",
        "an",
        "RBM",
        "and",
        "logZ",
        "is",
        "the",
        "only",
        "intractable",
        "term",
        "of",
        "the",
        "log",
        "likelihood",
        "."
      ]
    },
    {
      "sentence": "In the general case, interactions between different elements of h render the posterior P (h | v, y) intractable.",
      "tokens": [
        "In",
        "the",
        "general",
        "case",
        ",",
        "interactions",
        "between",
        "different",
        "elements",
        "of",
        "h",
        "render",
        "the",
        "posterior",
        "P",
        "(",
        "h",
        "|",
        "v",
        ",",
        "y",
        ")",
        "intractable",
        "."
      ]
    },
    {
      "sentence": "Salakhutdinov and Hinton (2009) overcome this by maximizing the lower bound on the log likelihood given by the mean field approximation to the posterior rather than maximizing the log likelihood itself.",
      "tokens": [
        "Salakhutdinov",
        "and",
        "Hinton",
        "(",
        "2009",
        ")",
        "overcome",
        "this",
        "by",
        "maximizing",
        "the",
        "lower",
        "bound",
        "on",
        "the",
        "log",
        "likelihood",
        "given",
        "by",
        "the",
        "mean",
        "field",
        "approximation",
        "to",
        "the",
        "posterior",
        "rather",
        "than",
        "maximizing",
        "the",
        "log",
        "likelihood",
        "itself",
        "."
      ]
    },
    {
      "sentence": "Again, block mean field inference over the layers makes this procedure efficient.",
      "tokens": [
        "Again",
        ",",
        "block",
        "mean",
        "field",
        "inference",
        "over",
        "the",
        "layers",
        "makes",
        "this",
        "procedure",
        "efficient",
        "."
      ]
    },
    {
      "sentence": "An interesting property of the DBM is that the training procedure thus involves feedback connections between the layers.",
      "tokens": [
        "An",
        "interesting",
        "property",
        "of",
        "the",
        "DBM",
        "is",
        "that",
        "the",
        "training",
        "procedure",
        "thus",
        "involves",
        "feedback",
        "connections",
        "between",
        "the",
        "layers",
        "."
      ]
    },
    {
      "sentence": "Consider the simple DBM consisting of all binary valued units, with the energy function E(v, h) = -v T W (1) h (1) -h (1)T W (2) h (2) .",
      "tokens": [
        "Consider",
        "the",
        "simple",
        "DBM",
        "consisting",
        "of",
        "all",
        "binary",
        "valued",
        "units",
        ",",
        "with",
        "the",
        "energy",
        "function",
        "E",
        "(",
        "v",
        ",",
        "h",
        ")",
        "=",
        "-v",
        "T",
        "W",
        "(",
        "1",
        ")",
        "h",
        "(",
        "1",
        ")",
        "-h",
        "(",
        "1",
        ")",
        "T",
        "W",
        "(",
        "2",
        ")",
        "h",
        "(",
        "2",
        ")",
        "."
      ]
    },
    {
      "sentence": "Approximate inference in this model involves repeatedly applying two fixed-point update equations to solve for the mean field approximation to the posterior.",
      "tokens": [
        "Approximate",
        "inference",
        "in",
        "this",
        "model",
        "involves",
        "repeatedly",
        "applying",
        "two",
        "fixed-point",
        "update",
        "equations",
        "to",
        "solve",
        "for",
        "the",
        "mean",
        "field",
        "approximation",
        "to",
        "the",
        "posterior",
        "."
      ]
    },
    {
      "sentence": "Essentially it involves running a recurrent net in order to obtain approximate expectations of the latent variables.",
      "tokens": [
        "Essentially",
        "it",
        "involves",
        "running",
        "a",
        "recurrent",
        "net",
        "in",
        "order",
        "to",
        "obtain",
        "approximate",
        "expectations",
        "of",
        "the",
        "latent",
        "variables",
        "."
      ]
    },
    {
      "sentence": "Beyond their theoretical appeal as a deep model that admits simultaneous training of all components using a generative cost, DBMs have achieved excellent performance in practice.",
      "tokens": [
        "Beyond",
        "their",
        "theoretical",
        "appeal",
        "as",
        "a",
        "deep",
        "model",
        "that",
        "admits",
        "simultaneous",
        "training",
        "of",
        "all",
        "components",
        "using",
        "a",
        "generative",
        "cost",
        ",",
        "DBMs",
        "have",
        "achieved",
        "excellent",
        "performance",
        "in",
        "practice",
        "."
      ]
    },
    {
      "sentence": "When they were first introduced, DBMs set the state of the art on the permutationinvariant version of the MNIST handwritten digit recognition task at 0.95.",
      "tokens": [
        "When",
        "they",
        "were",
        "first",
        "introduced",
        ",",
        "DBMs",
        "set",
        "the",
        "state",
        "of",
        "the",
        "art",
        "on",
        "the",
        "permutationinvariant",
        "version",
        "of",
        "the",
        "MNIST",
        "handwritten",
        "digit",
        "recognition",
        "task",
        "at",
        "0.95",
        "."
      ]
    },
    {
      "sentence": "(By permutation-invariant, we mean that permuting all of the input pixels prior to learning the network should not cause a change in performance, so using synthetic image distortions or convolution to engineer knowledge about the structure of the images into the system is not allowed).",
      "tokens": [
        "(",
        "By",
        "permutation-invariant",
        ",",
        "we",
        "mean",
        "that",
        "permuting",
        "all",
        "of",
        "the",
        "input",
        "pixels",
        "prior",
        "to",
        "learning",
        "the",
        "network",
        "should",
        "not",
        "cause",
        "a",
        "change",
        "in",
        "performance",
        ",",
        "so",
        "using",
        "synthetic",
        "image",
        "distortions",
        "or",
        "convolution",
        "to",
        "engineer",
        "knowledge",
        "about",
        "the",
        "structure",
        "of",
        "the",
        "images",
        "into",
        "the",
        "system",
        "is",
        "not",
        "allowed",
        ")",
        "."
      ]
    },
    {
      "sentence": "Recently, new techniques were used in conjunction with DBM pretraining to set a new state of the art of 0.79 % test error (Hinton et al., 2012) .",
      "tokens": [
        "Recently",
        ",",
        "new",
        "techniques",
        "were",
        "used",
        "in",
        "conjunction",
        "with",
        "DBM",
        "pretraining",
        "to",
        "set",
        "a",
        "new",
        "state",
        "of",
        "the",
        "art",
        "of",
        "0.79",
        "%",
        "test",
        "error",
        "(",
        "Hinton",
        "et",
        "al.",
        ",",
        "2012",
        ")",
        "."
      ]
    }
  ]
}