{
  "title": [
    {
      "sentence": "A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables",
      "tokens": [
        "A",
        "Comparison",
        "of",
        "Machine",
        "Learning",
        "Methods",
        "for",
        "Data",
        "with",
        "High-Cardinality",
        "Categorical",
        "Variables"
      ]
    }
  ],
  "abstract": [
    {
      "sentence": "High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set, or in other words, there are few data points per level.",
      "tokens": [
        "High-cardinality",
        "categorical",
        "variables",
        "are",
        "variables",
        "for",
        "which",
        "the",
        "number",
        "of",
        "different",
        "levels",
        "is",
        "large",
        "relative",
        "to",
        "the",
        "sample",
        "size",
        "of",
        "a",
        "data",
        "set",
        ",",
        "or",
        "in",
        "other",
        "words",
        ",",
        "there",
        "are",
        "few",
        "data",
        "points",
        "per",
        "level",
        "."
      ]
    },
    {
      "sentence": "Machine learning methods can have difficulties with high-cardinality variables.",
      "tokens": [
        "Machine",
        "learning",
        "methods",
        "can",
        "have",
        "difficulties",
        "with",
        "high-cardinality",
        "variables",
        "."
      ]
    },
    {
      "sentence": "In this article, we empirically compare several versions of two of the most successful machine learning methods, tree-boosting and deep neural networks, and linear mixed effects models using multiple tabular data sets with high-cardinality categorical variables.",
      "tokens": [
        "In",
        "this",
        "article",
        ",",
        "we",
        "empirically",
        "compare",
        "several",
        "versions",
        "of",
        "two",
        "of",
        "the",
        "most",
        "successful",
        "machine",
        "learning",
        "methods",
        ",",
        "tree-boosting",
        "and",
        "deep",
        "neural",
        "networks",
        ",",
        "and",
        "linear",
        "mixed",
        "effects",
        "models",
        "using",
        "multiple",
        "tabular",
        "data",
        "sets",
        "with",
        "high-cardinality",
        "categorical",
        "variables",
        "."
      ]
    },
    {
      "sentence": "We find that, first, machine learning models with random effects have higher prediction accuracy than their classical counterparts without random effects, and, second, tree-boosting with random effects outperforms deep neural networks with random effects.",
      "tokens": [
        "We",
        "find",
        "that",
        ",",
        "first",
        ",",
        "machine",
        "learning",
        "models",
        "with",
        "random",
        "effects",
        "have",
        "higher",
        "prediction",
        "accuracy",
        "than",
        "their",
        "classical",
        "counterparts",
        "without",
        "random",
        "effects",
        ",",
        "and",
        ",",
        "second",
        ",",
        "tree-boosting",
        "with",
        "random",
        "effects",
        "outperforms",
        "deep",
        "neural",
        "networks",
        "with",
        "random",
        "effects",
        "."
      ]
    }
  ],
  "introduction": [
    {
      "sentence": "Introduction High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set or, equivalently, there is little data per level.",
      "tokens": [
        "Introduction",
        "High-cardinality",
        "categorical",
        "variables",
        "are",
        "variables",
        "for",
        "which",
        "the",
        "number",
        "of",
        "different",
        "levels",
        "is",
        "large",
        "relative",
        "to",
        "the",
        "sample",
        "size",
        "of",
        "a",
        "data",
        "set",
        "or",
        ",",
        "equivalently",
        ",",
        "there",
        "is",
        "little",
        "data",
        "per",
        "level",
        "."
      ]
    },
    {
      "sentence": "Highcardinality categorical variables can pose difficulties for machine learning methods such as deep neural networks and tree-based models.",
      "tokens": [
        "Highcardinality",
        "categorical",
        "variables",
        "can",
        "pose",
        "difficulties",
        "for",
        "machine",
        "learning",
        "methods",
        "such",
        "as",
        "deep",
        "neural",
        "networks",
        "and",
        "tree-based",
        "models",
        "."
      ]
    },
    {
      "sentence": "A simple strategy for dealing with categorical variables is to use one-hot encoding or dummy variables.",
      "tokens": [
        "A",
        "simple",
        "strategy",
        "for",
        "dealing",
        "with",
        "categorical",
        "variables",
        "is",
        "to",
        "use",
        "one-hot",
        "encoding",
        "or",
        "dummy",
        "variables",
        "."
      ]
    },
    {
      "sentence": "But this approach often does not work well for high-cardinality categorical variables due to the reasons described below.",
      "tokens": [
        "But",
        "this",
        "approach",
        "often",
        "does",
        "not",
        "work",
        "well",
        "for",
        "high-cardinality",
        "categorical",
        "variables",
        "due",
        "to",
        "the",
        "reasons",
        "described",
        "below",
        "."
      ]
    },
    {
      "sentence": "For neural networks, a frequently adopted solution is to use entity embeddings [Guo and Berkhahn, 2016 ] that map every level of a categorical variable into a low-dimensional Euclidean space.",
      "tokens": [
        "For",
        "neural",
        "networks",
        ",",
        "a",
        "frequently",
        "adopted",
        "solution",
        "is",
        "to",
        "use",
        "entity",
        "embeddings",
        "[",
        "Guo",
        "and",
        "Berkhahn",
        ",",
        "2016",
        "]",
        "that",
        "map",
        "every",
        "level",
        "of",
        "a",
        "categorical",
        "variable",
        "into",
        "a",
        "low-dimensional",
        "Euclidean",
        "space",
        "."
      ]
    },
    {
      "sentence": "For tree-boosting, an alternative to one-hot encoding is to assign a number to every level of a categorical variable, and then consider this as a one-dimensional numeric variable.",
      "tokens": [
        "For",
        "tree-boosting",
        ",",
        "an",
        "alternative",
        "to",
        "one-hot",
        "encoding",
        "is",
        "to",
        "assign",
        "a",
        "number",
        "to",
        "every",
        "level",
        "of",
        "a",
        "categorical",
        "variable",
        ",",
        "and",
        "then",
        "consider",
        "this",
        "as",
        "a",
        "one-dimensional",
        "numeric",
        "variable",
        "."
      ]
    },
    {
      "sentence": "Another solution implemented in the LightGBM boosting library [Ke et al., 2017] works by partitioning all levels into two subsets using an approximate approach [Fisher, 1958] when finding splits in the tree-building algorithm.",
      "tokens": [
        "Another",
        "solution",
        "implemented",
        "in",
        "the",
        "LightGBM",
        "boosting",
        "library",
        "[",
        "Ke",
        "et",
        "al.",
        ",",
        "2017",
        "]",
        "works",
        "by",
        "partitioning",
        "all",
        "levels",
        "into",
        "two",
        "subsets",
        "using",
        "an",
        "approximate",
        "approach",
        "[",
        "Fisher",
        ",",
        "1958",
        "]",
        "when",
        "finding",
        "splits",
        "in",
        "the",
        "tree-building",
        "algorithm",
        "."
      ]
    },
    {
      "sentence": "Further, the CatBoost boosting library [Prokhorenkova et al., 2018] implements an approach based on ordered target statistics calculated using random partitions of the training data for handling categorical predictor variables.",
      "tokens": [
        "Further",
        ",",
        "the",
        "CatBoost",
        "boosting",
        "library",
        "[",
        "Prokhorenkova",
        "et",
        "al.",
        ",",
        "2018",
        "]",
        "implements",
        "an",
        "approach",
        "based",
        "on",
        "ordered",
        "target",
        "statistics",
        "calculated",
        "using",
        "random",
        "partitions",
        "of",
        "the",
        "training",
        "data",
        "for",
        "handling",
        "categorical",
        "predictor",
        "variables",
        "."
      ]
    },
    {
      "sentence": "Random effects [Laird et al., 1982, Pinheiro and Bates, 2006] can also be used as a tool for handling high-cardinality categorical variables.",
      "tokens": [
        "Random",
        "effects",
        "[",
        "Laird",
        "et",
        "al.",
        ",",
        "1982",
        ",",
        "Pinheiro",
        "and",
        "Bates",
        ",",
        "2006",
        "]",
        "can",
        "also",
        "be",
        "used",
        "as",
        "a",
        "tool",
        "for",
        "handling",
        "high-cardinality",
        "categorical",
        "variables",
        "."
      ]
    },
    {
      "sentence": "In a random effects model, it is assumed that a (potentially transformed) parameter µ ∈ R n of the response variable distribution equals the sum of fixed F (X) and random effects Zb: µ = F (X) + Zb, b ∼ N (0, Σ), where F (X) is the row-wise evaluation of a function F (•) : R p → R, F (X) = (F (X 1 ), .",
      "tokens": [
        "In",
        "a",
        "random",
        "effects",
        "model",
        ",",
        "it",
        "is",
        "assumed",
        "that",
        "a",
        "(",
        "potentially",
        "transformed",
        ")",
        "parameter",
        "µ",
        "∈",
        "R",
        "n",
        "of",
        "the",
        "response",
        "variable",
        "distribution",
        "equals",
        "the",
        "sum",
        "of",
        "fixed",
        "F",
        "(",
        "X",
        ")",
        "and",
        "random",
        "effects",
        "Zb",
        ":",
        "µ",
        "=",
        "F",
        "(",
        "X",
        ")",
        "+",
        "Zb",
        ",",
        "b",
        "∼",
        "N",
        "(",
        "0",
        ",",
        "Σ",
        ")",
        ",",
        "where",
        "F",
        "(",
        "X",
        ")",
        "is",
        "the",
        "row-wise",
        "evaluation",
        "of",
        "a",
        "function",
        "F",
        "(",
        "•",
        ")",
        ":",
        "R",
        "p",
        "→",
        "R",
        ",",
        "F",
        "(",
        "X",
        ")",
        "=",
        "(",
        "F",
        "(",
        "X",
        "1",
        ")",
        ",",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ", F (X n )) T , X i = (X i1 .",
      "tokens": [
        ",",
        "F",
        "(",
        "X",
        "n",
        ")",
        ")",
        "T",
        ",",
        "X",
        "i",
        "=",
        "(",
        "X",
        "i1",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ", X ip ) T ∈ R p is the i-th row of the fixed effects variables matrix X ∈ R n×p , i = 1, .",
      "tokens": [
        ",",
        "X",
        "ip",
        ")",
        "T",
        "∈",
        "R",
        "p",
        "is",
        "the",
        "i-th",
        "row",
        "of",
        "the",
        "fixed",
        "effects",
        "variables",
        "matrix",
        "X",
        "∈",
        "R",
        "n×p",
        ",",
        "i",
        "=",
        "1",
        ",",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ", n, b ∈ R m , and Z ∈ R n×m .",
      "tokens": [
        ",",
        "n",
        ",",
        "b",
        "∈",
        "R",
        "m",
        ",",
        "and",
        "Z",
        "∈",
        "R",
        "n×m",
        "."
      ]
    },
    {
      "sentence": "These models are called mixed effects models since they contain both fixed effects F (X) and random effects Zb.",
      "tokens": [
        "These",
        "models",
        "are",
        "called",
        "mixed",
        "effects",
        "models",
        "since",
        "they",
        "contain",
        "both",
        "fixed",
        "effects",
        "F",
        "(",
        "X",
        ")",
        "and",
        "random",
        "effects",
        "Zb",
        "."
      ]
    },
    {
      "sentence": "If the conditional response variable distribution is Gaussian and there is a single high-cardinality categorical variable, such a mixed effects model can also be written as y ij = F (x ij ) + b i + ϵ ij , b i iid ∼ N (0, σ 2 1 ), ϵ ij iid ∼ N (0, σ 2 ), (1) where j = 1, .",
      "tokens": [
        "If",
        "the",
        "conditional",
        "response",
        "variable",
        "distribution",
        "is",
        "Gaussian",
        "and",
        "there",
        "is",
        "a",
        "single",
        "high-cardinality",
        "categorical",
        "variable",
        ",",
        "such",
        "a",
        "mixed",
        "effects",
        "model",
        "can",
        "also",
        "be",
        "written",
        "as",
        "y",
        "ij",
        "=",
        "F",
        "(",
        "x",
        "ij",
        ")",
        "+",
        "b",
        "i",
        "+",
        "ϵ",
        "ij",
        ",",
        "b",
        "i",
        "iid",
        "∼",
        "N",
        "(",
        "0",
        ",",
        "σ",
        "2",
        "1",
        ")",
        ",",
        "ϵ",
        "ij",
        "iid",
        "∼",
        "N",
        "(",
        "0",
        ",",
        "σ",
        "2",
        ")",
        ",",
        "(",
        "1",
        ")",
        "where",
        "j",
        "=",
        "1",
        ",",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ", n i is the sample index within level i with n i being the number of samples for which the categorical variable attains level i, i = 1, .",
      "tokens": [
        ",",
        "n",
        "i",
        "is",
        "the",
        "sample",
        "index",
        "within",
        "level",
        "i",
        "with",
        "n",
        "i",
        "being",
        "the",
        "number",
        "of",
        "samples",
        "for",
        "which",
        "the",
        "categorical",
        "variable",
        "attains",
        "level",
        "i",
        ",",
        "i",
        "=",
        "1",
        ",",
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ".",
      "tokens": [
        "."
      ]
    },
    {
      "sentence": ", q is the level index with q being the number of levels of the categorical variable, and x ij are the fixed effects predictor variables for observation ij.",
      "tokens": [
        ",",
        "q",
        "is",
        "the",
        "level",
        "index",
        "with",
        "q",
        "being",
        "the",
        "number",
        "of",
        "levels",
        "of",
        "the",
        "categorical",
        "variable",
        ",",
        "and",
        "x",
        "ij",
        "are",
        "the",
        "fixed",
        "effects",
        "predictor",
        "variables",
        "for",
        "observation",
        "ij",
        "."
      ]
    },
    {
      "sentence": "The total number of samples is n = q i=1 n i .",
      "tokens": [
        "The",
        "total",
        "number",
        "of",
        "samples",
        "is",
        "n",
        "=",
        "q",
        "i=1",
        "n",
        "i",
        "."
      ]
    },
    {
      "sentence": "Further, the random effects b i and ϵ ij are assumed to be independent.",
      "tokens": [
        "Further",
        ",",
        "the",
        "random",
        "effects",
        "b",
        "i",
        "and",
        "ϵ",
        "ij",
        "are",
        "assumed",
        "to",
        "be",
        "independent",
        "."
      ]
    },
    {
      "sentence": "For this model, the matrix Z is simply a binary incidence matrix that maps every random effect b i to its corresponding observations and Σ = σ 2 I m .",
      "tokens": [
        "For",
        "this",
        "model",
        ",",
        "the",
        "matrix",
        "Z",
        "is",
        "simply",
        "a",
        "binary",
        "incidence",
        "matrix",
        "that",
        "maps",
        "every",
        "random",
        "effect",
        "b",
        "i",
        "to",
        "its",
        "corresponding",
        "observations",
        "and",
        "Σ",
        "=",
        "σ",
        "2",
        "I",
        "m",
        "."
      ]
    },
    {
      "sentence": "In (generalized) linear mixed effects models it is assumed that F (•) is a linear function: F (X) = Xβ.",
      "tokens": [
        "In",
        "(",
        "generalized",
        ")",
        "linear",
        "mixed",
        "effects",
        "models",
        "it",
        "is",
        "assumed",
        "that",
        "F",
        "(",
        "•",
        ")",
        "is",
        "a",
        "linear",
        "function",
        ":",
        "F",
        "(",
        "X",
        ")",
        "=",
        "Xβ",
        "."
      ]
    },
    {
      "sentence": "In the last years, linear mixed effects models have been extended to non-linear ones using single trees [Hajjem et al., 2011 , Sela and Simonoff, 2012 , Fu and Simonoff, 2015] , random forest [Hajjem et al., 2014] , tree-boosting [Sigrist, 2022 [Sigrist, , 2023]] , and most recently (in terms of first public preprint) deep neural networks [Simchoni and Rosset, 2021 , 2023 , Avanzi et al., 2023] .",
      "tokens": [
        "In",
        "the",
        "last",
        "years",
        ",",
        "linear",
        "mixed",
        "effects",
        "models",
        "have",
        "been",
        "extended",
        "to",
        "non-linear",
        "ones",
        "using",
        "single",
        "trees",
        "[",
        "Hajjem",
        "et",
        "al.",
        ",",
        "2011",
        ",",
        "Sela",
        "and",
        "Simonoff",
        ",",
        "2012",
        ",",
        "Fu",
        "and",
        "Simonoff",
        ",",
        "2015",
        "]",
        ",",
        "random",
        "forest",
        "[",
        "Hajjem",
        "et",
        "al.",
        ",",
        "2014",
        "]",
        ",",
        "tree-boosting",
        "[",
        "Sigrist",
        ",",
        "2022",
        "[",
        "Sigrist",
        ",",
        ",",
        "2023",
        "]",
        "]",
        ",",
        "and",
        "most",
        "recently",
        "(",
        "in",
        "terms",
        "of",
        "first",
        "public",
        "preprint",
        ")",
        "deep",
        "neural",
        "networks",
        "[",
        "Simchoni",
        "and",
        "Rosset",
        ",",
        "2021",
        ",",
        "2023",
        ",",
        "Avanzi",
        "et",
        "al.",
        ",",
        "2023",
        "]",
        "."
      ]
    },
    {
      "sentence": "In contrast to classical independent machine learning models, the random effects introduce dependence among samples.",
      "tokens": [
        "In",
        "contrast",
        "to",
        "classical",
        "independent",
        "machine",
        "learning",
        "models",
        ",",
        "the",
        "random",
        "effects",
        "introduce",
        "dependence",
        "among",
        "samples",
        "."
      ]
    },
    {
      "sentence": "1.1 Why are random effects useful for high-cardinality categorical variables?",
      "tokens": [
        "1.1",
        "Why",
        "are",
        "random",
        "effects",
        "useful",
        "for",
        "high-cardinality",
        "categorical",
        "variables",
        "?"
      ]
    },
    {
      "sentence": "For high-cardinality categorical variables, there is little data for every level.",
      "tokens": [
        "For",
        "high-cardinality",
        "categorical",
        "variables",
        ",",
        "there",
        "is",
        "little",
        "data",
        "for",
        "every",
        "level",
        "."
      ]
    },
    {
      "sentence": "Intuitively, if the response variable has a different (conditional) mean for many levels, traditional machine learning models (with, e.g., one-hot encoding, embeddings, or simply one-dimensional numeric variables) may have problems with over-or underfitting for such data.",
      "tokens": [
        "Intuitively",
        ",",
        "if",
        "the",
        "response",
        "variable",
        "has",
        "a",
        "different",
        "(",
        "conditional",
        ")",
        "mean",
        "for",
        "many",
        "levels",
        ",",
        "traditional",
        "machine",
        "learning",
        "models",
        "(",
        "with",
        ",",
        "e.g.",
        ",",
        "one-hot",
        "encoding",
        ",",
        "embeddings",
        ",",
        "or",
        "simply",
        "one-dimensional",
        "numeric",
        "variables",
        ")",
        "may",
        "have",
        "problems",
        "with",
        "over-or",
        "underfitting",
        "for",
        "such",
        "data",
        "."
      ]
    },
    {
      "sentence": "From the point of view of a classical bias-variance trade-off, independent machine learning models may have difficulties balancing this trade-off and finding an appropriate amount of regularization.",
      "tokens": [
        "From",
        "the",
        "point",
        "of",
        "view",
        "of",
        "a",
        "classical",
        "bias-variance",
        "trade-off",
        ",",
        "independent",
        "machine",
        "learning",
        "models",
        "may",
        "have",
        "difficulties",
        "balancing",
        "this",
        "trade-off",
        "and",
        "finding",
        "an",
        "appropriate",
        "amount",
        "of",
        "regularization",
        "."
      ]
    },
    {
      "sentence": "For instance, overfitting may occur which means that a model has a low bias but high variance.",
      "tokens": [
        "For",
        "instance",
        ",",
        "overfitting",
        "may",
        "occur",
        "which",
        "means",
        "that",
        "a",
        "model",
        "has",
        "a",
        "low",
        "bias",
        "but",
        "high",
        "variance",
        "."
      ]
    },
    {
      "sentence": "Broadly speaking, random effects act as a prior, or regularizer, which models the difficult part of a function, i.e., the part whose \"dimension\" is similar to the total sample size, and, in doing so, provide an effective way for finding a balance between over-and underfitting or bias and variance.",
      "tokens": [
        "Broadly",
        "speaking",
        ",",
        "random",
        "effects",
        "act",
        "as",
        "a",
        "prior",
        ",",
        "or",
        "regularizer",
        ",",
        "which",
        "models",
        "the",
        "difficult",
        "part",
        "of",
        "a",
        "function",
        ",",
        "i.e.",
        ",",
        "the",
        "part",
        "whose",
        "``",
        "dimension",
        "''",
        "is",
        "similar",
        "to",
        "the",
        "total",
        "sample",
        "size",
        ",",
        "and",
        ",",
        "in",
        "doing",
        "so",
        ",",
        "provide",
        "an",
        "effective",
        "way",
        "for",
        "finding",
        "a",
        "balance",
        "between",
        "over-and",
        "underfitting",
        "or",
        "bias",
        "and",
        "variance",
        "."
      ]
    },
    {
      "sentence": "For instance, for a single categorical variable, random effects models will shrink estimates of group intercept effects towards the global mean.",
      "tokens": [
        "For",
        "instance",
        ",",
        "for",
        "a",
        "single",
        "categorical",
        "variable",
        ",",
        "random",
        "effects",
        "models",
        "will",
        "shrink",
        "estimates",
        "of",
        "group",
        "intercept",
        "effects",
        "towards",
        "the",
        "global",
        "mean",
        "."
      ]
    },
    {
      "sentence": "This process is sometimes also called \"information pooling\".",
      "tokens": [
        "This",
        "process",
        "is",
        "sometimes",
        "also",
        "called",
        "``",
        "information",
        "pooling",
        "''",
        "."
      ]
    },
    {
      "sentence": "It represents a trade-off between completely ignoring the categorical variable (= underfitting / high bias and low variance) and giving every level in the categorical variable \"complete freedom\" in estimation (= overfitting / low bias and high variance).",
      "tokens": [
        "It",
        "represents",
        "a",
        "trade-off",
        "between",
        "completely",
        "ignoring",
        "the",
        "categorical",
        "variable",
        "(",
        "=",
        "underfitting",
        "/",
        "high",
        "bias",
        "and",
        "low",
        "variance",
        ")",
        "and",
        "giving",
        "every",
        "level",
        "in",
        "the",
        "categorical",
        "variable",
        "``",
        "complete",
        "freedom",
        "''",
        "in",
        "estimation",
        "(",
        "=",
        "overfitting",
        "/",
        "low",
        "bias",
        "and",
        "high",
        "variance",
        ")",
        "."
      ]
    },
    {
      "sentence": "Importantly, the amount of regularization, which is determined by the variance parameters of the model, is learned from the data.",
      "tokens": [
        "Importantly",
        ",",
        "the",
        "amount",
        "of",
        "regularization",
        ",",
        "which",
        "is",
        "determined",
        "by",
        "the",
        "variance",
        "parameters",
        "of",
        "the",
        "model",
        ",",
        "is",
        "learned",
        "from",
        "the",
        "data",
        "."
      ]
    },
    {
      "sentence": "Specifically, in the above single-level random effects model in (1), a (point) prediction ŷp for the response variable for a sample with predictor variables x p and categorical variable having level i is given by ŷp = F (x p ) + σ2 1 σ2 /n i + σ2 1 (ȳ i -Fi ), where F (x p ) is the trained function evaluated at x p , σ2 1 and σ2 are variance estimates, and ȳi and Fi are sample means of y ij and F (x ij ), respectively, for level i.",
      "tokens": [
        "Specifically",
        ",",
        "in",
        "the",
        "above",
        "single-level",
        "random",
        "effects",
        "model",
        "in",
        "(",
        "1",
        ")",
        ",",
        "a",
        "(",
        "point",
        ")",
        "prediction",
        "ŷp",
        "for",
        "the",
        "response",
        "variable",
        "for",
        "a",
        "sample",
        "with",
        "predictor",
        "variables",
        "x",
        "p",
        "and",
        "categorical",
        "variable",
        "having",
        "level",
        "i",
        "is",
        "given",
        "by",
        "ŷp",
        "=",
        "F",
        "(",
        "x",
        "p",
        ")",
        "+",
        "σ2",
        "1",
        "σ2",
        "/n",
        "i",
        "+",
        "σ2",
        "1",
        "(",
        "ȳ",
        "i",
        "-Fi",
        ")",
        ",",
        "where",
        "F",
        "(",
        "x",
        "p",
        ")",
        "is",
        "the",
        "trained",
        "function",
        "evaluated",
        "at",
        "x",
        "p",
        ",",
        "σ2",
        "1",
        "and",
        "σ2",
        "are",
        "variance",
        "estimates",
        ",",
        "and",
        "ȳi",
        "and",
        "Fi",
        "are",
        "sample",
        "means",
        "of",
        "y",
        "ij",
        "and",
        "F",
        "(",
        "x",
        "ij",
        ")",
        ",",
        "respectively",
        ",",
        "for",
        "level",
        "i",
        "."
      ]
    },
    {
      "sentence": "Ignoring the categorical variable would give the prediction ŷp = F (x p ), and a fully flexible model without regularization gives ŷp = F (x p )+(ȳ i -Fi ).",
      "tokens": [
        "Ignoring",
        "the",
        "categorical",
        "variable",
        "would",
        "give",
        "the",
        "prediction",
        "ŷp",
        "=",
        "F",
        "(",
        "x",
        "p",
        ")",
        ",",
        "and",
        "a",
        "fully",
        "flexible",
        "model",
        "without",
        "regularization",
        "gives",
        "ŷp",
        "=",
        "F",
        "(",
        "x",
        "p",
        ")",
        "+",
        "(",
        "ȳ",
        "i",
        "-Fi",
        ")",
        "."
      ]
    },
    {
      "sentence": "I.e., the difference between these two extreme cases and the random effects model is the shrinkage factor σ2 1 σ2 /ni+σ foot_1 1 (which goes to zero if the number of samples n i for level i is large).",
      "tokens": [
        "I.e.",
        ",",
        "the",
        "difference",
        "between",
        "these",
        "two",
        "extreme",
        "cases",
        "and",
        "the",
        "random",
        "effects",
        "model",
        "is",
        "the",
        "shrinkage",
        "factor",
        "σ2",
        "1",
        "σ2",
        "/ni+σ",
        "foot_1",
        "1",
        "(",
        "which",
        "goes",
        "to",
        "zero",
        "if",
        "the",
        "number",
        "of",
        "samples",
        "n",
        "i",
        "for",
        "level",
        "i",
        "is",
        "large",
        ")",
        "."
      ]
    },
    {
      "sentence": "Related to this, random effects models allow for more efficient (i.e., lower variance) estimation of the fixed effects function F (•) [Sigrist, 2022] .",
      "tokens": [
        "Related",
        "to",
        "this",
        ",",
        "random",
        "effects",
        "models",
        "allow",
        "for",
        "more",
        "efficient",
        "(",
        "i.e.",
        ",",
        "lower",
        "variance",
        ")",
        "estimation",
        "of",
        "the",
        "fixed",
        "effects",
        "function",
        "F",
        "(",
        "•",
        ")",
        "[",
        "Sigrist",
        ",",
        "2022",
        "]",
        "."
      ]
    },
    {
      "sentence": "See also Sigrist [2023, Section 1.1] for a discussion on why random effects are useful for modeling high-cardinality categorical variables.",
      "tokens": [
        "See",
        "also",
        "Sigrist",
        "[",
        "2023",
        ",",
        "Section",
        "1.1",
        "]",
        "for",
        "a",
        "discussion",
        "on",
        "why",
        "random",
        "effects",
        "are",
        "useful",
        "for",
        "modeling",
        "high-cardinality",
        "categorical",
        "variables",
        "."
      ]
    },
    {
      "sentence": "In line with the above argumentation, Sigrist [2023, Section 4 .1] find in empirical experiments that tree-boosting combined with random effects outperforms traditional independent tree-boosting the more, the lower the number of samples per level of a categorical variable, i.e., the higher the cardinality of a categorical variable.",
      "tokens": [
        "In",
        "line",
        "with",
        "the",
        "above",
        "argumentation",
        ",",
        "Sigrist",
        "[",
        "2023",
        ",",
        "Section",
        "4",
        ".1",
        "]",
        "find",
        "in",
        "empirical",
        "experiments",
        "that",
        "tree-boosting",
        "combined",
        "with",
        "random",
        "effects",
        "outperforms",
        "traditional",
        "independent",
        "tree-boosting",
        "the",
        "more",
        ",",
        "the",
        "lower",
        "the",
        "number",
        "of",
        "samples",
        "per",
        "level",
        "of",
        "a",
        "categorical",
        "variable",
        ",",
        "i.e.",
        ",",
        "the",
        "higher",
        "the",
        "cardinality",
        "of",
        "a",
        "categorical",
        "variable",
        "."
      ]
    },
    {
      "sentence": "2 Methods, data sets, and experimental settings In the following, we compare several methods using multiple real-world data sets with highcardinality categorical variables.",
      "tokens": [
        "2",
        "Methods",
        ",",
        "data",
        "sets",
        ",",
        "and",
        "experimental",
        "settings",
        "In",
        "the",
        "following",
        ",",
        "we",
        "compare",
        "several",
        "methods",
        "using",
        "multiple",
        "real-world",
        "data",
        "sets",
        "with",
        "highcardinality",
        "categorical",
        "variables",
        "."
      ]
    },
    {
      "sentence": "We use all the publicly available tabular data sets from Simchoni and Rosset [2021, 2023] and also the same experimental setting as in Simchoni and Rosset [2021, 2023] .",
      "tokens": [
        "We",
        "use",
        "all",
        "the",
        "publicly",
        "available",
        "tabular",
        "data",
        "sets",
        "from",
        "Simchoni",
        "and",
        "Rosset",
        "[",
        "2021",
        ",",
        "2023",
        "]",
        "and",
        "also",
        "the",
        "same",
        "experimental",
        "setting",
        "as",
        "in",
        "Simchoni",
        "and",
        "Rosset",
        "[",
        "2021",
        ",",
        "2023",
        "]",
        "."
      ]
    },
    {
      "sentence": "In addition, we include the Wages data set analyzed in Sigrist [2022] .",
      "tokens": [
        "In",
        "addition",
        ",",
        "we",
        "include",
        "the",
        "Wages",
        "data",
        "set",
        "analyzed",
        "in",
        "Sigrist",
        "[",
        "2022",
        "]",
        "."
      ]
    }
  ]
}